{"id": "ZSYzvzUNcQn", "cdate": 1695530370077, "mdate": null, "content": {"title": "Automated feature enhancement for predictive modeling using external knowledge", "abstract": "Supervised machine learning is the task of learning a function that maps features to a target. The strength of that function or the model depends directly on the features provided to the learning algorithm. Specifically, a crucial means of improving the model quality is to add new predictive features. This is often performed by domain specialists or data scientists. It is a hard and time-consuming task because the domain expert needs to identify data sources for new features, join them, and then select those that actually are relevant to the prediction. We present a new system called KAFE (Knowledge Aided Feature Engineering), an interactive predictive modeling system that automatically utilizes structured knowledge present on the web to perform feature addition to improve the accuracy of predictive models. In this proposal, we describe the key techniques such as feature inference and selection, relevant data indexing, and demonstrate its use through an interactive Jupyter notebook."}}
{"id": "SWeAMcbMXc", "cdate": 1648658966071, "mdate": 1648658966071, "content": {"title": "Causal Feature Selection for Algorithmic Fairness", "abstract": "The use of machine learning (ML) in high-stakes societal decisions\nhas encouraged the consideration of fairness throughout the ML\nlifecycle. Although data integration is one of the primary steps to\ngenerate high-quality training data, most of the fairness literature\nignores this stage. In this work, we consider fairness in the integra-\ntion component of data management, aiming to identify features\nthat improve prediction without adding any bias to the dataset.\nWe work under the causal fairness paradigm [45 ]. Without requir-\ning the underlying structural causal model a priori, we propose\nan approach to identify a sub-collection of features that ensure\nfairness of the dataset by performing conditional independence\ntests between different subsets of features. We use group testing to\nimprove the complexity of the approach. We theoretically prove the\ncorrectness of the proposed algorithm and show that sublinear con-\nditional independence tests are sufficient to identify these variables.\nA detailed empirical evaluation is performed on real-world datasets\nto demonstrate the efficacy and efficiency of our technique"}}
