{"id": "OAYsHncySL4", "cdate": 1653925429230, "mdate": null, "content": {"title": "The (Un)Surprising Effectiveness of Pre-Trained Vision Models for Control", "abstract": "Recent years have seen the emergence of pre-trained representations as a powerful abstraction for AI applications in computer vision, natural language, and speech. However, policy learning for control is still dominated by a tabula-rasa learning paradigm, with visuo-motor policies often trained from scratch using data from deployment environments.\n\nIn this context, we revisit the role of pre-trained visual representations (PVRs) for control, and in particular representations trained using large-scale computer vision datasets. Through extensive empirical evaluation in diverse control domains, we isolate and study the importance of different representation training methods, data augmentations, and feature hierarchies.\n\nOverall, we find that frozen pre-trained visual representations can be competitive or even better than ground-truth state representations to train control policies with imitation learning (behavior cloning). This is in spite of pre-training representations entirely on out-of-domain data from standard computer vision datasets, without even a single frame from the deployment domains."}}
{"id": "knKJgksd7kA", "cdate": 1621630218385, "mdate": null, "content": {"title": "Interesting Object, Curious Agent: Learning Task-Agnostic Exploration", "abstract": "Common approaches for task-agnostic exploration learn tabula-rasa --the agent assumes isolated environments and no prior knowledge or experience. However, in the real world, agents learn in many environments and always come with prior experiences as they explore new ones. Exploration is a lifelong process. In this paper, we propose a paradigm change in the formulation and evaluation of task-agnostic exploration. In this setup, the agent first learns to explore across many environments without any extrinsic goal in a task-agnostic manner.\nLater on, the agent effectively transfers the learned exploration policy to better explore new environments when solving tasks. In this context, we evaluate several baseline exploration strategies and present a simple yet effective approach to learning task-agnostic exploration policies. Our key idea is that there are two components of exploration: (1) an agent-centric component encouraging exploration of unseen parts of the environment based on an agent\u2019s belief; (2) an environment-centric component encouraging exploration of inherently interesting objects. We show that our formulation is effective and provides the most consistent exploration across several training-testing environment pairs. We also introduce benchmarks and metrics for evaluating task-agnostic exploration strategies. The source code is available at https://github.com/sparisi/cbet/."}}
{"id": "SkZWk0xd-B", "cdate": 1483228800000, "mdate": null, "content": {"title": "Policy Search with High-Dimensional Context Variables", "abstract": "Direct contextual policy search methods learn to improve policy\r\nparameters and simultaneously generalize these parameters\r\nto different context or task variables. However, learning\r\nfrom high-dimensional context variables, such as camera images,\r\nis still a prominent problem in many real-world tasks.\r\nA naive application of unsupervised dimensionality reduction\r\nmethods to the context variables, such as principal component\r\nanalysis, is insufficient as task-relevant input may be ignored.\r\nIn this paper, we propose a contextual policy search method in\r\nthe model-based relative entropy stochastic search framework\r\nwith integrated dimensionality reduction. We learn a model of\r\nthe reward that is locally quadratic in both the policy parameters\r\nand the context variables. Furthermore, we perform supervised\r\nlinear dimensionality reduction on the context variables\r\nby nuclear norm regularization. The experimental results\r\nshow that the proposed method outperforms naive dimensionality\r\nreduction via principal component analysis and\r\na state-of-the-art contextual policy search method."}}
{"id": "HJWEye-uWH", "cdate": 1420070400000, "mdate": null, "content": {"title": "Multi-Objective Reinforcement Learning with Continuous Pareto Frontier Approximation", "abstract": "This paper is about learning a continuous approximation of the Pareto frontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose a policy-based approach that exploits gradient information to generate solutions close to the Pareto ones. Differently from previous policy-gradient multi-objective algorithms, where n optimization routines are used to have n solutions, our approach performs a single gradient-ascent run that at each step generates an improved continuous approximation of the Pareto frontier. The idea is to exploit a gradient-based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non-trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two interesting MOMDPs."}}
