{"id": "vjKIKdXijK", "cdate": 1652737763801, "mdate": null, "content": {"title": "Convexity Certificates from Hessians", "abstract": "The Hessian of a differentiable convex function is positive semidefinite. Therefore, checking the Hessian of a given function is a natural approach to certify convexity. However, implementing this approach is not straightforward, since it requires a representation of the Hessian that allows its analysis. Here, we implement this approach for a class of functions that is rich enough to support classical machine learning. For this class of functions, it was recently shown how to compute computational graphs of their Hessians. We show how to check these graphs for positive-semidefiniteness. We compare our implementation of the Hessian approach with the well-established disciplined convex programming (DCP) approach and prove that the Hessian approach is at least as powerful as the DCP approach for differentiable functions. Furthermore, we show for a state-of-the-art implementation of the DCP approach that the Hessian approach is  actually more powerful, that is, it can certify the convexity of a larger class of differentiable functions."}}
{"id": "rjsEx-MldTB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Using Benson's Algorithm for Regularization Parameter Tracking.", "abstract": "Regularized loss minimization, where a statistical model is obtained from minimizing the sum of a loss function and weighted regularization terms, is still in widespread use in machine learning. The statistical performance of the resulting models depends on the choice of weights (regularization parameters) that are typically tuned by cross-validation. For finding the best regularization parameters, the regularized minimization problem needs to be solved for the whole parameter domain. A practically more feasible approach is covering the parameter domain with approximate solutions of the loss minimization problem for some prescribed approximation accuracy. The problem of computing such a covering is known as the approximate solution gamut problem. Existing algorithms for the solution gamut problem suffer from several problems. For instance, they require a grid on the parameter domain whose spacing is difficult to determine in practice, and they are not generic in the sense that they rely on problem specific plug-in functions. Here, we show that a well-known algorithm from vector optimization, namely the Benson algorithm, can be used directly for computing approximate solution gamuts while avoiding the problems of existing algorithms. Experiments for the Elastic Net on real world data sets demonstrate the effectiveness of Benson\u2019s algorithm for regularization parameter tracking."}}
{"id": "H1bQ0PZuWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Computing Higher Order Derivatives of Matrix and Tensor Expressions", "abstract": "Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives."}}
{"id": "SyZ8vn-ObH", "cdate": 1420070400000, "mdate": null, "content": {"title": "Tracking Approximate Solutions of Parameterized Optimization Problems over Multi-Dimensional (Hyper-)Parameter Domains", "abstract": "Many machine learning methods are given as parameterized optimization problems. Important examples of such parameters are regularization- and kernel hyperparameters. These parameters have to be tun..."}}
{"id": "BJ-2gnWd-H", "cdate": 1388534400000, "mdate": null, "content": {"title": "Robust and Efficient Kernel Hyperparameter Paths with Guarantees", "abstract": "Algorithmically, many machine learning tasks boil down to solving parameterized optimization problems. Finding good values for the parameters has significant influence on the statistical performance of these methods. Thus supporting the choice of parameter values algorithmically has received quite some attention recently, especially algorithms for computing the whole solution path of parameterized optimization problem. These algorithms can be used, for instance, to track the solution of a regularized learning problem along the regularization parameter path, or for tracking the solution of kernelized problems along a kernel hyperparameter path. Since exact path following algorithms can be numerically unstable, robust and efficient approximate path tracking algorithms became popular for regularized learning problems. By now algorithms with optimal path complexity are known for many regularized learning problems. That is not the case for kernel hyperparameter path tracking algorithms, where the exact path tracking algorithms can also suffer from numerical instabilities. The robust approximation algorithms for regularization path tracking can not be used directly for kernel hyperparameter path tracking problems since the latter fall into a different problem class. Here we address this problem by devising a robust and efficient path tracking algorithm that can also handle kernel hyperparameter paths and has asymptotically optimal complexity. We use this algorithm to compute approximate kernel hyperparamter solution paths for support vector machines and robust kernel regression. Experimental results for this problem applied to various data sets confirms the theoretical complexity analysis."}}
{"id": "ryELZd-O-H", "cdate": 1325376000000, "mdate": null, "content": {"title": "Approximating Concavely Parameterized Optimization Problems", "abstract": "We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\\varepsilon &gt;0$ by a set of size $O(1/\\sqrt{\\varepsilon})$. A lower bound of size $\\Omega (1/\\sqrt{\\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\\sqrt{\\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion."}}
{"id": "HJbr0DbuZS", "cdate": 1072915200000, "mdate": null, "content": {"title": "Kernel Methods for Implicit Surface Modeling", "abstract": "We describe methods for computing an implicit model of a hypersurface that is given only by a finite sampling. The methods work by mapping the sample points into a reproducing kernel Hilbert space and then deter- mining regions in terms of hyperplanes. 1 Introduction Suppose we are given a finite sampling (in machine learning terms, training data) x1, . . . , xm X , where the domain X is some hypersurface in Euclidean space Rd. The case d = 3 is especially interesting since these days there are many devices, e.g., laser range scanners, that allow the acquisition of point data from the boundary surfaces of solids. For further processing it is often necessary to transform this data into a continu- ous model. Today the most popular approach is to add connectivity information to the data by transforming them into a triangle mesh (see [4] for an example of such a transformation algorithm). But recently also implicit models, where the surface is modeled as the zero set of some sufficiently smooth function, gained some popularity [1]. They bear resemblance to level set methods used in computer vision [6]. One advantage of implicit models is that they easily allow the derivation of higher order differential quantities such as curvatures. Another advantage is that an inside-outside test, i.e., testing whether a query point lies on the bounded or unbounded side of the surface, boils down to determining the sign of a function-evaluation at the query point. Inside-outside tests are important when one wants to intersect two solids. The goal of this paper is, loosely speaking, to find a function which takes the value zero on a surface which <pre><code> (1) contains the training data and (2) is a \"reasonable\" implicit model of X . To capture properties of its shape even in the above general case, we need to exploit some structure on X . In line with a sizeable amount of recent work on kernel methods [11], we assume that this structure is given by a (positive definite) kernel, i.e., a real valued function <pre><code> Partially supported by the Swiss National Science Foundation under the project \"Non-linear manifold learning\". Figure 1: In the 2-D toy example depicted, o o the hyperplane w, (x) = separates all o o but one of the points from the origin. The out- . o o lier (x) is associated with a slack variable , o which is penalized in the objective function /||w|| (4). The distance from the outlier to the hy- o w o /||w|| perplane is / w ; the distance between hy- o x ( ) perplane and origin is / w . The latter im- plies that a small w corresponds to a large margin of separation from the origin. k on X X which can be expressed as <pre><code> k(x, x ) = (x), (x ) (1) for some map into a Hilbert space H. The space H is the reproducing kernel Hilbert space (RKHS) associated with k, and is called its feature map. A popular example, in the case where X is a normed space, is the Gaussian (where &gt; 0) <pre><code> x - x 2 k(x, x ) = exp - . (2) 2 2 The advantage of using a positive definite kernel as a similarity measure is that it allows us to construct geometric algorithms in Hilbert spaces. 2 Single-Class SVMs Single-class SVMs were introduced [8, 10] to estimate quantiles C {x X |f (x) [, [} of an unknown distribution P on X using kernel expansions. Here, <pre><code> f (x) = ik(xi, x) - , (3) i where x1, . . . , xm X are unlabeled data generated i.i.d. according to P . The single-class SVM approximately computes the smallest set C C containing a specified fraction of all training examples, where smallness is measured in terms of the norm in the RKHS H associated with k, and C is the family of sets corresponding to half-spaces in H. Depending on the kernel, this notion of smallness will coincide with the intuitive idea that the quantile estimate should not only contain a specified fraction of the training points, but it should also be sufficiently smooth so that the same is approximately true for previously unseen points sampled from P . Let us briefly describe the main ideas of the approach. The training points are mapped into H using the feature map associated with k, and then it is attempted to separate them from the origin with a large margin by solving the following quadratic program: for (0, 1],1 <pre><code> 1 1 minimize w 2 + i - (4) wH,R 2 m m ,R i subject to w, (xi) - i, i 0. (5) Since non-zero slack variables i are penalized in the objective function, we can expect that if w and solve this problem, then the decision function, f (x) = sgn ( w, (x) - ) will <pre><code> 1Here and below, bold face greek character denote vectors, e.g., = (1, . . . , m) , and indices i, j by default run over 1, . . . , m. Figure 2: Models computed with a single class SVM using a Gaussian kernel (2). The three examples differ in the value chosen for in the kernel - a large value (0.224 times the diameter of the hemisphere) in the left figure and a small value (0.062 times the diameter of the hemisphere) in the middle and right figure. In the right figure also non-zero slack variables (outliers) were allowed. Note that that the outliers in the right figure correspond to a sharp feature (non-smoothness) in the original surface. equal 1 for most examples xi contained in the training set,2 while the regularization term w will still be small. For an illustration, see Figure 1. The trade-off between these two goals is controlled by a parameter . One can show that the solution takes the form <pre><code> f (x) = sgn ik(xi, x) - , (6) i where the i are computed by solving the dual problem, <pre><code> 1 minimize ij k(xi, xj ) (7) Rm 2 ij 1 subject to 0 i and m i = 1. (8) i Note that according to (8), the training examples contribute with nonnegative weights i 0 to the solution (6). One can show that asymptotically, a fraction of all training examples will have strictly positive weights, and the rest will be zero (the \"-property\"). In our application we are not primarily interested in a decision function itself but in the boundaries of the regions in input space defined by the decision function. That is, we are interested in f -1(0), where f is the kernel expansion (3) and the points x1, . . . , xm X are sampled from some unknown hypersurface X Rd. We want to consider f -1(0) as a model for X . In the following we focus on the case d = 3. If we assume that the xi are sampled without noise from X which for example is a reasonable assumption for data obtained with a state of the art 3d laser scanning device we should set the slack variables in (4) and (5) to zero. In the dual problem this results in removing the upper constraints on the i in (8). Note that sample points with non-zero slack variable cannot be contained in f -1(0). But also sample points whose image in feature space lies above the optimal hyperplane are not contained in f -1(0) (see Figure 1) -- we will address this in the next section. It turns out that it is useful in practice to allow non-zero slack variables, because they prevent f -1(0) from decomposing into many connected components (see Figure 2 for an illustration). In our experience, one can ensure that the images of all sample points in feature space lie close to (or on) the optimal hyperplane can be achieved by choosing in the Gaussian 2We use the convention that sgn (z) equals 1 for z 0 and -1 otherwise. <pre><code> Figure 3: Two parallel hy- x ( * )o perplanes w, (x) = + o o /* () enclosing all but two . ||w|| o of the points. The outlier o (x()) is associated with (+ * ||w|| )/ (+)/||w|| o a slack variable (), which w o /||w|| o x ( ) o is penalized in the objective function (9). kernel (2) such that the Gaussians in the kernel expansion (3) are highly localized. How- ever, highly localized Gaussians are not well suited for interpolation -- the implicit surface decomposes into several components. Allowing outliers mitigates the situation to a certain extent. Another way to deal with the problem is to further restrict the optimal region in feature space. In the following we will pursue the latter approach."}}
