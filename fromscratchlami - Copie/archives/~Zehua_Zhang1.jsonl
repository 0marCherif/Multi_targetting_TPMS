{"id": "pH40MFxjcnG", "cdate": 1640995200000, "mdate": 1667370970241, "content": {"title": "Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning", "abstract": "We present a novel technique for self-supervised video representation learning by: (a) decoupling the learning objective into two contrastive subtasks respectively emphasizing spatial and temporal features, and (b) performing it hierarchically to encourage multi-scale understanding. Motivated by their effectiveness in supervised learning, we first introduce spatial-temporal feature learning decoupling and hierarchical learning to the context of unsupervised video learning. We show by experiments that augmentations can be manipulated as regularization to guide the network to learn desired semantics in contrastive learning, and we propose a way for the model to separately capture spatial and temporal features at multiple scales. We also introduce an approach to overcome the problem of divergent levels of instance invariance at different hierarchies by modeling the invariance as loss weights for objective re-weighting. Experiments on downstream action recognition benchmarks on UCF101 and HMDB51 show that our proposed Hierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial improvements over directly learning spatial-temporal features as a whole and achieves competitive performance when compared with other state-of-the-art unsupervised methods. Code will be made available."}}
{"id": "TXTVpDQ5Eo", "cdate": 1640995200000, "mdate": 1667370970233, "content": {"title": "Can Gaze Inform Egocentric Action Recognition?", "abstract": "We investigate the hypothesis that gaze-signal can improve egocentric action recognition on the standard benchmark, EGTEA Gaze++ dataset. In contrast to prior work where gaze-signal was only used during training, we formulate a novel neural fusion approach, Cross-modality Attention Blocks (CMA), to leverage gaze-signal for action recognition during inference as well. CMA combines information from different modalities at different levels of abstraction to achieve state-of-the-art performance for egocentric action recognition. Specifically, fusing the video-stream with optical-flow with CMA outperforms the current state-of-the-art by 3%. However, when CMA is employed to fuse gaze-signal with video-stream data, no improvements are observed. Further investigation of this counter-intuitive finding indicates that small spatial overlap between the network\u2019s attention-map and gaze ground-truth renders the gaze-signal uninformative for this benchmark. Based on our empirical findings, we recommend improvements to the current benchmark to develop practical systems for egocentric video understanding with gaze-signal."}}
{"id": "-4E9dCVN3mK", "cdate": 1577836800000, "mdate": 1667370970233, "content": {"title": "Interaction Graphs for Object Importance Estimation in On-road Driving Videos", "abstract": "A vehicle driving along the road is surrounded by many objects, but only a small subset of them influence the driver's decisions and actions. Learning to estimate the importance of each object on the driver's real-time decision-making may help better understand human driving behavior and lead to more reliable autonomous driving systems. Solving this problem requires models that understand the interactions between the ego-vehicle and the surrounding objects. However, interactions among other objects in the scene can potentially also be very helpful, e.g., a pedestrian beginning to cross the road between the ego-vehicle and the car in front will make the car in front less important. We propose a novel framework for object importance estimation using an interaction graph, in which the features of each object node are updated by interacting with others through graph convolution. Experiments show that our model outperforms state-of-the-art baselines with much less input and pre-processing."}}
{"id": "SJegTSBgLH", "cdate": 1567802807720, "mdate": null, "content": {"title": "A Self Validation Network for Object-Level Human Attention Estimation", "abstract": "Due to the foveated nature of human vision system, we need to search for an object in our view and then focus on it as we are performing a certain task. To mimic this procedure is interesting for many real-world applications, e.g., a good driver assistant system usually wants to know not only where but also what the driver is looking at. It can be seen as a special object detection task in which we want to detect only the object drawing the most attention from people's visual fields. However, combining traditional eye gaze prediction algorithm and object detection algorithm to solve this does not work well because it tackles the where and the what problems separately and thus leads to discrepancy between the highly related concepts of where and what which should be consistent in a high-level cognitive system. We here propose a novel unified model  (Mr. Net) with a Self Validation Module to bridge the spatial and the temporal branches as well as leverage the consistency of the where and the what concepts. We evaluate on two public datasets and the experiment results demonstrate that the Self Validation Module significantly benefits both training and testing, and our model outperforms the state-of-the-art baselines."}}
{"id": "mqKheNI247", "cdate": 1514764800000, "mdate": 1667370970243, "content": {"title": "Deepdiary: Lifelogging image captioning and summarization", "abstract": ""}}
{"id": "X3K0CEUrR4", "cdate": 1514764800000, "mdate": 1667370970243, "content": {"title": "From Coarse Attention to Fine-Grained Gaze: A Two-stage 3D Fully Convolutional Network for Predicting Eye Gaze in First Person Video", "abstract": ""}}
{"id": "f2zT4t1QMga", "cdate": 1483228800000, "mdate": 1667371024073, "content": {"title": "Exploring Inter-Observer Differences in First-Person Object Views Using Deep Learning Models", "abstract": "Recent advances in wearable camera technology have led many cognitive psychologists to study the development of the human visual system by recording the field of view of infants and toddlers. Meanwhile, the vast success of deep learning in computer vision is driving researchers in both disciplines to aim to benefit from each other's understanding. Towards this goal, we set out to explore how deep learning models could be used to gain developmentally relevant insight from such first-person data. We consider a dataset of first-person videos from different people freely interacting with a set of toy objects, and train different object-recognition models based on each subject's view. We observe large inter-observer differences and find that subjects who created more diverse images of an object result in models that learn more robust object representations."}}
