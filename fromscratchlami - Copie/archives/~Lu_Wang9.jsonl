{"id": "9kK4R_8nAsD", "cdate": 1675827741199, "mdate": null, "content": {"title": "Exploring Demonstration Ensembling for In-context Learning", "abstract": "In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for desired tasks, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are not very relevant to the test example. Second, due to the input length limit of transformer models, it can be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. DENSE predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment on 7 different language tasks. Our experiments show max ensembling to outperform concatenation by an average of 3.8 points. "}}
{"id": "wKOIoBseOII", "cdate": 1609459200000, "mdate": 1637111228243, "content": {"title": "Inference Time Style Control for Summarization", "abstract": "How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation. In experiments of summarizing with simplicity control, automatic evaluation and human judges both find our models producing outputs in simpler languages while still informative. We also generate news headlines with various ideological leanings, which can be distinguished by humans with a reasonable probability."}}
{"id": "pkrYDP40f4R", "cdate": 1609459200000, "mdate": 1637111226894, "content": {"title": "CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization", "abstract": "Shuyang Cao, Lu Wang. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "g_eRhAVBe03", "cdate": 1609459200000, "mdate": 1637111228238, "content": {"title": "Controllable Open-ended Question Generation with A New Question Type Ontology", "abstract": "Shuyang Cao, Lu Wang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "fw9eNmhwHy_", "cdate": 1609459200000, "mdate": 1637111226892, "content": {"title": "Attention Head Masking for Inference Time Content Selection in Abstractive Summarization", "abstract": "How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoder-decoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our models outperform prior state-of-the-art models on CNN/Daily Mail and New York Times datasets. Moreover, our inference-time masking technique is also data-efficient, requiring only 20% of the training samples to outperform BART fine-tuned on the full CNN/DailyMail dataset."}}
{"id": "YZ4HwPqwzXA", "cdate": 1609459200000, "mdate": 1637111228920, "content": {"title": "Controllable Open-ended Question Generation with A New Question Type Ontology", "abstract": "We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. A new dataset with 4,959 questions is labeled based on the new ontology. We then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and produce the question. Based on this framework, we further use both exemplars and automatically generated templates to improve controllability and diversity. Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics. Human judges also rate our model outputs highly in answerability, coverage of scope, and overall quality. Finally, our model variants with templates can produce questions with enhanced controllability and diversity."}}
{"id": "X8nb__6aZqF", "cdate": 1609459200000, "mdate": 1637111228734, "content": {"title": "Controllable Summarization with Constrained Markov Decision Process", "abstract": "We study controllable text summarization which allows users to gain control on a particular attribute (e.g., length limit) of the generated summaries. In this work, we propose a novel training framework based on Constrained Markov Decision Process (CMDP), which conveniently includes a reward function along with a set of constraints, to facilitate better summarization control. The reward function encourages the generation to resemble the human-written reference, while the constraints are used to explicitly prevent the generated summaries from violating user-imposed requirements. Our framework can be applied to control important attributes of summarization, including length, covered entities, and abstractiveness, as we devise specific constraints for each of these aspects. Extensive experiments on popular benchmarks show that our CMDP framework helps generate informative summaries while complying with a given attribute's requirement."}}
{"id": "VEG-T3hYgML", "cdate": 1609459200000, "mdate": 1637111228305, "content": {"title": "DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation", "abstract": "Xinyu Hua, Ashwin Sreevatsa, Lu Wang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "UDSEXctU_v", "cdate": 1609459200000, "mdate": 1637111226317, "content": {"title": "Inference Time Style Control for Summarization", "abstract": "Shuyang Cao, Lu Wang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "N-wzmy4R1Tg", "cdate": 1609459200000, "mdate": 1636784899476, "content": {"title": "Efficient Attentions for Long Document Summarization", "abstract": "Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
