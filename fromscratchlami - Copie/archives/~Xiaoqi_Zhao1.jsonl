{"id": "vhg4zU3cmQm", "cdate": 1668579832381, "mdate": 1668579832381, "content": {"title": "Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection", "abstract": "The main purpose of RGB-D salient object detection (SOD)\nis how to better integrate and utilize cross-modal fusion information.\nIn this paper, we explore these issues from a new perspective. We integrate the features of different modalities through densely connected\nstructures and use their mixed features to generate dynamic filters with\nreceptive fields of different sizes. In the end, we implement a kind of\nmore flexible and efficient multi-scale cross-modal feature processing,\ni.e. dynamic dilated pyramid module. In order to make the predictions\nhave sharper edges and consistent saliency regions, we design a hybrid\nenhanced loss function to further optimize the results. This loss function is also validated to be effective in the single-modal RGB SOD\ntask. In terms of six metrics, the proposed method outperforms the\nexisting twelve methods on eight challenging benchmark datasets. A\nlarge number of experiments verify the effectiveness of the proposed\nmodule and loss function. Our code, model and results are available at\nhttps://github.com/lartpang/HDFNet."}}
{"id": "c7jjj2AFW_G", "cdate": 1668579721323, "mdate": 1668579721323, "content": {"title": "Multi-scale Interactive Network for Salient Object Detection", "abstract": "Deep-learning based salient object detection methods\nachieve great progress. However, the variable scale and\nunknown category of salient objects are great challenges all\nthe time. These are closely related to the utilization of multi\u0002level and multi-scale features. In this paper, we propose the\naggregate interaction modules to integrate the features from\nadjacent levels, in which less noise is introduced because of\nonly using small up-/down-sampling rates. To obtain more\nefficient multi-scale features from the integrated features,\nthe self-interaction modules are embedded in each decoder\nunit. Besides, the class imbalance issue caused by the scale\nvariation weakens the effect of the binary cross entropy\nloss and results in the spatial inconsistency of the predic\u0002tions. Therefore, we exploit the consistency-enhanced loss\nto highlight the fore-/back-ground difference and preserve\nthe intra-class consistency. Experimental results on five\nbenchmark datasets demonstrate that the proposed method\nwithout any post-processing performs favorably against 23\nstate-of-the-art approaches. The source code will be publicly available at https://github.com/lartpang/MINet.\n"}}
{"id": "wlAAyun1z80", "cdate": 1668579578790, "mdate": 1668579578790, "content": {"title": "A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection", "abstract": "Existing RGB-D salient object detection (SOD) approaches\nconcentrate on the cross-modal fusion between the RGB stream and the\ndepth stream. They do not deeply explore the effect of the depth map\nitself. In this work, we design a single stream network to directly use the\ndepth map to guide early fusion and middle fusion between RGB and\ndepth, which saves the feature encoder of the depth stream and achieves\na lightweight and real-time model. We tactfully utilize depth information from two perspectives: (1) Overcoming the incompatibility prob\u0002lem caused by the great difference between modalities, we build a single\nstream encoder to achieve the early fusion, which can take full advantage\nof ImageNet pre-trained backbone model to extract rich and discrim\u0002inative features. (2) We design a novel depth-enhanced dual attention\nmodule (DEDA) to efficiently provide the fore-/back-ground branches\nwith the spatially filtered features, which enables the decoder to optimally perform the middle fusion. Besides, we put forward a pyramidally\nattended feature extraction module (PAFE) to accurately localize the\nobjects of different scales. Extensive experiments demonstrate that the\nproposed model performs favorably against most state-of-the-art methods under different evaluation metrics. Furthermore, this model is 55.5%\nlighter than the current lightest model and runs at a real-time speed of\n32 FPS when processing a 384 \u00d7 384 image."}}
{"id": "JZmTz-_VEc", "cdate": 1668574654775, "mdate": 1668574654775, "content": {"title": "Suppress and Balance: A Simple Gated Network for Salient Object Detection", "abstract": "Most salient object detection approaches use U-Net or feature pyramid networks (FPN) as their basic structures. These methods\nignore two key problems when the encoder exchanges information with\nthe decoder: one is the lack of interference control between them, the\nother is without considering the disparity of the contributions of different encoder blocks. In this work, we propose a simple gated network\n(GateNet) to solve both issues at once. With the help of multilevel gate\nunits, the valuable context information from the encoder can be optimally transmitted to the decoder. We design a novel gated dual branch\nstructure to build the cooperation among different levels of features and\nimprove the discriminability of the whole network. Through the dual\nbranch design, more details of the saliency map can be further restored.\nIn addition, we adopt the atrous spatial pyramid pooling based on the\nproposed \u201cFold\u201d operation (Fold-ASPP) to accurately localize salient ob\u0002jects of various scales. Extensive experiments on five challenging datasets\ndemonstrate that the proposed model performs favorably against most\nstate-of-the-art methods under different evaluation metrics.\n"}}
{"id": "MTyYk4Icdq", "cdate": 1668574431108, "mdate": 1668574431108, "content": {"title": "Automatic Polyp Segmentation via Multi-scale Subtraction Network", "abstract": "More than 90% of colorectal cancer is gradually transformed\nfrom colorectal polyps. In clinical practice, precise polyp segmentation\nprovides important information in the early detection of colorectal cancer. Therefore, automatic polyp segmentation techniques are of great\nimportance for both patients and doctors. Most existing methods are\nbased on U-shape structure and use element-wise addition or concatenation to fuse different level features progressively in decoder. However,\nboth the two operations easily generate plenty of redundant information, which will weaken the complementarity between different level features, resulting in inaccurate localization and blurred edges of polyps.\nTo address this challenge, we propose a multi-scale subtraction network\n(MSNet) to segment polyp from colonoscopy image. Specifically, we first\ndesign a subtraction unit (SU) to produce the difference features between adjacent levels in encoder. Then, we pyramidally equip the SUs\nat different levels with varying receptive fields, thereby obtaining rich\nmulti-scale difference information. In addition, we build a training-free\nnetwork \u201cLossNet\u201d to comprehensively supervise the polyp-aware features from bottom layer to top layer, which drives the MSNet to capture\nthe detailed and structural cues simultaneously. Extensive experiments\non five benchmark datasets demonstrate that our MSNet performs favorably against most state-of-the-art methods under different evaluation\nmetrics. Furthermore, MSNet runs at a real-time speed of\n\u223c70fps when\nprocessing a 352\n\u00d7 352 image. The source code will be publicly available\nat https://github.com/Xiaoqi-Zhao-DLUT/MSNet\n."}}
{"id": "4p8p2zRI81", "cdate": 1668574304263, "mdate": 1668574304263, "content": {"title": "Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation", "abstract": "Location and appearance are the key cues for video object segmentation. Many sources such as RGB, depth, optical flow and\nstatic saliency can provide useful information about the objects.\nHowever, existing approaches only utilize the RGB or RGB and\noptical flow. In this paper, we propose a novel multi-source fusion\nnetwork for zero-shot video object segmentation. With the help of\ninteroceptive spatial attention module (ISAM), spatial importance\nof each source is highlighted. Furthermore, we design a feature\npurification module (FPM) to filter the inter-source incompatible\nfeatures. By the ISAM and FPM, the multi-source features are effectively fused. In addition, we put forward an automatic predictor\nselection network (APS) to select the better prediction of either the\nstatic saliency predictor or the moving object predictor in order to\nprevent over-reliance on the failed results caused by low-quality\noptical flow maps. Extensive experiments on three challenging pub\u0002lic benchmarks (i.e. DAVIS16, Youtube-Objects and FBMS) show\nthat the proposed model achieves compelling performance against\nthe state-of-the-arts. The source code will be publicly available at\nhttps://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS."}}
{"id": "4B1YibOHf3", "cdate": 1668574150585, "mdate": 1668574150585, "content": {"title": "Self-Supervised Pretraining for RGB-D Salient Object Detection", "abstract": "Existing CNNs-Based RGB-D salient object detection (SOD)\nnetworks are all required to be pretrained on the ImageNet to\nlearn the hierarchy features which helps provide a good initialization. However, the collection and annotation of large-scale datasets are time-consuming and expensive. In this paper, we utilize self-supervised representation learning (SSL)\nto design two pretext tasks: the cross-modal auto-encoder and\nthe depth-contour estimation. Our pretext tasks require only\na few and unlabeled RGB-D datasets to perform pretraining, which makes the network capture rich semantic contexts and reduce the gap between two modalities, thereby\nproviding an effective initialization for the downstream task.\nIn addition, for the inherent problem of cross-modal fusion\nin RGB-D SOD, we propose a consistency-difference aggregation (CDA) module that splits a single feature fusion into\nmulti-path fusion to achieve an adequate perception of consistent and differential information. The CDA module is gen\u0002eral and suitable for cross-modal and cross-level feature fusion. Extensive experiments on six benchmark datasets show\nthat our self-supervised pretrained model performs favorably against most state-of-the-art methods pretrained on ImageNet. The source code will be publicly available at https:\n//github.com/Xiaoqi-Zhao-DLUT/SSLSOD.\n"}}
{"id": "Lq-yZuxq_7U", "cdate": 1668573979687, "mdate": 1668573979687, "content": {"title": "Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection", "abstract": "The recently proposed camouflaged object detection\n(COD) attempts to segment objects that are visually blended\ninto their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from high intrinsic\nsimilarity between the camouflaged objects and their background, the objects are usually diverse in scale, fuzzy in\nappearance, and even severely occluded. To deal with these\nproblems, we propose a mixed-scale triplet network, ZoomNet, which mimics the behavior of humans when observing\nvague images, i.e., zooming in and out. Specifically, our\nZoomNet employs the zoom strategy to learn the discriminative mixed-scale semantics by the designed scale integration unit and hierarchical mixed-scale unit, which fully\nexplores imperceptible clues between the candidate objects\nand background surroundings. Moreover, considering the\nuncertainty and ambiguity derived from indistinguishable\ntextures, we construct a simple yet effective regularization\nconstraint, uncertainty-aware loss, to promote the model\nto accurately produce predictions with higher confidence\nin candidate regions. Without bells and whistles, our proposed highly task-friendly model consistently surpasses the\nexisting 23 state-of-the-art methods on four public datasets.\nBesides, the superior performance over the recent cutting-edge models on the SOD task also verifies the effectiveness\nand generality of our model. The code will be available at\nhttps://github.com/lartpang/ZoomNet.\n"}}
{"id": "CivBup0yG_b", "cdate": 1668573722526, "mdate": 1668573722526, "content": {"title": "Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction", "abstract": "Benefiting from color independence, illumination\ninvariance and location discrimination attributed by the depth\nmap, it can provide important supplemental information for extracting salient objects in complex environments. However, high-quality depth sensors are expensive and can not be widely applied. While general depth sensors produce the noisy and sparse\ndepth information, which brings the depth-based networks with\nirreversible interference. In this paper, we propose a novel multi-task and multi-modal filtered transformer (MMFT) network for\nRGB-D salient object detection (SOD). Specifically, we unify three\ncomplementary tasks: depth estimation, salient object detection\nand contour estimation. The multi-task mechanism promotes\nthe model to learn the task-aware features from the auxiliary\ntasks. In this way, the depth information can be completed\nand purified. Moreover, we introduce a multi-modal filtered\ntransformer (MFT) module, which equips with three modality-specific filters to generate the transformer-enhanced feature for\neach modality. The proposed model works in a depth-free style\nduring the testing phase. Experiments show that it not only\nsignificantly surpasses the depth-based RGB-D SOD methods on\nmultiple datasets, but also precisely predicts a high-quality depth\nmap and salient contour at the same time. And, the resulted depth\nmap can help existing RGB-D SOD methods obtain significant\nperformance gain. The source code will be publicly available at\nhttps://github.com/Xiaoqi-Zhao-DLUT/MMFT."}}
{"id": "m8KnnYGNuF", "cdate": 1640995200000, "mdate": 1668740665304, "content": {"title": "Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction", "abstract": "Benefiting from color independence, illumination invariance and location discrimination attributed by the depth map, it can provide important supplemental information for extracting salient objects in complex environments. However, high-quality depth sensors are expensive and can not be widely applied. While general depth sensors produce the noisy and sparse depth information, which brings the depth-based networks with irreversible interference. In this paper, we propose a novel multi-task and multi-modal filtered transformer (MMFT) network for RGB-D salient object detection (SOD). Specifically, we unify three complementary tasks: depth estimation, salient object detection and contour estimation. The multi-task mechanism promotes the model to learn the task-aware features from the auxiliary tasks. In this way, the depth information can be completed and purified. Moreover, we introduce a multi-modal filtered transformer (MFT) module, which equips with three modality-specific filters to generate the transformer-enhanced feature for each modality. The proposed model works in a depth-free style during the testing phase. Experiments show that it not only significantly surpasses the depth-based RGB-D SOD methods on multiple datasets, but also precisely predicts a high-quality depth map and salient contour at the same time. And, the resulted depth map can help existing RGB-D SOD methods obtain significant performance gain. The source code will be publicly available at https://github.com/Xiaoqi-Zhao-DLUT/MMFT."}}
