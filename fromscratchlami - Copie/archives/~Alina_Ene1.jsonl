{"id": "zp15Ygjt1l", "cdate": 1672531200000, "mdate": 1682335912974, "content": {"title": "High Probability Convergence of Stochastic Gradient Methods", "abstract": "In this work, we describe a generic approach to show convergence with high probability for both stochastic convex and non-convex optimization with sub-Gaussian noise. In previous works for convex optimization, either the convergence is only in expectation or the bound depends on the diameter of the domain. Instead, we show high probability convergence with bounds depending on the initial distance to the optimal solution. The algorithms use step sizes analogous to the standard settings and are universal to Lipschitz functions, smooth functions, and their linear combinations. This method can be applied to the non-convex case. We demonstrate an $O((1+\\sigma^{2}\\log(1/\\delta))/T+\\sigma/\\sqrt{T})$ convergence rate when the number of iterations $T$ is known and an $O((1+\\sigma^{2}\\log(T/\\delta))/\\sqrt{T})$ convergence rate when $T$ is unknown for SGD, where $1-\\delta$ is the desired success probability. These bounds improve over existing bounds in the literature. Additionally, we demonstrate that our techniques can be used to obtain high probability bound for AdaGrad-Norm (Ward et al., 2019) that removes the bounded gradients assumption from previous works. Furthermore, our technique for AdaGrad-Norm extends to the standard per-coordinate AdaGrad algorithm (Duchi et al., 2011), providing the first noise-adapted high probability convergence for AdaGrad."}}
{"id": "PGXbChr6N_", "cdate": 1672531200000, "mdate": 1682385414354, "content": {"title": "Online Ad Allocation with Predictions", "abstract": "Display Ads and the generalized assignment problem are two well-studied online packing problems with important applications in ad allocation and other areas. In both problems, ad impressions arrive online and have to be allocated immediately to budget-constrained advertisers. Worst-case algorithms that achieve the ideal competitive ratio are known, but might act overly conservative given the predictable and usually tame nature of real-world input. Given this discrepancy, we develop an algorithm for both problems that incorporate machine-learned predictions and can thus improve the performance beyond the worst-case. Our algorithm is based on the work of Feldman et al. (2009) and similar in nature to Mahdian et al. (2007) who were the first to develop a learning-augmented algorithm for the related, but more structured Ad Words problem. We use a novel analysis to show that our algorithm is able to capitalize on a good prediction, while being robust against poor predictions. We experimentally evaluate our algorithm on synthetic and real-world data on a wide range of predictions. Our algorithm is consistently outperforming the worst-case algorithm without predictions."}}
{"id": "1KtU2ya2zh5", "cdate": 1663850383881, "mdate": null, "content": {"title": "META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for Unbounded Functions", "abstract": "We study the application of variance reduction (VR) techniques to general non-convex stochastic optimization problems. In this setting, the recent work STORM (Cutkosky & Orabona, 2019) overcomes the drawback of having to compute gradients of \u201cmega-batches\u201d that earlier VR methods rely on. There, STORM utilizes recursive momentum to achieve the VR effect and is then later made fully adaptive in STORM+ (Levy et al., 2021), where full-adaptivity removes the requirement for obtaining certain problem-specific parameters such as the smoothness of the objective and bounds on the variance and norm of the stochastic gradients in order to set the step size. However, STORM+ crucially relies on the assumption that the function values are bounded, excluding a large class of useful functions. In this work, we propose META-STORM, a generalized framework of STORM+ that removes this bounded function values assumption while still attaining the optimal convergence rate for non-convex optimization. META-STORM not only maintains full-adaptivity, removing the need to obtain problem specific parameters, but also improves the convergence rate\u2019s dependency on the problem parameters. Furthermore, META-STORM can utilize a large range of parameter settings that subsumes previous methods allowing for more flexibility in a wider range of settings. Finally, we demonstrate the effectiveness of META-STORM through experiments across common deep learning tasks. Our algorithm improves upon the previous work STORM+ and is competitive with widely used algorithms after the addition of per-coordinate update and exponential moving average heuristics."}}
{"id": "ULnHxczCBaE", "cdate": 1663850360957, "mdate": null, "content": {"title": "On the Convergence of AdaGrad(Norm) on $\\mathbb{R}^d$: Beyond Convexity, Non-Asymptotic Rate and Acceleration", "abstract": "Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the last iterate, instead of the average iterate. Finally, we give new accelerated adaptive algorithms and their convergence guarantee in the deterministic setting with explicit dependency on the problem parameters, improving upon the asymptotic rate shown in previous works. "}}
{"id": "S9fV9uTjj3Q", "cdate": 1640995200000, "mdate": 1673638301647, "content": {"title": "On the Convergence of AdaGrad on $\\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration", "abstract": ""}}
{"id": "Pne6lAshRJ", "cdate": 1640995200000, "mdate": 1673638301830, "content": {"title": "Adaptive Accelerated (Extra-)Gradient Methods with Variance Reduction", "abstract": ""}}
{"id": "OH7PaPxVWHd", "cdate": 1640995200000, "mdate": 1673638301843, "content": {"title": "META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for Unbounded Functions", "abstract": ""}}
{"id": "JaYGuTpQ2zR", "cdate": 1640995200000, "mdate": 1673638301634, "content": {"title": "High Probability Convergence for Accelerated Stochastic Mirror Descent", "abstract": ""}}
{"id": "9mRpCR461F", "cdate": 1640995200000, "mdate": 1673638301643, "content": {"title": "Adaptive and Universal Algorithms for Variational Inequalities with Optimal Convergence", "abstract": ""}}
{"id": "-hXonUfrkk", "cdate": 1640995200000, "mdate": 1673638301832, "content": {"title": "Streaming Algorithm for Monotone k-Submodular Maximization with Cardinality Constraints", "abstract": ""}}
