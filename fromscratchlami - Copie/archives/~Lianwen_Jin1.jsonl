{"id": "EONuSdDjJrp", "cdate": 1654523315628, "mdate": null, "content": {"title": "MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwriting Verification", "abstract": "Although online handwriting verification has made great progress recently, the verification performances are still far behind the real usage owing to the small scale of the datasets as well as the limited biometric mediums. Therefore, this paper proposes a new handwriting verification benchmark dataset named Multimodal Signature and Digit String (MSDS), which consists of two subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings), contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to the best of our knowledge, is the largest publicly available Chinese signature dataset for handwriting verification, at least eight times larger than existing online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit Strings, i.e, the actual phone numbers of users, which have not been explored yet. Extensive experiments with different baselines are respectively conducted for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of state-of-the-art methods on MSDS-TDS are generally better than those on MSDS-ChS, which indicates that the handwritten Token Digit String could be a more effective biometric than handwritten Chinese signature. This is a promising discovery that could inspire us to explore new biometric traits. The MSDS dataset is available at https://github.com/HCIILAB/MSDS."}}
{"id": "z-f5_r_QEZz", "cdate": 1609459200000, "mdate": null, "content": {"title": "Separating Content from Style Using Adversarial Learning for Recognizing Text in the Wild", "abstract": "Scene text recognition is an important task in computer vision. Despite tremendous progress achieved in the past few years, issues such as varying font styles, arbitrary shapes and complex backgrounds etc. have made the problem very challenging. In this work, we propose to improve text recognition from a new perspective by separating the text content from complex backgrounds, thus making the recognition considerably easier and significantly improving recognition accuracy. To this end, we exploit the generative adversarial networks (GANs) for removing backgrounds while retaining the text content . As vanilla GANs are not sufficiently robust to generate sequence-like characters in natural images, we propose an adversarial learning framework for the generation and recognition of multiple characters in an image. The proposed framework consists of an attention-based recognizer and a generative adversarial architecture. Furthermore, to tackle the issue of lacking paired training samples, we design an interactive joint training scheme, which shares attention masks from the recognizer to the discriminator, and enables the discriminator to extract the features of each character for further adversarial training. Benefiting from the character-level adversarial training, our framework requires only unpaired simple data for style supervision. Each target style sample containing only one randomly chosen character can be simply synthesized online during the training. This is significant as the training does not require costly paired samples or character-level annotations. Thus, only the input images and corresponding text labels are needed. In addition to the style normalization of the backgrounds, we refine character patterns to ease the recognition task. A feedback mechanism is proposed to bridge the gap between the discriminator and the recognizer. Therefore, the discriminator can guide the generator according to the confusion of the recognizer, so that the generated patterns are clearer for recognition. Experiments on various benchmarks, including both regular and irregular text, demonstrate that our method significantly reduces the difficulty of recognition. Our framework can be integrated into recent recognition methods to achieve new state-of-the-art recognition accuracy."}}
{"id": "mBKCEOLVVIV", "cdate": 1609459200000, "mdate": null, "content": {"title": "OPMP: An Omnidirectional Pyramid Mask Proposal Network for Arbitrary-Shape Scene Text Detection", "abstract": "Scene text detection methods have achieved significant progresses. However, stack-omnidirectional text dilemma, under-segmentation of very close text words, and over-segmentation of arbitrary-shape long text lines, are still main challenges. Motivated by these problems, we proposed a two stage method called omnidirectional pyramid mask proposal text detector (OPMP). OPMP removes anchor mechanism that requires heuristic non-maximum suppress processing. Instead, it uses an effective pyramid lengthwise and sidewise residual sequence modeling method to produce arbitrary-shape proposals. To accurately extract the features of text shape, OPMP enhances the backbone layers by a multiple arbitrary-shape fitting mechanism. Finally, a multi-grain text classification module is proposed, which reclassifies each text region robustly. Comprehensive ablation studies demonstrate the effectiveness of each proposed component. In addition, experiments on various benchmarks, including ICDAR2015, MLT, MSRA-TD500, CTW1500, and Total-text, show that our method outperforms previous state-of-the-art methods."}}
{"id": "kxCbYnNWsZ6", "cdate": 1609459200000, "mdate": null, "content": {"title": "STAN: A sequential transformation attention-based network for scene text recognition", "abstract": "Highlights \u2022 We propose a new method called sequential transformation attention network for irregular scene text recognition. \u2022 Taking decomposition as the key idea, our method can rectify irregular text effectively. \u2022 The proposed method is optimized in an end-to-end weakly supervised manner. \u2022 Our method achieves state-of-the-art results on text recognition benchmarks. Abstract Scene text with an irregular layout is difficult to recognize. To this end, a Sequential Transformation Attention-based Network (STAN), which comprises a sequential transformation network and an attention-based recognition network, is proposed for general scene text recognition. The sequential transformation network rectifies irregular text by decomposing the task into a series of patch-wise basic transformations, followed by a grid projection submodule to smooth the junction between neighboring patches. The entire rectification process is able to be trained in an end-to-end weakly supervised manner, requiring only images and their corresponding groundtruth text. Based on the rectified images, an attention-based recognition network is employed to predict a character sequence. Experiments on several benchmarks demonstrate the state-of-the-art performance of STAN on both regular and irregular text."}}
{"id": "faWkaC7q9Sc", "cdate": 1609459200000, "mdate": null, "content": {"title": "Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution", "abstract": "Visual information extraction (VIE) has attracted considerable attention recently owing to its various advanced applications such as document understanding, automatic marking and intelligent education. Most existing works decoupled this problem into several independent sub-tasks of text spotting (text detection and recognition) and information extraction, which completely ignored the high correlation among them during optimization. In this paper, we propose a robust visual information extraction system (VIES) towards real-world scenarios, which is a unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information. Specifically, the information extraction branch collects abundant visual and semantic representations from text spotting for multimodal feature fusion and conversely, provides higher-level semantic clues to contribute to the optimization of text spotting. Moreover, regarding the shortage of public benchmarks, we construct a fully-annotated dataset called EPHOIE (https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. Compared with the state-of-the-art methods, our VIES shows significant superior performance on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used SROIE dataset under the end-to-end scenario."}}
{"id": "acCKxv7b1J", "cdate": 1609459200000, "mdate": null, "content": {"title": "Skeleton-based action recognition using sparse spatio-temporal GCN with edge effective resistance", "abstract": "Graph convolutional neural networks have established significant success in solving various machine learning and computer vision problems. For skeleton-based action recognition, graph convolutional neural networks are the most suitable choice since human skeleton resembles to a graph. Stacking body skeletons over the length of video sequence results in a very complex spatio-temporal graph of many nodes and edges. Modeling the graph convolutional network directly with such a complex graph curtails the performance due to the redundancy of insignificant nodes and edges in the graph. Also for skeleton-based action recognition, the long-term contextual information is of central importance and many current architectures may fail to capture such contextual information. Therefore in order to alleviate these problems, we propose graph sparsification technique using edge effective resistance to better model the global context information and to eliminate redundant nodes and edges in the graph. Furthermore, we incorporate self-attention graph pooling to retain local properties and graph structures while pooling operation. To the best of our knowledge, we are the first to apply graph sparsification using edge effective resistance for skeleton-based action recognition and our proposed method is confirmed to be effective on action recognition, which achieves state-of-the-art results on publicly available datasets: UTD-MHAD, J-HMDB, NTU-RGB\u00a0+\u00a0D-60, NTU-RGB\u00a0+\u00a0D-120 and Kinetics dataset."}}
{"id": "y767YH5pTP", "cdate": 1577836800000, "mdate": null, "content": {"title": "Point-to-Set Similarity Based Deep Metric Learning for Offline Signature Verification", "abstract": "Offline signature verification is a challenging task, where the scarcity of the signature data per writer makes it a few-shot problem. We found that previous deep metric learning based methods, whether in pairs or triplets, are unaware of intra-writer variations and have low training efficiency because only point-to-point (P2P) distances are considered. To address this issue, we present a novel point-to-set (P2S) metric for offline signature verification in this paper. By dividing a training batch into a support set and a query set, our optimization goal is to pull each query to its belonging support set. To further strengthen the P2S metric, a hard mining scheme and a margin strategy are introduced. Experiments conducted on three datasets show the effectiveness of our proposed method."}}
{"id": "vcKR38Nd7WD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Attention-Based Handwritten Mathematical Expression Recognition with Scale Augmentation and Drop Attention", "abstract": "Handwritten mathematical expression recognition (HMER) is an important research direction in handwriting recognition. The performance of HMER suffers from the two-dimensional structure of mathematical expressions (MEs). To address this issue, in this paper, we propose a high-performance HMER model with scale augmentation and drop attention. Specifically, tackling ME with unstable scale in both horizontal and vertical directions, scale augmentation improves the performance of the model on MEs of various scales. An attention-based encoder-decoder network is used for extracting features and generating predictions. In addition, drop attention is proposed to further improve performance when the attention distribution of the decoder is not precise. Compared with previous methods, our method achieves state-of-the-art performance on two public datasets of CROHME 2014 and CROHME 2016."}}
{"id": "vC-l6QtqZzj", "cdate": 1577836800000, "mdate": null, "content": {"title": "On the General Value of Evidence, and Bilingual Scene-Text Visual Question Answering", "abstract": "Visual Question Answering (VQA) methods have made incredible progress, but suffer from a failure to generalize. This is visible in the fact that they are vulnerable to learning coincidental correlations in the data rather than deeper relations between image content and ideas expressed in language. We present a dataset that takes a step towards addressing this problem in that it contains questions expressed in two languages, and an evaluation process that co-opts a well understood image-based metric to reflect the method's ability to reason. Measuring reasoning directly encourages generalization by penalizing answers that are coincidentally correct. The dataset reflects the scene-text version of the VQA problem, and the reasoning evaluation can be seen as a text-based version of a referring expression challenge. Experiments and analysis are provided that show the value of the dataset."}}
{"id": "uC4dCadwcDh", "cdate": 1577836800000, "mdate": null, "content": {"title": "SCUT-HCCDoc: A new benchmark dataset of handwritten Chinese text in unconstrained camera-captured documents", "abstract": "Highlights \u2022 We propose a new large-scale SCUT-HCCDoc dataset for handwritten Chinese text detection, recognition and spotting in natural images. \u2022 We statistically analyzed the samples and annotations of SCUT- HCCDoc in image level, text level and character level. \u2022 We use state-of-the-art methods for baseline evaluation of text line detection, text line recognition, and end-to-end text spotting. \u2022 A comprehensive analysis of the challenge of the dataset and benchmark testing are provided. Abstract In this paper, we introduce a large-scale dataset, called SCUT-HCCDoc, to address challenging detection and recognition problems of handwritten Chinese text (HCT) in the camera-captured documents. Despite extensive studies of optical character recognition (OCR) and offline handwriting recognition for document images, text detection and recognition in the camera-captured documents remains an unsolved problem that is worth for extensive study and investigation. With recent advances in deep learning, researchers have proposed useful architectures for feature learning, detection, and recognition for the scene text. However, the performance of deep learning methods highly depends on the amount and diversity of training data. Previous OCR and offline HCT datasets were built under specific constraints, and most of the recent scene text datasets are for non-handwritten text. Hence, there is a lack of a comprehensive scene handwritten text benchmark. This study focuses on scenes with handwritten Chinese text. We introduce the SCUT-HCCDoc database for HCT detection, recognition and spotting. SCUT-HCCDoc contains 12,253 camera-captured document images with 116,629 text lines and 1,155,801 characters. The diversity of SCUT-HCCDoc can be described at three levels: (1) image-level diversity: image appearance and geometric variances caused by camera-captured settings (such as perspective, background, and resolution) and different applications (such as note-taking, test papers, and homework); (2) text-level diversity: variances of text line length, rotation, etc.; (3) character-level diversity: variances of character categories (up to 6109 classes with additional English letters, and digits), character size, individual writing style, etc. Three kinds of baseline experiments were conducted, where we used several popular text detection methods for text line detection, CTC-based/attention-based methods for text line recognition, and combine text detectors with CTC-based recognizer to achieve end-to-end text spotting. The results indicate the diversity of SCUT-HCCDoc and the challenges of HCT understanding in document images. The dataset is available at https://github.com/HCIILAB/SCUT-HCCDoc_Dataset_Release."}}
