{"id": "MjTHxK0G4c", "cdate": 1672531200000, "mdate": 1681650020800, "content": {"title": "A Practical Upper Bound for the Worst-Case Attribution Deviations", "abstract": ""}}
{"id": "sDNuHPA7Ib4", "cdate": 1663850018955, "mdate": null, "content": {"title": "Certification of Attribution Robustness for Euclidean Distance and Cosine Similarity Measure", "abstract": "Model attribution is a critical component of deep neural networks (DNNs) for its interpretability to complex models. Recent works bring up attention to the security of attributions as they are vulnerable to attribution attacks that generate similar images with dramatically different attributions. Studies have been working on empirically improving the robustness of DNNs against those attacks. However, due to their lack of certification, the actual robustness of the model for a testing point is not known. In this work, we define \\emph{certified attribution robustness} for the first time that upper bounds the dissimilarity of attributions after the samples are perturbed by any noises within a certain region while the classification results remain the same. Based on the definition, we propose different approaches to certify the attributions using Euclidean distance and cosine similarity under both $\\ell_2$ and $\\ell_\\infty$-norm perturbations constraints. The bounds developed by our theoretical study are validated on three datasets (MNIST, Fashion-MNIST and CIFAR-10), and two different types of attacks (PGD attack and IFIA attribution attack). The experimental results show that the bounds certify the model effectively."}}
{"id": "QPg5TTAdizy", "cdate": 1652737651837, "mdate": null, "content": {"title": "Exploiting the Relationship Between Kendall's Rank Correlation and Cosine Similarity for Attribution Protection", "abstract": "Model attributions are important in deep neural networks as they aid practitioners in understanding the models, but recent studies reveal that attributions can be easily perturbed by adding imperceptible noise to the input. The non-differentiable Kendall's rank correlation is a key performance index for attribution protection. In this paper, we first show that the expected Kendall's rank correlation is positively correlated to cosine similarity and then indicate that the direction of attribution is the key to attribution robustness. Based on these findings, we explore the vector space of attribution to explain the shortcomings of attribution defense methods using $\\ell_p$ norm and propose integrated gradient regularizer (IGR), which maximizes the cosine similarity between natural and perturbed attributions. Our analysis further exposes that IGR encourages neurons with the same activation states for natural samples and the corresponding perturbed samples. Our experiments on different models and datasets confirm our analysis on attribution protection and demonstrate a decent improvement in adversarial robustness."}}
{"id": "HdjYd-pbjV", "cdate": 1640995200000, "mdate": 1681650020739, "content": {"title": "Exploiting the Relationship Between Kendall's Rank Correlation and Cosine Similarity for Attribution Protection", "abstract": ""}}
{"id": "6_qvPLq3JG8", "cdate": 1640995200000, "mdate": 1683959985350, "content": {"title": "Exploiting the Relationship Between Kendall's Rank Correlation and Cosine Similarity for Attribution Protection", "abstract": "Model attributions are important in deep neural networks as they aid practitioners in understanding the models, but recent studies reveal that attributions can be easily perturbed by adding imperceptible noise to the input. The non-differentiable Kendall's rank correlation is a key performance index for attribution protection. In this paper, we first show that the expected Kendall's rank correlation is positively correlated to cosine similarity and then indicate that the direction of attribution is the key to attribution robustness. Based on these findings, we explore the vector space of attribution to explain the shortcomings of attribution defense methods using $\\ell_p$ norm and propose integrated gradient regularizer (IGR), which maximizes the cosine similarity between natural and perturbed attributions. Our analysis further exposes that IGR encourages neurons with the same activation states for natural samples and the corresponding perturbed samples. Our experiments on different models and datasets confirm our analysis on attribution protection and demonstrate a decent improvement in adversarial robustness."}}
{"id": "eIDd-WOxnnq", "cdate": 1577836800000, "mdate": 1681650020719, "content": {"title": "New Threats Against Object Detector with Non-local Block", "abstract": ""}}
{"id": "y9x0FTBrLG", "cdate": 1546300800000, "mdate": null, "content": {"title": "Probabilistic Deep Ordinal Regression Based on Gaussian Processes", "abstract": "With excellent representation power for complex data, deep neural networks (DNNs) based approaches are state-of-the-art for ordinal regression problem which aims to classify instances into ordinal categories. However, DNNs are not able to capture uncertainties and produce probabilistic interpretations. As a probabilistic model, Gaussian Processes (GPs) on the other hand offers uncertainty information, which is nonetheless lack of scalability for large datasets. This paper adapts traditional GPs regression for ordinal regression problem by using both conjugate and non-conjugate ordinal likelihood. Based on that, it proposes a deep neural network with a GPs layer on the top, which is trained end-to-end by the stochastic gradient descent method for both neural network parameters and GPs parameters. The parameters in the ordinal likelihood function are learned as neural network parameters so that the proposed framework is able to produce fitted likelihood functions for training sets and make probabilistic predictions for test points. Experimental results on three real-world benchmarks - image aesthetics rating, historical image grading and age group estimation - demonstrate that in terms of mean absolute error, the proposed approach outperforms state-of-the-art ordinal regression approaches and provides the confidence for predictions."}}
