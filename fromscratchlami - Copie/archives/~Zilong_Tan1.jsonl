{"id": "2vSdklK2gR", "cdate": 1609459200000, "mdate": 1682324425969, "content": {"title": "Interpreting Write Performance of Supercomputer I/O Systems with Regression Models", "abstract": "This work seeks to advance the state of the art in HPC I/O performance analysis and interpretation. In particular, we demonstrate effective techniques to: (1) model output performance in the presence of I/O interference from production loads; (2) build features from write patterns and key parameters of the system architecture and configurations; (3) employ suitable machine learning algorithms to improve model accuracy. We train models with five popular regression algorithms and conduct experiments on two distinct production HPC platforms. We find that the lasso and random forest models predict output performance with high accuracy on both of the target systems. We also explore use of the models to guide adaptation in I/O middleware systems, and show potential for improvements of at least 15% from model-guided adaptation on 70% of samples, and improvements up to 10 x on some samples for both of the target systems."}}
{"id": "hQYtrAchLY9", "cdate": 1577836800000, "mdate": 1682324426132, "content": {"title": "Learning Fair Representations for Kernel Models", "abstract": "Fair representations are a powerful tool for establishing criteria like statistical parity, proxy non-discrimination, and equality of opportunity in learned models. Existing techniques for learning..."}}
{"id": "CTGYTIXPTlv", "cdate": 1577836800000, "mdate": null, "content": {"title": "Model Reduction of Shallow CNN Model for Reliable Deployment of Information Extraction from Medical Reports", "abstract": "Shallow Convolution Neural Network (CNN) is a time-tested tool for the information extraction from cancer pathology reports. Shallow CNN performs competitively on this task to other deep learning models including BERT, which holds the state-of-the-art for many NLP tasks. The main insight behind this eccentric phenomenon is that the information extraction from cancer pathology reports require only a small number of domain-specific text segments to perform the task, thus making the most of the texts and contexts excessive for the task. Shallow CNN model is well-suited to identify these key short text segments from the labeled training set; however, the identified text segments remain obscure to humans. In this study, we fill this gap by developing a model reduction tool to make a reliable connection between CNN filters and relevant text segments by discarding the spurious connections. We reduce the complexity of shallow CNN representation by approximating it with a linear transformation of n-gram presence representation with a non-negativity and sparsity prior on the transformation weights to obtain an interpretable model. Our approach bridge the gap between the conventionally perceived trade-off boundary between accuracy on the one side and explainability on the other by model reduction."}}
{"id": "f8rN_ZkJ3wF", "cdate": 1514764800000, "mdate": 1682324426031, "content": {"title": "Scalable Algorithms for Learning High-Dimensional Linear Mixed Models", "abstract": ""}}
{"id": "CcvNTilQ5yT", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Integral Representations of Gaussian Processes", "abstract": "We propose a representation of Gaussian processes (GPs) based on powers of the integral operator defined by a kernel function, we call these stochastic processes integral Gaussian processes (IGPs). Sample paths from IGPs are functions contained within the reproducing kernel Hilbert space (RKHS) defined by the kernel function, in contrast sample paths from the standard GP are not functions within the RKHS. We develop computationally efficient non-parametric regression models based on IGPs. The main innovation in our regression algorithm is the construction of a low dimensional subspace that captures the information most relevant to explaining variation in the response. We use ideas from supervised dimension reduction to compute this subspace. The result of using the construction we propose involves significant improvements in the computational complexity of estimating kernel hyper-parameters as well as reducing the prediction variance."}}
{"id": "HJ-f3iZOZS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Partitioned Tensor Factorizations for Learning Mixed Membership Models", "abstract": "We present an efficient algorithm for learning mixed membership models when the number of variables p is much larger than the number of hidden components k. This algorithm reduces the computational..."}}
{"id": "c7-wLaDJIW7", "cdate": 1451606400000, "mdate": 1682324426199, "content": {"title": "Tempo: Robust and Self-Tuning Resource Management in Multi-tenant Parallel Databases", "abstract": ""}}
