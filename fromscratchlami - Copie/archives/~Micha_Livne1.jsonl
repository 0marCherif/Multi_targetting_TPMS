{"id": "mqWu7R4RFR", "cdate": 1683880785332, "mdate": 1683880785332, "content": {"title": "Improving Small Molecule Generation using Mutual Information Machine", "abstract": "We address the task of controlled generation of small molecules, which entails finding novel molecules with desired properties under certain constraints. Here we introduce MolMIM, a probabilistic auto-encoder for small molecule drug discovery that learns an informative and clustered latent space. MolMIM is trained with Mutual Information Machine (MIM) learning and provides a fixed-size representation of variable-length SMILES strings. Since encoder-decoder models can learn representations with \u201choles\u201d of invalid samples, here we propose a novel extension to the MIM training procedure which promotes a dense latent space and allows the model to sample valid molecules from random perturbations of latent codes. We provide a thorough comparison of MolMIM to several variable-size and fixed-size encoder-decoder models, demonstrating MolMIM\u2019s superior generation as measured in terms of validity, uniqueness, and novelty. We then utilize CMA-ES, a naive black-box, and gradient-free search algorithm, over MolMIM\u2019s latent space for the task of property-guided molecule optimization. We achieve state-of-the-art results in several constrained single-property optimization tasks and show competitive results in the challenging task of multi-objective optimization. We attribute the strong results to the structure of MolMIM\u2019s learned representation which promotes the clustering of similar molecules in the latent space, whereas CMA-ES is often used as a baseline optimization method. We also demonstrate MolMIM to be favorable in a compute-limited regime."}}
{"id": "iOJlwUTUyrN", "cdate": 1675975515341, "mdate": null, "content": {"title": "Improving Small Molecule Generation using Mutual Information Machine", "abstract": "We address the task of controlled generation of small molecules, which entails finding novel molecules with desired properties under certain constraints. \nHere we introduce MolMIM, a probabilistic auto-encoder for small molecule drug discovery that learns an informative and clustered latent space.\nMolMIM is trained with Mutual Information Machine (MIM) learning and provides a fixed-size representation of variable-length SMILES strings.\nSince encoder-decoder models can learn representations with ``holes'' of invalid samples, here we propose a novel extension to the MIM training procedure which promotes a dense latent space and allows the model to sample valid molecules from random perturbations of latent codes.\nWe provide a thorough comparison of MolMIM to several variable-size and fixed-size encoder-decoder models, demonstrating MolMIM's superior generation as measured in terms of validity, uniqueness, and novelty.\nWe then utilize CMA-ES, a naive black-box, and gradient-free search algorithm, over MolMIM's latent space for the task of property-guided molecule optimization.\nWe achieve state-of-the-art results in several constrained single-property optimization tasks and show competitive results in the challenging task of multi-objective optimization.\nWe attribute the strong results to the structure of MolMIM's learned representation which promotes the clustering of similar molecules in the latent space, whereas CMA-ES is often used as a baseline optimization method. \nWe also demonstrate MolMIM to be favorable in a compute-limited regime.\n"}}
{"id": "BJl9ZTVKwB", "cdate": 1569438978284, "mdate": null, "content": {"title": "MIM: Mutual Information Machine", "abstract": "    We introduce the Mutual Information Machine (MIM), an autoencoder framework\n    for learning joint distributions over observations and latent states. \n    The model formulation reflects two key design principles: 1) symmetry, to encourage \n    the encoder     and decoder to learn different factorizations of the same \n    underlying distribution; and 2) mutual information, to encourage the learning \n    of useful representations for downstream tasks. \n    The objective comprises the Jensen-Shannon divergence between the encoding and \n    decoding joint distributions, plus a mutual information regularizer. \n    We show that this can be bounded by a tractable cross-entropy loss between \n    the true model and a parameterized approximation, and relate this to \n    maximum likelihood estimation and variational autoencoders.\n    Experiments show that MIM is capable of learning a latent representation with high mutual information,\n    and good unsupervised clustering, while providing NLL comparable to VAE \n    (with a sufficiently expressive architecture)."}}
{"id": "SkNLOKWd-r", "cdate": 1262304000000, "mdate": null, "content": {"title": "Human Attributes from 3D Pose Tracking", "abstract": "We show that, from the output of a simple 3D human pose tracker one can infer physical attributes (e.g., gender and weight) and aspects of mental state (e.g., happiness or sadness). This task is useful for man-machine communication, and it provides a natural benchmark for evaluating the performance of 3D pose tracking methods (vs.\u00a0conventional Euclidean joint error metrics). Based on an extensive corpus of motion capture data, with physical and perceptual ground truth, we analyze the inference of subtle biologically-inspired attributes from cyclic gait data. It is shown that inference is also possible with partial observations of the body, and with motions as short as a single gait cycle. Learning models from small amounts of noisy video pose data is, however, prone to over-fitting. To mitigate this we formulate learning in terms of domain adaptation, for which mocap data is uses to regularize models for inference from video-based data."}}
