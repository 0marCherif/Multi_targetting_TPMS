{"id": "YwgN2ukT-xr", "cdate": 1672531200000, "mdate": 1682354030054, "content": {"title": "Unit Scaling: Out-of-the-Box Low-Precision Training", "abstract": ""}}
{"id": "g2t9L0mTz5K", "cdate": 1640995200000, "mdate": 1682354030052, "content": {"title": "8-bit Numerical Formats for Deep Neural Networks", "abstract": ""}}
{"id": "9mIhQqBa3eh", "cdate": 1640995200000, "mdate": 1682354030051, "content": {"title": "BESS: Balanced Entity Sampling and Sharing for Large-Scale Knowledge Graph Completion", "abstract": ""}}
{"id": "eYyvftCgtD", "cdate": 1632875710606, "mdate": null, "content": {"title": "GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures", "abstract": "Attention based language models have become a critical component in state-of-the-art natural language processing systems. However, these models have significant computational requirements, due to long training times, dense operations and large parameter count. In this work we demonstrate a set of modifications to the structure of a Transformer layer, producing a more efficient architecture. First, we rely on grouped transformations to reduce the computational cost of dense feed-forward layers, while preserving the expressivity of the model . Secondly, we add a grouped convolution module to complement the self-attention module, decoupling the learning of local and global interactions. We apply the resulting architecture to language representation learning and demonstrate its superior performance compared to BERT models of different scales. We further highlight its improved efficiency, both in terms of floating-point operations (FLOPs) and time-to-train."}}
{"id": "-e7awdzWsOc", "cdate": 1632875514370, "mdate": null, "content": {"title": "Towards Structured Dynamic Sparse Pre-Training of BERT", "abstract": "Identifying algorithms for computational efficient unsupervised training of large language models is an important and active area of research. \nIn this work, we develop and study a straightforward, dynamic always-sparse pre-training approach for BERT language modeling, which leverages periodic compression steps based on magnitude pruning followed by random parameter re-allocation. \nThis approach enables us to achieve Pareto improvements in terms of the number of floating-point operations (FLOPs) over statically sparse and dense models across a broad spectrum of network sizes. \nFurthermore, we demonstrate that training remains FLOP-efficient when using coarse-grained block sparsity, making it particularly promising for efficient execution on modern hardware accelerators."}}
{"id": "pzmwfDLoANS", "cdate": 1621630016862, "mdate": null, "content": {"title": "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "abstract": "We investigate the reasons for the performance degradation incurred with batch-independent normalization. We find that the prototypical techniques of layer normalization and instance normalization both induce the appearance of failure modes in the neural network's pre-activations: (i) layer normalization induces a collapse towards channel-wise constant functions; (ii) instance normalization induces a lack of variability in instance statistics, symptomatic of an alteration of the expressivity. To alleviate failure mode (i) without aggravating failure mode (ii), we introduce the technique \"Proxy Normalization\" that normalizes post-activations using a proxy distribution. When combined with layer normalization or group normalization, this batch-independent normalization emulates batch normalization's behavior and consistently matches or exceeds its performance."}}
{"id": "yV530nbJ2v1", "cdate": 1609459200000, "mdate": 1682354029997, "content": {"title": "Towards Structured Dynamic Sparse Pre-Training of BERT", "abstract": "Identifying algorithms for computational efficient unsupervised training of large language models is an important and active area of research. In this work, we develop and study a straightforward, dynamic always-sparse pre-training approach for BERT language modeling task, which leverages periodic compression steps based on magnitude pruning followed by random parameter re-allocation. This approach enables us to achieve Pareto improvements in terms of the number of floating-point operations (FLOPs) over statically sparse and dense models across a broad spectrum of network sizes. Furthermore, we demonstrate that training remains FLOP-efficient when using coarse-grained block sparsity, making it particularly promising for efficient execution on modern hardware accelerators."}}
{"id": "tpv92FvEuph", "cdate": 1609459200000, "mdate": 1681486382114, "content": {"title": "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "abstract": ""}}
{"id": "iiL-zqz6XSo", "cdate": 1609459200000, "mdate": 1681486272049, "content": {"title": "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "abstract": ""}}
{"id": "XWiImU96Ljh", "cdate": 1609459200000, "mdate": 1681486272052, "content": {"title": "Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training", "abstract": ""}}
