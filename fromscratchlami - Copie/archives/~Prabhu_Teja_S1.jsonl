{"id": "KaVbxkOSx_", "cdate": 1671958203951, "mdate": 1671958203951, "content": {"title": "PAUMER: Patch Pausing Transformer for Semantic Segmentation", "abstract": "We study the problem of improving the efficiency of segmentation transformers by using disparate amounts of computation different parts of the image. Our method, PAUMER, accomplishes this by pausing computation for patches that are deemed to not need any more computation before the final decoder. We use the entropy of predictions computed from intermediate activations as the pausing criterion, and find this aligns well with semantics of the image. Our method has a unique advantage that a single network trained with the proposed strategy can be effortlessly adapted at inference to various run-time requirements by modulating its pausing parameters. On two standard segmentation datasets, Cityscapes and ADE20K, we show that our method operates with about a 50% higher throughput with an mIoU drop of about 0.65% and 4.6% respectively.\n"}}
{"id": "GbBeI5z86uD", "cdate": 1633790965163, "mdate": null, "content": {"title": "Test time Adaptation through Perturbation Robustness", "abstract": "Data samples generated by several real world processes are dynamic in nature i.e., their characteristics vary with time. Thus it is not possible to train and tackle all possible distributional shifts between training and inference, using the host of transfer learning methods in literature. In this paper, we tackle this problem of adapting to domain shift at inference time i.e., we do not change the training process, but quickly adapt the model at test-time to handle any domain shift. For this, we propose to enforce consistency of predictions of data sampled in the vicinity of test sample on the image manifold. On a host of test scenarios like dealing with corruptions (CIFAR-10-C and CIFAR-100-C), and domain adaptation (VisDA-C), our method is at par or significantly outperforms previous methods."}}
{"id": "nfKzeS-XZgX", "cdate": 1609459200000, "mdate": 1631684072731, "content": {"title": "Uncertainty Reduction for Model Adaptation in Semantic Segmentation", "abstract": "Traditional methods for Unsupervised Domain Adaptation (UDA) targeting semantic segmentation exploit information common to the source and target domains, using both labeled source data and unlabeled target data. In this paper, we investigate a setting where the source data is unavailable, but the classifier trained on the source data is; hence named \"\"model adaptation\"\". Such a scenario arises when data sharing is prohibited, for instance, because of privacy, or Intellectual Property (IP) issues. To tackle this problem, we propose a method that reduces the uncertainty of predictions on the target domain data. We accomplish this in two ways: minimizing the entropy of the predicted posterior, and maximizing the noise robustness of the feature representation. We show the efficacy of our method on the transfer of segmentation from computer generated images to real-world driving images, and transfer between data collected in different cities, and surprisingly reach performance competitive with that of the methods that have access to source data."}}
{"id": "U4xKxpfgcXE", "cdate": 1577836800000, "mdate": 1631684072727, "content": {"title": "Optimizer Benchmarking Needs to Account for Hyperparameter Tuning", "abstract": "The performance of optimizers, particularly in deep learning, depends considerably on their chosen hyperparameter configuration. The efficacy of optimizers is often studied under near-optimal probl..."}}
{"id": "H1gEP6NFwr", "cdate": 1569439068300, "mdate": null, "content": {"title": "On the Tunability of Optimizers in Deep Learning", "abstract": "There is no consensus yet on the question whether adaptive gradient methods like Adam are easier to use than non-adaptive optimization methods like SGD. In this work, we fill in the important, yet ambiguous concept of \u2018ease-of-use\u2019 by defining an optimizer\u2019s tunability:  How easy is it to find good hyperparameter configurations using automatic random hyperparameter search? We propose a practical and universal quantitative measure for optimizer tunability that can form the basis for a fair optimizer benchmark.  Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, we find  that Adam is the most tunable for the majority of problems, especially with a low budget for hyperparameter tuning."}}
