{"id": "mTra5BIUyRV", "cdate": 1652737546285, "mdate": null, "content": {"title": "Fair Ranking with Noisy Protected Attributes", "abstract": "The fair-ranking problem, which asks to rank a given set of items to maximize utility subject to group fairness constraints, has received attention in the fairness, information retrieval, and machine learning literature. Recent works, however, observe that errors in socially-salient (including protected) attributes of items can significantly undermine fairness guarantees of existing fair-ranking algorithms and raise the problem of mitigating the effect of such errors. We study the fair-ranking problem under a model where socially-salient attributes of items are randomly and independently perturbed. We present a fair-ranking framework that incorporates group fairness requirements along with probabilistic information about perturbations in socially-salient attributes. We provide provable guarantees on the fairness and utility attainable by our framework and show that it is information-theoretically impossible to significantly beat these guarantees. Our framework works for multiple non-disjoint  attributes and a general class of fairness constraints that includes proportional and equal representation. Empirically, we observe that, compared to baselines, our algorithm outputs rankings with higher fairness, and has a similar or better fairness-utility trade-off compared to baselines."}}
{"id": "LEqVjnffcWo", "cdate": 1621630286186, "mdate": null, "content": {"title": "Fair Classification with Adversarial Perturbations", "abstract": "We study fair classification in the presence of an omniscient adversary that, given an $\\eta$, is allowed to choose an arbitrary $\\eta$-fraction of the training samples and arbitrarily perturb their protected attributes. The motivation comes from settings in which protected attributes can be incorrect due to strategic misreporting, malicious actors, or errors in imputation; and prior approaches that make stochastic or independence assumptions on errors may not satisfy their guarantees in this adversarial setting. Our main contribution is an optimization framework to learn fair classifiers in this adversarial setting that comes with provable guarantees on accuracy and fairness. Our framework works with multiple and non-binary protected attributes, is designed for the large class of linear-fractional fairness metrics, and can also handle perturbations besides protected attributes. We prove near-tightness of our framework's guarantees for natural hypothesis classes: no algorithm can have significantly better accuracy and any algorithm with better fairness must have lower accuracy. Empirically, we evaluate the classifiers produced by our framework for statistical rate on real-world and synthetic datasets for a family of adversaries."}}
{"id": "_lm2jZGeMoD", "cdate": 1609459200000, "mdate": null, "content": {"title": "The Effect of the Rooney Rule on Implicit Bias in the Long Term", "abstract": "The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change. In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist---independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected."}}
{"id": "UqN1aVVA17L", "cdate": 1609459200000, "mdate": null, "content": {"title": "Mitigating Bias in Set Selection with Noisy Protected Attributes", "abstract": "Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a \"denoised\" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness."}}
{"id": "DpkmuVyRNw", "cdate": 1577836800000, "mdate": null, "content": {"title": "Interventions for ranking in the presence of implicit bias", "abstract": ""}}
{"id": "SJlGaRBJMN", "cdate": 1546768058366, "mdate": null, "content": {"title": "Reproducibility report for: A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": ""}}
{"id": "HJ-iShbuZS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Toward Controlling Discrimination in Online Ad Auctions", "abstract": "Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender..."}}
