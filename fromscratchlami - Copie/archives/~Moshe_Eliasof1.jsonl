{"id": "zoQZxGHO6WF", "cdate": 1665013556319, "mdate": null, "content": {"title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations", "abstract": "Graph neural networks are have shown their efficacy in fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow networks. \nThis behaviour usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures\nto control this behaviour by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behaviour can be explained by similar analysis."}}
{"id": "fwn2Mqpy4pS", "cdate": 1663850277302, "mdate": null, "content": {"title": "$\\omega$GNNs: Deep Graph Neural Networks Enhanced by Multiple Propagation Operators", "abstract": "Graph Neural Networks (GNNs) are limited in their propagation operators. These operators often contain non-negative elements only and are shared across channels and layers, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs.\nIn this paper, we bridge this gap by incorporating trainable channel-wise weighting factors $\\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called $\\omega$GNN, and we study two variants: $\\omega$GCN and $\\omega$GAT.\nFor $\\omega$GCN, we theoretically analyse its behaviour and the impact of $\\omega$ on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth.\nAdditionally, we experiment with 15 real-world datasets on node- and graph-classification tasks, where our $\\omega$GCN and $\\omega$GAT perform better or on par with state-of-the-art methods. "}}
{"id": "YCgwkDo56q", "cdate": 1662812639850, "mdate": null, "content": {"title": "Global-Local Graph Neural Networks for Node-Classification", "abstract": "The task of graph node-classification is often approached using a \\emph{local} Graph Neural Network (GNN), that learns only local information from the node input features and their adjacency. In this paper we propose to benefit from global and local information through the form of learning \\emph{label}- and \\emph{node}- features to improve node-classification accuracy. We therefore call our method Global-Local-GNN (GLGNN).\nTo learn proper label features, for each label, we maximize the similarity between its features and nodes features that belong to the label, while maximizing the distance between nodes that do not belong to the considered label. We then use the learnt label features to predict the node-classification map. We demonstrate our GLGNN using GCN and GAT as GNN backbones, and show that our GLGNN approach improves baseline performance on the node-classification task."}}
{"id": "Qb3Jpm0jAt4", "cdate": 1632781958330, "mdate": null, "content": {"title": "MGIC: Multigrid-in-Channels Neural Network Architectures", "abstract": "Multigrid (MG) methods are effective at solving numerical PDEs in linear complexity. In this work we present a multigrid-in-channels (MGIC) approach that tackles the quadratic growth of the number of parameters with respect to the number of channels in standard convolutional neural networks (CNNs). Indeed, lightweight CNNs can achieve comparable accuracy to standard CNNs with fewer parameters; however, the number of weights still scales quadratically with the CNN's width. Our MGIC architectures replace each CNN block with an MGIC counterpart that utilizes a hierarchy of nested grouped convolutions of small group size to address this. \nHence, our proposed architectures scale linearly with respect to the network's width while retaining full coupling of the channels as in standard CNNs.\nOur extensive experiments on image classification, segmentation, and point cloud classification show that applying this strategy to different architectures reduces the number of parameters while obtaining similar or better accuracy."}}
{"id": "_dIqV256AT5", "cdate": 1632781957971, "mdate": null, "content": {"title": "Quantized convolutional neural networks through the lens of partial differential equations", "abstract": "Quantization of Convolutional Neural Networks (CNNs) is a common approach to ease the computational burden involved in the deployment of CNNs. However, fixed-point arithmetic is not natural to the type of computations involved in neural networks.  \nIn our work, we consider symmetric and stable variants of common CNNs for image classification, and Graph Convolutional Networks (GCNs) for graph node-classification. We demonstrate through several experiments that the property of forward stability preserves the action of a network under different quantization rates, allowing stable quantized networks to behave similarly to their non-quantized counterparts while using fewer parameters. We also find that at times, stability aids in improving accuracy. These properties are of particular interest for sensitive, resource-constrained or real-time applications. "}}
{"id": "wWtk6GxJB2x", "cdate": 1621630055302, "mdate": null, "content": {"title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations", "abstract": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. \nThis behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures\nto control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures."}}
{"id": "ZWG2j6KBd1y", "cdate": 1609459200000, "mdate": null, "content": {"title": "Mimetic Neural Networks: A unified framework for Protein Design and Folding", "abstract": "Recent advancements in machine learning techniques for protein folding motivate better results in its inverse problem -- protein design. In this work we introduce a new graph mimetic neural network, MimNet, and show that it is possible to build a reversible architecture that solves the structure and design problems in tandem, allowing to improve protein design when the structure is better estimated. We use the ProteinNet data set and show that the state of the art results in protein design can be improved, given recent architectures for protein folding."}}
{"id": "uAlw1yHv67p", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multigrid-in-Channels Neural Network Architectures", "abstract": "We present a multigrid-in-channels (MGIC) approach that tackles the quadratic growth of the number of parameters with respect to the number of channels in standard convolutional neural networks (CNNs). Thereby our approach addresses the redundancy in CNNs that is also exposed by the recent success of lightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard CNNs with fewer parameters; however, the number of weights still scales quadratically with the CNN's width. Our MGIC architectures replace each CNN block with an MGIC counterpart that utilizes a hierarchy of nested grouped convolutions of small group size to address this. Hence, our proposed architectures scale linearly with respect to the network's width while retaining full coupling of the channels as in standard CNNs. Our extensive experiments on image classification, segmentation, and point cloud classification show that applying this strategy to different architectures like ResNet and MobileNetV3 reduces the number of parameters while obtaining similar or better accuracy."}}
{"id": "Q4VZ4fQfbGm", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multimodal 3D Shape Reconstruction under Calibration Uncertainty Using Parametric Level Set Methods", "abstract": "We consider the problem of 3D shape reconstruction from multimodal data, given uncertain calibration parameters. Typically, 3D data modalities can come in diverse forms such as sparse point sets, volumetric slices, and 2D photos. To jointly process these data modalities, we exploit a parametric level set method that utilizes ellipsoidal radial basis functions. This method not only allows us to analytically and compactly represent the object; it also confers on us the ability to overcome calibration-related noise that originates from inaccurate acquisition parameters. This essentially implicit regularization leads to a highly robust and scalable reconstruction, surpassing other traditional methods. In our results we first demonstrate the ability of the method to compactly represent complex objects. We then show that our reconstruction method is robust both to a small number of measurements and to noise in the acquisition parameters. Finally, we demonstrate our reconstruction abilities from diverse modalities such as volume slices obtained from liquid displacement (similar to CT scans and X-rays) and visual measurements obtained from shape silhouettes as well as point clouds."}}
{"id": "0MWv9nCs1hG", "cdate": 1577836800000, "mdate": null, "content": {"title": "DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling", "abstract": "Graph Convolutional Networks (GCNs) have shown to be effective in handling unordered data like point clouds and meshes. In this work we propose novel approaches for graph convolution, pooling and unpooling, inspired from finite differences and algebraic multigrid frameworks. We form a parameterized convolution kernel based on discretized differential operators, leveraging the graph mass, gradient and Laplacian. This way, the parameterization does not depend on the graph structure, only on the meaning of the network convolutions as differential operators. To allow hierarchical representations of the input, we propose pooling and unpooling operations that are based on algebraic multigrid methods, which are mainly used to solve partial differential equations on unstructured grids. To motivate and explain our method, we compare it to standard convolutional neural networks, and show their similarities and relations in the case of a regular grid. Our proposed method is demonstrated in various experiments like classification and part-segmentation, achieving on par or better than state of the art results. We also analyze the computational cost of our method compared to other GCNs."}}
