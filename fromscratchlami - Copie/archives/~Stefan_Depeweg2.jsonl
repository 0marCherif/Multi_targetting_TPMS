{"id": "rFJtculICOg", "cdate": 1598895774291, "mdate": null, "content": {"title": "A benchmark environment motivated by industrial control problems", "abstract": "In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB\u2019s dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems."}}
{"id": "BkVoNoZOZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning", "abstract": "Bayesian neural networks with latent variables are scalable and flexible probabilistic models: they account for uncertainty in the estimation of the network weights and, by making use of latent var..."}}
{"id": "H1fl8S9ee", "cdate": null, "mdate": null, "content": {"title": "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks", "abstract": "We present an algorithm for policy search in stochastic dynamical systems using\nmodel-based reinforcement learning. The system dynamics are described with\nBayesian neural networks (BNNs) that include stochastic input variables.  These\ninput variables allow us to capture complex statistical\npatterns in the transition dynamics (e.g. multi-modality and\nheteroskedasticity), which are usually missed by alternative modeling approaches. After\nlearning the dynamics, our BNNs are then fed into an algorithm that performs\nrandom roll-outs and uses stochastic optimization for policy learning. We train\nour BNNs by minimizing $\\alpha$-divergences with $\\alpha = 0.5$, which usually produces better\nresults than other techniques such as variational Bayes. We illustrate the performance of our method by\nsolving a challenging problem where model-based approaches usually fail and by\nobtaining promising results in real-world scenarios including the control of a\ngas turbine and an industrial benchmark."}}
