{"id": "wjClgX-muzB", "cdate": 1652737403583, "mdate": null, "content": {"title": "Rethinking Variational Inference for Probabilistic Programs with Stochastic Support", "abstract": "We introduce Support Decomposition Variational Inference (SDVI), a new variational inference (VI) approach for probabilistic programs with stochastic support. Existing approaches to this problem rely on designing a single global variational guide on a variable-by-variable basis, while maintaining the stochastic control flow of the original program. SDVI instead breaks the program down into sub-programs with static support, before automatically building separate sub-guides for each. This decomposition significantly aids in the construction of suitable variational families, enabling, in turn, substantial improvements in inference performance."}}
{"id": "H0gOIL8j9xc", "cdate": 1646077518150, "mdate": null, "content": {"title": "Expectation Programming: Adapting Probabilistic Programming Systems to Estimate Expectations Efficiently", "abstract": "We show that the standard computational pipeline of probabilistic programming systems (PPSs) can be inefficient for estimating expectations and introduce the concept of expectation programming to address this. In expectation programming, the aim of the backend inference engine is to directly estimate expected return values of programs, as opposed to approximating their conditional distributions. This distinction, while subtle, allows us to achieve substantial performance improvements over the standard PPS computational pipeline by tailoring computation to the expectation we care about. We realize a particular instance of our expectation programming concept, Expectation Programming in Turing (EPT), by extending the PPS Turing to allow so-called target-aware inference to be run automatically. We then verify the statistical soundness of EPT theoretically, and show that it provides substantial empirical gains in practice."}}
{"id": "k--aDy7DJB_", "cdate": 1606146134112, "mdate": null, "content": {"title": "Expectation Programming: Adapting Probabilistic Programming Systems to Estimate Expectations Efficiently", "abstract": "NOTE: A full paper version of this abstract has been accepted to UAI 2022.\n\nBuilding on ideas from probabilistic programming, we introduce the concept of an expectation programming framework (EPF) that automates the calculation of expectations. Analogous to a probabilistic program, an expectation program is comprised of a mix of probabilistic constructs and deterministic calculations, between which a conditional distribution over internal variables and outputs is defined. However, the focus of the inference engine in an EPF is to directly calculate the expectation of the program return values, rather than this conditional distribution. This is made possible by exploiting recent advancements in target-aware Bayesian inference, through which we can tailor our inference engines to this expectation estimation, providing the potential for substantial improvements over the standard probabilistic programming pipeline. We realize a particular instantiation of our EPF concept by extending the probabilistic programming language Turing with a new @expectation macro that uses a series of program transformations to automatically run target--aware inference. We show that this leads to significant empirical gains in estimation performance compared to conventional use of Turing on two example problems."}}
