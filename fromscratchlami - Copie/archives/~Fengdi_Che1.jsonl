{"id": "Jx7LBaXM3M", "cdate": 1676827065946, "mdate": null, "content": {"title": "Loosely Consistent Emphatic Temporal-Difference Learning", "abstract": "There has been significant interest in searching for off-policy Temporal-Difference (TD) algorithms that find the same solution that would have been obtained in the on-policy regime. An important property of such algorithms is that their expected update has the same fixed point as that of On-policy TD($\\lambda$), which we call \\emph{loose consistency}. Notably, Full-IS-TD($\\lambda$) is the only existing loosely consistent method under general linear function approximation but, unfortunately, has a high variance and is scarcely practical. This notorious high variance issue motivates the introduction of ETD($\\lambda$), which tames down the variance but has a biased fixed point. Inspired by these two methods, we propose a new loosely consistent algorithm called \\emph{Average Emphatic TD} (AETD($\\lambda$)) with a transient bias, which strikes a balance between bias and variance. Further, we unify AETD($\\lambda$) with existing methods and obtain a new family of loosely consistent algorithms called \\emph{Loosely Consistent Emphatic TD} (LC-ETD($\\lambda$, $\\beta$, $\\nu$)), which can control a smooth bias-variance trade-off by varying the speed at which the transient bias fades. Through experiments on illustrative examples, we show the effectiveness and practicality of LC-ETD($\\lambda$, $\\beta$, $\\nu$)."}}
{"id": "KpaXq80MuY", "cdate": 1665251233671, "mdate": null, "content": {"title": "Bayesian Q-learning With Imperfect Expert Demonstrations", "abstract": "Guided exploration with expert demonstrations improves data efficiency for reinforcement learning, but current algorithms often overuse expert information. We propose a novel algorithm to speed up Q-learning with the help of a limited amount of imperfect expert demonstrations. The algorithm is based on a Bayesian framework to model suboptimal expert actions and derives Q-values' update rules by maximizing the posterior probability. It weighs expert information by the uncertainty of learnt Q-values and avoids excessive reliance on expert data, gradually reducing the usage of uninformative expert data. Experimentally, we evaluate our approach on a sparse-reward chain environment and six more complicated Atari games with delayed rewards. With the proposed methods, we can achieve better results than Deep Q-learning from Demonstrations (Hester et al., 2017) in most environments."}}
{"id": "rIq73puh9-", "cdate": 1664994277666, "mdate": null, "content": {"title": "Bayesian Q-learning With Imperfect Expert Demonstrations", "abstract": "Guided exploration with expert demonstrations improves data efficiency for reinforcement learning, but current algorithms often overuse expert information. We propose a novel algorithm to speed up Q-learning with the help of a limited amount of imperfect expert demonstrations. The algorithm avoids excessive reliance on expert data by relaxing the optimal expert assumption and gradually reducing the usage of uninformative expert data. Experimentally, we evaluate our approach on a sparse-reward chain environment and six more complicated Atari games with delayed rewards. We can achieve better results with the proposed methods than Deep Q-learning from Demonstrations (Hester et al., 2017) in most environments."}}
