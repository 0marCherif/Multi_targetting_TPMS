{"id": "wH0SdQr0jRH", "cdate": 1686250302838, "mdate": null, "content": {"title": "Regret Minimization via Saddle Point Optimization", "abstract": "A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. \nIn the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we propose a novel anytime variant of the Estimation-To-Decisions algorithm. Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. \nWe illustrate the results by deriving improved rates for high-dimensional linear bandits. Lastly, we point out connections to the information ratio, decoupling coefficient and PAC-DEC, and numerically evaluate the performance of E2D on simple examples."}}
{"id": "l63GayMxG-4", "cdate": 1683897960919, "mdate": 1683897960919, "content": {"title": "Efficient Planning in Combinatorial Action Spaces with Applications to Cooperative Multi-Agent Reinforcement Learning", "abstract": "A practical challenge in reinforcement learning are combinatorial action spaces that make planning computationally demanding. For example, in cooperative multi-agent reinforcement learning, a potentially large number of agents jointly optimize a global reward function, which leads to a combinatorial blow-up in the action space by the number of agents. As a minimal requirement, we assume access to an argmax oracle that allows to efficiently compute the greedy policy for any Q-function in the model class. Building on recent work in planning with local access to a simulator and linear function approximation, we propose efficient algorithms for this setting that lead to polynomial compute and query complexity in all relevant problem parameters. For the special case where the feature decomposition is additive, we further improve the bounds and extend the results to the kernelized setting with an efficient algorithm."}}
{"id": "EoRe-uoRAx6", "cdate": 1672531200000, "mdate": 1681650607918, "content": {"title": "Linear Partial Monitoring for Sequential Decision-Making: Algorithms, Regret Bounds and Applications", "abstract": ""}}
{"id": "4qo3gsuR0Z", "cdate": 1672531200000, "mdate": 1681650608031, "content": {"title": "Efficient Planning in Combinatorial Action Spaces with Applications to Cooperative Multi-Agent Reinforcement Learning", "abstract": ""}}
{"id": "ZmYHoQm0SWH", "cdate": 1663850251119, "mdate": null, "content": {"title": "Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off", "abstract": "A default assumption in reinforcement learning and optimal control is that experience arrives at discrete time points on a fixed clock cycle. Many applications, however, involve continuous systems where the time discretization is not fixed but instead can be managed by a learning algorithm. By analyzing Monte-Carlo value estimation for LQR systems in both finite-horizon and infinite-horizon settings, we uncover a fundamental trade-off between approximation and statistical error in value estimation. Importantly, these two errors behave differently with respect to time discretization, which implies that there is an optimal choice for the temporal resolution that depends on the data budget. These findings show how adapting the temporal resolution can provably improve value estimation quality in LQR systems from finite data. Empirically, we demonstrate the trade-off in numerical simulations of LQR instances and several non-linear environments."}}
{"id": "3OR2tbtnYC-", "cdate": 1663850035611, "mdate": null, "content": {"title": "Near-optimal Policy Identification in Active Reinforcement Learning", "abstract": "Many real-world reinforcement learning tasks require control of complex dynamical systems that involve both costly data acquisition processes and large state spaces. In cases where the expensive transition dynamics can be readily evaluated at specified states (e.g., via a simulator), agents can operate in what is often referred to as planning with a \\emph{generative model}. We propose the AE-LSVI algorithm for best policy identification, a novel variant of the kernelized least-squares value iteration (LSVI) algorithm that combines optimism with pessimism for active exploration (AE). AE-LSVI provably identifies a near-optimal policy \\emph{uniformly} over an entire state space and achieves polynomial sample complexity guarantees that are independent of the number of states. When specialized to the recently introduced offline contextual Bayesian optimization setting, our algorithm achieves improved sample complexity bounds. Experimentally, we demonstrate that AE-LSVI outperforms other RL algorithms in a variety of environments when robustness to the initial state is required. "}}
{"id": "oV4OW-wQmr", "cdate": 1640995200000, "mdate": 1681650607749, "content": {"title": "Tuning Particle Accelerators with Safety Constraints using Bayesian Optimization", "abstract": ""}}
{"id": "cJjCvlpBLa", "cdate": 1640995200000, "mdate": 1673939082006, "content": {"title": "Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off", "abstract": ""}}
{"id": "8HywRps_Yea", "cdate": 1640995200000, "mdate": 1681650607701, "content": {"title": "Near-optimal Policy Identification in Active Reinforcement Learning", "abstract": ""}}
{"id": "ppfhqZv7x0", "cdate": 1609459200000, "mdate": 1681650607689, "content": {"title": "Bias-Robust Bayesian Optimization via Dueling Bandits", "abstract": ""}}
