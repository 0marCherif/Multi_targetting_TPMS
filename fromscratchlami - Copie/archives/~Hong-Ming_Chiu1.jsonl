{"id": "e62ZssObZp", "cdate": 1652737710569, "mdate": null, "content": {"title": "Accelerating SGD for Highly Ill-Conditioned Huge-Scale Online Matrix Completion", "abstract": "The matrix completion problem seeks to recover a $d\\times d$ ground truth matrix of low rank $r\\ll d$ from observations of its individual elements. Real-world matrix completion is often a huge-scale optimization problem, with $d$ so large that even the simplest full-dimension vector operations with $O(d)$ time complexity become prohibitively expensive. Stochastic gradient descent (SGD) is one of the few algorithms capable of solving matrix completion on a huge scale, and can also naturally handle streaming data over an evolving ground truth. Unfortunately, SGD experiences a dramatic slow-down when the underlying ground truth is ill-conditioned; it requires at least $O(\\kappa\\log(1/\\epsilon))$ iterations to get $\\epsilon$-close to ground truth matrix with condition number $\\kappa$. In this paper, we propose a preconditioned version of SGD that preserves all the favorable practical qualities of SGD for huge-scale online optimization while also making it agnostic to $\\kappa$. For a symmetric ground truth and the Root Mean Square Error (RMSE) loss, we prove that the preconditioned SGD converges to $\\epsilon$-accuracy in $O(\\log(1/\\epsilon))$ iterations, with a rapid linear convergence rate as if the ground truth were perfectly conditioned with $\\kappa=1$. In our numerical experiments, we observe a similar acceleration for\nill-conditioned matrix completion under the root mean square error (RMSE) loss, Euclidean distance matrix (EDM) completion under pairwise square loss, and collaborative filtering under the Bayesian Personalized Ranking (BPR) loss."}}
{"id": "hyR9oIq1hc", "cdate": 1640995200000, "mdate": 1681685487160, "content": {"title": "Overcoming the Convex Relaxation Barrier for Neural Network Verification via Nonconvex Low-Rank Semidefinite Relaxations", "abstract": "Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a \"convex relaxation barrier\" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) and branch-and-bound (BnB) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatically fewer variables comparable to much weaker LP methods. Despite nonconvexity, we show how off-the-shelf local optimization algorithms can be used to achieve and to certify global optimality in polynomial time. Our experiments find that the nonconvex relaxation almost completely closes the gap towards exact certification of adversarially trained models."}}
{"id": "FFJWwgO2mu", "cdate": 1640995200000, "mdate": 1681685487163, "content": {"title": "Accelerating SGD for Highly Ill-Conditioned Huge-Scale Online Matrix Completion", "abstract": "The matrix completion problem seeks to recover a $d\\times d$ ground truth matrix of low rank $r\\ll d$ from observations of its individual elements. Real-world matrix completion is often a huge-scale optimization problem, with $d$ so large that even the simplest full-dimension vector operations with $O(d)$ time complexity become prohibitively expensive. Stochastic gradient descent (SGD) is one of the few algorithms capable of solving matrix completion on a huge scale, and can also naturally handle streaming data over an evolving ground truth. Unfortunately, SGD experiences a dramatic slow-down when the underlying ground truth is ill-conditioned; it requires at least $O(\\kappa\\log(1/\\epsilon))$ iterations to get $\\epsilon$-close to ground truth matrix with condition number $\\kappa$. In this paper, we propose a preconditioned version of SGD that preserves all the favorable practical qualities of SGD for huge-scale online optimization while also making it agnostic to $\\kappa$. For a symmetric ground truth and the Root Mean Square Error (RMSE) loss, we prove that the preconditioned SGD converges to $\\epsilon$-accuracy in $O(\\log(1/\\epsilon))$ iterations, with a rapid linear convergence rate as if the ground truth were perfectly conditioned with $\\kappa=1$. In our experiments, we observe a similar acceleration for item-item collaborative filtering on the MovieLens25M dataset via a pair-wise ranking loss, with 100 million training pairs and 10 million testing pairs. [See supporting code at https://github.com/Hong-Ming/ScaledSGD.]"}}
{"id": "A714_-pcxK", "cdate": 1577836800000, "mdate": 1681685487131, "content": {"title": "Graph Learning and Augmentation Based Interpolation of Signal Strength for Location-Aware Communications", "abstract": "A graph learning and augmentation (GLA) technique is proposed herein to solve the received signal power interpolation problem, which is important for preemptive resource allocation in location-aware communications. A graph parameterization results in the proposed GLA interpolator having superior mean-squared error performance and lower computational complexity than the traditional Gaussian process method. Simulation results and analytical complexity analysis are used to prove the efficacy of the GLA interpolator."}}
