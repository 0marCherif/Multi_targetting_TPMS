{"id": "wcNk7H-uZYU", "cdate": 1681930066177, "mdate": 1681930066177, "content": {"title": "High-throughput Generative Inference of Large Language Models with a Single GPU", "abstract": "The high computational and memory requirements of large language model (LLM) inference traditionally make it feasible only with multiple high-end accelerators. In this paper, we study how to lower the requirements of LLM inference down to one commodity GPU and achieve practical performance. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. Through a linear programming optimizer, it searches for the best pattern to store and access the tensors, including weights, activations, and attention key/value cache. FlexGen further compresses both weights and KV cache to 4 bits with negligible accuracy loss. Compared with state-of-the-art offloading systems, FlexGen runs OPT-175B up to 100 faster on a single 16GB GPU and achieves a practical generation throughput of 1 token/s for the first time. FlexGen also comes with a pipeline parallelism runtime to allow super-linear scaling on decoding if more distributed GPUs are given."}}
{"id": "UHoGOaGjEq", "cdate": 1652737734274, "mdate": null, "content": {"title": "Decentralized Training of Foundation Models in Heterogeneous Environments", "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron and Deepspeed, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational \u201ctasklets\u201d in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8\u00d7 faster than prior state-of-the-art training systems."}}
{"id": "QDPonrGtl1", "cdate": 1652737731293, "mdate": null, "content": {"title": "Fine-tuning Language Models over Slow Networks using Activation Quantization with Guarantees", "abstract": "Communication compression is a crucial technique for modern distributed learning systems to alleviate their communication bottlenecks over slower networks. Despite recent intensive studies of gradient compression for data parallel-style training, compressing the activations for models trained with pipeline parallelism is still an open problem. In this paper, we propose AQ-SGD, a novel activation compression algorithm for communication-efficient pipeline parallelism training over slow networks. Different from previous efforts in activation compression, instead of compressing activation values directly, AQ-SGD compresses the changes of the activations. This allows us to show, to the best of our knowledge for the first time, that one can still achieve $O(1/\\sqrt{T})$ convergence rate for non-convex objectives under activation compression, without making assumptions on gradient unbiasedness that do not hold for deep learning models with non-linear activation functions. We then show that AQ-SGD can be optimized and implemented efficiently, without additional end-to-end runtime overhead. We evaluated AQ-SGD to fine-tune language models with up to 1.5 billion parameters, compressing activation to 2-4 bits. AQ-SGD provides up to $4.3\\times$ end-to-end speed-up in slower networks, without sacrificing model quality. Moreover, we also show that AQ-SGD can be combined with state-of-the-art gradient compression algorithms to enable end-to-end communication compression: All communications between machines, including model gradients, forward activations, and backward gradients are compressed into lower precision. This provides up to $4.9\\times$ end-to-end speed-up, without sacrificing model quality."}}
{"id": "lApYb5LDqVJ", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Feature Fusion Framework and Its Application to Automatic Seizure Detection", "abstract": "Automatic analysis of biomedical signals plays an important role in the auxiliary diagnosis of diseases. Traditional methods extract hand-crafted features by imitating doctors' experience, while recent methods focus on extracting deep features automatically by designing the architectures of deep neural networks (DNNs). Combining these two kinds of features can not only take advantage of doctors' experience but also mine the hidden information in the raw data. But directly combining these features by fully connected layers may cause complex optimization hyper-planes. To better integrate doctors' experience and deep features that doctors can hardly describe, we propose a feature fusion framework named hybrid plus framework (HPF) and apply this framework to seizure detection. HPF mainly consists of two parts: (1) the FET module, where hand-crafted features are extracted and transformed to sparse categorical features; (2) the enhanced DNN, which contains a carefully designed neural network structure with the input being original signals and sparse categorical features. Experiments on the dataset of CHB-MIT show that HPF outperforms the state-of-the-art methods. Further experiments indicate that HPF is very flexible as many of its modules can be replaced."}}
{"id": "OkRG2HqbCgz", "cdate": 1609459200000, "mdate": null, "content": {"title": "Distributed Numerical and Machine Learning Computations via Two-Phase Execution of Aggregated Join Trees", "abstract": ""}}
{"id": "DES-8ous07U", "cdate": 1609459200000, "mdate": null, "content": {"title": "Lachesis: Automated Partitioning for UDF-Centric Analytics", "abstract": ""}}
{"id": "0UJxXqOoFzP", "cdate": 1609459200000, "mdate": null, "content": {"title": "Tensor Relational Algebra for Distributed Machine Learning System Design", "abstract": ""}}
{"id": "lBT_S4tTnbQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Declarative Recursive Computation on an RDBMS: or, Why You Should Use a Database For Distributed Machine Learning", "abstract": "We explore the close relationship between the tensor-based computations performed during modern machine learning, and relational database computations. We consider how to make a very small set of changes to a modern RDBMS to make it suitable for distributed learning computations. Changes include adding better support for recursion, and optimization and execution of very large compute plans. We also show that there are key advantages to using an RDBMS as a machine learning platform. In particular, DBMSbased learning allows for trivial scaling to large data sets and especially large models, where different computational units operate on different parts of a model that may be too large to fit into RAM."}}
{"id": "_KxFnCmT9Go", "cdate": 1577836800000, "mdate": null, "content": {"title": "Lachesis: Automated Generation of Persistent Partitionings for Big Data Applications", "abstract": "Persistent partitioning is effective in avoiding expensive shuffling operations. However it remains a significant challenge to automate this process for Big Data analytics workloads that extensively use user defined functions (UDFs), where sub-computations are hard to be reused for partitionings compared to relational applications. In addition, functional dependency that is widely utilized for partitioning selection is often unavailable in the unstructured data that is ubiquitous in UDF-centric analytics. We propose the Lachesis system, which represents UDF-centric workloads as workflows of analyzable and reusable sub-computations. Lachesis further adopts a deep reinforcement learning model to infer which sub-computations should be used to partition the underlying data. This analysis is then applied to automatically optimize the storage of the data across applications to improve the performance and users' productivity."}}
{"id": "FqSfet1M5D8", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Federated Learning Framework for Healthcare IoT devices", "abstract": "The Internet of Things (IoT) revolution has shown potential to give rise to many medical applications with access to large volumes of healthcare data collected by IoT devices. However, the increasing demand for healthcare data privacy and security makes each IoT device an isolated island of data. Further, the limited computation and communication capacity of wearable healthcare devices restrict the application of vanilla federated learning. To this end, we propose an advanced federated learning framework to train deep neural networks, where the network is partitioned and allocated to IoT devices and a centralized server. Then most of the training computation is handled by the powerful server. The sparsification of activations and gradients significantly reduces the communication overhead. Empirical study have suggested that the proposed framework guarantees a low accuracy loss, while only requiring 0.2% of the synchronization traffic in vanilla federated learning."}}
