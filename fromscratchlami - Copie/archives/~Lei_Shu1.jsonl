{"id": "-TKGZg77NA", "cdate": 1672531200000, "mdate": 1691659209501, "content": {"title": "Adapting a Language Model While Preserving its General Knowledge", "abstract": "Domain-adaptive pre-training (or DA-training for short), also known as post-training, aims to train a pre-trained general-purpose language model (LM) using an unlabeled corpus of a particular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach."}}
{"id": "4cdxptfCCg", "cdate": 1652737688183, "mdate": null, "content": {"title": "Measuring and Reducing Model Update Regression in Structured Prediction for NLP", "abstract": "Recent advance in deep learning has led to rapid adoption of machine learning based NLP models in a wide range of applications. Despite the continuous gain in accuracy, backward compatibility is also an important aspect for industrial applications, yet it received little research attention. Backward compatibility requires that the new model does not regress on cases that were correctly handled by its predecessor. This work studies model update regression in structured prediction tasks. We choose syntactic dependency parsing and conversational semantic parsing as representative examples of structured prediction tasks in NLP. First, we measure and analyze model update regression in different model update settings. Next, we explore and benchmark existing techniques for reducing model update regression including model ensemble and knowledge distillation. We further propose a simple and effective method, Backward-Congruent Re-ranking (BCR), by taking into account the characteristics of structured output. Experiments show that BCR can better mitigate model update regression than model ensemble and knowledge distillation approaches."}}
{"id": "UshiGzocXZO", "cdate": 1649648815353, "mdate": null, "content": {"title": " Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally with the goal\nof achieving two main objectives: overcoming catastrophic forgetting (CF) and\nencouraging knowledge transfer (KT) across tasks. However, most existing techniques focus only on overcoming CF and have no mechanism to encourage KT,\nand thus do not do well in KT. Although several papers have tried to deal with\nboth CF and KT, our experiments show that they suffer from serious CF when\nthe tasks do not have much shared knowledge. Another observation is that most\ncurrent CL methods do not use pre-trained models, but it has been shown that such\nmodels can significantly improve the end task performance. For example, in natural\nlanguage processing, fine-tuning a BERT-like pre-trained language model is one of\nthe most effective approaches. However, for CL, this approach suffers from serious\nCF. An interesting question is how to make the best use of pre-trained models for\nCL. This paper proposes a novel model called CTR to solve these problems. Our\nexperimental results demonstrate the effectiveness of CTR."}}
{"id": "rRfeRRTWIk5", "cdate": 1644727765925, "mdate": 1644727765925, "content": {"title": "Zero-Shot Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated data for supervised training/fine-tuning. It is a big challenge to scale ABSA to a large number of new domains. This paper aims to train a unified model that can perform zero-shot ABSA without using any annotated data for a new domain. We propose a method called contrastive post-training on review Natural Language Inference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer. We evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect sentiment classification (ASC), to end-to-end aspect-based sentiment analysis (E2E ABSA), which show ABSA can be conducted without any human annotated ABSA data."}}
{"id": "vl5FjvnutLd", "cdate": 1640995200000, "mdate": 1681691998349, "content": {"title": "Adapting a Language Model While Preserving its General Knowledge", "abstract": ""}}
{"id": "k-1RGc6kBG", "cdate": 1640995200000, "mdate": 1681691998244, "content": {"title": "Continual Training of Language Models for Few-Shot Learning", "abstract": ""}}
{"id": "dJbHsnfll9", "cdate": 1640995200000, "mdate": 1668171758753, "content": {"title": "Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP", "abstract": ""}}
{"id": "Hv5JkzjoWJZ", "cdate": 1640995200000, "mdate": 1672931533105, "content": {"title": "Continual Training of Language Models for Few-Shot Learning", "abstract": ""}}
{"id": "HgP_S4nkmTz", "cdate": 1640995200000, "mdate": 1696343403225, "content": {"title": "Zero-Shot Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated data for supervised training/fine-tuning. It is a big challenge to scale ABSA to a large number of new domains. This paper aims to train a unified model that can perform zero-shot ABSA without using any annotated data for a new domain. We propose a method called contrastive post-training on review Natural Language Inference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer. We evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect sentiment classification (ASC), to end-to-end aspect-based sentiment analysis (E2E ABSA), which show ABSA can be conducted without any human annotated ABSA data."}}
{"id": "CRlZes7N5C", "cdate": 1640995200000, "mdate": 1696343403315, "content": {"title": "Open Set Recognition Via Augmentation-Based Similarity Learning", "abstract": "The primary assumption of conventional supervised learning or classification is that the test samples are drawn from the same distribution as the training samples, which is called closed set learni..."}}
