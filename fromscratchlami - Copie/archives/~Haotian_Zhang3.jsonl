{"id": "Baaqq7UIMZC", "cdate": 1667591374594, "mdate": null, "content": {"title": "Grounded Language-Image Pre-training", "abstract": "This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representations semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head."}}
{"id": "wiBEFdAvl8L", "cdate": 1652737360887, "mdate": null, "content": {"title": "GLIPv2: Unifying Localization and Vision-Language Understanding ", "abstract": "We present GLIPv2, a grounded VL understanding model, that serves both localization tasks (e.g., object detection, instance segmentation) and Vision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2 elegantly unifies localization pre-training and Vision-Language Pre-training (VLP) with three pre-training tasks: phrase grounding as a VL reformulation of the detection task, region-word contrastive learning as a novel region-word level contrastive learning task, and the masked language modeling. This unification not only simplifies the previous multi-stage VLP procedure but also achieves mutual benefits between localization and understanding tasks. Experimental results show that a single GLIPv2 model (all model weights are shared) achieves near SoTA performance on various localization and understanding tasks. The model also shows (1) strong zero-shot and few-shot adaption performance on open-vocabulary object detection tasks and (2) superior grounding capability on VL understanding tasks. "}}
{"id": "r1dQhgWNsf", "cdate": 1640995200000, "mdate": 1668073596086, "content": {"title": "GLIPv2: Unifying Localization and Vision-Language Understanding", "abstract": "We present GLIPv2, a grounded VL understanding model, that serves both localization tasks (e.g., object detection, instance segmentation) and Vision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2 elegantly unifies localization pre-training and Vision-Language Pre-training (VLP) with three pre-training tasks: phrase grounding as a VL reformulation of the detection task, region-word contrastive learning as a novel region-word level contrastive learning task, and the masked language modeling. This unification not only simplifies the previous multi-stage VLP procedure but also achieves mutual benefits between localization and understanding tasks. Experimental results show that a single GLIPv2 model (all model weights are shared) achieves near SoTA performance on various localization and understanding tasks. The model also shows (1) strong zero-shot and few-shot adaption performance on open-vocabulary object detection tasks and (2) superior grounding capability on VL understanding tasks. Code will be released at https://github.com/microsoft/GLIP."}}
{"id": "e0OHkcqlk5", "cdate": 1640995200000, "mdate": 1668073596152, "content": {"title": "DIOR: DIstill Observations to Representations for Multi-Object Tracking and Segmentation", "abstract": "Multi-object tracking (MOT) has long been a crucial topic in the field of autonomous driving and security monitoring. With the saturation of the bounding-box-based MOT algorithms in recent years, a new task to track objects with instance segmentation, called multi-object tracking and segmentation (MOTS), provides a finer level of scene understanding and introduces potential improvements in tracking accuracy. In this paper, we introduce a video-based MOTS framework, named DI still Observations to Representations (DIOR). A feature distiller is designed to extract and balance the comprehensive object representations: 1) the temporal distiller aggregates context information for consistency of features and smoothness of prediction longitudinally; 2) the spatial distiller on the target of interest within each bounding box removes ambiguity and irrelevance of background in the learned features. The subsequent tracking steps start with Hungarian matching based on feature similarity and masks continuity, which is efficient and straightforward. In addition, we propose short-term retrieval (STR) and long-term re-identification (re-ID) modules to avoid missing associations due to failures in detection or possible occlusion. Our method achieves state-of-the-art performance in both MOTS20 and KITTI-MOTS benchmarks."}}
{"id": "K8zVzsAHTWT", "cdate": 1640995200000, "mdate": 1668073596090, "content": {"title": "TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers", "abstract": "In this paper, we present TransMVSNet, based on our exploration of feature matching in multi-view stereo (MVS). We analogize MVS back to its nature of a feature matching task and therefore propose a powerful Feature Matching Transformer (FMT) to leverage intra- (self-) and inter-(cross-) attention to aggregate long-range context information within and across images. To facilitate a better adaptation of the FMT, we leverage an Adaptive Receptive Field (ARF) module to ensure a smooth transit in scopes of features and bridge different stages with a feature pathway to pass transformed features and gradients across different scales. In addition, we apply pair-wise feature correlation to measure similarity between features, and adopt ambiguity-reducing focal loss to strengthen the supervision. To the best of our knowledge, TransMVSNet is the first attempt to leverage Transformer into the task of MVS. As a result, our method achieves state-of-the-art performance on DTU dataset, Tanks and Temples benchmark, and BlendedMVS dataset. Code is available at https://github.com/MegviiRobot/TransMVSNet."}}
{"id": "Hn9JpwbnWsC", "cdate": 1609459200000, "mdate": 1668073596068, "content": {"title": "Monocular 3D Localization of Vehicles in Road Scenes", "abstract": "Sensing and perception systems for autonomous driving vehicles in road scenes are composed of three crucial components: 3D-based object detection, tracking, and localization. While all three components are important, most relevant papers tend to only focus on one single component. We propose a monocular vision-based framework for 3D-based detection, tracking, and localization by effectively integrating all three tasks in a complementary manner. Our system contains an RCNN-based Localization Network (LOCNet), which works in concert with fitness evaluation score (FES) based single-frame optimization, to get more accurate and refined 3D vehicle localization. To better utilize the temporal information, we further use a multi-frame optimization technique, taking advantage of camera ego-motion and a 3D TrackletNet Tracker (3D TNT), to improve both accuracy and consistency in our 3D localization results. Our system outperforms state-of-the-art image-based solutions in diverse scenarios and is even comparable with LiDAR-based methods."}}
{"id": "5Hm0jOp9wO4", "cdate": 1609459200000, "mdate": 1668073596055, "content": {"title": "ROD2021 Challenge: A Summary for Radar Object Detection Challenge for Autonomous Driving Applications", "abstract": "The Radar Object Detection 2021 (ROD2021) Challenge, held in the ACM International Conference on Multimedia Retrieval (ICMR) 2021, has been introduced to detect and classify objects purely using an FMCW radar for autonomous driving applications. As a robust sensor to all-weather conditions, radar has rich information hidden in the radio frequencies, which can potentially achieve object detection and classification. This insight will provide a new object perception solution for an autonomous vehicle even in adverse driving scenarios. The ROD2021 Challenge is the first public benchmark focusing on this topic, which attracts great attention and participation. There are more than 260 participants among 37 teams from more than 10 countries with different academic and industrial affiliations, contributing about 300 submissions in the first phase and 400 submissions in the second phase. The final performance is evaluated by average precision (AP). Results add strong value and a better understanding of the radar object detection task for the autonomous vehicle community."}}
{"id": "xvgBeJPJ7h6", "cdate": 1577836800000, "mdate": 1668073596081, "content": {"title": "Bundle Adjustment for Monocular Visual Odometry Based on Detections of Traffic Signs", "abstract": "The technology for simultaneous localization and mapping (SLAM) has been well investigated with the rising interest in autonomous driving. Visual odometry (VO) is a variation of SLAM without global consistency for estimating the position and orientation of the moving object through analyzing the image sequences captured by associated cameras. However, in the real-world applications, we are inevitably to experience drift error problem in the VO process due to the frame-by-frame pose estimation. The drift can be more severe for monocular VO compared with stereo matching. By jointly refining the camera poses via several local keyframes and the coordinate of 3D map points triangulated from extracted features, bundle adjustment (BA) can mitigate the drift error problem only to some extent. To further improve the performance, we introduce a traffic sign feature-based joint BA module to eliminate and relieve the incrementally accumulated pose errors. The continuously extracted traffic sign feature with standard size and planar information will provide powerful additional constraints for improving the VO estimation accuracy through BA. Our framework can collaborate well with existing VO systems, e.g., ORB-SLAM2, and the traffic sign feature can also be replaced with feature extracted from other size-known planar objects. Experimental results by applying our traffic sign feature-based BA module show an improved vehicular localization accuracy compared with the state-of-the-art baseline VO method."}}
{"id": "dqiS1VFvL4N", "cdate": 1577836800000, "mdate": 1668073596036, "content": {"title": "IA-MOT: Instance-Aware Multi-Object Tracking with Motion Consistency", "abstract": "Multiple object tracking (MOT) is a crucial task in computer vision society. However, most tracking-by-detection MOT methods, with available detected bounding boxes, cannot effectively handle static, slow-moving and fast-moving camera scenarios simultaneously due to ego-motion and frequent occlusion. In this work, we propose a novel tracking framework, called \"instance-aware MOT\" (IA-MOT), that can track multiple objects in either static or moving cameras by jointly considering the instance-level features and object motions. First, robust appearance features are extracted from a variant of Mask R-CNN detector with an additional embedding head, by sending the given detections as the region proposals. Meanwhile, the spatial attention, which focuses on the foreground within the bounding boxes, is generated from the given instance masks and applied to the extracted embedding features. In the tracking stage, object instance masks are aligned by feature similarity and motion consistency using the Hungarian association algorithm. Moreover, object re-identification (ReID) is incorporated to recover ID switches caused by long-term occlusion or missing detection. Overall, when evaluated on the MOTS20 and KITTI-MOTS dataset, our proposed method won the first place in Track 3 of the BMTT Challenge in CVPR2020 workshops."}}
{"id": "wU5L5XbMuK", "cdate": 1546300800000, "mdate": 1668073596160, "content": {"title": "Bundle Adjustment for Monocular Visual Odometry Based on Detected Traffic Sign Features", "abstract": "Monocular visual odometry (VO), which is a subset of simultaneous localization and mapping (SLAM) used to determine the position and orientation of a moving object by analyzing the associated monocular camera image sequences, is a critical part in the vision system of autonomous driving. However, based on the frame-by-frame pose estimation, drift error can be incrementally accumulated. Bundle adjustment (BA) is thus introduced to deal with the error-drift problem through correlating several image frames together to optimize camera poses and extracted 3D map points simultaneously. In this paper, we propose a joint BA framework which takes into account additional constraints from the detected road traffic signs. This framework can be effectively integrated into existing VO systems, as evidenced by the improved vehicular localization accuracy in experimental performance when compared with the state-of-the-art baseline VO method."}}
