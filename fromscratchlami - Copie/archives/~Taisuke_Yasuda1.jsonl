{"id": "TTLLGx3eet", "cdate": 1663850198166, "mdate": null, "content": {"title": "Sequential Attention for Feature Selection", "abstract": "Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, and thus inherits all of its provable guarantees. Our theoretical and empirical analyses offer new explanations towards the effectiveness of attention and its connections to overparameterization, which may be of independent interest."}}
{"id": "Vij8ny0h8M", "cdate": 1640995200000, "mdate": 1674066419691, "content": {"title": "Active Linear Regression for \u2113p Norms and Beyond", "abstract": "We study active sampling algorithms for linear regression, which aim to query only a small number of entries of a target vector and output a near minimizer to the objective function. For $\\ell_{p}$ norm regression for any $ 0\\lt p \\lt \\infty$, we give an algorithm based on Lewis weight sampling which outputs $\\mathrm{a}(1+\\epsilon)$-approximate solution using just $\\tilde{O}(d/\\epsilon^{2})$ queries to b for $p\\in(0,1)$, $\\tilde{O}$ $(d/\\epsilon)$ queries for $p\\in(1,2)$, and $\\tilde{O}$ $(d^{p/2}/\\epsilon^{p})$ queries for $p\\in(2,\\ \\infty)$. For $p\\in(0,2)$, our bounds are optimal up to logarithmic factors, thus settling the query complexity for this range of p. For $p\\in(2,\\ \\infty)$, our dependence on d is optimal, while our dependence on $\\epsilon$ is off by at most a single $\\epsilon$ factor, up to logarithmic factors. Our result resolves an open question of Chen and Derezi\u0144ski, who gave near optimal bounds for the $\\ell_{1}$ norm, but required at least $d^{2}/\\epsilon^{2}$ samples for $\\ell_{p}$ regression with $p\\in(1,2)$, and gave no bounds for $p\\in(2,\\ \\infty)$ or $p\\in(0,1)$. We also provide the first total sensitivity upper bound for loss functions with at most degree p polynomial growth. This improves a recent result of Tukan, Maalouf, and Feldman. By combining this with our techniques for $\\ell_{p}$ regression, we obtain the first active regression algorithms for such loss functions, including the important cases of the Tukey and Huber losses. This answers another question of Chen and Derezi\u0144ski. Our sensitivity bounds also give improvements to a variety of previous results using sensitivity sampling, including Orlicz norm subspace embeddings, robust subspace approximation, and dimension reduction for smoothed p-norms. Finally, our active sampling results give the first sublinear time algorithms for Kronecker product regression under every $\\ell_{p}$ norm. Previous results required reading the entire b vector in the kernel feature space. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Extended abstract; full version available at https://arxiv.org/abs/2111.04888."}}
{"id": "Hs7kllrwqgT", "cdate": 1640995200000, "mdate": 1674066419691, "content": {"title": "Improved Algorithms for Low Rank Approximation from Sparsity", "abstract": "We overcome two major bottlenecks in the study of low rank approximation by assuming the low rank factors themselves are sparse. Specifically, (1) for low rank approximation with spectral norm error, we show how to improve the best known running time to running time plus low order terms depending on the sparsity of the low rank factors, and (2) for streaming algorithms for Frobenius norm error, we show how to bypass the known \u03a9(nk/\u220a) memory lower bound and obtain an sk(log n)/poly(\u220a) memory bound, where s is the number of non-zeros of each low rank factor. Although this algorithm runs in exponential time, as it must under standard complexity-theoretic assumptions, we also present polynomial time algorithms using poly(s, k, log n, \u220a\u20131) memory that output rank k approximations supported on an O(sk/\u220a) \u00d7 O(sk/\u220a) submatrix. Both the prior running time and the nk/\u220a memory for these problems were long-standing barriers; our results give a natural way of overcoming them assuming sparsity of the low rank factors."}}
{"id": "5OLnxTRUzED", "cdate": 1640995200000, "mdate": 1674066419685, "content": {"title": "High-Dimensional Geometric Streaming in Polynomial Space", "abstract": "Many existing algorithms for streaming geometric data analysis have been plagued by exponential dependencies in the space complexity, which are undesirable for processing high-dimensional data sets, i.e., large d. In particular, once $d\\geq\\log n$, there are no known non-trivial streaming algorithms for problems such as maintaining convex hulls and L\u00f6wner-John ellipsoids of n points, despite a long line of work in high-dimensional streaming computational geometry since [2]. We simultaneously improve all of these results to poly $(d,\\ \\log n)$ bits of space by trading off with a poly $(d,\\ \\log n)$ factor distortion. We achieve these results in a unified manner, by designing the first streaming algorithm for maintaining a coreset for $\\ell_{\\infty}$ subspace embeddings with poly $(d,\\ \\log n)$ space and poly $(d,\\ \\log n)$ distortion. Our algorithm also gives similar guarantees in the online coreset model. Along the way, we sharpen known results for online numerical linear algebra by replacing a $\\log$ condition number dependence with a $\\log n$ dependence, answering an open question of [13]. Our techniques provide a novel connection between leverage scores, a fundamental object in numerical linear algebra, and computational geometry. For $\\ell_{p}$ subspace embeddings, our improvements in online numerical linear algebra yield nearly optimal tradeoffs between space and distortion for one-pass streaming algorithms. For instance, we obtain a deterministic coreset using $o(d^{2}\\log n)$ space and $o((d\\log n)^{\\frac{1}{2}-\\frac{1}{p}})$ distortion for $p\\gt 2$, whereas previous deterministic algorithms incurred a poly (n) factor in the space or the distortion [26]. Our techniques have implications also in the offline setting, where we give optimal trade-offs between the space complexity and distortion of a subspace sketch data structure, which preprocesses an $n\\times d$ matrix A and outputs $\\Vert \\mathrm{A}\\mathrm{x}\\Vert_{p}$ up to a poly (d) factor distortion for any x. To do this we give an elementary proof of a \u201cchange of density\u201d theorem of [42] and make it algorithmic. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Extended abstract; full version available at https://arxiv.org/abs/ 2204.03790."}}
{"id": "-SUsbLM-9NA", "cdate": 1609459200000, "mdate": 1674066419747, "content": {"title": "Exponentially Improved Dimensionality Reduction for l1: Subspace Embeddings and Independence Testing", "abstract": "Despite many applications, dimensionality reduction in the $\\ell_1$-norm is much less understood than in the Euclidean norm. We give two new oblivious dimensionality reduction techniques for the $\\..."}}
{"id": "UPDb6_WDKm9", "cdate": 1577836800000, "mdate": null, "content": {"title": "Graph Spanners in the Message-Passing Model", "abstract": "Graph spanners are sparse subgraphs which approximately preserve all pairwise shortest-path distances in an input graph. The notion of approximation can be additive, multiplicative, or both, and many variants of this problem have been extensively studied. We study the problem of computing a graph spanner when the edges of the input graph are distributed across two or more sites in an arbitrary, possibly worst-case partition, and the goal is for the sites to minimize the communication used to output a spanner. We assume the message-passing model of communication, for which there is a point-to-point link between all pairs of sites as well as a coordinator who is responsible for producing the output. We stress that the subset of edges that each site has is not related to the network topology, which is fixed to be point-to-point. While this model has been extensively studied for related problems such as graph connectivity, it has not been systematically studied for graph spanners. We present the first tradeoffs for total communication versus the quality of the spanners computed, for two or more sites, as well as for additive and multiplicative notions of distortion. We show separations in the communication complexity when edges are allowed to occur on multiple sites, versus when each edge occurs on at most one site. We obtain nearly tight bounds (up to polylog factors) for the communication of additive 2-spanners in both the with and without duplication models, multiplicative (2k-1)-spanners in the with duplication model, and multiplicative 3 and 5-spanners in the without duplication model. Our lower bound for multiplicative 3-spanners employs biregular bipartite graphs rather than the usual Erd\u0151s girth conjecture graphs and may be of wider interest."}}
{"id": "rJZ4mnW_-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel $k$-means Clustering", "abstract": "Kernel methods generalize machine learning algorithms that only depend on the pairwise inner products of the dataset by replacing inner products with kernel evaluations, a function that passes inpu..."}}
{"id": "hKfjL3ysw9p", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Query Complexity of Mastermind with lp Distances", "abstract": "Consider a variant of the Mastermind game in which queries are l_p distances, rather than the usual Hamming distance. That is, a codemaker chooses a hidden vector y in {-k,-k+1,...,k-1,k}^n and answers to queries of the form ||y-x||_p where x in {-k,-k+1,...,k-1,k}^n. The goal is to minimize the number of queries made in order to correctly guess y. In this work, we show an upper bound of O(min{n,(n log k)/(log n)}) queries for any real 1<=p<infty and O(n) queries for p=infty. To prove this result, we in fact develop a nonadaptive polynomial time algorithm that works for a natural class of separable distance measures, i.e., coordinate-wise sums of functions of the absolute value. We also show matching lower bounds up to constant factors, even for adaptive algorithms for the approximation version of the problem, in which the problem is to output y' such that ||y'-y||_p <= R for any R <= k^{1-epsilon}n^{1/p} for constant epsilon>0. Thus, essentially any approximation of this problem is as hard as finding the hidden vector exactly, up to constant factors. Finally, we show that for the noisy version of the problem, i.e., the setting when the codemaker answers queries with any q = (1 +/- epsilon)||y-x||_p, there is no query efficient algorithm."}}
