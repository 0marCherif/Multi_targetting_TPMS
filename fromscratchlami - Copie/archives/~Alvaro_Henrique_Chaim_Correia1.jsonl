{"id": "r9tPMnquvWf", "cdate": 1701859428635, "mdate": 1701859428635, "content": {"title": "Continuous Mixtures of Tractable Probabilistic Models", "abstract": "Probabilistic models based on continuous latent spaces, such as variational autoencoders, can be understood as uncountable mixture models where components depend continuously on the latent code. They have proven to be expressive tools for generative and probabilistic modelling, but are at odds with tractable probabilistic inference, that is, computing marginals and conditionals of the represented probability distribution. Meanwhile, tractable probabilistic models such as probabilistic circuits (PCs) can be understood as hierarchical discrete mixture models, and thus are capable of performing exact inference efficiently but often show subpar performance in comparison to continuous latent-space models. In this paper, we investigate a hybrid approach, namely continuous mixtures of tractable models with a small latent dimension. While these models are analytically intractable, they are well amenable to numerical integration schemes based on a finite set of integration points. With a large enough number of integration points the approximation becomes de-facto exact. Moreover, for a finite set of integration points, the integration method effectively compiles the continuous mixture into a standard PC. In experiments, we show that this simple scheme proves remarkably effective, as PCs learnt this way set new state of the art for tractable models on many standard density estimation benchmarks."}}
{"id": "7juRi_WxZCH", "cdate": 1640995200000, "mdate": 1650034965584, "content": {"title": "Neural Simulated Annealing", "abstract": "Simulated annealing (SA) is a stochastic global optimisation technique applicable to a wide range of discrete and continuous variable problems. Despite its simplicity, the development of an effective SA optimiser for a given problem hinges on a handful of carefully handpicked components; namely, neighbour proposal distribution and temperature annealing schedule. In this work, we view SA from a reinforcement learning perspective and frame the proposal distribution as a policy, which can be optimised for higher solution quality given a fixed computational budget. We demonstrate that this Neural SA with such a learnt proposal distribution, parametrised by small equivariant neural networks, outperforms SA baselines on a number of problems: Rosenbrock's function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. We also show that Neural SA scales well to large problems - generalising to significantly larger problems than the ones seen during training - while achieving comparable performance to popular off-the-shelf solvers and other machine learning methods in terms of solution quality and wall-clock time."}}
{"id": "bHqI0DvSIId", "cdate": 1632875540096, "mdate": null, "content": {"title": "Neural Simulated Annealing", "abstract": "Simulated annealing (SA) is a stochastic global optimisation technique applicable to a wide range of discrete and continuous variable problems. Despite its simplicity, the development of an effective SA optimiser for a given problem hinges on a handful of carefully handpicked components; namely, neighbour proposal distribution and temperature annealing schedule. In this work, we view SA from a reinforcement learning perspective and frame the proposal distribution as a policy, which can be optimised for higher solution quality given a fixed computational budget. We demonstrate that this Neural SA with such a learnt proposal distribution outperforms SA baselines with hand-selected parameters on a number of problems: Rosenbrock's function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. We also show that Neural SA scales well to large problems while again outperforming popular off-the-shelf solvers in terms of solution quality and wall clock time."}}
{"id": "v2W18nb--4P", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Pruning for Score-Based Bayesian Network Structure Learning", "abstract": "Many algorithms for score-based Bayesian network structure learning (BNSL), in particular exact ones, take as input a collection of potentially optimal parent sets for each variable in the data. Co..."}}
{"id": "uVxOWQ69JT", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Robust Classification with Deep Generative Forests", "abstract": "Decision Trees and Random Forests are among the most widely used machine learning models, and often achieve state-of-the-art performance in tabular, domain-agnostic datasets. Nonetheless, being primarily discriminative models they lack principled methods to manipulate the uncertainty of predictions. In this paper, we exploit Generative Forests (GeFs), a recent class of deep probabilistic models that addresses these issues by extending Random Forests to generative models representing the full joint distribution over the feature space. We demonstrate that GeFs are uncertainty-aware classifiers, capable of measuring the robustness of each prediction as well as detecting out-of-distribution samples."}}
{"id": "cANoC-lViR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Joints in Random Forests", "abstract": "Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative learners and tools of central importance to the everyday machine learning practitioner and data scientist. Due to their discriminative nature, however, they lack principled methods to process inputs with missing features or to detect outliers, which requires pairing them with imputation techniques or a separate generative model. In this paper, we demonstrate that DTs and RFs can naturally be interpreted as generative models, by drawing a connection to Probabilistic Circuits, a prominent class of tractable probabilistic models. This reinterpretation equips them with a full joint distribution over the feature space and leads to Generative Decision Trees (GeDTs) and Generative Forests (GeFs), a family of novel hybrid generative-discriminative models. This family of models retains the overall characteristics of DTs and RFs while additionally being able to handle missing features by means of marginalisation. Under certain assumptions, frequently made for Bayes consistency results, we show that consistency in GeDTs and GeFs extend to any pattern of missing input features, if missing at random. Empirically, we show that our models often outperform common routines to treat missing data, such as K-nearest neighbour imputation, and moreover, that our models can naturally detect outliers by monitoring the marginal probability of input features."}}
{"id": "vlqTbDTVTN", "cdate": 1546300800000, "mdate": null, "content": {"title": "Towards Scalable and Robust Sum-Product Networks", "abstract": "Sum-Product Networks (SPNs) and their credal counterparts are machine learning models that combine good representational power with tractable inference. Yet they often have thousands of nodes which result in high processing times. We propose the addition of caches to the SPN nodes and show how this memoisation technique reduces inference times in a range of experiments. Moreover, we introduce class-selective SPNs, an architecture that is suited for classification tasks and enables efficient robustness computation in Credal SPNs. We also illustrate how robustness estimates relate to reliability through the accuracy of the model, and how one can explore robustness in ensemble modelling."}}
{"id": "irVuWaMzvmg", "cdate": 1546300800000, "mdate": null, "content": {"title": "An Experimental Study of Prior Dependence in Bayesian Network Structure Learning", "abstract": "The Bayesian Dirichlet equivalent uniform (BDeu) function is a popular score to evaluate the goodness of a Bayesian network structure given complete categorical data. Despite its interesting proper..."}}
{"id": "B8yNXZTHEqi", "cdate": 1546300800000, "mdate": 1650034965584, "content": {"title": "Human-in-the-Loop Feature Selection", "abstract": "Feature selection is a crucial step in the conception of Machine Learning models, which is often performed via datadriven approaches that overlook the possibility of tapping into the human decision-making of the model\u2019s designers and users. We present a human-in-the-loop framework that interacts with domain experts by collecting their feedback regarding the variables (of few samples) they evaluate as the most relevant for the task at hand. Such information can be modeled via Reinforcement Learning to derive a per-example feature selection method that tries to minimize the model\u2019s loss function by focusing on the most pertinent variables from a human perspective. We report results on a proof-of-concept image classification dataset and on a real-world risk classification task in which the model successfully incorporated feedback from experts to improve its accuracy."}}
{"id": "nkiBsa_2vbE", "cdate": 1514764800000, "mdate": null, "content": {"title": "Interpreting Embedding Models of Knowledge Bases: A Pedagogical Approach", "abstract": "Knowledge bases are employed in a variety of applications from natural language processing to semantic web search; alas, in practice their usefulness is hurt by their incompleteness. Embedding models attain state-of-the-art accuracy in knowledge base completion, but their predictions are notoriously hard to interpret. In this paper, we adapt \"pedagogical approaches\" (from the literature on neural networks) so as to interpret embedding models by extracting weighted Horn rules from them. We show how pedagogical approaches have to be adapted to take upon the large-scale relational aspects of knowledge bases and show experimentally their strengths and weaknesses."}}
