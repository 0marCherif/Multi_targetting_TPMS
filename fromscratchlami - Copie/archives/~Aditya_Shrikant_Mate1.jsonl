{"id": "Nmfuvm6yTVI", "cdate": 1665069646426, "mdate": null, "content": {"title": "Case Study: Applying Decision Focused Learning in the Real World", "abstract": "Many real world optimization problems with underlying unknown model parameters are solved using the predict-then-optimize framework. In particular, a model is learnt to first predict the parameters of the optimization problem, which is subsequently solved using an optimization algorithm. However, this approach maximises for the predictive accuracy rather than the quality of the final solution. \nDecision Focused Learning (DFL) solves this objective mismatch by integrating the optimization problem in the learning pipeline. Previous works have only shown the applicability of DFL in simulation settings. \nIn our work, we consider the optimization problem of scheduling limited live service calls in Maternal and Child Health Awareness Programs and model it using Restless Multi-Armed Bandits (RMAB).\nIn collaboration with an NGO, we conduct a large-scale field study consisting of 9000 beneficiaries for 6 weeks and track key engagement metrics in a mobile health awareness program. To the best of our knowledge this is the first real world study involving Decision Focused Learning. We demonstrate that beneficiaries in the DFL group experience statistically significant reductions in cumulative engagement drop, while those in the Predict-then-Optimize group do not. This establishes the practicality of use of decision focused learning for real world problems. We also demonstrate that DFL learns a better decision boundary between the RMAB actions, and strategically predicts parameters which contribute most to the final decision outcome. "}}
{"id": "rbg_o51Tl9", "cdate": 1646226079594, "mdate": null, "content": {"title": "Solving Structured Hierarchical Games Using Differential Backward Induction", "abstract": "From large-scale organizations to decentralized political systems, hierarchical strategic decision making is commonplace. We introduce a novel class of structured hierarchical games (SHGs) that formally capture such hierarchical strategic interactions. In an SHG, each player is a node in a tree, and strategic choices of players are sequenced from root to leaves, with root moving first, followed by its children, then followed by their children, and so on until the leaves. A player\u2019s utility in an SHG depends on its own decision, and on the choices of its parent and all the tree leaves. SHGs thus generalize simultaneous-move games, as well as Stackelberg games with many followers. We leverage the structure of both the sequence of player moves as well as payoff dependence to develop a novel gradient-based backpropagation-style algorithm, which we call Differential Backward Induction (DBI), for approximating equilibria of SHGs. We provide a sufficient condition for convergence of DBI and demonstrate its efficacy in finding approximate equilibrium solutions to several SHG models of hierarchical policy-making problems."}}
{"id": "BcLqJUIs5x5", "cdate": 1646077511461, "mdate": null, "content": {"title": "Solving Structured Hierarchical Games Using Differential Backward Induction", "abstract": "From large-scale organizations to decentralized political systems, hierarchical strategic decision making is commonplace. We introduce a novel class of \\emph{structured hierarchical games (SHGs)} that formally capture such hierarchical strategic interactions. In an SHG, each player is a node in a tree, and strategic choices of players are sequenced from root to leaves, with root moving first, followed by its children, then followed by their children, and so on until the leaves. A player's utility in an SHG depends on its own decision, and on the choices of its parent and \\emph{all} the tree leaves. SHGs thus generalize simultaneous-move games, as well as Stackelberg games with many followers.  We leverage the structure of both the sequence of player moves as well as payoff dependence to develop a gradient-based back propagation-style algorithm, which we call \\emph{Differential Backward Induction (DBI)}, for approximating equilibria of SHGs. We provide a sufficient condition for convergence of DBI and demonstrate its efficacy in finding approximate equilibrium solutions to several SHG models of hierarchical policy-making problems."}}
{"id": "Dgs2_rn29o", "cdate": 1620616112914, "mdate": null, "content": {"title": "Risk-Aware Interventions in Public Health: Planning with Restless Multi-Armed Bandits", "abstract": "Community Health Workers (CHWs) form an important component\nof health-care systems globally, especially in low-resource settings.\nCHWs are often tasked with monitoring the health of and intervening on their patient cohort. Previous work has developed several\nclasses of Restless Multi-Armed Bandits (RMABs) that are computationally tractable and indexable, a condition that guarantees\nasymptotic optimality, for solving such health monitoring and intervention problems (HMIPs). However, existing solutions to HMIPs\nfail to account for risk-sensitivity considerations of CHWs in the\nplanning stage and may run the danger of ignoring some patients\ncompletely because they are deemed less valuable to intervene on.\nAdditionally, these also rely on patients reporting their state of adherence accurately when intervened upon. Towards tackling these\nissues, our contributions in this paper are as follows: (1) We develop\nan RMAB solution to HMIPs that allows for reward functions that\nare monotone increasing, rather than linear, in the belief state and\nalso supports a wider class of observations. (2) We prove theoretical\nguarantees on the asymptotic optimality of our algorithm for any\narbitrary reward function. Additionally, we show that for the specific reward function considered in previous work, our theoretical\nconditions are stronger than the state-of-the-art guarantees. (3) We\nshow the applicability of these new results for addressing the three\nissues pertaining to: risk-sensitive planning, equitable allocation\nand reliance on perfect observations as highlighted above. We evaluate these techniques on both simulated as well as real data from\na prevalent CHW task of monitoring adherence of tuberculosis\npatients to their prescribed medication in Mumbai, India and show\nimproved performance over the state-of-the-art. Full paper and code\nis available at: https://github.com/AdityaMate/risk-aware-bandits"}}
{"id": "HJx2G1iNKv", "cdate": 1620613884947, "mdate": null, "content": {"title": "Collapsing Bandits and Their Applications to Public Health Interventions", "abstract": "We propose and study Collpasing Bandits, a new restless multi-armed bandit (RMAB) setting in which each arm follows a binary-state Markovian process with a special structure: when an arm is played, the state is fully observed, thus \"collapsing\" any uncertainty, but when an arm is passive, no observation is made, thus allowing uncertainty to evolve. The goal is to keep as many arms in the \"good\" state as possible by planning a limited budget of actions per round. Such Collapsing Bandits are natural models for many healthcare domains in which workers must simultaneously monitor patients and deliver interventions in a way that maximizes the health of their patient cohort. Our main contributions are as follows: (i) Building on the Whittle index technique for RMABs, we derive conditions under which the Collapsing Bandits problem is indexable. Our derivation hinges on novel conditions that characterize when the optimal policies may take the form of either \"forward\" or \"reverse\" threshold policies. (ii) We exploit the optimality of threshold policies to build fast algorithms for computing the Whittle index, including a closed-form. (iii) We evaluate our algorithm on several data distributions including data from a real-world healthcare task in which a worker must monitor and deliver interventions to maximize their patients' adherence to tuberculosis medication. Our algorithm achieves a 3-order-of-magnitude speedup compared to state-of-the-art RMAB techniques while achieving similar performance."}}
