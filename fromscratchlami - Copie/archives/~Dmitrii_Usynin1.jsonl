{"id": "xkRyfzWdYq5", "cdate": 1640995200000, "mdate": 1682329705743, "content": {"title": "Beyond Gradients: Exploiting Adversarial Priors in Model Inversion Attacks", "abstract": "Collaborative machine learning settings like federated learning can be susceptible to adversarial interference and attacks. One class of such attacks is termed model inversion attacks, characterised by the adversary reverse-engineering the model to extract representations and thus disclose the training data. Prior implementations of this attack typically only rely on the captured data (i.e. the shared gradients) and do not exploit the data the adversary themselves control as part of the training consortium. In this work, we propose a novel model inversion framework that builds on the foundations of gradient-based model inversion attacks, but additionally relies on matching the features and the style of the reconstructed image to data that is controlled by an adversary. Our technique outperforms existing gradient-based approaches both qualitatively and quantitatively, while still maintaining the same honest-but-curious threat model, allowing the adversary to obtain enhanced reconstructions while remaining concealed."}}
{"id": "ic8k67RpMq", "cdate": 1640995200000, "mdate": 1682329705807, "content": {"title": "Can collaborative learning be private, robust and scalable?", "abstract": "In federated learning for medical image analysis, the safety of the learning protocol is paramount. Such settings can often be compromised by adversaries that target either the private data used by the federation or the integrity of the model itself. This requires the medical imaging community to develop mechanisms to train collaborative models that are private and robust against adversarial data. In response to these challenges, we propose a practical open-source framework to study the effectiveness of combining differential privacy, model compression and adversarial training to improve the robustness of models against adversarial samples under train- and inference-time attacks. Using our framework, we achieve competitive model performance, a significant reduction in model's size and an improved empirical adversarial robustness without a severe performance degradation, critical in medical image analysis."}}
{"id": "hKWKyxrT0I6", "cdate": 1640995200000, "mdate": 1682329705768, "content": {"title": "Zen and the art of model adaptation: Low-utility-cost attack mitigations in collaborative machine learning", "abstract": ""}}
{"id": "dj8CpG9Xvb-", "cdate": 1640995200000, "mdate": 1681725024482, "content": {"title": "Can Collaborative Learning Be Private, Robust and Scalable?", "abstract": "In federated learning for medical image analysis, the safety of the learning protocol is paramount. Such settings can often be compromised by adversaries that target either the private data used by the federation or the integrity of the model itself. This requires the medical imaging community to develop mechanisms to train collaborative models that are private and robust against adversarial data. In response to these challenges, we propose a practical open-source framework to study the effectiveness of combining differential privacy, model compression and adversarial training to improve the robustness of models against adversarial samples under train- and inference-time attacks. Using our framework, we achieve competitive model performance, a significant reduction in model\u2019s size and an improved empirical adversarial robustness without a severe performance degradation, critical in medical image analysis."}}
{"id": "W9sTTeixvme", "cdate": 1640995200000, "mdate": 1682329705775, "content": {"title": "Membership Inference Attacks Against Semantic Segmentation Models", "abstract": "Membership inference attacks aim to infer whether a data record has been used to train a target model by observing its predictions. In sensitive domains such as healthcare, this can constitute a severe privacy violation. In this work we attempt to address the existing knowledge gap by conducting an exhaustive study of membership inference attacks and defences in the domain of semantic image segmentation. Our findings indicate that for certain threat models, these learning settings can be considerably more vulnerable than the previously considered classification settings. We additionally investigate a threat model where a dishonest adversary can perform model poisoning to aid their inference and evaluate the effects that these adaptations have on the success of membership inference attacks. We quantitatively evaluate the attacks on a number of popular model architectures across a variety of semantic segmentation tasks, demonstrating that membership inference attacks in this domain can achieve a high success rate and defending against them may result in unfavourable privacy-utility trade-offs or increased computational costs."}}
{"id": "PbpNRsUhOb", "cdate": 1640995200000, "mdate": 1681725024601, "content": {"title": "Differentially Private Graph Classification with GNNs", "abstract": "Graph Neural Networks (GNNs) have established themselves as the state-of-the-art models for many machine learning applications such as the analysis of social networks, protein interactions and molecules. Several among these datasets contain privacy-sensitive data. Machine learning with differential privacy is a promising technique to allow deriving insight from sensitive data while offering formal guarantees of privacy protection. However, the differentially private training of GNNs has so far remained under-explored due to the challenges presented by the intrinsic structural connectivity of graphs. In this work, we introduce differential privacy for graph-level classification, one of the key applications of machine learning on graphs. Our method is applicable to deep learning on multi-graph datasets and relies on differentially private stochastic gradient descent (DP-SGD). We show results on a variety of synthetic and public datasets and evaluate the impact of different GNN architectures and training hyperparameters on model performance for differentially private graph classification. Finally, we apply explainability techniques to assess whether similar representations are learned in the private and non-private settings and establish robust baselines for future work in this area."}}
{"id": "H5_9JHJ8Q9", "cdate": 1640995200000, "mdate": 1681725024723, "content": {"title": "SoK: Differential Privacy on Graph-Structured Data", "abstract": "In this work, we study the applications of differential privacy (DP) in the context of graph-structured data. We discuss the formulations of DP applicable to the publication of graphs and their associated statistics as well as machine learning on graph-based data, including graph neural networks (GNNs). The formulation of DP in the context of graph-structured data is difficult, as individual data points are interconnected (often non-linearly or sparsely). This connectivity complicates the computation of individual privacy loss in differentially private learning. The problem is exacerbated by an absence of a single, well-established formulation of DP in graph settings. This issue extends to the domain of GNNs, rendering private machine learning on graph-structured data a challenging task. A lack of prior systematisation work motivated us to study graph-based learning from a privacy perspective. In this work, we systematise different formulations of DP on graphs, discuss challenges and promising applications, including the GNN domain. We compare and separate works into graph analysis tasks and graph learning tasks with GNNs. Finally, we conclude our work with a discussion of open questions and potential directions for further research in this area."}}
{"id": "6EJTyUd-KE", "cdate": 1640995200000, "mdate": 1682329705775, "content": {"title": "Unified Interpretation of the Gaussian Mechanism for Differential Privacy Through the Sensitivity Index", "abstract": "The Gaussian mechanism (GM) represents a universally employed tool for achieving differential privacy (DP), and a large body of work has been devoted to its analysis. We argue that the three prevailing interpretations of the GM, namely epsilon/delta-DP, f-DP and R\u00e9nyi DP can be expressed by using a single parameter psi, which we term the sensitivity index. Psi uniquely characterises the GM and its properties by encapsulating its two fundamental quantities: the sensitivity of the query and the magnitude of the noise perturbation. With strong links to the ROC curve and the hypothesis-testing interpretation of DP, psi offers the practitioner a powerful method for interpreting, comparing and communicating the privacy guarantees of Gaussian mechanisms."}}
{"id": "0JBzdN2eOk", "cdate": 1640995200000, "mdate": 1682329705806, "content": {"title": "How Do Input Attributes Impact the Privacy Loss in Differential Privacy?", "abstract": "Differential privacy (DP) is typically formulated as a worst-case privacy guarantee over all individuals in a database. More recently, extensions to individual subjects or their attributes, have been introduced. Under the individual/per-instance DP interpretation, we study the connection between the per-subject gradient norm in DP neural networks and individual privacy loss and introduce a novel metric termed the Privacy Loss-Input Susceptibility (PLIS), which allows one to apportion the subject's privacy loss to their input attributes. We experimentally show how this enables the identification of sensitive attributes and of subjects at high risk of data reconstruction."}}
{"id": "Clre-Prt128", "cdate": 1632875451081, "mdate": null, "content": {"title": "Complex-valued deep learning with differential privacy", "abstract": "We present $\\zeta$-DP, an extension of differential privacy (DP) to complex-valued functions. After introducing the complex Gaussian mechanism, whose properties we characterise in terms of $(\\varepsilon, \\delta)$-DP and R\u00e9nyi-DP, we present $\\zeta$-DP stochastic gradient descent ($\\zeta$-DP-SGD), a variant of DP-SGD for training complex-valued neural networks. We experimentally evaluate $\\zeta$-DP-SGD on three complex-valued tasks, i.e. electrocardiogram classification, speech classification and magnetic resonance imaging (MRI) reconstruction. Moreover, we provide $\\zeta$-DP-SGD benchmarks for a large variety of complex-valued activation functions and on a complex-valued variant of the MNIST dataset. Our experiments demonstrate that DP training of complex-valued neural networks is possible with rigorous privacy guarantees and excellent utility."}}
