{"id": "HgQR0mXQ1_a", "cdate": 1663850048036, "mdate": null, "content": {"title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners", "abstract": "Recent advances in vision-language pre-training have pushed the state-of-the-art on various vision-language tasks, making machines more capable of multi-modal writing (image-to-text generation) and painting (text-to-image generation). However, few studies investigate if these two essential capabilities can be learned together and boost each other, making a versatile and powerful multi-modal foundation model. In this work, we disclose the potential of symmetric generative vision-language pre-training in learning to write and paint concurrently, and propose a new unified modal model, named DaVinci, trained with prefix language modeling and prefix image modeling, a simple generative self-supervised objective on image-text pairs. Thanks to the proposed prefix multi-modal modeling framework, DaVinci is simple to train, scalable to huge data, adaptable to both writing and painting tasks, and also strong on other vision, text, and multi-modal understanding tasks. DaVinci achieves competitive performance on a wide range of 27 generation/understanding tasks and demonstrates the superiority of combining vision/language generative pre-training. Furthermore, we carefully benchmark the performance of different vision-language pre-training objectives on different scales of pre-training datasets on a heterogeneous and broad distribution coverage. Our results demonstrate the potential of exploiting self-supervision in both language and vision inputs, and establish new, stronger baselines for future comparisons at different data scales. The code and pre-trained models are available at https://github.com/shizhediao/DaVinci."}}
{"id": "zIU2Y9dJ4rD", "cdate": 1640995200000, "mdate": 1653033522891, "content": {"title": "Contextual Representation Learning beyond Masked Language Modeling", "abstract": "How do masked language models (MLMs) such as BERT learn contextual representations? In this work, we analyze the learning dynamics of MLMs. We find that MLMs adopt sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs. To address these issues, we propose TACO, a simple yet effective representation learning approach to directly model global semantics. TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations. Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over existing MLMs. The code is available at https://github.com/FUZHIYI/TACO."}}
{"id": "Ic9e8G6VHCR", "cdate": 1640995200000, "mdate": 1653033522865, "content": {"title": "Contextual Representation Learning beyond Masked Language Modeling", "abstract": "Zhiyi Fu, Wangchunshu Zhou, Jingjing Xu, Hao Zhou, Lei Li. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "HsFn8q6p7eF", "cdate": 1640995200000, "mdate": 1653033522881, "content": {"title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning", "abstract": "Wangchunshu Zhou, Canwen Xu, Julian McAuley. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "xp9ZXsZImjS", "cdate": 1609459200000, "mdate": 1653033522877, "content": {"title": "A Survey on Green Deep Learning", "abstract": "In recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges."}}
{"id": "vBzgz8GLXXz", "cdate": 1609459200000, "mdate": 1653033522915, "content": {"title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge", "abstract": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley, Furu Wei. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "uvEtrlfjk6z", "cdate": 1609459200000, "mdate": 1653033522892, "content": {"title": "Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training", "abstract": "Wangchunshu Zhou, Qifei Li, Chenle Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "tcDK2c_awUx", "cdate": 1609459200000, "mdate": 1653033522768, "content": {"title": "Learning to Predict Persona Information forDialogue Personalization without Explicit Persona Description", "abstract": "Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. However, most current dialogue personalization approaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona information based on the dialogue history to personalize the dialogue agent without relying on any explicit persona descriptions during inference. Experimental results on the PersonaChat dataset show that the proposed method can improve the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. \"self persona\"), and improve the engagingness of the generated responses when conditioning on the predicted persona of the dialogue partner (i.e. \"their persona\"). We also find that a trained persona prediction model can be successfully transferred to other datasets and help generate more relevant responses."}}
{"id": "rj4ru42npja", "cdate": 1609459200000, "mdate": 1653033522679, "content": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective. Our proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin."}}
{"id": "qxABOqcYlo1", "cdate": 1609459200000, "mdate": 1653033522674, "content": {"title": "Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting", "abstract": "Wangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, Furu Wei. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
