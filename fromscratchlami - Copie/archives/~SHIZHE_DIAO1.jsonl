{"id": "qSCHRL8b96S", "cdate": 1663850542278, "mdate": null, "content": {"title": "PTUnifier: Pseudo Tokens as Paradigm Unifiers in Medical Vision-and-Language Pre-training", "abstract": "Medical vision-and-language pre-training (Med-VLP) has shown promising improvements on many downstream medical tasks owing to its applicability to extracting generic representations from medical images and texts. Practically, there exist two typical paradigms, i.e., the \\textbf{fusion-encoder paradigm} and the \\textbf{dual-encoder paradigm}, depending on whether a heavy fusion module is used. The former outperforms on multi-modal tasks owing to the sufficient interaction between modalities; the latter outperforms on uni-modal and cross-modal tasks due to the single-modality encoding ability. To take advantage of these two paradigms, we propose an effective yet straightforward scheme named PTUnifier to unify the two paradigms thanks to the identical input format by introducing visual and textual pseudo tokens, which serve as a feature bank that stores the most representative images/texts. By doing so, a single model could process various tasks adopting different input formats (i.e., image-only, text-only, and image-text-pair). Furthermore, we construct a pool of pseudo tokens (instead of static ones) to improve diversity and scalability. Experimental results show that our approach achieves state-of-the-art results on a broad range of tasks, spanning uni-modal tasks (i.e., image/text classification and text summarization), cross-modal tasks (i.e., image-to-text generation and image-text/text-image retrieval), and multi-modal tasks (i.e., visual question answering), demonstrating the effectiveness of our approach. Note that the adoption of pseudo tokens is orthogonal to most existing Med-VLP approaches, and we believe that our approach could be a beneficial and complementary extension to these approaches."}}
{"id": "HgQR0mXQ1_a", "cdate": 1663850048036, "mdate": null, "content": {"title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners", "abstract": "Recent advances in vision-language pre-training have pushed the state-of-the-art on various vision-language tasks, making machines more capable of multi-modal writing (image-to-text generation) and painting (text-to-image generation). However, few studies investigate if these two essential capabilities can be learned together and boost each other, making a versatile and powerful multi-modal foundation model. In this work, we disclose the potential of symmetric generative vision-language pre-training in learning to write and paint concurrently, and propose a new unified modal model, named DaVinci, trained with prefix language modeling and prefix image modeling, a simple generative self-supervised objective on image-text pairs. Thanks to the proposed prefix multi-modal modeling framework, DaVinci is simple to train, scalable to huge data, adaptable to both writing and painting tasks, and also strong on other vision, text, and multi-modal understanding tasks. DaVinci achieves competitive performance on a wide range of 27 generation/understanding tasks and demonstrates the superiority of combining vision/language generative pre-training. Furthermore, we carefully benchmark the performance of different vision-language pre-training objectives on different scales of pre-training datasets on a heterogeneous and broad distribution coverage. Our results demonstrate the potential of exploiting self-supervision in both language and vision inputs, and establish new, stronger baselines for future comparisons at different data scales. The code and pre-trained models are available at https://github.com/shizhediao/DaVinci."}}
{"id": "quuUj9ibbuZ", "cdate": 1651067075626, "mdate": 1651067075626, "content": {"title": "Efficient Neural Network Training via Forward and Backward Propagation Sparsification", "abstract": "Sparse training is a natural idea to accelerate the training speed of deep neural networks and save the memory usage, especially since large modern neural networks are significantly over-parameterized.  However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step.  This paper solves this problem by proposing an efficient sparse training method with completely sparse forward and backward passes. We first formulate the training process as a continuous minimization problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure.  For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algorithm is much more effective in accelerating the training process, up to an order of magnitude faster. "}}
{"id": "JnAU9HkXr2", "cdate": 1621629985443, "mdate": null, "content": {"title": "Efficient Neural Network Training via Forward and Backward Propagation Sparsification", "abstract": "Sparse training is a natural idea to accelerate the training speed of deep neural networks and save the memory usage, especially since large modern neural networks are significantly over-parameterized.  However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step.  This paper solves this problem by proposing an efficient sparse training method with completely sparse forward and backward passes. We first formulate the training process as a continuous minimization problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure.  For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algorithm is much more effective in accelerating the training process, up to an order of magnitude faster. "}}
{"id": "sT02FXyPBAF", "cdate": 1609459200000, "mdate": 1634225536486, "content": {"title": "Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation", "abstract": "Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song, Tong Zhang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "bZBwFVhbO9I", "cdate": 1609459200000, "mdate": 1634225536253, "content": {"title": "TILGAN: Transformer-based Implicit Latent GAN for Diverse and Coherent Text Generation", "abstract": "Shizhe Diao, Xinwei Shen, Kashun Shum, Yan Song, Tong Zhang. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021."}}
{"id": "rXrolE-OlWC", "cdate": 1577836800000, "mdate": 1634225539523, "content": {"title": "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations", "abstract": "Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, Yonggang Wang. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
{"id": "F1L-mt8N-S9", "cdate": 1577836800000, "mdate": 1634225536251, "content": {"title": "Keyphrase Generation with Cross-Document Attention", "abstract": "Keyphrase generation aims to produce a set of phrases summarizing the essentials of a given document. Conventional methods normally apply an encoder-decoder architecture to generate the output keyphrases for an input document, where they are designed to focus on each current document so they inevitably omit crucial corpus-level information carried by other similar documents, i.e., the cross-document dependency and latent topics. In this paper, we propose CDKGen, a Transformer-based keyphrase generator, which expands the Transformer to global attention with cross-document attention networks to incorporate available documents as references so as to generate better keyphrases with the guidance of topic information. On top of the proposed Transformer + cross-document attention architecture, we also adopt a copy mechanism to enhance our model via selecting appropriate words from documents to deal with out-of-vocabulary words in keyphrases. Experiment results on five benchmark datasets illustrate the validity and effectiveness of our model, which achieves the state-of-the-art performance on all datasets. Further analyses confirm that the proposed model is able to generate keyphrases consistent with references while keeping sufficient diversity. The code of CDKGen is available at https://github.com/SVAIGBA/CDKGen."}}
{"id": "HFp42l_lz9", "cdate": 1546300800000, "mdate": 1634225541644, "content": {"title": "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations", "abstract": "The pre-training of text encoders normally processes text as a sequence of tokens corresponding to small text units, such as word pieces in English and characters in Chinese. It omits information carried by larger text granularity, and thus the encoders cannot easily adapt to certain combinations of characters. This leads to a loss of important semantic information, which is especially problematic for Chinese because the language does not have explicit word boundaries. In this paper, we propose ZEN, a BERT-based Chinese (Z) text encoder Enhanced by N-gram representations, where different combinations of characters are considered during training. As a result, potential word or phase boundaries are explicitly pre-trained and fine-tuned with the character encoder (BERT). Therefore ZEN incorporates the comprehensive information of both the character sequence and words or phrases it contains. Experimental results illustrated the effectiveness of ZEN on a series of Chinese NLP tasks. We show that ZEN, using less resource than other published encoders, can achieve state-of-the-art performance on most tasks. Moreover, it is shown that reasonable performance can be obtained when ZEN is trained on a small corpus, which is important for applying pre-training techniques to scenarios with limited data. The code and pre-trained models of ZEN are available at https://github.com/sinovation/zen."}}
{"id": "J_jqY_AqhHI", "cdate": 1483228800000, "mdate": 1634225538085, "content": {"title": "GubaLex: Guba-Oriented Sentiment Lexicon for Big Texts in Finance", "abstract": "Trading in stock market depends mostly on investor's emotions though technical analysis is a viable tool there. In China, Guba is a typical platform for individual investors to share news and opinions on their favorite stocks. The texts posted in Guba by investors involve in richful emotions which can reflect their willingness on the stock. Few works focus on Guba sentiment analysis though numerous have been done on investor sentiment analysis in finance market for the purpose of understanding the market. Text mining is the most popular method to analyze the sentiment implied in the web text, which depends heavily on the lexicon. Existed lexicons for general purpose work badly on sentiment analysis for Guba messages. In this work, we construct a specified lexicon for Chinese Guba, named GubaLex, in considerations of the characteristics of the Guba text: short, emotion enriched, colloquial (informal), and stock market oriented. It is constructed by using the merge of HowNet and NTUSD as the basic sentiment lexicon, then adding stock terms from the Guba corpus and information in the area of stock market. Based on GubaLex, we develop the bullish lexicon GLBull and the bearish lexicon GL-Bear especially including bullish and bearish sentiment terms for further sentiment analysis. We also proposed an auto update module and sentiment classification algorithm for Guba texts. The experiments show the proposed lexicon works better in sentiment analysis than the previous, like HowNet and NTUSD."}}
