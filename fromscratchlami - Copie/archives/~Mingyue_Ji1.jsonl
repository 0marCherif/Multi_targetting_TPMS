{"id": "0z_PNPv5vOF", "cdate": 1653575390399, "mdate": 1653575390399, "content": {"title": "Demystifying Why Local Aggregation Helps: Convergence Analysis of", "abstract": "Hierarchical SGD (H-SGD) has emerged as a new distributed\nSGD algorithm for multi-level communication networks. In\nH-SGD, before each global aggregation, workers send their\nupdated local models to local servers for aggregations. De-\nspite recent research efforts, the effect of local aggregation\non global convergence still lacks theoretical understanding. In\nthis work, we first introduce a new notion of \u201cupward\u201d and\n\u201cdownward\u201d divergences. We then use it to conduct a novel\nanalysis to obtain a worst-case convergence upper bound for\ntwo-level H-SGD with non-IID data, non-convex objective\nfunction, and stochastic gradient. By extending this result to\nthe case with random grouping, we observe that this conver-\ngence upper bound of H-SGD is between the upper bounds\nof two single-level local SGD settings, with the number of\nlocal iterations equal to the local and global update periods\nin H-SGD, respectively. We refer to this as the \u201csandwich\nbehavior\u201d. Furthermore, we extend our analytical approach\nbased on \u201cupward\u201d and \u201cdownward\u201d divergences to study the\nconvergence for the general case of H-SGD with more than\ntwo levels, where the \u201csandwich behavior\u201d still holds. Our the-\noretical results provide key insights of why local aggregation\ncan be beneficial in improving the convergence of H-SGD."}}
{"id": "qSs7C7c4G8D", "cdate": 1652737647418, "mdate": null, "content": {"title": "A Unified Analysis of Federated Learning with Arbitrary Client Participation", "abstract": "Federated learning (FL) faces challenges of intermittent client availability and computation/communication efficiency. As a result, only a small subset of clients can participate in FL at a given time. It is important to understand how partial client participation affects convergence, but most existing works have either considered idealized participation patterns or obtained results with non-zero optimality error for generic patterns. In this paper, we provide a unified convergence analysis for FL with arbitrary client participation. We first introduce a generalized version of federated averaging (FedAvg) that amplifies parameter updates at an interval of multiple FL rounds. Then, we present a novel analysis that captures the effect of client participation in a single term. By analyzing this term, we obtain convergence upper bounds for a wide range of participation patterns, including both non-stochastic and stochastic cases, which match either the lower bound of stochastic gradient descent (SGD) or the state-of-the-art results in specific settings. We also discuss various insights, recommendations, and experimental results. \n"}}
