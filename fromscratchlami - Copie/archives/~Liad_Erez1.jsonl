{"id": "z6wDFuHTji1", "cdate": 1640995200000, "mdate": 1681705121298, "content": {"title": "Regret Minimization and Convergence to Equilibria in General-sum Markov Games", "abstract": "An abundance of recent impossibility results establish that regret minimization in Markov games with adversarial opponents is both statistically and computationally intractable. Nevertheless, none of these results preclude the possibility of regret minimization under the assumption that all parties adopt the same learning procedure. In this work, we present the first (to our knowledge) algorithm for learning in general-sum Markov games that provides sublinear regret guarantees when executed by all agents. The bounds we obtain are for swap regret, and thus, along the way, imply convergence to a correlated equilibrium. Our algorithm is decentralized, computationally efficient, and does not require any communication between agents. Our key observation is that online learning via policy optimization in Markov games essentially reduces to a form of weighted regret minimization, with unknown weights determined by the path length of the agents' policy sequence. Consequently, controlling the path length leads to weighted regret objectives for which sufficiently adaptive algorithms provide sublinear regret guarantees."}}
{"id": "1gLyEmOsKE8", "cdate": 1621630236046, "mdate": null, "content": {"title": "Towards Best-of-All-Worlds Online Learning with Feedback Graphs", "abstract": "We study the online learning with feedback graphs framework introduced by Mannor and Shamir (2011), in which the feedback received by the online learner is specified by a graph $G$ over the available actions.  We develop an algorithm that simultaneously achieves regret bounds of the form: $O(\\sqrt{\\theta(G) T})$ with adversarial losses; $O(\\theta(G)\\mathrm{polylog}{T})$ with stochastic losses; and $O(\\theta(G)\\mathrm{polylog}{T} + \\sqrt{\\theta(G) C})$ with stochastic losses subject to $C$ adversarial corruptions.  Here, $\\theta(G)$ is the $clique~covering~number$ of the graph $G$.  Our algorithm is an instantiation of Follow-the-Regularized-Leader with a novel regularization that can be seen as a product of a Tsallis entropy component (inspired by Zimmert and Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted stochastic case by Amir et al. (2020)), thus subtly interpolating between the two forms of entropies.  One of our key technical contributions is in establishing the convexity of this regularizer and controlling its inverse Hessian, despite its complex product structure."}}
{"id": "kl7Kgklk_5", "cdate": 1609459200000, "mdate": 1683878141515, "content": {"title": "Towards Best-of-All-Worlds Online Learning with Feedback Graphs", "abstract": "We study the online learning with feedback graphs framework introduced by Mannor and Shamir (2011), in which the feedback received by the online learner is specified by a graph $G$ over the available actions. We develop an algorithm that simultaneously achieves regret bounds of the form: $O(\\sqrt{\\theta(G) T})$ with adversarial losses; $O(\\theta(G)\\mathrm{polylog}{T})$ with stochastic losses; and $O(\\theta(G)\\mathrm{polylog}{T} + \\sqrt{\\theta(G) C})$ with stochastic losses subject to $C$ adversarial corruptions. Here, $\\theta(G)$ is the $clique~covering~number$ of the graph $G$. Our algorithm is an instantiation of Follow-the-Regularized-Leader with a novel regularization that can be seen as a product of a Tsallis entropy component (inspired by Zimmert and Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted stochastic case by Amir et al. (2020)), thus subtly interpolating between the two forms of entropies. One of our key technical contributions is in establishing the convexity of this regularizer and controlling its inverse Hessian, despite its complex product structure."}}
