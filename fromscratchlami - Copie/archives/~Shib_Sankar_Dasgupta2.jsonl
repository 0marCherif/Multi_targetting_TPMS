{"id": "dI6KBKNRp7", "cdate": 1662812630438, "mdate": null, "content": {"title": "An Analysis of Virtual Nodes in Graph Neural Networks for Link Prediction (Extended Abstract)", "abstract": "It is well known that the graph classification performance of graph neural networks often improves by adding an artificial virtual node to the graphs, which is connected to all graph nodes. Surprisingly, the advantage of using virtual nodes has never been theoretically investigated, and their impact on other problems is still an open research question. In this paper, we adapt the concept of virtual nodes to link prediction, where we usually have much larger, often very sparse or dense, and overall more heterogeneous graphs. In particular, we use multiple virtual nodes per graph and graph-based clustering to determine the connections to the graph nodes. We also provide a detailed theoretical analysis. We conducted experiments over different datasets of the Open Graph Benchmark, analyze the results in detail, and show that virtual nodes may yield rather stable performance increases and sometimes considerably boost performance."}}
{"id": "OE4kTmxv9_", "cdate": 1640995200000, "mdate": 1666979439359, "content": {"title": "Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings", "abstract": "Shib Dasgupta, Michael Boratko, Siddhartha Mishra, Shriya Atmakuri, Dhruvesh Patel, Xiang Li, Andrew McCallum. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "ETiaOyNwJW", "cdate": 1632875653150, "mdate": null, "content": {"title": "Revisiting Virtual Nodes in Graph Neural Networks for Link Prediction", "abstract": "It is well known that the graph classification performance of graph neural networks often improves by adding an artificial virtual node to the graphs, which is connected to all nodes in the graph. Intuitively, the virtual node provides a shortcut for message passing between nodes along the graph edges. Surprisingly, the impact of virtual nodes with other problems is still an open research question. \n\nIn this paper, we adapt the concept of virtual nodes to the link prediction scenario, where we usually have much larger, often dense, and more heterogeneous graphs. In particular, we use multiple virtual nodes per graph and graph-based clustering to determine the connections to the graph nodes. We also investigate alternative clustering approaches (e.g., random or more advanced) and compare to the original model with a single virtual node. We conducted extensive experiments over different datasets of the Open Graph Benchmark (OGB) and analyze the results in detail. We show that our virtual node extensions yield rather stable performance increases and allow standard graph neural networks to compete with complex state-of-the-art models, as well as with the models leading the OGB leaderboards."}}
{"id": "xXHdokb41Ya", "cdate": 1609459200000, "mdate": 1666979439491, "content": {"title": "Box-To-Box Transformations for Modeling Joint Hierarchies", "abstract": ""}}
{"id": "wSCqNyUodoF", "cdate": 1609459200000, "mdate": 1643336783678, "content": {"title": "Box Embeddings: An open-source library for representation learning using geometric structures", "abstract": "A major factor contributing to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with geometric structures (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacities. In this work, we introduce Box Embeddings, a Python library that enables researchers to easily apply and extend probabilistic box embeddings."}}
{"id": "rKI4sNeKL-5", "cdate": 1609459200000, "mdate": 1646854195225, "content": {"title": "Word2Box: Learning Word Representation Using Box Embeddings", "abstract": "Learning representations of words in a continuous space is perhaps the most fundamental task in NLP, however words interact in ways much richer than vector dot product similarity can provide. Many relationships between words can be expressed set-theoretically, for example, adjective-noun compounds (eg. \"red cars\"$\\subseteq$\"cars\") and homographs (eg. \"tongue\"$\\cap$\"body\" should be similar to \"mouth\", while \"tongue\"$\\cap$\"language\" should be similar to \"dialect\") have natural set-theoretic interpretations. Box embeddings are a novel region-based representation which provide the capability to perform these set-theoretic operations. In this work, we provide a fuzzy-set interpretation of box embeddings, and learn box representations of words using a set-theoretic training objective. We demonstrate improved performance on various word similarity tasks, particularly on less common words, and perform a quantitative and qualitative analysis exploring the additional unique expressivity provided by Word2Box."}}
{"id": "fASoQmxFV0D", "cdate": 1609459200000, "mdate": 1643336783633, "content": {"title": "Min/max stability and box distributions", "abstract": "In representation learning, capturing correlations between the represented elements is paramount. A recent line of work introduces the notion of learning region-based representations, with the obje..."}}
{"id": "JX7KVLPDGt", "cdate": 1609459200000, "mdate": 1643336783656, "content": {"title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning", "abstract": "Xuelu Chen, Michael Boratko, Muhao Chen, Shib Sankar Dasgupta, Xiang Lorraine Li, Andrew McCallum. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "B34Z94ltIbq", "cdate": 1609459200000, "mdate": 1646854194009, "content": {"title": "Box Embeddings: An open-source library for representation learning using geometric structures", "abstract": "Tejas Chheda, Purujit Goyal, Trang Tran, Dhruvesh Patel, Michael Boratko, Shib Sankar Dasgupta, Andrew McCallum. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2021."}}
{"id": "01uk8YZ6rK5", "cdate": 1609459200000, "mdate": null, "content": {"title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning", "abstract": "Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however, existing embedding methods only model triple-level uncertainty, and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts."}}
