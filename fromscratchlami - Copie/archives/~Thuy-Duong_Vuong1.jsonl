{"id": "yys4HAiU4lT", "cdate": 1640995200000, "mdate": 1684174230791, "content": {"title": "Composable Coresets for Constrained Determinant Maximization and Beyond", "abstract": "We study the task of determinant maximization under partition constraint, in the context of large data sets. Given a point set $V\\subset \\mathbb{R}^d$ that is partitioned into $s$ groups $V_1,..., V_s$, and integers $k_1,...,k_s$ where $k=\\sum_i k_i$, the goal is to pick $k_i$ points from group $i$ such that the overall determinant of the picked $k$ points is maximized. Determinant Maximization and its constrained variants have gained a lot of interest for modeling diversityand have found applications in the context of fairness and data summarization. We study the design of composable coresets for the constrained determinant maximization problem. Composable coresets are small subsets of the data that (approximately) preserve optimal solutions to optimization tasks and enable efficient solutions in several other large data models including the distributed and the streaming settings. In this work, we consider two regimes. For the case of $k>d$, we show a peeling algorithm that gives us a composable coreset of size $kd$ with an approximation factor of $d^{O(d)}$. We complement our results by showing that this approximation factor is tight. For the case of $k\\leq d$, we show that a simple modification of the previous algorithms results in an optimal coreset verified by our lower bounds. Our results apply to all strongly Rayleigh distribution and several other experimental design problems. In addition, we show coreset construction algorithms under the more general laminar matroid constraints."}}
{"id": "v4SeEH2PL0", "cdate": 1640995200000, "mdate": 1684174230795, "content": {"title": "Spectral independence, coupling, and the spectral gap of the Glauber dynamics", "abstract": ""}}
{"id": "luc4l1SZrH", "cdate": 1640995200000, "mdate": 1684174230805, "content": {"title": "Dimension reduction for maximum matchings and the Fastest Mixing Markov Chain", "abstract": "Let $G = (V,E)$ be an undirected graph with maximum degree $\\Delta$ and vertex conductance $\\Psi^*(G)$. We show that there exists a symmetric, stochastic matrix $P$, with off-diagonal entries supported on $E$, whose spectral gap $\\gamma^*(P)$ satisfies \\[\\Psi^*(G)^{2}/\\log\\Delta \\lesssim \\gamma^*(P) \\lesssim \\Psi^*(G).\\] Our bound is optimal under the Small Set Expansion Hypothesis, and answers a question of Olesker-Taylor and Zanetti, who obtained such a result with $\\log\\Delta$ replaced by $\\log|V|$. In order to obtain our result, we show how to embed a negative-type semi-metric $d$ defined on $V$ into a negative-type semi-metric $d'$ supported in $\\mathbb{R}^{O(\\log\\Delta)}$, such that the (fractional) matching number of the weighted graph $(V,E,d)$ is approximately equal to that of $(V,E,d')$."}}
{"id": "hMlnR1zikG", "cdate": 1640995200000, "mdate": 1684136155399, "content": {"title": "Optimal Sublinear Sampling of Spanning Trees and Determinantal Point Processes via Average-Case Entropic Independence", "abstract": "We design fast algorithms for repeatedly sampling from strongly Rayleigh distributions, which include random spanning tree distributions and determinantal point processes. For a graph $G=(V, E)$, we show how to approximately sample uniformly random spanning trees from $G$ in $\\widetilde{O}(\\lvert V\\rvert)$ time per sample after an initial $\\widetilde{O}(\\lvert E\\rvert)$ time preprocessing. For a determinantal point process on subsets of size $k$ of a ground set of $n$ elements, we show how to approximately sample in $\\widetilde{O}(k^\\omega)$ time after an initial $\\widetilde{O}(nk^{\\omega-1})$ time preprocessing, where $\\omega<2.372864$ is the matrix multiplication exponent. We even improve the state of the art for obtaining a single sample from determinantal point processes, from the prior runtime of $\\widetilde{O}(\\min\\{nk^2, n^\\omega\\})$ to $\\widetilde{O}(nk^{\\omega-1})$. In our main technical result, we achieve the optimal limit on domain sparsification for strongly Rayleigh distributions. In domain sparsification, sampling from a distribution $\\mu$ on $\\binom{[n]}{k}$ is reduced to sampling from related distributions on $\\binom{[t]}{k}$ for $t\\ll n$. We show that for strongly Rayleigh distributions, we can can achieve the optimal $t=\\widetilde{O}(k)$. Our reduction involves sampling from $\\widetilde{O}(1)$ domain-sparsified distributions, all of which can be produced efficiently assuming convenient access to approximate overestimates for marginals of $\\mu$. Having access to marginals is analogous to having access to the mean and covariance of a continuous distribution, or knowing \"isotropy\" for the distribution, the key assumption behind the Kannan-Lov\\'asz-Simonovits (KLS) conjecture and optimal samplers based on it. We view our result as a moral analog of the KLS conjecture and its consequences for sampling, for discrete strongly Rayleigh measures."}}
{"id": "P3BxU7XNk5M", "cdate": 1640995200000, "mdate": 1684136155265, "content": {"title": "From Sampling to Optimization on Discrete Domains with Applications to Determinant Maximization", "abstract": "We establish a connection between sampling and optimization on discrete domains. For a family of distributions $\\mu$ defined on size $k$ subsets of a ground set of elements, that is closed under ex..."}}
{"id": "KcIHqX3532D", "cdate": 1640995200000, "mdate": 1684136155274, "content": {"title": "Optimal Sublinear Sampling of Spanning Trees and Determinantal Point Processes via Average-Case Entropic Independence", "abstract": "We design fast algorithms for repeatedly sampling from strongly Rayleigh distributions, which include as special cases random spanning tree distributions and determinantal point processes. For a graph $G=(V,\\ E)$, we show how to approximately sample uniformly random spanning trees from G in $O(|V|)$ <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>  time per sample after an initial $O(|E|)$ time preprocessing. This is the first nearly-linear runtime in the output size, which is clearly optimal. For a determinantal point process on k-sized subsets of a ground set of n elements, defined via an $n\\times n$ kernel matrix, we show how to approximately sample in ${\\widetilde{O}}(k^{\\omega})$ time after an initial ${\\widetilde{O}}(nk^{\\omega-1})$ time preprocessing, where $\\omega\\lt 2.372864$ is the matrix multiplication exponent. The time to compute just the weight of the output set is simply $\\simeq k^{\\omega}$, a natural barrier that suggests our runtime might be optimal for determinantal point processes as well. As a corollary, we even improve the state of the art for obtaining a single sample from a determinantal point process, from the prior runtime of ${\\widetilde{O}}(\\min\\{nk^{2},\\ n^{\\omega}\\})$ to ${\\widetilde{O}}(nk^{\\omega-1})$.In our main technical result, we achieve the optimal limit on domain sparsification for strongly Rayleigh distributions. In domain sparsification, sampling from a distribution $\\mu$ on $\\binom{[n]}{k}$ is reduced to sampling from related distributions on $\\binom{[t]}{k}$ for $t\\ll n$. We show that for strongly Rayleigh distributions, the domain size can be reduced to nearly linear in the output size $t={\\widetilde{O}}(k)$, improving the state of the art from $t={\\widetilde{O}}(k^{2})$ for general strongly Rayleigh distributions and the more specialized $t={\\widetilde{O}}(k^{15})$ for sBanning tree distributions. Our reduction involves sampling from ${\\widetilde{O}}(1)$ domain-sparsified distributions, all of which can be produced efficiently assuming approximate overestimates for marginals of $\\mu$ are known and stored in a convenient data structure. Having access to marginals is the discrete analog of having access to the mean and covariance of a continuous distribution, or equivalently knowing \u201cisotropy\u201d for the distribution, the key behind optimal samplers in the continuous setting based on the famous Kannan-Lov\u00e1sz-Simonovits (KLS) conjecture. We view our result as analogous in spirit to the KLS conjecture and its consequences for sampling, but rather for discrete strongly Rayleigh measures. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Throughout, ${\\widetilde{O}}(\\cdot)$ hides polylogarithmic factors in n."}}
{"id": "GXn9yzCbdeA", "cdate": 1640995200000, "mdate": 1684174230794, "content": {"title": "On the Complexity of Sampling Redistricting Plans", "abstract": "A crucial task in the political redistricting problem is to sample redistricting plans i.e. a partitioning of the graph of census blocks into districts.   We show that Recombination [DeFord-Duchin-Solomon'21]-a popular Markov chain to sample redistricting plans-is exponentially slow mixing on simple subgraph of $\\mathbb{Z}_2.$ We show an alternative way to sample balance, compact and contiguous redistricting plans using a \"relaxed\" version of ReCom and rejection sampling."}}
{"id": "ELMqkKEOviM", "cdate": 1640995200000, "mdate": 1684136155400, "content": {"title": "Improved Sampling-to-Counting Reductions in High-Dimensional Expanders and Faster Parallel Determinantal Sampling", "abstract": "We study the problem of parallelizing sampling from distributions related to determinants: symmetric, nonsymmetric, and partition-constrained determinantal point processes, as well as planar perfect matchings. For these distributions, the partition function, a.k.a. the count, can be obtained via matrix determinants, a highly parallelizable computation; Csanky proved it is in NC. However, parallel counting does not automatically translate to parallel sampling, as classic reductions between the two are inherently sequential. We show that a nearly quadratic parallel speedup over sequential sampling can be achieved for all the aforementioned distributions. If the distribution is supported on subsets of size $k$ of a ground set, we show how to approximately produce a sample in $\\widetilde{O}(k^{\\frac{1}{2} + c})$ time with polynomially many processors for any constant $c>0$. In the two special cases of symmetric determinantal point processes and planar perfect matchings, our bound improves to $\\widetilde{O}(\\sqrt k)$ and we show how to sample exactly in these cases. As our main technical contribution, we fully characterize the limits of batching for the steps of sampling-to-counting reductions. We observe that only $O(1)$ steps can be batched together if we strive for exact sampling, even in the case of nonsymmetric determinantal point processes. However, we show that for approximate sampling, $\\widetilde{\\Omega}(k^{\\frac{1}{2}-c})$ steps can be batched together, for any entropically independent distribution, which includes all mentioned classes of determinantal point processes. Entropic independence and related notions have been the source of breakthroughs in Markov chain analysis in recent years, so we expect our framework to prove useful for distributions beyond those studied in this work."}}
{"id": "9Xjw0s_vRNk", "cdate": 1640995200000, "mdate": 1684136155323, "content": {"title": "Domain Sparsification of Discrete Distributions Using Entropic Independence", "abstract": "We present a framework for speeding up the time it takes to sample from discrete distributions \u03bc defined over subsets of size k of a ground set of n elements, in the regime where k is much smaller than n. We show that if one has access to estimates of marginals P_{S\u223c \u03bc} {i \u2208 S}, then the task of sampling from \u03bc can be reduced to sampling from related distributions \u03bd supported on size k subsets of a ground set of only n^{1-\u03b1}\u22c5 poly(k) elements. Here, 1/\u03b1 \u2208 [1, k] is the parameter of entropic independence for \u03bc. Further, our algorithm only requires sparsified distributions \u03bd that are obtained by applying a sparse (mostly 0) external field to \u03bc, an operation that for many distributions \u03bc of interest, retains algorithmic tractability of sampling from \u03bd. This phenomenon, which we dub domain sparsification, allows us to pay a one-time cost of estimating the marginals of \u03bc, and in return reduce the amortized cost needed to produce many samples from the distribution \u03bc, as is often needed in upstream tasks such as counting and inference. For a wide range of distributions where \u03b1 = \u03a9(1), our result reduces the domain size, and as a corollary, the cost-per-sample, by a poly(n) factor. Examples include monomers in a monomer-dimer system, non-symmetric determinantal point processes, and partition-constrained Strongly Rayleigh measures. Our work significantly extends the reach of prior work of Anari and Derezi\u0144ski who obtained domain sparsification for distributions with a log-concave generating polynomial (corresponding to \u03b1 = 1). As a corollary of our new analysis techniques, we also obtain a less stringent requirement on the accuracy of marginal estimates even for the case of log-concave polynomials; roughly speaking, we show that constant-factor approximation is enough for domain sparsification, improving over O(1/k) relative error established in prior work."}}
{"id": "1xgIHh4yA6", "cdate": 1640995200000, "mdate": 1682372669487, "content": {"title": "Entropic independence: optimal mixing of down-up random walks", "abstract": "We introduce a notion called entropic independence that is an entropic analog of spectral notions of high-dimensional expansion. Informally, entropic independence of a background distribution \u00b5 on k-sized subsets of a ground set of elements says that for any (possibly randomly chosen) set S, the relative entropy of a single element of S drawn uniformly at random carries at most O(1/k) fraction of the relative entropy of S. Entropic independence is the analog of the notion of spectral independence, if one replaces variance by entropy. We use entropic independence to derive tight mixing time bounds, overcoming the lossy nature of spectral analysis of Markov chains on exponential-sized state spaces. In our main technical result, we show a general way of deriving entropy contraction, a.k.a. modified log-Sobolev inequalities, for down-up random walks from spectral notions. We show that spectral independence of a distribution under arbitrary external fields automatically implies entropic independence. We furthermore extend our theory to the case where spectral independence does not hold under arbitrary external fields. To do this, we introduce a framework for obtaining tight mixing time bounds for Markov chains based on what we call restricted modified log-Sobolev inequalities, which guarantee entropy contraction not for all distributions, but for those in a sufficiently large neighborhood of the stationary distribution. To derive our results, we relate entropic independence to properties of polynomials: \u00b5 is entropically independent exactly when a transformed version of the generating polynomial of \u00b5 is upper bounded by its linear tangent; this property is implied by concavity of the said transformation, which was shown by prior work to be locally equivalent to spectral independence. We apply our results to obtain (1) tight modified log-Sobolev inequalities and mixing times for multi-step down-up walks on fractionally log-concave distributions, (2) the tight mixing time of O(nlogn) for Glauber dynamics on Ising models whose interaction matrix has eigenspectrum lying within an interval of length smaller than 1, improving upon the prior quadratic dependence on n, and (3) nearly-linear time O\u03b4(n) samplers for the hardcore and Ising models on n-node graphs that have \u03b4-relative gap to the tree-uniqueness threshold. In the last application, our bound on the running time does not depend on the maximum degree \u0394 of the graph, and is therefore optimal even for high-degree graphs, and in fact, is sublinear in the size of the graph for high-degree graphs."}}
