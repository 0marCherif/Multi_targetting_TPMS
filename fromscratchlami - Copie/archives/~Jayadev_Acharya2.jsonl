{"id": "zml9gDnulI9", "cdate": 1668734789009, "mdate": null, "content": {"title": "Hidden Poison: Machine unlearning enables camouflaged poisoning attacks", "abstract": "We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset."}}
{"id": "BrgzlYUUT3n", "cdate": 1665069637794, "mdate": null, "content": {"title": "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks", "abstract": "We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10,  Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a  poisoned dataset."}}
{"id": "MWoZh1gvbxA", "cdate": 1663849868699, "mdate": null, "content": {"title": "Hidden Poison: Machine unlearning enables camouflaged poisoning attacks", "abstract": "We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10,  Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a  poisoned dataset."}}
{"id": "_SxV6IkxPx1", "cdate": 1622151651876, "mdate": null, "content": {"title": "Estimating Sparse Discrete Distributions Under Privacy and Communication Constraints", "abstract": "We consider the problem of estimating sparse discrete distributions under local differential privacy (LDP) and communication constraints. We characterize the sample complexity for sparse estimation under LDP constraints up to a constant factor, and the sample complexity under communication constraints up to a logarithmic factor. Our upper bounds under LDP are based on the Hadamard Response, a private coin scheme that requires only one bit of communication per user. Under communication constraints we propose public coin schemes based on random hashing functions. Our tight lower bounds are based on recently proposed method of chi squared contractions."}}
{"id": "OGPb-AWo9s_", "cdate": 1622151525356, "mdate": null, "content": {"title": "Interactive Inference under Information Constraints", "abstract": "We study the role of interactivity in distributed statistical inference under information constraints, e.g., communication constraints and local differential privacy. We focus on the tasks of goodness-of-fit testing and estimation of discrete distributions. From prior work, these tasks are well understood under noninteractive protocols. Extending these approaches directly for interactive protocols is difficult due to correlations that can build due to interactivity; in fact, gaps can be found in prior claims of tight bounds of distribution estimation using interactive protocols.\nWe propose a new approach to handle this correlation and establish a unified method to establish lower bounds for both tasks. As an application, we obtain optimal bounds for both estimation and testing under local differential privacy and communication constraints. We also provide an example of a natural testing problem where interactivity helps."}}
{"id": "GfVeFihyLRe", "cdate": 1621629917000, "mdate": null, "content": {"title": "Distributed Estimation with Multiple Samples per User: Sharp Rates and Phase Transition", "abstract": "We obtain tight minimax rates for the problem of distributed estimation of discrete distributions under communication constraints, where $n$ users observing $m $ samples each can broadcast only $\\ell$  bits. Our main result is a tight characterization (up to logarithmic factors) of the error rate as a function of $m$, $\\ell$, the domain size, and the number of users under most regimes of interest. While previous work focused on the setting where each user only holds one sample, we show that as $m$ grows the $\\ell_1$ error rate gets reduced by a factor of $\\sqrt{m}$ for small $m$. However, for large $m$ we observe an interesting phase transition: the dependence of the error rate on the communication constraint $\\ell$ changes from $1/\\sqrt{2^{\\ell}}$ to $1/\\sqrt{\\ell}$."}}
{"id": "pvCLqcsLJ1N", "cdate": 1621629871432, "mdate": null, "content": {"title": "Remember What You Want to Forget: Algorithms for Machine Unlearning", "abstract": "We study the problem of unlearning datapoints from a learnt model. The learner first receives a dataset $S$ drawn i.i.d. from an unknown distribution, and outputs a model $\\widehat{w}$ that performs well on  unseen samples from the same distribution. However, at some point in the future, any training datapoint $z \\in S$ can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees.  We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity. \n\nFor the setting of convex losses, we provide an unlearning algorithm that can unlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In comparison, in general, differentially private learning (which implies unlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This demonstrates a novel separation between differential privacy and machine unlearning. "}}
{"id": "CsV-Gms_JKy", "cdate": 1621629870957, "mdate": null, "content": {"title": "Optimal Rates for Nonparametric Density Estimation under Communication Constraints", "abstract": "We consider density estimation for Besov spaces when the estimator is restricted to use only a limited number of bits about each sample. We provide a noninteractive adaptive estimator which exploits the sparsity of wavelet bases, along with a simulate-and-infer technique from parametric estimation under communication constraints. We show that our estimator is nearly rate-optimal by deriving minmax lower bounds that hold even when interactive protocols are allowed. Interestingly, while our wavelet-based estimator is almost rate-optimal for Sobolev spaces as well, it is unclear whether the standard Fourier basis, which arise naturally for those spaces, can be used to achieve the same performance."}}
{"id": "h7aSBWbX7S4", "cdate": 1621629869731, "mdate": null, "content": {"title": "Information-constrained optimization: can adaptive processing of gradients help?", "abstract": "We revisit first-order optimization under local information constraints such as local privacy, gradient quantization, and computational constraints limiting access to a few coordinates of the gradient. In this setting, the optimization algorithm is not allowed to directly access the complete output of the gradient oracle, but only gets limited information about it subject to the local information constraints.   We study the role of adaptivity in processing the gradient output to obtain this limited information from it, and obtain tight or nearly tight bounds for both convex and strongly convex optimization when adaptive gradient processing is allowed."}}
{"id": "SygsKNBg8B", "cdate": 1567802498544, "mdate": null, "content": {"title": "Estimating Entropy of Distributions in Constant Space", "abstract": "We consider the task of estimating entropy of $k$-ary distributions from samples in the streaming model, where space is limited. Our main contribution is an algorithm that takes $ O\\left( \\frac{k \\log (1/\\varepsilon)^2}{\\varepsilon^3} \\right)$  samples and using $O(1)$ memory words, obtains a $\\pm\\varepsilon$ estimate of $H(p)$. Without space limitations, the sample complexity has been established as $S(k,\\varepsilon)=\\Theta\\left(\\frac k{\\varepsilon\\log k}+\\frac{\\log^2 k}{\\varepsilon^2}\\right)$, which is sub-linear in the domain size $k$. All current sample-optimal estimators also require nearly-linear space in $k$.    Our algorithm partitions $[0,1]$ into intervals and estimates the entropy contribution of probabilities in each interval. The intervals are designed to trade the bias and variance within each interval.    Distribution property estimation and testing with limited memory is a largely unexplored research area. We hope that our work will spur many works in this field."}}
