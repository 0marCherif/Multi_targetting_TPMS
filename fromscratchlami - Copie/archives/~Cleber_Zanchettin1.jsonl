{"id": "xte2Bi-ZXFo", "cdate": 1672531200000, "mdate": 1695977207536, "content": {"title": "Exploring the Impact of Synthetic Data on Human Activity Recognition Tasks", "abstract": ""}}
{"id": "gN6cq0HZ2K", "cdate": 1672531200000, "mdate": 1695977207514, "content": {"title": "Improving small object detection with DETRAug", "abstract": "Small object detection is a challenge for computer vision models due to a shortage of image details, textures, and varying distances from the camera, resulting in objects of different scales and partial occlusion issues. In this paper, we present a new method for enhancing the robustness of image detection models using AUGMIX. Our approach involves applying various augmentations to the input images in a stochastic manner, resulting in a single output image after all transformations have been applied. In addition, we used the Jensen-Shannon loss to maintain a more stable model. In our experiments, we observed a decrease in the number of \u201cno-object\u201d detections, which refers to the detection of unrelated or background objects. The new approach was evaluated using the Deformable DETR, a model known for detecting small objects accurately, and compared to DETR and EfficientDet. We verified an improvement of at least 4.15% using the proposed technique and a more stable loss error."}}
{"id": "fwFIUcMmzG", "cdate": 1672531200000, "mdate": 1695977207513, "content": {"title": "Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication", "abstract": "Individuals with complex communication needs (CCN) often rely on augmentative and alternative communication (AAC) systems to have conversations and communique their wants. Such systems allow message authoring by arranging pictograms in sequence. However, the difficulty of finding the desired item to complete a sentence can increase as the user's vocabulary increases. This paper proposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram prediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus for Brazilian Portuguese to use as a training corpus. We tested different approaches to representing a pictogram for prediction: as a word (using pictogram captions), as a concept (using a dictionary definition), and as a set of synonyms (using related terms). We also evaluated the usage of images for pictogram prediction. The results demonstrate that using embeddings computed from the pictograms' caption, synonyms, or definitions have a similar performance. Using synonyms leads to lower perplexity, but using captions leads to the highest accuracies. This paper provides insight into how to represent a pictogram for prediction using a BERT-like model and the potential of using images for pictogram prediction."}}
{"id": "fIT4gKSLnU", "cdate": 1672531200000, "mdate": 1695977207527, "content": {"title": "Self-calibrated U-Net for Document Segmentation", "abstract": "Based on the need to digitalize identification documents per several institutions and companies, the segmentation task of textual information acquired importance. Commonly, Convolutional Neural Networks are applied to solve such problems. Vanilla convolution in deep learning models can provide sub-optimal performance in some learning tasks, such as semantic segmentation. Due to this, more specific proposals may be applied to better fit this context, such as self-calibrated convolutions, which consider unique characteristics of different feature maps. Considering this context, we propose a new fully convolutional network architecture based on U-Net for segmentation associated with self-calibrated convolution. We consider changing all vanilla convolution layers in this new neural network by self-calibrating convolutions. In this work, we evaluate our proposal in three different tasks of document segmentation. All experiments demonstrate an increase in the proposed neural network's segmentation performance compared with traditional U-Net in the analyzed context, but the inference time performance decreases."}}
{"id": "_X9UtIAlos", "cdate": 1672531200000, "mdate": 1695977207529, "content": {"title": "Learning What, Where and Which to Transfer", "abstract": "Deep learning models often require large datasets to perform well from scratch. Transfer learning methods solve this issue by using a pre-trained source network to improve a target network training. Recent approaches involve using feature maps from the source network to guide the target network training. The latest transfer learning methods use meta-networks to enhance the knowledge transfer process. These meta-networks bridge the source and target networks, deciding which pairs of feature map layers and channels should be matched for optimal knowledge transfer. This paper improves this approach by using pixel-level information, in addition to layers and channels, for better knowledge transfer. Our experiments on multiple datasets show that the proposed approach outperforms previous baselines in scenarios with limited labels per class. The source code is available at https://github.com/lucasdelimanogueira/L2T-www."}}
{"id": "9SNyVbEuor", "cdate": 1672531200000, "mdate": 1695977207529, "content": {"title": "Image Classification Understanding with Model Inspector Tool", "abstract": "This paper proposes a novel method called U Analysis for interpreting the behavior of image classification models. The method allows the evaluation of the interdependence between patches of information in an image and their impact on the model\u2019s classification performance. In addition, the paper introduces the Model Inspector tool that allows users to manipulate various visual features of an input image to understand better the model\u2019s robustness to different types of information. This work aims to provide a more comprehensive framework for model interpretation and help researchers and practitioners better understand the strengths and weaknesses of deep learning models in image classification. We perform experiments with CIFAR-10 and STL-10 datasets using the ResNet architecture. The findings show that ResNet model trained with CIFAR-10 and STL-10 presents counter-intuitive feature interdependence, which is seen as a weakness. This work can contribute to developing even more advanced tools for analyzing and understanding deep learning models."}}
{"id": "rtbgOV07M79", "cdate": 1648668208469, "mdate": 1648668208469, "content": {"title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples", "abstract": "In this article, we argue that the unsatisfactory out-of-distribution (OOD) detection performance of neural networks is mainly due to the SoftMax loss anisotropy and propensity to produce low entropy probability distributions in disagreement with the principle of maximum entropy. On the one hand, current OOD detection approaches usually do not directly fix the SoftMax loss drawbacks, but rather build techniques to circumvent it. Unfortunately, those methods usually produce undesired side effects (e.g., classification accuracy drop, additional hyperparameters, slower inferences, and collecting extra data). On the other hand, we propose replacing SoftMax loss with a novel loss function that does not suffer from the mentioned weaknesses. The proposed IsoMax loss is isotropic (exclusively distance-based) and provides high entropy posterior probability distributions. Replacing the SoftMax loss by IsoMax loss requires no model or training changes. Additionally, the models trained with IsoMax loss produce as fast and energy-efficient inferences as those trained using SoftMax loss. Moreover, no classification accuracy drop is observed. The proposed method does not rely on outlier/background data, hyperparameter tuning, temperature calibration, feature extraction, metric learning, adversarial training, ensemble procedures, or generative models. Our experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in replacement that significantly improves neural networks' OOD detection performance. Hence, it may be used as a baseline OOD detection approach to be combined with current or future OOD detection techniques to achieve even higher results."}}
{"id": "tezjgWJJou4", "cdate": 1640995200000, "mdate": 1682416588227, "content": {"title": "Unsupervised Multi-view Multi-person 3D Pose Estimation Using Reprojection Error", "abstract": "This work addresses multi-view multi-person 3D pose estimation in synchronized and calibrated camera views. Recent approaches estimate neural network weights in a supervised way; they rely on ground truth annotated datasets to compute the loss function and optimize the weights in the network. However, manually labeling ground truth datasets is labor-intensive, expensive, and prone to errors. Consequently, it is preferable not to rely heavily on labeled datasets. This work proposes an unsupervised approach to estimating 3D human poses requiring only an off-the-shelf 2D pose estimation method and the intrinsic and extrinsic camera parameters. Our approach uses reprojection error as a loss function instead of comparing the predicted 3D pose with the ground truth. First, we estimate the 3D pose of each person using the plane sweep stereo approach, in which the depth of each 2D joint related to each person is estimated in a selected target view. The estimated 3D pose is then projected onto each of the other views using camera parameters. Finally, the 2D reprojection error in the image plane is computed by comparing it with the estimated 2D pose corresponding to the same person. The 2D poses that correspond to the same person are identified using virtual depth planes, where each 3D pose is projected onto the reference view and compared to find the nearest 2D pose. Our proposed method learns to estimate 3D pose in an end-to-end unsupervised manner and does not require any manual parameter tuning, yet we achieved results close to state-of-the-art supervised methods on a public dataset. Our method achieves only 5.8% points below the fully supervised state-of-the-art method and only 5.1% points below the best geometric approach in the Campus dataset."}}
{"id": "reW7c6mfm5", "cdate": 1640995200000, "mdate": 1648668042667, "content": {"title": "ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign Language", "abstract": "Sign language is an essential resource enabling access to communication and proper socioemotional development for individuals suffering from disabling hearing loss. As this population is expected to reach 700 million by 2050, the importance of the language becomes even more essential as it plays a critical role to ensure the inclusion of such individuals in society. The Sign Language Recognition field aims to bridge the gap between users and non-users of sign languages. However, the scarcity in quantity and quality of datasets is one of the main challenges limiting the exploration of novel approaches that could lead to significant advancements in this research area. Thus, this paper contributes by introducing two new datasets for the American Sign Language: the first is composed of the three-dimensional representation of the signers and, the second, by an unprecedented linguistics-based representation containing a set of phonological attributes of the signs."}}
{"id": "pYsAj-fjODh", "cdate": 1640995200000, "mdate": 1682416588343, "content": {"title": "Training Aware Sigmoidal Optimization", "abstract": "Proper optimization of deep neural networks is an open research question since an optimal procedure to change the learning rate throughout training is still unknown. Manually defining a learning rate schedule involves troublesome, time-consuming try and error procedures to determine hyperparameters such as learning rate decay epochs and learning rate decay rates. Although adaptive learning rate optimizers automatize this process, recent studies suggest they may produce overfitting and reduce performance compared to fine-tuned learning rate schedules. Considering that deep neural networks loss functions present landscapes with much more saddle points than local minima, we proposed the Training Aware Sigmoidal Optimizer (TASO), consisting of a two-phase automated learning rate schedule. The first phase uses a high learning rate to fast traverse the numerous saddle point, while the second phase uses a low learning rate to approach the center of the local minimum previously found slowly. We compared the proposed approach with commonly used adaptive learning rates schedules such as Adam, RMSProp, and Adagrad. Our experiments showed that TASO outperformed all competing methods in both optimal (i.e., performing hyperparameter validation) and suboptimal (i.e., using default hyperparameters) scenarios."}}
