{"id": "rRDr9z2LrJq", "cdate": 1683882553074, "mdate": 1683882553074, "content": {"title": "An Interpretable Deep Reinforcement Learning Approach to Autonomous Driving", "abstract": "This paper investigates a deep reinforcement learn\u0002ing (RL) approach for autonomous driving. Since interpretability is essential for such a high-stake do\u0002main, in contrast to previous deep RL work, we exploit a recent neuro-symbolic model called dif\u0002ferentiable logic machine to learn an interpretable controller in the form of a first-order logic pro\u0002gram. As a proof of concept, we demonstrate the feasibility of our approach on two classical decision-making scenarios in autonomous driving: lane changing and intersection management. Our preliminary results obtained in a simple simula\u0002tor suggest that learning an interpretable controller does not penalize performance. Moreover, since\nthe controller is a logic program, it is understand\u0002able and is amenable to analysis."}}
{"id": "7CrXRhmzVVR", "cdate": 1655376335659, "mdate": null, "content": {"title": "Solving Complex Manipulation Tasks with Model-Assisted Model-Free Reinforcement Learning", "abstract": "In this paper, we propose a novel deep reinforcement learning approach for improving the sample efficiency of a model-free actor-critic method by using a learned model to encourage exploration. The basic idea consists in generating artificial transitions with noisy actions, which can be used to update the critic. To counteract the model bias, we introduce a high initialization for the critic and two filters for the artificial transitions. Finally, we evaluate our approach with the TD3 algorithm on different robotic tasks and demonstrate that it achieves a better performance with higher sample efficiency than several other model-based and model-free methods."}}
{"id": "TLnReGgZEdW", "cdate": 1632875662312, "mdate": null, "content": {"title": "Generalization in Deep RL for TSP Problems via Equivariance and Local Search", "abstract": "Deep reinforcement learning (RL) has proved to be a competitive heuristic for solving small-sized instances of traveling salesman problems (TSP), but its performance on larger-sized instances is insufficient. Since training on large instances is impractical, we design a novel deep RL approach with a focus on generalizability. Our proposition consisting of a simple deep learning architecture that learns with novel RL training techniques exploits two main ideas. First, we exploit equivariance to facilitate training. Second, we interleave efficient local search heuristics with the usual RL training to smooth the value landscape. In order to validate the whole approach, we empirically evaluate our proposition on random and realistic TSP problems against relevant state-of-the-art deep RL methods. Moreover, we present an ablation study to understand the contribution of each of its components."}}
{"id": "M_gk45ItxIp", "cdate": 1601308398231, "mdate": null, "content": {"title": "Interpretable Reinforcement Learning With Neural Symbolic Logic", "abstract": "Recent progress in deep reinforcement learning (DRL) can be largely attributed to the use of neural networks.  However, this black-box approach fails to explain the learned policy in a human understandable way.  To address this challenge and improve the transparency, we introduce symbolic logic into DRL and propose a Neural Symbolic Reinforcement Learning framework, in which states and actions are represented in an interpretable way using first-order logic.  This framework features a relational reasoning module, which performs on task-level in Hierarchical Reinforcement Learning, enabling end-to-end learning with prior symbolic knowledge.  Moreover, interpretability is enabled by extracting the logical rules learned by the reasoning module in a symbolic rule space, providing explainability on task level. Experimental results demonstrate better interpretability of subtasks, along with competing performance compared with existing approaches."}}
{"id": "Hk-XIMW_-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems", "abstract": "Social recommendation leverages social information to solve data sparsity and cold-start problems in traditional collaborative filtering methods. However, most existing models assume that social effects from friend users are static and under the forms of constant weights or fixed constraints. To relax this strong assumption, in this paper, we propose dual graph attention networks to collaboratively learn representations for two-fold social effects, where one is modeled by a user-specific attention weight and the other is modeled by a dynamic and context-aware attention weight. We also extend the social effects in user domain to item domain, so that information from related items can be leveraged to further alleviate the data sparsity problem. Furthermore, considering that different social effects in two domains could interact with each other and jointly influence users' preferences for items, we propose a new policy-based fusion strategy based on contextual multi-armed bandit to weigh interactions of various social effects. Experiments on one benchmark dataset and a commercial dataset verify the efficacy of the key components in our model. The results show that our model achieves great improvement for recommendation accuracy compared with other state-of-the-art social recommendation methods."}}
{"id": "HJbVSyWdbH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Optimizing Quantiles in Preference-Based Markov Decision Processes", "abstract": "In the Markov decision process model, policies are usually evaluated by expected cumulative rewards. As this decision criterion is not always suitable, we propose in this paper an algorithm for computing a policy optimal for the quantile criterion. Both finite and infinite horizons are considered. Finally we experimentally evaluate our approach on random MDPs and on a data center control problem."}}
{"id": "HJ-w1sWuWB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Multi-objective Bandits: Optimizing the Generalized Gini Index", "abstract": "We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to find a polic..."}}
{"id": "rkbwAXzO-B", "cdate": 1420070400000, "mdate": null, "content": {"title": "Solving MDPs with Skew Symmetric Bilinear Utility Functions", "abstract": "In this paper we adopt Skew Symmetric Bilinear (SSB) utility functions to compare policies in Markov Decision Processes (MDPs). By considering pairs of alternatives, SSB utility theory generalizes von Neumann and Morgenstern's expected utility (EU) theory to encompass rational decision behaviors that EU cannot accommodate. We provide a game-theoretic analysis of the problem of identifying an SSB-optimal policy in finite horizon MDPs and propose an algorithm based on a double oracle approach for computing an optimal (possibly randomized) policy. Finally, we present and discuss experimental results where SSB-optimal policies are computed for a popular TV contest according to several instantiations of SSB utility functions."}}
{"id": "H1-BGrzubB", "cdate": 1420070400000, "mdate": null, "content": {"title": "Optimization of Probabilistic Argumentation with Markov Decision Models", "abstract": "One prominent way to deal with conflicting viewpoints among agents is to conduct an argumentative debate: by exchanging arguments, agents can seek to persuade each other. In this paper we investigate the problem, for an agent, of optimizing a sequence of moves to be put forward in a debate, against an opponent assumed to behave stochastically, and equipped with an unknown initial belief state. Despite the prohibitive number of states induced by a naive mapping to Markov models, we show that exploiting several features of such interaction settings allows for optimal resolution in practice, in particular: (1) as debates take place in a public space (or common ground), they can readily be modelled as Mixed Observability Markov Decision Processes, (2) as argumentation problems are highly structured, one can design optimization techniques to prune the initial instance. We report on the experimental evaluation of these techniques."}}
{"id": "Bk481hZd-S", "cdate": 1420070400000, "mdate": null, "content": {"title": "Qualitative Multi-Armed Bandits: A Quantile-Based Approach", "abstract": "We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative sca..."}}
