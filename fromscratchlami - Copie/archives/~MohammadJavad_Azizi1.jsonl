{"id": "VsBO2cT0ZMe", "cdate": 1640995200000, "mdate": 1664053377417, "content": {"title": "Non-stationary Bandits and Meta-Learning with a Small Set of Optimal Arms", "abstract": "We study a sequential decision problem where the learner faces a sequence of $K$-armed stochastic bandit tasks. An adversary may design the tasks, but the adversary is constrained to choose the optimal arm of each task in a smaller (but unknown) subset of $M$ arms. The task boundaries might be known (the bandit meta-learning setting), or unknown (the non-stationary bandit setting). We design an algorithm based on a reduction to bandit submodular maximization and show that, in the regime of large number of tasks and small number of optimal arms, its regret in both settings is smaller than the simple baseline of $\\tilde{O}(\\sqrt{KNT})$ that can be obtained by using standard algorithms designed for non-stationary bandit problems. For the bandit meta-learning problem with fixed task length $\\tau$, we show that the regret of the algorithm is bounded as $\\tilde{O}(NM\\sqrt{M \\tau}+N^{2/3}M\\tau)$. Under additional assumptions on the identifiability of the optimal arms in each task, we show a bandit meta-learning algorithm with an improved $\\tilde{O}(N\\sqrt{M \\tau}+N^{1/2}\\sqrt{M K \\tau})$ regret."}}
{"id": "6_faM7eR_d", "cdate": 1640995200000, "mdate": 1664053377412, "content": {"title": "Fixed-Budget Best-Arm Identification in Structured Bandits", "abstract": "Best-arm identification (BAI) in a fixed-budget setting is a bandit problem where the learning agent maximizes the probability of identifying the optimal (best) arm after a fixed number of observations. Most works on this topic study unstructured problems with a small number of arms, which limits their applicability. We propose a general tractable algorithm that incorporates the structure, by successively eliminating suboptimal arms based on their mean reward estimates from a joint generalization model. We analyze our algorithm in linear and generalized linear models (GLMs), and propose a practical implementation based on a G-optimal design. In linear models, our algorithm has competitive error guarantees to prior works and performs at least as well empirically. In GLMs, this is the first practical algorithm with analysis for fixed-budget BAI."}}
{"id": "6NP4quW8Lq", "cdate": 1640995200000, "mdate": 1664053377411, "content": {"title": "Meta-Learning for Simple Regret Minimization", "abstract": "We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist algorithms for this meta-learning problem. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\\tilde{O}(m / \\sqrt{n})$. This is while we show that the meta simple regret of the frequentist algorithm is $\\tilde{O}(\\sqrt{m} n + m/ \\sqrt{n})$, and thus, worse. However, the algorithm is more general, because it does not need a prior distribution over the meta-parameters, and is easier to implement for various distributions. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them empirically in several environments."}}
{"id": "fbp-1nsyDw", "cdate": 1609459200000, "mdate": 1671863574767, "content": {"title": "A robust simulation optimization algorithm using kriging and particle swarm optimization: Application to surgery room optimization", "abstract": "Simulation optimization is an endeavor to determine the best combination of inputs that result in the best system performance criterion without evaluating all possible combinations. Since simulatio..."}}
{"id": "WwTFWgIwfF", "cdate": 1609459200000, "mdate": 1671863574779, "content": {"title": "Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed Bandits: Simple Sequential Elimination Algorithms", "abstract": "We consider the problem of finding, through adaptive sampling, which of $n$ options (arms) has the largest mean. Our objective is to determine a rule which identifies the best arm with a fixed minimum confidence using as few observations as possible, i.e. this is a fixed-confidence (FC) best arm identification (BAI) in multi-armed bandits. We study such problems under the Bayesian setting with both Bernoulli and Gaussian arms. We propose to use the classical \"vector at a time\" (VT) rule, which samples each remaining arm once in each round. We show how VT can be implemented and analyzed in our Bayesian setting and be improved by early elimination. Our analysis show that these algorithms guarantee an optimal strategy under the prior. We also propose and analyze a variant of the classical \"play the winner\" (PW) algorithm. Numerical results show that these rules compare favorably with state-of-art algorithms."}}
{"id": "SUcbIR9tdgc", "cdate": 1609459200000, "mdate": 1645939405758, "content": {"title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm", "abstract": "Best-arm identification (BAI) in a fixed-budget setting is a bandit problem where the learning agent maximizes the probability of identifying the best arm after a fixed number of observations. Most works on this topic study unstructured problems with a small number of arms, which limits their applicability. We propose a general tractable algorithm that incorporates the structure, by successively eliminating suboptimal arms based on their mean reward estimates from a joint generalization model. We analyze our algorithm in linear and generalized linear models (GLMs) and propose a practical implementation based on a G-optimal design. In linear models, our algorithm has competitive regret guarantees to prior works and performs at least as well empirically. In GLMs, this is the first practical algorithm with analysis for fixed-budget BAI."}}
{"id": "PR5w3EOyNJR", "cdate": 1609459200000, "mdate": 1671863574786, "content": {"title": "A data-driven robust optimization algorithm for black-box cases: An application to hyper-parameter optimization of machine learning algorithms", "abstract": ""}}
{"id": "oJG5d-69LV3", "cdate": 1546300800000, "mdate": 1671863574767, "content": {"title": "Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making", "abstract": "In recent years, automated data-driven decision-making systems have enjoyed a tremendous success in a variety of fields (e.g., to make product recommendations, or to guide the production of entertainment). More recently, these algorithms are increasingly being used to assist socially sensitive decision-making (e.g., to decide who to admit into a degree program or to prioritize individuals for public housing). Yet, these automated tools may result in discriminative decision-making in the sense that they may treat individuals unfairly or unequally based on membership to a category or a minority, resulting in disparate treatment or disparate impact and violating both moral and ethical standards. This may happen when the training dataset is itself biased (e.g., if individuals belonging to a particular group have historically been discriminated upon). However, it may also happen when the training dataset is unbiased, if the errors made by the system affect individuals belonging to a category or minority differently (e.g., if misclassification rates for Blacks are higher than for Whites). In this paper, we unify the definitions of unfairness across classification and regression. We propose a versatile mixed-integer optimization framework for learning optimal and fair decision trees and variants thereof to prevent disparate treatment and/or disparate impact as appropriate. This translates to a flexible schema for designing fair and interpretable policies suitable for socially sensitive decision-making. We conduct extensive computational studies that show that our framework improves the state-of-the-art in the field (which typically relies on heuristics) to yield non-discriminative decisions at lower cost to overall accuracy."}}
{"id": "_rjaZvYax4", "cdate": 1546300800000, "mdate": 1671863574767, "content": {"title": "Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making", "abstract": "In recent years, automated data-driven decision-making systems have enjoyed a tremendous success in a variety of fields (e.g., to make product recommendations, or to guide the production of entertainment). More recently, these algorithms are increasingly being used to assist socially sensitive decisionmaking (e.g., to decide who to admit into a degree program or to prioritize individuals for public housing). Yet, these automated tools may result in discriminative decision-making in the sense that they may treat individuals unfairly or unequally based on membership to a category or a minority, resulting in disparate treatment or disparate impact and violating both moral and ethical standards. This may happen when the training dataset is itself biased (e.g., if individuals belonging to a particular group have historically been discriminated upon). However, it may also happen when the training dataset is unbiased, if the errors made by the system affect individuals belonging to a category or minority differently (e.g., if misclassification rates for Blacks are higher than for Whites). In this paper, we unify the definitions of unfairness across classification and regression. We propose a versatile mixed-integer optimization framework for learning optimal and fair decision trees and variants thereof to prevent disparate treatment and/or disparate impact as appropriate. This translates to a flexible schema for designing fair and interpretable policies suitable for socially sensitive decision-making. We conduct extensive computational studies that show that our framework improves the state-of-the-art in the field (which typically relies on heuristics) to yield non-discriminative decisions at lower cost to overall accuracy."}}
{"id": "BThP7MvCL7o", "cdate": 1514764800000, "mdate": null, "content": {"title": "Designing Fair, Efficient, and Interpretable Policies for Prioritizing Homeless Youth for Housing Resources", "abstract": "We consider the problem of designing fair, efficient, and interpretable policies for prioritizing heterogeneous homeless youth on a waiting list for scarce housing resources of different types. We focus on point-based policies that use features of the housing resources (e.g., permanent supportive housing, rapid rehousing) and the youth (e.g., age, history of substance use) to maximize the probability that the youth will have a safe and stable exit from the housing program. The policies can be used to prioritize waitlisted youth each time a housing resource is procured. Our framework provides the policy-maker the flexibility to select both their desired structure for the policy and their desired fairness requirements. Our approach can thus explicitly trade-off interpretability and efficiency while ensuring that fairness constraints are met. We propose a flexible data-driven mixed-integer optimization formulation for designing the policy, along with an approximate formulation which can be solved efficiently for broad classes of interpretable policies using Bender\u2019s decomposition. We evaluate our framework using real-world data from the United States homeless youth housing system. We show that our framework results in policies that are more fair than the current policy in place and than classical interpretable machine learning approaches while achieving a similar (or higher) level of overall efficiency."}}
