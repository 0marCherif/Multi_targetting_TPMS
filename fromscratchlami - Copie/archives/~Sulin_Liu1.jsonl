{"id": "BsFAPzJlK4a", "cdate": 1672531200000, "mdate": 1695957670922, "content": {"title": "Sparse Bayesian optimization", "abstract": "Bayesian optimization (BO) is a powerful approach to sample-efficient optimization of black-box objective functions. However, the application of BO to areas such as recommendation systems often req..."}}
{"id": "UEcOAuxTZry", "cdate": 1650211523691, "mdate": 1650211523691, "content": {"title": "ProBF: Learning Probabilistic Safety Certificates with Barrier Functions", "abstract": "Safety-critical applications require controllers/policies that can guarantee safety\nwith high confidence. The control barrier function is a useful tool to guarantee\nsafety if we have access to the ground-truth system dynamics. In practice, we\nhave inaccurate knowledge of the system dynamics, which can lead to unsafe\nbehaviors due to unmodeled residual dynamics. Learning the residual dynamics\nwith deterministic machine learning models can prevent the unsafe behavior but can\nfail when the predictions are imperfect. In this situation, a probabilistic learning\nmethod that reasons about the uncertainty of its predictions can help provide\nrobust safety margins. In this work, we use a Gaussian process to model the\nprojection of the residual dynamics onto a control barrier function. We propose a\nnovel optimization procedure to generate safe controls that can guarantee safety\nwith high probability. The safety filter is provided with the ability to reason\nabout the uncertainty of the predictions from the GP. We show the efficacy of this\nmethod through experiments on Segway and Quadrotor simulations. Our proposed\nprobabilistic approach is able to reduce the number of safety violations significantly\nas compared to the deterministic approach with a neural network."}}
{"id": "0OFkcaQ4I7l", "cdate": 1640995200000, "mdate": 1664555073139, "content": {"title": "Sparse Bayesian Optimization", "abstract": "Bayesian optimization (BO) is a powerful approach to sample-efficient optimization of black-box objective functions. However, the application of BO to areas such as recommendation systems often requires taking the interpretability and simplicity of the configurations into consideration, a setting that has not been previously studied in the BO literature. To make BO applicable in this setting, we present several regularization-based approaches that allow us to discover sparse and more interpretable configurations. We propose a novel differentiable relaxation based on homotopy continuation that makes it possible to target sparsity by working directly with $L_0$ regularization. We identify failure modes for regularized BO and develop a hyperparameter-free method, sparsity exploring Bayesian optimization (SEBO) that seeks to simultaneously maximize a target objective and sparsity. SEBO and methods based on fixed regularization are evaluated on synthetic and real-world problems, and we show that we are able to efficiently optimize for sparsity."}}
{"id": "r4pzzbfPOR9", "cdate": 1609459200000, "mdate": 1664555073048, "content": {"title": "ProBF: Learning Probabilistic Safety Certificates with Barrier Functions", "abstract": "Safety-critical applications require controllers/policies that can guarantee safety with high confidence. The control barrier function is a useful tool to guarantee safety if we have access to the ground-truth system dynamics. In practice, we have inaccurate knowledge of the system dynamics, which can lead to unsafe behaviors due to unmodeled residual dynamics. Learning the residual dynamics with deterministic machine learning models can prevent the unsafe behavior but can fail when the predictions are imperfect. In this situation, a probabilistic learning method that reasons about the uncertainty of its predictions can help provide robust safety margins. In this work, we use a Gaussian process to model the projection of the residual dynamics onto a control barrier function. We propose a novel optimization procedure to generate safe controls that can guarantee safety with high probability. The safety filter is provided with the ability to reason about the uncertainty of the predictions from the GP. We show the efficacy of this method through experiments on Segway and Quadrotor simulations. Our proposed probabilistic approach is able to reduce the number of safety violations significantly as compared to the deterministic approach with a neural network."}}
{"id": "fx_Hkso6J6", "cdate": 1577836800000, "mdate": 1664555072982, "content": {"title": "The Landscape of Matrix Factorization Revisited", "abstract": "We revisit the landscape of the simple matrix factorization problem. For low-rank matrix factorization, prior work has shown that there exist infinitely many critical points all of which are either global minima or strict saddles. At a strict saddle the minimum eigenvalue of the Hessian is negative. Of interest is whether this minimum eigenvalue is uniformly bounded below zero over all strict saddles. To answer this we consider orbits of critical points under the general linear group. For each orbit we identify a representative point, called a canonical point. If a canonical point is a strict saddle, so is every point on its orbit. We derive an expression for the minimum eigenvalue of the Hessian at each canonical strict saddle and use this to show that the minimum eigenvalue of the Hessian over the set of strict saddles is not uniformly bounded below zero. We also show that a known invariance property of gradient flow ensures the solution of gradient flow only encounters critical points on an invariant manifold $\\mathcal{M}_C$ determined by the initial condition. We show that, in contrast to the general situation, the minimum eigenvalue of strict saddles in $\\mathcal{M}_{0}$ is uniformly bounded below zero. We obtain an expression for this bound in terms of the singular values of the matrix being factorized. This bound depends on the size of the nonzero singular values and on the separation between distinct nonzero singular values of the matrix."}}
{"id": "V7fJfE5knE3J", "cdate": 1577836800000, "mdate": null, "content": {"title": "Revisiting the Landscape of Matrix Factorization", "abstract": "Prior work has shown that low-rank matrix factorization has infinitely many critical points, each of which is either a global minimum or a (strict) saddle point. We revisit this problem and provide..."}}
{"id": "1Q_GSl522wV", "cdate": 1577836800000, "mdate": null, "content": {"title": "Task-Agnostic Amortized Inference of Gaussian Process Hyperparameters", "abstract": "Gaussian processes (GPs) are flexible priors for modeling functions. However, their success depends on the kernel accurately reflecting the properties of the data. One of the appeals of the GP framework is that the marginal likelihood of the kernel hyperparameters is often available in closed form, enabling optimization and sampling procedures to fit these hyperparameters to data. Unfortunately, point-wise evaluation of the marginal likelihood is expensive due to the need to solve a linear system; searching or sampling the space of hyperparameters thus often dominates the practical cost of using GPs. We introduce an approach to the identification of kernel hyperparameters in GP regression and related problems that sidesteps the need for costly marginal likelihoods. Our strategy is to \"amortize\" inference over hyperparameters by training a single neural network, which consumes a set of regression data and produces an estimate of the kernel function, useful across different tasks. To accommodate the varying dimension and cardinality of different regression problems, we use a hierarchical self-attention-based neural network that produces estimates of the hyperparameters which are invariant to the order of the input data points and data dimensions. We show that a single neural model trained on synthetic data is able to generalize directly to several different unseen real-world GP use cases. Our experiments demonstrate that the estimated hyperparameters are comparable in quality to those from the conventional model selection procedures, while being much faster to obtain, significantly accelerating GP regression and its related applications such as Bayesian optimization and Bayesian quadrature. The code and pre-trained model are available at https://github.com/PrincetonLIPS/AHGP."}}
{"id": "rkV9GeZ_-B", "cdate": 1514764800000, "mdate": null, "content": {"title": "Data Poisoning Attacks on Multi-Task Relationship Learning", "abstract": "Multi-task learning (MTL) is a machine learning paradigm that improves the performance of each task by exploiting useful information contained in multiple related tasks. However, the relatedness of tasks can be exploited by attackers to launch data poisoning attacks, which has been demonstrated a big threat to single-task learning. In this paper, we provide the first study on the vulnerability of MTL. Specifically, we focus on multi-task relationship learning (MTRL) models, a popular subclass of MTL models where task relationships are quantized and are learned directly from training data. We formulate the problem of computing optimal poisoning attacks on MTRL as a bilevel program that is adaptive to arbitrary choice of target tasks and attacking tasks. We propose an efficient algorithm called PATOM for computing optimal attack strategies. PATOM leverages the optimality conditions of the subproblem of MTRL to compute the implicit gradients of the upper level objective function. Experimental results on real-world datasets show that MTRL models are very sensitive to poisoning attacks and the attacker can significantly degrade the performance of target tasks, by either directly poisoning the target tasks or indirectly poisoning the related tasks exploiting the task relatedness. We also found that the tasks being attacked are always strongly correlated, which provides a clue for defending against such attacks."}}
{"id": "x8SNW1E5Ug", "cdate": 1483228800000, "mdate": 1664555073049, "content": {"title": "Communication-Efficient Distributed Primal-Dual Algorithm for Saddle Point Problem", "abstract": ""}}
{"id": "Hk-dVEW_bB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Distributed Multi-Task Relationship Learning", "abstract": "Multi-task learning aims to learn multiple tasks jointly by exploiting their relatedness to improve the generalization performance for each task. Traditionally, to perform multi-task learning, one needs to centralize data from all the tasks to a single machine. However, in many real-world applications, data of different tasks may be geo-distributed over different local machines. Due to heavy communication caused by transmitting the data and the issue of data privacy and security, it is impossible to send data of different task to a master machine to perform multi-task learning. Therefore, in this paper, we propose a distributed multi-task learning framework that simultaneously learns predictive models for each task as well as task relationships between tasks alternatingly in the parameter server paradigm. In our framework, we first offer a general dual form for a family of regularized multi-task relationship learning methods. Subsequently, we propose a communication-efficient primal-dual distributed optimization algorithm to solve the dual problem by carefully designing local subproblems to make the dual problem decomposable. Moreover, we provide a theoretical convergence analysis for the proposed algorithm, which is specific for distributed multi-task relationship learning. We conduct extensive experiments on both synthetic and real-world datasets to evaluate our proposed framework in terms of effectiveness and convergence."}}
