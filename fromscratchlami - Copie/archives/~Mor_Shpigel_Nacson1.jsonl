{"id": "xtbog7cfsr", "cdate": 1663850272178, "mdate": null, "content": {"title": "The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks", "abstract": "We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother' predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU networks are universal approximators, we prove that stable shallow networks are not. Namely, there is a function that cannot be well-approximated by stable single hidden-layer ReLU networks trained with a non-vanishing step size. This is while the same function can be realized as a stable two hidden-layer ReLU network. Finally, we prove that if a function is sufficiently smooth (in a Sobolev sense) then it can be approximated arbitrarily well using shallow ReLU networks that correspond to stable solutions of gradient descent."}}
{"id": "D1l9g3fRqc5", "cdate": 1609459200000, "mdate": null, "content": {"title": "On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent", "abstract": "Recent work has highlighted the role of initialization scale in determining the structure of the solutions that gradient methods converge to. In particular, it was shown that large initialization leads to the neural tangent kernel regime solution, whereas small initialization leads to so called \"rich regimes\". However, the initialization structure is richer than the overall scale alone and involves relative magnitudes of different weights and layers in the network. Here we show that these relative scales, which we refer to as initialization shape, play an important role in determining the learned model. We develop a novel technique for deriving the inductive bias of gradient-flow and use it to obtain closed-form implicit regularizers for multiple cases of interest."}}
{"id": "Bkeb7lHtvH", "cdate": 1569439768693, "mdate": null, "content": {"title": "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "abstract": "Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods.\nContributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability.  We provide empirical experiments to validate our theoretical findings."}}
{"id": "j7fnxUctfOQ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate", "abstract": "Stochastic Gradient Descent (SGD) is a central tool in machine learning. We prove that SGD converges to zero loss, even with a fixed (non-vanishing) learning rate \u2014 in the special case of homogeneo..."}}
{"id": "SJ-9Io-O-B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models", "abstract": "With an eye toward understanding complexity control in deep learning, we study how infinitesimal regularization or gradient descent optimization lead to margin maximizing solutions in both homogene..."}}
{"id": "BDmV1qeDd1J", "cdate": 1546300800000, "mdate": null, "content": {"title": "Convergence of Gradient Descent on Separable Data", "abstract": "We provide a detailed study on the implicit bias of gradient descent when optimizing loss functions with strictly monotone tails, such as the logistic loss, over separable datasets. We look at two ..."}}
{"id": "r1q7n9gAb", "cdate": 1518730180582, "mdate": null, "content": {"title": "The Implicit Bias of Gradient Descent on Separable Data", "abstract": "We show that gradient descent on an unregularized logistic regression\nproblem, for almost all separable datasets, converges to the same direction as the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore,\nwe show this convergence is very slow, and only logarithmic in the\nconvergence of the loss itself. This can help explain the benefit\nof continuing to optimize the logistic or cross-entropy loss even\nafter the training error is zero and the training loss is extremely\nsmall, and, as we show, even if the validation loss increases. Our\nmethodology can also aid in understanding implicit regularization\nin more complex models and with other optimization methods. "}}
