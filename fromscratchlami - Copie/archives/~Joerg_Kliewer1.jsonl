{"id": "hniMaX6I3mr", "cdate": 1609459200000, "mdate": null, "content": {"title": "Error correction for low power sensors in asynchronous communication", "abstract": "Highlights \u2022 Novel forward error correction schemes for sensor communication based on these sampling strategiesasynchronous sampling strategies, i.e., sampling via level crossing (LC) sampling and time encoding (TE), are proposed, where the dominant errors consist of pulse deletions and insertions, and where encoding is required to take place in an instantaneous fashion. \u2022 For LC sampling, concatenated inner and outer codes and a maximum a-posteriori (MAP) decoder are employed to provide performance gain. Rate trade-off between inner and outer codes is analyzed. Residual redundancy in the asynchronously sampled source signal can be successfully exploited to provide performance gain with less complexity. \u2022 For time encoding, timing position quantization scheme with low complexity is proposed for time encoding (TE) to provide performance gain. Abstract We propose a forward error correction scheme for asynchronous sampling via level crossing (LC) sampling and time encoding, where the dominant errors consist of pulse deletions and insertions, and where encoding is required to take place in an instantaneous fashion. For LC sampling the presented scheme consists of a combination of an outer systematic convolutional code, an embedded inner marker code, and power-efficient frequency-shift keying modulation at the sensor node. Decoding is first obtained via a maximum a-posteriori (MAP) decoder for the inner marker code, which achieves synchronization for the insertion and deletion channel, followed by MAP decoding for the outer convolutional code. Besides investigating the rate trade-off between marker and convolutional codes, we also show that residual redundancy in the asynchronously sampled source signal can be successfully exploited in combination with redundancy only from a marker code. This provides a low complexity alternative for deletion and insertion error correction compared to using explicit redundancy. For time encoding, only the pulse timing is of relevance at the receiver, and the outer channel code is replaced by a quantizer to represent the relative position of the pulse timing. Numerical simulations show that LC sampling outperforms time encoding in the low to moderate signal-to-noise ratio regime by a large margin."}}
{"id": "fct2gEYOqx", "cdate": 1609459200000, "mdate": null, "content": {"title": "Information Leakage in Zero-Error Source Coding: A Graph-Theoretic Perspective", "abstract": "We study the information leakage to a guessing adversary in zero-error source coding. The source coding problem is defined by a confusion graph capturing the distinguishability between source symbols. The information leakage is measured by the ratio of the adversary's successful guessing probability after and before eavesdropping the codeword, maximized over all possible source distributions. Such measurement under the basic adversarial model where the adversary makes a single guess and allows no distortion between its estimator and the true sequence is known as the maximum min-entropy leakage or the maximal leakage in the literature. We develop a single-letter characterization of the optimal normalized leakage under the basic adversarial model, together with an optimum-achieving scalar stochastic mapping scheme. An interesting observation is that the optimal normalized leakage is equal to the optimal compression rate with fixed-length source codes, both of which can be simultaneously achieved by some deterministic coding schemes. We then extend the leakage measurement to generalized adversarial models where the adversary makes multiple guesses and allows certain level of distortion, for which we derive single-letter lower and upper bounds."}}
{"id": "eM1kVkobmU", "cdate": 1609459200000, "mdate": null, "content": {"title": "Strong Coordination Over Noisy Channels", "abstract": "We study the problem of strong coordination of the actions of two nodes X and Y that communicate over a discrete memoryless channel (DMC) such that the actions follow a prescribed joint probability distribution. We propose two novel random coding schemes and a polar coding scheme for this noisy strong coordination problem, and derive inner and outer bounds for the respective strong coordination capacity region. The first scheme is a joint coordination-channel encoding scheme that utilizes the randomness provided by the communication channel to reduce the amount of local randomness required to generate the sequence of actions at Node Y. Based on this random coding scheme, we provide a characterization of the capacity region for a special case of the noisy strong coordination setup, namely, when the DMC is a deterministic channel. The second scheme exploits separate coordination and channel encoding where local randomness is extracted from the channel after decoding. Moreover, by leveraging the random coding results for this problem, we present an example in which the proposed joint encoding scheme is able to strictly outperform the separate encoding scheme in terms of achievable communication rate for the same amount of injected randomness into both systems. Thus, we establish the sub-optimality of the separation of strong coordination and channel encoding with respect to the communication rate over the DMC in this problem. Finally, the third scheme is a joint coordination-channel polar coding scheme for strong coordination. We show that polar codes are able to achieve the established inner bound to the strong noisy coordination capacity region and thus provide a constructive alternative to a random coding proof. Our polar coding scheme also offers a constructive solution to a channel simulation problem where a DMC and shared randomness are employed together to simulate another DMC."}}
{"id": "UmSqxVzM13A5", "cdate": 1609459200000, "mdate": null, "content": {"title": "Nested Array-Based Spatially Coupled LDPC Codes", "abstract": "Linear nested codes, where two or more sub-codes are nested in a global code, have been proposed as candidates for reliable multi-terminal communication. In this paper, we consider nested array-based spatially coupled low-density parity-check (SC-LDPC) codes and propose a line-counting based optimization scheme for minimizing the number of dominant absorbing sets in order to improve its performance in the high signal-to-noise ratio regime. Since the parity-check matrices of different nested sub-codes partially overlap, the optimization of one nested sub-code imposes constraints on the optimization of the other sub-codes. To tackle these constraints, a multi-step optimization process is applied first to one of the nested codes, then sequential optimization of the remaining nested codes is carried out based on the constraints imposed by the previously optimized sub-codes. Results show that the order of optimization has a significant impact on the number of dominant absorbing sets in the Tanner graph of the code, resulting in a tradeoff between the performance of a nested code structure and its optimization sequence: the code which is optimized without constraints has fewer harmful structures than the code which is optimized with constraints. We also show that for certain code parameters, dominant absorbing sets in the Tanner graphs of all nested codes are completely removed using our proposed optimization strategy."}}
{"id": "Of2iITSwR1M", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Code and Rate Equivalence Between Secure Network and Index Coding", "abstract": "Establishing code equivalences between index coding and network coding provides important insights for code design. Previous works showed an equivalence relation between any index-coding instance and a network-coding instance, for which a code for one instance can be translated to a code for the other instance with the same decoding-error performance. The equivalence also showed a surprising result that any network-coding instance can be mapped to an index-coding instance with a properly designed code translation. In this article, we extend the existing equivalence (instance map and code translation) to one between secure index coding and secure network coding, where eavesdroppers are present in the network. In the secure setting, any code construction needs to guarantee security constraints in addition to decoding-error performance. A rate equivalence between these two problems is also established."}}
{"id": "zg4GtrVQAKo", "cdate": 1601308357159, "mdate": null, "content": {"title": "Generative Adversarial User Privacy in Lossy Single-Server Information Retrieval", "abstract": "We consider the problem of information retrieval from a dataset of files stored on a single server under both a user distortion and a user privacy constraint. Specifically, a user requesting a file from the dataset should be able to reconstruct the requested file with a prescribed distortion, and in addition, the identity of the requested file should be kept private from the server with a prescribed privacy level. The proposed model can be seen as an extension of the well-known concept of private information retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement. We initiate the study of the tradeoff between download rate, distortion, and user privacy leakage, and show that the optimal rate-distortion-leakage tradeoff is convex and that it allows for a concise information-theoretical formulation in terms of mutual information in the limit of large file sizes. Moreover, we propose a new data-driven framework by leveraging recent advancements in generative adversarial models which allows a user to learn efficient schemes in terms of download rate from the data itself. Learning the scheme is formulated as a constrained minimax game between a user which desires to keep the identity of the requested file private and an adversary that tries to infer which file the user is interested in under a distortion constraint. In general, guaranteeing a certain privacy level leads to a higher rate-distortion tradeoff curve, and hence a sacrifice in either download rate or distortion. We evaluate the performance of the scheme on a synthetic Gaussian dataset as well as on the MNIST dataset. For the MNIST dataset, the data-driven approach significantly outperforms a proposed general achievable scheme combining source coding with the download of multiple files."}}
{"id": "yn10iFh6yqv", "cdate": 1577836800000, "mdate": null, "content": {"title": "Private Function Computation for Noncolluding Coded Databases", "abstract": "Private computation in a distributed storage system (DSS) is a generalization of the private information retrieval (PIR) problem. In such setting a user wishes to compute a function of $f$ messages stored in $n$ noncolluding coded databases, i.e., databases storing data encoded with an $[n,k]$ linear storage code, while revealing no information about the desired function to the databases. We consider the problem of private polynomial computation (PPC). In PPC, a user wishes to compute a multivariate polynomial of degree at most $g$ over $f$ variables (or messages) stored in multiple databases. First, we consider the private computation of polynomials of degree $g=1$, i.e., private linear computation (PLC) for coded databases. In PLC, a user wishes to compute a linear combination over the $f$ messages while keeping the coefficients of the desired linear combination hidden from the database. For a linearly encoded DSS, we present a capacity-achieving PLC scheme and show that the PLC capacity, which is the ratio of the desired amount of information and the total amount of downloaded information, matches the maximum distance separable coded capacity of PIR for a large class of linear storage codes. Then, we consider private computation of higher degree polynomials, i.e., $g>1$. For this setup, we construct two novel PPC schemes. In the first scheme, we consider Reed-Solomon coded databases with Lagrange encoding, which leverages ideas from recently proposed star-product PIR and Lagrange coded computation. The second scheme considers the special case of coded databases with systematic Lagrange encoding. Both schemes yield improved rates, while asymptotically, as $f\\rightarrow \\infty$, the systematic scheme gives a significantly better computation retrieval rate compared to all known schemes up to some storage code rate that depends on the maximum degree of the candidate polynomials."}}
{"id": "q-CltIMvnlF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distributed Secure Storage: Rate-Privacy Trade-Off and XOR-Based Coding Scheme", "abstract": "We consider the problem of storing data in a distributed manner over $T$ servers. We require the data (i) to be recoverable from the $T$ servers, and (ii) to remain private from any $T-1$ colluding servers, where privacy is quantified in terms of mutual information between the data and all the information available at the $T-1$ colluding servers. For this model, we determine (i) the fundamental trade-off between storage size and the level of desired privacy, (ii) the optimal amount of local randomness necessary at the encoder, and (iii)~an explicit low-complexity coding scheme that solely relies on XOR operations and that asymptotically (with the data size) matches the fundamental limits found."}}
{"id": "o7qeVLXlbis6", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learned Scheduling of LDPC Decoders Based on Multi-armed Bandits", "abstract": "The multi-armed bandit (MAB) problem refers to the dilemma encountered by a gambler when deciding which arm of a multi-armed slot machine to pull in order to maximize the total reward earned in a sequence of pulls. In this paper, we model the scheduling of a node-wise sequential LDPC decoder as a Markov decision process, where the underlying Tanner graph is viewed as a slot machine with multiple arms corresponding to the check nodes. A fictitious gambler decides which check node to pull (schedule) next by observing a reward associated with each pull. This interaction enables the gambler to discover an optimized scheduling policy that aims to reach a codeword output by propagating the fewest possible messages. Based on this policy, we contrive a novel MAB-based node-wise scheduling (MABNS) algorithm to perform sequential decoding of LDPC codes. Simulation results show that the MAB-NS scheme, aided by an appropriate scheduling policy, outperforms traditional scheduling schemes in terms of complexity and bit error probability."}}
{"id": "frV2f4fpWRG", "cdate": 1577836800000, "mdate": null, "content": {"title": "Authentication with Mildly Myopic Adversaries", "abstract": "In unsecured communications settings, ascertaining the trustworthiness of received information, called authentication, is paramount. We consider keyless authentication over an arbitrarily-varying channel, where channel states are chosen by a malicious adversary with access to noisy versions of transmitted sequences. We have shown previously that a channel condition termed U-overwritability is a sufficient condition for zero authentication capacity over such a channel, and also that with a deterministic encoder, a sufficiently clear-eyed adversary is essentially omniscient. In this paper, we show that even if the authentication capacity with a deterministic encoder and an essentially omniscient adversary is zero, allowing a stochastic encoder can result in a positive authentication capacity. Furthermore, the authentication capacity with a stochastic encoder can be equal to the no-adversary capacity of the underlying channel in this case. We illustrate this for a binary channel model, which provides insight into the more general case."}}
