{"id": "f4TvHQktSw", "cdate": 1640995200000, "mdate": 1681726217022, "content": {"title": "Empirical Evaluation and Theoretical Analysis for Representation Learning: A Survey", "abstract": "Representation learning enables us to automatically extract generic feature representations from a dataset to solve another machine learning task. Recently, extracted feature representations by a representation learning algorithm and a simple predictor have exhibited state-of-the-art performance on several machine learning tasks. Despite its remarkable progress, there exist various ways to evaluate representation learning algorithms depending on the application because of the flexibility of representation learning. To understand the current representation learning, we review evaluation methods of representation learning algorithms and theoretical analyses. On the basis of our evaluation survey, we also discuss the future direction of representation learning. Note that this survey is the extended version of Nozawa and Sato (2022)."}}
{"id": "c-vxahFwDZ", "cdate": 1640995200000, "mdate": 1681726217022, "content": {"title": "On the Surrogate Gap between Contrastive and Supervised Losses", "abstract": "Contrastive representation learning encourages data representation to make semantically similar pairs closer than randomly drawn negative samples, which has been successful in various domains such ..."}}
{"id": "UGqzhED7nw", "cdate": 1640995200000, "mdate": 1681726217022, "content": {"title": "Evaluation Methods for Representation Learning: A Survey", "abstract": "Representation learning enables us to automatically extract generic feature representations from a dataset to solve another machine learning task. Recently, extracted feature representations by a representation learning algorithm and a simple predictor have exhibited state-of-the-art performance on several machine learning tasks. Despite its remarkable progress, there exist various ways to evaluate representation learning algorithms depending on the application because of the flexibility of representation learning. To understand the current applications of representation learning, we review evaluation methods of representation learning algorithms. On the basis of our evaluation survey, we also discuss the future direction of representation learning. The extended version, https://arxiv.org/abs/2204.08226, gives more detailed discussions and a survey on theoretical analyses."}}
{"id": "tDirSp3pczB", "cdate": 1632875630133, "mdate": null, "content": {"title": "Sharp Learning Bounds for Contrastive Unsupervised Representation Learning", "abstract": "Contrastive unsupervised representation learning (CURL) encourages data representation to make semantically similar pairs closer than randomly drawn negative samples, which has been successful in various domains such as vision, language, and graphs. Although recent theoretical studies have attempted to explain its success by upper bounds of a downstream classification loss by the contrastive loss, they are still not tight enough to explain an experimental fact: larger negative samples improve the classification performance. This study establishes a downstream classification loss bound with a tight intercept in the negative sample size. By regarding the contrastive loss as a downstream loss estimator, our theory not only improves the existing learning bounds substantially but also explains why downstream classification empirically improves with larger negative samples because the estimation variance of the downstream loss decays with larger negative samples. We verify that our theory is consistent with experiments on synthetic, vision, and language datasets."}}
{"id": "pZ5X_svdPQ", "cdate": 1621629708373, "mdate": null, "content": {"title": "Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning", "abstract": "Instance discriminative self-supervised representation learning has been attracted attention thanks to its unsupervised nature and informative feature representation for downstream tasks. In practice, it commonly uses a larger number of negative samples than the number of supervised classes. However, there is an inconsistency in the existing analysis; theoretically, a large number of negative samples degrade classification performance on a downstream supervised task, while empirically, they improve the performance. We provide a novel framework to analyze this empirical result regarding negative samples using the coupon collector's problem. Our bound can implicitly incorporate the supervised loss of the downstream task in the self-supervised loss by increasing the number of negative samples. We confirm that our proposed analysis holds on real-world benchmark datasets."}}
{"id": "B0zVt9O50W9", "cdate": 1577836800000, "mdate": 1647384720627, "content": {"title": "PAC-Bayesian Contrastive Unsupervised Representation Learning", "abstract": "Contrastive unsupervised representation learning (CURL) is the state-of-the-art technique to learn representations (as a set of features) from unlabelled data. While CURL has collected several empi..."}}
{"id": "68tG1kd2VAt", "cdate": 1546300800000, "mdate": null, "content": {"title": "PAC-Bayes Analysis of Sentence Representation", "abstract": "Learning sentence vectors from an unlabeled corpus has attracted attention because such vectors can represent sentences in a lower dimensional and continuous space. Simple heuristics using pre-trained word vectors are widely applied to machine learning tasks. However, they are not well understood from a theoretical perspective. We analyze learning sentence vectors from a transfer learning perspective by using a PAC-Bayes bound that enables us to understand existing heuristics. We show that simple heuristics such as averaging and inverse document frequency weighted averaging are derived by our formulation. Moreover, we propose novel sentence vector learning algorithms on the basis of our PAC-Bayes analysis."}}
{"id": "Sk9QRnCIM", "cdate": 1518419489793, "mdate": null, "content": {"title": "Node Centralities and Classification Performance for Characterizing Node Embedding Algorithms", "abstract": "Embedding graph nodes into a vector space can allow the use of machine learning to e.g. predict node classes, but the study of node embedding algorithms is immature compared to the natural language processing field because of a diverse nature of graphs. We examine the performance of node embedding algorithms with respect to graph centrality measures that characterize diverse graphs, through systematic experiments with four node embedding algorithms, four or five graph centralities, and six datasets. Experimental results give insights into the properties of node embedding algorithms, which can be a basis for further research on this topic."}}
{"id": "oyHliPAW_HN", "cdate": 1514764800000, "mdate": null, "content": {"title": "Imputing Missing Values in EEG with Multivariate Autoregressive Models", "abstract": "Wearable measurement for electroencephalogram (EEG) is expected to enable brain-computer interfaces, biomedical engineering, and neuroscience studies in real environments. When wearable devices are in practical use, only the user (subject) can take care of measurement, unlike laboratory- oriented experiments, where experimenters are always with the subject. As a result, measurement troubles such as artifact contamination or electrode impairment cannot be easily corrected, and EEG recordings will become incomplete, including many missing values. If the missing values are imputed (interpolated) and complete data without missing entries are available, we can employ existing signal analysis techniques that assume compete data. In this paper, we propose an EEG signal imputation method based on multivariate autoregressive (MAR) modeling and its iterative estimation and simulation, inspired by the multiple imputation procedure. We evaluated the proposed method with real data with artificial missing entries. Experimental results show that the proposed method outperforms popular baseline interpolation methods. Our iterative scheme is simple yet effective, and can be the foundation for many extensions."}}
{"id": "2RewDLL1g12", "cdate": 1483228800000, "mdate": null, "content": {"title": "Scalable Algorithm for Probabilistic Overlapping Community Detection", "abstract": "In the data mining field, community detection, which decomposes a graph into multiple subgraphs, is one of the major techniques to analyze graph data. In recent years, the scalability of the community detection algorithm has been a crucial issue because of the growing size of real-world networks such as the co-author network and web graph. In this paper, we propose a scalable overlapping community detection method by using the stochastic variational Bayesian training of latent Dirichlet allocation (LDA) models, which predicts sets of neighbor nodes with a community mixture distribution. In the experiment, we show that the proposed method is much faster than previous methods and is capable of detecting communities even in a huge network that contains 60 million nodes and 1.8 billion edges. Furthermore, we compared different mini-batch sizes and the number of iterations in stochastic variational Bayesian inference to determine an empirical trade-off between efficiency and quality of overlapping community detection."}}
