{"id": "_Qwp7ATIa6P", "cdate": 1676827093101, "mdate": null, "content": {"title": "Deep Gaussian Mixture Ensembles", "abstract": "This work introduces a novel probabilistic deep learning technique called deep Gaussian mixture ensembles (DGMEs), which enables accurate quantification of both epistemic and aleatoric uncertainty. By assuming the data generating process follows that of a Gaussian mixture, DGMEs are capable of approximating complex probability distributions, such as heavy-tailed or multimodal distributions. Our contributions include the derivation of an expectation-maximization (EM) algorithm used for learning the model parameters, which results in an upper-bound on the log-likelihood of training data over that of standard deep ensembles.  Additionally, the proposed EM training procedure allows for learning of mixture weights, which is not commonly done in ensembles. Our experimental results demonstrate that DGMEs outperform state-of-the-art uncertainty quantifying deep learning models in handling complex predictive densities. "}}
{"id": "nbw3zjN3fu", "cdate": 1664816296580, "mdate": null, "content": {"title": "Fast Learning of Multidimensional Hawkes Processes via Frank-Wolfe", "abstract": "Hawkes processes have recently risen to the forefront of tools when it comes to modeling and generating sequential events data. Multidimensional Hawkes processes model both the self and cross-excitation between different types of events and have been applied successfully in various domain such as finance, epidemiology and personalized recommendations, among others. In this work we present an adaptation of the Frank-Wolfe algorithm for learning multidimensional Hawkes processes. Experimental results show that our approach has better or on par accuracy in terms of parameter estimation than other first order methods, while enjoying a significantly faster runtime."}}
{"id": "pw1Y-BJdf7", "cdate": 1640995200000, "mdate": 1683898983694, "content": {"title": "Online Learning for Mixture of Multivariate Hawkes Processes", "abstract": "Online learning of Hawkes processes has received increasing attention in the last couple of years especially for modeling a network of actors. However, these works typically either model the rich interaction between the events or the latent cluster of the actors or the network structure between the actors. We propose to model the latent structure of the network of actors as well as their rich interaction across events for real-world settings of medical and financial applications. Experimental results on both synthetic and real-world data showcase the efficacy of our approach."}}
{"id": "oy3_01fHHw", "cdate": 1640995200000, "mdate": 1683898983597, "content": {"title": "Fair When Trained, Unfair When Deployed: Observable Fairness Measures are Unstable in Performative Prediction Settings", "abstract": "Many popular algorithmic fairness measures depend on the joint distribution of predictions, outcomes, and a sensitive feature like race or gender. These measures are sensitive to distribution shift: a predictor which is trained to satisfy one of these fairness definitions may become unfair if the distribution changes. In performative prediction settings, however, predictors are precisely intended to induce distribution shift. For example, in many applications in criminal justice, healthcare, and consumer finance, the purpose of building a predictor is to reduce the rate of adverse outcomes such as recidivism, hospitalization, or default on a loan. We formalize the effect of such predictors as a type of concept shift-a particular variety of distribution shift-and show both theoretically and via simulated examples how this causes predictors which are fair when they are trained to become unfair when they are deployed. We further show how many of these issues can be avoided by using fairness definitions that depend on counterfactual rather than observable outcomes."}}
{"id": "I48voIl3V9", "cdate": 1640995200000, "mdate": 1683898983773, "content": {"title": "Differentially Private Learning of Hawkes Processes", "abstract": "Hawkes processes have recently gained increasing attention from the machine learning community for their versatility in modeling event sequence data. While they have a rich history going back decades, some of their properties, such as sample complexity for learning the parameters and releasing differentially private versions, are yet to be thoroughly analyzed. In this work, we study standard Hawkes processes with background intensity $\\mu$ and excitation function $\\alpha e^{-\\beta t}$. We provide both non-private and differentially private estimators of $\\mu$ and $\\alpha$, and obtain sample complexity results in both settings to quantify the cost of privacy. Our analysis exploits the strong mixing property of Hawkes processes and classical central limit theorem results for weakly dependent random variables. We validate our theoretical findings on both synthetic and real datasets."}}
{"id": "D41IvE_fHI", "cdate": 1640995200000, "mdate": 1683898983871, "content": {"title": "Online Learning for Mixture of Multivariate Hawkes Processes", "abstract": ""}}
{"id": "4ysewPBx5j", "cdate": 1640995200000, "mdate": 1683898983654, "content": {"title": "Fast Learning of Multidimensional Hawkes Processes via Frank-Wolfe", "abstract": "Hawkes processes have recently risen to the forefront of tools when it comes to modeling and generating sequential events data. Multidimensional Hawkes processes model both the self and cross-excitation between different types of events and have been applied successfully in various domain such as finance, epidemiology and personalized recommendations, among others. In this work we present an adaptation of the Frank-Wolfe algorithm for learning multidimensional Hawkes processes. Experimental results show that our approach has better or on par accuracy in terms of parameter estimation than other first order methods, while enjoying a significantly faster runtime."}}
{"id": "fvlbStPmtY", "cdate": 1609459200000, "mdate": 1683898983724, "content": {"title": "Diagnostics for conditional density models and Bayesian inference algorithms", "abstract": "There has been growing interest in the AI community for precise uncertainty quantification. Conditional density models f(y|x), where x represents potentially high-dimensional features, are an integ..."}}
{"id": "Oc2G5dhRCCO", "cdate": 1609459200000, "mdate": 1683898983672, "content": {"title": "Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification", "abstract": "Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, particularly outside asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce confidence sets with correct conditional coverage for small sample sizes. This paper unifies classical statistics with modern machine learning to present (i) a practical procedure for the Neyman construction of confidence sets with finite-sample guarantees of nominal coverage, and (ii) diagnostics that estimate conditional coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic, like the likelihood ratio, can leverage the LF2I machinery to create valid confidence sets and diagnostics without costly Monte Carlo samples at fixed parameter settings. We study the power of two test statistics (ACORE and BFF), which, respectively, maximize versus integrate an odds function over the parameter space. Our paper discusses the benefits and challenges of LF2I, with a breakdown of the sources of errors in LF2I confidence sets."}}
{"id": "ZeThKBcR8Ye", "cdate": 1577836800000, "mdate": 1683898983736, "content": {"title": "Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting", "abstract": "Parameter estimation, statistical tests and con\ufb01dence sets are the cornerstones of classical statistics that allow scientists to make inferences about the underlying process that generated the obse..."}}
