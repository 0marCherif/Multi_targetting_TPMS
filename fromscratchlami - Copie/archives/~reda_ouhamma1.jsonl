{"id": "LT6-Mxgb3QB", "cdate": 1652737342799, "mdate": null, "content": {"title": "Bilinear Exponential Family of MDPs: Frequentist Regret Bound with Tractable Exploration $\\&$ Planning", "abstract": "We study the problem of episodic reinforcement learning in continuous state-action spaces with unknown rewards and transitions. Specifically, we consider the setting where the rewards and transitions are modeled using parametric bilinear exponential families. We propose an algorithm, $\\texttt{BEF-RLSVI}$, that a) uses penalized maximum likelihood estimators to learn the unknown parameters, b) injects a calibrated Gaussian noise in the parameter of rewards to ensure exploration, and c) leverages linearity of the exponential family with respect to an underlying RKHS to perform tractable planning. We further provide a frequentist regret analysis of $\\texttt{BEF-RLSVI}$ that yields an upper bound of $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3K})$, where $d$ is the dimension of the parameters, $H$ is the episode length, and $K$ is the number of episodes. Our analysis improves the existing bounds for the bilinear exponential family of MDPs by $\\sqrt{H}$ and removes the handcrafted clipping deployed in existing $\\texttt{RLSVI}$-type algorithms. Our regret bound is order-optimal with respect to $H$ and $K$."}}
{"id": "9PPPvvg7YxS", "cdate": 1640995200000, "mdate": 1682346566902, "content": {"title": "Bilinear Exponential Family of MDPs: Frequentist Regret Bound with Tractable Exploration and Planning", "abstract": "We study the problem of episodic reinforcement learning in continuous state-action spaces with unknown rewards and transitions. Specifically, we consider the setting where the rewards and transitions are modeled using parametric bilinear exponential families. We propose an algorithm, BEF-RLSVI, that a) uses penalized maximum likelihood estimators to learn the unknown parameters, b) injects a calibrated Gaussian noise in the parameter of rewards to ensure exploration, and c) leverages linearity of the exponential family with respect to an underlying RKHS to perform tractable planning. We further provide a frequentist regret analysis of BEF-RLSVI that yields an upper bound of $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3K})$, where $d$ is the dimension of the parameters, $H$ is the episode length, and $K$ is the number of episodes. Our analysis improves the existing bounds for the bilinear exponential family of MDPs by $\\sqrt{H}$ and removes the handcrafted clipping deployed in existing \\RLSVI-type algorithms. Our regret bound is order-optimal with respect to $H$ and $K$."}}
{"id": "rDdb26AQ0SO", "cdate": 1621630015692, "mdate": null, "content": {"title": "Stochastic Online Linear Regression: the Forward Algorithm to Replace Ridge", "abstract": "We consider the problem of online linear regression in the stochastic setting. We derive high probability regret bounds for online $\\textit{ridge}$ regression and the $\\textit{forward}$ algorithm. This enables us to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. Our study advocates for the use of the forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, we explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. We showcase this modification in linear bandit settings where it yields improved regret bounds. Last, we provide numerical experiments to illustrate our results and endorse our intuitions."}}
{"id": "W6e384Lkjbw", "cdate": 1621630015692, "mdate": null, "content": {"title": "Stochastic Online Linear Regression: the Forward Algorithm to Replace Ridge", "abstract": "We consider the problem of online linear regression in the stochastic setting. We derive high probability regret bounds for online $\\textit{ridge}$ regression and the $\\textit{forward}$ algorithm. This enables us to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. Our study advocates for the use of the forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, we explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. We showcase this modification in linear bandit settings where it yields improved regret bounds. Last, we provide numerical experiments to illustrate our results and endorse our intuitions."}}
{"id": "5fPBtLSGk21", "cdate": 1621630012214, "mdate": null, "content": {"title": "Online Sign Identification: Minimization of the Number of Errors in Thresholding Bandits", "abstract": "In the fixed budget thresholding bandit problem, an algorithm sequentially allocates a budgeted number of samples to different distributions. It then predicts whether the mean of each distribution is larger or lower than a given threshold. We introduce a large family of algorithms (containing most existing relevant ones), inspired by the Frank-Wolfe algorithm, and provide a thorough yet generic analysis of their performance. This allowed us to construct new explicit algorithms, for a broad class of problems, whose losses are within a small constant factor of the non-adaptive oracle ones. Quite interestingly, we observed that adaptive methods\nempirically greatly out-perform non-adaptive oracles, an uncommon behavior in standard online learning settings, such as regret minimization. We explain this surprising phenomenon on an insightful toy problem."}}
{"id": "rVvGyWMHWP", "cdate": 1609459200000, "mdate": 1682346566906, "content": {"title": "Online Sign Identification: Minimization of the Number of Errors in Thresholding Bandits", "abstract": "In the fixed budget thresholding bandit problem, an algorithm sequentially allocates a budgeted number of samples to different distributions. It then predicts whether the mean of each distribution is larger or lower than a given threshold. We introduce a large family of algorithms (containing most existing relevant ones), inspired by the Frank-Wolfe algorithm, and provide a thorough yet generic analysis of their performance. This allowed us to construct new explicit algorithms, for a broad class of problems, whose losses are within a small constant factor of the non-adaptive oracle ones. Quite interestingly, we observed that adaptive methodsempirically greatly out-perform non-adaptive oracles, an uncommon behavior in standard online learning settings, such as regret minimization. We explain this surprising phenomenon on an insightful toy problem."}}
{"id": "lF6aNwoI-H", "cdate": 1609459200000, "mdate": 1678441777970, "content": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "abstract": ""}}
{"id": "Dj4F7xYOxXk", "cdate": 1609459200000, "mdate": 1682346566902, "content": {"title": "Stochastic Online Linear Regression: the Forward Algorithm to Replace Ridge", "abstract": "We consider the problem of online linear regression in the stochastic setting. We derive high probability regret bounds for online $\\textit{ridge}$ regression and the $\\textit{forward}$ algorithm. This enables us to compare online regression algorithms more accurately and eliminate assumptions of bounded observations and predictions. Our study advocates for the use of the forward algorithm in lieu of ridge due to its enhanced bounds and robustness to the regularization parameter. Moreover, we explain how to integrate it in algorithms involving linear function approximation to remove a boundedness assumption without deteriorating theoretical bounds. We showcase this modification in linear bandit settings where it yields improved regret bounds. Last, we provide numerical experiments to illustrate our results and endorse our intuitions."}}
{"id": "NX1He-aFO_F", "cdate": 1601308365644, "mdate": null, "content": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights."}}
