{"id": "Ojakr9ofova", "cdate": 1652737335278, "mdate": null, "content": {"title": "Scalable Infomin Learning", "abstract": "The task of infomin learning aims to learn a representation with high utility while being uninformative about a specified target, with the latter achieved by minimising the mutual information between the representation and the target. It has broad applications, ranging from training fair prediction models against protected attributes, to unsupervised learning with disentangled representations. Recent works on infomin learning mainly use adversarial training, which involves training a neural network to estimate mutual information or its proxy and thus is slow and difficult to optimise. Drawing on recent advances in slicing techniques, we propose a new infomin learning approach, which uses a novel proxy metric to mutual information. We further derive an accurate and analytically computable approximation to this proxy metric, thereby removing the need of constructing neural network-based mutual information estimators. Compared to baselines, experiments on algorithmic fairness, disentangled representation learning and domain adaptation verify that our method can more effectively remove unwanted information with limited time budget."}}
{"id": "SRDuJssQud", "cdate": 1601308124480, "mdate": null, "content": {"title": "Neural Approximate Sufficient Statistics for Implicit Models", "abstract": "We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. We apply our approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks. "}}
{"id": "USwEVCEoDy", "cdate": 1577836800000, "mdate": 1681655881193, "content": {"title": "On Breaking Deep Generative Model-based Defenses and Beyond", "abstract": "Deep neural networks have been proven to be vulnerable to the so-called adversarial attacks. Recently there have been efforts to defend such attacks with deep generative models. These defenses ofte..."}}
{"id": "o4wg9zciAAx", "cdate": 1546300800000, "mdate": 1681655881200, "content": {"title": "Deep Secure Quantization: On secure biometric hashing against similarity-based attacks", "abstract": ""}}
