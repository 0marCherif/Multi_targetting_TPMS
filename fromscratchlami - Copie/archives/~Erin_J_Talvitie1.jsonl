{"id": "gFj8bLuUPG6", "cdate": 1598894721233, "mdate": null, "content": {"title": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning", "abstract": "The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning. Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI. It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general. This paper attempts to understand the principles that underlie DQN's impressive performance and to better contextualize its success. We systematically evaluate the importance of key representational biases encoded by DQN's network by proposing simple linear representations that make use of these concepts. Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE. Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game. Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE."}}
{"id": "_-Vy0sSDsCu", "cdate": 1577836800000, "mdate": null, "content": {"title": "Selective Dyna-Style Planning Under Limited Model Capacity", "abstract": "In model-based reinforcement learning, planning with an imperfect model of the environment has the potential to harm learning progress. But even when a model is imperfect, it may still contain info..."}}
{"id": "Y2InACDe8CD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Selective Dyna-style Planning Under Limited Model Capacity", "abstract": "In model-based reinforcement learning, planning with an imperfect model of the environment has the potential to harm learning progress. But even when a model is imperfect, it may still contain information that is useful for planning. In this paper, we investigate the idea of using an imperfect model selectively. The agent should plan in parts of the state space where the model would be helpful but refrain from using the model where it would be harmful. An effective selective planning mechanism requires estimating predictive uncertainty, which arises out of aleatoric uncertainty, parameter uncertainty, and model inadequacy, among other sources. Prior work has focused on parameter uncertainty for selective planning. In this work, we emphasize the importance of model inadequacy. We show that heteroscedastic regression can signal predictive uncertainty arising from model inadequacy that is complementary to that which is detected by methods designed for parameter uncertainty, indicating that considering both parameter uncertainty and model inadequacy may be a more promising direction for effective selective planning than either in isolation."}}
{"id": "JUBjcYByFpK", "cdate": 1577836800000, "mdate": null, "content": {"title": "Hallucinating Value: A Pitfall of Dyna-style Planning with Imperfect Environment Models", "abstract": "Dyna-style reinforcement learning (RL) agents improve sample efficiency over model-free RL agents by updating the value function with simulated experience generated by an environment model. However, it is often difficult to learn accurate models of environment dynamics, and even small errors may result in failure of Dyna agents. In this paper, we investigate one type of model error: hallucinated states. These are states generated by the model, but that are not real states of the environment. We present the Hallucinated Value Hypothesis (HVH): updating values of real states towards values of hallucinated states results in misleading state-action values which adversely affect the control policy. We discuss and evaluate four Dyna variants; three which update real states toward simulated -- and therefore potentially hallucinated -- states and one which does not. The experimental results provide evidence for the HVH thus suggesting a fruitful direction toward developing Dyna algorithms robust to model error."}}
{"id": "B1xa4TE9DS", "cdate": 1569504565211, "mdate": null, "content": {"title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents", "abstract": "The Arcade Learning Environment (ALE) is an evaluation platform that poses the\nchallenge of building AI agents with general competency across dozens of Atari 2600 games.\nIt supports a variety of different problem settings and it has been receiving increasing\nattention from the scientific community, leading to some high-profile success stories such as\nthe much publicized Deep Q-Networks (DQN). In this article we take a big picture look at\nhow the ALE is being used by the research community. We show how diverse the evaluation\nmethodologies in the ALE have become with time, and highlight some key concerns when\nevaluating agents in the ALE. We use this discussion to present some methodological best\npractices and provide new benchmark results using these best practices. To further the\nprogress in the field, we introduce a new version of the ALE that supports multiple game\nmodes and provides a form of stochasticity we call sticky actions. We conclude this big\npicture look by revisiting challenges posed when the ALE was introduced, summarizing the\nstate-of-the-art in various problems and highlighting problems that remain open."}}
{"id": "xmB3c4YVeK6", "cdate": 1514764800000, "mdate": null, "content": {"title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents", "abstract": "The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open."}}
{"id": "qvrkBJGDoE", "cdate": 1514764800000, "mdate": 1682692749046, "content": {"title": "Learning the Reward Function for a Misspecified Model", "abstract": "In model-based reinforcement learning it is typical to decouple the problems of learning the dynamics model and learning the reward function. However, when the dynamics model is flawed, it may generate erroneous states that would never occur in the true environment. It is not clear a priori what value the reward function should assign to such states. This paper presents a novel error bound that accounts for the reward model's behavior in states sampled from the model. This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical performance guarantees in deterministic MDPs that do not assume a perfect model can be learned. Empirically, this approach to reward learning can yield dramatic improvements in control performance when the dynamics model is flawed."}}
{"id": "IUy82oGqVUs", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces.", "abstract": "Dyna is a fundamental approach to model-based reinforcement learning (MBRL) that interleaves planning, acting, and learning in an online setting. In the most typical application of Dyna, the dynamics model is used to generate one-step transitions from selected start states from the agent's history, which are used to update the agent's value function or policy as if they were real experiences. In this work, one-step Dyna was applied to several games from the Arcade Learning Environment (ALE). We found that the model-based updates offered surprisingly little benefit over simply performing more updates with the agent's existing experience, even when using a perfect model. We hypothesize that to get the most from planning, the model must be used to generate unfamiliar experience. To test this, we experimented with the \"shape\" of planning in multiple different concrete instantiations of Dyna, performing fewer, longer rollouts, rather than many short rollouts. We found that planning shape has a profound impact on the efficacy of Dyna for both perfect and learned models. In addition to these findings regarding Dyna in general, our results represent, to our knowledge, the first time that a learned dynamics model has been successfully used for planning in the ALE, suggesting that Dyna may be a viable approach to MBRL in the ALE and other high-dimensional problems."}}
{"id": "H1bI4VMOZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents (Extended Abstract)", "abstract": "The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community. In this paper we take a big picture look at how the ALE is being used by the research community. We focus on how diverse the evaluation methodologies in the ALE have become and we highlight some key concerns when evaluating agents in this platform. We use this discussion to present what we consider to be the best practices for future evaluations in the ALE. To further the progress in the field, we also introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions."}}
{"id": "EliLLKI_V", "cdate": 1514764800000, "mdate": 1682692749052, "content": {"title": "The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces", "abstract": "Dyna is a fundamental approach to model-based reinforcement learning (MBRL) that interleaves planning, acting, and learning in an online setting. In the most typical application of Dyna, the dynamics model is used to generate one-step transitions from selected start states from the agent's history, which are used to update the agent's value function or policy as if they were real experiences. In this work, one-step Dyna was applied to several games from the Arcade Learning Environment (ALE). We found that the model-based updates offered surprisingly little benefit over simply performing more updates with the agent's existing experience, even when using a perfect model. We hypothesize that to get the most from planning, the model must be used to generate unfamiliar experience. To test this, we experimented with the \"shape\" of planning in multiple different concrete instantiations of Dyna, performing fewer, longer rollouts, rather than many short rollouts. We found that planning shape has a profound impact on the efficacy of Dyna for both perfect and learned models. In addition to these findings regarding Dyna in general, our results represent, to our knowledge, the first time that a learned dynamics model has been successfully used for planning in the ALE, suggesting that Dyna may be a viable approach to MBRL in the ALE and other high-dimensional problems."}}
