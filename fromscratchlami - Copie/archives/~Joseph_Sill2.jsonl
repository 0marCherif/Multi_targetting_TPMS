{"id": "H1WC-EfuZH", "cdate": 1356998400000, "mdate": null, "content": {"title": "Collaborative Filtering on Ordinal User Feedback", "abstract": "We propose a collaborative filtering (CF) recommendation framework which is based on viewing user feedback on products as ordinal, rather than the more common numerical view. Such an ordinal view frequently provides a more natural reflection of the user intention when providing qualitative ratings, allowing users to have different internal scoring scales. Moreover, we can address scenarios where assigning numerical scores to different types of user feedback would not be easy. The framework can wrap most collaborative filtering algorithms, enabling algorithms previously designed for numerical values to handle ordinal values. We demonstrate our framework by wrapping a leading matrix factorization CF method. A cornerstone of our method is its ability to predict a full probability distribution of the expected item ratings, rather than only a single score for an item. One of the advantages this brings is a novel approach to estimating the confidence level in each individual prediction. Compared to previous approaches to confidence estimation, ours is more principled and empirically superior in its accuracy. We demonstrate the efficacy of the approach on two of the largest publicly available datasets: the Netflix data and the Yahoo! Music data."}}
{"id": "jpRLYDcrMIi", "cdate": 1293840000000, "mdate": null, "content": {"title": "OrdRec: an ordinal model for predicting personalized item rating distributions", "abstract": "We propose a collaborative filtering (CF) recommendation framework, which is based on viewing user feedback on products as ordinal, rather than the more common numerical view. This way, we do not need to interpret each user feedback value as a number, but only rely on the more relaxed assumption of having an order among the different feedback ratings. Such an ordinal view frequently provides a more natural reflection of the user intention when providing qualitative ratings, allowing users to have different internal scoring scales. Moreover, we can address scenarios where assigning numerical scores to different types of user feedback would not be easy. Our approach is based on a pointwise ordinal model, which allows it to linearly scale with data size. The framework can wrap most collaborative filtering algorithms, upgrading those algorithms designed to handle numerical values into being able to handle ordinal values. In particular, we demonstrate our framework with wrapping a leading matrix factorization CF method. A cornerstone of our method is its ability to predict a full probability distribution of the expected item ratings, rather than only a single score for an item. One of the advantages this brings is a novel approach to estimating the confidence level in each individual prediction. Compared to previous approaches to confidence estimation, ours is more principled and empirically superior in its accuracy. We demonstrate the efficacy of the approach on some of the largest publicly available datasets, the Netflix data, and the Yahoo! Music data."}}
{"id": "EV6w8pas4t6", "cdate": 1230768000000, "mdate": null, "content": {"title": "Feature-Weighted Linear Stacking", "abstract": "Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset."}}
{"id": "pxSGDaS8i51", "cdate": 1199145600000, "mdate": null, "content": {"title": "A linear fit gets the correct monotonicity directions", "abstract": "Let f be a function on \u211d d that is monotonic in every variable. There are 2 d possible assignments to the directions of monotonicity (two per variable). We provide sufficient conditions under which the optimal linear model obtained from a least squares regression on f will identify the monotonicity directions correctly. We show that when the input dimensions are independent, the linear fit correctly identifies the monotonicity directions. We provide an example to illustrate that in the general case, when the input dimensions are not independent, the linear fit may not identify the directions correctly. However, when the inputs are jointly Gaussian, as is often assumed in practice, the linear fit will correctly identify the monotonicity directions, even if the input dimensions are dependent. Gaussian densities are a special case of a more general class of densities (Mahalanobis densities) for which the result holds. Our results hold when f is a classification or regression function. If a finite data set is sampled from the function, we show that if the exact linear regression would have yielded the correct monotonicity directions, then the sample regression will also do so asymptotically (in a probabilistic sense). This result holds even if the data are noisy."}}
{"id": "Mdn6tQ_boer", "cdate": 1041379200000, "mdate": null, "content": {"title": "Using a Linear Fit to Determine Monotonicity Directions", "abstract": "Let f be a function on \u211d d that is monotonic in every variable. There are 2 d possible assignments to the directions of monotonicity (two per variable). We provide sufficient conditions under which the optimal linear model obtained from a least squares regression on f will identify the monotonicity directions correctly. We show that when the input dimensions are independent, the linear fit correctly identifies the monotonicity directions. We provide an example to illustrate that in the general case, when the input dimensions are not independent, the linear fit may not identify the directions correctly. However, when the inputs are jointly Gaussian, as is often assumed in practice, the linear fit will correctly identify the monotonicity directions, even if the input dimensions are dependent. Gaussian densities are a special case of a more general class of densities (Mahalanobis densities) for which the result holds. Our results hold when f is a classification or regression function. If a finite data set is sampled from the function, we show that if the exact linear regression would have yielded the correct monotonicity directions, then the sample regression will also do so asymptotically (in a probabilistic sense). This result holds even if the data are noisy."}}
{"id": "AkY1GCS8i5", "cdate": 1009843200000, "mdate": null, "content": {"title": "Robust image recognition by fusion of contextual information", "abstract": "This paper studies the fusion of contextual information in pattern recognition, with applications to biomedical image identification. In the real world there are cases where the identity of an object is ambiguous if the classification is based only on its own features. It is helpful to reduce the ambiguity by utilizing extra information, referred to as context, provided by accompanying objects. We investigate two techniques that incorporate context. The first approach, based on compound Bayesian theory, incorporates context by fusing the measurements of all objects under consideration. It is an optimal strategy in terms of achieving minimum set-by-set error probability. The second approach fuses the measurements of an object with explicitly extracted context. Its linear computational complexity makes it more tractable than the first approach, which requires exponential computation. These two techniques are applied to two medical applications: white blood cell image classification and microscopic urinalysis. It is demonstrated that superior classification performances are achieved by using context. In our particular applications, it reduces overall classification error, as well as false positive and false negative diagnosis rates."}}
{"id": "BkEE6EzObr", "cdate": 978307200000, "mdate": null, "content": {"title": "Reinforcement Learning in Distributed Domains: Beyond Team Games", "abstract": ""}}
{"id": "HJEOPdWdZS", "cdate": 915148800000, "mdate": null, "content": {"title": "Image Recognition in Context: Application to Microscopic Urinalysis", "abstract": "We propose a new and efficient technique for incorporating contextual information into object classification. Most of the current techniques face the problem of exponential computation cost. In this paper, we propose a new general framework that incorporates partial context at a linear cost. This technique is applied to microscopic urinalysis image recognition, resulting in a significant improvement of recognition rate over the context free approach. This gain would have been impossible using conventional context incorporation techniques. 1 BACKGROUND: RECOGNITION IN CONTEXT There are a number of pattern recognition problem domains where the classification of an object should be based on more than simply the appearance of the object itself. In remote sensing image classification, where each pixel is part of ground cover, a pixel is more like(cid:173) ly to be a glacier if it is in a mountainous area, than if surrounded by pixels of residential areas. In text analysis, one can expect to find certain letters occurring regularly in particu(cid:173) lar arrangement with other letters(qu, ee,est, tion, etc.). The information conveyed by the accompanying entities is referred to as contextual information. Human experts apply con(cid:173) textual information in their decision making [2][ 6]. It makes sense to design techniques and algorithms to make computers aggregate and utilize a more complete set of information in their decision making the way human experts do. In pattern recognition systems, however, *Author for correspondence 964 X B. Song, J Sill, Y. Abu-Mostafa and H. Kasdan the primary (and often only) source of information used to identify an object is the set of measurements, or features, associated with the object itself. Augmenting this information by incorporating context into the classification process can yield significant benefits. i = 1, ... N. With each object we associate a Consider a set of N objects Ti , class label Ci that is a member of a label set n = {1 , ... , D} . Each object Ti is characterized by a set of measurements Xi E R P, which we call a feature vec(cid:173) tor. Many techniques [1][2][4J[6} incorporate context by conditioning the posterior probability of objects' identities on the joint features of all accompanying objects. i.e .\u2022 P(Cl, C2,\u00b7\u00b7\u00b7 , cNlxl , . . . , XN). and then maximizing it with respectto Cl, C2, . .. , CN . It can be shown thatp(cl,c2, . . . ,cNlxl, . . . ,xN) ex p(cllxl) ... p(CNlxN) (~ci\"\"'(N\\ given certain reasonable assumptions."}}
{"id": "uJ1YnJNHAOi", "cdate": 883612800000, "mdate": null, "content": {"title": "The Capacity of Monotonic Functions", "abstract": "We consider the class M of monotonically increasing binary output functions. M has considerable practical significance in machine learning and pattern recognition because prior information often suggests a monotonic relationship between input and output variables. The decision boundaries of monotonic classifiers are compared and contrasted with those of linear classifiers. M is shown to have a VC dimension of \u221e, meaning that the VC bounds cannot guarantee generalization independent of input distribution. We demonstrate that when the input distribution is taken into account, however, the VC bounds become useful because the annealed VC entropy of M is modest for many distributions. Techniques for estimating the capacity and bounding the annealed VC entropy of M given the input distribution are presented and implemented."}}
{"id": "HJWPJd-OZS", "cdate": 852076800000, "mdate": null, "content": {"title": "Monotonic Networks", "abstract": "Monotonicity is a constraint which arises in many application do(cid:173) mains. We present a machine learning model, the monotonic net(cid:173) work, for which monotonicity can be enforced exactly, i.e., by virtue offunctional form . A straightforward method for implementing and training a monotonic network is described. Monotonic networks are proven to be universal approximators of continuous, differen(cid:173) tiable monotonic functions. We apply monotonic networks to a real-world task in corporate bond rating prediction and compare them to other approaches."}}
