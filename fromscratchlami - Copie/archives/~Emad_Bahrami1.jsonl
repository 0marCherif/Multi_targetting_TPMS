{"id": "oqsRnAlC3w", "cdate": 1672531200000, "mdate": 1698573330518, "content": {"title": "How Much Temporal Long-Term Context is Needed for Action Segmentation?", "abstract": "Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Breakfast, and Assembly101. Our experiments show that modeling the full context of a video is necessary to obtain the best performance for temporal action segmentation."}}
{"id": "tx_NDWn43H", "cdate": 1640995200000, "mdate": 1681657188757, "content": {"title": "Robust Action Segmentation from Timestamp Supervision", "abstract": ""}}
{"id": "nROuTF3M1Sw", "cdate": 1640995200000, "mdate": 1681657188768, "content": {"title": "TaylorSwiftNet: Taylor Driven Temporal Modeling for Swift Future Frame Prediction", "abstract": ""}}
{"id": "eGJBk1jTdVj", "cdate": 1609459200000, "mdate": 1667137467222, "content": {"title": "3D CNNs With Adaptive Temporal Feature Resolutions", "abstract": "While state-of-the-art 3D Convolutional Neural Networks (CNN) achieve very good results on action recognition datasets, they are computationally very expensive and require many GFLOPs. While the GFLOPs of a 3D CNN can be decreased by reducing the temporal feature resolution within the network, there is no setting that is optimal for all input clips. In this work, we therefore introduce a differentiable Similarity Guided Sampling (SGS) module, which can be plugged into any existing 3D CNN architecture. SGS empowers 3D CNNs by learning the similarity of temporal features and grouping similar features together. As a result, the temporal feature resolution is not anymore static but it varies for each input video clip. By integrating SGS as an additional layer within current 3D CNNs, we can convert them into much more efficient 3D CNNs with adaptive temporal feature resolutions (ATFR). Our evaluations show that the proposed module improves the state-of-the-art by reducing the computational cost (GFLOPs) by half while preserving or even improving the accuracy. We evaluate our module by adding it to multiple state-of-the-art 3D CNNs on various datasets such as Kinetics-600, Kinetics-400, mini-Kinetics, Something-Something V2, UCF101, and HMDB51."}}
