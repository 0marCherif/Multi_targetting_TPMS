{"id": "V_06QV-kZX", "cdate": 1663849954376, "mdate": null, "content": {"title": "Analyzing Tree Architectures in Ensembles via Neural Tangent Kernel", "abstract": "A soft tree is an actively studied variant of a decision tree that updates splitting rules using the gradient method. Although soft trees can take various architectures, their impact is not theoretically well known. In this paper, we formulate and analyze the Neural Tangent Kernel (NTK) induced by soft tree ensembles for arbitrary tree architectures. This kernel leads to the remarkable finding that only the number of leaves at each depth is relevant for the tree architecture in ensemble learning with an infinite number of trees. In other words, if the number of leaves at each depth is fixed, the training behavior in function space and the generalization performance are exactly the same across different tree architectures, even if they are not isomorphic. We also show that the NTK of asymmetric trees like decision lists does not degenerate when they get infinitely deep. This is in contrast to the perfect binary trees, whose NTK is known to degenerate and leads to worse generalization performance for deeper trees."}}
{"id": "vUH85MOXO7h", "cdate": 1632875734740, "mdate": null, "content": {"title": "A Neural Tangent Kernel Perspective of Infinite Tree Ensembles", "abstract": "In practical situations, the tree ensemble is one of the most popular models along with neural networks. A soft tree is a variant of a decision tree. Instead of using a greedy method for searching splitting rules, the soft tree is trained using a gradient method in which the entire splitting operation is formulated in a differentiable form. Although ensembles of such soft trees have been used increasingly in recent years, little theoretical work has been done to understand their behavior. By considering an ensemble of infinite soft trees, this paper introduces and studies the Tree Neural Tangent Kernel (TNTK), which provides new insights into the behavior of the infinite ensemble of soft trees. Using the TNTK, we theoretically identify several non-trivial properties, such as global convergence of the training, the equivalence of the oblivious tree structure, and the degeneracy of the TNTK induced by the deepening of the trees."}}
{"id": "lXS8SPwFPPU", "cdate": 1609459200000, "mdate": 1681650047101, "content": {"title": "Unintended Effects on Adaptive Learning Rate for Training Neural Network with Output Scale Change", "abstract": ""}}
{"id": "6ha1R0lgVqy", "cdate": 1609459200000, "mdate": 1681650047096, "content": {"title": "Interpretable Mixture Density Estimation by use of Differentiable Tree-module", "abstract": ""}}
