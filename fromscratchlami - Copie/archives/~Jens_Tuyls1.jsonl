{"id": "bCZpPHmz5Ep", "cdate": 1684073685247, "mdate": 1684073685247, "content": {"title": "Differentially Private Language Models Benefit from Public Pre-training", "abstract": "Language modeling is a keystone task in natu- ral language processing. When training a lan- guage model on sensitive information, differ- ential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultane- ously high-quality and privacy preserving by tuning a public base model on a private cor- pus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible."}}
{"id": "Aw26u6y79n", "cdate": 1684073613435, "mdate": 1684073613435, "content": {"title": "GENERATIVE MODELING OF ATMOSPHERIC CONVECTION", "abstract": "While cloud-resolving models can explicitly simulate the details of small-scale storm formation and morphology, these details are often ignored by climate models for lack of computational resources. Here, we explore the potential of generative modeling to cheaply recreate small-scale storms by designing and implement- ing a Variational Autoencoder (VAE) that performs structural replication, dimensionality reduction, and clus- tering of high-resolution vertical velocity fields. Trained on \u223c 6 \u00b7 106 samples spanning the globe, the VAE successfully reconstructs the spatial structure of con- vection, performs unsupervised clustering of convective organization regimes, and identifies anomalous storm activity, confirming the potential of generative modeling to power stochastic parameterizations of convection in climate models."}}
{"id": "EByiF8Svm1", "cdate": 1684073235867, "mdate": 1684073235867, "content": {"title": "AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models", "abstract": "Neural NLP models are increasingly accu- rate but are imperfect and opaque\u2014they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpreta- tion methods ameliorate this opacity by pro- viding explanations for specific model pre- dictions. Unfortunately, existing interpreta- tion codebases make it difficult to apply these methods to new models and tasks, which hin- ders adoption for practitioners and burdens in- terpretability researchers. We introduce Al- lenNLP Interpret, a flexible framework for in- terpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit\u2019s flexibility and util- ity by implementing live demos for five in- terpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension us- ing BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp. org/interpret."}}
{"id": "kfDB97GLgjf", "cdate": 1684072808314, "mdate": 1684072808314, "content": {"title": "Gradient-based Analysis of NLP Models is Manipulable", "abstract": "Gradient-based analysis methods, such as saliency map visualizations and adversarial in- put perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, their faithfulness. In this paper, however, we demonstrate that the gradients of a model are easily manipulable, and thus bring into ques- tion the reliability of gradient-based analyses. In particular, we merge the layers of a tar- get model with a FACADE model that over- whelms the gradients without affecting the pre- dictions. This FACADE model can be trained to have gradients that are misleading and ir- relevant to the task, such as focusing only on the stop words in the input. On a vari- ety of NLP tasks (text classification, NLI, and QA), we show that our method can manipulate numerous gradient-based analysis techniques: saliency maps, input reduction, and adversar- ial perturbations all identify unimportant or tar- geted tokens as being highly important. The code and a tutorial of this paper is available at http://ucinlp.github.io/facade."}}
{"id": "Ek7PSN7Y77z", "cdate": 1632875686634, "mdate": null, "content": {"title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games", "abstract": "Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods."}}
