{"id": "-A2OeAE5xXu", "cdate": 1672531200000, "mdate": 1695951166715, "content": {"title": "Faster Min-Cost Flow on Bounded Treewidth Graphs", "abstract": "We present a $\\widetilde{O}(m\\sqrt{\\tau}+n\\tau)$ time algorithm for finding a minimum-cost flow in graphs with $n$ vertices and $m$ edges, given a tree decomposition of width $\\tau$ and polynomially bounded integer costs and capacities. This improves upon the current best algorithms for general linear programs bounded by treewidth which run in $\\widetilde{O}(m \\tau^{(\\omega+1)/2})$ time by [Dong-Lee-Ye,21] and [Gu-Song,22], where $\\omega \\approx 2.37$ is the matrix multiplication exponent. Our approach leverages recent advances in structured linear program solvers and robust interior point methods. As a corollary, for any graph $G$ with $n$ vertices, $m$ edges, and treewidth $\\tau$, we obtain a $\\widetilde{O}(\\tau^3 \\cdot m)$ time algorithm to compute a tree decomposition of $G$ with width $O(\\tau \\cdot \\log n)$."}}
{"id": "X4WAq7JQHbA", "cdate": 1652737746528, "mdate": null, "content": {"title": "Decomposable Non-Smooth Convex Optimization with Nearly-Linear Gradient Oracle Complexity", "abstract": "Many fundamental problems in machine learning can be formulated by the convex program \n\\[ \\min_{\\theta\\in \\mathbb{R}^d}\\ \\sum_{i=1}^{n}f_{i}(\\theta), \\]\nwhere each $f_i$ is a convex, Lipschitz function supported on a subset of $d_i$ coordinates of $\\theta$. One common approach to this problem, exemplified by stochastic gradient descent, involves sampling one $f_i$ term at every iteration to make progress. This approach crucially relies on a notion of uniformity across the $f_i$'s, formally captured by their condition number. In this work, we give an algorithm that minimizes the above convex formulation to $\\epsilon$-accuracy in $\\widetilde{O}(\\sum_{i=1}^n d_i \\log (1 /\\epsilon))$ gradient computations, with no assumptions on the condition number.  The previous best algorithm independent of the condition number is the standard cutting plane method, which requires $O(nd \\log (1/\\epsilon))$ gradient computations. As a corollary, we improve upon the evaluation oracle complexity for decomposable submodular minimization by [Axiotis, Karczmarz, Mukherjee, Sankowski and Vladu, ICML 2021]. Our main technical contribution is an adaptive procedure to select an $f_i$ term at every iteration via a novel combination of cutting-plane and interior-point methods.\n"}}
{"id": "slKVqAflN5", "cdate": 1652737742065, "mdate": null, "content": {"title": "A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions", "abstract": "Zhang et al. (ICML 2020) introduced a novel modification of Goldstein's classical subgradient method, with an efficiency guarantee of $O(\\varepsilon^{-4})$ for minimizing Lipschitz functions. Their work, however, makes use of an oracle that is not efficiently implementable. In this paper, we obtain the same efficiency guarantee with a standard subgradient oracle, thus making our algorithm efficiently implementable. Our resulting method works on any Lipschitz function whose value and gradient can be evaluated at points of differentiability. We additionally present a new cutting plane algorithm that achieves an efficiency of  $O(d\\varepsilon^{-2}\\log S)$ for the class of $S$-smooth (and possibly non-convex) functions in low dimensions. Strikingly, this $\\epsilon$-dependence matches the lower bounds for the convex setting. "}}
{"id": "gOZOJ0yDEZL", "cdate": 1640995200000, "mdate": 1692735812857, "content": {"title": "Decomposable Non-Smooth Convex Optimization with Nearly-Linear Gradient Oracle Complexity", "abstract": "Many fundamental problems in machine learning can be formulated by the convex program \\[ \\min_{\\theta\\in \\mathbb{R}^d}\\ \\sum_{i=1}^{n}f_{i}(\\theta), \\]where each $f_i$ is a convex, Lipschitz function supported on a subset of $d_i$ coordinates of $\\theta$. One common approach to this problem, exemplified by stochastic gradient descent, involves sampling one $f_i$ term at every iteration to make progress. This approach crucially relies on a notion of uniformity across the $f_i$'s, formally captured by their condition number. In this work, we give an algorithm that minimizes the above convex formulation to $\\epsilon$-accuracy in $\\widetilde{O}(\\sum_{i=1}^n d_i \\log (1 /\\epsilon))$ gradient computations, with no assumptions on the condition number. The previous best algorithm independent of the condition number is the standard cutting plane method, which requires $O(nd \\log (1/\\epsilon))$ gradient computations. As a corollary, we improve upon the evaluation oracle complexity for decomposable submodular minimization by [Axiotis, Karczmarz, Mukherjee, Sankowski and Vladu, ICML 2021]. Our main technical contribution is an adaptive procedure to select an $f_i$ term at every iteration via a novel combination of cutting-plane and interior-point methods."}}
{"id": "e7D5dV9XDYE", "cdate": 1640995200000, "mdate": 1682449282688, "content": {"title": "Nested Dissection Meets IPMs: Planar Min-Cost Flow in Nearly-Linear Time", "abstract": "We present a nearly-linear time algorithm for finding a minimum-cost flow in planar graphs with polynomially bounded integer costs and capacities. The previous fastest algorithm for this problem is based on interior point methods (IPMs) and works for general sparse graphs in $O(n^{1.5}\\text{poly}(\\log n))$ time [Daitch-Spielman, STOC'08]. Intuitively, $\\Omega(n^{1.5})$ is a natural runtime barrier for IPM-based methods, since they require $\\sqrt{n}$ iterations, each routing a possibly-dense electrical flow. To break this barrier, we develop a new implicit representation for flows based on generalized nested-dissection [Lipton-Rose-Tarjan, JSTOR'79] and approximate Schur complements [Kyng-Sachdeva, FOCS'16]. This implicit representation permits us to design a data structure to route an electrical flow with sparse demands in roughly $\\sqrt{n}$ update time, resulting in a total running time of $O(n\\cdot\\text{poly}(\\log n))$. Our results immediately extend to all families of separable graphs."}}
{"id": "BrVLVtRIm3F", "cdate": 1640995200000, "mdate": 1650594880905, "content": {"title": "Nested Dissection Meets IPMs: Planar Min-Cost Flow in Nearly-Linear Time", "abstract": "We present a nearly-linear time algorithm for finding a minimum-cost flow in planar graphs with polynomially bounded integer costs and capacities. The previous fastest algorithm for this problem was based on interior point methods (IPMs) and worked for general sparse graphs in O(n1.5 poly(log n)) time [Daitch-Spielman, STOC'08]. Intuitively, \u03a9(n1.5) is a natural runtime barrier for IPM based methods, since they require iterations, each routing a possibly-dense electrical flow. To break this barrier, we develop a new implicit representation for flows based on generalized nested-dissection [Lipton-Rose-Tarjan, JSTOR'79] and approximate Schur complements [Kyng-Sachdeva, FOCS'16]. This implicit representation permits us to design a data structure to route an electrical flow with sparse demands in roughly update time, resulting in a total running time of O(n \u00b7 poly(log n)). Our results immediately extend to all families of separable graphs."}}
{"id": "3ene6vGluFa", "cdate": 1640995200000, "mdate": 1692735812858, "content": {"title": "A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions", "abstract": "Zhang et al. (ICML 2020) introduced a novel modification of Goldstein's classical subgradient method, with an efficiency guarantee of $O(\\varepsilon^{-4})$ for minimizing Lipschitz functions. Their work, however, makes use of an oracle that is not efficiently implementable. In this paper, we obtain the same efficiency guarantee with a standard subgradient oracle, thus making our algorithm efficiently implementable. Our resulting method works on any Lipschitz function whose value and gradient can be evaluated at points of differentiability. We additionally present a new cutting plane algorithm that achieves an efficiency of $O(d\\varepsilon^{-2}\\log S)$ for the class of $S$-smooth (and possibly non-convex) functions in low dimensions. Strikingly, this $\\epsilon$-dependence matches the lower bounds for the convex setting."}}
{"id": "X0IDKxeTVGp", "cdate": 1609459200000, "mdate": 1650594880911, "content": {"title": "A nearly-linear time algorithm for linear programs with small treewidth: a multiscale representation of robust central path", "abstract": "Arising from structural graph theory, treewidth has become a focus of study in fixed-parameter tractable algorithms. Many NP-hard problems are known to be solvable in O(n \u00b7 2O(\u03c4)) time, where \u03c4 is the treewidth of the input graph. Analogously, many problems in P should be solvable in O(n \u00b7 \u03c4O(1)) time; however, due to the lack of appropriate tools, only a few such results are currently known. In our paper, we show this holds for linear programs: Given a linear program of the form minAx=b,\u2113 \u2264 x\u2264 u c\u22a4 x whose dual graph GA has treewidth \u03c4, and a corresponding width-\u03c4 tree decomposition, we show how to solve it in time O(n\u00a0\u00b7\u00a0\u03c42\u00a0log(1/\u03b5)), where n is the number of variables and \u03b5 is the relative accuracy. When a tree decomposition is not given, we use existing techniques in vertex separators to obtain algorithms with O(n \u00b7 \u03c44 log(1/\u03b5)) and O(n \u00b7 \u03c42 log(1/\u03b5) + n1.5) run-times. Besides being the first of its kind, our algorithm has run-time nearly matching the fastest run-time for solving the sub-problem Ax=b (under the assumption that no fast matrix multiplication is used). We obtain these results by combining recent techniques in interior-point methods (IPMs), sketching, and a novel representation of the solution under a multiscale basis similar to the wavelet basis. This representation further yields the first IPM with o(rank(A)) time per iteration when the treewidth is small."}}
{"id": "gzebXe0b8LO", "cdate": 1577836800000, "mdate": 1650594880811, "content": {"title": "Robust Gaussian Covariance Estimation in Nearly-Matrix Multiplication Time", "abstract": "Robust covariance estimation is the following, well-studied problem in high dimensional statistics: given $N$ samples from a $d$-dimensional Gaussian $\\mathcal{N}(\\boldsymbol{0}, \\Sigma)$, but where an $\\varepsilon$-fraction of the samples have been arbitrarily corrupted, output $\\widehat{\\Sigma}$ minimizing the total variation distance between $\\mathcal{N}(\\boldsymbol{0}, \\Sigma)$ and $\\mathcal{N}(\\boldsymbol{0}, \\widehat{\\Sigma})$. This corresponds to learning $\\Sigma$ in a natural affine-invariant variant of the Frobenius norm known as the \\emph{Mahalanobis norm}. Previous work of Cheng et al demonstrated an algorithm that, given $N = \\widetilde{\\Omega}(d^2 / \\varepsilon^2)$ samples, achieved a near-optimal error of $O(\\varepsilon \\log 1 / \\varepsilon)$, and moreover, their algorithm ran in time $\\widetilde{O}(T(N, d) \\log \\kappa / \\mathrm{poly} (\\varepsilon))$, where $T(N, d)$ is the time it takes to multiply a $d \\times N$ matrix by its transpose, and $\\kappa$ is the condition number of $\\Sigma$. When $\\varepsilon$ is relatively small, their polynomial dependence on $1/\\varepsilon$ in the runtime is prohibitively large. In this paper, we demonstrate a novel algorithm that achieves the same statistical guarantees, but which runs in time $\\widetilde{O} (T(N, d) \\log \\kappa)$. In particular, our runtime has no dependence on $\\varepsilon$. When $\\Sigma$ is reasonably conditioned, our runtime matches that of the fastest algorithm for covariance estimation without outliers, up to poly-logarithmic factors, showing that we can get robustness essentially ``for free.''"}}
