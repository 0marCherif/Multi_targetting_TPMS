{"id": "w-Aq4vmnTOP", "cdate": 1652737835044, "mdate": null, "content": {"title": "On Learning and Refutation in Noninteractive Local Differential Privacy", "abstract": "We study two basic statistical tasks in  non-interactive local differential privacy (LDP): *learning* and *refutation*: learning requires finding a concept that best fits an unknown target function (from labelled samples drawn from a distribution), whereas  refutation requires distinguishing between data distributions that are well-correlated with some concept in the class, versus distributions where the labels are random. Our main result is a complete characterization of the sample complexity of agnostic PAC learning for non-interactive LDP protocols. We show that the optimal sample complexity for any concept class is captured by the approximate $\\gamma_2$ norm of a natural matrix associated with the class. Combined with previous work, this gives an *equivalence* between agnostic learning and refutation in the agnostic setting. "}}
{"id": "uE-QqVM3Z3d", "cdate": 1622039869361, "mdate": null, "content": {"title": "Near Neighbor Search via Efficient Average Distortion Embeddings", "abstract": "A recent series of papers by Andoni, Naor, Nikolov, Razenshteyn, and Waingarten (STOC 2018, FOCS 2018) has given approximate near neighbour search (NNS) data structures for a wide class of distance metrics, including all norms. In particular, these data structures achieve approximation on the order of p for \u2113dp norms with space complexity nearly linear in the dataset size n and polynomial in the dimension d, and query time sub-linear in n and polynomial in d. The main shortcoming is the exponential in d pre-processing time required for their construction.\nIn this paper, we describe a more direct framework for constructing NNS data structures for general norms. More specifically, we show via an algorithmic reduction that an efficient NNS data structure for a given metric is implied by an efficient average distortion embedding of it into \u21131 or into Euclidean space. In particular, the resulting data structures require only polynomial pre-processing time, as long as the embedding can be computed in polynomial time. As a concrete instantiation of this framework, we give an NNS data structure for \u2113p with efficient pre-processing that matches the approximation factor, space and query complexity of the aforementioned data structure of Andoni et al. On the way, we resolve a question of Naor (Analysis and Geometry in Metric Spaces, 2014) and provide an explicit, efficiently computable embedding of \u2113p, for p\u22652, into \u21132 with (quadratic) average distortion on the order of p. We expect our approach to pave the way for constructing efficient NNS data structures for all norms."}}
{"id": "ztPoaj50mvz", "cdate": 1577836800000, "mdate": null, "content": {"title": "Private Query Release Assisted by Public Data", "abstract": "We study the problem of differentially private query release assisted by access to public data. In this problem, the goal is to answer a large class $\\mathcal{H}$ of statistical queries with error no more than $\\alpha$ using a combination of public and private samples. The algorithm is required to satisfy differential privacy only with respect to the private samples. We study the limits of this task in terms of the private and public sample complexities. First, we show that we can solve the problem for any query class $\\mathcal{H}$ of finite VC-dimension using only $d/\\alpha$ public samples and $\\sqrt{p}d^{3/2}/\\alpha^2$ private samples, where $d$ and $p$ are the VC-dimension and dual VC-dimension of $\\mathcal{H}$, respectively. In comparison, with only private samples, this problem cannot be solved even for simple query classes with VC-dimension one, and without any private samples, a larger public sample of size $d/\\alpha^2$ is needed. Next, we give sample complexity lower bounds that exhibit tight dependence on $p$ and $\\alpha$. For the class of decision stumps, we give a lower bound of $\\sqrt{p}/\\alpha$ on the private sample complexity whenever the public sample size is less than $1/\\alpha^2$. Given our upper bounds, this shows that the dependence on $\\sqrt{p}$ is necessary in the private sample complexity. We also give a lower bound of $1/\\alpha$ on the public sample complexity for a broad family of query classes, which by our upper bound, is tight in $\\alpha$."}}
{"id": "dVCIAGvhh8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Locally Private Hypothesis Selection", "abstract": "We initiate the study of hypothesis selection under local differential privacy. Given samples from an unknown probability distribution $p$ and a set of $k$ probability distributions $\\mathcal{Q}$, we aim to output, under the constraints of $\\varepsilon$-local differential privacy, a distribution from $\\mathcal{Q}$ whose total variation distance to $p$ is comparable to the best such distribution. This is a generalization of the classic problem of $k$-wise simple hypothesis testing, which corresponds to when $p \\in \\mathcal{Q}$, and we wish to identify $p$. Absent privacy constraints, this problem requires $O(\\log k)$ samples from $p$, and it was recently shown that the same complexity is achievable under (central) differential privacy. However, the naive approach to this problem under local differential privacy would require $\\tilde O(k^2)$ samples. We first show that the constraint of local differential privacy incurs an exponential increase in cost: any algorithm for this problem requires at least $\\Omega(k)$ samples. Second, for the special case of $k$-wise simple hypothesis testing, we provide a non-interactive algorithm which nearly matches this bound, requiring $\\tilde O(k)$ samples. Finally, we provide sequentially interactive algorithms for the general case, requiring $\\tilde O(k)$ samples and only $O(\\log \\log k)$ rounds of interactivity. Our algorithms are achieved through a reduction to maximum selection with adversarial comparators, a problem of independent interest for which we initiate study in the parallel setting. For this problem, we provide a family of algorithms for each number of allowed rounds of interaction $t$, as well as lower bounds showing that they are near-optimal for every $t$. Notably, our algorithms result in exponential improvements on the round complexity of previous methods."}}
{"id": "XBuJDZXcu2", "cdate": 1577836800000, "mdate": null, "content": {"title": "Maximizing Determinants under Matroid Constraints", "abstract": "Given vectors $v_1,\\dots,v_n\\in\\mathbb{R}^d$ and a matroid $M=([n],I)$, we study the problem of finding a basis $S$ of $M$ such that $\\det(\\sum_{i \\in S}v_i v_i^\\top)$ is maximized. This problem appears in a diverse set of areas such as experimental design, fair allocation of goods, network design, and machine learning. The current best results include an $e^{2k}$-estimation for any matroid of rank $k$ and a $(1+\\epsilon)^d$-approximation for a uniform matroid of rank $k\\ge d+\\frac d\\epsilon$, where the rank $k\\ge d$ denotes the desired size of the optimal set. Our main result is a new approximation algorithm with an approximation guarantee that depends only on the dimension $d$ of the vectors and not on the size $k$ of the output set. In particular, we show an $(O(d))^{d}$-estimation and an $(O(d))^{d^3}$-approximation for any matroid, giving a significant improvement over prior work when $k\\gg d$. Our result relies on the existence of an optimal solution to a convex programming relaxation for the problem which has sparse support; in particular, no more than $O(d^2)$ variables of the solution have fractional values. The sparsity results rely on the interplay between the first-order optimality conditions for the convex program and matroid theory. We believe that the techniques introduced to show sparsity of optimal solutions to convex programs will be of independent interest. We also give a randomized algorithm that rounds a sparse fractional solution to a feasible integral solution to the original problem. To show the approximation guarantee, we utilize recent works on strongly log-concave polynomials and show new relationships between different convex programs studied for the problem. Finally, we use the estimation algorithm and sparsity results to give an efficient deterministic approximation algorithm with an approximation guarantee that depends solely on the dimension $d$."}}
{"id": "SRsL5B-kC1O", "cdate": 1577836800000, "mdate": null, "content": {"title": "On the Computational Complexity of Linear Discrepancy", "abstract": "Many problems in computer science and applied mathematics require rounding a vector $\\mathbf{w}$ of fractional values lying in the interval $[0,1]$ to a binary vector $\\mathbf{x}$ so that, for a given matrix $\\mathbf{A}$, $\\mathbf{A}\\mathbf{x}$ is as close to $\\mathbf{A}\\mathbf{w}$ as possible. For example, this problem arises in LP rounding algorithms used to approximate $\\mathsf{NP}$-hard optimization problems and in the design of uniformly distributed point sets for numerical integration. For a given matrix $\\mathbf{A}$, the worst-case error over all choices of $\\mathbf{w}$ incurred by the best possible rounding is measured by the linear discrepancy of $\\mathbf{A}$, a quantity studied in discrepancy theory, and introduced by Lovasz, Spencer, and Vesztergombi (EJC, 1986). We initiate the study of the computational complexity of linear discrepancy. Our investigation proceeds in two directions: (1) proving hardness results and (2) finding both exact and approximate algorithms to evaluate the linear discrepancy of certain matrices. For (1), we show that linear discrepancy is $\\mathsf{NP}$-hard. Thus we do not expect to find an efficient exact algorithm for the general case. Restricting our attention to matrices with a constant number of rows, we present a poly-time exact algorithm for matrices consisting of a single row and matrices with a constant number of rows and entries of bounded magnitude. We also present an exponential-time approximation algorithm for general matrices, and an algorithm that approximates linear discrepancy to within an exponential factor."}}
{"id": "95kNOn3s29", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sticky Brownian Rounding and its Applications to Constraint Satisfaction Problems", "abstract": "Semi-definite programming is a powerful tool in the design and analysis of approximation algorithms for combinatorial optimization problems. In particular, the random hyperplane rounding method of Goemans and Williamson [23] has been extensively studied for more than two decades, resulting in various extensions to the original technique and beautiful algorithms for a wide range of applications. Despite the fact that this approach yields tight approximation guarantees for some problems, e.g., Max-Cut, for many others, e.g., Max-SAT and Max-DiCut, the tight approximation ratio is still unknown. One of the main reasons for this is the fact that very few techniques for rounding semi-definite relaxations are known. In this work, we present a new general and simple method for rounding semi-definite programs, based on Brownian motion. Our approach is inspired by recent results in algorithmic discrepancy theory. We develop and present tools for analyzing our new rounding algorithms, utilizing mathematical machinery from the theory of Brownian motion, complex analysis, and partial differential equations. Focusing on constraint satisfaction problems, we apply our method to several classical problems, including Max-Cut, Max-2SAT, and Max-DiCut, and derive new algorithms that are competitive with the best known results. To illustrate the versatility and general applicability of our approach, we give new approximation algorithms for the Max-Cut problem with side constraints that crucially utilizes measure concentration results for the Sticky Brownian Motion, a feature missing from hyperplane rounding and its generalizations."}}
{"id": "8OM_tmnBjpf", "cdate": 1577836800000, "mdate": null, "content": {"title": "The power of factorization mechanisms in local and central differential privacy", "abstract": "We give new characterizations of the sample complexity of answering linear queries (statistical queries) in the local and central models of differential privacy: (1) In the non-interactive local model, we give the first approximate characterization of the sample complexity. Informally our bounds are tight to within polylogarithmic factors in the number of queries and desired accuracy. Our characterization extends to agnostic learning in the local model. (2) In the central model, we give a characterization of the sample complexity in the high-accuracy regime that is analogous to that of Nikolov, Talwar, and Zhang (STOC 2013), but is both quantitatively tighter and has a dramatically simpler proof. Our lower bounds apply equally to the empirical and population estimation problems. In both cases, our characterizations show that a particular factorization mechanism is approximately optimal, and the optimal sample complexity is bounded from above and below by well studied factorization norms of a matrix associated with the queries."}}
{"id": "1rjXbWxiGN", "cdate": 1577836800000, "mdate": null, "content": {"title": "On the Computational Complexity of Linear Discrepancy", "abstract": "Many problems in computer science and applied mathematics require rounding a vector \ud835\udc30 of fractional values lying in the interval [0,1] to a binary vector \ud835\udc31 so that, for a given matrix \ud835\udc00, \ud835\udc00\ud835\udc31 is as close to \ud835\udc00\ud835\udc30 as possible. For example, this problem arises in LP rounding algorithms used to approximate NP-hard optimization problems and in the design of uniformly distributed point sets for numerical integration. For a given matrix \ud835\udc00, the worst-case error over all choices of \ud835\udc30 incurred by the best possible rounding is measured by the linear discrepancy of \ud835\udc00, a quantity studied in discrepancy theory, and introduced by Lovasz, Spencer, and Vesztergombi (EJC, 1986). We initiate the study of the computational complexity of linear discrepancy. Our investigation proceeds in two directions: (1) proving hardness results and (2) finding both exact and approximate algorithms to evaluate the linear discrepancy of certain matrices. For (1), we show that linear discrepancy is NP-hard. Thus we do not expect to find an efficient exact algorithm for the general case. Restricting our attention to matrices with a constant number of rows, we present a poly-time exact algorithm for matrices consisting of a single row and matrices with a constant number of rows and entries of bounded magnitude. We also present an exponential-time approximation algorithm for general matrices, and an algorithm that approximates linear discrepancy to within an exponential factor."}}
{"id": "V2ZneEAbr72", "cdate": 1546300800000, "mdate": null, "content": {"title": "Preconditioning for the Geometric Transportation Problem", "abstract": "In the geometric transportation problem, we are given a collection of points P in d-dimensional Euclidean space, and each point is given a supply of mu(p) units of mass, where mu(p) could be a positive or a negative integer, and the total sum of the supplies is 0. The goal is to find a flow (called a transportation map) that transports mu(p) units from any point p with mu(p) > 0, and transports -mu(p) units into any point p with mu(p) < 0. Moreover, the flow should minimize the total distance traveled by the transported mass. The optimal value is known as the transportation cost, or the Earth Mover's Distance (from the points with positive supply to those with negative supply). This problem has been widely studied in many fields of computer science: from theoretical work in computational geometry, to applications in computer vision, graphics, and machine learning. In this work we study approximation algorithms for the geometric transportation problem. We give an algorithm which, for any fixed dimension d, finds a (1+epsilon)-approximate transportation map in time nearly-linear in n, and polynomial in epsilon^{-1} and in the logarithm of the total supply. This is the first approximation scheme for the problem whose running time depends on n as n * polylog(n). Our techniques combine the generalized preconditioning framework of Sherman, which is grounded in continuous optimization, with simple geometric arguments to first reduce the problem to a minimum cost flow problem on a sparse graph, and then to design a good preconditioner for this latter problem."}}
