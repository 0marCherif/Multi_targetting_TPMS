{"id": "XEziLHkFHXc", "cdate": 1677610067958, "mdate": 1677610067958, "content": {"title": "Constants Matter: The Performance Gains of Active Learning", "abstract": "Within machine learning, active learning studies\nthe gains in performance made possible by adaptively selecting data points to label. In this work,\nwe show through upper and lower bounds, that for\na simple benign setting of well-specified logistic\nregression on a uniform distribution over a sphere,\nthe expected excess error of both active learning\nand random sampling have the same inverse proportional dependence on the number of samples.\nImportantly, due to the nature of lower bounds,\nany more general setting does not allow a better\ndependence on the number of samples. Additionally, we show a variant of uncertainty sampling\ncan achieve a faster rate of convergence than random sampling by a factor of the Bayes error, a\nrecent empirical observation made by other work.\nQualitatively, this work is pessimistic with respect\nto the asymptotic dependence on the number of\nsamples, but optimistic with respect to finding\nperformance gains in the constants."}}
{"id": "xcfmlB-wj-", "cdate": 1672531200000, "mdate": 1681761650578, "content": {"title": "VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building", "abstract": "We introduce VOCALExplore, a system designed to support users in building domain-specific models over video datasets. VOCALExplore supports interactive labeling sessions and trains models using user-supplied labels. VOCALExplore maximizes model quality by automatically deciding how to select samples based on observed skew in the collected labels. It also selects the optimal video representations to use when training models by casting feature selection as a rising bandit problem. Finally, VOCALExplore implements optimizations to achieve low latency without sacrificing model performance. We demonstrate that VOCALExplore achieves close to the best possible model quality given candidate acquisition functions and feature extractors, and it does so with low visible latency (~1 second per iteration) and no expensive preprocessing."}}
{"id": "tGHi1HFNBx1", "cdate": 1663850461474, "mdate": null, "content": {"title": "Data Subset Selection via Machine Teaching", "abstract": "We study the problem of data subset selection: given a fully labeled dataset and a training procedure, select a subset such that training on that subset yields approximately the same test performance as training on the full dataset. We propose an algorithm, inspired by recent work in machine teaching, that has theoretical guarantees, compelling empirical performance, and is model-agnostic meaning the algorithm's only information comes from the predictions of models trained on subsets. Furthermore, we prove lower bounds that show that our algorithm achieves a subset with near-optimal size (under computational hardness assumptions) while training on a number of subsets that is optimal up to extraneous log factors. We then empirically compare our algorithm, machine teaching algorithms, and coreset techniques on six common image datasets with convolutional neural networks. We find that our machine teaching algorithm can find a subset of CIFAR10 of size less than 16k that yields the same performance (5-6% error) as training on the full dataset of size 50k."}}
{"id": "YEqY-luge-", "cdate": 1640995200000, "mdate": 1681761650619, "content": {"title": "Active Learning with Expected Error Reduction", "abstract": "Active learning has been studied extensively as a method for efficient data collection. Among the many approaches in literature, Expected Error Reduction (EER) (Roy and McCallum) has been shown to be an effective method for active learning: select the candidate sample that, in expectation, maximally decreases the error on an unlabeled set. However, EER requires the model to be retrained for every candidate sample and thus has not been widely used for modern deep neural networks due to this large computational cost. In this paper we reformulate EER under the lens of Bayesian active learning and derive a computationally efficient version that can use any Bayesian parameter sampling method (such as arXiv:1506.02142). We then compare the empirical performance of our method using Monte Carlo dropout for parameter sampling against state of the art methods in the deep active learning literature. Experiments are performed on four standard benchmark datasets and three WILDS datasets (arXiv:2012.07421). The results indicate that our method outperforms all other methods except one in the data shift scenario: a model dependent, non-information theoretic method that requires an order of magnitude higher computational cost (arXiv:1906.03671)."}}
{"id": "d-yRANoLcW", "cdate": 1609459200000, "mdate": 1681761650577, "content": {"title": "Comparing the Value of Labeled and Unlabeled Data in Method-of-Moments Latent Variable Estimation", "abstract": "Labeling data for modern machine learning is expensive and time-consuming. Latent variable models can be used to infer labels from weaker, easier-to-acquire sources operating on unlabeled data. Such models can also be trained using labeled data, presenting a key question: should a user invest in few labeled or many unlabeled points? We answer this via a framework centered on model misspecification in method-of-moments latent variable estimation. Our core result is a bias-variance decomposition of the generalization error, which shows that the unlabeled-only approach incurs additional bias under misspecification. We then introduce a correction that provably removes this bias in certain cases. We apply our decomposition framework to three scenarios -- well-specified, misspecified, and corrected models -- to 1) choose between labeled and unlabeled data and 2) learn from their combination. We observe theoretically and with synthetic experiments that for well-specified models, labeled points are worth a constant factor more than unlabeled points. With misspecification, however, their relative value is higher due to the additional bias but can be reduced with correction. We also apply our approach to study real-world weak supervision techniques for dataset construction."}}
{"id": "VI92CIuuH5", "cdate": 1609459200000, "mdate": 1681761650622, "content": {"title": "Understanding and analyzing the effectiveness of uncertainty sampling", "abstract": ""}}
{"id": "1ap4zg-G-x4", "cdate": 1609459200000, "mdate": 1650336249072, "content": {"title": "Comparing the Value of Labeled and Unlabeled Data in Method-of-Moments Latent Variable Estimation", "abstract": "Labeling data for modern machine learning is expensive and time-consuming. Latent variable models can be used to infer labels from weaker, easier-to-acquire sources operating on unlabeled data. Such models can also be trained using labeled data, presenting a key question: should a user invest in few labeled or many unlabeled points? We answer this via a framework centered on model misspecification in method-of-moments latent variable estimation. Our core result is a bias-variance decomposition of the generalization error, which shows that the unlabeled-only approach incurs additional bias under misspecification. We then introduce a correction that provably removes this bias in certain cases. We apply our decomposition framework to three scenarios\u2014well-specified, misspecified, and corrected models\u2014to 1) choose between labeled and unlabeled data and 2) learn from their combination. We observe theoretically and with synthetic experiments that for well-specified models, labeled points are worth a constant factor more than unlabeled points. With misspecification, however, their relative value is higher due to the additional bias but can be reduced with correction. We also apply our approach to study real-world weak supervision techniques for dataset construction."}}
{"id": "6585x8pW9jS", "cdate": 1598752729828, "mdate": null, "content": {"title": "A tight analysis of greedy yields subexponential time approximation for uniform decision tree", "abstract": "Decision Tree is a classic formulation of active learning: given n hypotheses with nonnegative weights summing to 1 and a set of tests that each partition the hypotheses, output a decision tree using the provided tests that uniquely identifies each hypothesis and has minimum (weighted) average depth. Previous works showed that the greedy algorithm achieves a O(log n)\napproximation ratio for this problem and it is NP-hard beat a O(log n) approximation, settling the complexity of the problem. However, for Uniform Decision Tree, i.e. Decision Tree with uniform weights, the story\nis more subtle. The greedy algorithm\u2019s O(log n) approximation ratio was the best known, but the\nlargest approximation ratio known to be NP-hard is 4 \u2212 \u03b5. We prove that the greedy algorithm\ngives a O(log n log COPT) approximation for Uniform Decision Tree, where COPT is the cost of the\noptimal tree and show this is best possible for the greedy algorithm. As a corollary, we resolve a\nconjecture of Kosaraju, Przytycka, and Borgstrom [KPB99]. Our results also hold for instances\nof Decision Tree whose weights are not too far from uniform. Leveraging this result, for all\n\u03b1 \u2208 (0, 1), we exhibit a 9.01\u03b1 approximation algorithm to Uniform Decision Tree running in\nsubexponential time 2O\u02dc(n\u03b1). As a corollary, achieving any super-constant approximation ratio\non Uniform Decision Tree is not NP-hard, assuming the Exponential Time Hypothesis. This\nwork therefore adds approximating Uniform Decision Tree to a small list of natural problems\nthat have subexponential time algorithms but no known polynomial time algorithms. Like the\nanalysis of the greedy algorithm, our analysis of the subexponential time algorithm gives similar\napproximation guarantees even for slightly nonuniform weights. A key technical contribution\nof our work is showing a connection between greedy algorithms for Uniform Decision Tree\nand for Min Sum Set Cover.\n"}}
{"id": "x9bQnmNDW8T", "cdate": 1577836800000, "mdate": 1681761650577, "content": {"title": "A Tight Analysis of Greedy Yields Subexponential Time Approximation for Uniform Decision Tree", "abstract": "Decision Tree is a classic formulation of active learning: given n hypotheses with nonnegative weights summing to 1 and a set of tests that each partition the hypotheses, output a decision tree using the provided tests that uniquely identifies each hypothesis and has minimum (weighted) average depth. Previous works showed that the greedy algorithm achieves a O(log n) approximation ratio for this problem and it is NP-hard beat a O(log n) approximation, settling the complexity of the problem. However, for Uniform Decision Tree, i.e. Decision Tree with uniform weights, the story is more subtle. The greedy algorithm's O(log n) approximation ratio was the best known, but the largest approximation ratio known to be NP-hard is 4 \u2013 \u03b5. We prove that the greedy algorithm gives a approximation for Uniform Decision Tree, where COPT is the cost of the optimal tree and show this is best possible for the greedy algorithm. As a corollary, we resolve a conjecture of Kosaraju, Przytycka, and Borgstrom [20]. Our results also hold for instances of DecisioN Tree whose weights are not too far from uniform. Leveraging this result, for all \u03b1 \u03f5 (0, 1), we exhibit a approximation algorithm to Uniform Decision Tree running in subexponential time . As a corollary, achieving any super-constant approximation ratio on Uniform Decision Tree is not NP-hard, assuming the Exponential Time Hypothesis. This work therefore adds approximating Uniform Decision Tree to a small list of natural problems that have subexponential time algorithms but no known polynomial time algorithms. Like the analysis of the greedy algorithm, our analysis of the subexponential time algorithm gives similar approximation guarantees even for slightly nonuniform weights. A key technical contribution of our work is showing a connection between greedy algorithms for Uniform Decision Tree and for Min Sum Set Cover."}}
{"id": "wh5B6teT0W", "cdate": 1577836800000, "mdate": null, "content": {"title": "Concept Bottleneck Models", "abstract": "We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art..."}}
