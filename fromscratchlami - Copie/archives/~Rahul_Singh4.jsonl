{"id": "gqmcptDnRD", "cdate": 1683229331608, "mdate": 1683229331608, "content": {"title": "Generalized Kernel Ridge Regression for Long Term Causal Inference: Treatment Effects, Dose Responses, and Counterfactual Distributions", "abstract": "I propose kernel ridge regression estimators for long term causal inference, where a short term experimental data set containing randomized treatment and short term surrogates is fused with a long term observational data set containing short term surrogates and long term outcomes. I propose estimators of treatment effects, dose responses, and counterfactual distributions with closed form solutions in terms of kernel matrix operations. I allow covariates, treatment, and surrogates to be discrete or continuous, and low, high, or infinite dimensional. For long term treatment effects, I prove  consistency, Gaussian approximation, and semiparametric efficiency. For long term dose responses, I prove uniform consistency with finite sample rates. For long term counterfactual distributions, I prove convergence in distribution."}}
{"id": "RVqADXLwl_", "cdate": 1683229298451, "mdate": 1683229298451, "content": {"title": "Generalized Kernel Ridge Regression for Causal Inference with Missing-at-Random Sample Selection", "abstract": "I propose kernel ridge regression estimators for nonparametric dose response curves and semiparametric treatment effects in the setting where an analyst has access to a selected sample rather than a random sample; only for select observations, the outcome is observed. I assume selection is as good as random conditional on treatment and a sufficiently rich set of observed covariates, where the covariates are allowed to cause treatment or be caused by treatment -- an extension of missingness-at-random (MAR). I propose estimators of means, increments, and distributions of counterfactual outcomes with closed form solutions in terms of kernel matrix operations, allowing treatment and covariates to be discrete or continuous, and low, high, or infinite dimensional. For the continuous treatment case, I prove uniform consistency with finite sample rates. For the discrete treatment case, I prove root-n consistency, Gaussian approximation, and semiparametric efficiency."}}
{"id": "OJhFIMP8bqa", "cdate": 1683229276688, "mdate": 1683229276688, "content": {"title": "Automatic Debiased Machine Learning for Dynamic Treatment Effects and General Nested Functionals", "abstract": "\nWe extend the idea of automated debiased machine learning to the dynamic treatment regime and more generally to nested functionals. We show that the multiply robust formula for the dynamic treatment regime with discrete treatments can be re-stated in terms of a recursive Riesz representer characterization of nested mean regressions. We then apply a recursive Riesz representer estimation learning algorithm that estimates de-biasing corrections without the need to characterize how the correction terms look like, such as for instance, products of inverse probability weighting terms, as is done in prior work on doubly robust estimation in the dynamic regime. Our approach defines a sequence of loss minimization problems, whose minimizers are the mulitpliers of the de-biasing correction, hence circumventing the need for solving auxiliary propensity models and directly optimizing for the mean squared error of the target de-biasing correction. We provide further applications of our approach to estimation of dynamic discrete choice models and estimation of long-term effects with surrogates."}}
{"id": "SCw8bF3G6q_", "cdate": 1683229234531, "mdate": 1683229234531, "content": {"title": "Debiased Kernel Methods", "abstract": "I propose a practical procedure based on bias correction and sample splitting to calculate confidence intervals for functionals of generic kernel methods, i.e. nonparametric estimators learned in a reproducing kernel Hilbert space (RKHS). For example, an analyst may desire confidence intervals for functionals of kernel ridge regression. I propose a bias correction that mirrors kernel ridge regression. The framework encompasses (i) evaluations over discrete domains, (ii) derivatives over continuous domains, (iii) treatment effects of discrete treatments, and (iv) incremental treatment effects of continuous treatments. For the target quantity, whether it is (i)-(iv), I prove root-n consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. I show that the classic assumptions of RKHS learning theory also imply inference."}}
{"id": "CKY1WbDh0B", "cdate": 1683229214228, "mdate": 1683229214228, "content": {"title": "Kernel Methods for Multistage Causal Inference: Mediation Analysis and Dynamic Treatment Effects", "abstract": "We propose simple estimators for mediation analysis and dynamic treatment effects over short horizons based on kernel ridge regression. We study both nonparametric response curves and semiparametric treatment effects, allowing treatments, mediators, and covariates to be continuous or discrete in general spaces. Our key innovation is a new RKHS technique called sequential mean embedding, which facilitates the construction of simple estimators for complex causal estimands, including new estimands without existing alternatives. In particular, we propose machine learning estimators of dynamic dose response curves and dynamic counterfactual distributions without restrictive linearity, Markov, or no-effect-modification assumptions. Our simple estimators preserve the generality of classic identification while also achieving nonasymptotic uniform rates for causal functions and semiparametric efficiency for causal scalars. In nonlinear simulations with many covariates, we demonstrate state-of-the-art performance. We estimate mediated and dynamic response curves of the US Job Corps program for disadvantaged youth, and share a data set that may serve as a benchmark in future work."}}
{"id": "yLH4Ox8OhZz", "cdate": 1683229183037, "mdate": 1683229183037, "content": {"title": "A Finite Sample Theorem for Longitudinal Causal Inference with Machine Learning: Long Term, Dynamic, and Mediated Effects", "abstract": "I construct and justify confidence intervals for longitudinal causal parameters estimated with machine learning. Longitudinal parameters include long term, dynamic, and mediated effects. I provide a nonasymptotic theorem for any longitudinal causal parameter estimated with any machine learning algorithm that satisfies a few simple, interpretable conditions. The main result encompasses local parameters defined for specific demographics as well as proximal parameters defined in the presence of unobserved confounding. Formally, I prove consistency, Gaussian approximation, and semiparametric efficiency. The rate of convergence is  for global parameters, and it degrades gracefully for local parameters. I articulate a simple set of conditions to translate mean square rates into statistical inference. A key feature of the main result is a new multiple robustness to ill posedness for proximal causal inference in longitudinal settings."}}
{"id": "MuD016TY26z", "cdate": 1683229162538, "mdate": 1683229162538, "content": {"title": "Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy", "abstract": "The 2020 US Census will be published with differential privacy, implemented by injecting synthetic noise into the data. Controversy has ensued, with debates that center on the painful trade-off between the privacy of respondents and the precision of economic analysis. Is this trade-off inevitable? To answer this question, we formulate a semiparametric model of causal inference with high dimensional data that may be noisy, missing, discretized, or privatized. We propose a new end-to-end procedure for data cleaning, estimation, and inference with data cleaning-adjusted confidence intervals. We prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. The rate of Gaussian approximation is  for semiparametric estimands such as average treatment effect, and it degrades gracefully for nonparametric estimands such as heterogeneous treatment effect. Our key assumption is that the true covariates are approximately low rank, which we interpret as approximate repeated measurements and validate in the Census. In our analysis, we provide nonasymptotic theoretical contributions to matrix completion, statistical learning, and semiparametric statistics. We verify the coverage of the data cleaning-adjusted confidence intervals in simulations. Finally, we conduct a semi-synthetic exercise calibrated to privacy levels mandated for the 2020 US Census."}}
{"id": "W8MgrxboUd0", "cdate": 1683229125513, "mdate": 1683229125513, "content": {"title": "A simple and general debiased machine learning theorem with finite-sample guarantees", "abstract": "Debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e. scalar summaries,\nof machine learning algorithms. For example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network. We provide a nonasymptotic debiased machine learning theorem that encompasses any\nglobal or local functional of any machine learning algorithm that satisfies a few\nsimple, interpretable conditions. Formally, we prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. The rate\nof convergence is n\n\u22121/2\nfor global functionals, and it degrades gracefully for local\nfunctionals. Our results culminate in a simple set of conditions that an analyst can\nuse to translate modern learning theory rates into traditional statistical inference.\nThe conditions reveal a general double robustness property for ill posed inverse\nproblems."}}
{"id": "ZWGza81DtUF", "cdate": 1683229082629, "mdate": 1683229082629, "content": {"title": "Double Robustness for Complier Parameters and a Semiparametric Test for Complier Characteristics", "abstract": "We propose a semiparametric test to evaluate (i) whether different instruments induce subpopulations of compliers with the same observable characteristics on average, and (ii) whether compliers have observable characteristics that are the same as the full population on average. The test is a flexible robustness check for the external validity of instruments. We use it to reinterpret the difference in LATE estimates that Angrist and Evans (1998) obtain when using different instrumental variables. To justify the test, we characterize the doubly robust moment for Abadie (2003)'s class of complier parameters, and we analyze a machine learning update to  weighting."}}
{"id": "BJCTRhmnMrg", "cdate": 1683229053266, "mdate": 1683229053266, "content": {"title": "Kernel Methods for Causal Functions: Dose, Heterogeneous, and Incremental Response Curves", "abstract": "We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves. Treatment and covariates may be discrete or continuous in general spaces. Due to a decomposition property specific to the RKHS, our estimators have simple closed form solutions. We prove uniform consistency with improved finite sample rates, via original analysis of generalized kernel ridge regression. We extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria. In nonlinear simulations with many covariates, we achieve state-of-the-art performance."}}
