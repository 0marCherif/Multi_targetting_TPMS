{"id": "jaMEBCLv4rZ", "cdate": 1683906229553, "mdate": 1683906229553, "content": {"title": "Pure exploration and regret minimization in matching bandits", "abstract": "Finding an optimal matching in a weighted graph is a standard combinatorial problem. We consider its semi-bandit version where either a pair or a full matching is sampled sequentially. We prove that it is possible to leverage a rank-1 assumption on the adjacency matrix to reduce the sample complexity and the regret of off-the-shelf algorithms up to reaching a linear dependency in the number of vertices (up to to poly-log terms)."}}
{"id": "8x-Om4LJbPk", "cdate": 1640995200000, "mdate": 1649229135885, "content": {"title": "Learning in Repeated Auctions", "abstract": "Learning in Repeated Auctions"}}
{"id": "6aPwpRpzLf", "cdate": 1640995200000, "mdate": 1649229135775, "content": {"title": "Jointly Efficient and Optimal Algorithms for Logistic Bandits", "abstract": "Logistic Bandits have recently undergone careful scrutiny by virtue of their combined theoretical and practical relevance. This research effort delivered statistically efficient algorithms, improving the regret of previous strategies by exponentially large factors. Such algorithms are however strikingly costly as they require $\\Omega(t)$ operations at each round. On the other hand, a different line of research focused on computational efficiency ($\\mathcal{O}(1)$ per-round cost), but at the cost of letting go of the aforementioned exponential improvements. Obtaining the best of both world is unfortunately not a matter of marrying both approaches. Instead we introduce a new learning procedure for Logistic Bandits. It yields confidence sets which sufficient statistics can be easily maintained online without sacrificing statistical tightness. Combined with efficient planning mechanisms we design fast algorithms which regret performance still match the problem-dependent lower-bound of Abeille et al. (2021). To the best of our knowledge, those are the first Logistic Bandit algorithms that simultaneously enjoy statistical and computational efficiency."}}
{"id": "zRcNfSUFoA", "cdate": 1609459200000, "mdate": 1649229135887, "content": {"title": "A Technical Note on Non-Stationary Parametric Bandits: Existing Mistakes and Preliminary Solutions", "abstract": "In this note we identify several mistakes appearing in the existing literature on non-stationary parametric bandits. More precisely, we study Generalized Linear Bandits (GLBs) in drifting environme..."}}
{"id": "xf7KgbnL2fH", "cdate": 1609459200000, "mdate": 1649229135886, "content": {"title": "Regret Bounds for Generalized Linear Bandits under Parameter Drift", "abstract": "Generalized Linear Bandits (GLBs) are powerful extensions to the Linear Bandit (LB) setting, broadening the benefits of reward parametrization beyond linearity. In this paper we study GLBs in non-stationary environments, characterized by a general metric of non-stationarity known as the variation-budget or \\emph{parameter-drift}, denoted $B_T$. While previous attempts have been made to extend LB algorithms to this setting, they overlook a salient feature of GLBs which flaws their results. In this work, we introduce a new algorithm that addresses this difficulty. We prove that under a geometric assumption on the action set, our approach enjoys a $\\tilde{\\mathcal{O}}(B_T^{1/3}T^{2/3})$ regret bound. In the general case, we show that it suffers at most a $\\tilde{\\mathcal{O}}(B_T^{1/5}T^{4/5})$ regret. At the core of our contribution is a generalization of the projection step introduced in Filippi et al. (2010), adapted to the non-stationary nature of the problem. Our analysis sheds light on central mechanisms inherited from the setting by explicitly splitting the treatment of the learning and tracking aspects of the problem."}}
{"id": "cWRwTU_Ada4", "cdate": 1609459200000, "mdate": 1649229135776, "content": {"title": "Pure Exploration and Regret Minimization in Matching Bandits", "abstract": "Finding an optimal matching in a weighted graph is a standard combinatorial problem. We consider its semi-bandit version where either a pair or a full matching is sampled sequentially. We prove tha..."}}
{"id": "IbBp-sJwhUV", "cdate": 1609459200000, "mdate": 1649229135773, "content": {"title": "Instance-Wise Minimax-Optimal Algorithms for Logistic Bandits", "abstract": "Logistic Bandits have recently attracted substantial attention, by providing an uncluttered yet challenging framework for understanding the impact of non-linearity in parametrized bandits. It was shown by Faury et al. (2020) that the learning-theoretic difficulties of Logistic Bandits can be embodied by a large (sometimes prohibitively) problem-dependent constant $\\kappa$, characterizing the magnitude of the reward\u2019s non-linearity. In this paper we introduce an algorithm for which we provide a refined analysis. This allows for a better characterization of the effect of non-linearity and yields improved problem-dependent guarantees. In most favorable cases this leads to a regret upper-bound scaling as $\\tilde{\\mathcal{O}}(d\\sqrt{T/\\kappa})$, which dramatically improves over the $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\kappa)$ state-of-the-art guarantees. We prove that this rate is \\emph{minimax-optimal} by deriving a $\\Omega(d\\sqrt{T/\\kappa})$ problem-dependent lower-bound. Our analysis identifies two regimes (permanent and transitory) of the regret, which ultimately re-conciliates (Faury et al., 2020) with the Bayesian approach of Dong et al. (2019). In contrast to previous works, we find that in the permanent regime non-linearity can dramatically ease the exploration-exploitation trade-off. While it also impacts the length of the transitory phase in a problem-dependent fashion, we show that this impact is mild in most reasonable configurations."}}
{"id": "fabfWf3JJQi", "cdate": 1602926479334, "mdate": null, "content": {"title": "Wasserstein Learning of Determinantal Point Processes", "abstract": "Determinantal point processes (DPPs) have received significant attention as an elegant probabilistic model for discrete subset selection.  Most prior work on DPP learning focuses on maximum likelihood estimation (MLE).  While efficient and scalable, MLE approaches do not leverage any subset similarity information and may fail to recover the true generative distribution of discrete data. In this work, by deriving a differentiable relaxation of a DPP sampling algorithm, we present a novel approach for learning DPPs that minimizes the Wasserstein distance between the model and data composed of observed subsets. Through an evaluation on a real-world dataset, we show that our Wasserstein learning approach provides significantly improved predictive performance on a generative task compared to DPPs trained using MLE."}}
{"id": "xAu71MK7zZv", "cdate": 1577836800000, "mdate": 1649229135882, "content": {"title": "Real-Time Optimisation for Online Learning in Auctions", "abstract": "In display advertising, a small group of sellers and bidders face each other in up to $10^{12}$ auctions a day. In this context, revenue maximisation via monopoly price learning is a high-value pro..."}}
{"id": "mV7aY-Y-YNO", "cdate": 1577836800000, "mdate": 1649229135775, "content": {"title": "Robust Stackelberg buyers in repeated auctions", "abstract": "We consider the practical and classical setting where the seller is using an exploration stage to learn the value distributions of the bidders before running a revenue-maximizing auction in a explo..."}}
