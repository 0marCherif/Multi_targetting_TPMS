{"id": "uBqcJ8QEe1", "cdate": 1676827090782, "mdate": null, "content": {"title": "Learning from Low Rank Tensor Data: A Random Tensor Theory Perspective", "abstract": "Under a simplified data model, this paper provides a theoretical analysis of learning from data that have an underlying low-rank tensor structure in both supervised and unsupervised settings. For the supervised setting, we provide an analysis of a Ridge classifier (with high regularization parameter) with and without knowledge of the low-rank structure of the data. Our results quantify analytically the gain in misclassification errors achieved by exploiting the low-rank structure for denoising purposes, as opposed to treating data as mere vectors. We further provide a similar analysis in the context of clustering, thereby quantifying the exact performance gap between tensor methods and standard approaches which treat data as simple vectors."}}
{"id": "4CQ9os3s4h3", "cdate": 1663850298348, "mdate": null, "content": {"title": "Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption", "abstract": "We propose a theoretical framework to analyze semi-supervised classification under the low density separation assumption in a high-dimensional regime. In particular, we introduce QLDS, a linear classification model, where the low density separation assumption is implemented via quadratic margin maximization.\nThe algorithm has an explicit solution with rich theoretical properties, and we show that particular cases of our algorithm are the least-square support vector machine in the supervised case, the spectral clustering in the fully unsupervised regime, and a class of semi-supervised graph-based approaches. As such, QLDS establishes a smooth bridge between these supervised and unsupervised learning methods. Using recent advances in the random matrix theory, we formally derive a theoretical evaluation of the classification error in the asymptotic regime.\nAs an application, we derive a hyperparameter selection policy that finds the best balance between the supervised and the unsupervised terms of our learning criterion.\nFinally, we provide extensive illustrations of our framework, as well as an experimental study on several benchmarks to demonstrate that QLDS, while being computationally more efficient, improves over cross-validation for hyperparameter selection, indicating a high promise of the usage of random matrix theory for semi-supervised model selection."}}
{"id": "Vf6WcUDnY7c", "cdate": 1663850285867, "mdate": null, "content": {"title": "Optimizing Spca-based Continual Learning: A Theoretical Approach", "abstract": "Catastrophic forgetting and the stability-plasticity dilemma are two major obstacles to continual learning. In this paper we first propose a theoretical analysis of a SPCA-based continual learning algorithm using high dimensional statistics. Second, we design OSCL  (Optimized Spca-based Continual Learning) which builds on a flexible task optimization based on the theory. By optimizing a single task, catastrophic forgetting can be prevented theoretically. While optimizing multi-tasks, the trade-off between integrating knowledge from the new task and retaining previous knowledge of the old task can be achieved by assigning appropriate weights to corresponding tasks in compliance with the objectives. Experimental results confirm that the various theoretical conclusions are robust to a wide range of data distributions. Besides, several applications on synthetic and real data show that the proposed method while being computationally efficient, achieves comparable results with some state of the art."}}
{"id": "Cri3xz59ga", "cdate": 1601308138106, "mdate": null, "content": {"title": "Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach", "abstract": "This article provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large ($p$) and numerous ($n$) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge, as $n,p\\to\\infty$, to a deterministic limit involving simple (small-dimensional) statistics of the data.\n\nWe prove (i) that the standard MTL LS-SVM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): our analysis provides a simple method to correct these biases, and that we reveal (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even for quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. \n\nExperiments on popular datasets demonstrate that our improved MTL LS-SVM method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi-task and transfer learning techniques."}}
{"id": "4W_gfyAVR2E", "cdate": 1577836800000, "mdate": null, "content": {"title": "Large Dimensional Asymptotics of Multi-Task Learning", "abstract": "Inspired by human learning, which transfers knowledge from learned tasks to solve new tasks, multitask learning aims at simultaneously solving multiple tasks by a smart exploitation of their similarities. How to relate the tasks so to optimize their performances is however a largely open problem.Based on a random matrix approach, this article proposes an asymptotic analysis of a support vector machine-inspired multitask learning scheme. The asymptotic performance of the algorithm, validated on both synthetic and real data, sets forth the relation between the statistics of the data in each task and the hyperparameters relating the tasks together. The article, as such, provides first insights on an offline control of multitask learning, which finds natural connections to the currently popular transfer learning paradigm."}}
{"id": "r1--gsWO-H", "cdate": 1546300800000, "mdate": null, "content": {"title": "Random Matrix Improved Covariance Estimation for a Large Class of Metrics", "abstract": "Relying on recent advances in statistical estimation of covariance distances based on random matrix theory, this article proposes an improved covariance and precision matrix estimation for a wide f..."}}
{"id": "lOx28hnahJ7", "cdate": 1546300800000, "mdate": null, "content": {"title": "Random Matrix-Improved Estimation of the Wasserstein Distance between two Centered Gaussian Distributions", "abstract": "This article proposes a method to consistently estimate functionals $\\frac1p\\sum_{i=1}^pf(\\lambda_i(C_1C_2))$ of the eigenvalues of the product of two covariance matrices $C_1,C_2\\in\\mathbb{R}^{p\\times p}$ based on the empirical estimates $\\lambda_i(\\hat C_1\\hat C_2)$ ($\\hat C_a=\\frac1{n_a}\\sum_{i=1}^{n_a} x_i^{(a)}x_i^{(a){{\\sf T}}}$), when the size $p$ and number $n_a$ of the (zero mean) samples $x_i^{(a)}$ are similar. As a corollary, a consistent estimate of the Wasserstein distance (related to the case $f(t)=\\sqrt{t}$) between centered Gaussian distributions is derived. The new estimate is shown to largely outperform the classical sample covariance-based `plug-in' estimator. Based on this finding, a practical application to covariance estimation is then devised which demonstrates potentially significant performance gains with respect to state-of-the-art alternatives."}}
{"id": "hZzoi5Wc2Cj", "cdate": 1546300800000, "mdate": null, "content": {"title": "Random Matrix-Improved Estimation of the Wasserstein Distance between two Centered Gaussian Distributions", "abstract": "This article proposes a method to consistently estimate functionals 1/p\u03a3 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i=1</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">p</sup> f(\u03bb <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> (C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> )) of the eigenvalues of the product of two covariance matrices C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> ,C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> \u2208R <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">p\u00d7p</sup> based on the empirical estimates \u03bb <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> (\u0108 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> \u0108 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> )(\u0108 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">a</sub> = 1/n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">a</sub> \u03a3 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i=1</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">na</sup> x <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(a)</sup> x <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(a)</sup> ), when the size p and number n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">a</sub> of the (zero mean) samples x <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(a)</sup> are similar. As a corollary, a consistent estimate of the Wasserstein distance (related to the case f(t) = \u221at) between centered Gaussian distributions is derived. The new estimate is shown to largely outperform the classical sample covariance-based `plug-in' estimator. Based on this finding, a practical application to covariance estimation is then devised which demonstrates potentially significant performance gains with respect to state-of-the-art alternatives."}}
{"id": "6qDKhVLV8Oe", "cdate": 1546300800000, "mdate": null, "content": {"title": "Random matrix-improved estimation of covariance matrix distances", "abstract": "Given two sets x 1 ( 1 ) , \u2026 , x n 1 ( 1 ) and x 1 ( 2 ) , \u2026 , x n 2 ( 2 ) \u2208 R p (or \u2102 p ) of random vectors with zero mean and positive definite covariance matrices C 1 and C 2 \u2208 R p \u00d7 p (or \u2102 p \u00d7 p ), respectively, this article provides novel estimators for a wide range of distances between C 1 and C 2 (along with divergences between some zero mean and covariance C 1 or C 2 probability measures) of the form 1 p \u2211 i = 1 n f ( \u03bb i ( C 1 \u2212 1 C 2 ) ) (with \u03bb i ( X ) the eigenvalues of matrix X ). These estimators are derived using recent advances in the field of random matrix theory and are asymptotically consistent as n 1 , n 2 , p \u2192 \u221e with non trivial ratios p \u2215 n 1 < 1 and p \u2215 n 2 < 1 (the case p \u2215 n 2 > 1 is also discussed). A first \u201cgeneric\u201d estimator, valid for a large set of f functions, is provided under the form of a complex integral. Then, for a selected set of atomic functions f which can be linearly combined into elaborate distances of practical interest (namely, f ( t ) = t , f ( t ) = ln ( t ) , f ( t ) = ln ( 1 + s t ) and f ( t ) = ln 2 ( t ) ), a closed-form expression is provided. Besides theoretical findings, simulation results suggest an outstanding performance advantage for the proposed estimators when compared to the classical \u201cplug-in\u201d estimator 1 p \u2211 i = 1 n f ( \u03bb i ( C \u02c6 1 \u2212 1 C \u02c6 2 ) ) (with C \u02c6 a = 1 n a \u2211 i = 1 n a x i ( a ) x i ( a ) \u22a4 ), and this even for very small values of n 1 , n 2 , p . A concrete application to kernel spectral clustering of covariance classes supports this claim."}}
{"id": "38K9BT8Ik1", "cdate": 1546300800000, "mdate": null, "content": {"title": "Estimation of Covariance Matrix Distances in the High Dimension Low Sample Size Regime", "abstract": "A broad family of distances between two covariance matrices C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> \u2208 R <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">p\u00d7p</sup> , among which the Frobenhius, Fisher, Battacharrya distances as well as the Kullback-Leibler, R\u00e9nyi and Wasserstein divergence for centered Gaussian distributions can be expressed as functionals [1/p] <b xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u03a3</b> <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i=1</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">p</sup> f(\u03bb <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> (C1 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-1</sup> C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> )) or [1/p] <b xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u03a3</b> <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i=1</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">p</sup> f(\u03bb <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> (C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> )) of the eigenvalue distribution of C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-1</sup> C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> or C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> . Consistent estimates of such distances based on few (n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> ) samples x <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> \u2208 R <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">p</sup> having covariance C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , C <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> have been recently proposed using random matrix tools in the regime where n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> ~ p. These estimates however demand that n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> > p for most functions f. The article proposes to alleviate this limitation using a polynomial approximation approach. The proposed method is supported by simulations in practical applications."}}
