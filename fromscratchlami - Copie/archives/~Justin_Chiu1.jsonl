{"id": "iwwGb62y5Qr", "cdate": 1672531200000, "mdate": 1694965663499, "content": {"title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations", "abstract": ""}}
{"id": "h9KrUP1BEP", "cdate": 1672531200000, "mdate": 1687833884421, "content": {"title": "HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision", "abstract": "Explainable multi-hop question answering (QA) not only predicts answers but also identifies rationales, i. e. subsets of input sentences used to derive the answers. This problem has been extensively studied under the supervised setting, where both answer and rationale annotations are given. Because rationale annotations are expensive to collect and not always available, recent efforts have been devoted to developing methods that do not rely on supervision for rationales. However, such methods have limited capacities in modeling interactions between sentences, let alone reasoning across multiple documents. This work proposes a principled, probabilistic approach for training explainable multi-hop QA systems without rationale supervision. Our approach performs multi-hop reasoning by explicitly modeling rationales as sets, enabling the model to capture interactions between documents and sentences within a document. Experimental results show that our approach is more accurate at selecting rationales than the previous methods, while maintaining similar accuracy in predicting answers."}}
{"id": "PkHSHZLig5H", "cdate": 1654191669797, "mdate": null, "content": {"title": "Modeling Perspective-Dependent Ambiguity in Collaborative Dialogue", "abstract": "Errors in reference generation and resolution can occur if a dialogue agent reasons incorrectly about their partner's perspective. We present, in a collaborative and visually grounded setting, a dialogue planner that infers its partner's perspective to produce referring expressions that the partner resolves correctly. The dialogue planner models the partner perspective as a latent variable, embedded in a partner model that is used for both model-based planning and incorporating evidence from partner responses. We validate our approach on \\textsc{OneCommon}, a challenging dialogue game where players have large differences in their perspectives. In symbolic selfplay, where agents partner with a copy of themselves using symbolic communication, the dialogue planner improves over a planning baseline that does not reason about the partner's full perspective."}}
{"id": "zieN_RnVGsA", "cdate": 1650667253689, "mdate": 1650667253689, "content": {"title": "Reference-centric models for grounded collaborative dialogue", "abstract": "We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner\u2019s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa, 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades.  Our agent substantially outperforms the previous state of the art for the task, obtaining a 20% relative improvement in successful task completion in self-play evaluations and a 50% relative improvement in success in human evaluations."}}
{"id": "71r2-Asl4-", "cdate": 1640995200000, "mdate": 1683926582700, "content": {"title": "Unsupervised Text Deidentification", "abstract": ""}}
{"id": "2h-PvvMgt3m", "cdate": 1640995200000, "mdate": 1694965663661, "content": {"title": "Teal: Learning-Accelerated Optimization of Traffic Engineering", "abstract": "The rapid expansion of global cloud wide-area networks (WANs) has posed a challenge for commercial optimization engines to efficiently solve network traffic engineering (TE) problems at scale. Existing acceleration strategies decompose TE optimization into concurrent subproblems but realize limited parallelism due to an inherent tradeoff between run time and allocation performance. We present Teal, a learning-based TE algorithm that leverages the parallel processing power of GPUs to accelerate TE control. First, Teal designs a flow-centric graph neural network (GNN) to capture WAN connectivity and network flows, learning flow features as inputs to downstream allocation. Second, to reduce the problem scale and make learning tractable, Teal employs a multi-agent reinforcement learning (RL) algorithm to independently allocate each traffic demand while optimizing a central TE objective. Finally, Teal fine-tunes allocations with ADMM (Alternating Direction Method of Multipliers), a highly parallelizable optimization algorithm for reducing constraint violations such as overutilized links. We evaluate Teal using traffic matrices from Microsoft's WAN. On a large WAN topology with >1,700 nodes, Teal generates near-optimal flow allocations while running several orders of magnitude faster than the production optimization engine. Compared with other TE acceleration schemes, Teal satisfies 6--32% more traffic demand and yields 197--625x speedups."}}
{"id": "Mcldz4OJ6QB", "cdate": 1621630171779, "mdate": null, "content": {"title": "Low-Rank Constraints for Fast Inference in Structured Models", "abstract": "Structured distributions, i.e. distributions over combinatorial spaces, are commonly used to learn latent probabilistic representations from observed data. However, scaling these models is bottlenecked by the high computational and memory complexity with respect to the size of the latent representations. Common models such as Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the number of hidden states respectively. This work demonstrates a simple approach to reduce the computational and memory complexity of a large class of structured models. We show that by viewing the central inference step as a matrix-vector product and using a low-rank constraint, we can trade off model expressivity and speed via the rank.  Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups."}}
{"id": "9Zi7t3c5CuF", "cdate": 1621630171779, "mdate": null, "content": {"title": "Low-Rank Constraints for Fast Inference in Structured Models", "abstract": "Structured distributions, i.e. distributions over combinatorial spaces, are commonly used to learn latent probabilistic representations from observed data. However, scaling these models is bottlenecked by the high computational and memory complexity with respect to the size of the latent representations. Common models such as Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the number of hidden states respectively. This work demonstrates a simple approach to reduce the computational and memory complexity of a large class of structured models. We show that by viewing the central inference step as a matrix-vector product and using a low-rank constraint, we can trade off model expressivity and speed via the rank.  Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups."}}
{"id": "cYOilg68eUR", "cdate": 1609459200000, "mdate": 1694965663681, "content": {"title": "Low-Rank Constraints for Fast Inference in Structured Models", "abstract": "Structured distributions, i.e. distributions over combinatorial spaces, are commonly used to learn latent probabilistic representations from observed data. However, scaling these models is bottlenecked by the high computational and memory complexity with respect to the size of the latent representations. Common models such as Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the number of hidden states respectively. This work demonstrates a simple approach to reduce the computational and memory complexity of a large class of structured models. We show that by viewing the central inference step as a matrix-vector product and using a low-rank constraint, we can trade off model expressivity and speed via the rank. Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups."}}
{"id": "aNMO3SupMD", "cdate": 1609459200000, "mdate": 1681667513109, "content": {"title": "Reference-Centric Models for Grounded Collaborative Dialogue", "abstract": ""}}
