{"id": "TbT8OaJow8", "cdate": 1674767584760, "mdate": 1674767584760, "content": {"title": "Distilling Double Descent", "abstract": "Distillation is the technique of training a \"student\" model based on examples that are labeled by a separate \"teacher\" model, which itself is trained on a labeled dataset. The most common explanations for why distillation \"works\" are predicated on the assumption that student is provided with \\emph{soft} labels, \\eg probabilities or confidences, from the teacher model. In this work, we show, that, even when the teacher model is highly overparameterized, and provides \\emph{hard} labels, using a very large held-out unlabeled dataset to train the student model can result in a model that outperforms more \"traditional\" approaches.\nOur explanation for this phenomenon is based on recent work on \"double descent\". It has been observed that, once a model's complexity roughly exceeds the amount required to memorize the training data, increasing the complexity \\emph{further} can, counterintuitively, result in \\emph{better} generalization. Researchers have identified several settings in which it takes place, while others have made various attempts to explain it (thus far, with only partial success). In contrast, we avoid these questions, and instead seek to \\emph{exploit} this phenomenon by demonstrating that a highly-overparameterized teacher can avoid overfitting via double descent, while a student trained on a larger independent dataset labeled by this teacher will avoid overfitting due to the size of its training set."}}
{"id": "jKnTWwW8eVN", "cdate": 1664731448466, "mdate": null, "content": {"title": "Differentially Private Adaptive Optimization with Delayed Preconditioners", "abstract": "Privacy noise may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP^2), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP^2 across several real-world datasets, demonstrating that it can improve convergence speed by as much as 4\u00d7 relative to non-adaptive baselines and match the performance of state-of-the-art optimization methods that require auxiliary data."}}
{"id": "j1zQGmQQOX1", "cdate": 1663850375229, "mdate": null, "content": {"title": "Differentially Private Adaptive Optimization with Delayed Preconditioners", "abstract": "Privacy costs may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP^2), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP^2 across several real-world datasets, demonstrating that it can improve convergence speed by as much as 4\u00d7 relative to non-adaptive baselines and match the performance of state-of-the-art optimization methods that require auxiliary data."}}
{"id": "VO-HUrkHSY", "cdate": 1663850374759, "mdate": null, "content": {"title": "FedLite: Improving Communication Efficiency in Federated Split Learning", "abstract": "In classical federated learning, clients contribute to the overall training by communicating local updates for the underlying model on their private data to a coordinating server.  However, updating and communicating the entire model becomes prohibitively expensive when resource-constrained clients collectively aim to train a large machine learning model. Split learning provides a natural solution in such a setting, where only a (small) part of the model is stored and trained on clients while the remaining (large) part of the model only stays at the servers. Unfortunately, the model partitioning employed in split learning significantly increases the communication cost compared to the classical federated learning algorithms. This paper addresses this issue by compressing the additional communication cost associated with split learning via a novel clustering algorithm and a gradient correction technique. An extensive empirical evaluation on standard image and text benchmarks shows that the proposed method can achieve up to 490x communication cost reduction with minimal drop in accuracy, and enables a desirable performance vs. communication trade-off."}}
{"id": "TJ2nxciYCk-", "cdate": 1663850035858, "mdate": null, "content": {"title": "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers", "abstract": "This paper studies a curious phenomenon that machine learning model with Transformer architectures have sparse activation maps. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by \"sparse\" we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels. We discuss how sparsity immediately implies a way to significantly reduce the FLOP count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small k brings a collection of desired properties, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence."}}
{"id": "5gri-cs4RVq", "cdate": 1663849982415, "mdate": null, "content": {"title": "Do We Need Neural Collapse? Learning Diverse Features for Fine-grained and Long-tail Classification", "abstract": "Feature extractors learned from supervised training of deep neural networks have demonstrated superior performance over handcrafted ones. Recently, it is shown that such learned features have a neural collapse property, where within-class features collapse to the class mean and different class means are maximally separated. This paper examines the neural collapse property in the context of fine-grained classification tasks, where a feature extractor pretrained from a classification task with coarse labels is used for generating features for a downstream classification task with fine-grained labels. We argue that the within-class feature collapse is an undesirable property for fine-grained classification. Hence, we introduce a geometric arrangement of features called the maximal-separating-cone, where within-class features lie in a cone of nontrivial radius instead of collapsing to the class mean, and cones of different classes are maximally separated. We present a technique based on classifier weight and training loss design to produce such an arrangement. Experimentally we demonstrate an improved fine-grained classification performance with a feature extractor pretrained by our method. Moreover, our technique also provides benefits for the classification on data with long-tail distribution over classes. Our work may motivate future efforts on the design of better geometric arrangements of deep features."}}
{"id": "qQ2XWWMkG-", "cdate": 1640995200000, "mdate": 1681497890166, "content": {"title": "In defense of dual-encoders for neural ranking", "abstract": ""}}
{"id": "k-JXOpdObP1", "cdate": 1640995200000, "mdate": 1682333460193, "content": {"title": "FedLite: A Scalable Approach for Federated Learning on Resource-constrained Clients", "abstract": "In classical federated learning, the clients contribute to the overall training by communicating local updates for the underlying model on their private data to a coordinating server. However, updating and communicating the entire model becomes prohibitively expensive when resource-constrained clients collectively aim to train a large machine learning model. Split learning provides a natural solution in such a setting, where only a small part of the model is stored and trained on clients while the remaining large part of the model only stays at the servers. However, the model partitioning employed in split learning introduces a significant amount of communication cost. This paper addresses this issue by compressing the additional communication using a novel clustering scheme accompanied by a gradient correction method. Extensive empirical evaluations on image and text benchmarks show that the proposed method can achieve up to $490\\times$ communication cost reduction with minimal drop in accuracy, and enables a desirable performance vs. communication trade-off."}}
{"id": "jGfkZSx2A62", "cdate": 1640995200000, "mdate": 1649949946656, "content": {"title": "Robust Training of Neural Networks using Scale Invariant Architectures", "abstract": "In contrast to SGD, adaptive gradient methods like Adam allow robust training of modern deep networks, especially large language models. However, the use of adaptivity not only comes at the cost of extra memory but also raises the fundamental question: can non-adaptive methods like SGD enjoy similar benefits? In this paper, we provide an affirmative answer to this question by proposing to achieve both robust and memory-efficient training via the following general recipe: (1) modify the architecture and make it scale invariant, i.e. the scale of parameter doesn't affect the output of the network, (2) train with SGD and weight decay, and optionally (3) clip the global gradient norm proportional to weight norm multiplied by $\\sqrt{\\tfrac{2\\lambda}{\\eta}}$, where $\\eta$ is learning rate and $\\lambda$ is weight decay. We show that this general approach is robust to rescaling of parameter and loss by proving that its convergence only depends logarithmically on the scale of initialization and loss, whereas the standard SGD might not even converge for many initializations. Following our recipe, we design a scale invariant version of BERT, called SIBERT, which when trained simply by vanilla SGD achieves performance comparable to BERT trained by adaptive methods like Adam on downstream tasks."}}
{"id": "i0-pFnOglyY", "cdate": 1640995200000, "mdate": 1682333460096, "content": {"title": "On the Algorithmic Stability and Generalization of Adaptive Optimization Methods", "abstract": "Despite their popularity in deep learning and machine learning in general, the theoretical properties of adaptive optimizers such as Adagrad, RMSProp, Adam or AdamW are not yet fully understood. In this paper, we develop a novel framework to study the stability and generalization of these optimization methods. Based on this framework, we show provable guarantees about such properties that depend heavily on a single parameter $\\beta_2$. Our empirical experiments support our claims and provide practical insights into the stability and generalization properties of adaptive optimization methods."}}
