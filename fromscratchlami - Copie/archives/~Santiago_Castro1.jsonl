{"id": "vOEXS39nOF", "cdate": 1663850374506, "mdate": null, "content": {"title": "Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions", "abstract": "We present Phenaki, a model capable of realistic video synthesis given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new causal model for learning video representation which compresses the video to a small discrete tokens representation. This tokenizer is auto-regressive in time, which allows it to work with video representations of different length. \nTo generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts."}}
{"id": "ldu9PeS22Hm", "cdate": 1640995200000, "mdate": 1664300064359, "content": {"title": "FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks", "abstract": "Large-scale pretrained image-text models have shown incredible zero-shot performance in a handful of tasks, including video ones such as action recognition and text-to-video retrieval. However, these models haven't been adapted to video, mainly because they don't account for the time dimension but also because video frames are different from the typical images (e.g., containing motion blur, less sharpness). In this paper, we present a fine-tuning strategy to refine these large-scale pretrained image-text models for zero-shot video understanding tasks. We show that by carefully adapting these models we obtain considerable improvements on two zero-shot Action Recognition tasks and three zero-shot Text-to-video Retrieval tasks. The code is available at https://github.com/bryant1410/fitclip"}}
{"id": "MhEzUbRzUn2", "cdate": 1640995200000, "mdate": 1664300064182, "content": {"title": "FIBER: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework", "abstract": "Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Oana Ignat, Nan Liu, Jonathan Stroud, Rada Mihalcea. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "4JYhxlNBjdAg", "cdate": 1640995200000, "mdate": 1664300064285, "content": {"title": "When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs", "abstract": "We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previous methods, and leads to improvements over previous work for the task of temporal action localization."}}
{"id": "tNorcdc-ibZW", "cdate": 1609459200000, "mdate": 1664300064413, "content": {"title": "WhyAct: Identifying Action Reasons in Lifestyle Vlogs", "abstract": "We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video."}}
{"id": "bJnNrHz4BVX", "cdate": 1609459200000, "mdate": 1664300064294, "content": {"title": "Overview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish", "abstract": "We present the results of HAHA at IberLEF 2021: Humor Analysis ba-sed on Human Annotation. This year\u2019s edition of the competition includes the two classic tasks of humor detection and rating, plus two novel tasks of humor logic me-chanism and target classi\ufb01cation. We describe the corpus created for the challenge, the competition phases, the submitted systems and the main results obtained."}}
{"id": "V0hIHxwxxg", "cdate": 1609459200000, "mdate": 1664300064410, "content": {"title": "Fill-in-the-blank as a Challenging Video Understanding Evaluation Framework", "abstract": "We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER -- a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework. The fill-in-the-blanks setting tests a model's understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the video and the surrounding text. The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks, namely: (1) video question answering using multiple-choice questions, where models perform relatively well because they exploit linguistic biases in the task formulation, thus making our framework challenging for the current state-of-the-art systems to solve; and (2) video captioning, which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth. The FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/."}}
{"id": "Dc61Ck7oxTn", "cdate": 1609459200000, "mdate": 1664300064295, "content": {"title": "WhyAct: Identifying Action Reasons in Lifestyle Vlogs", "abstract": ""}}
{"id": "JcQVZgPR9_g", "cdate": 1577836800000, "mdate": 1664300064485, "content": {"title": "HAHA 2019 Dataset: A Corpus for Humor Analysis in Spanish", "abstract": ""}}
{"id": "E44yMQbQdNY", "cdate": 1577836800000, "mdate": 1664300064412, "content": {"title": "LifeQA: A Real-life Dataset for Video Question Answering", "abstract": ""}}
