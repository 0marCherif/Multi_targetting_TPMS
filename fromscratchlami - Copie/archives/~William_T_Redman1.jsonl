{"id": "Apyzq9JO2b", "cdate": 1672531200000, "mdate": 1681660626733, "content": {"title": "On Equivalent Optimization of Machine Learning Methods", "abstract": "At the core of many machine learning methods resides an iterative optimization algorithm for their training. Such optimization algorithms often come with a plethora of choices regarding their implementation. In the case of deep neural networks, choices of optimizer, learning rate, batch size, etc. must be made. Despite the fundamental way in which these choices impact the training of deep neural networks, there exists no general method for identifying when they lead to equivalent, or non-equivalent, optimization trajectories. By viewing iterative optimization as a discrete-time dynamical system, we are able to leverage Koopman operator theory, where it is known that conjugate dynamics can have identical spectral objects. We find highly overlapping Koopman spectra associated with the application of online mirror and gradient descent to specific problems, illustrating that such a data-driven approach can corroborate the recently discovered analytical equivalence between the two optimizers. We extend our analysis to feedforward, fully connected neural networks, providing the first general characterization of when choices of learning rate, batch size, layer width, data set, and activation function lead to equivalent, and non-equivalent, evolution of network parameters during training. Among our main results, we find that learning rate to batch size ratio, layer width, nature of data set (handwritten vs. synthetic), and activation function affect the nature of conjugacy. Our data-driven approach is general and can be utilized broadly to compare the optimization of machine learning methods."}}
{"id": "SGGeYmd4MQ9", "cdate": 1648670753013, "mdate": 1648670753013, "content": {"title": "On Koopman mode decomposition and tensor component analysis", "abstract": "Koopman mode decomposition and tensor component analysis [also known as CANDECOMP (canonical decomposition)/PARAFAC (parallel factorization)] are two popular approaches of decomposing high dimensional datasets into modes that capture the most relevant features and/or dynamics. Despite their similar goal, the two methods are largely used by different scientific communities and are formulated in distinct mathematical languages. We examine the two together and show that, under certain conditions on the data, the theoretical decomposition given by the tensor component analysis is the same as that given by Koopman mode decomposition. This provides a \u201cbridge\u201d with which the two communities should be able to more effectively communicate. Our work provides new possibilities for algorithmic approaches to Koopman mode decomposition and tensor component analysis and offers a principled way in which to compare the two methods. Additionally, it builds upon a growing body of work showing that dynamical systems theory and Koopman operator theory, in particular, can be useful for problems that have historically made use of optimization theory."}}
{"id": "rCGl03PVM75", "cdate": 1648670645932, "mdate": 1648670645932, "content": {"title": "Optimizing neural networks via Koopman operator theory", "abstract": "Koopman operator theory, a powerful framework for discovering the underlying dynamics of nonlinear dynamical systems, was recently shown to be intimately connected with neural network training. In this work, we take the first steps in making use of this connection. As Koopman operator theory is a linear theory, a successful implementation of it in evolving network weights and biases offers the promise of accelerated training, especially in the context of deep networks, where optimization is inherently a non-convex problem. We show that Koopman operator theoretic methods allow for accurate predictions of weights and biases of feedforward, fully connected deep networks over a non-trivial range of training time. During this window, we find that our approach is> 10x faster than various gradient descent based methods (eg Adam, Adadelta, Adagrad), in line with our complexity analysis. We end by highlighting open questions in this exciting intersection between dynamical systems and neural network theory. We highlight additional methods by which our results could be expanded to broader classes of networks and larger training intervals, which shall be the focus of future work."}}
{"id": "aUnCtXcMVQ", "cdate": 1640995200000, "mdate": 1681650606646, "content": {"title": "Universality of Winning Tickets: A Renormalization Group Perspective", "abstract": ""}}
{"id": "H-tNmssSug", "cdate": 1640995200000, "mdate": 1681660626771, "content": {"title": "Algorithmic (Semi-)Conjugacy via Koopman Operator Theory", "abstract": "Iterative algorithms are of utmost importance in decision and control. With an ever growing number of algorithms being developed, distributed, and proprietarized, there is a similarly growing need for methods that can provide classification and comparison. By viewing iterative algorithms as discrete-time dynamical systems, we leverage Koopman operator theory to identify (semi-)conjugacies between algorithms using their spectral properties. This provides a general framework with which to classify and compare algorithms."}}
{"id": "0KpM3PFr5s", "cdate": 1640995200000, "mdate": 1681660626773, "content": {"title": "An Operator Theoretic View On Pruning Deep Neural Networks", "abstract": "The discovery of sparse subnetworks that are able to perform as well as full models has found broad applied and theoretical interest. While many pruning methods have been developed to this end, the..."}}
{"id": "pWBNOgdeURp", "cdate": 1632875567339, "mdate": null, "content": {"title": "An Operator Theoretic View On Pruning Deep Neural Networks", "abstract": "The discovery of sparse subnetworks that are able to perform as well as full models has found broad applied and theoretical interest. While many pruning methods have been developed to this end, the na\u00efve approach of removing parameters based on their magnitude has been found to be as robust as more complex, state-of-the-art algorithms. The lack of theory behind magnitude pruning's success, especially pre-convergence, and its relation to other pruning methods, such as gradient based pruning, are outstanding open questions in the field that are in need of being addressed. We make use of recent advances in dynamical systems theory, namely Koopman operator theory, to define a new class of theoretically motivated pruning algorithms. We show that these algorithms can be equivalent to magnitude and gradient based pruning, unifying these seemingly disparate methods, and find that they can be used to shed light on magnitude pruning's performance during the early part of training."}}
{"id": "aWA3-vIQDv", "cdate": 1632875495046, "mdate": null, "content": {"title": "Universality of Deep Neural Network Lottery Tickets: A Renormalization Group Perspective", "abstract": "Foundational work on the Lottery Ticket Hypothesis has suggested an exciting corollary: winning tickets found in the context of one task can be transferred to similar tasks, possibly even across different architectures. While this has become of broad practical and theoretical interest, to date, there exists no detailed understanding of why winning ticket universality exists, or any way of knowing a priori whether a given ticket can be transferred to a given task. To address these outstanding open questions, we make use of renormalization group theory, one of the most successful tools in theoretical physics. We find that iterative magnitude pruning, the method used for discovering winning tickets, is a renormalization group scheme. This opens the door to a wealth of existing numerical and theoretical tools, some of which we leverage here to examine winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitude pruning has found in the field of sparse machine learning."}}
{"id": "YWsDsznUN0B", "cdate": 1609459200000, "mdate": 1681660626765, "content": {"title": "On Koopman Mode Decomposition and Tensor Component Analysis", "abstract": "Koopman mode decomposition and tensor component analysis (also known as CANDECOMP/PARAFAC or canonical polyadic decomposition) are two popular approaches of decomposing high dimensional data sets into low dimensional modes that capture the most relevant features and/or dynamics. Despite their similar goal, the two methods are largely used by different scientific communities and formulated in distinct mathematical languages. We examine the two together and show that, under a certain (reasonable) condition on the data, the theoretical decomposition given by tensor component analysis is the \\textit{same} as that given by Koopman mode decomposition. This provides a \"bridge\" with which the two communities should be able to more effectively communicate. When this condition is not met, Koopman mode decomposition still provides a tensor decomposition with an \\textit{a priori} computable error, providing an alternative to the non-convex optimization that tensor component analysis requires. Our work provides new possibilities for algorithmic approaches to Koopman mode decomposition and tensor component analysis, provides a new perspective on the success of tensor component analysis, and builds upon a growing body of work showing that dynamical systems, and Koopman operator theory in particular, can be useful for problems that have historically made use of optimization theory."}}
{"id": "3bIcms6DpXU", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimizing Neural Networks via Koopman Operator Theory", "abstract": "Koopman operator theory, a powerful framework for discovering the underlying dynamics of nonlinear dynamical systems, was recently shown to be intimately connected with neural network training. In this work, we take the first steps in making use of this connection. As Koopman operator theory is a linear theory, a successful implementation of it in evolving network weights and biases offers the promise of accelerated training, especially in the context of deep networks, where optimization is inherently a non-convex problem. We show that Koopman operator theoretic methods allow for accurate predictions of weights and biases of feedforward, fully connected deep networks over a non-trivial range of training time. During this window, we find that our approach is &gt;10x faster than various gradient descent based methods (e.g. Adam, Adadelta, Adagrad), in line with our complexity analysis. We end by highlighting open questions in this exciting intersection between dynamical systems and neural network theory. We highlight additional methods by which our results could be expanded to broader classes of networks and larger training intervals, which shall be the focus of future work."}}
