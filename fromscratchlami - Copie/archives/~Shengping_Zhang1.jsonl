{"id": "swc28UDR8Wk", "cdate": 1689650519175, "mdate": null, "content": {"title": "DiffuGesture: Generating Human Gesture From Two-person Dialogue With Diffusion Models", "abstract": "This paper describes the DiffuGesture entry to the GENEA Challenge 2023. In this paper, we utilize conditional diffusion models to formulate the gesture generation problem. The DiffuGesture system generates human-like gestures from the two-person dialogue scenario, which are responsive to the interlocutor motions and accompany with the input speech. DiffuGesture system is built upon the recent DiffGesture [39]. Specifically, we introduce a lightweight transformer encoder to fuse the temporal relationships between human gestures and multi-modal conditions. Moreover, we adopt implicit classifier-free guidance to trade off between diversity and gesture quality. According to the collective evaluation released by GENEA Challenge 2023, our system demonstrates strong competitiveness in the appropriateness evaluation."}}
{"id": "442usSTXgFV", "cdate": 1667345590869, "mdate": 1667345590869, "content": {"title": "GRNet: Gridding Residual Network for Dense Point Cloud Completion", "abstract": "Estimating the complete 3D point cloud from an incomplete one is a key problem in many vision and robotics applications. Mainstream methods (e.g., PCN and TopNet) use Multi-layer Perceptrons (MLPs) to directly process point clouds, which may cause the loss of details because the structural and context of point clouds are not fully considered. To solve this problem, we introduce 3D grids as intermediate representations to regularize unordered point clouds. We therefore propose a novel Gridding Residual Network (GRNet) for point cloud completion. In particular, we devise two novel differentiable layers, named Gridding and Gridding Reverse, to convert between point clouds and 3D grids without losing structural information. We also present the differentiable Cubic Feature Sampling layer to extract features of neighboring points, which preserves context information. In addition, we design a new loss function, namely Gridding Loss, to calculate the L1 distance between the 3D grids of the predicted and ground truth point clouds, which is helpful to recover details. Experimental results indicate that the proposed GRNet performs favorably against state-of-the-art methods on the ShapeNet, Completion3D, and KITTI benchmarks."}}
{"id": "hi6WGHlRUd", "cdate": 1664499042341, "mdate": 1664499042341, "content": {"title": "SiamBAN: Target-Aware Tracking With Siamese Box Adaptive Network", "abstract": "Variation of scales or aspect ratios has been one of the main challenges for tracking. To overcome this challenge, most\nexisting methods adopt either multi-scale search or anchor-based schemes, which use a predefined search space in a handcrafted way\nand therefore limit their performance in complicated scenes. To address this problem, recent anchor-free based trackers have been\nproposed without using prior scale or anchor information. However, an inconsistency problem between classification and regression\ndegrades the tracking performance. To address the above issues, we propose a simple yet effective tracker (named Siamese Box\nAdaptive Network, SiamBAN) to learn a target-aware scale handling schema in a data-driven manner. Our basic idea is to predict the\ntarget boxes in a per-pixel fashion through a fully convolutional network, which is anchor-free. Specifically, SiamBAN divides the\ntracking problem into classification and regression tasks, which directly predict objectiveness and regress bounding boxes, respectively.\nA no-prior box design is proposed to avoid tuning hyper-parameters related to candidate boxes, which makes SiamBAN more flexible.\nSiamBAN further uses a target-aware branch to address the inconsistency problem. Experiments on benchmarks including VOT2018,\nVOT2019, OTB100, UAV123, LaSOT and TrackingNet show that SiamBAN achieves promising performance and runs at 35 FPS"}}
{"id": "bcVThZZ-uKM", "cdate": 1640995200000, "mdate": 1668520724262, "content": {"title": "Continuous Prediction of Lower-Limb Kinematics From Multi-Modal Biomedical Signals", "abstract": "The fast-growing techniques of measuring and fusing multi-modal biomedical signals enable advanced motor intent decoding schemes of lower-limb exoskeletons, meeting the increasing demand for rehabilitative or assistive applications of take-home healthcare. Challenges of exoskeletons\u2019 motor intent decoding schemes remain in making a continuous prediction to compensate for the hysteretic response caused by mechanical transmission. In this paper, we solve this problem by proposing an ahead-of-time continuous prediction of lower-limb kinematics, with the prediction of knee angles during level walking as a case study. Firstly, an end-to-end kinematics prediction network(KinPreNet), <xref ref-type=\"fn\" rid=\"fn1\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><sup>1</sup></xref> consisting of a feature extractor and an angle predictor, is proposed and experimentally compared with features and methods traditionally used in ahead-of-time prediction of gait phases. Secondly, inspired by the electromechanical delay(EMD), we further explore our algorithm\u2019s capability of compensating response delay of mechanical transmission by validating the performance of the different sections of prediction time. And we experimentally reveal the time boundary of compensating the hysteretic response. Thirdly, a comparison of employing EMG signals or not is performed to reveal the EMG and kinematic signals\u2019 collaborated contributions to the continuous prediction. During the experiments, EMG signals of nine muscles and knee angles calculated from inertial measurement unit (IMU) signals are recorded from ten healthy subjects. Our algorithm can predict knee angles with the averaged RMSE of 3.98 deg which is better than the 15.95-deg averaged RMSE of utilizing the traditional methods of ahead-of-time prediction. The best prediction time is in the interval of 27ms and 108ms. To the best of our knowledge, this is the first study of continuously predicting lower-limb kinematics in an ahead-of-time manner based on the electromechanical delay (EMD)."}}
{"id": "B7WDEjXhRl7", "cdate": 1640995200000, "mdate": 1668520724266, "content": {"title": "Progressive Limb-Aware Virtual Try-On", "abstract": "Existing image-based virtual try-on methods directly transfer specific clothing to a human image without utilizing clothing attributes to refine the transferred clothing geometry and textures, which causes incomplete and blurred clothing appearances. In addition, these methods usually mask the limb textures of the input for the clothing-agnostic person representation, which results in inaccurate predictions for human limb regions (i.e., the exposed arm skin), especially when transforming between long-sleeved and short-sleeved garments. To address these problems, we present a progressive virtual try-on framework, named PL-VTON, which performs pixel-level clothing warping based on multiple attributes of clothing and embeds explicit limb-aware features to generate photo-realistic try-on results. Specifically, we design a Multi-attribute Clothing Warping (MCW) module that adopts a two-stage alignment strategy based on multiple attributes to progressively estimate pixel-level clothing displacements. A Human Parsing Estimator (HPE) is then introduced to semantically divide the person into various regions, which provides structural constraints on the human body and therefore alleviates texture bleeding between clothing and limb regions. Finally, we propose a Limb-aware Texture Fusion (LTF) module to estimate high-quality details in limb regions by fusing textures of the clothing and the human body with the guidance of explicit limb-aware features. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art virtual try-on methods both qualitatively and quantitatively."}}
{"id": "5pws3Qs2-i8", "cdate": 1640995200000, "mdate": 1668520724272, "content": {"title": "BacklitNet: A dataset and network for backlit image enhancement", "abstract": ""}}
{"id": "zpfHsOhH91N", "cdate": 1609459200000, "mdate": 1668520724263, "content": {"title": "Toward 3D object reconstruction from stereo images", "abstract": ""}}
{"id": "uoxykIthR5OP", "cdate": 1609459200000, "mdate": 1668520724560, "content": {"title": "Efficient Regional Memory Network for Video Object Segmentation", "abstract": "Recently, several Space-Time Memory based networks have shown that the object cues (e.g. video frames as well as the segmented object masks) from the past frames are useful for segmenting objects in the current frame. However, these methods exploit the information from the memory by global-to-global matching between the current and past frames, which lead to mismatching to similar objects and high computational complexity. To address these problems, we propose a novel local-to-local matching solution for semi-supervised VOS, namely Regional Memory Network (RMNet). In RMNet, the precise regional memory is constructed by memorizing local regions where the target objects appear in the past frames. For the current query frame, the query regions are tracked and predicted based on the optical flow estimated from the previous frame. The proposed local-to-local matching effectively eliminates the ambiguity of similar objects in both memory and query frames, which allows the information to be passed from the regional memory to the query region efficiently and effectively. Experimental results indicate that the proposed RMNet performs favorably against state-of-the-art methods on the DAVIS and YouTube-VOS datasets."}}
{"id": "ok8oeg2oOad", "cdate": 1609459200000, "mdate": 1668520724561, "content": {"title": "Low-light image enhancement via deep Retinex decomposition and bilateral learning", "abstract": ""}}
{"id": "mme4N2BH1CS", "cdate": 1609459200000, "mdate": 1668520724265, "content": {"title": "Editorial: Recent Advances on the Mobile Multimedia Services and Applications", "abstract": ""}}
