{"id": "bVvMOtLMiw", "cdate": 1632875739832, "mdate": null, "content": {"title": "DIVA: Dataset Derivative of a Learning Task", "abstract": "We present a method to compute the derivative of a learning task with respect to a dataset. A learning task is a function from a training set to the validation error, which can be represented by a trained deep neural network (DNN). The ``dataset derivative'' is a linear operator, computed around the trained model, that informs how perturbations of the weight of each training sample affect the validation error, usually computed on a separate validation dataset.  Our method, DIVA (Differentiable Validation) hinges on a closed-form differentiable expression of the leave-one-out cross-validation error around a pre-trained DNN. Such expression constitutes the dataset derivative. DIVA could be used for dataset auto-curation, for example removing samples with faulty annotations, augmenting a dataset with additional relevant samples, or rebalancing. More generally, DIVA can be used to optimize the dataset, along with the parameters of the model, as part of the training process without the need for a separate validation dataset, unlike bi-level optimization methods customary in AutoML. To illustrate the flexibility of DIVA, we report experiments on sample auto-curation tasks such as outlier rejection, dataset extension, and automatic aggregation of multi-modal data."}}
{"id": "PhtumOa45rE", "cdate": 1598871537316, "mdate": null, "content": {"title": "Optimization Theory for ReLU Neural Networks Trained with Normalization Layers", "abstract": "The success of deep neural networks is in part due to the use of normalization layers. Normaliza- tion layers like Batch Normalization, Layer Nor- malization and Weight Normalization are ubiqui- tous in practice, as they improve generalization performance and speed up training significantly. Nonetheless, the vast majority of current deep learning theory and non-convex optimization liter- ature focuses on the un-normalized setting, where the functions under consideration do not exhibit the properties of commonly normalized neural net- works. In this paper, we bridge this gap by giving the first global convergence result for two-layer neural networks with ReLU activations trained with a normalization layer, namely Weight Nor- malization. Our analysis shows how the introduc- tion of normalization layers changes the optimiza- tion landscape and can enable faster convergence as compared with un-normalized neural networks."}}
{"id": "HJggj3VKPH", "cdate": 1569438871622, "mdate": null, "content": {"title": "On the Dynamics and Convergence of Weight Normalization for Training Neural Networks", "abstract": "We present a proof of convergence for ReLU networks trained with weight normalization. In the analysis, we consider over-parameterized 2-layer ReLU networks initialized at random and trained with batch gradient descent and a fixed step size. The proof builds on recent theoretical works that bound the trajectory of parameters from their initialization and monitor the network predictions via the evolution of a ''neural tangent kernel'' (Jacot et al. 2018).  We discover that training with weight normalization decomposes such a kernel via the so called ''length-direction decoupling''.  This in turn leads to two convergence regimes and can rigorously explain the utility of WeightNorm. From the modified convergence we make a few curious observations including a natural form of ''lazy training'' where the direction of each weight vector remains stationary. "}}
{"id": "HJNRviZO-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Wasserstein of Wasserstein Loss for Learning Generative Models", "abstract": "The Wasserstein distance serves as a loss function for unsupervised learning which depends on the choice of a ground metric on sample space. We propose to use the Wasserstein distance itself as the..."}}
