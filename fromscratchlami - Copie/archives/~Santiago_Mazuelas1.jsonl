{"id": "AFedHRYIOSS", "cdate": 1676827113137, "mdate": null, "content": {"title": "Efficient Learning of Minimax Risk Classifiers in High Dimensions", "abstract": "High-dimensional} data is common in multiple areas, such as health care and genomics, where the number of features can be hundreds of thousands. In such scenarios, the large number of features often lead to inefficient learning. Constraint generation methods have recently enabled efficient learning of L1-regularized support vector machines (SVM). In this paper, we leverage such methods to obtain an efficient learning algorithm for the recently proposed minimax risk classifiers (MRC). The proposed iterative algorithm also provides a sequence of worst-case error probabilities and performs feature selection. Experiments on multiple high-dimensional datasets show that the proposed algorithm is efficient in high-dimensional scenarios. In addition, the worst-case error probability provides useful information about the classifier performance, and the features selected by the algorithm are competitive with the state-of-the-art.\n"}}
{"id": "WK22pk7bSFR", "cdate": 1663850112692, "mdate": null, "content": {"title": "Forward and Backward Lifelong Learning with Time-dependent Tasks", "abstract": "For a sequence of classification tasks that arrive over time, lifelong learning methods can boost the effective sample size of each task by leveraging information from preceding and succeeding tasks (forward and backward learning). However, backward learning is often prone to a so-called catastrophic forgetting in which a task\u2019s performance gets worse while trying to repeatedly incorporate information from succeeding tasks.  In addition, current lifelong learning techniques are designed for i.i.d. tasks and cannot capture the usual higher similarities between consecutive tasks. This paper presents lifelong learning methods based on minimax risk classifiers (LMRCs) that effectively exploit forward and backward learning and account for time-dependent tasks. In addition, we analytically characterize the increase in effective sample size provided by forward and backward learning in terms of  the tasks\u2019 expected quadratic change. The experimental evaluation shows that LMRCs can result in a significant performance improvement, especially for reduced sample sizes. "}}
{"id": "v8sywCj1_K", "cdate": 1640995200000, "mdate": 1681654482254, "content": {"title": "Generalized Maximum Entropy for Supervised Classification", "abstract": ""}}
{"id": "s1YyUXv2z0", "cdate": 1640995200000, "mdate": 1681654482251, "content": {"title": "Minimax risk classifiers with 0-1 loss", "abstract": ""}}
{"id": "-WrjGAomYqB", "cdate": 1640995200000, "mdate": 1681654482027, "content": {"title": "Minimax Classification under Concept Drift with Multidimensional Adaptation and Performance Guarantees", "abstract": ""}}
{"id": "wVIkxvE4FUJ", "cdate": 1577836800000, "mdate": 1681654482075, "content": {"title": "Probabilistic Load Forecasting Based on Adaptive Online Learning", "abstract": ""}}
{"id": "bmWlGnX5bX", "cdate": 1577836800000, "mdate": 1681654482225, "content": {"title": "Minimax Classification with 0-1 Loss and Performance Guarantees", "abstract": ""}}
{"id": "1ETdfduIvE", "cdate": 1577836800000, "mdate": 1681654482056, "content": {"title": "General Supervision via Probabilistic Transformations", "abstract": ""}}
{"id": "c-AImUamzM", "cdate": 1514764800000, "mdate": 1681654482256, "content": {"title": "Soft Range Information for Network Localization", "abstract": ""}}
{"id": "4ne4j2GKf3y", "cdate": 1514764800000, "mdate": 1681654482268, "content": {"title": "Spatiotemporal Information Coupling in Network Navigation", "abstract": ""}}
