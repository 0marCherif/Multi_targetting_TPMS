{"id": "7HBQxyrmOlj", "cdate": 1672531200000, "mdate": 1681803089906, "content": {"title": "Multi-agent Reinforcement Learning with Graph Q-Networks for Antenna Tuning", "abstract": "Future generations of mobile networks are expected to contain more and more antennas with growing complexity and more parameters. Optimizing these parameters is necessary for ensuring the good performance of the network. The scale of mobile networks makes it challenging to optimize antenna parameters using manual intervention or hand-engineered strategies. Reinforcement learning is a promising technique to address this challenge but existing methods often use local optimizations to scale to large network deployments. We propose a new multi-agent reinforcement learning algorithm to optimize mobile network configurations globally. By using a value decomposition approach, our algorithm can be trained from a global reward function instead of relying on an ad-hoc decomposition of the network performance across the different cells. The algorithm uses a graph neural network architecture which generalizes to different network topologies and learns coordination behaviors. We empirically demonstrate the performance of the algorithm on an antenna tilt tuning problem and a joint tilt and power control problem in a simulated environment."}}
{"id": "kCsjtc9EKZ", "cdate": 1640995200000, "mdate": 1681803090139, "content": {"title": "Model Based Residual Policy Learning with Applications to Antenna Control", "abstract": "Non-differentiable controllers and rule-based policies are widely used for controlling real systems such as robots and telecommunication networks. In this paper, we present a practical reinforcement learning method which improves upon such existing policies with a model-based approach for better sample efficiency. Our method significantly outperforms state-of-the-art model-based methods, in terms of sample efficiency, on several widely used robotic benchmark tasks. We also demonstrate the effectiveness of our approach on a control problem in the telecommunications domain, where model-based methods have not previously been explored. Experimental results indicate that a strong initial performance can be achieved and combined with improved sample efficiency. We further motivate the design of our algorithm with a theoretical lower bound on the performance."}}
{"id": "gxv6psJ-Fd", "cdate": 1640995200000, "mdate": 1681803090056, "content": {"title": "Edge-distributed Coordinated Hyper-Parameter Search for Energy Saving SON Use-Case", "abstract": "Energy Efficient operation of ultra-dense hetero-geneous network deployments is a big challenge for mobile networks. AI-assisted energy saving is one of the potential self-organizing network use cases for radio access network intelli-gence that can be used to predict the service load. This prediction can in turn be leveraged for proactively turning OFF/ON the capacity booster small cells within the coverage of always ON macro cells. These ML workloads can reside in macro cell base stations as opposed to conventional cloud-centric architecture to meet beyond 5G ambitious requirements of ultra-low latency, highest reliability, and scalability. However, the power-hungry hyperparameter search of ML workloads distributed at edges of the radio access network is a major challenge that can have substantial effect on the overall energy -efficiency of the network. In this paper, we illustrate how coordinated efficient training of distributed edge- ML models driven energy saving functions can enhance network energy efficiency. We validate the proposed method through a data-driven simulation methodology augmenting real traffic traces and comparing it with variants of legacy edge-ML hyper-parameter search techniques."}}
{"id": "yNl6dCb6Rud", "cdate": 1609459200000, "mdate": 1681803090114, "content": {"title": "Coordinated Reinforcement Learning for Optimizing Mobile Networks", "abstract": "Mobile networks are composed of many base stations and for each of them many parameters must be optimized to provide good services. Automatically and dynamically optimizing all these entities is challenging as they are sensitive to variations in the environment and can affect each other through interferences. Reinforcement learning (RL) algorithms are good candidates to automatically learn base station configuration strategies from incoming data but they are often hard to scale to many agents. In this work, we demonstrate how to use coordination graphs and reinforcement learning in a complex application involving hundreds of cooperating agents. We show how mobile networks can be modeled using coordination graphs and how network optimization problems can be solved efficiently using multi- agent reinforcement learning. The graph structure occurs naturally from expert knowledge about the network and allows to explicitly learn coordinating behaviors between the antennas through edge value functions represented by neural networks. We show empirically that coordinated reinforcement learning outperforms other methods. The use of local RL updates and parameter sharing can handle a large number of agents without sacrificing coordination which makes it well suited to optimize the ever denser networks brought by 5G and beyond."}}
{"id": "pZZyZkXCwH-", "cdate": 1609459200000, "mdate": 1681803090100, "content": {"title": "Coordinated Hyper-Parameter Search for Edge Machine Learning in Beyond-5G Networks", "abstract": "Edge intelligence in radio access network (RAN) is an emerging concept wherein machine learning (ML) driven self-organizing network (SON) functions reside in edge nodes or base stations (BSs). This allows to fulfill ultra-low latency, scalability, ultra-high reliability and signaling needs and requirements for beyond-5G applications. However, the resource-intensive hyper-parameter search of edge-distributed ML workloads is a major challenge that undermines the gains of ML driven automated networks. In current edge-ML, resource-constraints force the ML driven SON functions to incline towards inference-only workloads at the edge, whereas their training is done in the central infrastructure in non-collaborative fashion. This requires a redundant transfer of big data for training from the edge to the central location and a hyper-parameter search with high space time complexity, eventually resulting in high signaling costs and delays. This paper aims to address these challenges by presenting the Coordinated HypEr-paramETer seArcH (CHEETAH) frame-work for edge intelligence. It is a resource-efficient, low time-complexity distributed framework that eliminates the transfer of large training data from the edge to the central location and reduces the time required to find optimal (or near-optimal) hyper-parameters. Using a traffic forecasting use-case based on real network data, we demonstrate that as compared to state-of-the-art, the CHEETAH framework offers an attractive pragmatic solution in terms of predicting the best hyper-parameter configuration while keeping the associated costs low."}}
{"id": "c7HqlzOcoJ", "cdate": 1609459200000, "mdate": 1681746009305, "content": {"title": "A Graph Attention Learning Approach to Antenna Tilt Optimization", "abstract": "6G will move mobile networks towards increasing levels of complexity. To deal with this complexity, optimization of network parameters is key to ensure high performance and timely adaptivity to dynamic network environments. The optimization of the antenna tilt provides a practical and cost-efficient method to improve coverage and capacity in the network. Previous methods based on Reinforcement Learning (RL) have shown great promise for tilt optimization by learning adaptive policies outperforming traditional tilt optimization methods. However, most existing RL methods are based on single-cell features representation, which fails to fully characterize the agent state, resulting in suboptimal performance. Also, most of such methods lack scalability, due to state-action explosion, and generalization ability. In this paper, we propose a Graph Attention Q-learning (GAQ) algorithm for tilt optimization. GAQ relies on a graph attention mechanism to select relevant neighbors information, improve the agent state representation, and update the tilt control policy based on a history of observations using a Deep Q-Network (DQN). We show that GAQ efficiently captures important network information and outperforms standard DQN with local information by a large margin. In addition, we demonstrate its ability to generalize to network deployments of different sizes and densities."}}
{"id": "kDCKoUKPsE", "cdate": 1577836800000, "mdate": 1681803090109, "content": {"title": "Reinforcement Learning with Iterative Reasoning for Merging in Dense Traffic", "abstract": "Maneuvering in dense traffic is a challenging task for autonomous vehicles because it requires reasoning about the stochastic behaviors of many other participants. In addition, the agent must achieve the maneuver within a limited time and distance. In this work, we propose a combination of reinforcement learning and game theory to learn merging behaviors. We design a training curriculum for a reinforcement learning agent using the concept of level-k behavior. This approach exposes the agent to a broad variety of behaviors during training, which promotes learning policies that are robust to model discrepancies. We show that our approach learns more efficient policies than traditional training methods."}}
{"id": "U05YM7mPkz8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Point-Based Methods for Model Checking in Partially Observable Markov Decision Processes.", "abstract": "Autonomous systems are often required to operate in partially observable environments. They must reliably execute a specified objective even with incomplete information about the state of the environment. We propose a methodology to synthesize policies that satisfy a linear temporal logic formula in a partially observable Markov decision process (POMDP). By formulating a planning problem, we show how to use point-based value iteration methods to efficiently approximate the maximum probability of satisfying a desired logical formula and compute the associated belief state policy. We demonstrate that our method scales to large POMDP domains and provides strong bounds on the performance of the resulting policy."}}
{"id": "8zsudlEPKH", "cdate": 1577836800000, "mdate": 1681803090141, "content": {"title": "Reinforcement Learning with Iterative Reasoning for Merging in Dense Traffic", "abstract": "Maneuvering in dense traffic is a challenging task for autonomous vehicles because it requires reasoning about the stochastic behaviors of many other participants. In addition, the agent must achieve the maneuver within a limited time and distance. In this work, we propose a combination of reinforcement learning and game theory to learn merging behaviors. We design a training curriculum for a reinforcement learning agent using the concept of level-$k$ behavior. This approach exposes the agent to a broad variety of behaviors during training, which promotes learning policies that are robust to model discrepancies. We show that our approach learns more efficient policies than traditional training methods."}}
{"id": "-H8rNkaLAXj", "cdate": 1577836800000, "mdate": 1655048329465, "content": {"title": "Point-Based Methods for Model Checking in Partially Observable Markov Decision Processes", "abstract": "Autonomous systems are often required to operate in partially observable environments. They must reliably execute a specified objective even with incomplete information about the state of the environment. We propose a methodology to synthesize policies that satisfy a linear temporal logic formula in a partially observable Markov decision process (POMDP). By formulating a planning problem, we show how to use point-based value iteration methods to efficiently approximate the maximum probability of satisfying a desired logical formula and compute the associated belief state policy. We demonstrate that our method scales to large POMDP domains and provides strong bounds on the performance of the resulting policy."}}
