{"id": "RUQ1zwZR8_", "cdate": 1621630343637, "mdate": null, "content": {"title": "Differentially Private Learning with Adaptive Clipping", "abstract": "Existing approaches for training neural networks with user-level differential privacy (e.g., DP Federated Averaging) in federated learning (FL) settings involve bounding the contribution of each user's model update by {\\em clipping} it to some constant value. However there is no good {\\em a priori} setting of the clipping norm across tasks and learning settings: the update norm distribution depends on the model architecture and loss, the amount of data on each device, the client learning rate, and possibly various other parameters. We propose a method wherein instead of a fixed clipping norm, one clips to a value at a specified quantile of the update norm distribution, where the value at the quantile is itself estimated online, with differential privacy. The method tracks the quantile closely, uses a negligible amount of privacy budget, is compatible with other federated learning technologies such as compression and secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg. Experiments demonstrate that adaptive clipping to the median update norm works well across a range of federated learning tasks, eliminating the need to tune any clipping hyperparameter."}}
{"id": "Hy-nxPWu-S", "cdate": 1420070400000, "mdate": null, "content": {"title": "Interactive Control of Diverse Complex Characters with Neural Networks", "abstract": "We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks -- swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions."}}
{"id": "HyE1z3-ubB", "cdate": 1356998400000, "mdate": null, "content": {"title": "Deep Canonical Correlation Analysis", "abstract": "We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correla..."}}
{"id": "ByZCToWObB", "cdate": 1167609600000, "mdate": null, "content": {"title": "Scalable training of L", "abstract": "The L-BFGS limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to its non-differentiability whenever some parameter is zero. Efficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), based on L-BFGS, that can efficiently optimize the L1-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than L-BFGS on the analogous L2-regularized problem. We also present a proof that OWL-QN is guaranteed to converge to a globally optimal parameter vector."}}
{"id": "BkW9z2l_ZB", "cdate": 1167609600000, "mdate": null, "content": {"title": "A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing", "abstract": "This paper presents a comparative study of five parameter estimation algorithms on four NLP tasks. Three of the five algorithms are well-known in the computational linguistics community: Maximum Entropy (ME) estimation with L2 regularization, the Averaged Perceptron (AP), and Boosting. We also investigate ME estimation with L1 regularization using a novel optimization algorithm, and BLasso, which is a version of Boosting with Lasso (L1) regularization. We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model (LM) adaptation task. Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a Conditional Markov Model (CMM) for part of speech tagging and a Conditional Random Field (CRF) for Chinese word segmentation. Our experiments show that across tasks, three of the estimators \u2014 ME estimation with L1 or L2 regularization, and AP \u2014 are in a near statistical tie for first place."}}
{"id": "BJbjBMGubB", "cdate": 1136073600000, "mdate": null, "content": {"title": "A Hybrid Markov/Semi-Markov Conditional Random Field for Sequence Segmentation", "abstract": "Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling. Both models have advantages in terms of the type of features they most naturally represent. We propose a hybrid model that is capable of representing both types of features, and describe efficient algorithms for its training and inference. We demonstrate that our hybrid model achieves error reductions of 18% and 25% over a standard order-1 CRF and a semi-Markov CRF (resp.) on the task of Chinese word segmentation. We also propose the use of a powerful feature for the semi-Markov CRF: the log conditional odds that a given token sequence constitutes a chunk according to a generative model, which reduces error by an additional 13%. Our best system achieves 96.8% F-measure, the highest reported score on this test set."}}
{"id": "H1NUDMM_br", "cdate": 1072915200000, "mdate": null, "content": {"title": "Verb Sense and Subcategorization: Using Joint Inference to Improve Performance on Complementary Task", "abstract": "Galen Andrew, Trond Grenager, Christopher Manning. Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. 2004."}}
{"id": "rk48MXGdWS", "cdate": 1041379200000, "mdate": null, "content": {"title": "A Portfolio Approach to Algorithm Selection", "abstract": ""}}
