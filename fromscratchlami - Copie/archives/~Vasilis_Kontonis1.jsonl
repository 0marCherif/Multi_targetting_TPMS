{"id": "MlNzmdm9jz", "cdate": 1672531200000, "mdate": 1681665102311, "content": {"title": "SLaM: Student-Label Mixing for Semi-Supervised Knowledge Distillation", "abstract": "Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates ``soft'' pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with margin under random classification noise, and provide the first convergence analysis for so-called ``forward loss-adjustment\" methods."}}
{"id": "HIhnwt-9jD4", "cdate": 1672531200000, "mdate": 1681749572717, "content": {"title": "Efficient Testable Learning of Halfspaces with Adversarial Label Noise", "abstract": "We give the first polynomial-time algorithm for the testable learning of halfspaces in the presence of adversarial label noise under the Gaussian distribution. In the recently introduced testable learning model, one is required to produce a tester-learner such that if the data passes the tester, then one can trust the output of the robust learner on the data. Our tester-learner runs in time $\\poly(d/\\eps)$ and outputs a halfspace with misclassification error $O(\\opt)+\\eps$, where $\\opt$ is the 0-1 error of the best fitting halfspace. At a technical level, our algorithm employs an iterative soft localization technique enhanced with appropriate testers to ensure that the data distribution is sufficiently similar to a Gaussian."}}
{"id": "dgWo-UyVEsa", "cdate": 1652737825200, "mdate": null, "content": {"title": "Linear Label Ranking with Bounded Noise", "abstract": "Label Ranking (LR) is the supervised task of learning a sorting function that maps feature vectors $x \\in \\mathbb{R}^d$ to rankings $\\sigma(x) \\in \\mathbb S_k$ over a finite set of $k$ labels. We focus on the fundamental case of learning linear sorting functions (LSFs) under Gaussian marginals: $x$ is sampled from the $d$-dimensional standard normal and  the ground truth ranking $\\sigma^\\star(x)$ is the ordering induced by  sorting the coordinates of the vector $W^\\star x$, where  $W^\\star \\in \\mathbb{R}^{k \\times d}$ is unknown. We consider learning LSFs in the presence of bounded noise: assuming that a noiseless example is of the form $(x, \\sigma^\\star(x))$, we observe $(x, \\pi)$, where for any pair of elements $i \\neq j$, the probability that the order of $i, j$ is different in $\\pi$ than in  $\\sigma^\\star(x)$ is at most $\\eta < 1/2$. We design efficient non-proper and proper learning algorithms that  learn hypotheses within normalized Kendall's Tau distance $\\epsilon$ from the ground truth  with $N= \\widetilde{O}(d\\log(k)/\\epsilon)$ labeled examples and runtime $\\mathrm{poly}(N, k)$. For the more challenging top-$r$ disagreement loss, we give an efficient proper learning algorithm that achieves $\\epsilon$ top-$r$ disagreement with the ground truth with $N = \\widetilde{O}(d k r /\\epsilon)$ samples and $\\mathrm{poly}(N)$ runtime."}}
{"id": "M34VHvEU4NZ", "cdate": 1652737639379, "mdate": null, "content": {"title": "Weighted Distillation with Unlabeled Examples", "abstract": "Distillation with unlabeled examples is a popular and powerful method for training deep neural networks in settings where the amount of labeled data is limited: A large \u201cteacher\u201d neural network is trained on the labeled data available, and then it is used to generate labels on an unlabeled dataset (typically much larger in size). These labels are then utilized to train the smaller \u201cstudent\u201d model which will actually be deployed. Naturally, the success of the approach depends on the quality of the teacher\u2019s labels, since the student could be confused if trained on inaccurate data. This paper proposes a principled approach for addressing this issue based on a \u201cdebiasing\" reweighting of the student\u2019s loss function tailored to the distillation training paradigm. Our method is hyper-parameter free, data-agnostic, and simple to implement. We demonstrate significant improvements on popular academic datasets and we accompany our results with a theoretical analysis which rigorously justifies the performance of our method in certain settings.\n"}}
{"id": "zAuujoiCog", "cdate": 1640995200000, "mdate": 1681665102317, "content": {"title": "Weighted Distillation with Unlabeled Examples", "abstract": "Distillation with unlabeled examples is a popular and powerful method for training deep neural networks in settings where the amount of labeled data is limited: A large ''teacher'' neural network is trained on the labeled data available, and then it is used to generate labels on an unlabeled dataset (typically much larger in size). These labels are then utilized to train the smaller ''student'' model which will actually be deployed. Naturally, the success of the approach depends on the quality of the teacher's labels, since the student could be confused if trained on inaccurate data. This paper proposes a principled approach for addressing this issue based on a ''debiasing'' reweighting of the student's loss function tailored to the distillation training paradigm. Our method is hyper-parameter free, data-agnostic, and simple to implement. We demonstrate significant improvements on popular academic datasets and we accompany our results with a theoretical analysis which rigorously justifies the performance of our method in certain settings."}}
{"id": "gzUeJ7gsRS", "cdate": 1640995200000, "mdate": 1681749572748, "content": {"title": "Learning a Single Neuron with Adversarial Label Noise via Gradient Descent", "abstract": "We study the fundamental problem of learning a single neuron, i.e., a function of the form $\\mathbf{x}\\mapsto\\sigma(\\mathbf{w}\\cdot\\mathbf{x})$ for monotone activations $\\sigma:\\mathbb{R}\\mapsto\\mathbb{R}$, with respect to the $L_2^2$-loss in the presence of adversarial label noise. Specifically, we are given labeled examples from a distribution $D$ on $(\\mathbf{x}, y)\\in\\mathbb{R}^d \\times \\mathbb{R}$ such that there exists $\\mathbf{w}^\\ast\\in\\mathbb{R}^d$ achieving $F(\\mathbf{w}^\\ast)=\\epsilon$, where $F(\\mathbf{w})=\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[(\\sigma(\\mathbf{w}\\cdot \\mathbf{x})-y)^2]$. The goal of the learner is to output a hypothesis vector $\\mathbf{w}$ such that $F(\\mathbb{w})=C\\, \\epsilon$ with high probability, where $C>1$ is a universal constant. As our main contribution, we give efficient constant-factor approximate learners for a broad class of distributions (including log-concave distributions) and activation functions. Concretely, for the class of isotropic log-concave distributions, we obtain the following important corollaries: For the logistic activation, we obtain the first polynomial-time constant factor approximation (even under the Gaussian distribution). Our algorithm has sample complexity $\\widetilde{O}(d/\\epsilon)$, which is tight within polylogarithmic factors. For the ReLU activation, we give an efficient algorithm with sample complexity $\\tilde{O}(d\\, \\polylog(1/\\epsilon))$. Prior to our work, the best known constant-factor approximate learner had sample complexity $\\tilde{\\Omega}(d/\\epsilon)$. In both of these settings, our algorithms are simple, performing gradient-descent on the (regularized) $L_2^2$-loss. The correctness of our algorithms relies on novel structural results that we establish, showing that (essentially all) stationary points of the underlying non-convex loss are approximately optimal."}}
{"id": "g7nu-js7KaJ", "cdate": 1640995200000, "mdate": 1681749572728, "content": {"title": "Learning general halfspaces with general Massart noise under the Gaussian distribution", "abstract": "We study the problem of PAC learning halfspaces on \u211dd with Massart noise under the Gaussian distribution. In the Massart model, an adversary is allowed to flip the label of each point x with unknown probability \u03b7(x) \u2264 \u03b7, for some parameter \u03b7 \u2208 [0,1/2]. The goal is to find a hypothesis with misclassification error of OPT + \u0454, where OPT is the error of the target halfspace. This problem had been previously studied under two assumptions: (i) the target halfspace is homogeneous (i.e., the separating hyperplane goes through the origin), and (ii) the parameter \u03b7 is strictly smaller than 1/2. Prior to this work, no nontrivial bounds were known when either of these assumptions is removed. We study the general problem and establish the following: [leftmargin = *] For \u03b7 <1/2, we give a learning algorithm for general halfspaces with sample and computational complexity dO\u03b7(log(1/\u03b3))poly(1/\u0454), where \u03b3 max{\u0454, min{Pr[f(x) = 1], Pr[f(x) = \u22121]} } is the \u201cbias\u201d of the target halfspace f. Prior efficient algorithms could only handle the special case of \u03b3 = 1/2. Interestingly, we establish a qualitatively matching lower bound of d\u03a9(log(1/\u03b3)) on the complexity of any Statistical Query (SQ) algorithm. For \u03b7 = 1/2, we give a learning algorithm for general halfspaces with sample and computational complexity O\u0454(1) \u00a0dO(log(1/\u0454)). This result is new even for the subclass of homogeneous halfspaces; prior algorithms for homogeneous Massart halfspaces provide vacuous guarantees for \u03b7=1/2. We complement our upper bound with a nearly-matching SQ lower bound of d\u03a9(log(1/\u0454) ), which holds even for the special case of homogeneous halfspaces. Taken together, our results qualitatively characterize the complexity of learning general halfspaces with general Massart noise under Gaussian marginals. Our techniques rely on determining the existence (or non-existence) of low-degree polynomials whose expectations distinguish Massart halfspaces from random noise."}}
{"id": "eonVPwUCDih", "cdate": 1640995200000, "mdate": 1683712378891, "content": {"title": "Weighted Distillation with Unlabeled Examples", "abstract": "Distillation with unlabeled examples is a popular and powerful method for training deep neural networks in settings where the amount of labeled data is limited: A large \u201cteacher\u201d neural network is trained on the labeled data available, and then it is used to generate labels on an unlabeled dataset (typically much larger in size). These labels are then utilized to train the smaller \u201cstudent\u201d model which will actually be deployed. Naturally, the success of the approach depends on the quality of the teacher\u2019s labels, since the student could be confused if trained on inaccurate data. This paper proposes a principled approach for addressing this issue based on a \u201cdebiasing\" reweighting of the student\u2019s loss function tailored to the distillation training paradigm. Our method is hyper-parameter free, data-agnostic, and simple to implement. We demonstrate significant improvements on popular academic datasets and we accompany our results with a theoretical analysis which rigorously justifies the performance of our method in certain settings."}}
{"id": "Yl1wHlO9Ed", "cdate": 1640995200000, "mdate": 1683712379093, "content": {"title": "Linear Label Ranking with Bounded Noise", "abstract": "Label Ranking (LR) is the supervised task of learning a sorting function that maps feature vectors $x \\in \\mathbb{R}^d$ to rankings $\\sigma(x) \\in \\mathbb S_k$ over a finite set of $k$ labels. We focus on the fundamental case of learning linear sorting functions (LSFs) under Gaussian marginals: $x$ is sampled from the $d$-dimensional standard normal and the ground truth ranking $\\sigma^\\star(x)$ is the ordering induced by sorting the coordinates of the vector $W^\\star x$, where $W^\\star \\in \\mathbb{R}^{k \\times d}$ is unknown. We consider learning LSFs in the presence of bounded noise: assuming that a noiseless example is of the form $(x, \\sigma^\\star(x))$, we observe $(x, \\pi)$, where for any pair of elements $i \\neq j$, the probability that the order of $i, j$ is different in $\\pi$ than in $\\sigma^\\star(x)$ is at most $\\eta &lt; 1/2$. We design efficient non-proper and proper learning algorithms that learn hypotheses within normalized Kendall's Tau distance $\\epsilon$ from the ground truth with $N= \\widetilde{O}(d\\log(k)/\\epsilon)$ labeled examples and runtime $\\mathrm{poly}(N, k)$. For the more challenging top-$r$ disagreement loss, we give an efficient proper learning algorithm that achieves $\\epsilon$ top-$r$ disagreement with the ground truth with $N = \\widetilde{O}(d k r /\\epsilon)$ samples and $\\mathrm{poly}(N)$ runtime."}}
{"id": "WU9CV4kkSC", "cdate": 1640995200000, "mdate": 1681749572725, "content": {"title": "Learning General Halfspaces with Adversarial Label Noise via Online Gradient Descent", "abstract": "We study the problem of learning general {\u2014} i.e., not necessarily homogeneous {\u2014} halfspaces with adversarial label noise under the Gaussian distribution. Prior work has provided a sophisticated p..."}}
