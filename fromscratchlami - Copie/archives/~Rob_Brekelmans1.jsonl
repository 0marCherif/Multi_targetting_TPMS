{"id": "l1TN1BQCEm4", "cdate": 1680040300756, "mdate": 1680040300756, "content": {"title": "Rho-Tau Bregman Information and the Geometry of Annealing Paths", "abstract": "Markov Chain Monte Carlo methods for sampling from complex distributions and estimating normalization constants often simulate samples from a sequence of intermediate distributions along an annealing path, which bridges between a tractable initial distribution and a target density of interest. Prior work has constructed annealing paths using quasi-arithmetic means, and interpreted the resulting intermediate densities as minimizing an expected divergence to the endpoints. We provide a comprehensive analysis of this 'centroid' property using Bregman divergences under a monotonic embedding of the density function, thereby associating common divergences such as Amari's and Renyi's alpha-divergences, (alpha,beta)-divergences, and the Jensen-Shannon divergence with intermediate densities along an annealing path. Our analysis highlights the interplay between parametric families, quasi-arithmetic means, and divergence functions using the rho-tau Bregman divergence framework of Zhang 2004;2013."}}
{"id": "igeZ3A1Bn_g", "cdate": 1680040220819, "mdate": 1680040220819, "content": {"title": "Action Matching: A Variational Method for Learning Stochastic Dynamics from Samples", "abstract": "Learning the continuous dynamics of a system from snapshots of its time evolution is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modelling. In these settings, we assume that only uncorrelated samples rather than full trajectory data are available. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equation or optimal transport solvers. Inspired by connections with optimal transport, we derive extensions of Action Matching to learn stochastic differential equations and dynamics involving creation or destruction of probability mass. Finally, we showcase applications of Action Matching by achieving competitive performance in a diverse set of experiments from biology, physics, and generative modelling."}}
{"id": "ta-K62Xoh_u", "cdate": 1670664546377, "mdate": 1670664546377, "content": {"title": "Information-theoretic Diffusion", "abstract": "Denoising diffusion models have spurred significant gains in density modeling and image generation, precipitating an industrial revolution in text-guided AI art generation.  We introduce a new mathematical foundation for diffusion models inspired by classic results in information theory that connect Information with Minimum Mean Square Error regression, the so-called I-MMSE relations. We generalize the I-MMSE relations to \\emph{exactly} relate the data distribution to an optimal denoising regression problem, leading to an elegant refinement of existing diffusion bounds.  This new insight leads to several improvements for probability distribution estimation, including theoretical justification for diffusion model ensembling. Remarkably, our framework shows how continuous and discrete probabilities can be learned with the same regression objective, avoiding domain-specific generative models used in variational methods."}}
{"id": "UvmDCdSPDOW", "cdate": 1663850452865, "mdate": null, "content": {"title": "Information-Theoretic Diffusion", "abstract": "Denoising diffusion models have spurred significant gains in density modeling and image generation, precipitating an industrial revolution in text-guided AI art generation.  We introduce a new mathematical foundation for diffusion models inspired by classic results in information theory that connect Information with Minimum Mean Square Error regression, the so-called I-MMSE relations. We generalize the I-MMSE relations to \\emph{exactly} relate the data distribution to an optimal denoising regression problem, leading to an elegant refinement of existing diffusion bounds.  This new insight leads to several improvements for probability distribution estimation, including a theoretical justification for diffusion model ensembling. Remarkably, our framework shows how continuous and discrete probabilities can be learned with the same regression objective, avoiding domain-specific generative models used in variational methods."}}
{"id": "T0B9AoM_bFg", "cdate": 1632875768056, "mdate": null, "content": {"title": "Improving Mutual Information Estimation with Annealed and Energy-Based Bounds", "abstract": "Mutual information (MI) is a fundamental quantity in information theory and machine learning. However, direct estimation of MI is intractable, even if the true joint probability density for the variables of interest is known, as it involves estimating a potentially high-dimensional log partition function. In this work, we present a unifying view of existing MI bounds from the perspective of importance sampling, and propose three novel bounds based on this approach. Since a tight MI bound without density information requires a sample size exponential in the true MI, we assume either a single marginal or the full joint density information is known. In settings where the full joint density is available, we propose Multi-Sample Annealed Importance Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large values of MI in our experiments. In settings where only a single marginal distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds. Our GIWAE bound unifies variational and contrastive bounds in a single framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our MINE-AIS method improves upon existing energy-based methods such as MINE-DV and MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC sampling to estimate gradients for training and Multi-Sample AIS for evaluating the bound. Our methods are particularly suitable for evaluating MI in deep generative models, since explicit forms of the marginal or joint densities are often available. We evaluate our bounds on estimating the MI of VAEs and GANs trained on the MNIST and CIFAR datasets, and showcase significant gains over existing bounds in these challenging settings with high ground truth MI."}}
{"id": "QkVi6KogNbu", "cdate": 1621630085776, "mdate": null, "content": {"title": "Stochastic Approximation of Gaussian Free Energy for Risk-Sensitive Reinforcement Learning", "abstract": "We introduce a stochastic approximation rule for estimating the free energy from i.i.d. samples generated by a Gaussian distribution with unknown mean and variance. The rule is a simple modification of the Rescorla-Wagner rule, where the (sigmoidal) stimulus is taken to be either the event of over- or underestimating a target value. Since the Gaussian free energy is known to be a certainty-equivalent sensitive to the mean and the variance, the learning rule has applications in risk-sensitive decision-making. In particular, we show how use the rule in combination with the temporal-difference error in order to obtain risk-sensitive, model-free reinforcement learning algorithms."}}
{"id": "RoTADibt26_", "cdate": 1603141809639, "mdate": null, "content": {"title": "Likelihood Ratio Exponential Families", "abstract": "The exponential family is well known in machine learning and statistical physics as the maximum entropy distribution subject to a set of observed constraints, while the geometric mixture path is common in MCMC methods such as annealed importance sampling (AIS).   Linking these two ideas, recent work has interpreted the geometric mixture path as an exponential family of distributions to analyse the thermodynamic variational objective (TVO).\n\nIn this work, we extend \\textit{likelihood ratio exponential families} to include solutions to RD optimization, the IB method, and recent `` RDC' approaches which combine RD and IB.  This  provides a common mathematical framework for understanding these methods via the conjugate duality of exponential families and hypothesis testing.  Further, we collect existing results to provide a variational representation of intermediate RD or TVO distributions as a minimizing an expectation of KL divergences.  This solution also corresponds to a size-power tradeoff using the likelihood ratio test and the Neyman Pearson lemma.   In thermodynamic integration bounds such as the TVO, we identify the intermediate distribution whose expected sufficient statistics match the log partition function."}}
{"id": "ZBJ20FRVPD", "cdate": 1603141809315, "mdate": null, "content": {"title": "Annealed Importance Sampling with q-Paths", "abstract": "Annealed importance sampling (AIS) is the gold standard for estimating partition functions or marginal likelihoods, corresponding to importance sampling over a path of distributions between a tractable base and an unnormalized target.  While AIS yields an unbiased estimator for any path, existing literature has been limited to the geometric mixture or moment-averaged paths associated with the KL divergence and exponential family.   We explore using $q$-paths for AIS, which are related to the homogeneous power means, deformed exponential family, and $\\alpha$-divergence, and include the geometric path as a special case."}}
{"id": "GAzPBMaBgH_", "cdate": 1598294027959, "mdate": null, "content": {"title": "Disentangled representations via synergy minimization", "abstract": "Scientists often seek simplified representations of complex systems to facilitate prediction and understanding. If the factors comprising a representation allow us to make accurate predictions about our system, but obscuring any subset of the factors destroys our ability to make predictions, we say that the representation exhibits informational synergy. We argue that synergy is an undesirable feature in learned representations and that explicitly minimizing synergy can help disentangle the true factors of variation in underlying data. We explore different ways of quantifying synergy, deriving new closed-form expressions in some cases, and then show how to modify learning to produce representations that are minimally synergistic. We introduce a benchmark task to disentangle separate characters from images of words. We demonstrate that Minimally Synergistic (MinSyn) representations correctly disentangle characters while methods relying on statistical independence fail."}}
{"id": "HyEPuw-dWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Invariant Representations without Adversarial Training", "abstract": "Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations."}}
