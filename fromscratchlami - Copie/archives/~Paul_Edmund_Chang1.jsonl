{"id": "uLrdQul8lo", "cdate": 1694432023486, "mdate": 1694432023486, "content": {"title": "Sparse Function-space Representation of Neural Networks", "abstract": "Deep neural networks (NNs) are known to lack uncertainty estimates and struggle to incorporate new data. We present a method that mitigates these issues by converting NNs from weight space to function space, via a dual parameterization. Importantly, the dual parameterization enables us to formulate a sparse representation that captures information from the entire data set. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We provide proof-of-concept demonstrations with the proposed approach for quantifying uncertainty in supervised learning on UCI benchmark tasks."}}
{"id": "_RvQKPW8ZCp", "cdate": 1667319434697, "mdate": null, "content": {"title": "Sequential Learning in GPs with Memory and Bayesian Leverage Score", "abstract": "Limited access to previous data is challenging when using Gaussian process (GP) models for sequential learning. This results in inaccuracies in posterior, hyperparameter learning, and inducing variables. The recently proposed \u2018dual\u2019 sparse GP model enables inference of variational parameters in such a setup. In this paper, using the dual GP, we tackle the problem arising due to a lack of access to previous data for estimating hyperparameters of a sparse Gaussian process. We propose utilizing the concept of \u2018memory\u2019. To pick representative memory, we develop the \u2018Bayesian leverage score\u2019 built on the ridge leverage score. We experiment and perform an ablation study with a sequential learning data set, split MNIST, to showcase the usefulness of the proposed method."}}
{"id": "b-88mXTMg4J", "cdate": 1621630239430, "mdate": null, "content": {"title": "Dual Parameterization of Sparse Variational Gaussian Processes", "abstract": "Sparse variational Gaussian process (SVGP) methods are a common choice for non-conjugate Gaussian process inference because of their computational benefits. In this paper, we improve their computational efficiency by using a dual parameterization where each data example is assigned dual parameters, similarly to site parameters used in expectation propagation. Our dual parameterization speeds-up inference using natural gradient descent, and provides a tighter evidence lower bound for hyperparameter learning. The approach has the same memory cost as the current SVGP methods, but it is faster and more accurate."}}
{"id": "HkxNKk2VKS", "cdate": 1571237756459, "mdate": null, "content": {"title": "Global Approximate Inference via Local Linearisation for Temporal Gaussian Processes", "abstract": "The extended Kalman filter (EKF) is a classical signal processing algorithm which performs efficient approximate Bayesian inference in non-conjugate models by linearising the local measurement function, avoiding the need to compute intractable integrals when calculating the posterior. In some cases the EKF outperforms methods which rely on cubature to solve such integrals, especially in time-critical real-world problems. The drawback of the EKF is its local nature, whereas state-of-the-art methods such as variational inference or expectation propagation (EP) are considered global approximations. We formulate power EP as a nonlinear Kalman filter, before showing that linearisation results in a globally iterated algorithm that exactly matches the EKF on the first pass through the data, and iteratively improves the linearisation on subsequent passes. An additional benefit is the ability to calculate the limit as the EP power tends to zero, which removes the instability of the EP-like algorithm. The resulting inference scheme solves non-conjugate temporal Gaussian process models in linear time, $\\mathcal{O}(n)$, and in closed form."}}
