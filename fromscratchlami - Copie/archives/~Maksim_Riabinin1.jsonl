{"id": "lXJiXdYlKv", "cdate": 1705961712911, "mdate": 1705961712911, "content": {"title": "Multi-Task Transformer Networks for Search Relevance Prediction and Ranking", "abstract": "Modern search systems rely on high-quality ranking models that order webpages according to the relevance of their content to the text of the query. It is often possible to leverage several datasets with varying data quality, size and target variables, enhancing the overall sys- tem with each model trained on its respective dataset. However, training a separate model for each task comes at the cost of high computational demands at inference time. We propose to view the ranking problem with several heterogeneous datasets in a multi-task setting and to train a single BERT model as a way to mitigate this issue. We show that with a combination of multi-task and distillation techniques, it is possible to replace multiple ranking models with a single model of the same size without any drops in quality and with single-task performance gains of 20\u201340%. In addition, we propose a new task reweighting approach, which is easy to implement and yields consistent gains when compared to base- lines. Finally, we demonstrate that the same method can be successfully applied to all 9 of GLUE tasks with similar conclusions."}}
{"id": "wcNk7H-uZYU", "cdate": 1681930066177, "mdate": 1681930066177, "content": {"title": "High-throughput Generative Inference of Large Language Models with a Single GPU", "abstract": "The high computational and memory requirements of large language model (LLM) inference traditionally make it feasible only with multiple high-end accelerators. In this paper, we study how to lower the requirements of LLM inference down to one commodity GPU and achieve practical performance. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. Through a linear programming optimizer, it searches for the best pattern to store and access the tensors, including weights, activations, and attention key/value cache. FlexGen further compresses both weights and KV cache to 4 bits with negligible accuracy loss. Compared with state-of-the-art offloading systems, FlexGen runs OPT-175B up to 100 faster on a single 16GB GPU and achieves a practical generation throughput of 1 token/s for the first time. FlexGen also comes with a pipeline parallelism runtime to allow super-linear scaling on decoding if more distributed GPUs are given."}}
{"id": "TRLsr8tuyvk", "cdate": 1672531200000, "mdate": 1681483695225, "content": {"title": "SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient", "abstract": ""}}
{"id": "KKdF2-Ef1ed", "cdate": 1672531200000, "mdate": 1681483695227, "content": {"title": "Is This Loss Informative? Speeding Up Textual Inversion with Deterministic Objective Evaluation", "abstract": ""}}
{"id": "0dMmuFaW_v", "cdate": 1672531200000, "mdate": 1681483695219, "content": {"title": "High-throughput Generative Inference of Large Language Models with a Single GPU", "abstract": ""}}
{"id": "Ls_NTjgWXZV", "cdate": 1665866746108, "mdate": null, "content": {"title": "Petals: Collaborative Inference and Fine-tuning of Large Models", "abstract": "Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy significantly outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with $\\approx$ 1 step per second. Unlike most inference APIs, Petals also natively exposes the hidden states of served models, allowing its users to train and share custom model extensions based on efficient fine-tuning methods."}}
{"id": "HLQyRgRnoXo", "cdate": 1663850307663, "mdate": null, "content": {"title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet", "abstract": "Large language models (LLMs) are useful in many NLP tasks and become more capable with size, scaling to over 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using a pre-trained 100B+ model requires high-end hardware, making it inaccessible to most researchers. Recent studies in memory-efficient training (e.g. offloading) could alleviate these costs, but they do not cover important use cases of LLMs, such as autoregressive inference. In this work, we investigate methods for cost-efficient inference of large language models, comparing local and distributed strategies. We observe that a large enough model (100B+) could run efficiently on geodistributed devices in a consumer-grade network, for example by connecting existing compute resources of multiple research groups or pooling under-utilized compute from multiple cloud regions. To run LLMs in this unconventional setting, we develop a fault-tolerant algorithm for inferencing language models. We propose Petals - a decentralized system for running LLMs - and show that it can run BLOOM-176B over the Internet over $10\\times$ faster than offloading for sequential generation. We evaluate the performance of our system in both simulated conditions and an actual distributed system spanning two continents. The design of Petals allows participants to inference, and fine-tune, or inference fine-tuned models simultaneously without affecting each other's results."}}
{"id": "-azium0cV9", "cdate": 1663850105576, "mdate": null, "content": {"title": "SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient", "abstract": "Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap ``preemptible'' instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM Parallelism (Stochastically Wired Adaptively Rebalanced Model Parallelism), a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM Parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model with 1B shared parameters ($\\approx$13B before sharing) on preemptible T4 GPUs with less than 200 Mb/s network."}}
{"id": "J0nhRuMkdGf", "cdate": 1652737388312, "mdate": null, "content": {"title": "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees", "abstract": "Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across various applications, we need to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods allow for the use of both unbiased (such as Rand$k$; MASHA1) and contractive (such as Top$k$; MASHA2) compressors. New algorithms support bidirectional compressions, and also can be modified for stochastic setting with batches and for federated learning with partial participation of clients. We empirically validated our conclusions using two experimental setups: a standard bilinear min-max problem, and large-scale distributed adversarial training of transformers."}}
{"id": "a6bUqKM9dnq", "cdate": 1640995200000, "mdate": 1681483695231, "content": {"title": "RuCoLA: Russian Corpus of Linguistic Acceptability", "abstract": ""}}
