{"id": "e62ZssObZp", "cdate": 1652737710569, "mdate": null, "content": {"title": "Accelerating SGD for Highly Ill-Conditioned Huge-Scale Online Matrix Completion", "abstract": "The matrix completion problem seeks to recover a $d\\times d$ ground truth matrix of low rank $r\\ll d$ from observations of its individual elements. Real-world matrix completion is often a huge-scale optimization problem, with $d$ so large that even the simplest full-dimension vector operations with $O(d)$ time complexity become prohibitively expensive. Stochastic gradient descent (SGD) is one of the few algorithms capable of solving matrix completion on a huge scale, and can also naturally handle streaming data over an evolving ground truth. Unfortunately, SGD experiences a dramatic slow-down when the underlying ground truth is ill-conditioned; it requires at least $O(\\kappa\\log(1/\\epsilon))$ iterations to get $\\epsilon$-close to ground truth matrix with condition number $\\kappa$. In this paper, we propose a preconditioned version of SGD that preserves all the favorable practical qualities of SGD for huge-scale online optimization while also making it agnostic to $\\kappa$. For a symmetric ground truth and the Root Mean Square Error (RMSE) loss, we prove that the preconditioned SGD converges to $\\epsilon$-accuracy in $O(\\log(1/\\epsilon))$ iterations, with a rapid linear convergence rate as if the ground truth were perfectly conditioned with $\\kappa=1$. In our numerical experiments, we observe a similar acceleration for\nill-conditioned matrix completion under the root mean square error (RMSE) loss, Euclidean distance matrix (EDM) completion under pairwise square loss, and collaborative filtering under the Bayesian Personalized Ranking (BPR) loss."}}
{"id": "5-Of1DTlq", "cdate": 1621630133061, "mdate": null, "content": {"title": "Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix Factorization", "abstract": "In practical instances of nonconvex matrix factorization, the rank of the true solution $r^{\\star}$ is often unknown, so the rank $r$\nof the model can be over-specified as $r>r^{\\star}$. This over-parameterized regime of matrix factorization significantly slows down the convergence of local search algorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when $r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing variant of nonconvex matrix factorization that restores the convergence rate of gradient descent back to linear, even in the over-parameterized case, while also making it agnostic to possible ill-conditioning in the ground truth. Classical gradient descent in a neighborhood of the solution slows down due to the need for the model matrix factor to become singular. Our key result is that this singularity can be corrected by $\\ell_{2}$ regularization with a specific range of values for the damping parameter. In fact, a good damping parameter can be inexpensively estimated from the current iterate. The resulting algorithm, which we call preconditioned gradient descent or PrecGD, is stable under noise, and converges linearly to an information theoretically optimal error bound. Our numerical experiments find that PrecGD works equally well in restoring the linear convergence of other variants of nonconvex matrix factorization in the over-parameterized regime."}}
