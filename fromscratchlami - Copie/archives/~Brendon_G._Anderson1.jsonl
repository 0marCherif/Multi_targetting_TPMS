{"id": "2n4Ss3tgTZ4", "cdate": 1682376034192, "mdate": 1682376034192, "content": {"title": "Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off", "abstract": "Machine learning models have recently found tremendous success in data-driven control systems. However, standard learning models often suffer from an accuracy-robustness trade-off, which is a limitation that must be overcome in the control of safety-critical systems that require both high performance and rigorous robustness guarantees. In this work, we build upon the recent ``locally biased smoothing'' method to develop classifiers that simultaneously inherit high accuracy from standard models and high robustness from robust models. Specifically, we extend locally biased smoothing to the multi-class setting, and then overcome its performance bottleneck by generalizing the formulation to ``mix'' the outputs of a standard neural network and a robust neural network. We prove that when the robustness of the robust base model is certifiable, within a closed-form $\\ell_p$ radius, no alteration or attack on an input can result in misclassification of the mixed classifier; the proposed model inherits the certified robustness. Moreover, we use numerical experiments on the CIFAR-10 benchmark dataset to verify that the mixed model noticeably improves the accuracy-robustness trade-off."}}
{"id": "E_75Xm1UDQP", "cdate": 1677628800000, "mdate": 1681765773389, "content": {"title": "Data-Driven Certification of Neural Networks With Random Input Noise", "abstract": "Methods to certify the robustness of neural networks in the presence of input uncertainty are vital in safety-critical settings. Most certification methods in literature are designed for adversarial or worst-case inputs, but researchers have recently shown a need for methods that consider random input noise. In this article, we examine the setting where inputs are subject to random noise coming from an arbitrary probability distribution. We propose a robustness certification method that lower-bounds the probability that network outputs are safe. This bound is cast as a chance-constrained optimization problem, which is then reformulated using input\u2013output samples to make the optimization constraints tractable. We develop sufficient conditions for the resulting optimization to be convex, as well as on the number of samples needed to make the robustness bound hold with overwhelming probability. Case studies on synthetic, MNIST, and CIFAR-10 networks experimentally demonstrate that this method is able to certify robustness against various input noise regimes over larger uncertainty regions than prior state-of-the-art techniques."}}
{"id": "jWGphtm_XYR", "cdate": 1673390980714, "mdate": 1673390980714, "content": {"title": "Data-Driven Certification of Neural Networks with Random Input Noise", "abstract": "Methods to certify the robustness of neural networks in the presence of input uncertainty are vital\nin safety-critical settings. Most certification methods in the literature are designed for adversarial or\nworst-case inputs, but researchers have recently shown a need for methods that consider random input\nnoise. In this paper, we examine the setting where inputs are subject to random noise coming from\nan arbitrary probability distribution. We propose a robustness certification method that lower-bounds\nthe probability that network outputs are safe. This bound is cast as a chance-constrained optimization\nproblem, which is then reformulated using input-output samples to make the optimization constraints\ntractable. We develop sufficient conditions for the resulting optimization to be convex, as well as on\nthe number of samples needed to make the robustness bound hold with overwhelming probability. We\nshow for a special case that the proposed optimization reduces to an intuitive closed-form solution. Case\nstudies on synthetic, MNIST, and CIFAR-10 networks experimentally demonstrate that this method is\nable to certify robustness against various input noise regimes over larger uncertainty regions than prior\nstate-of-the-art techniques."}}
{"id": "JKfcw9Aahk", "cdate": 1673390945604, "mdate": 1673390945604, "content": {"title": "Towards Optimal Randomized Smoothing: A Semi-Infinite Linear Programming Approach", "abstract": "Randomized smoothing is a leading approach\nto producing certifiably robust classifiers. The\ngoal of optimal randomized smoothing is to maximize the average certified radius over the space\nof smoothing distributions. We theoretically\nstudy this problem through the lens of infinitedimensional optimization over measure spaces,\nand prove that the nonconvex infinite program is\nlower-bounded by a conic linear program wherein\nthe classifier\u2019s confidence acts as a surrogate objective to optimize. A semi-infinite linear programming approximation to the problem is presented, whose sub-problems are proven to attain\nnontrivial strong duality. A proof-of-concept experiment demonstrates the effectiveness of the\nproposed approach."}}
{"id": "qyl7-EcaNik", "cdate": 1673390895104, "mdate": 1673390895104, "content": {"title": "Certified Robustness via Locally Biased Randomized Smoothing", "abstract": "The successful incorporation of machine learning models into safety-critical control systems requires rigorous robustness guarantees. Randomized smoothing remains one of the state-of-the-art\nmethods for robustification with theoretical guarantees. We show that using uniform and unbiased smoothing measures, as is standard in the literature, relies on the underlying assumption that\nsmooth decision boundaries yield good robustness, which manifests into a robustness-accuracy\ntradeoff. We generalize the smoothing framework to remove this assumption and learn a locally\noptimal robustification of the decision boundary based on training data, a method we term locally\nbiased randomized smoothing. We prove nontrivial closed-form certified robust radii for the resulting model, avoiding Monte Carlo certifications as used by other smoothing methods. Experiments\non synthetic, MNIST, and CIFAR-10 data show a notable increase in the certified radii and accuracy\nover conventional smoothing."}}
{"id": "_iEvhtPM_Cn", "cdate": 1673390858674, "mdate": 1673390858674, "content": {"title": "Tightened Convex Relaxations for Neural Network Robustness Certification", "abstract": "In this paper, we consider the problem of certifying the robustness of neural networks to perturbed and\nadversarial input data. Such certification is imperative for\nthe application of neural networks in safety-critical decisionmaking and control systems. Certification techniques using\nconvex optimization have been proposed, but they often suffer\nfrom relaxation errors that void the certificate. Our work\nexploits the structure of ReLU networks to improve relaxation\nerrors through a novel partition-based certification procedure.\nThe proposed method is proven to tighten existing linear\nprogramming relaxations, and asymptotically achieves zero\nrelaxation error as the partition is made finer. We develop a\nfinite partition that attains zero relaxation error and use the\nresult to derive a tractable partitioning scheme that minimizes\nthe worst-case relaxation error. Experiments using real data\nshow that the partitioning procedure is able to issue robustness\ncertificates in cases where prior methods fail. Consequently,\npartition-based certification procedures are found to provide\nan intuitive, effective, and theoretically justified method for\ntightening existing convex relaxation techniques."}}
{"id": "jm4PMPx8hs", "cdate": 1672531200000, "mdate": 1681765773402, "content": {"title": "Asymmetric Certified Robustness via Feature-Convex Neural Networks", "abstract": "Recent works have introduced input-convex neural networks (ICNNs) as learning models with advantageous training, inference, and generalization properties linked to their convex structure. In this paper, we propose a novel feature-convex neural network architecture as the composition of an ICNN with a Lipschitz feature map in order to achieve adversarial robustness. We consider the asymmetric binary classification setting with one \"sensitive\" class, and for this class we prove deterministic, closed-form, and easily-computable certified robust radii for arbitrary $\\ell_p$-norms. We theoretically justify the use of these models by characterizing their decision region geometry, extending the universal approximation theorem for ICNN regression to the classification setting, and proving a lower bound on the probability that such models perfectly fit even unstructured uniformly distributed data in sufficiently high dimensions. Experiments on Malimg malware classification and subsets of MNIST, Fashion-MNIST, and CIFAR-10 datasets show that feature-convex classifiers attain state-of-the-art certified $\\ell_1$-radii as well as substantial $\\ell_2$- and $\\ell_{\\infty}$-radii while being far more computationally efficient than any competitive baseline."}}
{"id": "_zZJXa39QB", "cdate": 1672531200000, "mdate": 1681650433701, "content": {"title": "Improving the Accuracy-Robustness Trade-off of Classifiers via Adaptive Smoothing", "abstract": ""}}
{"id": "IXsI73NDuqN", "cdate": 1663850149723, "mdate": null, "content": {"title": "Asymmetric Certified Robustness via Feature-Convex Neural Networks", "abstract": "Recent works have introduced input-convex neural networks (ICNNs) as learning models with advantageous training, inference, and generalization properties linked to their convex structure. In this paper, we propose a novel feature-convex neural network (FCNN) architecture as the composition of an ICNN with a Lipschitz feature map in order to achieve adversarial robustness. We consider the asymmetric binary classification setting with one \"sensitive\" class, and for this class we prove deterministic, closed-form, and easily-computable certified robust radii for arbitrary $\\ell_p$-norms. We theoretically justify the use of these models by characterizing their decision region geometry, extending the universal approximation theorem for ICNN regression to the classification setting, and proving a lower bound on the probability that such models perfectly fit even unstructured uniformly distributed data in sufficiently high dimensions. Experiments on Malimg malware classification as well as subsets of MNIST, CIFAR-10, and ImageNet-scale datasets show that FCNNs can attain orders of magnitude larger certified $\\ell_1$-radii than competing methods while maintaining substantial $\\ell_2$- and $\\ell_{\\infty}$-radii."}}
{"id": "Wfw1ayq91X", "cdate": 1640995200000, "mdate": 1681765773420, "content": {"title": "Certified Robustness via Locally Biased Randomized Smoothing", "abstract": "The successful incorporation of machine learning models into safety-critical control systems requires rigorous robustness guarantees. Randomized smoothing remains one of the state-of-the-art method..."}}
