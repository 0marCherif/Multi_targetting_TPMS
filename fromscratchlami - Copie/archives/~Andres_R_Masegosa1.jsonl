{"id": "_HRYvHFgHeV", "cdate": 1621629850402, "mdate": null, "content": {"title": "Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote", "abstract": "We present a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality (a.k.a. one-sided Chebyshev\u2019s), which is amenable to efficient minimization. The new form resolves the optimization challenge faced by prior oracle bounds based on the Chebyshev-Cantelli inequality, the C-bounds [Germain et al., 2015], and, at the same time, it improves on the oracle bound based on second order Markov\u2019s inequality introduced by Masegosa et al. [2020]. We also derive a new concentration of measure inequality, which we name PAC-Bayes-Bennett, since it combines PAC-Bayesian bounding with Bennett\u2019s inequality. We use it for empirical estimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the new bounds can improve on the work of Masegosa et al. [2020]. Both the parametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett inequality may be of independent interest for the study of concentration of measure in other domains."}}
{"id": "HbTzvugzOp", "cdate": 1621629850402, "mdate": null, "content": {"title": "Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote", "abstract": "We present a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality (a.k.a. one-sided Chebyshev\u2019s), which is amenable to efficient minimization. The new form resolves the optimization challenge faced by prior oracle bounds based on the Chebyshev-Cantelli inequality, the C-bounds [Germain et al., 2015], and, at the same time, it improves on the oracle bound based on second order Markov\u2019s inequality introduced by Masegosa et al. [2020]. We also derive a new concentration of measure inequality, which we name PAC-Bayes-Bennett, since it combines PAC-Bayesian bounding with Bennett\u2019s inequality. We use it for empirical estimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the new bounds can improve on the work of Masegosa et al. [2020]. Both the parametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett inequality may be of independent interest for the study of concentration of measure in other domains."}}
{"id": "rAGn27G5d6", "cdate": 1546300800000, "mdate": null, "content": {"title": "InferPy: Probabilistic Modeling with Deep Neural Networks Made Easy.", "abstract": "InferPy is a Python package for probabilistic modeling with deep neural networks. It defines a user-friendly API that trades-off model complexity with ease of use, unlike other libraries whose focus is on dealing with very general probabilistic models at the cost of having a more complex API. In particular, this package allows to define, learn and evaluate general hierarchical probabilistic models containing deep neural networks in a compact and simple way. InferPy is built on top of Tensorflow Probability and Keras."}}
{"id": "FjoHfbRNqGc", "cdate": 1546300800000, "mdate": null, "content": {"title": "InferPy: Probabilistic modeling with Tensorflow made easy.", "abstract": "Highlights \u2022 InferPy is a high-level Python API for probabilistic modeling. \u2022 It built on top of Edward and Tensorflow. \u2022 Edward\u2019s drawback is that the user has to manage complex multidimensional tensors. \u2022 InferPy focuses on hierarchical probabilistic models. Abstract InferPy is a high-level Python API for probabilistic modeling built on top of Edward and Tensorflow. InferPy, which is strongly inspired by Keras, focuses on being user-friendly by using an intuitive set of abstractions that make easy to deal with complex probabilistic models. It should be seen as an interface rather than a standalone machine-learning framework. In general, InferPy has the focus on enabling flexible data processing, easy-to-code probabilistic modeling, scalable inference and robust model validation. Previous article in issue Next article in issue"}}
{"id": "48_DVqepn7j", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning from i.i.d. data under model miss-specification.", "abstract": "Virtually any model we use in machine learning to make predictions does not perfectly represent reality. So, most of the learning happens under model misspecification. In this work, we present a novel analysis of the generalization performance of Bayesian model averaging under model misspecification and i.i.d. data using a new family of second-order PAC-Bayes bounds. This analysis shows, in simple and intuitive terms, that Bayesian model averaging provides suboptimal generalization performance when the model is misspecified. In consequence, we provide strong theoretical arguments showing that Bayesian methods are not optimal for learning predictive models, unless the model class is perfectly specified. Using novel second-order PAC-Bayes bounds, we derive a new family of Bayesian-like algorithms, which can be implemented as variational and ensemble methods. The output of these algorithms is a new posterior distribution, different from the Bayesian posterior, which induces a posterior predictive distribution with better generalization performance. Experiments with Bayesian neural networks illustrate these findings."}}
{"id": "248_ZLcktK", "cdate": 1546300800000, "mdate": null, "content": {"title": "Probabilistic Models with Deep Neural Networks.", "abstract": "Recent advances in statistical inference have significantly expanded the toolbox of probabilistic modeling. Historically, probabilistic modeling has been constrained to (i) very restricted model classes where exact or approximate probabilistic inference were feasible, and (ii) small or medium-sized data sets which fit within the main memory of the computer. However, developments in variational inference, a general form of approximate probabilistic inference originated in statistical physics, are allowing probabilistic modeling to overcome these restrictions: (i) Approximate probabilistic inference is now possible over a broad class of probabilistic models containing a large number of parameters, and (ii) scalable inference methods based on stochastic gradient descent and distributed computation engines allow to apply probabilistic modeling over massive data sets. One important practical consequence of these advances is the possibility to include deep neural networks within a probabilistic model to capture complex non-linear stochastic relationships between random variables. These advances in conjunction with the release of novel probabilistic modeling toolboxes have greatly expanded the scope of application of probabilistic models, and allow these models to take advantage of the recent strides made by the deep learning community. In this paper we review the main concepts, methods and tools needed to use deep neural networks within a probabilistic modeling framework."}}
{"id": "hq3spf-st4O", "cdate": 1514764800000, "mdate": null, "content": {"title": "Scalable importance sampling estimation of Gaussian mixture posteriors in Bayesian networks.", "abstract": "Highlights \u2022 A new scalable importance sampling algorithm for Bayesian networks is proposed. \u2022 It fits Gaussian mixture posteriors in conditional linear Gaussian Bayesian networks. \u2022 Based on a stochastic gradient ascent procedure to dynamically update the posteriors. \u2022 Following a Map/Reduce approach, with no need to store the full sample. \u2022 Available in the AMIDST open-source toolbox for probabilistic machine learning. Abstract In this paper we propose a scalable importance sampling algorithm for computing Gaussian mixture posteriors in conditional linear Gaussian Bayesian networks. Our contribution is based on using a stochastic gradient ascent procedure taking as input a stream of importance sampling weights, so that a mixture of Gaussians is dynamically updated with no need to store the full sample. The algorithm has been designed following a Map/Reduce approach and is therefore scalable with respect to computing resources. The implementation of the proposed algorithm is available as part of the AMIDST open-source toolbox for scalable probabilistic machine learning ( http://www.amidsttoolbox.com ). Previous article in issue Next article in issue"}}
{"id": "bRrXf0Nos0", "cdate": 1514764800000, "mdate": null, "content": {"title": "Virtual Subconcept Drift Detection in Discrete Data Using Probabilistic Graphical Models.", "abstract": "A common problem in mining data streams is that the distribution of the data might change over time. This situation, which is known as concept drift, should be detected for ensuring the accuracy of the models. In this paper we propose a method for subconcept drift detection in discrete streaming data using probabilistic graphical models. In particular, our approach is based on the use of conditional linear Gaussian Bayesian networks with latent variables. We demonstrate and analyse the proposed model using synthetic and real data."}}
{"id": "y5oNydQS1X5", "cdate": 1483228800000, "mdate": null, "content": {"title": "MAP inference in dynamic hybrid Bayesian networks.", "abstract": "In this paper, we study the maximum a posteriori (MAP) problem in dynamic hybrid Bayesian networks. We are interested in finding the sequence of values of a class variable that maximizes the posterior probability given evidence. We propose an approximate solution based on transforming the MAP problem into a simpler belief update problem. The proposed solution constructs a set of auxiliary networks by grouping consecutive instantiations of the variable of interest, thus capturing some of the potential temporal dependences between these variables while ignoring others. Belief update is carried out independently in the auxiliary models, after which the results are combined, producing a configuration of values for the class variable along the entire time sequence. Experiments have been carried out to analyze the behavior of the approach. The algorithm has been implemented using Java 8 streams, and its scalability has been evaluated."}}
{"id": "syusgfbwd8", "cdate": 1483228800000, "mdate": null, "content": {"title": "Scaling up Bayesian variational inference using distributed computing clusters.", "abstract": "Highlights \u2022 Bayesian inference over modern data sets is challenging because data does not fit into the main memory of a computer. \u2022 Stochastic methods have been so far the main approach to scale up Bayesian inference for this problem. \u2022 Apache Spark or Apache Flink provide a simple framework for efficiently processing distributed data. \u2022 This paper proposes alternatives approaches for scaling up Bayesian inference using modern distributed computing frameworks. \u2022 This approach is derived from new theoretical insights from variational message passing algorithms. Abstract In this paper we present an approach for scaling up Bayesian learning using variational methods by exploiting distributed computing clusters managed by modern big data processing tools like Apache Spark or Apache Flink, which efficiently support iterative map-reduce operations. Our approach is defined as a distributed projected natural gradient ascent algorithm, has excellent convergence properties, and covers a wide range of conjugate exponential family models. We evaluate the proposed algorithm on three real-world datasets from different domains (the Pubmed abstracts dataset, a GPS trajectory dataset, and a financial dataset) and using several models (LDA, factor analysis, mixture of Gaussians and linear regression models). Our approach compares favorably to stochastic variational inference and streaming variational Bayes, two of the main current proposals for scaling up variational methods. For the scalability analysis, we evaluate our approach over a network with more than one billion nodes and approx. 75 % latent variables using a computer cluster with 128 processing units (AWS). The proposed methods are released as part of an open-source toolbox for scalable probabilistic machine learning ( http://www.amidsttoolbox.com ) Masegosa et al. (2017) [29] . Previous article in issue Next article in issue"}}
