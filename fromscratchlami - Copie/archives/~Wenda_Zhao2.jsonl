{"id": "w6S7Ur87-y", "cdate": 1667356351481, "mdate": 1667356351481, "content": {"title": "Self-supervised feature adaption for infrared and visible image fusion", "abstract": "Benefitting from the strong feature extraction capability of deep learning, infrared and visible image fusion\nhas made a great progress. Since infrared and visible images are obtained by different sensors with different\nimaging mechanisms, there exists domain discrepancy, which becomes stumbling block for effective fusion. In\nthis paper, we propose a novel self-supervised feature adaption framework for infrared and visible image\nfusion. We implement a self-supervised strategy that facilitates the backbone network to extract features\nwith adaption while retaining the vital information by reconstructing the source images. Specifically, we\npreliminary adopt an encoder network to extract features with adaption. Then, two decoders with attention\nmechanism blocks are utilized to reconstruct the source images in a self-supervised way, forcing the adapted\nfeatures to contain vital information of the source images. Further, considering the case that source images\ncontain low-quality information, we design a novel infrared and visible image fusion and enhancement\nmodel, improving the fusion method\u2019s robustness. Experiments are constructed to evaluate the proposed\nmethod qualitatively and quantitatively, which show that the proposed method achieves the state-of-art\nperformance comparing with existing infrared and visible image fusion methods."}}
{"id": "ckv1HJrV6N", "cdate": 1667352897437, "mdate": 1667352897437, "content": {"title": "Diversity Consistency Learning for Remote Sensing Object Recognition with Limited Labels", "abstract": "Annotating remote sensing object recognition needs\nhigh professionalism, and thus limited labeled samples are\navailable. Suffering from this, general remote sensing object\nrecognition methods are facing low recognition accuracy. Addressing\nthis issue, this paper proposes a diversity consistency\nlearning for remote sensing object recognition with limited labels.\nSpecifically, diversity generation model is designed as a teacher\nmodel to generate diverse results, which is trained with labeled\nsamples. Then, round consistency distillation model is introduced\nto distill the knowledge of diverse pseudo labels to a student\nnetwork, which is trained with unlabeled samples. Especially,\ndiverse pseudo labels are generated by the well-trained diversity\ngeneration model, which can improve recognition accuracy since\ndiverse pseudo label errors can cancel each other out. Extensive\nexperiments on two widely-used datasets of FS23 and HRSC2016\ndemonstrate the superior performance of our method compared\nwith the state of the arts."}}
{"id": "bn4-2IWxKx", "cdate": 1667352844998, "mdate": 1667352844998, "content": {"title": "Enhancing Diversity of Defocus Blur Detectors via Cross-Ensemble Network", "abstract": "Defocus blur detection (DBD) is a fundamental yet chal-\nlenging topic, since the homogeneous region is obscure and\nthe transition from the focused area to the unfocused region\nis gradual. Recent DBD methods make progress through ex-\nploring deeper or wider networks with the expense of high\nmemory and computation. In this paper, we propose a nov-\nel learning strategy by breaking DBD problem into multi-\nple smaller defocus blur detectors and thus estimate errors\ncan cancel out each other. Our focus is the diversity en-\nhancement via cross-ensemble network. Specifically, we de-\nsign an end-to-end network composed of two logical parts:\nfeature extractor network (FENet) and defocus blur detec-\ntor cross-ensemble network (DBD-CENet). FENet is con-\nstructed to extract low-level features. Then the features are\nfed into DBD-CENet containing two parallel-branches for\nlearning two groups of defocus blur detectors. For each in-\ndividual, we design cross-negative and self-negative corre-\nlations and an error function to enhance ensemble diversity\nand balance individual accuracy. Finally, the multiple defo-\ncus blur detectors are combined with a uniformly weighted\naverage to obtain the final DBD map. Experimental results\nindicate the superiority of our method in terms of accura-\ncy and speed when compared with several state-of-the-art\nmethods"}}
{"id": "kuz6bd-13c", "cdate": 1640995200000, "mdate": 1668561571656, "content": {"title": "Teaching Teachers First and Then Student: Hierarchical Distillation to Improve Long-Tailed Object Recognition in Aerial Images", "abstract": "Remote sensing data distribution generally exposes the long-tailed characteristic. This will limit the object recognition performance of existing deep models when they are trained with such unbalanced data. In this article, we propose a novel hierarchical distillation framework (HDF) to address the long-tailed object recognition in aerial images. First, we notice that not only student model should learn feature representations from teachers but also teacher models should learn feature representations from each other. Therefore, we build hierarchical teacher-wise distillation (HTWD) to improve the feature representations of the teacher models trained with middle and tail data, which is achieved by distilling the feature representations of the teacher model trained with head data. Second, we notice that the feature representations of the middle and tail classes cannot be effectively distilled from the teacher to the student since too little middle and tail data can be used to learn. Thus, we propose self-calibrated sampling (SCS) learning that enforces the student to strengthen the learning of the middle and tail data, thereby improving the student\u2019 feature learning ability. Extensive experiments on two widely used DOTA and FGSC-23 datasets demonstrate the superior performance of the proposed method compared with state-of-the-art methods. Model and code are publicly available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/wdzhao123/T2FTS</uri> ."}}
{"id": "c1xAypW5Dr4", "cdate": 1640995200000, "mdate": 1668561571804, "content": {"title": "United Defocus Blur Detection and Deblurring via Adversarial Promoting Learning", "abstract": "Understanding blur from a single defocused image contains two tasks of defocus detection and deblurring. This paper makes the earliest effort to jointly learn both defocus detection and deblurring without using pixel-level defocus detection annotation and paired defocus deblurring ground truth. We build on the observation that these two tasks are supplementary to each other: Defocus detection can segment the focused area from the defocused image to guide the defocus deblurring; Conversely, to achieve better defocus deblurring, an accurate defocus detection as the guide is essential. Therefore, we implement an adversarial promoting learning framework to jointly handle defocus detection and defocus deblurring. Specifically, a defocus detection generator $$G_{ws}$$ is implemented to represent the defocused image as a layered composition of two elements: defocused image $$I_{df}$$ and a focused image $$I_f$$ . Then, $$I_{df}$$ and $$I_f$$ are fed into a self-referenced defocus deblurring generator $$G_{sr}$$ to generate a deblurred image. Two generators of $$G_{ws}$$ and $$G_{sr}$$ are optimized alternately in an adversarial manner against a discriminator D with unpaired realistic fully-clear images. Thus, $$G_{sr}$$ will produce a deblurred image to fool D, and $$G_{ws}$$ is forced to generate an accurate defocus detection map to effectively guide $$G_{sr}$$ . Comprehensive experiments on two defocus detection datasets and one defocus deblurring dataset demonstrate the effectiveness of our framework. Code and model are available at: https://github.com/wdzhao123/APL ."}}
{"id": "XJQMIJhtyoI", "cdate": 1640995200000, "mdate": 1668561571650, "content": {"title": "Feature Balance for Fine-Grained Object Classification in Aerial Images", "abstract": "Fine-grained object classification (FGOC) focuses on identifying subcategories of objects, which is crucial in military and civilian. Existing FGOC methods primarily focus on high-resolution aerial images, limiting their application on low-resolution (LR) FGOC that is a more realistic setting, especially on resource-constrained satellite devices. It is more challenging to deal with LR FGOC since objects\u2019 details are blurred or missing. Addressing this issue, we make the first attempt to explore LR FGOC and propose a novel pipeline based on two technical insights: 1) feature balance strategy discriminatively integrates super-resolution weak and strong detailed presentations into coarse features of LR aerial images, achieving a feature balance to avoid that the weak detailed presentations are inhibited by the strong ones and 2) iterative interaction mechanism alternately refines feature details of the discriminative ship regions and optimizes the performance of FGOC. Moreover, we build a low-resolution fine-grained object (LFS) dataset to promote further study and evaluation. Extensive experiments on the proposed LFS dataset and the other three object datasets of DOTA, FS23, and HRSC2016 demonstrate that our method outperforms state-of-the-art algorithms. Dataset and code are publicly available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/wdzhao123/FBNet</uri> ."}}
{"id": "SsWJ-8h1lj1", "cdate": 1640995200000, "mdate": 1668561571649, "content": {"title": "Image-Scale-Symmetric Cooperative Network for Defocus Blur Detection", "abstract": "Defocus blur detection (DBD) for natural images is a challenging vision task especially in the presence of homogeneous regions and gradual boundaries. In this paper, we propose a novel image-scale-symmetric cooperative network (IS2CNet) for DBD. On one hand, in the process of image scales from large to small, IS2CNet gradually spreads the recept of image content. Thus, the homogeneous region detection map can be optimized gradually. On the other hand, in the process of image scales from small to large, IS2CNet gradually feels the high-resolution image content, thereby gradually refining transition region detection. In addition, we propose a hierarchical feature integration and bi-directional delivering mechanism to transfer the hierarchical feature of previous image scale network to the input and tail of the current image scale network for guiding the current image scale network to better learn the residual. The proposed approach achieves state-of-the-art performance on existing datasets. <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Codes and results are available at: <uri>https://github.com/wdzhao123/IS2CNet</uri></i> ."}}
{"id": "NDIsSFSi-At", "cdate": 1640995200000, "mdate": 1668561571805, "content": {"title": "Generalizable Crowd Counting via Diverse Context Style Learning", "abstract": "Existing crowd counting approaches predominantly perform well on the training-testing protocol. However, due to large style discrepancies not only among images but also within a single image, they suffer from obvious performance degradation when applied to unseen domains. In this paper, we aim to design a generalizable crowd counting framework which is trained on a source domain but can generalize well on the other domains. To reach this, we propose a gated ensemble learning framework. Specifically, we first propose a diverse fine-grained style attention model to help learn discriminative content feature representations, allowing for exploiting diverse features to improve generalization. We then introduce a channel-level binary gating ensemble model, where diverse feature prior, input-dependent guidance and density grade classification constraint are implemented, to optimally select diverse content features to participate in the ensemble, taking advantage of their complementary while avoiding redundancy. Extensive experiments show that our gating ensemble approach achieves superior generalization performance among four public datasets. Codes are publicly available at  <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/wdzhao123/DCSL</uri> ."}}
{"id": "3fAM-AoZd7E", "cdate": 1640995200000, "mdate": 1668561571577, "content": {"title": "Diversity Consistency Learning for Remote-Sensing Object Recognition With Limited Labels", "abstract": "Annotating remote-sensing object recognition needs high professionalism, and thus limited labeled samples are available. Suffering from this, general remote-sensing object recognition methods are facing low recognition accuracy. Addressing this issue, this article proposes a diversity consistency learning (DCL) for remote-sensing object recognition with limited labels. Specifically, the diversity generation model (DGM) is designed as a teacher model to generate diverse results, which is trained with labeled samples. Then, a round consistency distillation model (RCDM) is introduced to distill the knowledge of diverse pseudo-labels to a student network, which is trained with unlabeled samples. Especially, diverse pseudo-labels are generated by the well-trained DGM, which can improve recognition accuracy since diverse pseudo-label errors can cancel each other out. Extensive experiments on two widely used datasets of FS23 and HRSC2016 demonstrate the superior performance of our method compared with the state of the arts (SOTAs)."}}
{"id": "xbGIITkIq4", "cdate": 1609459200000, "mdate": 1668561571691, "content": {"title": "Self-supervised feature adaption for infrared and visible image fusion", "abstract": ""}}
