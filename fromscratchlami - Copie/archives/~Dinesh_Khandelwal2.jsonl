{"id": "XLUsXbeo9WV", "cdate": 1680531671734, "mdate": 1680531671734, "content": {"title": "Translucent answer predictions in multi-hop reading comprehension", "abstract": "Research on the task of Reading Comprehension style Question Answering (RCQA) has gained momentum in recent years due to the emergence of human annotated datasets and associated leaderboards, for example CoQA, HotpotQA, SQuAD, TriviaQA, etc. While state-of-the-art has advanced considerably, there is still ample opportunity to advance it further on some important variants of the RCQA task. In this paper, we propose a novel deep neural architecture, called TAP (Translucent Answer Prediction), to identify answers and evidence (in the form of supporting facts) in an RCQA task requiring multi-hop reasoning. TAP comprises two loosely coupled networks\u2013Local and Global Interaction eXtractor (LoGIX) and Answer Predictor (AP). LoGIX predicts supporting facts, whereas AP consumes these predicted supporting facts to predict the answer span. The novel design of LoGIX is inspired by two key design desiderata\u2013local context and global interaction\u2013that we identified by analyzing examples of multi-hop RCQA task. The loose coupling between LoGIX and the AP reveals the set of sentences used by the AP in predicting an answer. Therefore, answer predictions of TAP can be interpreted in a translucent manner. TAP offers state-of-the-art performance on the HotpotQA (Yang et al. 2018) dataset\u2013an apt dataset for multi-hop RCQA task\u2013as it occupies Rank-1 on its leaderboard (https://hotpotqa. github. io/) at the time of submission."}}
{"id": "UT75yKgc-j", "cdate": 1664884605660, "mdate": null, "content": {"title": "Image Manipulation via Neuro-Symbolic Networks", "abstract": "Image manipulation via natural language text -- an extremely useful task for multiple AI applications but requires complex reasoning over multi-modal spaces. Neuro-symbolic approaches has been quite effective in solving such tasks as they offer better modularity, interpretability, and generalizability. A noteworthy such approach is NSCL [10] developed for the task of Visual Question Answering (VQA). We extend NSCL for the image manipulation task and propose a solution referred to as NEUROSIM. Unlike previous works, which either require supervised data training or can only deal with simple reasoning instructions over single object scenes; NEUROSIM can perform complex multi-hop reasoning over multi-object scenes and requires only weak supervision in the form of annotated data for the VQA task. On the language side, NEUROSIM contains neural modules that parse an instruction into a symbolic program over a Domain Specific Language (DSL) comprising manipulation operations that guide the manipulation. On the perceptual side, NEUROSIM contains neural modules which first generate a scene graph of the input image and then change the scene graph representation following the parsed instruction. To train these modules, we design novel loss functions capable of testing the correctness of manipulated object and scene graph representations via query networks. An image decoder is trained to render the final image from the manipulated scene graph representation. Extensive experiments demonstrate that NEUROSIM is highly competitive with state-of-the-art supervised baselines."}}
{"id": "RqJZTlQMph", "cdate": 1663850244797, "mdate": null, "content": {"title": "Weakly Supervised Neuro-Symbolic Image Manipulation via Multi-Hop Complex Instructions", "abstract": "We are interested in image manipulation via natural language text \u2013 a task that is extremely useful for multiple AI applications but requires complex reasoning over multi-modal spaces. Recent work on neuro-symbolic approaches (Mao et al., 2019) (NSCL) has been quite effective for solving VQA as they offer better modularity, interpretability, and generalizability. We extend NSCL for the image manipulation task and propose a solution referred to as NeuroSIM. Previous work either requires supervised training data in the form of manipulated images or can only deal with very simple reasoning instructions over single object scenes. In contrast, NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides the manipulation. We design neural modules for manipulation, as well as novel loss functions that are capable of testing the correctness of manipulated object and scene graph representations via query networks trained merely on VQA data. An image decoder is trained to render the final image from the manipulated scene graph. Extensive experiments demonstrate that NeuroSIM, without using target images as supervision, is highly competitive with SOTA baselines that make use of supervised data for manipulation."}}
{"id": "y-7fo29myNa", "cdate": 1640995200000, "mdate": 1652608795860, "content": {"title": "A Benchmark for Generalizable and Interpretable Temporal Question Answering over Knowledge Bases", "abstract": "Knowledge Base Question Answering (KBQA) tasks that involve complex reasoning are emerging as an important research direction. However, most existing KBQA datasets focus primarily on generic multi-hop reasoning over explicit facts, largely ignoring other reasoning types such as temporal, spatial, and taxonomic reasoning. In this paper, we present a benchmark dataset for temporal reasoning, TempQA-WD, to encourage research in extending the present approaches to target a more challenging set of complex reasoning tasks. Specifically, our benchmark is a temporal question answering dataset with the following advantages: (a) it is based on Wikidata, which is the most frequently curated, openly available knowledge base, (b) it includes intermediate sparql queries to facilitate the evaluation of semantic parsing based approaches for KBQA, and (c) it generalizes to multiple knowledge bases: Freebase and Wikidata. The TempQA-WD dataset is available at https://github.com/IBM/tempqa-wd."}}
{"id": "pTX7RE8_EZs", "cdate": 1640995200000, "mdate": 1652608795851, "content": {"title": "Targeted Extraction of Temporal Facts from Textual Resources for Improved Temporal Question Answering over Knowledge Bases", "abstract": "Knowledge Base Question Answering (KBQA) systems have the goal of answering complex natural language questions by reasoning over relevant facts retrieved from Knowledge Bases (KB). One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors. In this paper, we address this particular challenge for systems handling a specific category of questions called temporal questions, where answer derivation involve reasoning over facts asserting point/intervals of time for various events. We propose a novel approach where a targeted temporal fact extraction technique is used to assist KBQA whenever it fails to retrieve temporal facts from the KB. We use $\\lambda$-expressions of the questions to logically represent the component facts and the reasoning steps needed to derive the answer. This allows us to spot those facts that failed to get retrieved from the KB and generate textual queries to extract them from the textual resources in an open-domain question answering fashion. We evaluated our approach on a benchmark temporal question answering dataset considering Wikidata and Wikipedia respectively as the KB and textual resource. Experimental results show a significant $\\sim$30\\% relative improvement in answer accuracy, demonstrating the effectiveness of our approach."}}
{"id": "_tLZo3qSXif", "cdate": 1640995200000, "mdate": 1681670175489, "content": {"title": "Zero-shot Entity Linking with Less Data", "abstract": "G P Shrivatsa Bhargav, Dinesh Khandelwal, Saswati Dana, Dinesh Garg, Pavan Kapanipathi, Salim Roukos, Alexander Gray, L Venkata Subramaniam. Findings of the Association for Computational Linguistics: NAACL 2022. 2022."}}
{"id": "TKJt3w66rkf", "cdate": 1640995200000, "mdate": 1681670175563, "content": {"title": "SYGMA: A System for Generalizable and Modular Question Answering Over Knowledge Bases", "abstract": "Sumit Neelam, Udit Sharma, Hima Karanam, Shajith Ikbal, Pavan Kapanipathi, Ibrahim Abdelaziz, Nandana Mihindukulasooriya, Young-Suk Lee, Santosh Srivastava, Cezar Pendus, Saswati Dana, Dinesh Garg, Achille Fokoue, G P Shrivatsa Bhargav, Dinesh Khandelwal, Srinivas Ravishankar, Sairam Gurajada, Maria Chang, Rosario Uceda-Sosa, Salim Roukos, Alexander Gray, Guilherme Lima, Ryan Riegel, Francois Luus, L V Subramaniam. Findings of the Association for Computational Linguistics: EMNLP 2022. 2022."}}
{"id": "MK35-FKQAFF", "cdate": 1640995200000, "mdate": 1681670175602, "content": {"title": "A Deep Neural Approach to KGQA via SPARQL Silhouette Generation", "abstract": "Knowledge Graph Question Answering (KGQA) has become a prominent area in natural language processing due to the emergence of large scale Knowledge Graphs (KGs). Semantic parsing based approach is the predominant direction to solve the KGQA task where natural language question is translated into a logic form such as SPARQL query. Recently Neural Machine Translation (NMT) based approaches are gaining momentum in order to translate natural language query to structured query languages thereby solving the KGQA task. However, most of these methods struggle with out-of-vocabulary words where test entities and relations are not seen during training time. In this work, we propose a modular two stage neural architecture to solve the KGQA task. Stage-I of our approach comprises a NMT-based seq2seq module that translates a question into a sketch of the desired SPARQL query called a SPARQL silhouette. Stage-II of our approach comprises a Neural Graph Search (NGS) module which aims to improve the quality of the SPARQL silhouette by detecting the right relations in the underlying knowledge graph. Experimental results show that we achieve substantial improvements and obtain state-of-the-art performance or comparable results to the best performing systems on two benchmark datasets. We believe, our proposed approach is novel and will lead to dynamic KGQA solutions that are well-suited for practical applications."}}
{"id": "8REVPGZVsP", "cdate": 1630638357327, "mdate": null, "content": {"title": "Explanations for CommonsenseQA: New Dataset and Models", "abstract": "CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for $11K$ QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in $F_1$ score, property generation model achieves a respectable $F_1$ score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric."}}
{"id": "x9gmMYujaVK", "cdate": 1609459200000, "mdate": 1652608795868, "content": {"title": "SYGMA: System for Generalizable Modular Question Answering OverKnowledge Bases", "abstract": "Knowledge Base Question Answering (KBQA) tasks that in-volve complex reasoning are emerging as an important re-search direction. However, most KBQA systems struggle withgeneralizability, particularly on two dimensions: (a) acrossmultiple reasoning types where both datasets and systems haveprimarily focused on multi-hop reasoning, and (b) across mul-tiple knowledge bases, where KBQA approaches are specif-ically tuned to a single knowledge base. In this paper, wepresent SYGMA, a modular approach facilitating general-izability across multiple knowledge bases and multiple rea-soning types. Specifically, SYGMA contains three high levelmodules: 1) KB-agnostic question understanding module thatis common across KBs 2) Rules to support additional reason-ing types and 3) KB-specific question mapping and answeringmodule to address the KB-specific aspects of the answer ex-traction. We demonstrate effectiveness of our system by evalu-ating on datasets belonging to two distinct knowledge bases,DBpedia and Wikidata. In addition, to demonstrate extensi-bility to additional reasoning types we evaluate on multi-hopreasoning datasets and a new Temporal KBQA benchmarkdataset on Wikidata, namedTempQA-WD1, introduced in thispaper. We show that our generalizable approach has bettercompetetive performance on multiple datasets on DBpediaand Wikidata that requires both multi-hop and temporal rea-soning"}}
