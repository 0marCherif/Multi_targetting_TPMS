{"id": "_en6jdfaW5D", "cdate": 1680527004985, "mdate": 1680527004985, "content": {"title": "Periodic flashing coordinated reset stimulation paradigm reduces sensitivity to ON and OFF period durations", "abstract": "Pathological synchronization in the basal ganglia network has been considered an important component of Parkinson\u2019s disease pathophysiology. An established treatment for some patients with Parkinson\u2019s disease is deep brain stimulation, in which a tonic high-frequency pulse train is delivered to target regions of the brain. In recent years, a novel neuromodulation paradigm called coordinated reset stimulation has been proposed, which aims to reverse the pathological synchrony by sequentially delivering short high-frequency bursts to distinct sub-regions of the pathologically synchronized network, with an average intra-burst interval for each sub-region corresponding to period of the pathological oscillation. It has further been proposed that the resultant desynchronization can be enhanced when stimulation is interrupted periodically, and that it is particularly beneficial to precisely tune the stimulation ON and OFF time-windows to the underlying pathological frequency. Pre-clinical and clinical studies of coordinated reset stimulation have relied on these proposals for their stimulation protocols. In this study, we present a modified ON-OFF coordinated reset stimulation paradigm called periodic flashing and study its behavior through computational modeling using the Kuramoto coupled phase oscillator model. We demonstrate that in contrast to conventional coordinated reset stimulation, the periodic flashing variation does not exhibit a need for precise turning of the ON-OFF periods to the pathological frequency, and demonstrates desynchronization for a wide range of ON and OFF periods. We provide a mechanistic explanation for the previously observed sensitivities and demonstrate that they are an artifact of the specific ON-OFF cycling paradigm used. As a practical consequence, the periodic flashing paradigm simplifies the tuning of optimal stimulation parameters by decreasing the dimension of the search space. It also suggests new, more flexible ways of delivering coordinated reset stimulation."}}
{"id": "-k_Wfa3k1K", "cdate": 1672773348682, "mdate": 1672773348682, "content": {"title": "Meta-learning synaptic plasticity and memory addressing for continual familiarity detection", "abstract": "Over the course of a lifetime, we process a continual stream of information. Extracted from this stream, memories must be efficiently encoded and stored in an addressable manner for retrieval. To explore potential mechanisms, we consider a familiarity detection task in which a subject reports whether an image has been previously encountered. We design a feedforward network endowed with synaptic plasticity and an addressing matrix, meta-learned to optimize familiarity detection over long intervals. We find that anti-Hebbian plasticity leads to better performance than Hebbian plasticity and replicates experimental results such as repetition suppression. A combinatorial addressing function emerges, selecting a unique neuron as an index into the synaptic memory matrix for storage or retrieval. Unlike previous models, this network operates continuously and generalizes to intervals it has not been trained on. Our work suggests a biologically plausible mechanism for continual learning and demonstrates an effective application of machine learning for neuroscience discovery."}}
{"id": "sMRdrUIrZbT", "cdate": 1621630016168, "mdate": null, "content": {"title": "Biological learning in key-value memory networks", "abstract": "In neuroscience, classical Hopfield networks are the standard biologically plausible model of long-term memory, relying on Hebbian plasticity for storage and attractor dynamics for recall. In contrast, memory-augmented neural networks in machine learning commonly use a key-value mechanism to store and read out memories in a single step. Such augmented networks achieve impressive feats of memory compared to traditional variants, yet their biological relevance is unclear. We propose an implementation of basic key-value memory that stores inputs using a combination of biologically plausible three-factor plasticity rules. The same rules are recovered when network parameters are meta-learned. Our network performs on par with classical Hopfield networks on autoassociative memory tasks and can be naturally extended to continual recall, heteroassociative memory, and sequence learning. Our results suggest a compelling alternative to the classical Hopfield network as a model of biological long-term memory."}}
{"id": "6pkC8GUsyDO", "cdate": 1621630016168, "mdate": null, "content": {"title": "Biological learning in key-value memory networks", "abstract": "In neuroscience, classical Hopfield networks are the standard biologically plausible model of long-term memory, relying on Hebbian plasticity for storage and attractor dynamics for recall. In contrast, memory-augmented neural networks in machine learning commonly use a key-value mechanism to store and read out memories in a single step. Such augmented networks achieve impressive feats of memory compared to traditional variants, yet their biological relevance is unclear. We propose an implementation of basic key-value memory that stores inputs using a combination of biologically plausible three-factor plasticity rules. The same rules are recovered when network parameters are meta-learned. Our network performs on par with classical Hopfield networks on autoassociative memory tasks and can be naturally extended to continual recall, heteroassociative memory, and sequence learning. Our results suggest a compelling alternative to the classical Hopfield network as a model of biological long-term memory."}}
{"id": "7lcPYJUx9w", "cdate": 1609459200000, "mdate": 1682346679107, "content": {"title": "Biological learning in key-value memory networks", "abstract": "In neuroscience, classical Hopfield networks are the standard biologically plausible model of long-term memory, relying on Hebbian plasticity for storage and attractor dynamics for recall. In contrast, memory-augmented neural networks in machine learning commonly use a key-value mechanism to store and read out memories in a single step. Such augmented networks achieve impressive feats of memory compared to traditional variants, yet their biological relevance is unclear. We propose an implementation of basic key-value memory that stores inputs using a combination of biologically plausible three-factor plasticity rules. The same rules are recovered when network parameters are meta-learned. Our network performs on par with classical Hopfield networks on autoassociative memory tasks and can be naturally extended to continual recall, heteroassociative memory, and sequence learning. Our results suggest a compelling alternative to the classical Hopfield network as a model of biological long-term memory."}}
