{"id": "xBn5KVpGfI", "cdate": 1685577600000, "mdate": 1703524949926, "content": {"title": "Towards Lower Bounds on the Depth of ReLU Neural Networks", "abstract": "We contribute to a better understanding of the class of functions that can be represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning any function. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). As a by-product of our investigations, we settle an old conjecture about piecewise linear functions by Wang and Sun [IEEE Trans. Inform. Theory, 51 (2005), pp. 4425\u20134431] in the affirmative. We also present upper bounds on the sizes of neural networks required to represent functions with logarithmic depth."}}
{"id": "oVaZz6_8cYQ", "cdate": 1672531200000, "mdate": 1681239748121, "content": {"title": "Lower Bounds on the Depth of Integral ReLU Neural Networks via Lattice Polytopes", "abstract": ""}}
{"id": "iMElHqwOoZs", "cdate": 1672531200000, "mdate": 1703524949981, "content": {"title": "ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation", "abstract": "This paper studies the expressive power of artificial neural networks with rectified linear units. In order to study them as a model of real-valued computation, we introduce the concept of Max-Affine Arithmetic Programs and show equivalence between them and neural networks concerning natural complexity measures. We then use this result to show that two fundamental combinatorial optimization problems can be solved with polynomial-size neural networks. First, we show that for any undirected graph with n nodes, there is a neural network (with fixed weights and biases) of size $$\\mathcal {O}(n^3)$$ that takes the edge weights as input and computes the value of a minimum spanning tree of the graph. Second, we show that for any directed graph with n nodes and m arcs, there is a neural network of size $$\\mathcal {O}(m^2n^2)$$ that takes the arc capacities as input and computes a maximum flow. Our results imply that these two problems can be solved with strongly polynomial time algorithms that solely uses affine transformations and maxima computations, but no comparison-based branchings."}}
{"id": "a_l-tiMdXk", "cdate": 1672531200000, "mdate": 1703524949988, "content": {"title": "A First Order Method for Linear Programming Parameterized by Circuit Imbalance", "abstract": "Various first order approaches have been proposed in the literature to solve Linear Programming (LP) problems, recently leading to practically efficient solvers for large-scale LPs. From a theoretical perspective, linear convergence rates have been established for first order LP algorithms, despite the fact that the underlying formulations are not strongly convex. However, the convergence rate typically depends on the Hoffman constant of a large matrix that contains the constraint matrix, as well as the right hand side, cost, and capacity vectors. We introduce a first order approach for LP optimization with a convergence rate depending polynomially on the circuit imbalance measure, which is a geometric parameter of the constraint matrix, and depending logarithmically on the right hand side, capacity, and cost vectors. This provides much stronger convergence guarantees. For example, if the constraint matrix is totally unimodular, we obtain polynomial-time algorithms, whereas the convergence guarantees for approaches based on primal-dual formulations may have arbitrarily slow convergence rates for this class. Our approach is based on a fast gradient method due to Necoara, Nesterov, and Glineur (Math. Prog. 2019); this algorithm is called repeatedly in a framework that gradually fixes variables to the boundary. This technique is based on a new approximate version of Tardos's method, that was used to obtain a strongly polynomial algorithm for combinatorial LPs (Oper. Res. 1986)."}}
{"id": "Yf-XuTKCVsm", "cdate": 1672531200000, "mdate": 1683702999436, "content": {"title": "Training Neural Networks is NP-Hard in Fixed Dimension", "abstract": "We study the parameterized complexity of training two-layer neural networks with respect to the dimension of the input data and the number of hidden neurons, considering ReLU and linear threshold activation functions. Albeit the computational complexity of these problems has been studied numerous times in recent years, several questions are still open. We answer questions by Arora et al. [ICLR '18] and Khalife and Basu [IPCO '22] showing that both problems are NP-hard for two dimensions, which excludes any polynomial-time algorithm for constant dimension. We also answer a question by Froese et al. [JAIR '22] proving W[1]-hardness for four ReLUs (or two linear threshold neurons) with zero training error. Finally, in the ReLU case, we show fixed-parameter tractability for the combined parameter number of dimensions and number of ReLUs if the network is assumed to compute a convex map. Our results settle the complexity status regarding these parameters almost completely."}}
{"id": "NS2_3N7u58f", "cdate": 1672531200000, "mdate": 1703524949983, "content": {"title": "Mode Connectivity in Auction Design", "abstract": "Optimal auction design is a fundamental problem in algorithmic game theory. This problem is notoriously difficult already in very simple settings. Recent work in differentiable economics showed that neural networks can efficiently learn known optimal auction mechanisms and discover interesting new ones. In an attempt to theoretically justify their empirical success, we focus on one of the first such networks, RochetNet, and a generalized version for affine maximizer auctions. We prove that they satisfy mode connectivity, i.e., locally optimal solutions are connected by a simple, piecewise linear path such that every solution on the path is almost as good as one of the two local optima. Mode connectivity has been recently investigated as an intriguing empirical and theoretically justifiable property of neural networks used for prediction problems. Our results give the first such analysis in the context of differentiable economics, where neural networks are used directly for solving non-convex optimization problems."}}
{"id": "IYa5--e3eO", "cdate": 1672531200000, "mdate": 1703524949977, "content": {"title": "Lower Bounds on the Depth of Integral ReLU Neural Networks via Lattice Polytopes", "abstract": ""}}
{"id": "2mvALOAWaxY", "cdate": 1663850270979, "mdate": null, "content": {"title": "Lower Bounds on the Depth of Integral ReLU Neural Networks via Lattice Polytopes", "abstract": "We prove that the set of functions representable by ReLU neural networks with integer weights strictly increases with the network depth while allowing arbitrary width. More precisely, we show that $\\lceil\\log_2(n)\\rceil$ hidden layers are indeed necessary to compute the maximum of $n$ numbers, matching known upper bounds. Our results are based on the known duality between neural networks and Newton polytopes via tropical geometry. The integrality assumption implies that these Newton polytopes are lattice polytopes. Then, our depth lower bounds follow from a parity argument on the normalized volume of faces of such polytopes."}}
{"id": "uP4Q-S0FIv", "cdate": 1640995200000, "mdate": 1681239748205, "content": {"title": "Online algorithms to schedule a proportionate flexible flow shop of batching machines", "abstract": ""}}
{"id": "rYknnkEteT", "cdate": 1640995200000, "mdate": 1681239748115, "content": {"title": "Training Fully Connected Neural Networks is \u2203R-Complete", "abstract": ""}}
