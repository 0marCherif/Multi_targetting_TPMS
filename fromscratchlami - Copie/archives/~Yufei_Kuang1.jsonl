{"id": "i7gGYH-wYz-", "cdate": 1676827066444, "mdate": null, "content": {"title": "Learning Robust Representation for Reinforcement Learning with Distractions by Reward Sequence Prediction", "abstract": "Reinforcement learning algorithms have achieved remarkable success in acquiring behavioral skills directly from pixel inputs. However, their application in real-world scenarios presents challenges due to their sensitivity to visual distractions (e.g., changes in viewpoint and light). A key factor contributing to this challenge is that the learned representations often suffer from overfitting task-irrelevant information. By comparing several representation learning methods, we find that the key to alleviating overfitting in representation learning is to choose proper prediction targets. Motivated by our comparison, we propose a novel representation learning approach---namely, reward sequence prediction (RSP)---that uses reward sequences or their transforms (e.g., discrete time Fourier transform) as prediction targets. RSP can efficiently learn robust representations as reward sequences rarely contain task-irrelevant information while providing a large number of supervised signals to accelerate representation learning. An appealing feature is that RSP makes no assumption about the type of distractions and thus can improve performance even when multiple types of distractions exist. We evaluate our approach in Distracting Control Suite. Experiments show that our method achieves state-of-the-art sample efficiency and generalization ability in tasks with distractions. "}}
{"id": "JRysJUehLk", "cdate": 1672531200000, "mdate": 1681570440009, "content": {"title": "Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model", "abstract": ""}}
{"id": "Zob4P9bRNcK", "cdate": 1663850127255, "mdate": null, "content": {"title": "Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model", "abstract": "Cutting planes (cuts) are important for solving mixed-integer linear programs (MILPs), which formulate a wide range of important real-world applications. Cut selection---which aims to select a proper subset of the candidate cuts to improve the efficiency of solving MILPs---heavily depends on (P1) which cuts should be preferred, and (P2) how many cuts should be selected. Although many modern MILP solvers tackle (P1)-(P2) by manually designed heuristics, machine learning offers a promising approach to learn more effective heuristics from MILPs collected from specific applications. However, many existing learning-based methods focus on learning which cuts should be preferred, neglecting the importance of learning the number of cuts that should be selected. Moreover, we observe from extensive empirical results that (P3) what order of selected cuts should be preferred has a significant impact on the efficiency of solving MILPs as well. To address this challenge, we propose a novel hierarchical sequence model (HEM) to learn cut selection policies via reinforcement learning. Specifically, HEM consists of a two-level model: (1) a higher-level model to learn the number of cuts that should be selected, (2) and a lower-level model---that formulates the cut selection task as a sequence to sequence learning problem---to learn policies selecting an ordered subset with the size determined by the higher-level model. To the best of our knowledge, HEM is the first method that can tackle (P1)-(P3) in cut selection simultaneously from a data-driven perspective. Experiments show that HEM significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs, including MIPLIB 2017. Moreover, experiments demonstrate that HEM well generalizes to MILPs that are significantly larger than those seen during training."}}
{"id": "WrqWxYJjK3", "cdate": 1640995200000, "mdate": 1681570440171, "content": {"title": "Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization", "abstract": ""}}
{"id": "O9N-CwUqAau", "cdate": 1609459200000, "mdate": 1681570440541, "content": {"title": "Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization", "abstract": ""}}
{"id": "hWIuhl474AC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Promoting Stochasticity for Expressive Policies via a Simple and Efficient Regularization Method", "abstract": "Many recent reinforcement learning (RL) methods learn stochastic policies with entropy regularization for exploration and robustness. However, in continuous action spaces, integrating entropy regularization with expressive policies is challenging and usually requires complex inference procedures. To tackle this problem, we propose a novel regularization method that is compatible with a broad range of expressive policy architectures. An appealing feature is that, the estimation of our regularization terms is simple and efficient even when the policy distributions are unknown. We show that our approach can effectively promote the exploration in continuous action spaces. Based on our regularization, we propose an off-policy actor-critic algorithm. Experiments demonstrate that the proposed algorithm outperforms state-of-the-art regularized RL methods in continuous control tasks."}}
