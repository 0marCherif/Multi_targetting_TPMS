{"id": "OgK9XuZhO6", "cdate": 1668387739223, "mdate": 1668387739223, "content": {"title": "Transferring Domain-Agnostic Knowledge in Video Question Answering", "abstract": "Video question answering (VideoQA) is designed to answer a given question based\non a relevant video clip. The current available large-scale datasets have made it possible\nto formulate VideoQA as the joint understanding of visual and language information.\nHowever, this training procedure is costly and still less competent with human performance. In this paper, we investigate a transfer learning method by the introduction of\ndomain-agnostic knowledge and domain-specific knowledge. First, we develop a novel\ntransfer learning framework, which finetunes the pre-trained model by applying domainagnostic knowledge as the medium. Second, we construct a new VideoQA dataset with\n21,412 human-generated question-answer samples for comparable transfer of knowledge. Our experiments show that: (i) domain-agnostic knowledge is transferable and (ii)\nour proposed transfer learning framework can boost VideoQA performance effectively."}}
{"id": "kXlZCD9Z7B", "cdate": 1668387558845, "mdate": 1668387558845, "content": {"title": "BERT Representations for Video Question Answering", "abstract": "Visual question answering (VQA) aims at answering\nquestions about the visual content of an image or a video.\nCurrently, most work on VQA is focused on image-based\nquestion answering, and less attention has been paid into\nanswering questions about videos. However, VQA in video\npresents some unique challenges that are worth studying:\nit not only requires to model a sequence of visual features\nover time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a\nsequential modelling technique based on Transformers, to\nencode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments,\nwe exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two\nwell-known video VQA datasets: TVQA and Pororo."}}
{"id": "w-rA57cFPM", "cdate": 1640995200000, "mdate": 1666625470137, "content": {"title": "An Intelligent Color Recommendation Tool for Landing Page Design", "abstract": ""}}
{"id": "tRrEgahCHo", "cdate": 1640995200000, "mdate": 1666625470135, "content": {"title": "AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval", "abstract": "Evaluation measures have a crucial impact on the direction of research. Therefore, it is of utmost importance to develop appropriate and reliable evaluation measures for new applications where conventional measures are not well suited. Video Moment Retrieval (VMR) is one such application, and the current practice is to use R@K, <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\theta$</tex> for evaluating VMR systems. However, this measure has two disadvantages. First, it is rank-insensitive: It ignores the rank positions of successfully localised moments in the top-K ranked list by treating the list as a set. Second, it binarizes the Intersection over Union (IoU) of each retrieved video moment using the threshold <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\theta$</tex> and thereby ignoring fine-grained localisation quality of ranked moments. We propose an alternative measure for evaluating VMR, called Average Max IoU (AxIoU), which is free from the above two problems. We show that AxIoU satisfies two important axioms for VMR evaluation, namely, Invariance against Redundant Moments and Monotonicity with respect to the Best Moment, and also that R@ K, <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\theta$</tex> satisfies the first axiom only. We also empirically examine how Ax-IoU agrees with R@K, <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\theta$</tex> , as well as its stability with respect to change in the test data and human-annotated temporal boundaries."}}
{"id": "sRS6EWUyXek", "cdate": 1640995200000, "mdate": 1666625470294, "content": {"title": "Does Robustness on ImageNet Transfer to Downstream Tasks?", "abstract": "As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classification. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classification from different domains. This raises a question: Can these robust image classifiers transfer robustness to downstream tasks? For object detection and semantic segmentation, we find that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classification, we find that models that are robustified for ImageNet do not retain robustness when fully fine-tuned. These findings suggest that current robustification techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning."}}
{"id": "r8S3T6EkRRh", "cdate": 1640995200000, "mdate": 1666625470270, "content": {"title": "Optimal Correction Cost for Object Detection Evaluation", "abstract": "Mean Average Precision (mAP) is the primary evaluation measure for object detection. Although object detection has a broad range of applications, mAP evaluates detectors in terms of the performance of ranked instance retrieval. Such the assumption for the evaluation task does not suit some downstream tasks. To alleviate the gap between downstream tasks and the evaluation scenario, we propose Optimal Correction Cost (OC-cost), which assesses detection accuracy at image level. OC-cost computes the cost of correcting detections to ground truths as a measure of accuracy. The cost is obtained by solving an optimal transportation problem between the detections and the ground truths. Unlike mAP, OC-cost is designed to penalize false positive and false negative detections properly, and every image in a dataset is treated equally. Our experimental result validates that OC-cost has better agreement with human preference than a ranking-based measure, i.e., mAP for a single image. We also show that detectors' rankings by OC-cost are more consistent on different data splits than mAP. Our goal is not to replace mAP with OC-cost but provide an additional tool to evaluate detectors from another aspect. To help future researchers and developers choose a target measure, we provide a series of experiments to clarify how mAP and OC-cost differ."}}
{"id": "kNVObW7v6hK", "cdate": 1640995200000, "mdate": 1666625470270, "content": {"title": "AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval", "abstract": "Evaluation measures have a crucial impact on the direction of research. Therefore, it is of utmost importance to develop appropriate and reliable evaluation measures for new applications where conventional measures are not well suited. Video Moment Retrieval (VMR) is one such application, and the current practice is to use R@$K,\\theta$ for evaluating VMR systems. However, this measure has two disadvantages. First, it is rank-insensitive: It ignores the rank positions of successfully localised moments in the top-$K$ ranked list by treating the list as a set. Second, it binarizes the Intersection over Union (IoU) of each retrieved video moment using the threshold $\\theta$ and thereby ignoring fine-grained localisation quality of ranked moments. We propose an alternative measure for evaluating VMR, called Average Max IoU (AxIoU), which is free from the above two problems. We show that AxIoU satisfies two important axioms for VMR evaluation, namely, \\textbf{Invariance against Redundant Moments} and \\textbf{Monotonicity with respect to the Best Moment}, and also that R@$K,\\theta$ satisfies the first axiom only. We also empirically examine how AxIoU agrees with R@$K,\\theta$, as well as its stability with respect to change in the test data and human-annotated temporal boundaries."}}
{"id": "hKYaDjICnm", "cdate": 1640995200000, "mdate": 1666625470140, "content": {"title": "Does Robustness on ImageNet Transfer to Downstream Tasks?", "abstract": "As clean ImageNet accuracy nears its ceiling, the re-search community is increasingly more concerned about ro-bust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classification. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classification from different domains. This raises a question: Can these robust image classifiers transfer robustness to downstream tasks? For object detection and semantic segmentation, we find that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neu-ral Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classification, we find that models that are robustified for ImageNet do not re-tain robustness when fully fine-tuned. These findings sug-gest that current robustification techniques tend to empha-size ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning."}}
{"id": "_TsbgOj4qa", "cdate": 1640995200000, "mdate": 1666625470138, "content": {"title": "Video Summarization Overview", "abstract": ""}}
{"id": "Wa45kPXVdMV", "cdate": 1640995200000, "mdate": 1666625470296, "content": {"title": "Learning More May Not Be Better: Knowledge Transferability in Vision and Language Tasks", "abstract": "Is more data always better to train vision-and-language models? We study knowledge transferability in multi-modal tasks. The current tendency in machine learning is to assume that by joining multiple datasets from different tasks their overall performance will improve. However, we show that not all the knowledge transfers well or has a positive impact on related tasks, even when they share a common goal. We conduct an exhaustive analysis based on hundreds of cross-experiments on 12 vision-and-language tasks categorized in 4 groups. Whereas tasks in the same group are prone to improve each other, results show that this is not always the case. Other factors such as dataset size or pre-training stage have also a great impact on how well the knowledge is transferred."}}
