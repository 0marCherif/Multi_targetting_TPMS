{"id": "yulAchHedcT", "cdate": 1632875547835, "mdate": null, "content": {"title": "Faster Neural Net Inference via Forests of Sparse Oblique Decision Trees", "abstract": " It is widely established that large neural nets can be considerably compressed by techniques such as pruning, quantization or low-rank factorization. We show that neural nets can be further compressed by replacing layers of it with a special type of decision forest. This consists of sparse oblique trees, trained with the Tree Alternating Optimization (TAO) algorithm, using a teacher-student approach. We find we can replace the fully-connected and some convolutional layers of standard architectures with a decision forest containing very few, shallow trees so that the prediction accuracy is preserved or improved, but the number of parameters and especially the inference time is greatly reduced. For example, replacing last 7 layers of VGG16 with a single tree reduces the inference FLOPs by 7440$\\times$ with a marginal increase in the test error, and a boosted ensemble of nine trees can match the network's performance while still reducing the FLOPs 6289$\\times$. The idea is orthogonal to other compression approaches, which can also be used on other parts of the net not being replaced by a forest."}}
{"id": "wT7FqOZCDS_", "cdate": 1620733773784, "mdate": null, "content": {"title": "Optimal Selection of Matrix Shape and Decomposition Scheme for Neural Network Compression ", "abstract": "When applying the low-rank decomposition to neural networks,tensor-shaped weights need to be reshaped into a matrix first. While many matrix reshapes are possible, some of them induce a low-rank decomposition scheme that can be more efficiently implemented as a sequence of layers. This poses the following problem: how should one select both the matrix reshape and associated low-rank decomposition scheme in order to compress a neural network so that its implementation is as efficient as possible? We formulate this problem as a mixed-integer optimization over the weights, ranks,and decompositions schemes; and we provide an efficient alternating optimization algorithm involving two simple steps: a step over the weights of the neural network (solved by SGD), and a step over the ranks and decomposition schemes (solved by an SVD).Our algorithm automatically selects the most suitable ranks and decomposition schemes to efficiently reduce compression costs (e.g.,FLOPs) of various networks."}}
{"id": "b1lMDqXeYNF", "cdate": 1620733631563, "mdate": null, "content": {"title": "Optimal Quantization using Scaled Codebook ", "abstract": "We study the problem of quantizingNsorted, scalar datapoints with a fixed codebook containing K entries that are allowed to be rescaled.  The problem is defined as finding the optimal scaling factor \u03b1 and the datapoint assignments into  the \u03b1-scaled  codebook  to  minimize  the  squared  error between original and quantized points.  Previously, the globally optimal algorithms for this problem were derived only for certain codebooks (binary and ternary) or under the assumption of certain distributions (Gaussian,  Laplacian).  By studying the properties of the optimal quantizer, we derive an O(NKlogK) algorithm that is guaranteed to  find  the  optimal  quantization  parameters  for  any  fixed codebook regardless of data distribution.  We apply our algorithm to synthetic and real-world neural network quantization problems and demonstrate the effectiveness of ourapproach."}}
{"id": "iD-RRpnE695", "cdate": 1599596917627, "mdate": null, "content": {"title": "Structured Multi-Hashing for Model Compression", "abstract": "Despite the success of deep neural networks (DNNs), state-of-the-art models are too large to deploy on low-resource devices or common server configurations in which multiple models are held in memory. Model compression methods address this limitation by reducing the memory footprint, latency, or energy consumption of a model with minimal impact on accuracy. We focus on the task of reducing the number of learnable variables in the model. In this work we combine ideas from weight hashing and dimensionality reductions resulting in a simple and powerful structured multi-hashing method based on matrix products that allows direct control of model size of any deep network and is trained end-to-end. We demonstrate the strength of our approach by compressing models from the ResNet, EfficientNet, and MobileNet architecture families. Our method allows us to drastically decrease the number of variables while maintaining high accuracy. For instance, by applying our approach to EfficentNet-B4 (16M parameters) we reduce it to the size of B0 (5M parameters), while gaining over 3% in accuracy over B0 baseline. On the commonly used benchmark CIFAR10 we reduce the ResNet32 model by 75% with no loss in quality, and are able to do a 10x compression while still achieving above 90% accuracy."}}
{"id": "5vW4EwYUV7b", "cdate": 1599596759140, "mdate": null, "content": {"title": "Low-rank Compression of Neural Nets: Learning the Rank of Each Layer", "abstract": "Neural net compression can be achieved by approximating each layer\u2019s weight matrix by a low-rank matrix. The real difficulty in doing this is not in training the resulting neural net (made up of one low-rank matrix per layer), but in determining what the optimal rank of each layer is\u2014effectively, an architecture search problem with one hyper-parameter per layer.  We show that, with a suitable formulation, this problem is amenable to a mixed discrete-continuous optimization jointly over the ranks and over the matrix elements, and give a corresponding algorithm. Weshow that this indeed can select ranks much better than existing approaches, making low-rank compression much more attractive than previously thought. For example, we can make a VGG network faster than a ResNet and with nearly the same classification error."}}
{"id": "H1fZLbSti7", "cdate": 1540079945180, "mdate": null, "content": {"title": "A Flexible, Extensible Software Framework for Neural Net Compression", "abstract": "We propose a software framework based on ideas of the Learning-Compression algorithm , that allows one to compress any neural network by different compression mechanisms (pruning, quantization, low-rank, etc.). By design, the learning of the neural net (handled by SGD) is decoupled from the compression of its parameters (handled by a signal compression function), so that the framework can be easily extended to handle different combinations of neural net and compression type. In addition, it has other advantages, such as easy integration with deep learning frameworks, efficient training time, competitive practical performance in the loss-compression tradeoff, and reasonable convergence guarantees. Our toolkit is written in Python and Pytorch and we plan to make it available by the workshop time, and eventually open it for contributions from the community."}}
{"id": "HJVDo0ZdWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "\"Learning-Compression\" Algorithms for Neural Net Pruning", "abstract": "Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates \"learning\" steps that optimize a regularized, data-dependent loss and \"compression\" steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruning-level user parameter, we achieve state-of-the-art pruning in nets of various sizes."}}
