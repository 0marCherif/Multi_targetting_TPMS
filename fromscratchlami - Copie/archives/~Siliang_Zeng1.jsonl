{"id": "W5SrUCN0yUa", "cdate": 1686324860594, "mdate": null, "content": {"title": "A Bayesian Approach to Robust Inverse Reinforcement Learning", "abstract": "We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert's reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert\u2019s model of the environment is to develop efficient algorithms to estimate the expert's reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms."}}
{"id": "4pr5WPHKx0F", "cdate": 1672531200000, "mdate": 1682354573608, "content": {"title": "Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning", "abstract": ""}}
{"id": "FfELl5h3Nec", "cdate": 1653752160554, "mdate": null, "content": {"title": "Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees", "abstract": "Inverse reinforcement learning (IRL) aims to recover the reward function and the associated optimal policy that best fits observed sequences of states and actions implemented by an expert. Many algorithms for IRL have an inherent nested structure: the inner loop finds the optimal policy given parametrized rewards while the outer loop updates the estimates towards optimizing a measure of fit. For high dimensional environments such nested-loop structure entails a significant computational burden. To reduce the computational burden of a nested loop, novel methods such as SQIL [1] and IQ-Learn [2] emphasize policy estimation at the expense of reward estimation accuracy. However, without accurate estimated rewards, it is not possible to do counterfactual analysis such as predicting the optimal policy under different environment dynamics and/or learning new tasks. In this paper we develop a novel single-loop algorithm for IRL that does not compromise reward estimation accuracy. In the proposed algorithm, each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm provably converges to a stationary solution with a finite-time guarantee. If the reward is parameterized linearly, we show the identified solution corresponds to the solution of the maximum entropy IRL problem. Finally, by using robotics control problems in Mujoco and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks."}}
{"id": "0RMDK39mGg", "cdate": 1652737684336, "mdate": null, "content": {"title": "A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization", "abstract": "Bilevel optimization has been shown to be a powerful framework for formulating multi-task machine learning problems, e.g., reinforcement learning (RL) and meta-learning, where the decision variables are coupled in both levels of the minimization problems. In practice, the learning tasks would be located at different computing resource environments, and thus there is a need for deploying a decentralized training framework to implement multi-agent and multi-task learning. We develop a stochastic linearized augmented Lagrangian method (SLAM) for solving general nonconvex bilevel optimization problems over a graph, where both upper and lower optimization variables are able to achieve a consensus. We also establish that the theoretical convergence rate of the proposed SLAM to the Karush-Kuhn-Tucker (KKT) points of this class of problems is on the same order as the one achieved by the classical distributed stochastic gradient descent for only single-level nonconvex minimization problems. Numerical results tested on multi-agent RL problems showcase the superiority of SLAM compared with the benchmarks."}}
{"id": "zbt3VmTsRIj", "cdate": 1652737655063, "mdate": null, "content": {"title": "Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees", "abstract": "Inverse reinforcement learning (IRL) aims to recover the reward function and the associated optimal policy that best fits observed sequences of states and actions implemented by an expert. Many algorithms for IRL have an inherent nested structure: the inner loop finds the optimal policy given parametrized rewards while the outer loop updates the estimates towards optimizing a measure of fit. For high dimensional environments such nested-loop structure entails a significant computational burden. To reduce the computational burden of a nested loop, novel methods such as SQIL \\cite{reddy2019sqil} and IQ-Learn \\cite{garg2021iq} emphasize policy estimation at the expense of reward estimation accuracy. However, without accurate estimated rewards, it is not possible to do counterfactual analysis such as predicting the optimal policy under different environment dynamics and/or learning new tasks. In this paper we develop a novel {\\em single-loop} algorithm for IRL that does not compromise reward estimation accuracy. In the proposed algorithm, each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm provably converges to a stationary solution with a finite-time guarantee. If the reward is parameterized linearly we show the identified solution corresponds to the solution of the maximum entropy IRL problem. Finally, by using robotics control problems in Mujoco and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks."}}
{"id": "S_y6mFUmwvP", "cdate": 1640995200000, "mdate": 1682354573575, "content": {"title": "Learning to Coordinate in Multi-Agent Systems: A Coordinated Actor-Critic Algorithm and Finite-Time Guarantees", "abstract": "Multi-agent reinforcement learning (MARL) has attracted much research attention recently. However, unlike its single-agent counterpart, many theoretical and algorithmic aspects of MARL have not bee..."}}
{"id": "7OFiDKy4QZ", "cdate": 1640995200000, "mdate": 1682354573613, "content": {"title": "Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees", "abstract": ""}}
{"id": "nNpDhjI2T_s", "cdate": 1632875608714, "mdate": null, "content": {"title": "Learning to Coordinate in Multi-Agent Systems: A Coordinated Actor-Critic Algorithm and Finite-Time Guarantees", "abstract": "Multi-agent reinforcement learning (MARL) has attracted much research attention recently. However, unlike its single-agent counterpart, many theoretical and algorithmic aspects of MARL have not been well-understood. In this paper, we study the emergence of coordinated behavior by autonomous agents using an actor-critic (AC) algorithm. Specifically, we propose and analyze a class of coordinated actor-critic algorithms (CAC) in which individually parametrized policies have a shared part (which is jointly optimized among all agents) and a personalized part (which is only locally optimized). Such kind of partially personalized policy allows agents to learn to coordinate by leveraging peers\u2019 past experience and adapt to individual tasks. The flexibility in our design allows the proposed MARL-CAC algorithm to be used in a fully decentralized setting, where the agents can only communicate with their neighbors, as well as a federated setting, where the agents occasionally communicate with a server while optimizing their (partially personalized) local models. Theoretically, we show that under some standard regularity assumptions, the proposed MARL-CAC algorithm requires $\\mathcal{O}(\\epsilon^{-\\frac{5}{2}})$ samples to achieve an\u000f$\\epsilon$-stationary solution (defined as the solution whose squared norm of the gradient of the objective function is less than\u000f$\\epsilon$). To the best of our knowledge, this work provides the first finite-sample guarantee for decentralized AC algorithm with partially personalized policies.\n"}}
{"id": "HjFtRc83eBB", "cdate": 1621630150475, "mdate": null, "content": {"title": "A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum", "abstract": "This paper proposes a new algorithm -- the  \\underline{S}ingle-timescale Do\\underline{u}ble-momentum \\underline{St}ochastic \\underline{A}pprox\\underline{i}matio\\underline{n} (SUSTAIN) -- for tackling stochastic unconstrained bilevel optimization problems. We focus on bilevel problems where the lower level subproblem is strongly-convex and the upper level objective function is smooth. Unlike prior works which rely on \\emph{two-timescale} or \\emph{double loop} techniques, we design a stochastic momentum-assisted gradient estimator for both the upper and lower level updates. The latter allows us to control the error in the stochastic gradient updates due to inaccurate solution to both subproblems. If the upper objective function is smooth but possibly non-convex, we show that {SUSTAIN}~requires $O(\\epsilon^{-3/2})$  iterations (each using $O(1)$ samples) to find an $\\epsilon$-stationary solution. The $\\epsilon$-stationary solution is defined as the point whose squared norm of the gradient of the outer function is less than or equal to $\\epsilon$.  The total number of stochastic gradient samples required for the upper and lower level objective functions matches the best-known complexity for single-level stochastic gradient algorithms. We also analyze the case when the upper level objective function is strongly-convex. "}}
{"id": "rfO5P8qhCK0", "cdate": 1609459200000, "mdate": 1653668433285, "content": {"title": "A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum", "abstract": "This paper proposes a new algorithm -- the \\underline{S}ingle-timescale Do\\underline{u}ble-momentum \\underline{St}ochastic \\underline{A}pprox\\underline{i}matio\\underline{n} (SUSTAIN) -- for tackling stochastic unconstrained bilevel optimization problems. We focus on bilevel problems where the lower level subproblem is strongly-convex and the upper level objective function is smooth. Unlike prior works which rely on \\emph{two-timescale} or \\emph{double loop} techniques, we design a stochastic momentum-assisted gradient estimator for both the upper and lower level updates. The latter allows us to control the error in the stochastic gradient updates due to inaccurate solution to both subproblems. If the upper objective function is smooth but possibly non-convex, we show that {SUSTAIN}~requires $O(\\epsilon^{-3/2})$ iterations (each using $O(1)$ samples) to find an $\\epsilon$-stationary solution. The $\\epsilon$-stationary solution is defined as the point whose squared norm of the gradient of the outer function is less than or equal to $\\epsilon$. The total number of stochastic gradient samples required for the upper and lower level objective functions matches the best-known complexity for single-level stochastic gradient algorithms. We also analyze the case when the upper level objective function is strongly-convex."}}
