{"id": "zo3xyJwLRGi", "cdate": 1672531200000, "mdate": 1681516520052, "content": {"title": "MAGVLT: Masked Generative Vision-and-Language Transformer", "abstract": ""}}
{"id": "M6N7mXCcnU", "cdate": 1672531200000, "mdate": 1677745228983, "content": {"title": "The Liver Tumor Segmentation Benchmark (LiTS)", "abstract": ""}}
{"id": "ppyhnsPLvg", "cdate": 1654518799825, "mdate": 1654518799825, "content": {"title": "Insights from the NeurIPS 2021 NetHack Challenge", "abstract": "In this report, we summarize the takeaways from the first NeurIPS 2021 NetHack Challenge. Participants were tasked with developing a program or agent that can win (i.e., \u2018ascend\u2019 in) the popular dungeon-crawler game of NetHack by interacting with the NetHack Learning Environment (NLE), a scalable, procedurally generated, and challenging Gym environment for reinforcement learning (RL). The challenge showcased community-driven progress in AI with many diverse approaches significantly beating the previously best results on NetHack. Furthermore, it served as a direct comparison between neural (e.g., deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on NetHack symbolic bots currently outperform deep RL by a large margin. Lastly, no agent got close to winning the game, illustrating NetHack\u2019s suitability as a long-term benchmark for AI research."}}
{"id": "FJ42JCNNUYT", "cdate": 1652737566227, "mdate": null, "content": {"title": "LECO: Learnable Episodic Count for Task-Specific Intrinsic Reward", "abstract": "Episodic count has been widely used to design a simple yet effective intrinsic motivation for reinforcement learning with a sparse reward. However, the use of episodic count in a high-dimensional state space as well as over a long episode time requires a thorough state compression and fast hashing, which hinders rigorous exploitation of it in such hard and complex exploration environments. Moreover, the interference from task-irrelevant observations in the episodic count may cause its intrinsic motivation to overlook task-related important changes of states, and the novelty in an episodic manner can lead to repeatedly revisit the familiar states across episodes. In order to resolve these issues, in this paper, we propose a learnable hash-based episodic count, which we name LECO, that efficiently performs as a task-specific intrinsic reward in hard exploration problems. In particular, the proposed intrinsic reward consists of the episodic novelty and the task-specific modulation where the former employs a vector quantized variational autoencoder to automatically obtain the discrete state codes for fast counting while the latter regulates the episodic novelty by learning a modulator to optimize the task-specific extrinsic reward. The proposed LECO specifically enables the automatic transition from exploration to exploitation during reinforcement learning. We experimentally show that in contrast to the previous exploration methods LECO successfully solves hard exploration problems and also scales to large state spaces through the most difficult tasks in MiniGrid and DMLab environments."}}
{"id": "mUuYtoxFfBy", "cdate": 1640995200000, "mdate": 1681516519892, "content": {"title": "LECO: Learnable Episodic Count for Task-Specific Intrinsic Reward", "abstract": ""}}
{"id": "_6BT6R8LaHj", "cdate": 1640995200000, "mdate": 1681516520153, "content": {"title": "Selective Token Generation for Few-shot Natural Language Generation", "abstract": ""}}
{"id": "XPlw436v6I", "cdate": 1640995200000, "mdate": 1681516519930, "content": {"title": "Selective Token Generation for Few-shot Natural Language Generation", "abstract": ""}}
{"id": "QMuPvkkxHpj", "cdate": 1640995200000, "mdate": 1664786934363, "content": {"title": "Insights From the NeurIPS 2021 NetHack Challenge", "abstract": "In this report, we summarize the takeaways from the first NeurIPS 2021 NetHack Challenge. Participants were tasked with developing a program or agent that can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by interacting with the NetHack Learning Environment (NLE), a scalable, procedurally generated, and challenging Gym environment for reinforcement learning (RL). The challenge showcased community-driven progress in AI with many diverse approaches significantly beating the previously best results on NetHack. Furthermore, it served as a direct comparison between neural (e.g., deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on NetHack symbolic bots currently outperform deep RL by a large margin. Lastly, no agent got close to winning the game, illustrating NetHack's suitability as a long-term benchmark for AI research."}}
{"id": "ADGwDsbkt7H", "cdate": 1640995200000, "mdate": 1668240646537, "content": {"title": "Contrastive Regularization for Semi-Supervised Learning", "abstract": ""}}
{"id": "9bPn-I155q", "cdate": 1640995200000, "mdate": 1668240646551, "content": {"title": "Contrastive Regularization for Semi-Supervised Learning", "abstract": "Consistency regularization on label predictions becomes a fundamental technique in semi-supervised learning, but it still requires a large number of training iterations for high performance. In this study, we analyze that the consistency regularization restricts the propagation of labeling information due to the exclusion of samples with unconfident pseudo-labels in the model updates. Then, we propose contrastive regularization to improve both efficiency and accuracy of the consistency regularization by well-clustered features of unlabeled data. In specific, after strongly augmented samples are assigned to clusters by their pseudo-labels, our contrastive regularization updates the model so that the features with confident pseudo-labels aggregate the features in the same cluster, while pushing away features in different clusters. As a result, the information of confident pseudo-labels can be effectively propagated into more unlabeled samples during training by the well-clustered features. On benchmarks of semi-supervised learning tasks, our contrastive regularization improves the previous consistency-based methods and achieves state-of-the-art results, especially with fewer training iterations. Our method also shows robust performance on open-set semi-supervised learning where unlabeled data includes out-of-distribution samples."}}
