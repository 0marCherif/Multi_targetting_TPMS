{"id": "A_bRaHYi1P", "cdate": 1675318109339, "mdate": null, "content": {"title": "Behavioral differences: insights, explanations and comparisons of French and US Twitter usage during elections", "abstract": "Social networks and social media have played a key role for observing and influencing how the political landscape takes shape and dynamically shifts. It is especially true in events such as national elections as indicated by earlier studies with Facebook (Williams and Gulati, in: Proceedings of the annual meeting of the American Political Science Association, 2009) and Twitter (Larsson and Moe in New Med Soc 14(5):729\u2013747, 2012). Not surprisingly in an attempt to better understand and simplify these networks, community discovery methods have been used, such as the Louvain method (Blondel et al. in J Stat Mechanics Theory Exp 2008(10):P10008, 2008) to understand elections (Gaumont et al. in PLoS ONE 13(9):e0201879, 2018). However, most community-based studies first simplify the complex Twitter data into a single network based on (for example) follower, retweet or friendship properties. This requires ignoring some information or combining many types of information into a graph, which can mask many insights. In this paper, we explore Twitter data as a time-stamped vertex-labeled graph. The graph structure can be given by a structural relation between the users such as retweet, friendship or follower relation, whilst the behavior of the individual is given by their posting behavior which is modeled as a time-evolving vertex labels. We explore leveraging existing community discovery methods to find communities using just the structural data and then describe these communities using behavioral data. We explore two complimentary directions: (1)  creating a taxonomy of hashtags based on their community usage and (2) efficiently describing the communities expanding our recently published work. We have created two datasets, one each for the French and US elections from which we compare and contrast insights on the usage of hashtags."}}
{"id": "2anIiLnwSIp", "cdate": 1664952474760, "mdate": 1664952474760, "content": {"title": "Writing Style Author Embedding Evaluation", "abstract": "Learning authors representations from their textual productions is now widely used to solve multiple downstream tasks, such as classification, link prediction or user recommendation. Author embedding methods are often built on top of either Doc2Vec (Le and Mikolov, 2014) or the Transformer architecture (Devlin et al., 2019). Evaluating the quality of these embeddings and what they capture is a difficult task. Most articles use either classification accuracy or authorship attribution, which does not clearly measure the quality of the representation space, if it really captures what it has been built for. In this paper, we propose a novel evaluation framework of author embedding methods based on the writing style. It allows to quantify if the embedding space effectively captures a set of stylistic features, chosen to be the best proxy of an author writing style. This approach gives less importance to the topics conveyed by the documents. It turns out that recent models are mostly driven by the inner semantic of authors\u2019 production. They are outperformed by simple baselines, based on state-of-the-art pretrained sentence embedding models, on several linguistic axes. These baselines can grasp complex linguistic phenomena and writing style more efficiently, paving the way for designing new style-driven author embedding models."}}
{"id": "ovha5L1PIbv", "cdate": 1664952385906, "mdate": 1664952385906, "content": {"title": "Gaussian Embedding of Linked Documents from a Pretrained Semantic Space", "abstract": "Gaussian Embedding of Linked Documents (GELD) is a new method that embeds linked documents (e.g., citation networks) onto a pretrained semantic space (e.g., a set of word embeddings). We formulate the problem in such a way that we model each document as a Gaussian distribution in the word vector space. We design a generative model that combines both words and links in a consistent way. Leveraging the variance of a document allows us to model the uncertainty related to word and link generation. In most cases, our method outperforms state-of-the-art methods when using our document vectors as features for usual downstream tasks. In particular, GELD achieves better accuracy in classification and link prediction on Cora and Dblp. In addition, we demonstrate qualitatively the convenience of several properties of our method. We provide the implementation of GELD and the evaluation datasets to the community (https://github.com/AntoineGourru/DNEmbedding)."}}
{"id": "UEbXZRy-BDW", "cdate": 1664952326150, "mdate": 1664952326150, "content": {"title": "Document network projection in pretrained word embedding space", "abstract": "We present Regularized Linear Embedding (RLE), a novel method that projects a collection of linked documents (e.g., citation network) into a pretrained word embedding space. In addition to the textual content, we leverage a matrix of pairwise similarities providing complementary information (e.g., the network proximity of two documents in a citation graph). We first build a simple word vector average for each document, and we use the similarities to alter this average representation. The document representations can help to solve many information retrieval tasks, such as recommendation, classification and clustering. We demonstrate that our approach outperforms or matches existing document network embedding methods on node classification and link prediction tasks. Furthermore, we show that it helps identifying relevant keywords to describe document classes."}}
{"id": "ecsoxXEbxL", "cdate": 1648727877908, "mdate": 1648727877908, "content": {"title": "Dynamic Gaussian Embedding of Authors", "abstract": "Authors publish documents in a dynamic manner. Their topic of interest and writing style might shift over time. Tasks such as author classification, author identification or link prediction are difficult to solve in such complex data settings. We propose a new represen- tation learning model, DGEA (for Dynamic Gaussian Embedding of Authors), that is more suited to solve these tasks by capturing this temporal evolution. We formulate a general embedding frame- work: author representation at time \ud835\udc61 is a Gaussian distribution that leverages pre-trained document vectors, and that depends on the publications observed until \ud835\udc61 . The representations should retain some form of multi-topic information and temporal smoothness. We propose two models that fit into this framework. The first one, K-DGEA, uses a first order Markov model optimized with an Expectation Maximization Algorithm with Kalman Equations. The second, R-DGEA, makes use of a Recurrent Neural Network to model the time dependence. We evaluate our method on several quantitative tasks: author identification, classification, and co-authorship prediction, on two datasets written in English. In addition, our model is language agnostic since it only requires pre-trained document embeddings. It outperforms existing baselines by up to 18% on an author classification task on a news articles dataset."}}
{"id": "bSul4nRaew-", "cdate": 1640995200000, "mdate": 1682423813840, "content": {"title": "Dynamic Gaussian Embedding of Authors", "abstract": "Authors publish documents in a dynamic manner. Their topic of interest and writing style might shift over time. Tasks such as author classification, author identification or link prediction are difficult to solve in such complex data settings. We propose a new representation learning model, DGEA (for Dynamic Gaussian Embedding of Authors), that is more suited to solve these tasks by capturing this temporal evolution. We formulate a general embedding framework: author representation at time t is a Gaussian distribution that leverages pre-trained document vectors, and that depends on the publications observed until t. The representations should retain some form of multi-topic information and temporal smoothness. We propose two models that fit into this framework. The first one, K-DGEA, uses a first order Markov model optimized with an Expectation Maximization Algorithm with Kalman Equations. The second, R-DGEA, makes use of a Recurrent Neural Network to model the time dependence. We evaluate our method on several quantitative tasks: author identification, classification, and co-authorship prediction, on two datasets written in English. In addition, our model is language agnostic since it only requires pre-trained document embeddings. It outperforms existing baselines by up to 18% on an author classification task on a news articles dataset."}}
{"id": "EGLp8PXwnIL", "cdate": 1640995200000, "mdate": 1682423813875, "content": {"title": "Explainable Clustering via Exemplars: Complexity and Efficient Approximation Algorithms", "abstract": "Explainable AI (XAI) is an important developing area but remains relatively understudied for clustering. We propose an explainable-by-design clustering approach that not only finds clusters but also exemplars to explain each cluster. The use of exemplars for understanding is supported by the exemplar-based school of concept definition in psychology. We show that finding a small set of exemplars to explain even a single cluster is computationally intractable; hence, the overall problem is challenging. We develop an approximation algorithm that provides provable performance guarantees with respect to clustering quality as well as the number of exemplars used. This basic algorithm explains all the instances in every cluster whilst another approximation algorithm uses a bounded number of exemplars to allow simpler explanations and provably covers a large fraction of all the instances. Experimental results show that our work is useful in domains involving difficult to understand deep embeddings of images and text."}}
{"id": "2nhfCMPoiY", "cdate": 1640995200000, "mdate": 1682423813837, "content": {"title": "QAnswer: Towards Question Answering Search over Websites", "abstract": "Question Answering (QA) is increasingly used by search engines to provide results to their end-users, yet very few websites currently use QA technologies for their search functionality. To illustrate the potential of QA technologies for the website search practitioner, we demonstrate web searches that combine QA over knowledge graphs and QA over free text \u2013 each being usually tackled separately. We also discuss the different benefits and drawbacks of both approaches for web site searches. We use the case studies made of websites hosted by the Wikimedia Foundation (namely Wikipedia and Wikidata). Differently from a search engine (e.g. Google, Bing, etc), the data are indexed integrally, i.e. we do not index only a subset, and they are indexed exclusively, i.e. we index only data available on the corresponding website."}}
{"id": "I0mYg0-U2kn", "cdate": 1609459200000, "mdate": 1682423813846, "content": {"title": "Apprentissage de repr\u00e9sentations d'auteurs et de documents: approches probabilistes \u00e0 partir de repr\u00e9sentations pr\u00e9-entra\u00een\u00e9es. (Representation Learning for authors and documents: probabilistic approaches using pretrained representations)", "abstract": "La r\u00e9volution num\u00e9rique a entra\u00een\u00e9 une croissance exponentielle de la quantit\u00e9 d'informations stock\u00e9es \u00e0 long terme. Une part importante de cette information est textuelle (pages Web, m\u00e9dias sociaux, etc.). Les mod\u00e8les de traitement du langage naturel (NLP), qui permettent de classer ou de regrouper cette information, ont besoin que le texte soit repr\u00e9sent\u00e9 sous forme d'objets math\u00e9matiques : on parle alors d'apprentissage de repr\u00e9sentations. L'objectif de l'apprentissage de repr\u00e9sentations est de construire des repr\u00e9sentations d'objets textuels (mots, documents, auteurs) dans un espace vectoriel de faible dimension. La similarit\u00e9 entre les repr\u00e9sentations vectorielles de ces objets devrait \u00eatre li\u00e9e \u00e0 leur proximit\u00e9 s\u00e9mantique ou \u00e0 leur similarit\u00e9 stylistique. En plus du texte lui-m\u00eame, les documents sont souvent associ\u00e9s \u00e0 des m\u00e9tadonn\u00e9es. Ils peuvent \u00eatre li\u00e9s (par exemple, par des r\u00e9f\u00e9rences hypertextes), associ\u00e9s \u00e0 leurs auteurs, et horodat\u00e9s. Il a \u00e9t\u00e9 d\u00e9montr\u00e9 que ces informations am\u00e9liorent la qualit\u00e9 de la repr\u00e9sentation d'un document. N\u00e9anmoins, l'incorporation de ces m\u00e9tadonn\u00e9es n'est pas triviale. De plus, le langage naturel a rapidement \u00e9volu\u00e9 au cours des derni\u00e8res d\u00e9cennies. Les mod\u00e8les de repr\u00e9sentation sont maintenant entra\u00een\u00e9s sur des quantit\u00e9s massives de donn\u00e9es textuelles et affin\u00e9s pour des t\u00e2ches sp\u00e9cifiques. Ces mod\u00e8les sont d'un grand int\u00e9r\u00eat lorsqu'on travaille avec de petits ensembles de donn\u00e9es, permettant de transf\u00e9rer des connaissances \u00e0 partir de sources d'information pertinentes. Il est crucial de d\u00e9velopper des mod\u00e8les d'apprentissage de repr\u00e9sentations qui peuvent incorporer ces repr\u00e9sentations pr\u00e9-entra\u00een\u00e9es. La plupart des travaux ant\u00e9rieurs apprennent une repr\u00e9sentation ponctuelle. C'est une limitation s\u00e9rieuse car la langue est plus complexe que cela : les mots sont souvent polys\u00e9miques, et les documents sont, la plupart du temps, sur plusieurs sujets. Une branche de la litt\u00e9rature propose d'apprendre des distributions probabilistes dans un espace s\u00e9mantique pour contourner ce probl\u00e8me. Dans cette th\u00e8se, nous pr\u00e9sentons tout d'abord la th\u00e9orie de l'apprentissage automatique, ainsi qu'un aper\u00e7u g\u00e9n\u00e9ral des travaux existants en apprentissage de repr\u00e9sentations de mots et de documents (sans m\u00e9tadonn\u00e9es). Nous nous concentrons ensuite sur l'apprentissage de repr\u00e9sentations de documents li\u00e9s. Nous pr\u00e9sentons les travaux ant\u00e9rieurs du domaine et proposons deux contributions : le mod\u00e8le RLE (Regularized Linear Embedding), et le mod\u00e8le GELD (Gaussian Embedding of Linked Documents). Ensuite, nous explorons l'apprentissage des repr\u00e9sentations d'auteurs et de documents dans le m\u00eame espace vectoriel. Nous pr\u00e9sentons les travaux les plus r\u00e9cents et notre contribution VADE (Variational Authors and Documents Embedding). Enfin, nous \u00e9tudions la probl\u00e9matique de l'apprentissage de repr\u00e9sentations dynamiques d'auteurs : leurs repr\u00e9sentations doivent \u00e9voluer dans le temps. Nous pr\u00e9sentons d'abord les mod\u00e8les existants, puis nous proposons une contribution originale, DGEA (Dynamic Gaussian Embedding of Authors). De plus, nous proposons plusieurs axes scientifiques pour am\u00e9liorer nos contributions, et quelques questions ouvertes pour de futures recherches."}}
{"id": "HCyWBj0Ytfi", "cdate": 1609459200000, "mdate": 1682423813835, "content": {"title": "Apprentissage Conjoint de Repr\u00e9sentations d'Auteurs et de Documents", "abstract": ""}}
