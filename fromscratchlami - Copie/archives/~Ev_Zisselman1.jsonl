{"id": "-1aznZBMj75", "cdate": 1672531200000, "mdate": 1695994416429, "content": {"title": "Explore to Generalize in Zero-Shot RL", "abstract": "We study zero-shot generalization in reinforcement learning - optimizing a policy on a set of training tasks such that it will perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that $\\textit{explores}$ the domain effectively is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariance-based approaches. Our $\\textit{Explore to Generalize}$ algorithm (ExpGen) builds on this insight: We train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions, which are guaranteed to generalize and drive us to a novel part of the state space, where the ensemble may potentially agree again. We show that our approach is the state-of-the-art on several tasks in the ProcGen challenge that have so far eluded effective generalization. For example, we demonstrate a success rate of $82\\%$ on the Maze task and $74\\%$ on Heist with $200$ training levels."}}
{"id": "PulUJw-VV_U", "cdate": 1640995200000, "mdate": 1681631956127, "content": {"title": "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability", "abstract": ""}}
{"id": "exKoCmcIqfD", "cdate": 1609459200000, "mdate": 1681631956219, "content": {"title": "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability", "abstract": ""}}
{"id": "MMfBkv8vK9-", "cdate": 1577836800000, "mdate": 1681631955796, "content": {"title": "Deep Residual Flow for Out of Distribution Detection", "abstract": ""}}
{"id": "7-Btjfiln3P", "cdate": 1577836800000, "mdate": 1681631956157, "content": {"title": "Deep Residual Flow for Novelty Detection", "abstract": ""}}
{"id": "rjlgwHpMgdTr", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Local Block Coordinate Descent Algorithm for the CSC Model.", "abstract": "The Convolutional Sparse Coding (CSC) model has recently gained considerable traction in the signal and image processing communities. By providing a global, yet tractable, model that operates on the whole image, the CSC was shown to overcome several limitations of the patch-based sparse model while achieving superior performance in various applications. Contemporary methods for pursuit and learning the CSC dictionary often rely on the Alternating Direction Method of Multipliers (ADMM) in the Fourier domain for the computational convenience of convolutions, while ignoring the local characterizations of the image. In this work we propose a new and simple approach that adopts a localized strategy, based on the Block Coordinate Descent algorithm. The proposed method, termed Local Block Coordinate Descent (LoBCoD), operates locally on image patches. Furthermore, we introduce a novel stochastic gradient descent version of LoBCoD for training the convolutional filters. This Stochastic-LoBCoD leverages the benefits of online learning, while being applicable even to a single training image. We demonstrate the advantages of the proposed algorithms for image inpainting and multi-focus image fusion, achieving state-of-the-art results."}}
{"id": "Rd5ISBQbwsg", "cdate": 1546300800000, "mdate": 1683888177396, "content": {"title": "A Local Block Coordinate Descent Algorithm for the CSC Model", "abstract": "The Convolutional Sparse Coding (CSC) model has recently gained considerable traction in the signal and image processing communities. By providing a global, yet tractable, model that operates on the whole image, the CSC was shown to overcome several limitations of the patch-based sparse model while achieving superior performance in various applications. Contemporary methods for pursuit and learning the CSC dictionary often rely on the Alternating Direction Method of Multipliers (ADMM) in the Fourier domain for the computational convenience of convolutions, while ignoring the local characterizations of the image. In this work we propose a new and simple approach that adopts a localized strategy, based on the Block Coordinate Descent algorithm. The proposed method, termed Local Block Coordinate Descent (LoBCoD), operates locally on image patches. Furthermore, we introduce a novel stochastic gradient descent version of LoBCoD for training the convolutional filters. This Stochastic-LoBCoD leverages the benefits of online learning, while being applicable even to a single training image. We demonstrate the advantages of the proposed algorithms for image inpainting and multi-focus image fusion, achieving state-of-the-art results."}}
{"id": "lgjcTaRIVGY", "cdate": 1514764800000, "mdate": 1683888178020, "content": {"title": "A Local Block Coordinate Descent Algorithm for the Convolutional Sparse Coding Model", "abstract": "The Convolutional Sparse Coding (CSC) model has recently gained considerable traction in the signal and image processing communities. By providing a global, yet tractable, model that operates on the whole image, the CSC was shown to overcome several limitations of the patch-based sparse model while achieving superior performance in various applications. Contemporary methods for pursuit and learning the CSC dictionary often rely on the Alternating Direction Method of Multipliers (ADMM) in the Fourier domain for the computational convenience of convolutions, while ignoring the local characterizations of the image. A recent work by Papyan et al. suggested the SBDL algorithm for the CSC, while operating locally on image patches. SBDL demonstrates better performance compared to the Fourier-based methods, albeit still relying on the ADMM. In this work we maintain the localized strategy of the SBDL, while proposing a new and much simpler approach based on the Block Coordinate Descent algorithm - this method is termed Local Block Coordinate Descent (LoBCoD). Furthermore, we introduce a novel stochastic gradient descent version of LoBCoD for training the convolutional filters. The Stochastic-LoBCoD leverages the benefits of online learning, while being applicable to a single training image. We demonstrate the advantages of the proposed algorithms for image inpainting and multi-focus image fusion, achieving state-of-the-art results."}}
