{"id": "VR6g22FQeNF", "cdate": 1667354643904, "mdate": 1667354643904, "content": {"title": "Unsupervised Pre-training for Temporal Action Localization Tasks", "abstract": "Unsupervised video representation learning has made remarkable achievements in recent years. However, most existing methods are designed and optimized for video classification. These pre-trained models can be sub-optimal for temporal localization tasks due to the inherent discrepancy between video-level classification and clip-level localization. To bridge this gap, we make the first attempt to propose a self-supervised pretext task, coined as Pseudo Action Localization (PAL) to Unsupervisedly Pre-train feature encoders for Temporal Action Localization tasks (UP-TAL). Specifically, we first randomly select temporal regions, each of which contains multiple clips, from one video as pseudo actions and then paste them onto different temporal positions of the other two videos. The pretext task is to align the features of pasted pseudo action regions from two synthetic videos and maximize the agreement between them. Compared to the existing unsupervised video representation learning approaches, our PAL adapts better to downstream TAL tasks by introducing a temporal equivariant contrastive learning paradigm in a temporally dense and scale-aware manner. Extensive experiments show that PAL can utilize large-scale unlabeled video data to significantly boost the performance of existing TAL methods. Our codes and models will be made publicly available at https://github. com/zhang-can/UP-TAL."}}
{"id": "Es20ASuPE_S", "cdate": 1667354498550, "mdate": 1667354498550, "content": {"title": "Temporal Distinct Representation Learning for Action Recognition", "abstract": "Motivated by the previous success of Two-Dimensional Convolutional Neural Network (2D CNN) on image recognition, researchers endeavor to leverage it to characterize videos. However, one limitation of applying 2D CNN to analyze videos is that different frames of a video share the same 2D CNN kernels, which may result in repeated and redundant information utilization, especially in the spatial semantics extraction process, hence neglecting the critical variations among frames. In this paper, we attempt to tackle this issue through two ways. 1) Design a sequential channel filtering mechanism, i.e., Progressive Enhancement Module (PEM), to excite the discriminative channels of features from different frames step by step, and thus avoid repeated information extraction. 2) Create a Temporal Diversity Loss (TD Loss) to force the kernels to concentrate on and capture the variations among frames rather than the image regions with similar appearance. Our method is evaluated on benchmark temporal reasoning datasets Something-Something V1 and V2, and it achieves visible improvements over the best competitor by 2.4% and 1.3%, respectively. Besides, performance improvements over the 2D-CNN-based state-of-the-arts on the large-scale dataset Kinetics are also witnessed."}}
{"id": "sL8mQ4L_5L", "cdate": 1663849848825, "mdate": null, "content": {"title": "FlexPose: Pose Distribution Adaptation with Few-shot Guidance", "abstract": "Annotating human pose images can be costly. Meanwhile, there is an unavoidable major performance drop when a pre-trained pose estimation model is evaluated on a new dataset. We observe that pose distributions from different datasets share similar pose priors with different geometric transformations, which inspires us to learn a pose generator that can flexibly be adapted to generate the pose of a new pose distribution. In this paper, we treat human poses as skeleton images and propose a scheme to transfer a pre-trained pose annotation generator to generate poses from the transferred distribution of a newly collected dataset with only a few annotation guidances. By finetuning a limited number of linear layers, the transferred generator is able to generate similar pose annotations to the target pose distribution. We evaluate our FlexPose on several cross-dataset settings qualitatively and quantitatively. FlexPose surprisingly achieves around 41.8$\\%$ performance improvement on the Unsupervised Pose Estimation task when it transfers the pose distribution of COCO, 3DHP and Surreal dataset to that of the H36M dataset."}}
{"id": "Skbhz5-dWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deformable Pose Traversal Convolution for 3D Action and Gesture Recognition", "abstract": "The representation of 3D pose plays a critical role for 3D action and gesture recognition. Rather than representing a 3D pose directly by its joint locations, in this paper, we propose a Deformable Pose Traversal Convolution Network that applies one-dimensional convolution to traverse the 3D pose for its representation. Instead of fixing the receptive field when performing traversal convolution, it optimizes the convolution kernel for each joint, by considering contextual joints with various weights. This deformable convolution better utilizes the contextual joints for action and gesture recognition and is more robust to noisy joints. Moreover, by feeding the learned pose feature to a LSTM, we perform end-to-end training that jointly optimizes 3D pose representation and temporal sequence recognition. Experiments on three benchmark datasets validate the competitive performance of our proposed method, as well as its efficiency and robustness to handle noisy joints of pose."}}
{"id": "B1-AXyfdWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Hand PointNet: 3D Hand Pose Estimation Using Point Sets", "abstract": "Convolutional Neural Network (CNN) has shown promising results for 3D hand pose estimation in depth images. Different from existing CNN-based hand pose estimation methods that take either 2D images or 3D volumes as the input, our proposed Hand PointNet directly processes the 3D point cloud that models the visible surface of the hand for pose regression. Taking the normalized point cloud as the input, our proposed hand pose regression network is able to capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose. In order to further improve the accuracy of fingertips, we design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location. Experiments on three challenging hand pose datasets show that our proposed method outperforms state-of-the-art methods."}}
{"id": "HJ4V8TWdZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Spatio-Temporal Naive-Bayes Nearest-Neighbor (ST-NBNN) for Skeleton-Based Action Recognition", "abstract": "Motivated by previous success of using non-parametric methods to recognize objects, e.g., NBNN [2], we extend it to recognize actions using skeletons. Each 3D action is presented by a sequence of 3D poses. Similar to NBNN, our proposed Spatio-Temporal-NBNN applies stage-to-class distance to classify actions. However, ST-NBNN takes the spatio-temporal structure of 3D actions into consideration and relaxes the Naive Bayes assumption of NBNN. Specifically, ST-NBNN adopts bilinear classifiers [19] to identify both key temporal stages as well as spatial joints for action classification. Although only using a linear classifier, experiments on three benchmark datasets show that by combining the strength of both non-parametric and parametric models, ST-NBNN can achieve competitive performance compared with state-of-the-art results using sophisticated models such as deep learning. Moreover, by identifying key skeleton joints and temporal stages for each action class, our ST-NBNN can capture the essential spatio-temporal patterns that play key roles of recognizing actions, which is not always achievable by using end-to-end models."}}
