{"id": "3mlITJRYYbs", "cdate": 1663850342535, "mdate": null, "content": {"title": "Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation", "abstract": "Early sensory systems in the brain rapidly adapt to fluctuating input statistics, which requires recurrent communication between neurons. Mechanistically, such recurrent communication is often indirect and mediated by local interneurons. In this work, we explore the computational benefits of mediating recurrent communication via interneurons compared with direct recurrent connections. To this end, we consider two mathematically tractable recurrent neural networks that statistically whiten their inputs --- one with direct recurrent connections and the other with interneurons that mediate recurrent communication. By analyzing the corresponding continuous synaptic dynamics and numerically simulating the networks, we show that the network with interneurons is more robust to initialization than the network with direct recurrent connections in the sense that the convergence time for the synaptic dynamics in the network with interneurons (resp. direct recurrent connections) scales logarithmically (resp. linearly) with the spectrum of their initialization. Our results suggest that interneurons are computationally useful for rapid adaptation to changing input statistics. Interestingly, the network with interneurons is an overparameterized solution of the whitening objective for the network with direct recurrent connections, so our results can be viewed as a recurrent neural network analogue of the implicit acceleration phenomenon observed in overparameterized feedforward linear networks."}}
{"id": "48TmED6BvGZ", "cdate": 1652737426354, "mdate": null, "content": {"title": "Biological Learning of Irreducible Representations of Commuting Transformations", "abstract": "A longstanding challenge in neuroscience is to understand neural mechanisms underlying the brain\u2019s remarkable ability to learn and detect transformations of objects due to motion. Translations and rotations of images can be viewed as orthogonal transformations in the space of pixel intensity vectors. Every orthogonal transformation can be decomposed into rotations within irreducible two-dimensional subspaces (or representations). For sets of commuting transformations, known as toroidal groups, Cohen and Welling proposed a mathematical framework for learning the irreducible representations. We explore the possibility that the brain also learns irreducible representations using a biologically plausible learning mechanism. The first is based on SVD of the anti-symmetrized outer product of the vectors representing consecutive images and is implemented by a single-layer neural network. The second is based on PCA of the difference between consecutive frames and is implemented in a two-layer network but with greater biological plausibility. Both networks learn image rotations (replicating Cohen and Welling\u2019s results) as well as  translations. It would be interesting to search for the proposed networks in nascent connectomics and physiology datasets."}}
{"id": "WjsNN3wyNKR", "cdate": 1609459200000, "mdate": null, "content": {"title": "Heavy Traffic Limits for Join-the-Shortest-Estimated-Queue Policy Using Delayed Information", "abstract": "We consider a load-balancing problem for a network of parallel queues in which information on the state of the queues is subject to a delay. In this setting, adopting a routing policy that performs..."}}
{"id": "I0AIMonKyTo", "cdate": 1609459200000, "mdate": 1652450002800, "content": {"title": "A Biologically Plausible Neural Network for Multichannel Canonical Correlation Analysis", "abstract": "Cortical pyramidal neurons receive inputs from multiple distinct neural populations and integrate these inputs in separate dendritic compartments. We explore the possibility that cortical microcircuits implement canonical correlation analysis (CCA), an unsupervised learning method that projects the inputs onto a common subspace so as to maximize the correlations between the projections. To this end, we seek a multichannel CCA algorithm that can be implemented in a biologically plausible neural network. For biological plausibility, we require that the network operates in the online setting and its synaptic update rules are local. Starting from a novel CCA objective function, we derive an online optimization algorithm whose optimization steps can be implemented in a single-layer neural network with multicompartmental neurons and local non-Hebbian learning rules. We also derive an extension of our online CCA algorithm with adaptive output rank and output whitening. Interestingly, the extension maps onto a neural network whose neural architecture and synaptic updates resemble neural circuitry and non-Hebbian plasticity observed in the cortex."}}
{"id": "AExDWtmIJDe", "cdate": 1609459200000, "mdate": 1652450002775, "content": {"title": "Sensitivity Analysis for the Stationary Distribution of Reflected Brownian Motion in a Convex Polyhedral Cone", "abstract": "Reflected Brownian motion (RBM) in a convex polyhedral cone arises in a variety of applications ranging from the theory of stochastic networks to mathematical finance, and under general stability c..."}}
{"id": "_QD_3RskkuZ", "cdate": 1577836800000, "mdate": null, "content": {"title": "A simple normative network approximates local non-Hebbian learning in the cortex", "abstract": "To guide behavior, the brain extracts relevant features from high-dimensional data streamed by sensory organs. Neuroscience experiments demonstrate that the processing of sensory inputs by cortical neurons is modulated by instructive signals which provide context and task-relevant information. Here, adopting a normative approach, we model these instructive signals as supervisory inputs guiding the projection of the feedforward data. Mathematically, we start with a family of Reduced-Rank Regression (RRR) objective functions which include Reduced Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation Analysis (CCA), and derive novel offline and online optimization algorithms, which we call Bio-RRR. The online algorithms can be implemented by neural networks whose synaptic learning rules resemble calcium plateau potential dependent plasticity observed in the cortex. We detail how, in our model, the calcium plateau potential can be interpreted as a backpropagating error signal. We demonstrate that, despite relying exclusively on biologically plausible local learning rules, our algorithms perform competitively with existing implementations of RRMSE and CCA."}}
{"id": "PHIPOmwI7RU", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Biologically Plausible Neural Network for Slow Feature Analysis", "abstract": "Learning latent features from time series data is an important problem in both machine learning and brain function. One approach, called Slow Feature Analysis (SFA), leverages the slowness of many salient features relative to the rapidly varying input signals. Furthermore, when trained on naturalistic stimuli, SFA reproduces interesting properties of cells in the primary visual cortex and hippocampus, suggesting that the brain uses temporal slowness as a computational principle for learning latent features. However, despite the potential relevance of SFA for modeling brain function, there is currently no SFA algorithm with a biologically plausible neural network implementation, by which we mean an algorithm operates in the online setting and can be mapped onto a neural network with local synaptic updates. In this work, starting from an SFA objective, we derive an SFA algorithm, called Bio-SFA, with a biologically plausible neural network implementation. We validate Bio-SFA on naturalistic stimuli."}}
{"id": "DByhy5K-d0P", "cdate": 1577836800000, "mdate": 1652450002801, "content": {"title": "A simple normative network approximates local non-Hebbian learning in the cortex", "abstract": "To guide behavior, the brain extracts relevant features from high-dimensional data streamed by sensory organs. Neuroscience experiments demonstrate that the processing of sensory inputs by cortical neurons is modulated by instructive signals which provide context and task-relevant information. Here, adopting a normative approach, we model these instructive signals as supervisory inputs guiding the projection of the feedforward data. Mathematically, we start with a family of Reduced-Rank Regression (RRR) objective functions which include Reduced Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation Analysis (CCA), and derive novel offline and online optimization algorithms, which we call Bio-RRR. The online algorithms can be implemented by neural networks whose synaptic learning rules resemble calcium plateau potential dependent plasticity observed in the cortex. We detail how, in our model, the calcium plateau potential can be interpreted as a backpropagating error signal. We demonstrate that, despite relying exclusively on biologically plausible local learning rules, our algorithms perform competitively with existing implementations of RRMSE and CCA."}}
{"id": "3E1yLpnqzd", "cdate": 1420070400000, "mdate": null, "content": {"title": "Existence, Uniqueness, and Stability of Slowly Oscillating Periodic Solutions for Delay Differential Equations with Nonnegativity Constraints", "abstract": "Deterministic dynamical system models with delayed feedback and nonnegativity constraints arise in a variety of applications in science and engineering. Under certain conditions oscillatory behavior has been observed and it is of interest to know when this behavior is periodic. Here we consider one-dimensional delay differential equations with nonnegativity constraints as prototypes for such models. We obtain sufficient conditions for the existence of slowly oscillating periodic solutions (SOPS) of such equations when the delay/lag interval is long and the dynamics depend only on the current and delayed state. Under further assumptions, including possibly longer delay intervals and restricting the dynamics to depend only on the delayed state, we prove uniqueness and exponential stability for such solutions. To prove these results, we develop a theory for studying perturbations of these constrained SOPS. We illustrate our results with simple examples of biochemical reaction network models and an Internet rate control model."}}
