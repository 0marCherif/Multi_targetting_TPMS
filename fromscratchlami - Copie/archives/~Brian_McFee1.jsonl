{"id": "z54jwYtnXa", "cdate": 1672531200000, "mdate": 1700584483387, "content": {"title": "Foley Sound Synthesis at the DCASE 2023 Challenge", "abstract": "The addition of Foley sound effects during post-production is a common technique used to enhance the perceived acoustic properties of multimedia content. Traditionally, Foley sound has been produced by human Foley artists, which involves manual recording and mixing of sound. However, recent advances in sound synthesis and generative models have generated interest in machine-assisted or automatic Foley synthesis techniques. To promote further research in this area, we have organized a challenge in DCASE 2023: Task 7 - Foley Sound Synthesis. Our challenge aims to provide a standardized evaluation framework that is both rigorous and efficient, allowing for the evaluation of different Foley synthesis systems. We received 17 submissions, and performed both objective and subjective evaluation to rank them according to three criteria: audio quality, fit-to-category, and diversity. Through this challenge, we hope to encourage active participation from the research community and advance the state-of-the-art in automatic Foley synthesis. In this technical report, we provide a detailed overview of the Foley sound synthesis challenge, including task definition, dataset, baseline, evaluation scheme and criteria, challenge result, and discussion."}}
{"id": "vq_3Vq1I_o", "cdate": 1672531200000, "mdate": 1700584483392, "content": {"title": "Transfer Learning and Bias Correction with Pre-trained Audio Embeddings", "abstract": "Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applications in MIR have sufficient quantities of training data, it is becoming increasingly common to transfer models across domains. This approach allows representations derived for one task to be applied to another, and can result in high accuracy with less stringent training data requirements for the downstream task. However, the properties of pre-trained audio embeddings are not fully understood. Specifically, and unlike traditionally engineered features, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition. We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets."}}
{"id": "S05PpJaZ1WW", "cdate": 1672531200000, "mdate": 1700584483389, "content": {"title": "Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization", "abstract": "As deeper and more complex models are developed for the task of sound event localization and detection (SELD), the demand for annotated spatial audio data continues to increase. Annotating field recordings with 360$^{\\circ}$ video takes many hours from trained annotators, while recording events within motion-tracked laboratories are bounded by cost and expertise. Because of this, localization models rely on a relatively limited amount of spatial audio data in the form of spatial room impulse response (SRIR) datasets, which limits the progress of increasingly deep neural network based approaches. In this work, we demonstrate that simulated geometrical acoustics can provide an appealing solution to this problem. We use simulated geometrical acoustics to generate a novel SRIR dataset that can train a SELD model to provide similar performance to that of a real SRIR dataset. Furthermore, we demonstrate using simulated data to augment existing datasets, improving on benchmarks set by state of the art SELD models. We explore the potential and limitations of geometric acoustic simulation for localization and event detection. We also propose further studies to verify the limitations of this method, as well as further methods to generate synthetic data for SELD tasks without the need to record more data."}}
{"id": "g0XHhxWjIx", "cdate": 1640995200000, "mdate": 1695988832970, "content": {"title": "Music Recommendation Systems: Techniques, Use Cases, and Challenges", "abstract": "This chapter gives an introduction to music recommender systems, considering the unique characteristics of the music domain. We take a user-centric perspective, by organizing our discussion with respect to current use cases and challenges. More precisely, we categorize music recommendation tasks into three major types of use cases: basic music recommendation, lean-in exploration, and lean-back listening. Subsequently, we explain the main categories of music recommender systems from a technical perspective, including content-based filtering, sequential recommendation, and recent psychology-inspired approaches. To round off the chapter, we provide a discussion of challenges faced in music recommendation research and practice, and of approaches that address these challenges. Topics we address here include creating multi-faceted recommendation lists, considering intrinsic user characteristics, making fair recommendations, explaining recommendations, evaluation, dealing with missing and negative feedback, designing user interfaces, and providing open tools and data sources."}}
{"id": "WPuhQDsjGPx", "cdate": 1640995200000, "mdate": 1700584483361, "content": {"title": "A Proposal for Foley Sound Synthesis Challenge", "abstract": "Foley\" refers to sound effects that are added to multimedia during post-production to enhance its perceived acoustic properties, e.g., by simulating the sounds of footsteps, ambient environmental sounds, or visible objects on the screen. While foley is traditionally produced by foley artists, there is increasing interest in automatic or machine-assisted techniques building upon recent advances in sound synthesis and generative models. To foster more participation in this growing research area, we propose a challenge for automatic foley synthesis. Through case studies on successful previous challenges in audio and machine learning, we set the goals of the proposed challenge: rigorous, unified, and efficient evaluation of different foley synthesis systems, with an overarching goal of drawing active participation from the research community. We outline the details and design considerations of a foley sound synthesis challenge, including task definition, dataset requirements, and evaluation criteria."}}
{"id": "OBSMlnLq42", "cdate": 1640995200000, "mdate": 1700584483378, "content": {"title": "Learning Multi-Level Representations for Hierarchical Music Structure Analysis.", "abstract": ""}}
{"id": "w_YrLyc86L4", "cdate": 1609459200000, "mdate": null, "content": {"title": "Multi-Task Self-Supervised Pre-Training for Music Classification", "abstract": "Deep learning is very data hungry, and supervised learning especially requires massive labeled data to work well. Machine listening research often suffers from limited labeled data problem, as human annotations are costly to acquire, and annotations for audio are time consuming and less intuitive. Besides, models learned from labeled dataset often embed biases specific to that particular dataset. Therefore, unsupervised learning techniques become popular approaches in solving machine listening problems. Particularly, a self-supervised learning technique utilizing reconstructions of multiple hand-crafted audio features has shown promising results when it is applied to speech domain such as emotion recognition and automatic speech recognition (ASR). In this paper, we apply self-supervised and multi-task learning methods for pre-training music encoders, and explore various design choices including encoder architectures, weighting mechanisms to combine losses from multiple tasks, and worker selections of pretext tasks. We investigate how these design choices interact with various downstream music classification tasks. We find that using various music specific workers altogether with weighting mechanisms to balance the losses during pre-training helps improve and generalize to the downstream tasks."}}
{"id": "qhwOERf7dY", "cdate": 1609459200000, "mdate": null, "content": {"title": "Sound Event Detection in Urban Audio With Single and Multi-Rate PCEN", "abstract": "Recent literature has demonstrated that the use of per-channel energy normalization (PCEN), has significant performance improvements over traditional log-scaled mel-frequency spectrograms in acoustic sound event detection (SED) in a multi-class setting with overlapping events. However, the configuration of PCEN's parameters is sensitive to the recording environment, the characteristics of the class of events of interest, and the presence of multiple overlapping events. This leads to improvements on a class-by-class basis, but poor cross-class performance. In this article, we experiment using PCEN spectrograms as an alternative method for SED in urban audio using the UrbanSED dataset, demonstrating per-class improvements based on parameter configuration. Furthermore, we address cross-class performance with PCEN using a novel method, Multi-Rate PCEN (MRPCEN). We demonstrate cross-class SED performance with MRPCEN, demonstrating improvements to cross-class performance compared to traditional single-rate PCEN."}}
{"id": "Sdr9tdtoci", "cdate": 1609459200000, "mdate": 1700584483391, "content": {"title": "Automatic Hierarchy Expansion for Improved Structure and Chord Evaluation", "abstract": "Partitioning a recording into non-overlapping time intervals comes in many forms. There is the structural segmentation task which labels structures either syntactically as A and B, or structurally as verse or chorus. The chord annotation task is similar, labeling segments by their chords. While many of these annotations are flat, this article extends the method by McFee and Kinnaird (2019) for automatically enhancing structural annotations by inferring (and expanding) hierarchical information from the segment labels. One of our extensions adds new rules that allow for structural labels with a wider vocabulary than the syntactical ones in the SALAMI dataset. Using this first extension, we compare annotations from the Beatles-TUT and Isophonics datasets to investigate similarities between these annotations. Our second extension creates a multi-level annotation for chords that addresses a number of current challenges in chord evaluation. Using a large collection of chord annotations (manually and automatically generated), we investigate how and where the multi-level hierarchies can enhance (or detract from) comparing chord annotations."}}
{"id": "RQ6r1eYYvp", "cdate": 1609459200000, "mdate": 1700584483459, "content": {"title": "Diarization of Legal Proceedings. Identifying and Transcribing Judicial Speech from Recorded Court Audio", "abstract": "United States Courts make audio recordings of oral arguments available as public record, but these recordings rarely include speaker annotations. This paper addresses the Speech Audio Diarization problem, answering the question of \"Who spoke when?\" in the domain of judicial oral argument proceedings. We present a workflow for diarizing the speech of judges using audio recordings of oral arguments, a process we call Reference-Dependent Speaker Verification. We utilize a speech embedding network trained with the Generalized End-to-End Loss to encode speech into d-vectors and a pre-defined reference audio library based on annotated data. We find that by encoding reference audio for speakers and full arguments and computing similarity scores we achieve a 13.8% Diarization Error Rate for speakers covered by the reference audio library on a held-out test set. We evaluate our method on the Supreme Court of the United States oral arguments, accessed through the Oyez Project, and outline future work for diarizing legal proceedings. A code repository for this research is available at github.com/JeffT13/rd-diarization"}}
