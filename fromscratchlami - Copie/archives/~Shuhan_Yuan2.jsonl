{"id": "MUDOOpVuwG6", "cdate": 1672531200000, "mdate": 1681658591169, "content": {"title": "Achieving Counterfactual Fairness for Anomaly Detection", "abstract": "Ensuring fairness in anomaly detection models has received much attention recently as many anomaly detection applications involve human beings. However, existing fair anomaly detection approaches mainly focus on association-based fairness notions. In this work, we target counterfactual fairness, which is a prevalent causation-based fairness notion. The goal of counterfactually fair anomaly detection is to ensure that the detection outcome of an individual in the factual world is the same as that in the counterfactual world where the individual had belonged to a different group. To this end, we propose a counterfactually fair anomaly detection (CFAD) framework which consists of two phases, counterfactual data generation and fair anomaly detection. Experimental results on a synthetic dataset and two real datasets show that CFAD can effectively detect anomalies as well as ensure counterfactual fairness."}}
{"id": "yBDN3pYa2O", "cdate": 1640995200000, "mdate": 1681658591205, "content": {"title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations", "abstract": "Anomaly detection in sequential data has been studied for a long time because of its potential in various applications, such as detecting abnormal system behaviors from log data. Although many approaches can achieve good performance on anomalous sequence detection, how to identify the anomalous entries in sequences is still challenging due to a lack of information at the entry-level. In this work, we propose a novel framework called CFDet for fine-grained anomalous entry detection. CFDet leverages the idea of interpretable machine learning. Given a sequence that is detected as anomalous, we can consider anomalous entry detection as an interpretable machine learning task because identifying anomalous entries in the sequence is to provide an interpretation to the detection result. We make use of the deep support vector data description (Deep SVDD) approach to detect anomalous sequences and propose a novel counterfactual interpretation-based approach to identify anomalous entries in the sequences. Experimental results on three datasets show that CFDet can correctly detect anomalous entries."}}
{"id": "uiyEsJmdYTX", "cdate": 1640995200000, "mdate": 1681658591175, "content": {"title": "Contrastive Learning for Insider Threat Detection", "abstract": "Insider threat detection techniques typically employ supervised learning models for detecting malicious insiders by using insider activity audit data. In many situations, the number of detected malicious insiders is extremely limited. To address this issue, we present a contrastive learning-based insider threat detection framework, CLDet, and empirically evaluate its efficacy in detecting malicious sessions that contain malicious activities from insiders. We evaluate our framework along with state-of-the-art baselines on two unbalanced benchmark datasets. Our framework exhibits relatively superior performance on these unbalanced datasets in effectively detecting malicious sessions."}}
{"id": "gDgCqk0pV9i", "cdate": 1640995200000, "mdate": 1681658591203, "content": {"title": "On Interpretable Anomaly Detection Using Causal Algorithmic Recourse", "abstract": "As many deep anomaly detection models have been deployed in the real-world, interpretable anomaly detection becomes an emerging task. Recent studies focus on identifying features of samples leading to abnormal outcomes but cannot recommend a set of actions to flip the abnormal outcomes. In this work, we focus on interpretations via algorithmic recourse that shows how to act to revert abnormal predictions by suggesting actions on features. The key challenge is that algorithmic recourse involves interventions in the physical world, which is fundamentally a causal problem. To tackle this challenge, we propose an interpretable Anomaly Detection framework using Causal Algorithmic Recourse (ADCAR), which recommends recourse actions and infers counterfactual of abnormal samples guided by the causal mechanism. Experiments on three datasets show that ADCAR can flip the abnormal labels with minimal interventions."}}
{"id": "dhDtWrJrGce", "cdate": 1640995200000, "mdate": 1681658591199, "content": {"title": "Few-shot Anomaly Detection and Classification Through Reinforced Data Selection", "abstract": "Due to the scarcity of anomalies, deep anomaly detection models are predominately trained in an unsupervised or semi-supervised manner depending on the availability of a small number of labeled samples. Currently, most unsupervised approaches detect anomalies by identifying the deviate patterns, and some semi-supervised studies also use labeled anomalies to improve performance. However, few studies have focused on how to take advantage of potential anomalies in an easily obtained and large-scale unlabeled dataset. Meanwhile, in a semi-supervised setting, although we assume having a small number of labeled anomalies, the task of anomaly classification is under-exploited. In this work, considering the problem of anomaly detection and classification by giving limited labeled samples as well as a large number of unlabeled samples, we propose a few-shot anomaly detection and classification model through reinforced data selection (FADS), a novel framework that iteratively improves the performance of anomaly detection and classification by exploring the unlabeled dataset to augment the training set. Experimental results show that FADS is able to improve the performance of anomaly detection and classification with only a few labeled samples initially."}}
{"id": "Z5MiB-Rf-N", "cdate": 1640995200000, "mdate": 1681658591203, "content": {"title": "Generating Textual Adversaries with Minimal Perturbation", "abstract": "Many word-level adversarial attack approaches for textual data have been proposed in recent studies. However, due to the massive search space consisting of combinations of candidate words, the existing approaches face the problem of preserving the semantics of texts when crafting adversarial counterparts. In this paper, we develop a novel attack strategy to find adversarial texts with high similarity to the original texts while introducing minimal perturbation. The rationale is that we expect the adversarial texts with small perturbation can better preserve the semantic meaning of original texts. Experiments show that, compared with state-of-the-art attack approaches, our approach achieves higher success rates and lower perturbation rates in four benchmark datasets."}}
{"id": "WPkV9Xcni_X", "cdate": 1640995200000, "mdate": 1681658591171, "content": {"title": "Fraud Detection via Contrastive Positive Unlabeled Learning", "abstract": "Many online system fraud detection techniques employ deep learning models for identifying malicious user activity sessions. In many real-world scenarios, few labeled malicious and many unlabeled sessions exist. In such scenarios, the fraud detection problem can be effectively addressed through the Positive Unlabeled (PU) learning technique. Despite this fact, possible malicious sessions can be extremely diverse, which makes learning a good decision boundary challenging. In this paper, we present a novel contrastive positive unlabeled learning (ConPU) model for fraud detection and in particular, propose a contrastive loss function for PU learning. ConPU indirectly approximates the cluster center of normal sessions in the representation space by using distributions of unlabeled sessions and malicious sessions, predicts labels of sessions in the unlabeled set by analyzing their proximity between normal and malicious session cluster centers in the representation space, and then incorporates both positive pairs and negative pairs into the contrastive loss function. As a result, ConPU can derive separable representations as well as accurate cluster centers of normal and malicious sessions in the representation space. We theoretically demonstrate the efficacy o f o ur d eveloped l oss f unction i n C onPU. Additionally, we empirically evaluate ConPU on benchmark datasets, in which, ConPU demonstrates substantial performance improvement over state-of-the-art baselines."}}
{"id": "UkkKf8WWfT", "cdate": 1640995200000, "mdate": 1681658591201, "content": {"title": "Robust Hate Speech Detection via Mitigating Spurious Correlations", "abstract": "Kshitiz Tiwari, Shuhan Yuan, Lu Zhang. Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2022."}}
{"id": "SbfNccunxuS", "cdate": 1640995200000, "mdate": 1665002924169, "content": {"title": "On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning", "abstract": "A simple and natural algorithm for reinforcement learning (RL) is Monte Carlo Exploring Starts (MCES), where the Q-function is estimated by averaging the Monte Carlo returns, and the policy is improved by choosing actions that maximize the current estimate of the Q-function. Exploration is performed by \"exploring starts\", that is, each episode begins with a randomly chosen state and action, and then follows the current policy to the terminal state. In the classic book on RL by Sutton & Barto (2018), it is stated that establishing convergence for the MCES algorithm is one of the most important remaining open theoretical problems in RL. However, the convergence question for MCES turns out to be quite nuanced. Bertsekas & Tsitsiklis (1996) provide a counter-example showing that the MCES algorithm does not necessarily converge. Tsitsiklis (2002) further shows that if the original MCES algorithm is modified so that the Q-function estimates are updated at the same rate for all state-action pairs, and the discount factor is strictly less than one, then the MCES algorithm converges. In this paper we make headway with the original and more efficient MCES algorithm given in Sutton et al. (1998), establishing almost sure convergence for Optimal Policy Feed-Forward MDPs, which are MDPs whose states are not revisited within any episode when using an optimal policy. Such MDPs include a large class of environments such as all deterministic environments and all episodic environments with a timestep or any monotonically changing values as part of the state. Different from the previous proofs using stochastic approximations, we introduce a novel inductive approach, which is very simple and only makes use of the strong law of large numbers."}}
{"id": "QsCsdNAfye", "cdate": 1640995200000, "mdate": 1681658591203, "content": {"title": "Coded Hate Speech Detection via Contextual Information", "abstract": "Hate speech on online social media seriously affects the experience of common users. Many online social media platforms deploy automatic hate speech detection programs to filter out hateful content. To evade detection, coded words have been used to represent the targeted groups in hate speech. For example, on Twitter, \u201cGoogle\u201d is used to indicate African-Americans, and \u201cSkittles\u201d is used to indicate Muslim. As a result, it would be difficult to determine whether a hateful text including \u201cGoogle\u201d targets African-Americans or the search engine. In this paper, we develop a coded hate speech detection framework, called CODE, to detect hate speech by judging whether coded words like Google or Skittles are used in the coded meaning or not. Based on a proposed two-layer structure, CODE is able to detect the hateful texts with observed coded words as well as newly emerged coded words. Experimental results on a Twitter dataset show the effectiveness of our approach."}}
