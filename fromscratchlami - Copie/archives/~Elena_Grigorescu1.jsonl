{"id": "KzC7Pejhp3z", "cdate": 1652737766496, "mdate": null, "content": {"title": "Learning-Augmented Algorithms for Online Linear and Semidefinite Programming", "abstract": "Semidefinite programming (SDP) is a unifying framework that generalizes both linear programming and quadratically-constrained  quadratic programming, while also yielding efficient solvers, both in theory and in practice. However, there exist known impossibility results for approximating the optimal solution when constraints for covering SDPs arrive in an online fashion. In this paper, we study online covering linear and semidefinite programs in which the algorithm is augmented with advice from a possibly erroneous predictor. We show that if the predictor is accurate, we can efficiently bypass these impossibility results and achieve a constant-factor approximation to the optimal solution, i.e., consistency. On the other hand, if the predictor is inaccurate, under some technical conditions, we achieve results that match both the classical optimal upper bounds and the tight lower bounds up to constant factors, i.e., robustness. \n\nMore broadly, we introduce a framework that extends both (1) the online set cover problem augmented with machine-learning predictors, studied by Bamas, Maggiori, and Svensson (NeurIPS 2020), and (2) the online covering SDP problem, initiated by Elad, Kale, and Naor (ICALP 2016).  Specifically, we obtain general online learning-augmented algorithms for covering linear programs with fractional advice and constraints, and initiate the study of learning-augmented algorithms for covering SDP problems. \n\nOur techniques are based on the primal-dual framework of Buchbinder and Naor (Mathematics of Operations Research, 34, 2009) and can be further adjusted to handle constraints where the variables lie in a bounded region, i.e., box constraints. "}}
{"id": "7pIt78Ke0Y", "cdate": 1577836800000, "mdate": null, "content": {"title": "Periodicity in Data Streams with Wildcards", "abstract": "We investigate the problem of detecting periodic trends within a string S of length n, arriving in the streaming model, containing at most k wildcard characters, where k = o(n). A wildcard character is a special character that can be assigned any other character. We say that S has wildcard-period p if there exists an assignment to each of the wildcard characters so that in the resulting stream the prefix of length n \u2212 p equals the suffix of length n \u2212 p. We present a two-pass streaming algorithm that computes wildcard-periods of S using O(k3polylogn)$\\mathcal {O}(k^{3} \\text {polylog} n)$ bits of space, while we also show that this problem cannot be solved in sublinear space in one pass. We also give a one-pass randomized streaming algorithm that computes all wildcard-periods p of S with p<n2$p<\\frac {n}{2}$ and no wildcard characters appearing in the last p symbols of S, using O(k3log9n)$\\mathcal {O}(k^{3}\\log ^{9} n)$ space."}}
{"id": "wqO-bO0_kKC", "cdate": 1546300800000, "mdate": null, "content": {"title": "Flipping Out with Many Flips: Hardness of Testing k-Monotonicity", "abstract": "A function $f:\\{0,1\\}^n\\rightarrow \\{0,1\\}$ is said to be $k$-monotone if it flips between 0 and 1 at most $k$ times on every ascending chain. Such functions represent a natural generalization of (1-)monotone functions, and have been recently studied in circuit complexity, PAC learning, and cryptography. Our work is part of a renewed focus in understanding testability of properties characterized by freeness of arbitrary order patterns as a generalization of monotonicity. Recently, Canonne et al. [Innovations in Theoretical Computer Science, Schloss-Dagstuhl--Leibniz-Zentrum f\u00fcr Informatik GmBH, Wadern, Germany, 2017, 29] initiate the study of $k$-monotone functions in the area of property testing, and Newman et al. [SODA, SIAM, Philadelphia, 2017, pp. 1582--1597] study testability of families characterized by freeness from order patterns on real-valued functions over the line $[n]$ domain. We study $k$-monotone functions in the more relaxed parametrized property testing model, introduced by Parnas, Ron, and Rubinfeld [J. Comput. System Sci., 72 (2006), pp. 1012--1042]. In this process we show strong lower bounds on testing $k$-monotonicity. Specifically, we show that testing 2-monotonicity on the hypercube nonadaptively with one-sided error requires an exponential in $\\sqrt{n}$ number of queries. This behavior shows a stark contrast with testing (1-)monotonicity, which only needs $\\tilde{O}\\mleft(\\sqrt{n}\\mright)$ queries. Furthermore, even the apparently easier task of distinguishing 2-monotone functions from functions that are far from being $n^{.01}$-monotone also requires an exponential number of queries."}}
{"id": "QQlLy0ktFLQ", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Maximum Binary Tree Problem", "abstract": "We introduce and investigate the approximability of the maximum binary tree problem (MBT) in directed and undirected graphs. The goal in MBT is to find a maximum-sized binary tree in a given graph. MBT is a natural variant of the well-studied longest path problem, since both can be viewed as finding a maximum-sized tree of bounded degree in a given graph. The connection to longest path motivates the study of MBT in directed acyclic graphs (DAGs), since the longest path problem is solvable efficiently in DAGs. In contrast, we show that MBT in DAGs is in fact hard: it has no efficient $\\exp(-O(\\log n/ \\log \\log n))$-approximation algorithm under the exponential time hypothesis, where $n$ is the number of vertices in the input graph. In undirected graphs, we show that MBT has no efficient $\\exp(-O(\\log^{0.63}{n}))$-approximation under the exponential time hypothesis. Our inapproximability results rely on self-improving reductions and structural properties of binary trees. We also show constant-factor inapproximability assuming $\\text{P}\\neq \\text{NP}$. In addition to inapproximability results, we present algorithmic results along two different flavors: (1) We design a randomized algorithm to verify if a given directed graph on $n$ vertices contains a binary tree of size $k$ in $2^k \\text{poly}(n)$ time. (2) Motivated by the longest heapable subsequence problem, introduced by Byers, Heeringa, Mitzenmacher, and Zervas (ANALCO 2011), which is equivalent to MBT in permutation DAGs, we design efficient algorithms for MBT in bipartite permutation graphs."}}
{"id": "ID6ZS-OMmv", "cdate": 1546300800000, "mdate": null, "content": {"title": "Structural Results on Matching Estimation with Applications to Streaming", "abstract": "We study the problem of estimating the size of a matching when the graph is revealed in a streaming fashion. Our results are multifold: 1. We give a tight structural result relating the size of a maximum matching to the arboricity $$\\alpha $$ \u03b1 of a graph, which has been one of the most studied graph parameters for matching algorithms in data streams. One of the implications is an algorithm that estimates the matching size up to a factor of $$(\\alpha +2)(1+\\varepsilon )$$ ( \u03b1 + 2 ) ( 1 + \u03b5 ) using $$\\tilde{O}(\\alpha n^{2/3})$$ O ~ ( \u03b1 n 2 / 3 ) space in insertion-only graph streams and $$\\tilde{O}(\\alpha n^{4/5})$$ O ~ ( \u03b1 n 4 / 5 ) space in dynamic streams, where n is the number of nodes in the graph. We also show that in the vertex arrival insertion-only model, an $$(\\alpha +2)$$ ( \u03b1 + 2 ) approximation can be achieved using only $$O(\\log {n})$$ O ( log n ) space. 2. We further show that the weight of a maximum weighted matching can be efficiently estimated by augmenting any routine for estimating the size of an unweighted matching. Namely, given an algorithm for computing a $$\\lambda $$ \u03bb -approximation in the unweighted case, we obtain a $$2(1+\\varepsilon )\\cdot \\lambda $$ 2 ( 1 + \u03b5 ) \u00b7 \u03bb approximation for the weighted case, while only incurring a multiplicative logarithmic factor in the space bounds. The algorithm is implementable in any streaming model, including dynamic streams. 3. We also investigate algebraic aspects of computing matchings in data streams, by proposing new algorithms and lower bounds based on analyzing the rank of the Tutte-matrix of the graph. In particular, we present an algorithm determining whether there exists a matching of size k using $$O(k^2\\log n)$$ O ( k 2 log n ) space. 4. We also show a lower bound of $$\\Omega (n^{1-\\varepsilon })$$ \u03a9 ( n 1 - \u03b5 ) space for small approximation factors to the maximum matching size in insertion-only streams. This lower bound also holds for approximating the rank of a matrix."}}
{"id": "28fIj0DD_W", "cdate": 1546300800000, "mdate": null, "content": {"title": "Nearly Optimal Sparse Group Testing.", "abstract": "Group testing is the process of pooling arbitrary subsets from a set of n items so as to identify, with a minimal number of tests, a \u201csmall\u201d subset of d defective items. In \u201cclassical\u201d non-adaptive group testing, it is known that when d is substantially smaller than n, \u0398(dlog(n)) tests are both information-theoretically necessary and sufficient to guarantee recovery with high probability. Group testing schemes in the literature that meet this bound require most items to be tested \u03a9(log(n)) times, and most tests to incorporate \u03a9(n/d) items. Motivated by physical considerations, we study group testing models in which the testing procedure is constrained to be \u201csparse.\u201d Specifically, we consider (separately) scenarios in which 1) items are finitely divisible and hence may participate in at most \u03b3 \u2208 o(log(n)) tests; or 2) tests are size-constrained to pool no more than \u03c1 \u2208 o(n/d) items per test. For both scenarios, we provide information-theoretic lower bounds on the number of tests required to guarantee high probability recovery. In particular, one of our main results shows that \u03b3-finite divisibility of items forces any non-adaptive group testing algorithm with the probability of recovery error at most \u03f5 to perform at least \u03b3d(n/d) <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(1-5\u03f5)/\u03b3</sup> tests. Analogously, for \u03c1-sized constrained tests, we show an information-theoretic lower bound of \u03a9(n/\u03c1) tests for high-probability recovery-hence in both settings the number of tests required grows dramatically (relative to the classical setting) as a function of n. In both scenarios, we provide both randomized constructions and explicit constructions of designs with computationally efficient reconstruction algorithms that require a number of tests that is optimal up to constant or small polynomial factors in some regimes of n, d, \u03b3, and \u03c1. The randomized design/reconstruction algorithm in the \u03c1-sized test scenario is universal-independent of the value of d, as long as \u03c1 \u2208 o(n/d). We also investigate the effect of unreliability/noise in test outcomes, and show that whereas the impact of noise in test outcomes can be obviated with a small (constant factor) penalty in the number of tests in the \u03c1-sized tests scenario, there is no group-testing procedure, regardless of the number of tests, that can combat noise in the \u03b3-divisible scenario."}}
{"id": "wGLbT6RxQ9G", "cdate": 1514764800000, "mdate": null, "content": {"title": "NP-Hardness of Reed-Solomon Decoding, and the Prouhet-Tarry-Escott Problem.", "abstract": "Establishing the complexity of bounded distance decoding for Reed--Solomon codes is a fundamental open problem in coding theory, explicitly asked by Guruswami and Vardy [IEEE Trans. Inform. Theory, 51 (2005), pp. 2249--2256]. The problem is motivated by the large current gap between the regime when it is NP-hard and the regime when it is efficiently solvable (i.e., the Johnson radius). We show the first NP-hardness results for asymptotically smaller decoding radii than the maximum likelihood decoding radius of Guruswami and Vardy. Specifically, for Reed--Solomon codes of length $N$ and dimension $K=\\Theta(N)$, we show that it is NP-hard to decode more than $ N-K- c\\frac{\\log N}{\\log\\log N}$ errors (with $c>0$ an absolute constant). Moreover, we show that the problem is NP-hard under quasi-polynomial-time reductions for an error amount $> N-K- c\\log{N}$ (with $c>0$ an absolute constant). An alternative natural reformulation of the bounded distance decoding problem for Reed--Solomon codes is as a polynomial reconstruction problem. In this view, our results show that it is NP-hard to decide whether there exists a degree $K$ polynomial passing through $K+ c\\frac{\\log N}{\\log\\log N}$ points from a given set of points $(a_1, b_1), (a_2, b_2)\\ldots, (a_N, b_N)$. Furthermore, it is NP-hard under quasi-polynomial-time reductions to decide whether there is a degree $K$ polynomial passing through $K+c\\log{N}$ many points. These results follow from the NP-hardness of a generalization of the classical subset sum problem to higher moments, called moments subset sum, which has been a known open problem, and which may be of independent interest. We further reveal a strong connection with the well-studied Prouhet--Tarry--Escott problem in number theory, which turns out to capture a main barrier in extending our techniques. We believe the Prouhet--Tarry--Escott problem deserves further study in the theoretical computer science community."}}
{"id": "vRoM23xHqu", "cdate": 1514764800000, "mdate": null, "content": {"title": "Lattice-based Locality Sensitive Hashing is Optimal.", "abstract": "Locality sensitive hashing (LSH) was introduced by Indyk and Motwani (STOC'98) to give the first sublinear time algorithm for the c-approximate nearest neighbor (ANN) problem using only polynomial space. At a high level, an LSH family hashes \"nearby\" points to the same bucket and \"far away\" points to different buckets. The quality of measure of an LSH family is its LSH exponent, which helps determine both query time and space usage. In a seminal work, Andoni and Indyk (FOCS '06) constructed an LSH family based on random ball partitionings of space that achieves an LSH exponent of 1/c^2 for the l_2 norm, which was later shown to be optimal by Motwani, Naor and Panigrahy (SIDMA '07) and O'Donnell, Wu and Zhou (TOCT '14). Although optimal in the LSH exponent, the ball partitioning approach is computationally expensive. So, in the same work, Andoni and Indyk proposed a simpler and more practical hashing scheme based on Euclidean lattices and provided computational results using the 24-dimensional Leech lattice. However, no theoretical analysis of the scheme was given, thus leaving open the question of finding the exponent of lattice based LSH. In this work, we resolve this question by showing the existence of lattices achieving the optimal LSH exponent of 1/c^2 using techniques from the geometry of numbers. At a more conceptual level, our results show that optimal LSH space partitions can have periodic structure. Understanding the extent to which additional structure can be imposed on these partitions, e.g. to yield low space and query complexity, remains an important open problem."}}
{"id": "ovXJLydAPFL", "cdate": 1514764800000, "mdate": null, "content": {"title": "Flipping out with Many Flips: Hardness of Testing k-Monotonicity", "abstract": "A function f:{0,1}^n - > {0,1} is said to be k-monotone if it flips between 0 and 1 at most k times on every ascending chain. Such functions represent a natural generalization of (1-)monotone functions, and have been recently studied in circuit complexity, PAC learning, and cryptography. Our work is part of a renewed focus in understanding testability of properties characterized by freeness of arbitrary order patterns as a generalization of monotonicity. Recently, Canonne et al. (ITCS 2017) initiate the study of k-monotone functions in the area of property testing, and Newman et al. (SODA 2017) study testability of families characterized by freeness from order patterns on real-valued functions over the line [n] domain. We study k-monotone functions in the more relaxed parametrized property testing model, introduced by Parnas et al. (JCSS, 72(6), 2006). In this process we resolve a problem left open in previous work. Specifically, our results include the following. 1) Testing 2-monotonicity on the hypercube non-adaptively with one-sided error requires an exponential in sqrt{n} number of queries. This behavior shows a stark contrast with testing (1-)monotonicity, which only needs O~(sqrt{n}) queries (Khot et al. (FOCS 2015)). Furthermore, even the apparently easier task of distinguishing 2-monotone functions from functions that are far from being n^{.01}-monotone also requires an exponential number of queries. 2) On the hypercube [n]^d domain, there exists a testing algorithm that makes a constant number of queries and distinguishes functions that are k-monotone from functions that are far from being O(kd^2) -monotone. Such a dependency is likely necessary, given the lower bound above for the hypercube."}}
{"id": "njAMbSD2Iun", "cdate": 1514764800000, "mdate": null, "content": {"title": "Brief Announcement: Relaxed Locally Correctable Codes in Computationally Bounded Channels.", "abstract": "We study variants of locally decodable and locally correctable codes in computationally bounded, adversarial channels, under the assumption that collision-resistant hash functions exist, and with no public-key or private-key cryptographic setup. Specifically, we provide constructions of relaxed locally correctable and relaxed locally decodable codes over the binary alphabet, with constant information rate, and poly-logarithmic locality. Our constructions compare favorably with existing schemes built under much stronger cryptographic assumptions, and with their classical analogues in the computationally unbounded, Hamming channel. Our constructions crucially employ collision-resistant hash functions and local expander graphs, extending ideas from recent cryptographic constructions of memory-hard functions."}}
