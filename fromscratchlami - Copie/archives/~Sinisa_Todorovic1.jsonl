{"id": "M_ONb1BhdLu", "cdate": 1668014867460, "mdate": 1668014867460, "content": {"title": "Anchor-Constrained Viterbi for Set-Supervised Action Segmentation", "abstract": "This paper is about action segmentation under weak supervision in training, where the ground truth provides only\na set of actions present, but neither their temporal ordering\nnor when they occur in a training video. We use a Hidden Markov Model (HMM) grounded on a multilayer perceptron (MLP) to label video frames, and thus generate a\npseudo-ground truth for the subsequent pseudo-supervised\ntraining. In testing, a Monte Carlo sampling of action sets\nseen in training is used to generate candidate temporal sequences of actions, and select the maximum posterior sequence. Our key contribution is a new anchor-constrained\nViterbi algorithm (ACV) for generating the pseudo-ground\ntruth, where anchors are salient action parts estimated for\neach action from a given ground-truth set. Our evaluation on the tasks of action segmentation and alignment on\nthe benchmark Breakfast, MPII Cooking2, Hollywood Extended datasets demonstrates our superior performance relative to that of prior work."}}
{"id": "wZw8mxwh8q", "cdate": 1668014815805, "mdate": 1668014815805, "content": {"title": "Action Shuffle Alternating Learning for Unsupervised Action Segmentation", "abstract": "This paper addresses unsupervised action segmentation.\nPrior work captures the frame-level temporal structure of\nvideos by a feature embedding that encodes time locations\nof frames in the video. We advance prior work with a\nnew self-supervised learning (SSL) of a feature embedding\nthat accounts for both frame- and action-level structure of\nvideos. Our SSL trains an RNN to recognize positive and\nnegative action sequences, and the RNN\u2019s hidden layer is\ntaken as our new action-level feature embedding. The positive and negative sequences consist of action segments sampled from videos, where in the former the sampled action\nsegments respect their time ordering in the video, and in\nthe latter they are shuffled. As supervision of actions is not\navailable and our SSL requires access to action segments,\nwe specify an HMM that explicitly models action lengths,\nand infer a MAP action segmentation with the Viterbi algorithm. The resulting action segmentation is used as pseudoground truth for estimating our action-level feature embedding and updating the HMM. We alternate the above steps\nwithin the Generalized EM framework, which ensures convergence. Our evaluation on the Breakfast, YouTube Instructions, and 50Salads datasets gives superior results to\nthose of the state of the art."}}
{"id": "mV08k0fNx0", "cdate": 1668014767461, "mdate": 1668014767461, "content": {"title": "DESTR: Object Detection with Split Transformer", "abstract": "h model capacity, making them viable models for object detection. However, Transformers still lag in performance behind CNN-based detectors. This is, we believe, because: (a) Cross-attention is used for both classification and bounding-box regression tasks; (b) Transformer\u2019s decoder poorly initializes content queries; and (c)\nSelf-attention poorly accounts for certain prior knowledge\nwhich could help improve inductive bias. These limitations\nare addressed with the corresponding three contributions.\nFirst, we propose a new Detection Split Transformer (DESTR) that separates estimation of cross-attention into two\nindependent branches \u2013 one tailored for classification and\nthe other for box regression. Second, we use a mini-detector\nto initialize the content queries in the decoder with classification and regression embeddings of the respective heads\nin the mini-detector. Third, we augment self-attention in the\ndecoder to additionally account for pairs of adjacent object\nqueries. Our experiments on the MS-COCO dataset show\nthat DESTR outperforms DETR and its successors."}}
{"id": "wfr8p3Muzj", "cdate": 1648702586650, "mdate": 1648702586650, "content": {"title": "iFS-RCNN: An Incremental Few-shot Instance Segmenter", "abstract": "This paper addresses incremental few-shot instance segmentation, where a few examples of new object classes arrive when access to training examples of old classes is not available anymore, and the goal is to perform well on both old and new classes. We make two contributions by extending the common Mask-RCNN framework in its second stage \u2013 namely, we specify a new object class classifier based on the probit function and a new uncertainty-guided bounding-box predictor. The former leverages Bayesian learning to address a paucity of training examples of new classes. The latter learns not only to predict object bounding boxes but also to estimate the uncertainty of the prediction as a guidance for bounding box refinement. We also specify two new loss functions in terms of the estimated object-class distribution and bounding-box uncertainty. Our contributions produce significant performance gains on the COCO dataset over the state of the art \u2013 specifically, the gain of +6 on the new classes and +16 on the old classes in the AP instance segmentation metric. Furthermore, we are the first to evaluate the incremental few-shot setting on the more challenging LVIS dataset."}}
{"id": "dYOmgEtq6NQ", "cdate": 1648702292246, "mdate": 1648702292246, "content": {"title": "A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation", "abstract": "This paper addresses weakly supervised amodal instance segmentation, where the goal is to segment both visible and occluded (amodal) object parts, while training provides only ground-truth visible (modal) segmentations. Following prior work, we use data manipulation to generate occlusions in training images and thus train a segmenter to predict amodal segmentations of the manipulated data. The resulting predictions on training images are taken as the pseudo-ground truth for the standard training of Mask-RCNN, which we use for amodal instance segmentation of test images. For generating the pseudo-ground truth, we specify a new Amodal Segmenter based on Boundary Uncertainty estimation (ASBU) and make two contributions. First, while prior work uses the occluder's mask, our ASBU uses the occlusion boundary as input. Second, ASBU estimates an uncertainty map of the prediction. The estimated uncertainty regularizes learning such that lower segmentation loss is incurred on regions with high uncertainty. ASBU achieves significant performance improvement relative to the state of the art on the COCOA and KINS datasets in three tasks: amodal instance segmentation, amodal completion, and ordering recovery."}}
{"id": "4hPe2EO6_8t", "cdate": 1648702137766, "mdate": 1648702137766, "content": {"title": "Fapis: A few-shot anchor-free part-based instance segmenter", "abstract": "This paper is about few-shot instance segmentation, where training and test image sets do not share the same object classes. We specify and evaluate a new few-shot anchor-free part-based instance segmenter (FAPIS). Our key novelty is in explicit modeling of latent object parts shared across training object classes, which is expected to facilitate our few-shot learning on new classes in testing. We specify a new anchor-free object detector aimed at scoring and regressing locations of foreground bounding boxes, as well as estimating relative importance of latent parts within each box. Also, we specify a new network for delineating and weighting latent parts for the final instance segmentation within every detected bounding box. Our evaluation on the benchmark COCO-20i dataset demonstrates that we significantly outperform the state of the art."}}
{"id": "A0-N2UUxSQm", "cdate": 1648702035457, "mdate": null, "content": {"title": "A self-supervised GAN for unsupervised few-shot object recognition", "abstract": "This paper addresses unsupervised few-shot object recognition, where all training images are unlabeled, and test images are divided into queries and a few labeled support images per object class of interest. The training and test images do not share object classes. We extend the vanilla GAN with two loss functions, both aimed at self-supervised learning. The first is a reconstruction loss that enforces the discriminator to reconstruct the probabilistically sampled latent code which has been used for generating the \u201cfake\u201d image. The second is a triplet loss that enforces the discriminator to output image encodings that are closer for more similar images. Evaluation, comparisons, and detailed ablation studies are done in the context of few-shot classification. Our approach significantly outperforms the state of the art on the Mini-Imagenet and Tiered-Imagenet datasets."}}
{"id": "54NBOJLbCZs", "cdate": 1648701862860, "mdate": null, "content": {"title": "Feature weighting and boosting for few-shot segmentation", "abstract": "This paper is about few-shot segmentation of foreground objects in images. We train a CNN on small subsets of training images, each mimicking the few-shot setting. In each subset, one image serves as the query and the other (s) as support image (s) with ground-truth segmentation. The CNN first extracts feature maps from the query and support images. Then, a class feature vector is computed as an average of the support's feature maps over the known foreground. Finally, the target object is segmented in the query image by using a cosine similarity between the class feature vector and the query's feature map. We make two contributions by:(1) Improving discriminativeness of features so their activations are high on the foreground and low elsewhere; and (2) Boosting inference with an ensemble of experts guided with the gradient of loss incurred when segmenting the support images in testing. Our evaluations on the PASCAL-5i and COCO-20i datasets demonstrate that we significantly outperform existing approaches."}}
{"id": "Hn0tM83cAS", "cdate": 1582834747346, "mdate": null, "content": {"title": "Monocular Depth Estimation Using Neural Regression Forest", "abstract": "\nThis paper presents a novel deep architecture, called\nneural regression forest (NRF), for depth estimation from\na single image. NRF combines random forests and convolutional neural networks (CNNs). Scanning windows extracted from the image represent samples which are passed\ndown the trees of NRF for predicting their depth. At every\ntree node, the sample is filtered with a CNN associated with\nthat node. Results of the convolutional filtering are passed\nto left and right children nodes, i.e., corresponding CNNs,\nwith a Bernoulli probability, until the leaves, where depth\nestimations are made. CNNs at every node are designed\nto have fewer parameters than seen in recent work, but\ntheir stacked processing along a path in the tree effectively\namounts to a deeper CNN. NRF allows for parallelizable\ntraining of all \u201cshallow\u201d CNNs, and efficient enforcing of\nsmoothness in depth estimation results. Our evaluation on\nthe benchmark Make3D and NYUv2 datasets demonstrates\nthat NRF outperforms the state of the art, and gracefully\nhandles gradually decreasing training datasets.\n"}}
{"id": "AtcpkKb2Ce", "cdate": 1582834642168, "mdate": null, "content": {"title": "A Multi-Scale CNN for Affordance Segmentation in RGB Images", "abstract": "Given a single RGB image our goal is to label every pixel\nwith an affordance type. By affordance, we mean an object\u2019s capability\nto readily support a certain human action, without requiring precursor\nactions. We focus on segmenting the following five affordance types in\nindoor scenes: \u2018walkable\u2019, \u2018sittable\u2019, \u2018lyable\u2019, \u2018reachable\u2019, and \u2018movable\u2019.\nOur approach uses a deep architecture, consisting of a number of multiscale convolutional neural networks, for extracting mid-level visual cues\nand combining them toward affordance segmentation. The mid-level cues\ninclude depth map, surface normals, and segmentation of four types of\nsurfaces \u2013 namely, floor, structure, furniture and props. For evaluation,\nwe augmented the NYUv2 dataset with new ground-truth annotations\nof the five affordance types. We are not aware of prior work which starts\nfrom pixels, infers mid-level cues, and combines them in a feed-forward\nfashion for predicting dense affordance maps of a single RGB image."}}
