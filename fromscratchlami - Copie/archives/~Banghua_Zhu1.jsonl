{"id": "pm_WNYd7SP", "cdate": 1675827732365, "mdate": null, "content": {"title": "Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons", "abstract": "We  provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF).  Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under  both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound for max entropy IRL."}}
{"id": "_DVCpegg-ln", "cdate": 1672531200000, "mdate": 1680236227531, "content": {"title": "Online Learning in Stackelberg Games with an Omniscient Follower", "abstract": ""}}
{"id": "GLU0tUXqx6", "cdate": 1672531200000, "mdate": 1680236227518, "content": {"title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons", "abstract": ""}}
{"id": "FZCFlj2_c7z", "cdate": 1663850130007, "mdate": null, "content": {"title": "Jump-Start Reinforcement Learning", "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent\u2019s behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks that present exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that it is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial."}}
{"id": "jatwLvEOfdu", "cdate": 1640995200000, "mdate": 1680236227516, "content": {"title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism", "abstract": ""}}
{"id": "ZCAaWOszexv", "cdate": 1640995200000, "mdate": 1680236227582, "content": {"title": "Robust Estimation for Non-parametric Families via Generative Adversarial Networks", "abstract": ""}}
{"id": "XuZiG2kwoDP", "cdate": 1640995200000, "mdate": 1680236227531, "content": {"title": "The Sample Complexity of Online Contract Design", "abstract": ""}}
{"id": "VJjQCzN-VRc", "cdate": 1640995200000, "mdate": 1680236227576, "content": {"title": "Byzantine-Robust Federated Learning with Optimal Statistical Rates and Privacy Guarantees", "abstract": ""}}
{"id": "KbnuSu68hH-", "cdate": 1640995200000, "mdate": 1669138101091, "content": {"title": "Jump-Start Reinforcement Learning", "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial."}}
{"id": "65mj5Ij34Ls", "cdate": 1640995200000, "mdate": 1680236227524, "content": {"title": "Minimax Off-Policy Evaluation for Multi-Armed Bandits", "abstract": ""}}
