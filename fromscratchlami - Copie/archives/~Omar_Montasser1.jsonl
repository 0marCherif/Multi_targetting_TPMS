{"id": "lezFNiBwa1", "cdate": 1668734796022, "mdate": null, "content": {"title": "Certifiable Robustness Against Patch Attacks Using an ERM Oracle", "abstract": "Consider patch attacks, where at test-time an adversary manipulates a test image with a patch in order to induce a targeted mis-classification. We consider a recent defense to patch attacks, Patch-Cleanser (Xiang et al., 2022). The Patch-Cleanser algorithm requires a prediction model to have a \u201ctwo-mask correctness\u201d property, meaning that the prediction model should correctly classify any image when\nany two blank masks replace portions of the image. To this end, Xiang et al. (2022) learn a prediction model to be robust to two-mask operations by augmenting the training set by adding pairs of masks at random locations of training images, and performing empirical risk minimization (ERM) on the augmented dataset. However, in the non-realizable setting when no predictor is perfectly correct on all two-mask operations on all images, we exhibit an example where ERM fails. To overcome this challenge, we propose a different algorithm that provably learns a predictor robust to all two-mask operations using an ERM oracle, based on prior work by Feige et al. (2015a) ."}}
{"id": "l1WlfNaRkKw", "cdate": 1652737633047, "mdate": null, "content": {"title": "A Theory of PAC Learnability under Transformation Invariances", "abstract": "Transformation invariances are present in many real-world problems. For example, image classification is usually invariant to rotation and color transformation: a rotated car in a different color is still identified as a car. Data augmentation, which adds the transformed data into the training set and trains a model on the augmented data, is one commonly used technique to build these invariances into the learning process. However, it is unclear how data augmentation performs theoretically and what the optimal algorithm is in presence of transformation invariances. In this paper, we study PAC learnability under transformation invariances in three settings according to different levels of realizability: (i) A hypothesis fits the augmented data; (ii) A hypothesis fits only the original data and the transformed data lying in the support of the data distribution; (iii) Agnostic case. One interesting observation is that distinguishing between the original data and the transformed data is necessary to achieve optimal accuracy in setting (ii) and (iii), which implies that any algorithm not differentiating between the original and transformed data (including data augmentation) is not optimal. Furthermore, this type of algorithms can even ``harm'' the accuracy. In setting (i), although it is unnecessary to distinguish between the two data sets, data augmentation still does not perform optimally. Due to such a difference, we propose two combinatorial measures characterizing the optimal sample complexity in setting (i) and (ii)(iii) and provide the optimal algorithms."}}
{"id": "03Qml_SaPqV", "cdate": 1652737592193, "mdate": null, "content": {"title": "Adversarially Robust Learning: A Generic Minimax Optimal Learner and Characterization", "abstract": "We present a minimax optimal learner for the problem of learning predictors robust to adversarial examples at test-time. Interestingly, we find that this requires new algorithmic ideas and approaches to adversarially robust learning. In particular, we show, in a strong negative sense, the suboptimality of the robust learner proposed by Montasser, Hanneke, and Srebro [2019] and a broader family of learners we identify as local learners. Our results are enabled by adopting a global perspective, specifically, through a key technical contribution: the  the global one-inclusion graph, which may be of independent interest, that generalizes the classical one-inclusion graph due to Haussler, Littlestone, and Warmuth [1994]. Finally, as a byproduct, we identify a dimension characterizing qualitatively and quantitatively what classes of predictors $\\mathcal{H}$ are robustly learnable. This resolves an open problem due to Montasser et al. [2019], and closes a (potentially) infinite gap between the established upper and lower bounds on the sample complexity of adversarially robust learning. "}}
{"id": "s776AhRFm67", "cdate": 1652737591139, "mdate": null, "content": {"title": "Boosting Barely Robust Learners: A New Perspective on Adversarial Robustness", "abstract": "We present an oracle-efficient algorithm for boosting the adversarial robustness of barely robust learners. Barely robust learning algorithms learn predictors that are adversarially robust only on a small fraction $\\beta \\ll 1$ of the data distribution. Our proposed notion of barely robust learning requires robustness with respect to a ``larger'' perturbation set; which we show is necessary for strongly robust learning, and that weaker relaxations are not sufficient for strongly robust learning. Our results reveal a qualitative and quantitative equivalence between two seemingly unrelated problems: strongly robust learning and barely robust learning."}}
{"id": "9HboZKHyS6M", "cdate": 1609459200000, "mdate": null, "content": {"title": "Adversarially Robust Learning with Unknown Perturbation Sets", "abstract": "We study the problem of learning predictors that are robust to adversarial examples with respect to an unknown perturbation set, relying instead on interaction with an adversarial attacker or access to attack oracles, examining different models for such interactions. We obtain upper bounds on the sample complexity and upper and lower bounds on the number of required interactions, or number of successful attacks, in different interaction models, in terms of the VC and Littlestone dimensions of the hypothesis class of predictors, and without any assumptions on the perturbation set."}}
{"id": "q_E-0653yMc", "cdate": 1577836800000, "mdate": null, "content": {"title": "Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples", "abstract": "We present a transductive learning algorithm that takes as input training examples from a distribution $P$ and arbitrary (unlabeled) test examples, possibly chosen by an adversary. This is unlike prior work that assumes that test examples are small perturbations of $P$. Our algorithm outputs a selective classifier, which abstains from predicting on some examples. By considering selective transductive learning, we give the first nontrivial guarantees for learning classes of bounded VC dimension with arbitrary train and test distributions---no prior guarantees were known even for simple classes of functions such as intervals on the line. In particular, for any function in a class $C$ of bounded VC dimension, we guarantee a low test error rate and a low rejection rate with respect to $P$. Our algorithm is efficient given an Empirical Risk Minimizer (ERM) for $C$. Our guarantees hold even for test examples chosen by an unbounded white-box adversary. We also give guarantees for generalization, agnostic, and unsupervised settings."}}
{"id": "lZArz1_5t0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Identifying unpredictable test examples with worst-case guarantees", "abstract": "Often times, whether it be for adversarial or natural reasons, the distributions of test and training data differ. We give an algorithm that, given sets of training and test examples, identifies regions of test examples that cannot be predicted with low error. These regions are classified as ? or equivalently omitted from classification. Assuming only that labels are consistent with a family of classifiers of low VC dimension, the algorithm is shown to make few misclassification errors and few errors of omission in both adversarial and covariate-shift settings. Previous models of learning with different training and test distributions required assumptions connecting the two."}}
{"id": "eYAl_LhW2sY", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reducing Adversarially Robust Learning to Non-Robust PAC Learning", "abstract": "We study the problem of reducing adversarially robust learning to standard PAC learning, i.e. the complexity of learning adversarially robust predictors using access to only a black-box non-robust learner. We give a reduction that can robustly learn any hypothesis class C using any non-robust learner A for C. The number of calls to A depends logarithmically on the number of allowed adversarial perturbations per example, and we give a lower bound showing this is unavoidable."}}
{"id": "ZMypzYK6Oto", "cdate": 1577836800000, "mdate": null, "content": {"title": "Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples", "abstract": "We present a transductive learning algorithm that takes as input training examples from a distribution P and arbitrary (unlabeled) test examples, possibly chosen by an adversary. This is unlike prior work that assumes that test examples are small perturbations of P. Our algorithm outputs a selective classifier, which abstains from predicting on some examples. By considering selective transductive learning, we give the first nontrivial guarantees for learning classes of bounded VC dimension with arbitrary train and test distributions\u2014no prior guarantees were known even for simple classes of functions such as intervals on the line. In particular, for any function in a class C of bounded VC dimension, we guarantee a low test error rate and a low rejection rate with respect to P. Our algorithm is efficient given an Empirical Risk Minimizer (ERM) for C. Our guarantees hold even for test examples chosen by an unbounded white-box adversary. We also give guarantees for generalization, agnostic, and unsupervised settings."}}
{"id": "RqSTSq__RBt", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reducing Adversarially Robust Learning to Non-Robust PAC Learning", "abstract": "We study the problem of reducing adversarially robust learning to standard PAC learning, i.e. the complexity of learning adversarially robust predictors using access to only a black-box non-robust learner. We give a reduction that can robustly learn any hypothesis class $\\mathcal{C}$ using any non-robust learner $\\mathcal{A}$ for $\\mathcal{C}$. The number of calls to $\\mathcal{A}$ depends logarithmically on the number of allowed adversarial perturbations per example, and we give a lower bound showing this is unavoidable."}}
