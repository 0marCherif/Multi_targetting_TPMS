{"id": "HK_5RD22ZS", "cdate": 1684137113259, "mdate": 1684137113259, "content": {"title": "Fair Synthetic Data Does not Necessarily Lead to Fair Models", "abstract": "The Wasserstein GAN (WGAN) is a well-established model allowing for the generation of high-quality synthetic data approximating a given real dataset. We study TabFairGAN, a known tabular variation of WGAN in which a custom penalty term is added to the generator's loss, forcing it to produce fair data. Here we measure the fairness of synthetic data using demographic parity, i.e., the gap in the proportions of positive outcome between different sensitive groups. We reproduce some results from the paper and highlight empirically the fact that although the synthetic data achieves low demographic parity, a classification model trained on said data and evaluated on real data may still output predictions that achieve high demographic parity -- hence is unfair. In particular, we show empirically this gap holds for most parts spectrum of the fairness-accuracy tradeoff, besides the large-penalty case where the model mode collapses to the most frequent target outcome, and the low-penalty case where the data is not constrained to be fair. "}}
{"id": "67mi8NA_-ho", "cdate": 1664816298551, "mdate": null, "content": {"title": "Fair Synthetic Data Does not Necessarily Lead to Fair Models", "abstract": "The Wasserstein GAN (WGAN) is a well-established model allowing for the generation of high-quality synthetic data approximating a given real dataset. We study TabFairGAN, a known tabular variation of WGAN in which a custom penalty term is added to the generator's loss, forcing it to produce fair data. Here we measure the fairness of synthetic data using demographic parity, i.e., the gap in the proportions of positive outcome between different sensitive groups. We reproduce some results from the paper and highlight empirically the fact that although the synthetic data achieves low demographic parity, a classification model trained on said data and evaluated on real data may still output predictions that achieve high demographic parity -- hence is unfair. In particular, we show empirically this gap holds for most parts spectrum of the fairness-accuracy tradeoff, besides the large-penalty case where the model mode collapses to the most frequent target outcome, and the low-penalty case where the data is not constrained to be fair. "}}
{"id": "gL68u5UuWa", "cdate": 1663850432271, "mdate": null, "content": {"title": "Maximum Likelihood Learning of Energy-Based Models for Simulation-Based Inference", "abstract": "We introduce two Synthetic Likelihood methods for Simulation-Based Inference (SBI), to conduct either amortized or targeted inference from experimental observations when a high-fidelity simulator is available. Both methods learn a Conditional Energy-Based Model (EBM) of the likelihood using synthetic data generated by the simulator, conditioned on parameters drawn from a proposal distribution. The learned likelihood can then be combined with any prior to obtain a posterior estimate, from which samples can be drawn using MCMC. \nOur methods uniquely combine a flexible Energy-Based Model and the minimization of a KL loss: this is in contrast to other synthetic likelihood methods, which either rely on normalizing flows, or minimize score-based objectives; choices that come with known pitfalls. Our first method, Amortized Unnormalized Neural Likelihood Estimation (AUNLE), introduces a tilting trick during training that allows to perform inference using efficient MCMC techniques. Our second method, Sequential UNLE (SUNLE), employs a doubly intractable approach in order to re-use simulation data and improve posterior accuracy for a specific observation. \nWe demonstrate the properties of both methods on a range of synthetic datasets, and apply it to a neuroscience model of the pyloric network in the crab, matching the performance of other synthetic likelihood methods at a fraction of the simulation budget."}}
{"id": "wph_3smhuec", "cdate": 1652737834413, "mdate": null, "content": {"title": "Non-Convex Bilevel Games with Critical Point Selection Maps", "abstract": "Bilevel optimization problems involve two nested objectives, where an upper-level objective depends on a solution to a lower-level problem. When the latter is non-convex, multiple critical points may be present, leading to an ambiguous definition of the problem. In this paper, we introduce a key ingredient for resolving this ambiguity through the concept of a selection map which allows one to choose a particular solution to the lower-level problem. Using such maps, we define a class of hierarchical games between two agents that resolve the ambiguity in bilevel problems. \nThis new class of games requires introducing new analytical tools in Morse theory to extend implicit differentiation, a technique used in bilevel optimization resulting from the implicit function theorem. In particular, we establish the validity of such a method even when the latter theorem is inapplicable due to degenerate critical points.\nFinally, we show that algorithms for solving bilevel problems based on unrolled optimization solve these games up to approximation errors due to finite computational power. \nA simple correction to these algorithms is then proposed for removing these errors."}}
{"id": "3PN4iyXBeF", "cdate": 1632875678691, "mdate": null, "content": {"title": "Amortized Implicit Differentiation for Stochastic Bilevel Optimization", "abstract": "We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Specifically, we consider  algorithms based on inexact implicit differentiation and we exploit a warm-start strategy to amortize the estimation of the exact gradient. We then introduce a unified theoretical framework inspired by the study of singularly perturbed systems to analyze such amortized algorithms. By using this framework, our analysis shows these algorithms to match the computational complexity of oracle methods that have access to an unbiased estimate of the gradient, thus outperforming many existing results for bilevel optimization.\nWe illustrate these findings on synthetic experiments and demonstrate the efficiency of these algorithms on hyper-parameter optimization experiments involving several thousands of variables. "}}
{"id": "ZBeCVICs1Ua", "cdate": 1621630350672, "mdate": null, "content": {"title": "KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support", "abstract": "We study the gradient flow for a relaxed approximation to the Kullback-Leibler (KL) divergence\nbetween a moving source and a fixed target distribution.\nThis approximation, termed the\nKALE (KL approximate lower-bound estimator), solves a regularized version of\nthe Fenchel dual problem defining the KL over a restricted class of functions.\nWhen using a Reproducing Kernel Hilbert Space (RKHS) to define the function\nclass, we show that the KALE continuously interpolates between the KL and the\nMaximum Mean Discrepancy (MMD). Like the MMD and other Integral Probability\nMetrics, the KALE remains well defined for mutually singular\ndistributions. Nonetheless, the KALE inherits from the limiting KL a greater \nsensitivity to mismatch in the support of the distributions, compared with the MMD. These two properties make the\nKALE gradient flow particularly well suited when the target distribution is supported on a low-dimensional manifold. Under an assumption of sufficient smoothness of the trajectories, we show the global convergence of the KALE flow. We propose a particle implementation of the flow given initial samples from the source and the target distribution, which we use to empirically confirm the KALE's properties."}}
{"id": "a4WgjcLeZIn", "cdate": 1621629897781, "mdate": null, "content": {"title": "Tactical Optimism and Pessimism for Deep Reinforcement Learning", "abstract": "In recent years, deep off-policy actor-critic algorithms have become a dominant approach to reinforcement learning for continuous control. One of the primary drivers of this improved performance is the use of pessimistic value updates to address function approximation errors, which previously led to disappointing performance. However, a direct consequence of pessimism is reduced exploration, running counter to theoretical support for the efficacy of optimism in the face of uncertainty. So which approach is best? In this work, we show that the most effective degree of optimism can vary both across tasks and over the course of learning. Inspired by this insight, we introduce a novel deep actor-critic framework, Tactical Optimistic and Pessimistic (TOP) estimation, which switches between optimistic and pessimistic value learning online.  This is achieved by formulating the selection as a multi-arm bandit problem. We show in a series of continuous control tasks that TOP outperforms existing methods which rely on a fixed degree of optimism, setting a new state of the art in challenging pixel-based environments. Since our changes are simple to implement, we believe these insights can easily be incorporated into a multitude of off-policy algorithms. "}}
{"id": "lVRVu9y2vZ", "cdate": 1617671727523, "mdate": null, "content": {"title": "Synchronizing Probability Measures on Rotations via Optimal Transport", "abstract": "We introduce a new paradigm, measure synchronization, for synchronizing graphs with measure-valued edges. We formulate this problem as maximization of the cycle-consistency in the space of probability measures over relative rotations. In particular, we aim at estimating marginal distributions of absolute orientations by synchronizing the conditional ones, which are defined on the Riemannian manifold of quaternions. Such graph optimization on distributions-on-manifolds enables a natural treatment of multimodal hypotheses, ambiguities and uncertainties arising in many computer vision applications such as SLAM, SfM, and object pose estimation. We first formally define the problem as a generalization of the classical rotation graph synchronization, where in our case the vertices denote probability measures over rotations. We then measure the quality of the synchronization by using Sinkhorn divergences, which reduces to other popular metrics such as Wasserstein distance or the maximum mean discrepancy as limit cases. We propose a nonparametric Riemannian particle optimization approach to solve the problem. Even though the problem is non-convex, by drawing a connection to the recently proposed sparse optimization methods, we show that the proposed algorithm converges to the global optimum in a special case of the problem under certain conditions. Our qualitative and quantitative experiments show the validity of our approach and we bring in new perspectives to the study of synchronization."}}
{"id": "OHgnfSrn2jv", "cdate": 1601308201804, "mdate": null, "content": {"title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning", "abstract": "A novel optimization approach is proposed for application to policy gradient methods and evolution strategies for reinforcement learning (RL). The procedure uses a computationally efficient \\emph{Wasserstein natural gradient} (WNG) descent that takes advantage of the geometry induced by a Wasserstein penalty to speed optimization. This method follows the recent theme in RL of including divergence penalties in the objective to establish trust regions. Experiments on challenging tasks demonstrate improvements in both computational cost and performance over advanced baselines. \n"}}
{"id": "0PtUPB9z6qK", "cdate": 1601308175674, "mdate": null, "content": {"title": "Generalized Energy Based Models", "abstract": "We introduce the Generalized Energy Based Model (GEBM) for generative modelling. These models combine two  trained components: a base distribution (generally an implicit model), which can learn the support of data with low intrinsic dimension in a high dimensional space; and an energy function, to refine the probability mass on the learned support. \nBoth the energy function and base jointly constitute the final model, unlike GANs, which retain only the base distribution (the \"generator\").  \nGEBMs are trained by alternating between learning the energy and the base. \nWe show that both training stages are well-defined: the energy is learned by maximising a generalized likelihood, and the resulting energy-based loss provides informative gradients for learning the base.\nSamples from the posterior on the latent space of the trained model can be obtained via MCMC, thus finding regions in this space that produce better quality samples.\nEmpirically, the GEBM samples on image-generation tasks are of much better quality than those from the learned generator alone, indicating that all else being equal, the GEBM will outperform a GAN of the same complexity. When using normalizing flows as base measures, GEBMs succeed on density modelling tasks returning comparable performance to direct maximum likelihood of the same networks."}}
