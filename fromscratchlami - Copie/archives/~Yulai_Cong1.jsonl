{"id": "UfFXUfAsnPH", "cdate": 1663849872302, "mdate": null, "content": {"title": "Big Learning: A Universal Machine Learning Paradigm?", "abstract": "Recent breakthroughs based on big/foundation models reveal a vague avenue for AI, that is, \\emph{big data, big/foundation models, big learning, $\\cdots$}. Following that avenue, here we elaborate on our newly introduced big learning. Specifically, big learning exhaustively exploits the information/tasks inherent in its large-scale \\emph{complete/incomplete} training data, by learning to simultaneously model many/all joint/conditional/marginal data distributions (thus named big learning) with one universal foundation model. We reveal that big learning is what existing foundation models are implicitly doing; accordingly, our big learning provides high-level guidance for flexible design and improvements of foundation models. Besides, big learning ($i$) is equipped with great flexibilities for complete/incomplete training data and for customizing trustworthy data tasks; ($ii$) potentially delivers all joint/conditional/marginal data capabilities after training; ($iii$) significantly reduces the training-test gap with improved model generalization; and ($iv$) potentially unifies conventional machine learning paradigms and enables their flexible cooperations, manifested as a universal learning paradigm. Preliminary experiments verified the effectiveness of the presented big learning. \n"}}
{"id": "RgdpJq320fL", "cdate": 1594394548977, "mdate": null, "content": {"title": "GO Hessian for Expectation-Based Objectives", "abstract": "An unbiased low-variance gradient estimator, termed GO gradient, was proposed recently for expectation-based objectives $E_{q_{\\gamma}(y)} [f(y)]$, where the random variable (RV) y may be drawn from a stochastic computation graph with continuous (non-reparameterizable) internal nodes and continuous/discrete leaves. Upgrading the GO gradient, we present for $E_{q_{\\gamma}(y)} [f(y)]$ an unbiased low-variance Hessian estimator, named GO Hessian. Considering practical implementation, we reveal that GO Hessian is easy-to-use with auto-differentiation and Hessian-vector products, enabling efficient cheap exploitation of curvature information over stochastic computation graphs. As representative examples, we present the GO Hessian for non-reparameterizable gamma and negative binomial RVs/nodes. Based on the GO Hessian, we design a new second-order method for $E_{q_{\\gamma}(y)} [f(y)]$, with rigorous experiments conducted to verify its effectiveness and efficiency."}}
{"id": "aaCF2Xzs6pJ", "cdate": 1594394369762, "mdate": null, "content": {"title": "GAN Memory with No Forgetting", "abstract": "Seeking to address the fundamental issue of memory in lifelong learning, we propose a GAN memory that is capable of realistically remembering a stream of generative processes with no forgetting. Our GAN memory is based on recognizing that one can modulate the \u201cstyle\u201d of a GAN model to form perceptually-distant targeted generation. Accordingly, we propose to do sequential style modulations atop a well-behaved base GAN model, to form sequential targeted generative models, while simultaneously benefiting from the transferred base knowledge. Experiments demonstrate the superiority of our method over existing approaches and its effectiveness in alleviating catastrophic forgetting for lifelong classification problems"}}
{"id": "xchjMEWD17", "cdate": 1594394238765, "mdate": null, "content": {"title": "Bridging Maximum Likelihood and Adversarial Learning via \u03b1-Divergence", "abstract": "Maximum likelihood (ML) and adversarial learning are two popular approaches for training generative models, and from many perspectives these techniques are complementary. ML learning encourages the capture of all data modes, and it is typically characterized by stable raining. However, ML learning tends to distribute probability mass diffusely over the data space, e.g., yielding blurry synthetic images. Adversarial learning is well known to synthesize highly realistic natural images, despite practical challenges like mode dropping and delicate training. We propose an \u03b1-Bridge to unify the advantages of ML and adversarial learning, enabling the smooth transfer from one to the other via the \u03b1-divergence. We reveal that generalizations of the \u03b1-Bridge are closely related to approaches developed recently to regularize adversarial learning, providing insights into that prior work, and further understanding of why the \u03b1-Bridge performs well in practice"}}
{"id": "c_N_xk-wFWF", "cdate": 1594393993108, "mdate": null, "content": {"title": "On Leveraging Pretrained GANs for Generation with Limited Data ", "abstract": "Recent work has shown generative adversarial networks (GANs) can generate highly realistic images, that are often indistinguishable (by humans) from real images. Most images so generated are not contained in the training dataset, suggesting potential for augmenting raining sets with GAN-generated data. While this scenario is of particular relevance when there are limited data available, there is still the issue of training the GAN itself based on that limited data. To facilitate this, we leverage existing GAN models pretrained on large-scale datasets (like ImageNet) to introduce additional knowledge (which may not exist within the limited data), following the concept of transfer learning. Demonstrated by natural-image generation, we reveal that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be transferred to facilitate generation in a perceptuallydistinct target domain with limited training data. To further adapt the transferred filters to the target domain, we propose adaptive filter modulation (AdaFM). An extensive set of experiments is presented to demonstrate the effectiveness of the proposed techniques on generation with limited data."}}
{"id": "sAc5aq-V8XG", "cdate": 1577836800000, "mdate": null, "content": {"title": "GO Hessian for Expectation-Based Objectives", "abstract": "An unbiased low-variance gradient estimator, termed GO gradient, was proposed recently for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}}(\\boldsymbol{y})} [f(\\boldsymbol{y})]$, where the random variable (RV) $\\boldsymbol{y}$ may be drawn from a stochastic computation graph with continuous (non-reparameterizable) internal nodes and continuous/discrete leaves. Upgrading the GO gradient, we present for $\\mathbb{E}_{q_{\\boldsymbol{\\boldsymbol{\\gamma}}}(\\boldsymbol{y})} [f(\\boldsymbol{y})]$ an unbiased low-variance Hessian estimator, named GO Hessian. Considering practical implementation, we reveal that GO Hessian is easy-to-use with auto-differentiation and Hessian-vector products, enabling efficient cheap exploitation of curvature information over stochastic computation graphs. As representative examples, we present the GO Hessian for non-reparameterizable gamma and negative binomial RVs/nodes. Based on the GO Hessian, we design a new second-order method for $\\mathbb{E}_{q_{\\boldsymbol{\\boldsymbol{\\gamma}}}(\\boldsymbol{y})} [f(\\boldsymbol{y})]$, with rigorous experiments conducted to verify its effectiveness and efficiency."}}
{"id": "q-PJTgc-W0w", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bridging Maximum Likelihood and Adversarial Learning via \u03b1-Divergence", "abstract": "Maximum likelihood (ML) and adversarial learning are two popular approaches for training generative models, and from many perspectives these techniques are complementary. ML learning encourages the capture of all data modes, and it is typically characterized by stable training. However, ML learning tends to distribute probability mass diffusely over the data space, e.g., yielding blurry synthetic images. Adversarial learning is well known to synthesize highly realistic natural images, despite practical challenges like mode dropping and delicate training. We propose an \u03b1-Bridge to unify the advantages of ML and adversarial learning, enabling the smooth transfer from one to the other via the \u03b1-divergence. We reveal that generalizations of the \u03b1-Bridge are closely related to approaches developed recently to regularize adversarial learning, providing insights into that prior work, and further understanding of why the \u03b1-Bridge performs well in practice."}}
{"id": "lKKT3B40_94", "cdate": 1577836800000, "mdate": null, "content": {"title": "GAN Memory with No Forgetting", "abstract": "As a fundamental issue in lifelong learning, catastrophic forgetting is directly caused by inaccessible historical data; accordingly, if the data (information) were memorized perfectly, no forgetting should be expected. Motivated by that, we propose a GAN memory for lifelong learning, which is capable of remembering a stream of datasets via generative processes, with \\emph{no} forgetting. Our GAN memory is based on recognizing that one can modulate the \"style\" of a GAN model to form perceptually-distant targeted generation. Accordingly, we propose to do sequential style modulations atop a well-behaved base GAN model, to form sequential targeted generative models, while simultaneously benefiting from the transferred base knowledge. The GAN memory -- that is motivated by lifelong learning -- is therefore itself manifested by a form of lifelong learning, via forward transfer and modulation of information from prior tasks. Experiments demonstrate the superiority of our method over existing approaches and its effectiveness in alleviating catastrophic forgetting for lifelong classification problems. Code is available at https://github.com/MiaoyunZhao/GANmemory_LifelongLearning."}}
{"id": "SypmJKm4Nn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bridging Maximum Likelihood and Adversarial Learning via \u03b1-Divergence", "abstract": "Maximum likelihood (ML) and adversarial learning are two popular approaches for training generative models, and from many perspectives these techniques are complementary. ML learning encourages the capture of all data modes, and it is typically characterized by stable training. However, ML learning tends to distribute probability mass diffusely over the data space, $e.g.$, yielding blurry synthetic images. Adversarial learning is well known to synthesize highly realistic natural images, despite practical challenges like mode dropping and delicate training. We propose an $\\alpha$-Bridge to unify the advantages of ML and adversarial learning, enabling the smooth transfer from one to the other via the $\\alpha$-divergence. We reveal that generalizations of the $\\alpha$-Bridge are closely related to approaches developed recently to regularize adversarial learning, providing insights into that prior work, and further understanding of why the $\\alpha$-Bridge performs well in practice."}}
{"id": "GewbwhhxcGG", "cdate": 1577836800000, "mdate": null, "content": {"title": "GAN Memory with No Forgetting", "abstract": "As a fundamental issue in lifelong learning, catastrophic forgetting is directly caused by inaccessible historical data; accordingly, if the data (information) were memorized perfectly, no forgetting should be expected. Motivated by that, we propose a GAN memory for lifelong learning, which is capable of remembering a stream of datasets via generative processes, with \\emph{no} forgetting. Our GAN memory is based on recognizing that one can modulate the ``style'' of a GAN model to form perceptually-distant targeted generation. Accordingly, we propose to do sequential style modulations atop a well-behaved base GAN model, to form sequential targeted generative models, while simultaneously benefiting from the transferred base knowledge. The GAN memory -- that is motivated by lifelong learning -- is therefore itself manifested by a form of lifelong learning, via forward transfer and modulation of information from prior tasks. Experiments demonstrate the superiority of our method over existing approaches and its effectiveness in alleviating catastrophic forgetting for lifelong classification problems. Code is available at \\url{https://github.com/MiaoyunZhao/GANmemory_LifelongLearning}."}}
