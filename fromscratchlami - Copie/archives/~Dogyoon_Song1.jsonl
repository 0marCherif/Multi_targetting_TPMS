{"id": "3pVx2dVGIeJ", "cdate": 1677628800000, "mdate": 1681957089317, "content": {"title": "On approximations of the PSD cone by a polynomial number of smaller-sized PSD cones", "abstract": "We study the problem of approximating the cone of positive semidefinite (PSD) matrices with a cone that can be described by smaller-sized PSD constraints. Specifically, we ask the question: \u201chow closely can we approximate the set of unit-trace $$n \\times n$$ n \u00d7 n PSD matrices, denoted by D, using at most N number of $$k \\times k$$ k \u00d7 k PSD constraints?\u201d In this paper, we prove lower bounds on N to achieve a good approximation of D by considering two constructions of an approximating set. First, we consider the unit-trace $$n \\times n$$ n \u00d7 n symmetric matrices that are PSD when restricted to a fixed set of k-dimensional subspaces in $${\\mathbb {R}}^n$$ R n . We prove that if this set is a good approximation of D, then the number of subspaces must be at least exponentially large in n for any $$k = o(n)$$ k = o ( n ) . Second, we show that any set S that approximates D within a constant approximation ratio must have superpolynomial $${\\varvec{S}}_+^k$$ S + k -extension complexity. To be more precise, if S is a constant factor approximation of D, then S must have $${\\varvec{S}}_+^k$$ S + k -extension complexity at least $$\\exp ( C \\cdot \\min \\{ \\sqrt{n}, n/k \\})$$ exp ( C \u00b7 min { n , n / k } ) where C is some absolute constant. In addition, we show that any set S such that $$D \\subseteq S$$ D \u2286 S and the Gaussian width of S is at most a constant times larger than the Gaussian width of D must have $${\\varvec{S}}_+^k$$ S + k -extension complexity at least $$\\exp ( C \\cdot \\min \\{ n^{1/3}, \\sqrt{n/k} \\})$$ exp ( C \u00b7 min { n 1 / 3 , n / k } ) . These results imply that the cone of $$n \\times n$$ n \u00d7 n PSD matrices cannot be approximated by a polynomial number of $$k \\times k$$ k \u00d7 k PSD constraints for any $$k = o(n / \\log ^2 n)$$ k = o ( n / log 2 n ) . These results generalize the recent work of Fawzi (Math Oper Res 46(4):1479\u20131489, 2021) on the hardness of polyhedral approximations of $${\\varvec{S}}_+^n$$ S + n , which corresponds to the special case with $$k=1$$ k = 1 ."}}
{"id": "jyXvF-Rl3H", "cdate": 1672531200000, "mdate": 1681655685048, "content": {"title": "Robustness-preserving Lifelong Learning via Dataset Condensation", "abstract": ""}}
{"id": "JF7K41P1gDh", "cdate": 1609459200000, "mdate": 1681957089307, "content": {"title": "Addressing Missing Data and Scalable Optimization for Data-driven Decision Making", "abstract": "Data-driven decision making has become a necessary commodity in virtually every domain of human endeavor, fueled by the exponential growth in the availability of data and the rapid increase in our computing power. In principle, if the collected data contain sufficient information, it is possible to build a useful model for making decisions. Nevertheless, there are a few challenges to address to bring it into reality. First, the gathered data can be contaminated by noise, or even by missing values. Second, building a model from data usually involves solving an optimization problem, which may require prohibitively large computational resources. In this thesis, we explore two research directions, motivated by these two challenges. In the first part of the thesis, we consider statistical learning problems with missing data, and discuss the efficacy of data imputation approaches in predictive modeling tasks. To this end, we first review low-rank matrix completion techniques and establish a novel error analysis for the matrix estimation beyond the traditional mean squared error (Frobenius norm), focusing on the singular value thresholding algorithm. Thereafter, we study two specific predictive problem settings -- namely, errors-in-variables regression and Q-learning with thrifty exploration -- and argue that the predictions based on the imputed data are typically nearly as accurate as the predictions made when the complete data were available. In the second part of the thesis, we investigate the tradeoff between the scalability and the quality of optimal solutions in the context of approximate semidefinite programming. Specifically, we ask the question: \u201chow closely can we approximate the set of unit-trace n \u00d7 n positive semidefinite (PSD) matrices, denoted by D\u207f, using at most N number of k \u00d7 k PSD constraints?\u201d We show that any set S that approximates D\u207f within a constant approximation ratio must have superpolynomially large S\u1d4f\u208a -extension complexity for all k = o(n/ log n). Our results imply that it is impossible to globally approximate a large-scale PSD cone using only a few, smaller-sized PSD constraints. Therefore, we conclude that local, problem-adaptive techniques are essential to approximate SDPs for enhanced scalability."}}
{"id": "JYmPw_VYBye", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation", "abstract": "We consider the question of learning $Q$-function in a sample efficient manner for reinforcement learning with continuous state and action spaces under a generative model. If $Q$-function is Lipschitz continuous, then the minimal sample complexity for estimating $\\epsilon$-optimal $Q$-function is known to scale as $\\Omega(\\frac{1}{\\epsilon^{d_1+d_2+2}})$ per classical non-parametric learning theory, where $d_1$ and $d_2$ denote the dimensions of the state and action spaces respectively. The $Q$-function, when viewed as a kernel, induces a Hilbert-Schmidt operator and hence possesses square-summable spectrum. This motivates us to consider a parametric class of $Q$-functions parameterized by its \"rank\" $r$, which contains all Lipschitz $Q$-functions as $r\\to\\infty$. As our key contribution, we develop a simple, iterative learning algorithm that finds $\\epsilon$-optimal $Q$-function with sample complexity of $\\widetilde{O}(\\frac{1}{\\epsilon^{\\max(d_1, d_2)+2}})$ when the optimal $Q$-function has low rank $r$ and the discounting factor $\\gamma$ is below a certain threshold. Thus, this provides an exponential improvement in sample complexity. To enable our result, we develop a novel Matrix Estimation algorithm that faithfully estimates an unknown low-rank matrix in the $\\ell_\\infty$ sense even in the presence of arbitrary bounded noise, which might be of interest in its own right. Empirical results on several stochastic control tasks confirm the efficacy of our \"low-rank\" algorithms."}}
{"id": "IaAlGG4cRo", "cdate": 1577836800000, "mdate": 1681957089297, "content": {"title": "Nearest Neighbors for Matrix Estimation Interpreted as Blind Regression for Latent Variable Model", "abstract": "We consider the setup of nonparametric blind regression for estimating the entries of a large m \u00d7 n matrix, when provided with a small, random fraction of noisy measurements. We assume that all rows u \u2208 [m] and columns i \u2208 [n] of the matrix are associated to latent features xrow(u) and xcol(i) respectively, and the (u, i)-th entry of the matrix, A(u, i) is equal to f(x <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">row</sub> (u), x <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">col</sub> (i)) for a latent functionf. Given noisy observations of a small, random subset of the matrix entries, our goal is to estimate the unobserved entries of the matrix as well as to \u201cdenoise\u201d the observed entries. As the main result of this work, we introduce a nearest-neighbor-based estimation algorithm, and establish its consistency when the underlying latent function f is Lipschitz, the underlying latent space is a bounded diameter Polish space, and the random fraction of observed entries in the matrix is at least max (m <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-1+\u03b4</sup> , n <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-1/2+\u03b4</sup> ), for any \u03b4 > 0. As an important byproduct, our analysis sheds light into the performance of the classical collaborative filtering algorithm for matrix completion, which has been widely utilized in practice. Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides a principled improvement over basic collaborative filtering and is competitive with matrix factorization methods. Our algorithm has a natural extension to the setting of tensor completion via flattening the tensor to matrix. When applied to the setting of image in-painting, which is a 3-order tensor, we find that our approach is competitive with respect to state-of-art tensor completion algorithms across benchmark images."}}
{"id": "mkQr7QC1_8a", "cdate": 1546300800000, "mdate": 1681957089302, "content": {"title": "On Robustness of Principal Component Regression", "abstract": "Consider the setting of Linear Regression where the observed response variables, in expectation, are linear functions of the p-dimensional covariates. Then to achieve vanishing prediction error, the number of required samples scales faster than p\u03c32, where \u03c32 is a bound on the noise variance. In a high-dimensional setting where p is large but the covariates admit a low-dimensional representation (say r \u226a p), then Principal Component Regression (PCR), cf. [36], is an effective approach; here, the response variables are regressed with respect to the principal components of the covariates. The resulting number of required samples to achieve vanishing prediction error now scales faster than r\u03c32(\u226a p\u03c32). Despite the tremendous utility of PCR, its ability to handle settings with noisy, missing, and mixed (discrete and continuous) valued covariates is not understood and remains an important open challenge, cf. [24]. As the main contribution of this work, we address this challenge by rigorously establishing that PCR is robust to noisy, sparse, and possibly mixed valued covariates. Specifically, under PCR, vanishing prediction error is achieved with the number of samples scaling as r max(\u03c32, \u03c1\u22124 log5(p)), where \u03c1 denotes the fraction of observed (noisy) covariates. We establish generalization error bounds on the performance of PCR, which provides a systematic approach in selecting the correct number of components r in a data-driven manner. The key to our result is a simple, but powerful equivalence between (i) PCR and (ii) Linear Regression with covariate pre-processing via Hard Singular Value Thresholding (HSVT). From a technical standpoint, this work advances the state-of-the-art analysis for HSVT by establishing stronger guarantees with respect to the \u2225\u00b7\u22252,\u221e-error for the estimated matrix rather than the Frobenius norm/mean-squared error (MSE) as is commonly done in the matrix estimation / completion literature."}}
{"id": "H1Z1EDW_WH", "cdate": 1451606400000, "mdate": null, "content": {"title": "Blind Regression: Nonparametric Regression for Latent Variable Models via Collaborative Filtering", "abstract": "We introduce the framework of {\\em blind regression} motivated by {\\em matrix completion} for recommendation systems: given $m$ users, $n$ movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix. Following the framework of non-parametric statistics, we posit that user $u$ and movie $i$ have features $x_1(u)$ and $x_2(i)$ respectively, and their corresponding rating $y(u,i)$ is a noisy measurement of $f(x_1(u), x_2(i))$ for some unknown function $f$. In contrast with classical regression, the features $x = (x_1(u), x_2(i))$ are not observed, making it challenging to apply standard regression methods to predict the unobserved ratings. Inspired by the classical Taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all Lipschitz functions. In fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice. Assuming each entry is sampled independently with probability at least $\\max(m^{-1+\\delta},n^{-1/2+\\delta})$ with $\\delta &gt; 0$, we prove that the expected fraction of our estimates with error greater than $\\epsilon$ is less than $\\gamma^2 / \\epsilon^2$ plus a polynomially decaying term, where $\\gamma^2$ is the variance of the additive entry-wise noise term. Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods."}}
