{"id": "8rtlZhGloB", "cdate": 1699148018207, "mdate": 1699148018207, "content": {"title": "DRO: Deep Recurrent Optimizer for Video to Depth", "abstract": "There are increasing interests of studying the video-to-depth (V2D) problem with machine learning techniques. While earlier methods directly learn a mapping from images to depth maps and camera poses, more recent works enforce multi-view geometry constraints through optimization embedded in the learning framework. This paper presents a novel optimization method based on recurrent neural networks to further exploit the potential of neural networks in V2D. Specifically, our neural optimizer alternately updates the depth and camera poses through iterations to minimize a feature-metric cost, and two gated recurrent units iteratively improve the results by tracing historical information. Extensive experimental results demonstrate that our method outperforms previous methods and is more efficient in computation and memory consumption than cost-volume-based methods. In particular, our self-supervised method outperforms previous supervised methods on the KITTI and ScanNet datasets. Our source code will be made public."}}
{"id": "HQz8X_iKRT", "cdate": 1668793754049, "mdate": 1668793754049, "content": {"title": "RCP: Recurrent Closest Point for Point Cloud", "abstract": "3D motion estimation including scene flow and point\ncloud registration has drawn increasing interest. Inspired\nby 2D flow estimation, recent methods employ deep neural\nnetworks to construct the cost volume for estimating accurate 3D flow. However, these methods are limited by the fact\nthat it is difficult to define a search window on point clouds\nbecause of the irregular data structure. In this paper, we\navoid this irregularity by a simple yet effective method. We\ndecompose the problem into two interlaced stages, where\nthe 3D flows are optimized point-wisely at the first stage\nand then globally regularized in a recurrent network at the\nsecond stage. Therefore, the recurrent network only receives\nthe regular point-wise information as the input. In the experiments, we evaluate the proposed method on both the 3D\nscene flow estimation and the point cloud registration task.\nFor 3D scene flow estimation, we make comparisons on the\nwidely used FlyingThings3D [32] and KITTI [33] datasets.\nFor point cloud registration, we follow previous works and\nevaluate the data pairs with large pose and partially overlapping from ModelNet40 [65]. The results show that our\nmethod outperforms the previous method and achieves a\nnew state-of-the-art performance on both 3D scene flow estimation and point cloud registration, which demonstrates\nthe superiority of the proposed zero-order method on irregular point cloud data. Our source code is available at\nhttps://github.com/gxd1994/RCP."}}
{"id": "4jU28iijyy", "cdate": 1668021940218, "mdate": 1668021940218, "content": {"title": "Learning camera localization via dense scene matching", "abstract": "Camera localization aims to estimate 6 DoF camera poses from RGB images. Traditional methods detect and match interest points between a query image and a pre-built 3D model. Recent learning-based approaches encode scene structures into a specific convolutional neural network (CNN) and thus are able to predict dense coordinates from RGB images. However, most of them require re-training or re-adaption for a new scene and have difficulties in handling large-scale scenes due to limited network capacity. We present a new method for scene agnostic camera localization using dense scene matching (DSM), where the cost volume is constructed between a query image and a scene. The cost volume and the corresponding coordinates are processed by a CNN to predict dense coordinates. Camera poses can then be solved by PnP algorithms. In addition, our method can be extended to temporal domain, giving extra performance boost during testing time. Our scene-agnostic approach achieves comparable accuracy as the existing scene-specific approaches on the 7scenes and Cambridge benchmark. This approach also remarkably outperforms state-of-the-art scene-agnostic dense coordinate regression network SANet."}}
{"id": "SZ1e7qQdsJj", "cdate": 1652770415388, "mdate": 1652770415388, "content": {"title": "Learning to Zoom Inside Camera Imaging Pipeline", "abstract": "Existing single image super-resolution methods are ei- ther designed for synthetic data, or for real data but in the RGB-to-RGB or the RAW-to-RGB domain. This paper proposes to zoom an image from RAW to RAW inside the camera imaging pipeline. The RAW-to-RAW domain closes the gap between the ideal and the real degradation models. It also excludes the image signal processing pipeline, which refo- cuses the model learning onto the super-resolution. To these ends, we design a method that receives a low-resolution RAW as the input and estimates the desired higher-resolution RAW jointly with the degradation model. In our method, two convolutional neural networks are learned to constrain the high-resolution image and the degradation model in lower-dimensional subspaces. This subspace constraint con- verts the ill-posed SISR problem to a well-posed one. To demonstrate the superiority of the proposed method and the RAW-to-RAW domain, we conduct evaluations on the RealSR and the SR-RAW datasets. The results show that our method performs superiorly over the state-of-the-arts both qualitatively and quantitatively, and it also generalizes well and enables zero-shot transfer across different sensors."}}
{"id": "NS7btE4A_uH", "cdate": 1577836800000, "mdate": null, "content": {"title": "LSM: Learning Subspace Minimization for Low-Level Vision", "abstract": "We study the energy minimization problem in low-level vision tasks from a novel perspective. We replace the heuristic regularization term with a data-driven learnable subspace constraint, and preserve the data term to exploit domain knowledge derived from the first principles of a task. This learning subspace minimization (LSM) framework unifies the network structures and the parameters for many different low-level vision tasks, which allows us to train a single network for multiple tasks simultaneously with shared parameters, and even generalizes the trained network to an unseen task as long as the data term can be formulated. We validate our LSM frame on four low-level tasks including edge detection, interactive segmentation, stereo matching, and optical flow, and validate the network on various datasets. The experiments demonstrate that the proposed LSM generates state-of-the-art results with smaller model size, faster training convergence, and real-time inference."}}
{"id": "pFNGZcXRUA", "cdate": 1546300800000, "mdate": null, "content": {"title": "SANet: Scene Agnostic Network for Camera Localization", "abstract": "This paper presents a scene agnostic neural architecture for camera localization, where model parameters and scenes are independent from each other.Despite recent advancement in learning based methods, most approaches require training for each scene one by one, not applicable for online applications such as SLAM and robotic navigation, where a model must be built on-the-fly.Our approach learns to build a hierarchical scene representation and predicts a dense scene coordinate map of a query RGB image on-the-fly given an arbitrary scene. The 6D camera pose of the query image can be estimated with the predicted scene coordinate map. Additionally, the dense prediction can be used for other online robotic and AR applications such as obstacle avoidance. We demonstrate the effectiveness and efficiency of our method on both indoor and outdoor benchmarks, achieving state-of-the-art performance."}}
{"id": "CdyC76gihQV", "cdate": 1546300800000, "mdate": null, "content": {"title": "Joint Stabilization and Direction of 360\u00b0 Videos", "abstract": "Three-hundred-sixty-degree (360\u00b0) video provides an immersive experience for viewers, allowing them to freely explore the world by turning their head. However, creating high-quality 360\u00b0\u00a0video content can be challenging, as viewers may miss important events by looking in the wrong direction, or they may see things that ruin the immersion, such as stitching artifacts and the film crew. We take advantage of the fact that not all directions are equally likely to be observed; most viewers are more likely to see content located at \u201ctrue north,\u201d i.e., in front of them, due to ergonomic constraints. We therefore propose 360\u00b0\u00a0video direction, where the video is jointly optimized to orient important events to the front of the viewer and visual clutter behind them, while producing smooth camera motion. Unlike traditional video, viewers can still explore the space as desired, but with the knowledge that the most important content is likely to be in front of them. Constraints can be user guided, either added directly on the equirectangular projection or by recording \u201cguidance\u201d viewing directions while watching the video in a VR headset or automatically computed, such as via visual saliency or forward-motion direction. To accomplish this, we propose a new motion estimation technique specifically designed for 360\u00b0\u00a0video that outperforms the commonly used five-point algorithm on wide-angle video. We additionally formulate the direction problem as an optimization where a novel parametrization of spherical warping allows us to correct for some degree of parallax effects. We compare our approach to recent methods that address stabilization-only and converting 360\u00b0\u00a0video to narrow field-of-view video. Our pipeline can also enable the viewing of wide-angle non-360\u00b0\u00a0footage in a spherical 360\u00b0\u00a0space, giving an immersive \u201cvirtual cinema\u201d experience for a wide range of existing content filmed with first-person cameras."}}
{"id": "B1gabhRcYX", "cdate": 1538087941275, "mdate": null, "content": {"title": "BA-Net: Dense Bundle Adjustment Networks", "abstract": "This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable, so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image, and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method."}}
{"id": "O4FtniFjTpe", "cdate": 1483228800000, "mdate": null, "content": {"title": "GSLAM: Initialization-Robust Monocular Visual SLAM via Global Structure-from-Motion", "abstract": "Many monocular visual SLAM algorithms are derived from incremental structure-from-motion (SfM) methods. This work proposes a novel monocular SLAM method which integrates recent advances made in global SfM. In particular, we present two main contributions to visual SLAM. First, we solve the visual odometry problem by a novel rank-1 matrix factorization technique which is more robust to the errors in map initialization. Second, we adopt a recent global SfM method for the pose-graph optimization, which leads to a multi-stage linear formulation and enables L1 optimization for better robustness to false loops. The combination of these two approaches generates more robust reconstruction and is significantly faster (4X) than recent state-of-the-art SLAM systems. We also present a new dataset recorded with ground truth camera motion in a Vicon motion capture room, and compare our method to prior systems on it and established benchmark datasets."}}
{"id": "3l683R53Gx", "cdate": 1420070400000, "mdate": null, "content": {"title": "Linear Global Translation Estimation with Feature Tracks", "abstract": ""}}
