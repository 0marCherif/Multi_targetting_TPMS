{"id": "WG3vmsteqR_", "cdate": 1652737794888, "mdate": null, "content": {"title": "The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization", "abstract": "The logit outputs of a feedforward neural network at initialization are conditionally Gaussian, given a random covariance matrix defined by the penultimate layer. In this work, we study the distribution of this random matrix. Recent work has shown that shaping the activation function as network depth grows large is necessary for this covariance matrix to be non-degenerate. However, the current infinite-width-style understanding of this shaping method is unsatisfactory for large depth: infinite-width analyses ignore the microscopic fluctuations from layer to layer, but these fluctuations accumulate over many layers. \n\nTo overcome this shortcoming, we study the random covariance matrix in the shaped infinite-depth-and-width limit. We identify the precise scaling of the activation function necessary to arrive at a non-trivial limit, and show that the random covariance matrix is governed by a stochastic differential equation (SDE) that we call the Neural Covariance SDE. Using simulations, we show that the SDE closely matches the distribution of the random covariance matrix of finite networks. Additionally, we recover an if-and-only-if condition for exploding and vanishing norms of large shaped networks based on the activation function. "}}
{"id": "rxgHtZQSxq", "cdate": 1645715836549, "mdate": 1645715836549, "content": {"title": "Acceleration of Gossip Algorithms through the Euler-Poisson-Darboux Equation", "abstract": "Gossip algorithms and their accelerated versions have been studied exclusively in discrete time on graphs. In this work, we take a different approach, and consider the scaling limit of gossip algorithms in both large graphs and large number of iterations. These limits lead to well-known partial differential equations (PDEs) with insightful properties. On lattices, we prove that the non-accelerated gossip algorithm of Boyd et al. [2006] converges to the heat equation, and the accelerated Jacobi polynomial iteration of Berthier et al. [2020] converges to the Euler-Poisson-Darboux (EPD) equation - a damped wave equation. Remarkably, with appropriate parameters, the fundamental solution of the EPD equation has the ideal gossip behaviour: a uniform density over an ellipsoid, whose radius increases at a rate proportional to t - the fastest possible rate for locally communicating gossip algorithms. This is in contrast with the heat equation where the density spreads on a typical scale of t\u221a. Additionally, we provide simulations demonstrating that the gossip algorithms are accurately approximated by their limiting PDEs."}}
{"id": "SnEez8-mrl5", "cdate": 1645715785876, "mdate": 1645715785876, "content": {"title": "Analysis of Langevin Monte Carlo from Poincar\u00e9 to Log-Sobolev", "abstract": "Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution \u03c0 under the sole assumption that \u03c0 satisfies a Poincar\u00e9 inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or R\u00e9nyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that \u03c0 satisfies either a Lata\u0142a--Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincar\u00e9 and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions."}}
{"id": "-h99IwQN-f", "cdate": 1621630050510, "mdate": null, "content": {"title": "The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization", "abstract": "Theoretical results show that neural networks can be approximated by Gaussian processes in the infinite-width limit. However, for fully connected networks, it has been previously shown that for any fixed network width, $n$, the Gaussian approximation gets worse as the network depth, $d$, increases. Given that modern networks are deep, this raises the question of how well modern architectures, like ResNets, are captured by the infinite-width limit. To provide a better approximation, we study ReLU ResNets in the infinite-depth-and-width limit, where \\emph{both} depth and width tend to infinity as their ratio, $d/n$, remains constant. In contrast to the Gaussian infinite-width limit, we show theoretically that the network exhibits log-Gaussian behaviour at initialization in the infinite-depth-and-width limit, with parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we demonstrate that even basic properties of standard ResNet architectures are poorly captured by the Gaussian limit, but remarkably well captured by our log-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at initialization are hypoactivated: fewer than half of the ReLUs are activated. Additionally, we calculate the interlayer correlations, which have the effect of exponentially increasing the variance of the network output. Based on our analysis, we introduce \\emph{Balanced ResNets}, a simple architecture modification, which eliminates hypoactivation and interlayer correlations and is more amenable to theoretical analysis."}}
{"id": "SZbFAlQrlc", "cdate": 1609459200000, "mdate": 1645715665459, "content": {"title": "Higher Order Generalization Error for First Order Discretization of Langevin Diffusion", "abstract": "We propose a novel approach to analyze generalization error for discretizations of Langevin diffusion, such as the stochastic gradient Langevin dynamics (SGLD). For an $\\epsilon$ tolerance of expected generalization error, it is known that a first order discretization can reach this target if we run $\\Omega(\\epsilon^{-1} \\log (\\epsilon^{-1}) )$ iterations with $\\Omega(\\epsilon^{-1})$ samples. In this article, we show that with additional smoothness assumptions, even first order methods can achieve arbitrarily runtime complexity. More precisely, for each $N>0$, we provide a sufficient smoothness condition on the loss function such that a first order discretization can reach $\\epsilon$ expected generalization error given $\\Omega( \\epsilon^{-1/N} \\log (\\epsilon^{-1}) )$ iterations with $\\Omega(\\epsilon^{-1})$ samples."}}
{"id": "HCM-Y0emSlq", "cdate": 1609459200000, "mdate": 1645715665467, "content": {"title": "The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width Limit at Initialization", "abstract": "Theoretical results show that neural networks can be approximated by Gaussian processes in the infinite-width limit. However, for fully connected networks, it has been previously shown that for any fixed network width, $n$, the Gaussian approximation gets worse as the network depth, $d$, increases. Given that modern networks are deep, this raises the question of how well modern architectures, like ResNets, are captured by the infinite-width limit. To provide a better approximation, we study ReLU ResNets in the infinite-depth-and-width limit, where both depth and width tend to infinity as their ratio, $d/n$, remains constant. In contrast to the Gaussian infinite-width limit, we show theoretically that the network exhibits log-Gaussian behaviour at initialization in the infinite-depth-and-width limit, with parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we demonstrate that even basic properties of standard ResNet architectures are poorly captured by the Gaussian limit, but remarkably well captured by our log-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at initialization are hypoactivated: fewer than half of the ReLUs are activated. Additionally, we calculate the interlayer correlations, which have the effect of exponentially increasing the variance of the network output. Based on our analysis, we introduce Balanced ResNets, a simple architecture modification, which eliminates hypoactivation and interlayer correlations and is more amenable to theoretical analysis."}}
{"id": "HI7WYAxXHg9", "cdate": 1577836800000, "mdate": 1645715665461, "content": {"title": "Riemannian Langevin Algorithm for Solving Semidefinite Programs", "abstract": "We propose a Langevin diffusion-based algorithm for non-convex optimization and sampling on a product manifold of spheres. Under a logarithmic Sobolev inequality, we establish a guarantee for finite iteration convergence to the Gibbs distribution in terms of Kullback--Leibler divergence. We show that with an appropriate temperature choice, the suboptimality gap to the global minimum is guaranteed to be arbitrarily small with high probability. As an application, we consider the Burer--Monteiro approach for solving a semidefinite program (SDP) with diagonal constraints, and analyze the proposed Langevin algorithm for optimizing the non-convex objective. In particular, we establish a logarithmic Sobolev inequality for the Burer--Monteiro problem when there are no spurious local minima, but under the presence saddle points. Combining the results, we then provide a global optimality guarantee for the SDP and the Max-Cut problem. More precisely, we show that the Langevin algorithm achieves $\\epsilon$ accuracy with high probability in $\\widetilde{\\Omega}( \\epsilon^{-5} )$ iterations."}}
