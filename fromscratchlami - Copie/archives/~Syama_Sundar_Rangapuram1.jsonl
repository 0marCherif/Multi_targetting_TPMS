{"id": "oPHuNpJl3c", "cdate": 1664928778720, "mdate": null, "content": {"title": "Adaptive Sampling for Probabilistic Forecasting under Distribution Shift", "abstract": "The world is not static: This causes real-world time series to change over time\nthrough external, and potentially disruptive, events such as macroeconomic cycles\nor the COVID-19 pandemic. We present an adaptive sampling strategy that selects\nthe part of the time series history that is relevant for forecasting. We achieve this by\nlearning a discrete distribution over relevant time steps by Bayesian optimization.\nWe instantiate this idea with a two-step method that is pre-trained with uniform\nsampling and then training a lightweight adaptive architecture with adaptive sam-\npling. We show with synthetic and real-world experiments that this method adapts\nto distribution shift and significantly reduces the forecasting error of the base model\nfor three out of five datasets."}}
{"id": "EKPSenGCx9g", "cdate": 1648734377132, "mdate": 1648734377132, "content": {"title": "End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series", "abstract": "This paper presents a novel approach for hierarchical time series forecasting that produces coherent, probabilistic forecasts without requiring any explicit post-processing reconciliation. Unlike the state-of-the-art, the proposed method simultaneously learns from all time series in the hierarchy and incorporates the reconciliation step into a single trainable model. This is achieved by applying the reparameterization trick and casting reconciliation as an optimization problem with a closed-form solution. These model features make end-to-end learning of hierarchical forecasts possible, while accomplishing the challenging task of generating forecasts that are both probabilistic and coherent. Importantly, our approach also accommodates general aggregation constraints including grouped and temporal hierarchies. An extensive empirical evaluation on real-world hierarchical datasets demonstrates the advantages of the proposed approach over the state-of-the-art."}}
{"id": "JpNH4CW_zl", "cdate": 1632875514161, "mdate": null, "content": {"title": "Multivariate Time Series Forecasting with Latent Graph Inference", "abstract": "This paper introduces a new architecture for multivariate time series forecasting that simultaneously infers and leverages relations among time series. We cast our method as a modular extension to univariate architectures where relations among individual time series are dynamically inferred in the latent space obtained after encoding the whole input signal. Our approach is flexible enough to scale gracefully according to the needs of the forecasting task under consideration. In its most straight-forward and general version, we infer a potentially fully connected graph to model the interactions between time series, which allows us to obtain competitive forecast accuracy compared with the state-of-the-art in graph neural networks for forecasting. In addition, whereas previous latent graph inference methods scale O(N^2) w.r.t. the number of nodes N (representing the time series), we show how to configure our approach to cater for the scale of modern time series panels. By assuming the inferred graph to be bipartite where one partition consists of the original N nodes and we introduce K nodes (taking inspiration from low-rank-decompositions) we reduce the time complexity of our procedure to O(NK). This allows us to leverage the dependency structure with a small trade-off in forecasting accuracy. We demonstrate the effectiveness of our method for a variety of datasets where it performs better or very competitively to previous methods under both the fully connected and bipartite assumptions."}}
{"id": "XCs9rM255KZ", "cdate": 1621629806581, "mdate": null, "content": {"title": "Neural Flows: Efficient Alternative to Neural ODEs", "abstract": "Neural ordinary differential equations describe how values change in time. This is the reason why they gained importance in modeling sequential data, especially when the observations are made at irregular intervals. In this paper we propose an alternative by directly modeling the solution curves - the flow of an ODE - with a neural network. This immediately eliminates the need for expensive numerical solvers while still maintaining the modeling capability of neural ODEs. We propose several flow architectures suitable for different applications by establishing precise conditions on when a function defines a valid flow. Apart from computational efficiency, we also provide empirical evidence of favorable generalization performance via applications in time series modeling, forecasting, and density estimation."}}
{"id": "zTALdBUUrqQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Normalizing Kalman Filters for Multivariate Time Series Analysis", "abstract": "This paper tackles the modelling of large, complex and multivariate time series panels in a probabilistic setting. To this extent, we present a novel approach reconciling classical state space models with deep learning methods. By augmenting state space models with normalizing flows, we mitigate imprecisions stemming from idealized assumptions in state space models. The resulting model is highly flexible while still retaining many of the attractive properties of state space models, e.g., uncertainty and observation errors are properly accounted for, inference is tractable, sampling is efficient, good generalization performance is observed, even in low data regimes. We demonstrate competitiveness against state-of-the-art deep learning methods on the tasks of forecasting real world data and handling varying levels of missing data."}}
{"id": "fEzj8KrhL51", "cdate": 1577836800000, "mdate": null, "content": {"title": "Neural forecasting: Introduction and literature overview", "abstract": "Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the field: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature."}}
{"id": "c5SorBcBo9F", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deep Rao-Blackwellised Particle Filters for Time Series Forecasting", "abstract": "This work addresses efficient inference and learning in switching Gaussian linear dynamical systems using a Rao-Blackwellised particle filter and a corresponding Monte Carlo objective. To improve the forecasting capabilities, we extend this classical model by conditionally linear state-to-switch dynamics, while leaving the partial tractability of the conditional Gaussian linear part intact. Furthermore, we use an auxiliary variable approach with a decoder-type neural network that allows for more complex non-linear emission models and multivariate observations. We propose a Monte Carlo objective that leverages the conditional linearity by computing the corresponding conditional expectations in closed-form and a suitable proposal distribution that is factorised similarly to the optimal proposal distribution. We evaluate our approach on several popular time series forecasting datasets as well as image streams of simulated physical systems. Our results show improved forecasting performance compared to other deep state-space model approaches."}}
{"id": "7iI5gIYD2LI", "cdate": 1577836800000, "mdate": null, "content": {"title": "GluonTS: Probabilistic and Neural Time Series Modeling in Python", "abstract": "We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simplifies the time series modeling pipeline by providing the necessary components and tools for quick model development, efficient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms."}}
{"id": "-Jj7gKMWC7h", "cdate": 1577836800000, "mdate": null, "content": {"title": "Elastic Machine Learning Algorithms in Amazon SageMaker", "abstract": "There is a large body of research on scalable machine learning (ML). Nevertheless, training ML models on large, continuously evolving datasets is still a difficult and costly undertaking for many companies and institutions. We discuss such challenges and derive requirements for an industrial-scale ML platform. Next, we describe the computational model behind Amazon SageMaker, which is designed to meet such challenges. SageMaker is an ML platform provided as part of Amazon Web Services (AWS), and supports incremental training, resumable and elastic learning as well as automatic hyperparameter optimization. We detail how to adapt several popular ML algorithms to its computational model. Finally, we present an experimental evaluation on large datasets, comparing SageMaker to several scalable, JVM-based implementations of ML algorithms, which we significantly outperform with regard to computation time and cost."}}
{"id": "GpmMR7cEDA", "cdate": 1546300800000, "mdate": null, "content": {"title": "Probabilistic Forecasting with Spline Quantile Function RNNs", "abstract": "In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural n..."}}
