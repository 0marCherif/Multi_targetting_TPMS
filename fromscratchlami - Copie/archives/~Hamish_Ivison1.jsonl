{"id": "IfmvVnVLSIG", "cdate": 1708455479210, "mdate": 1708455479210, "content": {"title": "OLMo: Accelerating the Science of Language Models", "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation."}}
{"id": "WtW_s7EDWPe", "cdate": 1663850460635, "mdate": null, "content": {"title": "Data-Efficient Finetuning Using Cross-Task Nearest Neighbors", "abstract": "Language models trained on massive prompted multitask datasets like T0 (Sanh et al., 2021) or FLAN (Wei et al., 2021) can generalize to tasks unseen during training. We show that training on a carefully chosen subset of instances can outperform training on all available data on a variety of datasets. We assume access to a small number (250-1000) of unlabeled target task instances, select their nearest neighbors from a pool of multitask data, and use the retrieved data to train target task specific models. Our method is more data-efficient than training a single multitask model, while still outperforming it by large margins. We evaluate across a diverse set of tasks not in the multitask pool we retrieve from, including those used to evaluate T0 and in addition, more complex tasks including legal and scientific document QA. We retrieve small subsets of P3 (the collection of prompted datasets from which T0\u2019s training data was sampled) and finetune T5 models that outperform the 3-billion parameter variant of T0 (T0-3B) by 8-30% on 11 out of 12 evaluation datasets while using at most 2% of the data used to train T0-3B. These models also provide a better initialization than T0-3B for few-shot finetuning on target-task data, as shown by a 3-23% relative improvement over few-shot finetuned T0-3B models on 8 datasets."}}
{"id": "vZ9MG2C_AYF", "cdate": 1640995200000, "mdate": 1681746230301, "content": {"title": "Hyperdecoders: Instance-specific decoders for multi-task NLP", "abstract": ""}}
{"id": "phTrtP_Qz4", "cdate": 1640995200000, "mdate": 1673990368886, "content": {"title": "Data-Efficient Finetuning Using Cross-Task Nearest Neighbors", "abstract": ""}}
{"id": "Y1P5nWP0fl", "cdate": 1640995200000, "mdate": 1672883102250, "content": {"title": "HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation", "abstract": ""}}
{"id": "z85DoKLkV16", "cdate": 1609459200000, "mdate": 1680530335219, "content": {"title": "Local Interpretations for Explainable Natural Language Processing: A Survey", "abstract": ""}}
