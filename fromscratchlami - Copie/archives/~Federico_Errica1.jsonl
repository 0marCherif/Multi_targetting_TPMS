{"id": "rWPQnTubL1u", "cdate": 1679902220365, "mdate": 1679902220365, "content": {"title": "Deep Graph Networks for Drug Repurposing with Multi-Protein Targets", "abstract": "In the early phases of the COVID-19 pandemic, repurposing of drugs approved for use in other diseases helped counteract the aggressiveness of the virus. Therefore, the availability of effective and flexible methodologies to speed up and prioritize the repurposing process is fundamental to tackle present and future challenges to worldwide health. This work addresses the problem of drug repurposing through the lens of deep learning for graphs, by designing an architecture that exploits both structural and biological information to propose a reduced set of drugs that may be effective against an unknown disease. Our main contribution is a method to repurpose a drug against multiple proteins, rather than the most common single-drug/single-protein setting. The method leverages graph embeddings to encode the relevant proteins' and drugs' information based on gene ontology data and structural similarities. Finally, we publicly release a comprehensive and unified data repository for graph-based analysis to foster further studies on COVID-19 and drug repurposing. We empirically validate the proposed approach in a general drug repurposing setting, showing that it generalizes better than single protein repurposing schemes. We conclude the manuscript with an exemplified application of our method to the COVID-19 use case. All source code is publicly available."}}
{"id": "Rupm2vTg1pe", "cdate": 1632875629307, "mdate": null, "content": {"title": "The Infinite Contextual Graph Markov Model", "abstract": "The Contextual Graph Markov Model is a deep, unsupervised, and probabilistic model for graphs that is trained incrementally on a layer-by-layer basis. As with most Deep Graph Networks, an inherent limitation is the lack of an automatic mechanism to choose the size of each layer's latent representation. In this paper, we circumvent the problem by extending the Contextual Graph Markov Model with Hierarchical Dirichlet Processes. The resulting model for graphs can automatically adjust the complexity of each layer without the need to perform an extensive model selection. To improve the scalability of the method, we introduce a novel approximated inference procedure that better deals with larger graph topologies. The quality of the learned unsupervised representations is then evaluated across a set of eight graph classification tasks, showing competitive performances against end-to-end supervised methods. The analysis is complemented by studies on the importance of depth, hyper-parameters, and compression of the graph embeddings. We believe this to be an important step towards the theoretically grounded and automatic construction of deep probabilistic architectures for graphs."}}
{"id": "OOgfgnmjvTZ", "cdate": 1623571555768, "mdate": 1623571555768, "content": {"title": "Graph Mixture Density Networks", "abstract": "We introduce the Graph Mixture Density Networks, a new family of machine learning models that can fit multimodal output distributions conditioned on graphs of arbitrary topology. By combining ideas from mixture models and graph representation learning, we address a broader class of challenging conditional density estimation problems that rely on structured data. In this respect, we evaluate our method on a new benchmark application that leverages random graphs for stochastic epidemic simulations. We show a significant improvement in the likelihood of epidemic outcomes when taking into account both multimodality and structure. The empirical analysis is complemented by two real-world regression tasks showing the effectiveness of our approach in modeling the output prediction uncertainty. Graph Mixture Density Networks open appealing research opportunities in the study of structure-dependent phenomena that exhibit non-trivial conditional output distributions."}}
{"id": "FrRsRT957i", "cdate": 1623571489686, "mdate": 1623571489686, "content": {"title": "Modeling Edge Features with Deep Bayesian Graph Networks", "abstract": "We propose an extension of the Contextual Graph Markov Model, a deep and probabilistic machine learning model for graphs, to model the distribution of edge features. Our approach is architectural, as we introduce an additional Bayesian network mapping edge features into discrete states to be used by the original model. In doing so, we are also able to build richer graph representations even in the absence of edge features, which is confirmed by the performance improvements on standard graph classification benchmarks. Moreover, we successfully test our proposal in a graph regression scenario where edge features are of fundamental importance, and we show that the learned edge representation provides substantial performance improvements against the original model on three link prediction tasks. By keeping the computational complexity linear in the number of edges, the proposed model is amenable to large-scale graph processing."}}
{"id": "losAQJF9Jou", "cdate": 1577836800000, "mdate": null, "content": {"title": "Accelerating the identification of informative reduced representations of proteins with deep learning for graphs", "abstract": "The limits of molecular dynamics (MD) simulations of macromolecules are steadily pushed forward by the relentless developments of computer architectures and algorithms. This explosion in the number and extent (in size and time) of MD trajectories induces the need of automated and transferable methods to rationalise the raw data and make quantitative sense out of them. Recently, an algorithmic approach was developed by some of us to identify the subset of a protein's atoms, or mapping, that enables the most informative description of it. This method relies on the computation, for a given reduced representation, of the associated mapping entropy, that is, a measure of the information loss due to the simplification. Albeit relatively straightforward, this calculation can be time consuming. Here, we describe the implementation of a deep learning approach aimed at accelerating the calculation of the mapping entropy. The method relies on deep graph networks, which provide extreme flexibility in the input format. We show that deep graph networks are accurate and remarkably efficient, with a speedup factor as large as $10^5$ with respect to the algorithmic computation of the mapping entropy. Applications of this method, which entails a great potential in the study of biomolecules when used to reconstruct its mapping entropy landscape, reach much farther than this, being the scheme easily transferable to the computation of arbitrary functions of a molecule's structure."}}
{"id": "j6s0__RmWdz", "cdate": 1577836800000, "mdate": null, "content": {"title": "A gentle introduction to deep learning for graphs", "abstract": "The adaptive processing of graph data is a long-standing research topic that has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is a tutorial introduction to the field of deep learning for graphs. It favors a consistent and progressive presentation of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view of the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. Moreover, it introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. We complement the methodological exposition with a discussion of interesting research challenges and applications in the field."}}
{"id": "Le7CRzT_A4l", "cdate": 1577836800000, "mdate": null, "content": {"title": "Theoretically Expressive and Edge-aware Graph Learning", "abstract": "We propose a new Graph Neural Network that combines recent advancements in the field. We give theoretical contributions by proving that the model is strictly more general than the Graph Isomorphism Network and the Gated Graph Neural Network, as it can approximate the same functions and deal with arbitrary edge values. Then, we show how a single node information can flow through the graph unchanged."}}
{"id": "3Peq3H7AQu", "cdate": 1577836800000, "mdate": null, "content": {"title": "Probabilistic Learning on Graphs via Contextual Architectures", "abstract": "We propose a novel methodology for representation learning on graph-structured data, in which a stack of Bayesian Networks learns different distributions of a vertex's neighbourhood. Through an incremental construction policy and layer-wise training, we can build deeper architectures with respect to typical graph convolutional neural networks, with benefits in terms of context spreading between vertices. First, the model learns from graphs via maximum likelihood estimation without using target labels. Then, a supervised readout is applied to the learned graph embeddings to deal with graph classification and vertex classification tasks, showing competitive results against neural models for graphs. The computational complexity is linear in the number of edges, facilitating learning on large scale data sets. By studying how depth affects the performances of our model, we discover that a broader context generally improves performances. In turn, this leads to a critical analysis of some benchmarks used in literature."}}
{"id": "HygDF6NFPB", "cdate": 1569439103100, "mdate": null, "content": {"title": "A Fair Comparison of Graph Neural Networks for Graph Classification", "abstract": "Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works.\nAs such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models."}}
{"id": "SJlNnhVYDr", "cdate": 1569438892294, "mdate": null, "content": {"title": "Soft Token Matching for Interpretable Low-Resource Classification", "abstract": "We propose a model to tackle classification tasks in the presence of very little training data. To this aim, we introduce a novel matching mechanism to focus on elements of the input by using vectors that represent semantically meaningful concepts for the task at hand.\nBy leveraging highlighted portions of the training data, a simple, yet effective, error boosting technique guides the learning process. In practice, it increases the error associated to relevant parts of the input by a given factor. Results on text classification tasks confirm the benefits of the proposed approach in both balanced and unbalanced cases, thus being of practical use when labeling new examples is expensive. In addition, the model is interpretable, as it allows for human inspection of the learned weights."}}
