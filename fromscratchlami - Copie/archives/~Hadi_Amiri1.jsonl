{"id": "vd55GfiY34", "cdate": 1688069088373, "mdate": 1688069088373, "content": {"title": "HuCurl: Human-induced Curriculum Discovery", "abstract": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks."}}
{"id": "LGTmlJ10Kes", "cdate": 1632875753817, "mdate": null, "content": {"title": "Curriculum Discovery through an Encompassing Curriculum Learning Framework", "abstract": "We describe a curriculum learning framework capable of discovering optimal curricula in addition to performing standard curriculum learning. We show that this framework encompasses existing curriculum learning approaches such as difficulty-based data sub-sampling, data pruning, and loss re-weighting. We employ the proposed framework to address the following key questions in curriculum learning research: (a) What is the best curriculum to train a given model on a given dataset? (b) What are the characteristics of optimal curricula for different datasets and different difficulty scoring functions? We show that our framework outperforms competing state-of-the-art curriculum learning approaches in natural language inference and other text classification tasks. In addition, exhaustive experiments illustrate the generalizability of the discovered curricula across the three datasets and two difficulty scoring functions."}}
{"id": "SJWhD7bObH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Serial Recall Effects in Neural Language Modeling", "abstract": "Hassan Hajipoor, Hadi Amiri, Maseud Rahgozar, Farhad Oroumchian. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "Hk-6cQZOWS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Vector of Locally Aggregated Embeddings for Text Representation", "abstract": "Hadi Amiri, Mitra Mohtarami. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "BJEYNXbuZr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Neural Self-Training through Spaced Repetition", "abstract": "Self-training is a semi-supervised learning approach for utilizing unlabeled data to create better learners. The efficacy of self-training algorithms depends on their data sampling techniques. The majority of current sampling techniques are based on predetermined policies which may not effectively explore the data space or improve model generalizability. In this work, we tackle the above challenges by introducing a new data sampling technique based on spaced repetition that dynamically samples informative and diverse unlabeled instances with respect to individual learner and instance characteristics. The proposed model is specifically effective in the context of neural models which can suffer from overfitting and high-variance gradients when trained with small amount of labeled data. Our model outperforms current semi-supervised learning approaches developed for neural networks on publicly-available datasets."}}
{"id": "HyZfHXZuZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Spotting Spurious Data with Neural Networks", "abstract": "Author Summary Given a body and an environment, what is the brain complexity needed in order to generate a desired set of behaviors? The general understanding is that the physical properties of the body and the environment correlate with the required brain complexity. More precisely, it has been pointed that naturally evolved intelligent systems tend to exploit their embodiment constraints and that this allows them to express complex behaviors with relatively concise brains. Although this principle of parsimonious control has been formulated quite some time ago, only recently one has begun to develop the formalism that is required for making quantitative statements on the sufficient brain complexity given embodiment constraints. In this work we propose a precise mathematical approach that links the physical and behavioral constraints of an agent to the required controller complexity. As controller architecture we choose a well-known artificial neural network, the conditional restricted Boltzmann machine, and define its complexity as the number of hidden units. We conduct experiments with a virtual six-legged walking creature, which provide evidence for the accuracy of the theoretical predictions."}}
{"id": "BJE3XfGObr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks", "abstract": ""}}
{"id": "ryWUY7bdWS", "cdate": 1451606400000, "mdate": null, "content": {"title": "The UMD CLPsych 2016 Shared Task System: Text Representation for Predicting Triage of Forum Posts about Mental Health", "abstract": ""}}
{"id": "HkbZue-uWr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Short Text Representation for Detecting Churn in Microblogs", "abstract": "Churn happens when a customer leaves a brand or stop using its services. Brands reduce their churn rates by identifying and retaining potential churners through customer retention campaigns. In this paper, we consider the problem of classifying micro-posts as churny or non-churny with respect to a given brand. Motivated by the recent success of recurrent neural networks (RNNs) in word representation, we propose to utilize RNNs to learn micro-post and churn indicator representations. We show that such representations improve the performance of churn detection in microblogs and lead to more accurate ranking of churny contents. Furthermore, in this research we show that state-of-the-art sentiment analysis approaches fail to identify churny contents. Experiments on Twitter data about three telco brands show the utility of our approach for this task."}}
{"id": "HkN3sogO-S", "cdate": 1451606400000, "mdate": null, "content": {"title": "Learning Text Pair Similarity with Context-sensitive Autoencoders", "abstract": ""}}
