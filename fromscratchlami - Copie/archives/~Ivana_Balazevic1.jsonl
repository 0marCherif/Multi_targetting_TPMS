{"id": "t8iB5Qc27N", "cdate": 1683893914680, "mdate": 1683893914680, "content": {"title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models", "abstract": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs."}}
{"id": "PpOtGYNVT6A", "cdate": 1601308356829, "mdate": null, "content": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning."}}
{"id": "gLWj29369lW", "cdate": 1601308098950, "mdate": null, "content": {"title": "Interpreting Knowledge Graph Relation Representation from Word Embeddings", "abstract": "Many models learn representations of knowledge graph data by exploiting its low-rank latent structure, encoding known relations between entities and enabling unknown facts to be inferred. To predict whether a relation holds between entities, embeddings are typically compared in the latent space following a relation-specific mapping. Whilst their predictive performance has steadily improved, how such models capture the underlying latent structure of semantic information remains unexplained. Building on recent theoretical understanding of word embeddings, we categorise knowledge graph relations into three types and for each derive explicit requirements of their representations. We show that empirical properties of relation representations and the relative performance of leading knowledge graph representation methods are justified by our analysis."}}
{"id": "LNbLsYrbq4T", "cdate": 1598877639210, "mdate": null, "content": {"title": "Learning the Prediction Distribution for Semi-Supervised Learning with Normalising Flows", "abstract": "As data volumes continue to grow, the labelling process increasingly becomes a bottleneck, creating demand for methods that leverage information from unlabelled data. Impressive results have been achieved in semi-supervised learning (SSL) for image classification, nearing fully supervised performance, with only a fraction of the data labelled. In this work, we propose a probabilistically principled general approach to SSL that considers the distribution over label predictions, for labels of different complexity, from \"one-hot\" vectors to binary vectors and images. Our method regularises an underlying supervised model, using a normalising flow that learns the posterior distribution over predictions for labelled data, to serve as a prior over the predictions on unlabelled data. We demonstrate the general applicability of this approach on a range of computer vision tasks with varying output complexity: classification, attribute prediction and image-to-image translation."}}
{"id": "bI0vgAzSysi", "cdate": 1598877561468, "mdate": null, "content": {"title": "What the Vec? Towards Probabilistically Grounded Embeddings", "abstract": "Word2Vec (W2V) and GloVe are popular, fast and efficient word embedding algorithms. Their embeddings are widely used and perform well on a variety of natural language processing tasks. Moreover, W2V has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and relatively simple model architecture, a theoretical understanding of what the embedding parameters of W2V and GloVe learn and why that is useful in downstream tasks has been lacking. We show that different interactions between PMI vectors reflect semantic word relationships, such as similarity and paraphrasing, that are encoded in low dimensional word embeddings under a suitable projection, theoretically explaining why embeddings of W2V and GloVe work. As a consequence, we also reveal an interesting mathematical interconnection between the considered semantic relationships themselves."}}
{"id": "qKyQFC2BJAGV", "cdate": 1598877452225, "mdate": null, "content": {"title": "Hypernetwork Knowledge Graph Embeddings", "abstract": "Knowledge graphs are graphical representations of large databases of facts, which typically suffer from incompleteness. Inferring missing relations (links) between entities (nodes) is the task of link prediction. A recent state-of-the-art approach to link prediction, ConvE, implements a convolutional neural network to extract features from concatenated subject and relation vectors. Whilst results are impressive, the method is unintuitive and poorly understood. We propose a hypernetwork architecture that generates simplified relation-specific convolutional filters that (i) outperforms ConvE and all previous approaches across standard datasets; and (ii) can be framed as tensor factorization and thus set within a well established family of factorization models for link prediction. We thus demonstrate that convolution simply offers a convenient computational means of introducing sparsity and parameter tying to find an effective trade-off between non-linear expressiveness and the number of parameters to learn."}}
{"id": "RvvBKhTjAsZ", "cdate": 1598787087579, "mdate": null, "content": {"title": "A Probabilistic Framework for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "abstract": "In semi-supervised learning (SSL), a rule to predict labels y for data x is learned from labelled data (x^l, y^l) and unlabelled samples x^u. Strong progress has been made by combining a variety of methods, some of which pertain to p(x), e.g. data augmentation that generates artificial samples from true x; whilst others relate to model outputs p(y|x), e.g. regularising predictions on unlabelled data to minimise entropy or induce mutual exclusivity. Focusing on the latter, we fill a gap in the standard text by introducing a unifying probabilistic model for discriminative semi-supervised learning, mirroring that for classical generative methods. We show that several SSL methods can be theoretically justified under our model as inducing approximate priors over predicted parameters of p(y|x). For tasks where labels represent binary attributes, our model leads to a principled approach to neuro-symbolic SSL, bridging the divide between statistical learning and logical rules."}}
{"id": "SygcSlHFvS", "cdate": 1569439810208, "mdate": null, "content": {"title": "On Understanding Knowledge Graph Representation", "abstract": "Many methods have been developed to represent knowledge graph data, which implicitly exploit low-rank latent structure in the data to encode known information and enable unknown facts to be inferred. To predict whether a relationship holds between entities, their embeddings are typically compared in the latent space following a relation-specific mapping. Whilst link prediction has steadily improved, the latent structure, and hence why such models capture semantic information, remains unexplained. We build on recent theoretical interpretation of word embeddings as a basis to consider an explicit structure for representations of relations between entities. For identifiable relation types, we are able to predict properties and justify the relative performance of leading knowledge graph representation methods, including their often overlooked ability to make independent predictions."}}
{"id": "SJe6uNSe8B", "cdate": 1567802485196, "mdate": null, "content": {"title": "Multi-relational Poincar\u00e9 Graph Embeddings", "abstract": "Hyperbolic embeddings have recently gained attention in machine learning due to their ability to represent hierarchical data more accurately and succinctly than their Euclidean analogues. However, multi-relational knowledge graphs often exhibit multiple simultaneous hierarchies, which current hyperbolic models do not capture. To address this, we propose a model that embeds multi-relational graph data in the Poincar\u00e9 ball model of hyperbolic space. Our Multi-Relational Poincar\u00e9 model (MuRP) learns relation-specific parameters to transform entity embeddings by M\u00f6bius matrix-vector multiplication and M\u00f6bius addition. Experiments on the hierarchical WN18RR knowledge graph show that our multi-relational Poincar\u00e9 embeddings outperform their Euclidean counterpart and existing embedding methods on the link prediction task, particularly at lower dimensionality."}}
