{"id": "EIvakhvxrj6", "cdate": 1640995200000, "mdate": 1668895554537, "content": {"title": "Eccentric regularization: minimizing hyperspherical energy without explicit projection", "abstract": "Several regularization methods have recently been introduced which force the latent activations of an autoencoder or deep neural network to conform to either a Gaussian or hyperspherical distribution, or to minimize the implicit rank of the distribution in latent space. In the present work, we introduce a simple and novel regularizing loss function which simulates a pairwise repulsive force between items and an attractive force of each item toward the origin. We show that minimizing this loss function in isolation achieves a hyperspherical distribution, and demonstrate its effectiveness as a regularizer for an image auto-encoder. Moreover, a reduction in the regularization parameter leads to a modest increase in the eccentricity of the distribution in latent space. This enhances image generation, and allows the eigenvectors of the covariance matrix to be extracted as deep principal components, which can be used for data analysis, image generation, visualization and downstream classification."}}
{"id": "c0AD3ll9Wyv", "cdate": 1632875599045, "mdate": null, "content": {"title": "Can Label-Noise Transition Matrix Help to Improve Sample Selection and Label Correction?", "abstract": "Existing methods for learning with noisy labels can be generally divided into two categories: (1) sample selection and label correction based on the memorization effect of neural networks; (2) loss correction with the transition matrix. So far, the two categories of methods have been studied independently because they are designed according to different philosophies, i.e., the memorization effect is a property of the neural networks independent of label noise while the transition matrix is exploited to model the distribution of label noise. In this paper, we take a first step in unifying these two paradigms by showing that modelling the distribution of label noise with the transition matrix can also help sample selection and label correction, which leads to better robustness against different types of noise. More specifically, we first train a network with the loss corrected by the transition matrix and then use the confidence of the estimated clean class posterior from the network to select and re-label instances. Our proposed method demonstrates strong robustness on multiple benchmark datasets under various types of noise."}}
{"id": "cH1Rs2Hczzy", "cdate": 1609459200000, "mdate": 1631716505801, "content": {"title": "Eccentric Regularization: Minimizing Hyperspherical Energy without explicit projection", "abstract": "Several regularization methods have recently been introduced which force the latent activations of an autoencoder or deep neural network to conform to either a Gaussian or hyperspherical distribution, or to minimize the implicit rank of the distribution in latent space. In the present work, we introduce a novel regularizing loss function which simulates a pairwise repulsive force between items and an attractive force of each item toward the origin. We show that minimizing this loss function in isolation achieves a hyperspherical distribution. Moreover, when used as a regularizing term, the scaling factor can be adjusted to allow greater flexibility and tolerance of eccentricity, thus allowing the latent variables to be stratified according to their relative importance, while still promoting diversity. We apply this method of Eccentric Regularization to an autoencoder, and demonstrate its effectiveness in image generation, representation learning and downstream classification tasks."}}
{"id": "LJIIvK3ewhG", "cdate": 1609459200000, "mdate": 1631233056892, "content": {"title": "Provably End-to-end Label-Noise Learning without Anchor Points", "abstract": "In label-noise learning, the transition matrix plays a key role in building statistically consistent classifiers. Existing consistent estimators for the transition matrix have been developed by exploiting anchor points. However, the anchor-point assumption is not always satisfied in real scenarios. In this paper, we propose an end-to-end framework for solving label-noise learning without anchor points, in which we simultaneously optimize two objectives: the cross entropy loss between the noisy label and the predicted probability by the neural network, and the volume of the simplex formed by the columns of the transition matrix. Our proposed framework can identify the transition matrix if the clean class-posterior probabilities are sufficiently scattered. This is by far the mildest assumption under which the transition matrix is provably identifiable and the learned classifier is statistically consistent. Experimental results on benchmark datasets demonstrate the effectiveness and robustness of the proposed method."}}
{"id": "2LmcjLDsPjz", "cdate": 1609459200000, "mdate": 1631233049928, "content": {"title": "Provably End-to-end Label-noise Learning without Anchor Points", "abstract": "In label-noise learning, the transition matrix plays a key role in building statistically consistent classifiers. Existing consistent estimators for the transition matrix have been developed by exp..."}}
