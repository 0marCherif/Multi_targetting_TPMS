{"id": "kbFvo8IXuuT", "cdate": 1546300800000, "mdate": null, "content": {"title": "Adversarial Bootstrapping for Dialogue Model Training", "abstract": "Open domain neural dialogue models, despite their successes, are known to produce responses that lack relevance, diversity, and in many cases coherence. These shortcomings stem from the limited ability of common training objectives to directly express these properties as well as their interplay with training datasets and model architectures. Toward addressing these problems, this paper proposes bootstrapping a dialogue response generator with an adversarially trained discriminator. The method involves training a neural generator in both autoregressive and traditional teacher-forcing modes, with the maximum likelihood loss of the auto-regressive outputs weighted by the score from a metric-based discriminator model. The discriminator input is a mixture of ground truth labels, the teacher-forcing outputs of the generator, and distractors sampled from the dataset, thereby allowing for richer feedback on the autoregressive outputs of the generator. To improve the calibration of the discriminator output, we also bootstrap the discriminator with the matching of the intermediate features of the ground truth and the generator's autoregressive output. We explore different sampling and adversarial policy optimization strategies during training in order to understand how to encourage response diversity without sacrificing relevance. Our experiments shows that adversarial bootstrapping is effective at addressing exposure bias, leading to improvement in response relevance and coherence. The improvement is demonstrated with the state-of-the-art results on the Movie and Ubuntu dialogue datasets with respect to human evaluations and BLUE, ROGUE, and distinct n-gram scores."}}
{"id": "WuruZW5dAw", "cdate": 1546300800000, "mdate": null, "content": {"title": "Multi-turn Dialogue Response Generation with Autoregressive Transformer Models", "abstract": "Neural dialogue models, despite their successes, still suffer from lack of relevance, diversity, and in many cases coherence in their generated responses. These issues can attributed to reasons including (1) short-range model architectures that capture limited temporal dependencies, (2) limitations of the maximum likelihood training objective, (3) the concave entropy profile of dialogue datasets resulting in short and generic responses, and (4) the out-of-vocabulary problem leading to generation of a large number of <UNK> tokens. On the other hand, transformer-based models such as GPT-2 have demonstrated an excellent ability to capture long-range structures in language modeling tasks. In this paper, we present DLGNet, a transformer-based model for dialogue modeling. We specifically examine the use of DLGNet for multi-turn dialogue response generation. In our experiments, we evaluate DLGNet on the open-domain Movie Triples dataset and the closed-domain Ubuntu Dialogue dataset. DLGNet models, although trained with only the maximum likelihood objective, achieve significant improvements over state-of-the-art multi-turn dialogue models. They also produce best performance to date on the two datasets based on several metrics, including BLEU, ROUGE, and distinct n-gram. Our analysis shows that the performance improvement is mostly due to the combination of (1) the long-range transformer architecture with (2) the injection of random informative paddings. Other contributing factors include the joint modeling of dialogue context and response, and the 100% tokenization coverage from the byte pair encoding (BPE)."}}
{"id": "7H3b6hCMQzk", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Persona-based Multi-turn Conversation Model in an Adversarial Learning Framework", "abstract": "In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to multi-turn dialogue by modifying the state-of-the-art hredGAN architecture. To achieve this, we introduce an additional input modality into the encoder and decoder of hredGAN to capture other attributes such as speaker identity, location, sub-topics, and other external attributes that might be available from the corpus of human-to-human interactions. The resulting persona hredGAN ($phredGAN$) shows better performance than both the existing persona-based Seq2Seq and hredGAN models when those external attributes are available in a multi-turn dialogue corpus. This superiority is demonstrated on TV drama series with character consistency (such as Big Bang Theory and Friends) and customer service interaction datasets such as Ubuntu dialogue corpus in terms of perplexity, BLEU, ROUGE, and Distinct n-gram scores."}}
{"id": "SJxzPsAqFQ", "cdate": 1538087769891, "mdate": null, "content": {"title": "Multi-turn Dialogue Response Generation in an Adversarial Learning Framework", "abstract": "We propose an adversarial learning approach to the generation of multi-turn dialogue responses. Our proposed framework, hredGAN, is based on conditional generative adversarial networks (GANs). The GAN's generator is a modified hierarchical recurrent encoder-decoder network (HRED) and the discriminator is a word-level bidirectional RNN that shares context and word embedding with the generator. During inference, noise samples conditioned on the dialogue history are used to perturb the generator's latent space to generate several possible responses. The final response is the one ranked best by the discriminator. The hredGAN shows major advantages over existing methods: (1) it generalizes better than networks trained using only the log-likelihood criterion, and (2) it generates longer, more informative and more diverse responses with high utterance and topic relevance even with limited training data. This superiority is demonstrated on the Movie triples and Ubuntu dialogue datasets with both the automatic and human evaluations."}}
{"id": "rkeYUsRqKQ", "cdate": 1538087761417, "mdate": null, "content": {"title": "An Adversarial Learning Framework for a Persona-based Multi-turn Dialogue Model", "abstract": "In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) $phredGAN_a$, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) $phredGAN_d$, a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of $phredGAN$ on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset)."}}
{"id": "BD2Xribofcd", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Persona-Based Multi-turn Conversation Model in an Adversarial Learning Framework", "abstract": "In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to multi-turn dialogue by modifying the state-of-the-art hredGAN architecture. To achieve this, we introduce an additional input modality into the encoder and decoder of hredGAN to capture other attributes such as speaker identity, location, sub-topics, and other external attributes that might be available from the corpus of human-to-human interactions. The resulting persona hredGAN (phredGAN) shows better performance than both the existing persona-based Seq2Seq and hredGAN models when those external attributes are available in a multi-turn dialogue corpus. This superiority is demonstrated on TV drama series with character consistency (such as Big Bang Theory and Friends) and customer service interaction datasets such as Ubuntu dialogue corpus in terms of perplexity, BLEU, ROUGE, and Distinct n-gram scores."}}
{"id": "bXChE6r9KN_", "cdate": 1483228800000, "mdate": null, "content": {"title": "Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural Network", "abstract": "Advanced driver assistance systems (ADAS) can be significantly improved with effective driver action prediction (DAP). Predicting driver actions early and accurately can help mitigate the effects of potentially unsafe driving behaviors and avoid possible accidents. In this paper, we formulate driver action prediction as a timeseries anomaly prediction problem. While the anomaly (driver actions of interest) detection might be trivial in this context, finding patterns that consistently precede an anomaly requires searching for or extracting features across multi-modal sensory inputs. We present such a driver action prediction system, including a real-time data acquisition, processing and learning framework for predicting future or impending driver action. The proposed system incorporates camera-based knowledge of the driving environment and the driver themselves, in addition to traditional vehicle dynamics. It then uses a deep bidirectional recurrent neural network (DBRNN) to learn the correlation between sensory inputs and impending driver behavior achieving accurate and high horizon action prediction. The proposed system performs better than other existing systems on driver action prediction tasks and can accurately predict key driver actions including acceleration, braking, lane change and turning at durations of 5sec before the action is executed by the driver."}}
{"id": "d7Q31uUVBQe", "cdate": 1420070400000, "mdate": null, "content": {"title": "Ergodic capacity analysis of cooperative amplify-and-forward relay networks over generalized fading channels", "abstract": "In this research article, we have developed a novel moment generating function (that exploits an exponential\u2010type integral representation for the logarithmic function) and a cumulative distribution f..."}}
{"id": "Zt7Ibz04iXF", "cdate": 1388534400000, "mdate": null, "content": {"title": "A unified framework for the performance analyses of diversity energy detectors over fading channels", "abstract": ""}}
{"id": "oL-L4TBR_lD", "cdate": 1325376000000, "mdate": null, "content": {"title": "New series representations for generalized Nuttall Q-function with applications", "abstract": "In this article, we derive six distinct series representations of the generalized Nuttall Q-function. Interestingly, some of these expressions generalise some of the recent results on series representation of the Marcum Q-function since the Marcum Q-function is a special case of the Nuttall Q-function. To the best of our knowledge, five of these representations are novel and the other one is included for the completeness of the article. The performances of these representations are compared using the magnitude of the truncation error which gives a measure of the accuracy and rate of convergence and allow us to show the strength of each representation. We then discussed the area of applications of our result. We showed that our Nuttall Q-function representations and their corresponding special case (Marcum Q-function) (though not limited to only these), can be used in (i) the computation of outage probability of wireless communication system with co-channel interferers, (ii) the evaluation of detection probability of energy detector over generalized fading channels in cognitive radio system, and (iii) the computation of cumulative distribution of selection diversity combining in correlated Nakagami-m fading channel. In all these areas of application, our solution has no restriction on any parameter of interest."}}
