{"id": "PCqsA8kvnk", "cdate": 1683559893688, "mdate": null, "content": {"title": "Learning active tactile perception through belief-space control", "abstract": "Robot operating in an open world can encounter\nnovel objects with unknown physical properties, such as mass,\nfriction, or size. It is desirable to be able to sense those\nproperty through contact-rich interaction, before performing\ndownstream tasks with the objects. We propose a method for\nautonomously learning active tactile perception policies, by\nlearning a generative world model leveraging a differentiable\nbayesian filtering algorithm, and designing an information-\ngathering model predictive controller. We test the method on\nthree simulated tasks: mass estimation, height estimation and\ntoppling height estimation. Our method is able to discover\npolicies which gather information about the desired property\nin an intuitive manner."}}
{"id": "_4tcqR3nQII", "cdate": 1651329599211, "mdate": null, "content": {"title": "Learning active tactile perception through belief-space control", "abstract": "Robot operating in an open world can encounter\nnovel objects with unknown physical properties, such as mass,\nfriction, or size. It is desirable to be able to sense those\nproperty through contact-rich interaction, before performing\ndownstream tasks with the objects. We propose a method for\nautonomously learning active tactile perception policies, by\nlearning a generative world model leveraging a differentiable\nbayesian filtering algorithm, and designing an information-\ngathering model predictive controller. We test the method on\nthree simulated tasks: mass estimation, height estimation and\ntoppling height estimation. Our method is able to discover\npolicies which gather information about the desired property\nin an intuitive manner."}}
{"id": "WtqqNoMYl2o", "cdate": 1640995200000, "mdate": 1683559422437, "content": {"title": "Finger-STS: Combined Proximity and Tactile Sensing for Robotic Manipulation", "abstract": "This paper introduces and develops novel touch sensing technologies that enable robots to better sense and react to to intermittent contact interactions. We present Finger-STS, a robotic finger embodiment of the See-Through-your-Skin (STS) sensor that can capture 1) an \u201cin the hand\u201d visual perspective of an object that is being manipulated and 2) a high resolution tactile imprint of the contact geometry. We demonstrate the value of the sensor on a Bead Maze task. Here the multimodal feedback provided by the Finger-STS is leveraged by a robot to locate a bead visually and to guide it across a wire in response to tactile cues, with no additional sensing or planning required. To achieve this, we introduce a set of relevant visuotactile operations using computer vision-based algorithms. In particular, we sense the proximity of the object relative to the sensor as well as the nature of contact as a high resolution stick/slip vector field tracking the object motion in the finger."}}
{"id": "_Ki1Ca3OHvJ", "cdate": 1609459200000, "mdate": 1683559422488, "content": {"title": "Multimodal dynamics modeling for off-road autonomous vehicles", "abstract": "Dynamics modeling in outdoor and unstructured environments is difficult because different elements in the environment interact with the robot in ways that can be hard to predict. Leveraging multiple sensors to perceive maximal information about the robot\u2019s environment is thus crucial when building a model to perform predictions about the robot\u2019s dynamics with the goal of doing motion planning. We design a model capable of long-horizon motion predictions, leveraging vision, lidar and proprioception, which is robust to arbitrarily missing modalities at test time. We demonstrate in simulation that our model is able to leverage vision to predict traction changes. We then test our model using a real-world challenging dataset of a robot navigating through a forest, performing predictions in trajectories unseen during training. We try different modality combinations at test time and show that, while our model performs best when all modalities are present, it is still able to perform better than the baseline even when receiving only raw vision input and no proprioception, as well as when only receiving proprioception. Overall, our study demonstrates the importance of leveraging multiple sensors when doing dynamics modeling in outdoor conditions."}}
{"id": "vgJKL-AHMY", "cdate": 1577836800000, "mdate": 1683559422632, "content": {"title": "Multimodal dynamics modeling for off-road autonomous vehicles", "abstract": "Dynamics modeling in outdoor and unstructured environments is difficult because different elements in the environment interact with the robot in ways that can be hard to predict. Leveraging multiple sensors to perceive maximal information about the robot's environment is thus crucial when building a model to perform predictions about the robot's dynamics with the goal of doing motion planning. We design a model capable of long-horizon motion predictions, leveraging vision, lidar and proprioception, which is robust to arbitrarily missing modalities at test time. We demonstrate in simulation that our model is able to leverage vision to predict traction changes. We then test our model using a real-world challenging dataset of a robot navigating through a forest, performing predictions in trajectories unseen during training. We try different modality combinations at test time and show that, while our model performs best when all modalities are present, it is still able to perform better than the baseline even when receiving only raw vision input and no proprioception, as well as when only receiving proprioception. Overall, our study demonstrates the importance of leveraging multiple sensors when doing dynamics modeling in outdoor conditions."}}
{"id": "bM3XxOfScyV", "cdate": 1577836800000, "mdate": 1681747112573, "content": {"title": "Vision-Based Goal-Conditioned Policies for Underwater Navigation in the Presence of Obstacles", "abstract": ""}}
{"id": "KRpCilXPCld", "cdate": 1577836800000, "mdate": 1681747112574, "content": {"title": "Vision-Based Goal-Conditioned Policies for Underwater Navigation in the Presence of Obstacles", "abstract": "We present Nav2Goal, a data-efficient and end-to-end learning method for goal-conditioned visual navigation. Our technique is used to train a navigation policy that enables a robot to navigate close to sparse geographic waypoints provided by a user without any prior map, all while avoiding obstacles and choosing paths that cover user-informed regions of interest. Our approach is based on recent advances in conditional imitation learning. General-purpose, safe and informative actions are demonstrated by a human expert. The learned policy is subsequently extended to be goal-conditioned by training with hindsight relabelling, guided by the robot's relative localization system, which requires no additional manual annotation. We deployed our method on an underwater vehicle in the open ocean to collect scientifically relevant data of coral reefs, which allowed our robot to operate safely and autonomously, even at very close proximity to the coral. Our field deployments have demonstrated over a kilometer of autonomous visual navigation, where the robot reaches on the order of 40 waypoints, while collecting scientifically relevant data. This is done while travelling within 0.5 m altitude from sensitive corals and exhibiting significant learned agility to overcome turbulent ocean conditions and to actively avoid collisions."}}
{"id": "8f0R6y_fma", "cdate": 1577836800000, "mdate": 1683559422527, "content": {"title": "Automatic three-dimensional mapping for tree diameter measurements in inventory operations", "abstract": "Forestry is a major industry in many parts of the world, yet this potential domain of application area has been overlooked by the robotics community. For instance, forest inventory, a cornerstone of ..."}}
{"id": "rMnvb2BvI1", "cdate": 1546300800000, "mdate": 1683559422452, "content": {"title": "Automatic 3D Mapping for Tree Diameter Measurements in Inventory Operations", "abstract": "Forestry is a major industry in many parts of the world. It relies on forest inventory, which consists of measuring tree attributes. We propose to use 3D mapping, based on the iterative closest point algorithm, to automatically measure tree diameters in forests from mobile robot observations. While previous studies showed the potential for such technology, they lacked a rigorous analysis of diameter estimation methods in challenging forest environments. Here, we validated multiple diameter estimation methods, including two novel ones, in a new varied dataset of four different forest sites, 11 trajectories, totaling 1458 tree observations and 1.4 hectares. We provide recommendations for the deployment of mobile robots in a forestry context. We conclude that our mapping method is usable in the context of automated forest inventory, with our best method yielding a root mean square error of 3.45 cm for our whole dataset, and 2.04 cm in ideal conditions consisting of mature forest with well spaced trees."}}
