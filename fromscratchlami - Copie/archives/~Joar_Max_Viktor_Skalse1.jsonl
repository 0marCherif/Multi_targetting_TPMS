{"id": "Vcvg76ZmcZt", "cdate": 1676827078635, "mdate": null, "content": {"title": "On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks", "abstract": "In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms."}}
{"id": "3tgegVVh2j6", "cdate": 1668734796647, "mdate": null, "content": {"title": "All\u2019s Well That Ends Well: Avoiding Side Effects with Distance-Impact Penalties", "abstract": "Misspecifying the reward function of a reinforcement learning agent may cause catastrophic side effects.\nIn this work, we investigate \\textit{distance-impact penalties}: a general-purpose auxiliary reward based on a state-distance measure that captures, and thus can be used to penalise, side effects. We prove that the size of the penalty depends only on an agent's final impact on the environment.\nDistance-impact penalties are scalable, general, and immediately compatible with model-free algorithms.\nWe analyse the sensitivity of an agent's behaviour to the choice of penalty, expanding results about reward-shaping, proving sufficient and necessary conditions for policy-optimality to be invariant to misspecification, and providing error bounds for optimal policies.\nFinally, we empirically investigate distance-impact penalties in a range of grid-world environments, demonstrating their ability to prevent side effects whilst permitting task completion."}}
{"id": "Hn21kZHiCK", "cdate": 1668734795781, "mdate": null, "content": {"title": "A general framework for reward function distances", "abstract": "In reward learning, it is helpful to be able to measure distances between reward functions, for example to evaluate learned reward models. Using simple metrics such as L^2 distances is not ideal because reward functions that are equivalent in terms of their optimal policies can nevertheless have high L^2 distance. EPIC and DARD are distances specifically designed for reward functions that address this by being invariant under certain transformations that leave optimal policies unchanged. However, EPIC and DARD are designed in an ad-hoc manner, only consider a subset of relevant reward transformations, and suffer from serious pathologies in some settings. In this paper, we define a general class of reward function distance metrics, of which EPIC is a special case. This framework lets as address all these issues with EPIC and DARD, and allows for the development of reward function distance metrics in a more principled manner."}}
{"id": "5l1NgpzAfH", "cdate": 1668734780592, "mdate": null, "content": {"title": "The Reward Hypothesis is False", "abstract": "The \\emph{reward hypothesis} is the hypothesis that \\enquote{all of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal}\\citep{sutton2018reinforcement}.\nIn this paper, we will argue that this hypothesis is false.\nWe will look at three natural classes of reinforcement learning tasks (multi-objective reinforcement learning, risk-averse reinforcement learning, and modal reinforcement learning), and then prove mathematically that these tasks cannot be expressed using any scalar, Markovian reward function.\nWe thus disprove the reward hypothesis by providing many examples of tasks which are both natural and intuitive to describe, but which are nonetheless impossible to express using reward functions.\nIn the process, we provide necessary and sufficient conditions for when a multi-objective reinforcement learning problem can be reduced to ordinary, scalar reward reinforcement learning. We also call attention to a new class of reinforcement learning problems (namely those we call \\enquote{modal} problems), which have so far not been given any systematic treatment in the reinforcement learning literature."}}
{"id": "rDdh2bJ4nV", "cdate": 1668734779968, "mdate": null, "content": {"title": "Misspecification in Inverse Reinforcement Learning", "abstract": "The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\\pi$.\nTo do this, we need a model of how $\\pi$ relates to $R$. In the current literature, the most common models are \\emph{optimality}, \\emph{Boltzmann rationality}, and \\emph{causal entropy maximisation}.\nOne of the primary motivations behind IRL is to infer human preferences from human behaviour.\nHowever, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are \\emph{misspecified}, which raises the worry that they might lead to unsound inferences if applied to real-world data.\nIn this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecification in IRL, together with formal tools that can be used to easily derive the misspecification robustness of new IRL models."}}
{"id": "M4UxoupR3az", "cdate": 1663850021239, "mdate": null, "content": {"title": "The Reward Hypothesis is False", "abstract": "The reward hypothesis is the hypothesis that \"all of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal\". In this paper, we will argue that this hypothesis is false. We will look at three natural classes of reinforcement learning tasks (multi-objective reinforcement learning, risk-averse reinforcement learning, and modal reinforcement learning), and then prove mathematically that these tasks cannot be expressed using any scalar, Markovian reward function. We thus disprove the reward hypothesis by providing many examples of tasks which are both natural and intuitive to describe, but which are nonetheless impossible to express using reward functions. In the process, we provide necessary and sufficient conditions for when a multi-objective reinforcement learning problem can be reduced to ordinary, scalar reward reinforcement learning. We also call attention to a new class of reinforcement learning problems (namely those we call \"modal\" problems), which have so far not been given any systematic treatment in the reinforcement learning literature."}}
{"id": "yb3HOXO3lX2", "cdate": 1652737841570, "mdate": null, "content": {"title": "Defining and Characterizing Reward Gaming", "abstract": "We provide the first formal definition of \\textbf{reward hacking}, a phenomenon where optimizing an imperfect proxy reward function, $\\mathcal{\\tilde{R}}$, leads to poor performance according to the true reward function, $\\mathcal{R}$.  \nWe say that a proxy is \\textbf{unhackable} if increasing the expected proxy return can never decrease the expected true return.\nIntuitively, it might be possible to create an unhackable proxy by leaving some terms out of the reward function (making it ``narrower'') or overlooking fine-grained distinctions between roughly equivalent outcomes, but we show this is usually not the case.\nA key insight is that the linearity of reward (in state-action visit counts) makes unhackability a very strong condition. \nIn particular, for the set of all stochastic policies, two reward functions can only be unhackable if one of them is constant.\nWe thus turn our attention to deterministic policies and finite sets of stochastic policies, where non-trivial unhackable pairs always exist, and establish necessary and sufficient conditions for the existence of simplifications, an important special case of unhackability.\nOur results reveal a tension between using reward functions to specify narrow tasks and aligning AI systems with human values."}}
{"id": "eqRTPB134q0", "cdate": 1632875497550, "mdate": null, "content": {"title": "Invariance in Policy Optimisation and Partial Identifiability in Reward Learning", "abstract": "It is challenging to design a reward function for complex, real-world tasks. Reward learning algorithms let one instead infer a reward function from data. However, multiple reward functions often explain the data equally well, even in the limit of infinite data. Prior work has focused on situations where the reward function is uniquely recoverable, by introducing additional assumptions or data sources. By contrast, we formally characterise this partial identifiability for popular data sources such as demonstrations and trajectory preferences. We analyse the impact of this ambiguity on downstream tasks such as policy optimisation, including under shifts in environment dynamics. These results have implications for the practical design and selection of data sources for reward learning."}}
{"id": "cx2q4cOBnne", "cdate": 1621630060081, "mdate": null, "content": {"title": "Reinforcement Learning in Newcomblike Environments", "abstract": "Newcomblike decision problems have been studied extensively in the decision theory literature, but they have so far been largely absent in the reinforcement learning literature. In this paper we study value-based reinforcement learning algorithms in the Newcomblike setting, and answer some of the fundamental theoretical questions about the behaviour of such algorithms in these environments. We show that a value-based reinforcement learning agent cannot converge to a policy that is not \\emph{ratifiable}, i.e., does not only choose actions that are optimal given that policy. This gives us a powerful tool for reasoning about the limit behaviour of agents -- for example, it lets us show that there are Newcomblike environments in which a reinforcement learning agent cannot converge to any optimal policy. We show that a ratifiable policy always exists in our setting, but that there are cases in which a reinforcement learning agent normally cannot converge to it (and hence cannot converge at all). We also prove several results about the possible limit behaviours of agents in cases where they do not converge to any policy."}}
