{"id": "RmuPbRPSfQ", "cdate": 1672531200000, "mdate": 1681652008836, "content": {"title": "Quantile LSTM: A Robust LSTM for Anomaly Detection In Time Series Data", "abstract": ""}}
{"id": "7eINk14sK8U", "cdate": 1672531200000, "mdate": 1681652008945, "content": {"title": "An improved communication strategy in vehicular ad hoc networks: adaptive game theoretic modelling approach", "abstract": ""}}
{"id": "k5e6oQP2zHx", "cdate": 1663849999221, "mdate": null, "content": {"title": "QUANTILE-LSTM: A ROBUST LSTM FOR ANOMALY DETECTION", "abstract": "Anomalies refer to departure of systems and devices from their normal behaviour in standard operating conditions. An anomaly in an industrial device can indicate an upcoming failure, often in the temporal direction. In this paper, we make two contributions: 1) we estimate conditional quantiles, and consider three different ways to define anomalies based on the estimated quantiles and 2) use a new\nlearnable activation function in the popular Long Short Term Memory (LSTM) architecture to model temporal long-range dependency. In particular, we propose Parametrized Elliot Function (Parametric Elliot Function (PEF)) as an activation function inside LSTM, which saturates lately compared to sigmoid and tanh. The proposed algorithms are compared with other well known anomaly detection algorithms, such as Isolation Forest (iForest), Elliptic Envelope, Autoencoder,and modern Deep Learning models such as Deep Autoencoding Gaussian Mixture Model (DAGMM), Generative Adversarial Networks (GAN) etc. The algorithms are evaluated in terms of various performance metrics, such as precision and recall. The algorithms are experimented on multiple industrial timeseries datasets such as Yahoo, AWS, GE, and machine sensor. We have found the LSTM based quantile algorithms are very effective and outperformed the existing algorithms in identifying the anomalies.\n"}}
{"id": "uZqgLXOnYEE", "cdate": 1640995200000, "mdate": 1663588227203, "content": {"title": "AdaSwarm: Augmenting Gradient-Based Optimizers in Deep Learning With Swarm Intelligence", "abstract": "This paper introduces AdaSwarm, a novel gradient-free optimizer which has similar or even better performance than the Adam optimizer adopted in neural networks. In order to support our proposed AdaSwarm, a novel Exponentially weighted Momentum Particle Swarm Optimizer (EMPSO), is proposed. The ability of AdaSwarm to tackle optimization problems is attributed to its capability to perform good gradient approximations. We show that, the gradient of any function, differentiable or not, can be approximated by using the parameters of EMPSO. This is a novel technique to simulate GD which lies at the boundary between numerical methods and swarm intelligence. Mathematical proofs of the gradient approximation produced are also provided. AdaSwarm competes closely with several state-of-the-art (SOTA) optimizers. We also show that AdaSwarm is able to handle a variety of loss functions during backpropagation, including the maximum absolute error (MAE)."}}
{"id": "uKXxt5HaLd9", "cdate": 1640995200000, "mdate": 1681652008737, "content": {"title": "LipGene: Lipschitz Continuity Guided Adaptive Learning Rates for Fast Convergence on Microarray Expression Data Sets", "abstract": ""}}
{"id": "lvwLATZbUb", "cdate": 1640995200000, "mdate": 1663588227242, "content": {"title": "Estimation and Applications of Quantiles in Deep Binary Classification", "abstract": "Conditional quantiles obtained via regression are used as a robust alternative to classical conditional means in econometrics and statistics, as they can capture the uncertainty in a prediction, and model tail behaviors, while making very few distributional assumptions. In this work, we extend the notion of conditional quantiles to the binary classification setting\u2014allowing us to quantify the uncertainty in the predictions, increase resilience to label noise, and provide new insights into the functions learnt by the models. We accomplish this by defining a new loss called binary quantile regression loss. We compute the Lipschitz constant of the proposed loss and show that its curvature is bounded under some regularity conditions. These properties are later used to characterize the error rates of the learning algorithms and to accelerate the training regime with using Lipschitz adaptive learning rates. We leverage the estimated quantiles to obtain individualized confidence scores that provide an accurate measure of a prediction being misclassified. We aggregate these scores to provide two additional metrics, namely, confidence score and retention rate, which can be used to withhold decisions and increase model accuracy. We also study the robustness of the proposed nonparametric binary quantile classification framework, and finally, we demonstrate that quantiles aid in explainability as they can be used to obtain several univariate summary statistics that can be directly applied to existing explanation tools."}}
{"id": "ie0c1zFvbdW", "cdate": 1640995200000, "mdate": 1663588227092, "content": {"title": "LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks", "abstract": "High-throughput Genomics is ushering a new era in personalized health care, and targeted drug design and delivery. Mining these large datasets, and obtaining calibrated predictions is of immediate relevance and utility. In our work, we develop methods for Gene Expression Inference based on Deep neural networks. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of house keeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. However, check loss, used in quantile regression to drive the estimation process is not differentiable. We propose log-cosh as a smooth-alternative to the check loss. We apply our methods on GEO microarray dataset. We also extend the method to binary classification setting. Furthermore, we investigate other consequences of the smoothness of the loss in faster convergence."}}
{"id": "WSC6CtcbZ6s", "cdate": 1640995200000, "mdate": 1663588227157, "content": {"title": "Hamiltonian Monte Carlo Particle Swarm Optimizer", "abstract": "We introduce the Hamiltonian Monte Carlo Particle Swarm Optimizer (HMC-PSO), an optimization algorithm that reaps the benefits of both Exponentially Averaged Momentum PSO and HMC sampling. The coupling of the position and velocity of each particle with Hamiltonian dynamics in the simulation allows for extensive freedom for exploration and exploitation of the search space. It also provides an excellent technique to explore highly non-convex functions while ensuring efficient sampling. We extend the method to approximate error gradients in closed form for Deep Neural Network (DNN) settings. We discuss possible methods of coupling and compare its performance to that of state-of-the-art optimizers on the Golomb's Ruler problem and Classification tasks."}}
{"id": "VHbXk-j1WhL", "cdate": 1640995200000, "mdate": 1681652008905, "content": {"title": "Investigation of a Machine learning methodology for the SKA pulsar search pipeline", "abstract": ""}}
{"id": "ODQp4l4ota", "cdate": 1640995200000, "mdate": 1681652008772, "content": {"title": "Study of Heterogeneous User Behavior in Crowd Evacuation in Presence of Wheelchair Users", "abstract": ""}}
