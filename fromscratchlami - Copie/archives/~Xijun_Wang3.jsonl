{"id": "NsUqfWtvtV", "cdate": 1684318671766, "mdate": null, "content": {"title": "METEOR: A Dense, Heterogeneous, and Unstructured Traffic Dataset With Rare Behaviors", "abstract": "We present a new traffic dataset, {\\textsc{Meteor}\\xspace}, which captures traffic patterns and multi-agent driving behaviors in unstructured scenarios. {\\textsc{Meteor}\\xspace}~consists of more than $1000$ one-minute videos, over $2$ million annotated frames with bounding boxes and GPS trajectories for $16$ unique agent categories, and more than $13$ million bounding boxes for traffic agents. {\\textsc{Meteor}\\xspace} is a dataset for rare and interesting, multi-agent driving behaviors that are grouped into traffic violations, atypical interactions, and  diverse scenarios. Every video in {\\textsc{Meteor}\\xspace} is tagged using a diverse range of factors corresponding to weather, time of the day, road conditions, and traffic density. We use \\rain to benchmark perception methods for object detection and multi-agent behavior prediction. Our key finding is that state-of-the-art models for object detection and behavior prediction, which otherwise succeed on existing datasets such as Waymo, fail on the {\\textsc{Meteor}\\xspace} dataset. {\\textsc{Meteor}\\xspace} is a step towards developing more sophisticated perception models for dense, heterogeneous, and unstructured scenarios."}}
{"id": "9kaZNBEzy1u", "cdate": 1683908750808, "mdate": 1683908750808, "content": {"title": "Auxiliary Modality Learning for Visual Computing", "abstract": "Driven by the need from real-world applications, Auxiliary Modality Learning (AML) offers the possibility to utilize more information from auxiliary data in training, while only requiring data from one or fewer modalities in testing, to save the overall computational cost and reduce the amount of input data for inferencing. In this work, we formally define \u201cAuxiliary Modality Learning\u201d (AML), systematically classify types of auxiliary modality (in visual computing) and architectures for AML, and analyze their performance. We also analyze the conditions under which AML works well from the optimization and data distribution perspectives. To guide various choices to achieve optimal performance using AML, we propose a novel method to assist in choosing the best auxiliary modality and estimating an upper bound performance before executing AML. In addition, we propose a new AML method using generalized curriculum distillation to enable more effective curriculum learning. Our method achieves the best performance compared to other SOTA methods."}}
{"id": "wrJse4UwC_", "cdate": 1683908439121, "mdate": 1683908439121, "content": {"title": "Small-shot Multi-modal Distillation for Vision-based Autonomous Steering", "abstract": "Abstract\u2014 In this paper, we propose a novel learning framework for autonomous systems that uses a small amount of \u201cauxiliary information\u201d that complements the learning of the main modality, called \u201csmall-shot auxiliary modality distillation network (AMD-S-Net)\u201d. The AMD-S-Net contains a two-stream framework design that can fully extract information from different types of data (i.e., paired/unpaired multi-modality data) to distill knowledge more effectively. We also propose a novel training paradigm based on the \u201creset operation\u201d that enables the teacher to explore the local loss landscape near the student domain iteratively, providing local landscape information and potential directions to discover better solutions by the student, thus achieving higher learning performance. Our experiments show that AMD-S-Net and our training paradigm outperform other SOTA methods by up to 12.7% and 18.1% improvement in autonomous steering, respectively."}}
{"id": "yxV1RWGmq7", "cdate": 1672531200000, "mdate": 1700172921454, "content": {"title": "Auxiliary Modality Learning with Generalized Curriculum Distillation", "abstract": "Driven by the need from real-world applications, Auxiliary Modality Learning (AML) offers the possibility to utilize more information from auxiliary data in training, while only requiring data from..."}}
{"id": "w3zJf9Mzme2", "cdate": 1672531200000, "mdate": 1700172921451, "content": {"title": "ICAR: Image-based Complementary Auto Reasoning", "abstract": "Scene-aware Complementary Item Retrieval (CIR) is a challenging task which requires to generate a set of compatible items across domains. Due to the subjectivity, it is difficult to set up a rigorous standard for both data collection and learning objectives. To address this challenging task, we propose a visual compatibility concept, composed of similarity (resembling in color, geometry, texture, and etc.) and complementarity (different items like table vs chair completing a group). Based on this notion, we propose a compatibility learning framework, a category-aware Flexible Bidirectional Transformer (FBT), for visual \"scene-based set compatibility reasoning\" with the cross-domain visual similarity input and auto-regressive complementary item generation. We introduce a \"Flexible Bidirectional Transformer (FBT)\" consisting of an encoder with flexible masking, a category prediction arm, and an auto-regressive visual embedding prediction arm. And the inputs for FBT are cross-domain visual similarity invariant embeddings, making this framework quite generalizable. Furthermore, our proposed FBT model learns the inter-object compatibility from a large set of scene images in a self-supervised way. Compared with the SOTA methods, this approach achieves up to 5.3% and 9.6% in FITB score and 22.3% and 31.8% SFID improvement on fashion and furniture, respectively."}}
{"id": "unF9EaSYYTq", "cdate": 1672531200000, "mdate": 1695984576302, "content": {"title": "SCSC: Spatial Cross-scale Convolution Module to Strengthen both CNNs and Transformers", "abstract": "This paper presents a module, Spatial Cross-scale Convolution (SCSC), which is verified to be effective in improving both CNNs and Transformers. Nowadays, CNNs and Transformers have been successful in a variety of tasks. Especially for Transformers, increasing works achieve state-of-the-art performance in the computer vision community. Therefore, researchers start to explore the mechanism of those architectures. Large receptive fields, sparse connections, weight sharing, and dynamic weight have been considered keys to designing effective base models. However, there are still some issues to be addressed: large dense kernels and self-attention are inefficient, and large receptive fields make it hard to capture local features. Inspired by the above analyses and to solve the mentioned problems, in this paper, we design a general module taking in these design keys to enhance both CNNs and Transformers. SCSC introduces an efficient spatial cross-scale encoder and spatial embed module to capture assorted features in one layer. On the face recognition task, FaceResNet with SCSC can improve 2.7% with 68% fewer FLOPs and 79% fewer parameters. On the ImageNet classification task, Swin Transformer with SCSC can achieve even better performance with 22% fewer FLOPs, and ResNet with CSCS can improve 5.3% with similar complexity. Furthermore, a traditional network (e.g., ResNet) embedded with SCSC can match Swin Transformer's performance."}}
{"id": "eeB8ruak6W", "cdate": 1672531200000, "mdate": 1695984576302, "content": {"title": "METEOR: A Dense, Heterogeneous, and Unstructured Traffic Dataset with Rare Behaviors", "abstract": "We present a new traffic dataset, Meteor, which captures traffic patterns and multi-agent driving behaviors in unstructured scenarios. Meteor consists of more than 1000 one-minute videos, over 2 million annotated frames with bounding boxes and GPS trajectories for 16 unique agent categories, and more than 13 million bounding boxes for traffic agents. Meteor is a dataset for rare and interesting, multi-agent driving behaviors that are grouped into traffic violations, atypical interactions, and diverse scenarios. Every video in Meteor is tagged using a diverse range of factors corresponding to weather, time of the day, road conditions, and traffic density. We use Meteor to benchmark perception methods for object detection and multi-agent behavior prediction. Our key finding is that state-of-the-art models for object detection and behavior prediction, which otherwise succeed on existing datasets such as Waymo, fail on the Meteor dataset. Meteor is a step towards developing more sophisticated perception models for dense, heterogeneous, and unstructured scenarios."}}
{"id": "VZvUJUulOb", "cdate": 1672531200000, "mdate": 1700172921467, "content": {"title": "Small-shot Multi-modal Distillation for Vision-based Autonomous Steering", "abstract": "In this paper, we propose a novel learning framework for autonomous systems that uses a small amount of \u201cauxiliary information\u201d that complements the learning of the main modality, called \u201csmall-shot auxiliary modality distillation network (AMD-S-Net)\u201d. The AMD-S-Net contains a two-stream framework design that can fully extract information from different types of data (i.e., paired/unpaired multi-modality data) to distill knowledge more effectively. We also propose a novel training paradigm based on the \u201creset operation\u201d that enables the teacher to explore the local loss landscape near the student domain iteratively, providing local landscape information and potential directions to discover better solutions by the student, thus achieving higher learning performance. Our experiments show that AMD-S-Net and our training paradigm outperform other SOTA methods by up to 12.7% and 18.1% improvement in autonomous steering, respectively."}}
{"id": "S_MkprZGFK", "cdate": 1672531200000, "mdate": 1695984576307, "content": {"title": "Triplet Knowledge Distillation", "abstract": "In Knowledge Distillation, the teacher is generally much larger than the student, making the solution of the teacher likely to be difficult for the student to learn. To ease the mimicking difficulty, we introduce a triplet knowledge distillation mechanism named TriKD. Besides teacher and student, TriKD employs a third role called anchor model. Before distillation begins, the pre-trained anchor model delimits a subspace within the full solution space of the target problem. Solutions within the subspace are expected to be easy targets that the student could mimic well. Distillation then begins in an online manner, and the teacher is only allowed to express solutions within the aforementioned subspace. Surprisingly, benefiting from accurate but easy-to-mimic hints, the student can finally perform well. After the student is well trained, it can be used as the new anchor for new students, forming a curriculum learning strategy. Our experiments on image classification and face recognition with various models clearly demonstrate the effectiveness of our method. Furthermore, the proposed TriKD is also effective in dealing with the overfitting issue. Moreover, our theoretical analysis supports the rationality of our triplet distillation."}}
{"id": "Iv33uo2YLyA", "cdate": 1672531200000, "mdate": 1698559729629, "content": {"title": "CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition", "abstract": "We present CrossLoc3D, a novel 3D place recognition method that solves a large-scale point matching problem in a cross-source setting. Cross-source point cloud data corresponds to point sets captured by depth sensors with different accuracies or from different distances and perspectives. We address the challenges in terms of developing 3D place recognition methods that account for the representation gap between points captured by different sources. Our method handles cross-source data by utilizing multi-grained features and selecting convolution kernel sizes that correspond to most prominent features. Inspired by the diffusion models, our method uses a novel iterative refinement process that gradually shifts the embedding spaces from different sources to a single canonical space for better metric learning. In addition, we present CS-Campus3D, the first 3D aerial-ground cross-source dataset consisting of point cloud data from both aerial and ground LiDAR scans. The point clouds in CS-Campus3D have representation gaps and other features like different views, point densities, and noise patterns. We show that our CrossLoc3D algorithm can achieve an improvement of 4.74% - 15.37% in terms of the top 1 average recall on our CS-Campus3D benchmark and achieves performance comparable to state-of-the-art 3D place recognition method on the Oxford RobotCar. The code and CS-CAMPUS3D benchmark will be available at github.com/rayguan97/crossloc3d."}}
