{"id": "TMV3EWjZMlr", "cdate": 1640995200000, "mdate": 1676004924721, "content": {"title": "Adaptive Multi-Corpora Language Model Training for Speech Recognition", "abstract": "Neural network language model (NNLM) plays an essential role in automatic speech recognition (ASR) systems, especially in adaptation tasks when text-only data is available. In practice, an NNLM is typically trained on a combination of data sampled from multiple corpora. Thus, the data sampling strategy is important to the adaptation performance. Most existing works focus on designing static sampling strategies. However, each corpus may show varying impacts at different NNLM training stages. In this paper, we introduce a novel adaptive multi-corpora training algorithm that dynamically learns and adjusts the sampling probability of each corpus along the training process. The algorithm is robust to corpora sizes and domain relevance. Compared with static sampling strategy baselines, the proposed approach yields remarkable improvement by achieving up to relative 7% and 9% word error rate (WER) reductions on in-domain and out-of-domain adaptation tasks, respectively."}}
{"id": "A739QbvYAnb", "cdate": 1640995200000, "mdate": 1676004924708, "content": {"title": "Warping Layer: Representation Learning for Label Structures in Weakly Supervised Learning", "abstract": "Many learning tasks only receive weak supervision, such as semi-supervised learning and few-shot learning. With limited labeled data, prior structures become especially important, and prominent examples include hierarchies and mutual exclusions in the class space. However, most existing approaches only learn the representations separately in the feature space and the label space, and do not explicitly enforce the logical relationships. In this paper, we propose a novel warping layer that jointly learns representations in both spaces, and thanks to the modularity and differentiability, it can be directly embedded into generative models to leverage the prior hierarchical structure and unlabeled data. The effectiveness of the warping layer is demonstrated on both few-shot and semi-supervised learning, outperforming the state of the art in practice."}}
{"id": "rAx2hQRp3km", "cdate": 1620327268186, "mdate": null, "content": {"title": "Proximal Mapping for Deep Regularization", "abstract": "Underpinning the success of deep learning is effective regularizations that allow a variety of priors in data to be modeled. For example, robustness to adversarial perturbations, and correlations between multiple modalities. However, most regu- larizers are specified in terms of hidden layer outputs, which are not themselves optimization variables. In contrast to prevalent methods that optimize them indi- rectly through model weights, we propose inserting proximal mapping as a new layer to the deep network, which directly and explicitly produces well regularized hidden layer outputs. The resulting technique is shown well connected to kernel warping and dropout, and novel algorithms were developed for robust temporal learning and multiview modeling, both outperforming state-of-the-art methods"}}
{"id": "wlTonWpuW2u", "cdate": 1577836800000, "mdate": null, "content": {"title": "Proximal Mapping for Deep Regularization", "abstract": "Underpinning the success of deep learning is effective regularizations that allow a variety of priors in data to be modeled. For example, robustness to adversarial perturbations, and correlations between multiple modalities. However, most regularizers are specified in terms of hidden layer outputs, which are not themselves optimization variables. In contrast to prevalent methods that optimize them indirectly through model weights, we propose inserting proximal mapping as a new layer to the deep network, which directly and explicitly produces well regularized hidden layer outputs. The resulting technique is shown well connected to kernel warping and dropout, and novel algorithms were developed for robust temporal learning and multiview modeling, both outperforming state-of-the-art methods."}}
{"id": "raf-RihObZq", "cdate": 1577836800000, "mdate": 1646525606216, "content": {"title": "Convex Representation Learning for Generalized Invariance in Semi-Inner-Product Space", "abstract": "Invariance (defined in a general sense) has been one of the most effective priors for representation learning. Direct factorization of parametric models is feasible only for a small range of invari..."}}
{"id": "N6-_549xQ3o", "cdate": 1577836800000, "mdate": null, "content": {"title": "Convex Representation Learning for Generalized Invariance in Semi-Inner-Product Space", "abstract": "Invariance (defined in a general sense) has been one of the most effective priors for representation learning. Direct factorization of parametric models is feasible only for a small range of invariances, while regularization approaches, despite improved generality, lead to nonconvex optimization. In this work, we develop a convex representation learning algorithm for a variety of generalized invariances that can be modeled as semi-norms. Novel Euclidean embeddings are introduced for kernel representers in a semi-inner-product space, and approximation bounds are established. This allows invariant representations to be learned efficiently and effectively as confirmed in our experiments, along with accurate predictions."}}
{"id": "IZQFKiWLXE", "cdate": 1577836800000, "mdate": null, "content": {"title": "Proximal Mapping for Deep Regularization", "abstract": "Underpinning the success of deep learning is effective regularizations that allow a variety of priors in data to be modeled. For example, robustness to adversarial perturbations, and correlations between multiple modalities. However, most regularizers are specified in terms of hidden layer outputs, which are not themselves optimization variables. In contrast to prevalent methods that optimize them indirectly through model weights, we propose inserting proximal mapping as a new layer to the deep network, which directly and explicitly produces well regularized hidden layer outputs. The resulting technique is shown well connected to kernel warping and dropout, and novel algorithms were developed for robust temporal learning and multiview modeling, both outperforming state-of-the-art methods."}}
{"id": "H1edV6VKvS", "cdate": 1569439024210, "mdate": null, "content": {"title": "ProxNet: End-to-End Learning of  Structured Representation by Proximal Mapping", "abstract": "Underpinning the success of deep learning is the effective regularization that allows a broad range of structures in data to be compactly modeled in a deep architecture.  Examples include transformation invariances, robustness to adversarial/random perturbations, and correlations between multiple modalities. However, most existing methods incorporate such priors either by auto-encoders, whose result is used to initialize supervised learning, or by augmenting the data with exemplifications of the transformations which, despite the improved performance of  supervised learning, leaves it unclear whether the learned latent representation does encode the desired regularities. To address these issues, this work proposes an \\emph{end-to-end} representation learning framework that allows prior structures to be encoded \\emph{explicitly} in the hidden layers, and to be trained efficiently in conjunction with the supervised target. Our approach is based on proximal mapping in a reproducing kernel Hilbert space, and leverages differentiable optimization. The resulting technique is applied to generalize dropout and invariant kernel warping, and to develop novel algorithms for multiview modeling and robust temporal learning."}}
{"id": "dxACpywmz5", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning Invariant Representations with Kernel Warping", "abstract": "Invariance is an effective prior that has been extensively used to bias supervised learning with a \\emph{given} representation of data. In order to learn invariant representations, wavelet and scattering based methods \u201chard code\u201d invariance over the \\emph{entire} sample space, hence restricted to a limited range of transformations. Kernels based on Haar integration also work only on a \\emph{group} of transformations. In this work, we break this limitation by designing a new representation learning algorithm that incorporates invariances \\emph{beyond transformation}. Our approach, which is based on warping the kernel in a data-dependent fashion, is computationally efficient using random features, and leads to a deep kernel through multiple layers. We apply it to convolutional kernel networks and demonstrate its stability."}}
