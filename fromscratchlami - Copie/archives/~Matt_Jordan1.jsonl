{"id": "-hWhz9xfrB9", "cdate": 1663850454933, "mdate": null, "content": {"title": "Lovasz Theta Contrastive Learning", "abstract": "We establish a connection between the Lovasz theta function of a graph and the widely used InfoNCE loss. We show that under certain conditions, the minima of the InfoNCE loss are related to minimizing the Lovasz theta function on the empty similarity graph between the samples. Building on this connection, we generalize contrastive learning on weighted similarity graphs between samples. Our Lovasz theta contrastive loss uses a weighted graph that can be learned to take into account similarities between our data. We evaluate our method on image classification tasks, demonstrating an improvement of $1 \\%$ in the supervised case and up to $4 \\%$ in the unsupervised case."}}
{"id": "OFsja-NZGbY", "cdate": 1652737814707, "mdate": null, "content": {"title": "Zonotope Domains for Lagrangian Neural Network Verification", "abstract": "Neural network verification aims to provide provable bounds for the output of a neural network for a given input range. Notable prior works in this domain have either generated bounds using abstract domains, which preserve some dependency between intermediate neurons in the network; or framed verification as an optimization problem and solved a relaxation using Lagrangian methods. A key drawback of the latter technique is that each neuron is treated independently, thereby ignoring important neuron interactions. We provide an approach that merges these two threads and uses zonotopes within a Lagrangian decomposition. Crucially, we can decompose the problem of verifying a deep neural network into the verification of many 2-layer neural networks. While each of these problems is provably hard, we provide efficient relaxation methods that are amenable to efficient dual ascent procedures. Our technique yields bounds that improve upon both linear programming and Lagrangian-based verification techniques in both time and bound tightness."}}
{"id": "oK20qcux0H", "cdate": 1640995200000, "mdate": 1673281022808, "content": {"title": "Zonotope Domains for Lagrangian Neural Network Verification", "abstract": ""}}
{"id": "HCOdL3dWab", "cdate": 1621630322484, "mdate": null, "content": {"title": "Inverse Problems Leveraging Pre-trained Contrastive Representations", "abstract": "We study a new family of inverse problems for recovering representations of corrupted data. We assume access to a pre-trained representation learning network R(x) that operates on clean images, like CLIP. The problem is to recover the representation of an image R(x), if we are only given a corrupted version A(x), for some known forward operator A. We propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. Using a linear probe on our robust representations, we achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking. We evaluate on a subset of ImageNet and observe that our method is robust to varying levels of distortion. Our method outperforms end-to-end baselines even with a fraction of the labeled data in a wide range of forward operators. "}}
{"id": "MCpo_ee1cOx", "cdate": 1609459200000, "mdate": 1673281022795, "content": {"title": "Provable Lipschitz Certification for Generative Models", "abstract": ""}}
{"id": "evtuyxttLQq", "cdate": 1577836800000, "mdate": 1673281022818, "content": {"title": "Exactly Computing the Local Lipschitz Constant of ReLU Networks", "abstract": ""}}
{"id": "B1MX2HHlUS", "cdate": 1567802795466, "mdate": null, "content": {"title": "Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes", "abstract": "We propose a novel method for computing exact pointwise robustness of deep  neural networks for all convex lp norms. Our algorithm, GeoCert, finds the largest  lp ball centered at an input point x0, within which the output class of a given neural  network with ReLU nonlinearities remains unchanged. We relate the problem  of computing pointwise robustness of these networks to that of computing the  maximum norm ball with a fixed center that can be contained in a non-convex polytope. This is a challenging problem in general, however we show that there  exists an efficient algorithm to compute this for polyhedral complices. Further we show that piecewise linear neural networks partition the input space into a polyhedral complex. Our algorithm has the ability to almost immediately output a nontrivial lower bound to the pointwise robustness which is iteratively improved until it ultimately becomes tight. We empirically show that our approach generates a distance lower bounds that are tighter compared to prior work, under moderate time constraints."}}
{"id": "SzTRWIfCoR", "cdate": 1546300800000, "mdate": 1673281022822, "content": {"title": "Quantifying Perceptual Distortion of Adversarial Examples", "abstract": ""}}
