{"id": "AzRMQCt9zy", "cdate": 1664731447862, "mdate": null, "content": {"title": "Neural DAG Scheduling via One-Shot Priority Sampling", "abstract": "We consider the problem of scheduling operations/nodes, the dependency among which is characterized by a Directed Acyclic Graph (DAG). Due to its NP-hard nature, heuristic algorithms were traditionally used to acquire reasonably good solutions, and more recent works have proposed Machine Learning (ML) heuristics that can generalize to unseen graphs and outperform the non-ML heuristics. However, it is computationally costly to generate solutions using existing ML schedulers since they adopt the episodic reinforcement learning framework that necessitates multi-round neural network processing. We propose a novel ML scheduler that uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules. Since the one-shot encoder can efficiently sample the priorities in parallel, our algorithm runs significantly faster than existing ML baselines and has comparable run time with the fast traditional heuristics. We empirically show that our algorithm generates better schedules than both non-neural and neural baselines across various real-world and synthetic scheduling tasks."}}
{"id": "WL8FlAugqQ", "cdate": 1663850190757, "mdate": null, "content": {"title": "Neural DAG Scheduling via One-Shot Priority Sampling", "abstract": "We consider the problem of scheduling operations/nodes, the dependency among which is characterized by a Directed Acyclic Graph (DAG). Due to its NP-hard nature, heuristic algorithms were traditionally used to acquire reasonably good solutions, and more recent works have proposed Machine Learning (ML) heuristics that can generalize to unseen graphs and outperform the non-ML heuristics. However, it is computationally costly to generate solutions using existing ML schedulers since they adopt the episodic reinforcement learning framework that necessitates multi-round neural network processing. We propose a novel ML scheduler that uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules. Since the one-shot encoder can efficiently sample the priorities in parallel, our algorithm runs significantly faster than existing ML baselines and has comparable run time with the fast traditional heuristics. We empirically show that our algorithm generates better schedules than both non-neural and neural baselines across various real-world and synthetic scheduling tasks."}}
{"id": "e2Lle5cij9D", "cdate": 1632875715096, "mdate": null, "content": {"title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions", "abstract": "Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with gradient descent-ascent (GDA), but it is unclear whether the optimization problem contains any saddle points, or whether heuristic methods can find them in practice. In this work, we analyze the training of Wasserstein GANs with two-layer neural network discriminators through the lens of convex duality, and for a variety of generators expose the conditions under which Wasserstein GANs can be solved exactly with convex optimization approaches, or can be represented as convex-concave games. Using this convex duality interpretation, we further demonstrate the impact of different activation functions of the discriminator. Our observations are verified with numerical results demonstrating the power of the convex interpretation, with an application in progressive training of convex architectures corresponding to linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments is available at https://github.com/ardasahiner/ProCoGAN."}}
{"id": "gAHwQeTyMB5", "cdate": 1609459200000, "mdate": null, "content": {"title": "Neural Spectrahedra and Semidefinite Lifts: Global Convex Optimization of Polynomial Activation Neural Networks in Fully Polynomial-Time", "abstract": "The training of two-layer neural networks with nonlinear activation functions is an important non-convex optimization problem with numerous applications and promising performance in layerwise deep learning. In this paper, we develop exact convex optimization formulations for two-layer neural networks with second degree polynomial activations based on semidefinite programming. Remarkably, we show that semidefinite lifting is always exact and therefore computational complexity for global optimization is polynomial in the input dimension and sample size for all input data. The developed convex formulations are proven to achieve the same global optimal solution set as their non-convex counterparts. More specifically, the globally optimal two-layer neural network with polynomial activations can be found by solving a semidefinite program (SDP) and decomposing the solution using a procedure we call Neural Decomposition. Moreover, the choice of regularizers plays a crucial role in the computational tractability of neural network training. We show that the standard weight decay regularization formulation is NP-hard, whereas other simple convex penalties render the problem tractable in polynomial time via convex programming. We extend the results beyond the fully connected architecture to different neural network architectures including networks with vector outputs and convolutional architectures with pooling. We provide extensive numerical simulations showing that the standard backpropagation approach often fails to achieve the global optimum of the training loss. The proposed approach is significantly faster to obtain better test accuracy compared to the standard backpropagation procedure."}}
{"id": "49AjeWzoCG6", "cdate": 1609459200000, "mdate": null, "content": {"title": "Discrete scaling based on operator theory", "abstract": "Signal scaling is a fundamental operation of practical importance in which a signal is made wider or narrower along the coordinate direction(s). Scaling, also referred to as magnification or zooming, is complicated for signals of a discrete variable since it cannot be accomplished simply by moving the signal values to new coordinate points. Most practical approaches to discrete scaling involve interpolation. We propose a new approach based on hyperdifferential operator theory that does not involve conventional interpolation. This approach provides a self-consistent and pure definition of discrete scaling that is fully consistent with discrete Fourier transform theory. It can potentially be applied to other problems in signal theory and analysis such as transform designs. Apart from its theoretical elegance, it also provides a basis for numerical implementation."}}
{"id": "V1Q6mpjJlmK", "cdate": 1577836800000, "mdate": null, "content": {"title": "Debiasing Distributed Second Order Optimization with Surrogate Sketching and Scaled Regularization", "abstract": "In distributed second order optimization, a standard strategy is to average many local estimates, each of which is based on a small sketch or batch of the data. However, the local estimates on each machine are typically biased, relative to the full solution on all of the data, and this can limit the effectiveness of averaging. Here, we introduce a new technique for debiasing the local estimates, which leads to both theoretical and empirical improvements in the convergence rate of distributed second order methods. Our technique has two novel components: (1) modifying standard sketching techniques to obtain what we call a surrogate sketch; and (2) carefully scaling the global regularization parameter for local computations. Our surrogate sketches are based on determinantal point processes, a family of distributions for which the bias of an estimate of the inverse Hessian can be computed exactly. Based on this computation, we show that when the objective being minimized is $l_2$-regularized with parameter $\\lambda$ and individual machines are each given a sketch of size $m$, then to eliminate the bias, local estimates should be computed using a shrunk regularization parameter given by $\\lambda^\\prime=\\lambda(1-\\frac{d_\\lambda}{m})$, where $d_\\lambda$ is the $\\lambda$-effective dimension of the Hessian (or, for quadratic problems, the data matrix)."}}
{"id": "GnyHBHzOJzq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distributed Averaging Methods for Randomized Second Order Optimization", "abstract": "We consider distributed optimization problems where forming the Hessian is computationally challenging and communication is a significant bottleneck. We develop unbiased parameter averaging methods for randomized second order optimization that employ sampling and sketching of the Hessian. Existing works do not take the bias of the estimators into consideration, which limits their application to massively parallel computation. We provide closed-form formulas for regularization parameters and step sizes that provably minimize the bias for sketched Newton directions. We also extend the framework of second order averaging methods to introduce an unbiased distributed optimization framework for heterogeneous computing systems with varying worker resources. Additionally, we demonstrate the implications of our theoretical findings via large scale experiments performed on a serverless computing platform."}}
{"id": "21PouJHw_R6", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distributed Sketching Methods for Privacy Preserving Regression", "abstract": "In this work, we study distributed sketching methods for large scale regression problems. We leverage multiple randomized sketches for reducing the problem dimensions as well as preserving privacy and improving straggler resilience in asynchronous distributed systems. We derive novel approximation guarantees for classical sketching methods and analyze the accuracy of parameter averaging for distributed sketches. We consider random matrices including Gaussian, randomized Hadamard, uniform sampling and leverage score sampling in the distributed setting. Moreover, we propose a hybrid approach combining sampling and fast random projections for better computational efficiency. We illustrate the performance of distributed sketches in a serverless computing platform with large scale experiments."}}
{"id": "1M4JdXJy8M", "cdate": 1577836800000, "mdate": null, "content": {"title": "Debiasing Distributed Second Order Optimization with Surrogate Sketching and Scaled Regularization", "abstract": "In distributed second order optimization, a standard strategy is to average many local estimates, each of which is based on a small sketch or batch of the data. However, the local estimates on each machine are typically biased, relative to the full solution on all of the data, and this can limit the effectiveness of averaging. Here, we introduce a new technique for debiasing the local estimates, which leads to both theoretical and empirical improvements in the convergence rate of distributed second order methods. Our technique has two novel components: (1) modifying standard sketching techniques to obtain what we call a surrogate sketch; and (2) carefully scaling the global regularization parameter for local computations. Our surrogate sketches are based on determinantal point processes, a family of distributions for which the bias of an estimate of the inverse Hessian can be computed exactly. Based on this computation, we show that when the objective being minimized is $l_2$-regularized with parameter $\\lambda$ and individual machines are each given a sketch of size $m$, then to eliminate the bias, local estimates should be computed using a shrunk regularization parameter given by $\\lambda^{\\prime}=\\lambda\\cdot(1-\\frac{d_{\\lambda}}{m})$, where $d_{\\lambda}$ is the $\\lambda$-effective dimension of the Hessian (or, for quadratic problems, the data matrix)."}}
{"id": "pFhOE-W9LU0", "cdate": 1546300800000, "mdate": null, "content": {"title": "Discrete Linear Canonical Transform Based on Hyperdifferential Operators", "abstract": "Linear canonical transforms (LCTs) are of importance in many areas of science and engineering with many applications. Therefore, a satisfactory discrete implementation is of considerable interest. Although there are methods that link the samples of the input signal to the samples of the linear canonical transformed output signal, no widely-accepted definition of the discrete LCT has been established. We introduce a new approach to defining the discrete linear canonical transform (DLCT) by employing operator theory. Operators are abstract entities that can have both continuous and discrete concrete manifestations. Generating the continuous and discrete manifestations of LCTs from the same abstract operator framework allows us to define the continuous and discrete transforms in a structurally analogous manner. By utilizing hyperdifferential operators, we obtain a DLCT matrix, which is totally compatible with the theory of the discrete Fourier transform (DFT) and its dual and circulant structure, which makes further analytical manipulations and progress possible. The proposed DLCT is to the continuous LCT, what the DFT is to the continuous Fourier transform. The DLCT of the signal is obtained simply by multiplying the vector holding the samples of the input signal by the DLCT matrix."}}
