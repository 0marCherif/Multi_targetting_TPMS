{"id": "xw1KUW5_Z0", "cdate": 1672531200000, "mdate": 1683661396010, "content": {"title": "A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm", "abstract": "We study the problem of list-decodable Gaussian covariance estimation. Given a multiset $T$ of $n$ points in $\\mathbb R^d$ such that an unknown $\\alpha<1/2$ fraction of points in $T$ are i.i.d. samples from an unknown Gaussian $\\mathcal{N}(\\mu, \\Sigma)$, the goal is to output a list of $O(1/\\alpha)$ hypotheses at least one of which is close to $\\Sigma$ in relative Frobenius norm. Our main result is a $\\mathrm{poly}(d,1/\\alpha)$ sample and time algorithm for this task that guarantees relative Frobenius norm error of $\\mathrm{poly}(1/\\alpha)$. Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) -- a key ingredient in the recent work of [BDJ+22] on robustly learning arbitrary GMMs. Combined with the other components of [BDJ+22], our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs. At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings."}}
{"id": "uon5i762Rt", "cdate": 1672531200000, "mdate": 1683661396098, "content": {"title": "Gaussian Mean Testing Made Simple", "abstract": ""}}
{"id": "hfKU-w0JJ8-", "cdate": 1672531200000, "mdate": 1674791575291, "content": {"title": "Simple Binary Hypothesis Testing under Local Differential Privacy and Communication Constraints", "abstract": "We study simple binary hypothesis testing under both local differential privacy (LDP) and communication constraints. We qualify our results as either minimax optimal or instance optimal: the former hold for the set of distribution pairs with prescribed Hellinger divergence and total variation distance, whereas the latter hold for specific distribution pairs. For the sample complexity of simple hypothesis testing under pure LDP constraints, we establish instance-optimal bounds for distributions with binary support; minimax-optimal bounds for general distributions; and (approximately) instance-optimal, computationally efficient algorithms for general distributions. When both privacy and communication constraints are present, we develop instance-optimal, computationally efficient algorithms that achieve the minimum possible sample complexity (up to universal constants). Our results on instance-optimal algorithms hinge on identifying the extreme points of the joint range set $\\mathcal A$ of two distributions $p$ and $q$, defined as $\\mathcal A := \\{(\\mathbf T p, \\mathbf T q) | \\mathbf T \\in \\mathcal C\\}$, where $\\mathcal C$ is the set of channels characterizing the constraints."}}
{"id": "LJdUUOmWjX", "cdate": 1652737841231, "mdate": null, "content": {"title": "List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering", "abstract": "We study the problem of list-decodable sparse mean estimation. Specifically, for a parameter $\\alpha \\in (0, 1/2)$, we are given $m$ points in $\\mathbb{R}^n$, $\\lfloor \\alpha m \\rfloor$ of which are i.i.d. samples from a distribution $D$ with unknown $k$-sparse mean $\\mu$. No assumptions are made on the remaining points, which form the majority of the dataset. The goal is to return a small list of candidates containing a vector $\\hat \\mu$ such that $\\|\\hat \\mu - \\mu\\|_2$ is small. Prior work had studied the problem of list-decodable mean estimation in the dense setting. In this work, we develop a novel, conceptually simpler technique for list-decodable mean estimation. As the main application of our approach, we provide the first sample and computationally efficient algorithm for list-decodable sparse mean estimation. In particular, for distributions with  ``certifiably bounded'' $t$-th moments in $k$-sparse directions and sufficiently light tails, our algorithm achieves error of $(1/\\alpha)^{O(1/t)}$ with sample complexity $m = (k\\log(n))^{O(t)}/\\alpha$ and running time $\\mathrm{poly}(mn^t)$. For the special case of Gaussian inliers, our algorithm achieves the optimal error guarantee $\\Theta (\\sqrt{\\log(1/\\alpha)})$ with quasi-polynomial complexity. We complement our upper bounds with nearly-matching statistical query and low-degree polynomial testing lower bounds. "}}
{"id": "jWgGtPmi8c", "cdate": 1652737624329, "mdate": null, "content": {"title": "Outlier-Robust Sparse Mean Estimation for Heavy-Tailed Distributions", "abstract": "We study the fundamental task of outlier-robust mean estimation  for heavy-tailed distributions in the presence of sparsity. Specifically, given a small number of corrupted samples from a high-dimensional heavy-tailed distribution whose mean $\\mu$ is guaranteed to be sparse, the goal is to efficiently compute a hypothesis that accurately approximates $\\mu$ with high probability. Prior work had obtained efficient algorithms for robust sparse mean estimation of light-tailed distributions. In this work, we give the first sample-efficient and polynomial-time robust sparse mean estimator for heavy-tailed distributions under mild moment assumptions. Our algorithm achieves the optimal asymptotic error using a number of samples scaling logarithmically with the ambient dimension. Importantly, the sample complexity of our method is optimal as a function of the failure probability $\\tau$, having an {\\em additive} $\\log(1/\\tau)$ dependence. Our algorithm leverages the stability-based approach from the algorithmic robust statistics literature, with crucial (and necessary) adaptations required in our setting. Our analysis may be of independent interest, involving the delicate design of a (non-spectral) decomposition for positive semi-definite matrices satisfying certain sparsity properties."}}
{"id": "yc5foFY2mcR", "cdate": 1640995200000, "mdate": 1653672539025, "content": {"title": "Streaming Algorithms for High-Dimensional Robust Statistics", "abstract": "We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust estimation tasks. Unfortunately, all previous algorithms require storing the entire dataset, incurring memory at least quadratic in the dimension. In this work, we develop the first efficient streaming algorithms for high-dimensional robust statistics with near-optimal memory requirements (up to logarithmic factors). Our main result is for the task of high-dimensional robust mean estimation in (a strengthening of) Huber's contamination model. We give an efficient single-pass streaming algorithm for this task with near-optimal error guarantees and space complexity nearly-linear in the dimension. As a corollary, we obtain streaming algorithms with near-optimal space complexity for several more complex tasks, including robust covariance estimation, robust regression, and more generally robust stochastic optimization."}}
{"id": "qMS17gkoU1N", "cdate": 1640995200000, "mdate": 1674791575272, "content": {"title": "Simple Binary Hypothesis Testing under Communication Constraints", "abstract": "We study simple binary hypothesis testing under communication constraints, a.k.a. \u201cdecentralized detection\u201d. Here, each sample is mapped to a message from a finite set of messages via a channel before being revealed to a statistician. In the absence of communication constraints, it is well known that the sample complexity is characterized by the Hellinger distance between the distributions. We show that the sample complexity of hypothesis testing under communication constraints is at most a logarithmic factor larger than in the unconstrained setting, and demonstrate that distributions exist in which this characterization is tight. We also provide a polynomial-time algorithm which achieves the aforementioned sample complexity. Our proofs rely on a new reverse data processing inequality and a reverse Markov\u2019s inequality, which may be of independent interest."}}
{"id": "dYBjwFXcYQ", "cdate": 1640995200000, "mdate": 1674791575282, "content": {"title": "Outlier-Robust Sparse Mean Estimation for Heavy-Tailed Distributions", "abstract": "We study the fundamental task of outlier-robust mean estimation for heavy-tailed distributions in the presence of sparsity. Specifically, given a small number of corrupted samples from a high-dimensional heavy-tailed distribution whose mean $\\mu$ is guaranteed to be sparse, the goal is to efficiently compute a hypothesis that accurately approximates $\\mu$ with high probability. Prior work had obtained efficient algorithms for robust sparse mean estimation of light-tailed distributions. In this work, we give the first sample-efficient and polynomial-time robust sparse mean estimator for heavy-tailed distributions under mild moment assumptions. Our algorithm achieves the optimal asymptotic error using a number of samples scaling logarithmically with the ambient dimension. Importantly, the sample complexity of our method is optimal as a function of the failure probability $\\tau$, having an additive $\\log(1/\\tau)$ dependence. Our algorithm leverages the stability-based approach from the algorithmic robust statistics literature, with crucial (and necessary) adaptations required in our setting. Our analysis may be of independent interest, involving the delicate design of a (non-spectral) decomposition for positive semi-definite matrices satisfying certain sparsity properties."}}
{"id": "T6c_mlKAo5", "cdate": 1640995200000, "mdate": 1674791575274, "content": {"title": "Communication-constrained hypothesis testing: Optimality, robustness, and reverse data processing inequalities", "abstract": "We study hypothesis testing under communication constraints, where each sample is quantized before being revealed to a statistician. Without communication constraints, it is well known that the sample complexity of simple binary hypothesis testing is characterized by the Hellinger distance between the distributions. We show that the sample complexity of simple binary hypothesis testing under communication constraints is at most a logarithmic factor larger than in the unconstrained setting and this bound is tight. We develop a polynomial-time algorithm that achieves the aforementioned sample complexity. Our framework extends to robust hypothesis testing, where the distributions are corrupted in the total variation distance. Our proofs rely on a new reverse data processing inequality and a reverse Markov inequality, which may be of independent interest. For simple $M$-ary hypothesis testing, the sample complexity in the absence of communication constraints has a logarithmic dependence on $M$. We show that communication constraints can cause an exponential blow-up leading to $\\Omega(M)$ sample complexity even for adaptive algorithms."}}
{"id": "RSUlF7H5xK", "cdate": 1640995200000, "mdate": 1674791575295, "content": {"title": "Gaussian Mean Testing Made Simple", "abstract": "We study the following fundamental hypothesis testing problem, which we term Gaussian mean testing. Given i.i.d. samples from a distribution $p$ on $\\mathbb{R}^d$, the task is to distinguish, with high probability, between the following cases: (i) $p$ is the standard Gaussian distribution, $\\mathcal{N}(0,I_d)$, and (ii) $p$ is a Gaussian $\\mathcal{N}(\\mu,\\Sigma)$ for some unknown covariance $\\Sigma$ and mean $\\mu \\in \\mathbb{R}^d$ satisfying $\\|\\mu\\|_2 \\geq \\epsilon$. Recent work gave an algorithm for this testing problem with the optimal sample complexity of $\\Theta(\\sqrt{d}/\\epsilon^2)$. Both the previous algorithm and its analysis are quite complicated. Here we give an extremely simple algorithm for Gaussian mean testing with a one-page analysis. Our algorithm is sample optimal and runs in sample linear time."}}
