{"id": "kUk79nBY__2", "cdate": 1675398519688, "mdate": null, "content": {"title": "A Hitchhiker's Guide to Momentum", "abstract": "Polyak momentum is one of the most iconic methods in optimization. Despite it's simplicit, it features rich dynamics that depend both on the step-size and momentum parameter. In this blog post we identify the different regions of the parameter space and discuss their convergence properties using the theory of Chebyshev polynomials."}}
{"id": "MbZ3_HSk9-K", "cdate": 1664731452166, "mdate": null, "content": {"title": "Momentum Extragradient is Optimal for Games with Cross-Shaped Spectrum", "abstract": "The extragradient method has recently gained a lot of attention, due to its convergence behavior on smooth games. In games, the eigenvalues of the Jacobian of the vector field are distributed on the complex plane, exhibiting more convoluted dynamics compared to minimization. In this work, we take a polynomial-based analysis of the extragradient with momentum for optimizing games with \\emph{cross-shaped} spectrum on the complex plane. We show two results: first, the extragradient with momentum exhibits three different modes of convergence based on the hyperparameter setup: when the eigenvalues are distributed $(i)$ on the real line, $(ii)$ both on the real line along with complex conjugates, and $(iii)$ only as complex conjugates. Then, we focus on the case $(ii)$, i.e., when the spectrum of the Jacobian has \\emph{cross-shaped} structure, as observed in training generative adversarial networks. For this problem class, we derive the optimal hyperparameters and show that the extragradient with momentum achieves accelerated convergence rate."}}
{"id": "i1h0gZ0KTxZ", "cdate": 1664731450750, "mdate": null, "content": {"title": "A Novel Stochastic Gradient Descent Algorithm for LearningPrincipal Subspaces", "abstract": "In this paper,  we derive an algorithm that learns a principal subspace from sample entries,  can be applied when the approximate subspace is represented by a neural network, and hence can bescaled to datasets with an effectively infinite number of rows and columns.  Our method consistsin defining a loss function whose minimizer is the desired principal subspace, and constructing agradient estimate of this loss whose bias can be controlled."}}
{"id": "A7IpSdqNWHy", "cdate": 1664731447384, "mdate": null, "content": {"title": "A Second-order Regression Model Shows Edge of Stability Behavior", "abstract": "Recent studies of learning algorithms have shown that there is a regime with an initial increase in the largest eigenvalue of the loss Hessian (progressive sharpening), followed by a stabilization of the eigenvalue near the maximum value which allows convergence (edge of stability). We consider a class of predictive models that are quadratic in the parameters, which we call second-order regression models. This is in contrast with the neural tangent kernel regime, where the predictive function is linear in the parameters. For quadratic objectives in two dimensions, we prove that this second order regression model exhibits both progressive sharpening and edge of stability behavior. We then show that in higher dimensions, the model shows this behavior generically without the structure of a neural network, due to a non-linearity induced in the learning dynamics. Finally, we show that edge of stability behavior in neural networks is correlated with the behavior in quadratic regression models."}}
{"id": "R2M14I9LEwW", "cdate": 1663850478851, "mdate": null, "content": {"title": "A second order regression model shows edge of stability behavior", "abstract": "Recent studies of learning algorithms have shown that there is a regime with an initial increase in the largest eigenvalue of the loss Hessian (progressive sharpening), followed by a stabilization of the eigenvalue near the maximum value which allows convergence (edge of stability). We consider a class of predictive models that are quadratic in the parameters, which we call second-order regression models. This is in contrast with the neural tangent kernel regime, where the predictive function is linear in the parameters. For quadratic objectives in two dimensions, we prove that this second order regression model exhibits both progressive sharpening and edge of stability behavior. We then show that in higher dimensions, the model shows this behavior generically without the structure of a neural network, due to a non-linearity induced in the learning dynamics. Finally, we show that edge of stability behavior in neural networks is correlated with the behavior in quadratic regression models."}}
{"id": "b57KM4ydqpp", "cdate": 1652737542225, "mdate": null, "content": {"title": "The Curse of Unrolling: Rate of Differentiating Through Optimization", "abstract": "Computing the Jacobian of the solution of an optimization problem is a central problem in machine learning, with applications in hyperparameter optimization, meta-learning, optimization as a layer, and dataset distillation, to name a few. Unrolled differentiation is a popular heuristic that approximates the solution using an iterative solver and differentiates it through the computational path. This work provides a non-asymptotic convergence-rate analysis of this approach on quadratic objectives for gradient descent and the Chebyshev method. We show that to ensure convergence of the Jacobian, we can either 1) choose a large learning rate leading to a fast asymptotic convergence but accept that the algorithm may have an arbitrarily long burn-in phase or 2) choose a smaller learning rate leading to an immediate but slower convergence. We refer to this phenomenon as the curse of unrolling.\nFinally, we discuss open problems relative to this approach, such as deriving a practical update rule for the optimal unrolling strategy and making novel connections with the field of Sobolev orthogonal polynomials."}}
{"id": "Q-HOv_zn6G", "cdate": 1652737382793, "mdate": null, "content": {"title": "Efficient and Modular Implicit Differentiation", "abstract": "Automatic differentiation (autodiff) has revolutionized machine learning.  It\nallows to express complex computations by composing elementary ones in creative\nways and removes the burden of computing their derivatives by hand. More\nrecently, differentiation of optimization problem solutions has attracted\nwidespread attention with applications such as optimization layers, and in\nbi-level problems such as hyper-parameter optimization and meta-learning.\nHowever, so far, implicit differentiation remained difficult to use for\npractitioners, as it often required case-by-case tedious mathematical\nderivations and implementations. In this paper, we propose\nautomatic implicit differentiation, an efficient\nand modular approach for implicit differentiation of optimization problems. In\nour approach, the user defines directly in Python a function $F$ capturing the\noptimality conditions of the problem to be differentiated. Once this is done, we\nleverage autodiff of $F$ and the implicit function theorem to automatically\ndifferentiate the optimization problem.  Our approach thus combines the benefits\nof implicit differentiation and autodiff.  It is efficient as it can be added on\ntop of any state-of-the-art solver and modular as the optimality condition\nspecification is decoupled from the implicit differentiation mechanism.  We show\nthat seemingly simple principles allow to recover many existing implicit\ndifferentiation methods and create new ones easily.  We demonstrate the ease of\nformulating and solving bi-level optimization problems using our framework. We\nalso showcase an application to the sensitivity analysis of molecular dynamics."}}
{"id": "xDwOMWsHp_A", "cdate": 1650586647390, "mdate": 1650586647390, "content": {"title": "Super-Acceleration with Cyclical Step-sizes", "abstract": "We develop a convergence-rate analysis of momentum with cyclical step-sizes. We show that under some assumption on the spectral gap of Hessians in machine learning, cyclical step-sizes are provably faster than constant step-sizes. More precisely, we develop a convergence rate analysis for quadratic objectives that provides optimal parameters and shows that cyclical learning rates can improve upon traditional lower complexity bounds. We further propose a systematic approach to design optimal first order methods for quadratic minimization with a given spectral structure. Finally, we provide a local convergence rate analysis beyond quadratic minimization for the proposed methods and illustrate our findings through benchmarks on least squares and logistic regression problems."}}
{"id": "TQ75Md-FqQp", "cdate": 1632875747919, "mdate": null, "content": {"title": "Efficient and Modular Implicit Differentiation", "abstract": "Automatic differentiation (autodiff) has revolutionized machine learning.  It allows expressing complex computations by composing elementary ones in creative ways and removes the tedious burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted a great deal of research, with applications as a layer in a neural network, and in bi-level optimization, including hyper-parameter optimization. However, the formulae for these derivatives often involves a tedious manual derivation and implementation. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function $F$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of $F$ to automatically differentiate the optimization problem. This way, our approach combines the benefits of implicit differentiation and autodiff.  We show that seemingly simple principles allow to recover all recently proposed implicit differentiation methods and create new ones easily. We describe in details a JAX implementation of our framework and demonstrate the ease of differentiating through optimization problems thanks to it on four diverse tasks: hyperparameter optimization of multiclass SVMs, dataset distillation, task-driven dictionary learning and sensitivity analysis of molecular dynamics."}}
{"id": "VKtGrkUvCR", "cdate": 1632875676396, "mdate": null, "content": {"title": "Only tails matter: Average-Case Universality and Robustness in the Convex Regime", "abstract": "Recent works have studied the average convergence properties of first-order optimization methods on distributions of quadratic problems. The average-case framework allows a more fine-grained and representative analysis of convergence than usual worst-case results, in exchange for a more precise hypothesis over the data generating process, namely assuming knowledge of the expected spectral distribution (e.s.d) of the random matrix associated with the problem. In this work, we show that a problem's asymptotic average complexity is determined by the concentration of eigenvalues near the edges of the e.s.d. We argue that having \u00e0 priori information on this concentration is a more grounded assumption than complete knowledge of the e.s.d.,  and that basing our analysis on the approximate concentration is effectively a middle ground between the coarseness of the worst-case convergence and this more unrealistic hypothesis. We introduce the Generalized Chebyshev method, asymptotically optimal under a hypothesis on this concentration, and globally optimal when the e.s.d. follows  a Beta distribution. We compare its performance to classical optimization algorithms, such as Gradient Descent or Nesterov's scheme, and we show that, asymptotically, Nesterov's method is universally nearly-optimal in the average-case."}}
