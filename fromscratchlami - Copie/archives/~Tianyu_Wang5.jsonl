{"id": "mzaX5eyWnY", "cdate": 1684343102175, "mdate": 1684343102175, "content": {"title": "Spatially Invariant Unsupervised 3D Object-Centric Learning and Scene Decomposition", "abstract": "We tackle the problem of object-centric learning on point clouds, which is crucial for high-level relational reasoning and scalable machine intelligence. In particular, we introduce a framework, SPAIR3D, to factorize a 3D point cloud into a spatial mixture model where each component corresponds to one object. To model the spatial mixture model on point clouds, we derive the Chamfer Mixture Loss, which fits naturally into our variational training pipeline. Moreover, we adopt an object-specification scheme that describes each object\u2019s location relative to its local voxel grid cell. Such a scheme allows SPAIR3D to model scenes with an arbitrary number of objects. We evaluate our method on the task of unsupervised scene decomposition. Experimental results demonstrate that SPAIR3D has strong scalability and is capable of detecting and segmenting an unknown number of objects from a point cloud in an unsupervised manner."}}
{"id": "yvF7mAuWv3z", "cdate": 1663850059752, "mdate": null, "content": {"title": "Scalable 3D Object-centric Learning", "abstract": "We tackle the task of unsupervised 3D object-centric representation learning on scenes of potentially unbounded scale. \n  Existing approaches to object-centric representation learning exhibit significant limitations in achieving scalable inference due to their dependencies on a fixed global coordinate system. \n  In contrast, we propose to learn view-invariant 3D object representations in localized object coordinate systems. \n  To this end, we estimate the object pose and appearance representation separately and explicitly project object representations across views. \n  We adopt amortized variational inference to process sequential input and update object representations online. \n  To scale up our model to scenes with an arbitrary number of objects, we further introduce a Cognitive Map that allows the registration and querying of objects on a global map. \n  We employ the object-centric neural radiance field (NeRF) as our 3D scene representation, which is jointly inferred by our unsupervised object-centric learning framework. \n  Experimental results demonstrate that our method can infer and maintain object-centric representations of unbounded 3D scenes. \n  Further combined with a per-object NeRF finetuning process, our model can achieve scalable high-quality object-aware scene reconstruction."}}
{"id": "92leLHqlcvv", "cdate": 1652737458803, "mdate": null, "content": {"title": "A Direct Approximation of AIXI Using Logical State Abstractions", "abstract": "We propose a practical integration of logical state abstraction with AIXI, a Bayesian optimality notion for reinforcement learning agents, to significantly expand the model class that AIXI agents can be approximated over to complex history-dependent and structured environments. The state representation and reasoning framework is based on higher-order logic, which can be used to define and enumerate complex features on non-Markovian and structured environments. We address the problem of selecting the right subset of features to form state abstractions by adapting the $\\Phi$-MDP optimisation criterion from state abstraction theory. Exact Bayesian model learning is then achieved using a suitable generalisation of Context Tree Weighting over abstract state sequences. The resultant architecture can be integrated with different planning algorithms. Experimental results on controlling epidemics on large-scale contact networks validates the agent's performance."}}
{"id": "GiddFXGDmqp", "cdate": 1632875488961, "mdate": null, "content": {"title": "Spatially Invariant Unsupervised 3D Object-Centric Learning and Scene Decomposition", "abstract": "We tackle the problem of deep object-centric learning from a point cloud which is crucial for high-level relational reasoning and scalable machine intelligence. \nIn particular, we introduce a framework, SPAIR3D, to factorize a 3D point cloud into a spatial mixture model where each component corresponds to one object. \nTo model the spatial mixture model on point clouds, we derive the Chamfer Mixture Loss, which fits naturally into our variational training pipeline. Moreover, we adopt an object-specification scheme that describes each object\u2019s location relative to its local voxel grid cell. \nSuch a scheme allows SPAIR3D to model scenes with an arbitrary number of objects. \nWe evaluate our method on the task of unsupervised scene decomposition.\nExperimental results demonstrate that SPAIR3D has strong scalability and is capable of detecting and segmenting an unknown number of objects from a point cloud in an unsupervised manner."}}
