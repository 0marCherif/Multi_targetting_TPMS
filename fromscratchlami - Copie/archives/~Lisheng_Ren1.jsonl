{"id": "p-56bnzZhQ7", "cdate": 1652737837198, "mdate": null, "content": {"title": "Cryptographic Hardness of Learning Halfspaces with Massart Noise", "abstract": "We study the complexity of PAC learning halfspaces in the presence of Massart noise. In this problem, we are given i.i.d. labeled examples $(\\mathbf{x}, y) \\in \\mathbb{R}^N \\times \\{ \\pm 1\\}$, where the distribution of $\\mathbf{x}$ is arbitrary and the label $y$ is a Massart corruption of $f(\\mathbf{x})$, for an unknown halfspace $f: \\mathbb{R}^N \\to \\{ \\pm 1\\}$, with flipping probability $\\eta(\\mathbf{x}) \\leq \\eta < 1/2$. The goal of the learner is to compute a hypothesis with small 0-1 error. Our main result is the first computational hardness result for this learning problem. Specifically, assuming the (widely believed) subexponential-time hardness of the Learning with Errors (LWE) problem, we show that no polynomial-time Massart halfspace learner can achieve error better than $\\Omega(\\eta)$, even if the optimal 0-1 error is small, namely $\\mathrm{OPT}  =  2^{-\\log^{c} (N)}$ for any universal constant $c \\in (0, 1)$. Prior work had provided qualitatively similar evidence of hardness in the Statistical Query model. Our computational hardness result essentially resolves the polynomial PAC learnability of Massart halfspaces, by showing that known efficient learning algorithms for the problem are nearly best possible."}}
{"id": "OYqCR-f-dg", "cdate": 1652737790588, "mdate": null, "content": {"title": "SQ Lower Bounds for Learning Single Neurons with Massart Noise", "abstract": "We study the problem of PAC learning a single neuron in the presence of Massart noise. Specifically, for a known activation function $f: \\mathbb{R}\\to \\mathbb{R}$, the learner is given access to labeled examples $(\\mathbf{x}, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$, where the marginal distribution of $\\mathbf{x}$ is arbitrary and the corresponding label $y$ is a Massart corruption of $f(\\langle \\mathbf{w}, \\mathbf{x} \\rangle)$. The goal of the learner is to output a hypothesis $h: \\mathbb{R}^d \\to \\mathbb{R}$ with small squared loss. For a range of activation functions, including ReLUs, we establish super-polynomial Statistical Query (SQ) lower bounds for this learning problem. In more detail, we prove that no efficient SQ algorithm can approximate the optimal error within any constant factor. Our main technical contribution is a novel SQ-hard construction for learning $\\{ \\pm 1\\}$-weight Massart halfspaces on the Boolean hypercube that is interesting on its own right.\n"}}
