{"id": "z4aO_hRxqW", "cdate": 1675970198473, "mdate": null, "content": {"title": "Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement", "abstract": "Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards consumption entails risks. To promote digital well-being, most platforms now offer a service that periodically prompts users to take breaks. These, however, must be set up manually, and so may be suboptimal for both users and the system. In this paper, we study the role of breaks in recommendation, and propose a framework for learning optimal breaking policies that promote and sustain long-term engagement. Based on the notion that user-system dynamics incorporate both positive and negative feedback, we cast recommendation as Lotka-Volterra dynamics. We give an efficient learning algorithm, provide theoretical guarantees, and evaluate our approach on semi-synthetic data."}}
{"id": "e9i40gRPeC", "cdate": 1665069637301, "mdate": null, "content": {"title": "Learning to Take a Break: Sustainable Optimization of Long-Term User Engagement", "abstract": "Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take a break. These, however, must be set up manually, and so may be suboptimal for both users and the system.\nIn this paper, we propose a framework for optimizing long-term engagement by learning individualized breaking policies. Using Lotka-Volterra dynamics, we model users as acting based on two balancing latent states: drive, and interest---which must be conserved. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically evaluate its performance on semi-synthetic data."}}
{"id": "fwP9Bc4E71", "cdate": 1663849925367, "mdate": null, "content": {"title": "Learning to Take a Break: Sustainable Optimization of Long-Term User Engagement", "abstract": "Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take a break. These, however, must be set up manually, and so may be suboptimal for both users and the system.\nIn this paper, we propose a framework for optimizing long-term engagement by learning individualized breaking policies. Using Lotka-Volterra dynamics, we model users as acting based on two balancing latent states: drive, and interest---which must be conserved. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically evaluate its performance on semi-synthetic data."}}
{"id": "A-4til2q9g", "cdate": 1640995200000, "mdate": 1671566050277, "content": {"title": "Learning to Take a Break: Sustainable Optimization of Long-Term User Engagement", "abstract": "Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take breaks. These, however, must be set up manually, and so may be suboptimal for both users and the system. In this paper, we study the role of breaks in recommendation, and propose a framework for learning optimal breaking policies that promote and sustain long-term engagement. Based on the notion that recommendation dynamics are susceptible to both positive and negative feedback, we cast recommendation as a Lotka-Volterra dynamical system, where breaking reduces to a problem of optimal control. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically demonstrate the utility of our approach on semi-synthetic data."}}
{"id": "VlbI4ETtjqm", "cdate": 1546300800000, "mdate": 1652207700257, "content": {"title": "The Complexity of User Retention", "abstract": "This paper studies families of distributions T that are amenable to retentive learning, meaning that an expert can retain users that seek to predict their future, assuming user attributes are sampled from T and exposed gradually over time. Limited attention span is the main problem experts face in our model. We make two contributions. First, we formally define the notions of retentively learnable distributions and properties. Along the way, we define a retention complexity measure of distributions and a natural class of retentive scoring rules that model the way users evaluate experts they interact with. These rules are shown to be tightly connected to truth-eliciting \"proper scoring rules\" studied in Decision Theory since the 1950's [McCarthy, PNAS 1956]. Second, we take a first step towards relating retention complexity to other measures of significance in computational complexity. In particular, we show that linear properties (over the binary field) are retentively learnable, whereas random Low Density Parity Check (LDPC) codes have, with high probability, maximal retention complexity. Intriguingly, these results resemble known results from the field of property testing and suggest that deeper connections between retentive distributions and locally testable properties may exist."}}
{"id": "ADRHe_tGKuF", "cdate": 1546300800000, "mdate": 1652207700259, "content": {"title": "Evaluating Expert Curation in a Baby Milestone Tracking App", "abstract": "Early childhood developmental screening is critical for timely detection and intervention. babyTRACKS (Formerly Baby CROINC, CROwd INtelligence Curation.) is a free, live, interactive developmental tracking mobile app with over 3,000 children's diaries. Parents write or select short milestone texts, like \"began taking first steps,\" to record their babies' developmental achievements, and receive crowd-based percentiles to evaluate development and catch potential delays. Currently, an expert-based Curated Crowd Intelligence (CCI) process manually groups incoming novel parent-authored milestone texts according to their similarity to existing milestones in the database (for example, starting to walk), or determining that the milestone represents a new developmental concept not seen before in another child's diary. CCI cannot scale well, however, and babyTRACKS is mature enough, with a rich enough database of existing milestone texts, to now consider machine learning tools to replace or assist the human curators. Three new studies explore (1) the usefulness of automation, by analyzing the human cost of CCI and how the work is currently broken down; (2) the validity of automation, by testing the inter-rater reliability of curators; and (3) the value of automation, by appraising the \"real world\" clinical value of milestones when assessing child development. We conclude that automation can indeed be appropriate and helpful for a large percentage, though not all, of CCI work. We further establish realistic upper bounds for algorithm performance; confirm that the babyTRACKS milestones dataset is valid for training and testing purposes; and verify that it represents clinically meaningful developmental information."}}
{"id": "3pFbSPUOVXP", "cdate": 1514764800000, "mdate": 1652207700259, "content": {"title": "Brief Announcement: Towards an Abstract Model of User Retention Dynamics", "abstract": "A theoretical model is suggested for abstracting the interaction between an expert system and its users, with a focus on reputation and incentive compatibility. The model assumes users interact with the system while keeping in mind a single \"retention parameter\" that measures the strength of their belief in its predictive power, and the system's objective is to reinforce and maximize this parameter through \"informative\" and \"correct\" predictions. We define a natural class of retentive scoring rules to model the way users update their retention parameter and thus evaluate the experts they interact with. Assuming agents in the model have an incentive to report their true belief, these rules are shown to be tightly connected to truth-eliciting \"proper scoring rules\" studied in Decision Theory. The difference between users and experts is modeled by imposing different limits on their predictive abilities, characterized by a parameter called memory span. We prove the monotonicity theorem (\"more knowledge is better\"), which shows that experts with larger memory span retain better in expectation. Finally, we focus on the intrinsic properties of phenomena that are amenable to collaborative discovery with a an expert system. Assuming user types (or \"identities\") are sampled from a distribution D, the retention complexity of D is the minimal initial retention value (or \"strength of faith\") that a user must have before approaching the expert, in order for the expert to retain that user throughout the collaborative discovery, during which the user \"discovers\" his true \"identity\". We then take a first step towards relating retention complexity to other established computational complexity measures by studying retention dynamics when D is a uniform distribution over a linear space."}}
{"id": "D6_LBYN_og", "cdate": 1483228800000, "mdate": 1652207700251, "content": {"title": "Collaborative Discovery: A study of Guru-follower dynamics", "abstract": ""}}
{"id": "2H2jrPHZFs9", "cdate": 1483228800000, "mdate": 1652207700258, "content": {"title": "Baby CROINC: an online, crowd-based, expert-curated system for monitoring child development", "abstract": "Baby CROINC (CROwd INtelligence Curation) is an online early-childhood development tracker designed to be both personalized and objective. To meet these goals, we rely on Curated Crowd Intelligence (CCI), a process in which experts curate personalized inputs to connect with the crowd's aggregate data, providing parents with objective and personalized feedback on their children's development. In this paper, we describe Baby CROINC's design, with a focus on CCI, and assess the extent to which it meets its design goals of objectivity and personalization. In Baby CROINC, parents create a diary by adding developmental milestones to a timeline. Visual statistics are presented per milestone. Expert curators clarify, merge, and classify milestones which are new to the system. Diary personalization was evident through users' rich and diverse milestone choices, and by the continuous system increase in new canonical developmental concepts. Findings demonstrate the objectivity of the crowd-based percentiles extracted from Baby CROINC, based on consistency of developmental differences in preterm vs. fullterm and boys vs. girls with established research, and the correlation between medians reported in our system and those appearing on the U.S. Centers for Disease Control and Prevention's Milestones webpage.1 CCI led to a dramatic increase in users' ability to view crowd-based statistics, indicating that CCI is critical for enabling objectivity while maintaining personalization."}}
