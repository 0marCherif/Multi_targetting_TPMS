{"id": "mI5HydUBFY", "cdate": 1667337320407, "mdate": 1667337320407, "content": {"title": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning", "abstract": "The limited availability of annotated data often hinders real-world applications of machine learning. To efficiently learn from small quantities of multimodal data, we leverage the linguistic knowledge from a large pre-trained language model (PLM) and quickly adapt it to new domains of image captioning. To effectively utilize a pretrained model, it is critical to balance the visual input and prior linguistic knowledge from pretraining. We propose VisualGPT, which employs a novel self-resurrecting encoder decoder attention mechanism to quickly adapt the PLM with a small amount of in-domain image-text data. The proposed self-resurrecting activation unit produces sparse activations that prevent accidental overwriting of linguistic knowledge. When trained on 0.1%, 0.5% and 1% of the respective training sets, VisualGPT surpasses the best baseline by up to 10.0% CIDEr on MS COCO and 17.9% CIDEr on Conceptual Captions. Furthermore,\nVisualGPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset\u3002"}}
{"id": "dZrQR7OR11", "cdate": 1663850315258, "mdate": null, "content": {"title": "Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach", "abstract": "The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy."}}
{"id": "CWmvjOEhgH-", "cdate": 1663849877417, "mdate": null, "content": {"title": "MPCFORMER: FAST, PERFORMANT AND PRIVATE TRANSFORMER INFERENCE WITH MPC", "abstract": "Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60$\\times$ or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to $\\text{BERT}_\\text{BASE}$, while being 5.3$\\times$ faster. On the GLUE benchmark, it achieves 97% performance of $\\text{BERT}_\\text{BASE}$ with a 2.2$\\times$ speedup. MPCFORMER remains effective with different trained Transformer weights such as $\\text{ROBERTA}_\\text{BASE}$ and larger models including $\\text{BERT}_\\text{LARGE}$. Code is available at https://github.com/MccRee177/MPCFormer."}}
{"id": "9TdCcMlmsLm", "cdate": 1632875604978, "mdate": null, "content": {"title": "Text Generation with Efficient (Soft) $Q$-Learning", "abstract": "Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods."}}
{"id": "S1ZLTX-O-B", "cdate": 1546300800000, "mdate": null, "content": {"title": "AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning", "abstract": "Han Guo, Ramakanth Pasunuru, Mohit Bansal. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "Sk-iSiluZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation", "abstract": "An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model's learned saliency and entailment skills."}}
{"id": "rJbwlGMubH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Towards Improving Abstractive Summarization via Entailment Generation", "abstract": ""}}
