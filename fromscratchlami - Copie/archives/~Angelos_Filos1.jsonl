{"id": "t7rf_q-jIe", "cdate": 1666034717116, "mdate": null, "content": {"title": "Composing Task Knowledge with Modular Successor Feature Approximators", "abstract": "Recently, the Successor Features and Generalized Policy Improvement (SF&GPI)\nframework has been proposed as a method for learning, composing and transferring\npredictive knowledge and behavior. SF&GPI works by having an agent learn\npredictive representations (SFs) that can be combined for transfer to new tasks\nwith GPI. However, to be effective this approach requires state features that are\nuseful to predict, and these state-features are typically hand-designed. In this\nwork, we present a novel neural network architecture, \u201cModular Successor Feature\nApproximators\u201d (MSFA), where modules both discover what is useful to predict,\nand learn their own predictive representations. We show that MSFA is able to\nbetter generalize compared to baseline architectures for learning SFs and a modular\nnetwork that discovers factored state representations."}}
{"id": "9jsWJfk3xR", "cdate": 1665251228483, "mdate": null, "content": {"title": "In-context Reinforcement Learning with Algorithm Distillation", "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data."}}
{"id": "OridE7C5BP2", "cdate": 1664943346067, "mdate": null, "content": {"title": "In-context Reinforcement Learning with Algorithm Distillation", "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data."}}
{"id": "DrtSx1z40Ib", "cdate": 1663850378007, "mdate": null, "content": {"title": "Composing Task Knowledge With Modular Successor Feature Approximators", "abstract": "Recently, the Successor Features and Generalized Policy Improvement (SF&GPI) framework has been proposed as a method for learning, composing and transferring predictive knowledge and behavior. SF&GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, \u201cModular Successor Feature Approximators\u201d (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and a modular network that discovers factored state representations.\n\n"}}
{"id": "hy0a5MMPUv", "cdate": 1663850323468, "mdate": null, "content": {"title": "In-context Reinforcement Learning with Algorithm Distillation", "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data."}}
{"id": "rUqeQ7wZUbc", "cdate": 1646823195407, "mdate": null, "content": {"title": "Model-Value Inconsistency as a Signal for Epistemic Uncertainty", "abstract": "Using a model of the environment and a value function, an agent can construct many estimates of a state\u2019s value, by unrolling the model for different lengths and bootstrapping with its value function. Our key insight is that one can treat this set of value estimates as a type of ensemble, which we call an \\emph{implicit value ensemble} (IVE). Consequently, the discrepancy between these estimates can be used as a proxy for the agent\u2019s epistemic uncertainty; we term this signal \\emph{model-value inconsistency} or \\emph{self-inconsistency} for short. Unlike prior work which estimates uncertainty by training an ensemble of many models and/or value functions, this approach requires only the single model and value function which are already being learned in most model-based reinforcement learning algorithms. We provide empirical evidence in both tabular and function approximation settings from pixels that self-inconsistency is useful (i) as a signal for exploration, (ii) for acting safely under distribution shifts, and (iii) for robustifying value-based planning with a learned model."}}
{"id": "uJ2_JTpVCvc", "cdate": 1633790966501, "mdate": null, "content": {"title": "Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection Tasks", "abstract": "Bayesian deep learning seeks to equip deep neural networks with the ability to precisely quantify their predictive uncertainty, and has promised to make deep learning more reliable for safety-critical real-world applications. Yet, existing Bayesian deep learning methods fall short of this promise; new methods continue to be evaluated on unrealistic test beds that do not reflect the complexities of downstream real-world tasks that would benefit most from reliable uncertainty quantification. We propose a set of real-world tasks that accurately reflect such complexities and are designed to assess the reliability of predictive models in safety-critical scenarios. Specifically, we curate two publicly available datasets of high-resolution human retina images exhibiting varying degrees of diabetic retinopathy, a medical condition that can lead to blindness, and use them to design a suite of automated diagnosis tasks that require reliable predictive uncertainty quantification. We use these tasks to benchmark well-established and state-of-the-art Bayesian deep learning methods on task-specific evaluation metrics. We provide an easy-to-use codebase for fast and easy benchmarking following reproducibility and software design principles. We provide implementations of all methods included in the benchmark as well as results computed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and evaluation on at least 6 random seeds each. A full version of this paper is available at https://openreview.net/pdf?id=jyd4Lyjr2iB."}}
{"id": "jyd4Lyjr2iB", "cdate": 1629495983823, "mdate": null, "content": {"title": "Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection Tasks", "abstract": "Bayesian deep learning seeks to equip deep neural networks with the ability to precisely quantify their predictive uncertainty, and has promised to make deep learning more reliable for safety-critical real-world applications. Yet, existing Bayesian deep learning methods fall short of this promise; new methods continue to be evaluated on unrealistic test beds that do not reflect the complexities of downstream real-world tasks that would benefit most from reliable uncertainty quantification. We propose the RETINA Benchmark, a set of real-world tasks that accurately reflect such complexities and are designed to assess the reliability of predictive models in safety-critical scenarios. Specifically, we curate two publicly available datasets of high-resolution human retina images exhibiting varying degrees of diabetic retinopathy, a medical condition that can lead to blindness, and use them to design a suite of automated diagnosis tasks that require reliable predictive uncertainty quantification. We use these tasks to benchmark well-established and state-of-the-art Bayesian deep learning methods on task-specific evaluation metrics. We provide an easy-to-use codebase for fast and easy benchmarking following reproducibility and software design principles. We provide implementations of all methods included in the benchmark as well as results computed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and evaluation on at least 6 random seeds each."}}
{"id": "x2rdRAx3QF", "cdate": 1621630080465, "mdate": null, "content": {"title": "Self-Consistent Models and Values", "abstract": "Learned models of the environment provide reinforcement learning (RL) agents with flexible ways of making predictions about the environment.\nModels enable planning, i.e. using more computation to improve value functions or policies, without requiring additional environment interactions.\nIn this work, we investigate a way of augmenting model-based RL, by additionally encouraging a learned model and value function to be jointly \\emph{self-consistent}.\nThis lies in contrast to classic planning methods like Dyna, which only update the value function to be consistent with the model.\nWe propose a number of possible self-consistency updates, study them empirically in both the tabular and function approximation settings, and find that with appropriate choices self-consistency can be useful both for policy evaluation and control."}}
{"id": "H-PvDNIex", "cdate": 1579955770322, "mdate": null, "content": {"title": "Uncertainty Evaluation Metric for Brain Tumour Segmentation", "abstract": "In this paper, we describe and explore the metric that was designed to assess and rank uncertainty measures for the task of brain tumour sub-tissue segmentation in the BraTS 2019 sub-challenge on uncertainty quantification. The metric is designed to (1) reward uncertainty measures where high confidence is assigned to correct assertions, and where incorrect assertions are assigned low confidence and (2) penalize measures that have higher percentages of under-confident correct assertions.  Here, the workings of the metrics explored based on a number of popular uncertainty measures evaluated on the BraTS2019 dataset"}}
