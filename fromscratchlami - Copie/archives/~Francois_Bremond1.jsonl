{"id": "6WlVoZbVbj6", "cdate": 1668593742624, "mdate": 1668593742624, "content": {"title": "Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection", "abstract": "In video understanding, most cross-modal knowledge distillation (KD) methods are tailored for classification tasks, focusing on the discriminative representation of the trimmed videos. However, action detection requires not only categorizing actions, but also localizing them in untrimmed videos. Therefore, transferring knowledge pertaining to temporal relations is critical for this task which is missing in the previous cross-modal KD frameworks. To this end, we aim at learning an augmented RGB representation for action detection, taking advantage of additional modalities at training time through KD. We propose a KD framework consisting of two levels of distillation. On one hand, atomic-level distillation encourages the RGB student to learn the sub-representation of the actions from the teacher in a contrastive manner. On the other hand, sequence-level distillation encourages the student to learn the temporal knowledge from the teacher, which consists of transferring the Global Contextual Relations and the action Boundary Saliency. The result is an Augmented-RGB stream that can achieve competitive performance as the two-stream network while using only RGB at inference time. Extensive experimental analysis shows that our proposed distillation framework is generic and outperforms other popular cross-modal distillation methods in the action detection task.\n"}}
{"id": "PmQr2WNn8r", "cdate": 1668593661487, "mdate": 1668593661487, "content": {"title": "Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection", "abstract": "Designing activity detection systems that can be successfully deployed in daily-living environments requires datasets that pose the challenges typical of real-world scenarios. In this paper, we introduce a new untrimmed daily-living dataset that features several real-world challenges: Toyota Smarthome Untrimmed (TSU). TSU contains a wide variety of activities performed in a spontaneous manner. The dataset contains dense annotations including elementary, composite activities, and activities involving interactions with objects. We provide an analysis of the real-world challenges featured by our dataset, highlighting the open issues for detection algorithms. We show that current state-of-the-art methods fail to achieve satisfactory performance on the TSU dataset. Therefore, we propose a new baseline method for activity detection to tackle the novel challenges provided by our dataset. This method leverages one modality (i.e. optic flow) to generate the attention weights to guide another modality (i.e RGB) to better detect the activity boundaries. This is particularly beneficial to detect activities characterized by high temporal variance. We show that the method we propose outperforms state-of-the-art methods on TSU and on another popular challenging dataset, Charades."}}
{"id": "UHKVpSFJn5q", "cdate": 1667609901334, "mdate": 1667609901334, "content": {"title": "MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection", "abstract": "Action detection is a significant and challenging task, especially in densely-labelled datasets of untrimmed videos.\nSuch data consist of complex temporal relations including\ncomposite or co-occurring actions. To detect actions in\nthese complex settings, it is critical to capture both shortterm and long-term temporal information efficiently. To\nthis end, we propose a novel \u2018ConvTransformer\u2019 network\nfor action detection: MS-TCT. This network comprises\nof three main components: (1) a Temporal Encoder module which explores global and local temporal relations at\nmultiple temporal resolutions, (2) a Temporal Scale Mixer\nmodule which effectively fuses multi-scale features, creating a unified feature representation, and (3) a Classification\nmodule which learns a center-relative position of each action instance in time, and predicts frame-level classification\nscores. Our experimental results on multiple challenging\ndatasets such as Charades, TSU and MultiTHUMOS, validate the effectiveness of the proposed method, which outperforms the state-of-the-art methods on all three datasets."}}
{"id": "Jr41e6GxJn", "cdate": 1667609787348, "mdate": 1667609787348, "content": {"title": "VPN: Learning Video-Pose Embedding for Activities of Daily Living", "abstract": "In this paper, we focus on the spatio-temporal aspect of recognizing Activities of Daily Living (ADL). ADL have two specific properties (i) subtle spatio-temporal patterns and (ii) similar visual patterns varying with time. Therefore, ADL may look very similar and often necessitate to look at their fine-grained details to distinguish them. Because the recent spatio-temporal 3D ConvNets are too rigid to capture the subtle visual patterns across an action, we propose a novel Video-Pose Network: VPN. The 2 key components of this VPN are a spatial embedding and an attention network. The spatial embedding projects the 3D poses and RGB cues in a common semantic space. This enables the action recognition framework to learn better spatio-temporal features exploiting both modalities. In order to discriminate similar actions, the attention network provides two functionalities - (i) an end-to-end learnable pose backbone exploiting the topology of human body, and (ii) a coupler to provide joint spatio-temporal attention weights across a video. Experiments show that VPN outperforms the state-of-the-art results for action classification on a large scale human activity dataset: NTU-RGB+D 120, its subset NTU-RGB+D 60, a real-world challenging human activity dataset: Toyota Smarthome and a small scale human-object interaction dataset Northwestern UCLA."}}
{"id": "qbiywr8TWqG", "cdate": 1667609516732, "mdate": 1667609516732, "content": {"title": "Toyota Smarthome: Real-World Activities of Daily Living", "abstract": "The performance of deep neural networks is strongly\ninfluenced by the quantity and quality of annotated data.\nMost of the large activity recognition datasets consist of\ndata sourced from the web, which does not reflect challenges that exist in activities of daily living. In this paper,\nwe introduce a large real-world video dataset for activities\nof daily living: Toyota Smarthome. The dataset consists of\n16K RGB+D clips of 31 activity classes, performed by seniors in a smarthome. Unlike previous datasets, videos were\nfully unscripted. As a result, the dataset poses several challenges: high intra-class variation, high class imbalance,\nsimple and composite activities, and activities with similar motion and variable duration. Activities were annotated\nwith both coarse and fine-grained labels. These characteristics differentiate Toyota Smarthome from other datasets\nfor activity recognition. As recent activity recognition approaches fail to address the challenges posed by Toyota\nSmarthome, we present a novel activity recognition method\nwith attention mechanism. We propose a pose driven spatiotemporal attention mechanism through 3D ConvNets. We\nshow that our novel method outperforms state-of-the-art\nmethods on benchmark datasets, as well as on the Toyota\nSmarthome dataset. We release the dataset for research\nuse."}}
{"id": "wF-QfZebInw", "cdate": 1663850244190, "mdate": null, "content": {"title": "Pose Transfer using a Single Spatial Transformation", "abstract": "In this paper, we address the problem of pose transfer. The goal is to generate a source image in a new target pose. The pose is already provided by a set of spatial landmarks. The transfer function is directly estimated from the difference between the landmarks given in the new target pose and the landmarks of the source image. Existing methods perform the task using two specialized networks, one to move the patches of the source sample and the other one to generate the new patches that are not visible in the source image. Contrary to these strategies, we develop an end-to-end trainable neural network that learns to estimate both these visible and invisible parts using a simple warping module. In other words, we propose a flow estimation method that not only displaces the patches to their new locations but also generates new pixels that are not visible in the source image, all in an unsupervised manner without the need for a ground-truth flow map.  In this way, moving the patches and introducing new parts are unified into a single network, ensuring that an overall solution is achieved for these two mutual tasks. Additionally, this method avoids the need for a human observer to determine a trade-off between the performance of the two separated networks, thus avoiding a cartoonish addition of the new parts to the visible areas. Extensive experiments demonstrate the superiority of our method over state-of-the-art algorithms. We conduct our experiments on two well-known datasets: Deepfashion and Market1501.\n"}}
{"id": "7r6kDq0mK_", "cdate": 1632875497348, "mdate": null, "content": {"title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation", "abstract": "Due to the remarkable progress of deep generative models, animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation is instrumental in transferring motion from driving videos to still images. However, such approaches fail in case the source image and driving video encompass large appearance variation. Moreover, the extraction of structure information requires additional modules that endow the animation-model with increased complexity. Deviating from such models, we here introduce the Latent Image Animator (LIA), a self-supervised autoencoder that evades need for structure representation. LIA is streamlined to animate images by linear navigation in the latent space. Specifically, motion in generated video is constructed by linear displacement of codes in the latent space. Towards this, we learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement in the latent space. Extensive quantitative and qualitative analysis suggests that our model systematically and significantly outperforms state-of-art methods on VoxCeleb, Taichi and TED-talk datasets w.r.t. generated quality."}}
{"id": "vZU_h_LXJs1M", "cdate": 1609459200000, "mdate": 1632857997335, "content": {"title": "PDAN: Pyramid Dilated Attention Network for Action Detection", "abstract": "Handling long and complex temporal information is an important challenge for action detection tasks. This challenge is further aggravated by densely distributed actions in untrimmed videos. Previous action detection methods fail in selecting the key temporal information in long videos. To this end, we introduce the Dilated Attention Layer (DAL). Compared to previous temporal convolution layer, DAL allocates attentional weights to local frames in the kernel, which enables it to learn better local representation across time. Furthermore, we introduce Pyramid Dilated Attention Network (PDAN) which is built upon DAL. With the help of multiple DALs with different dilation rates, PDAN can model short-term and long-term temporal relations simultaneously by focusing on local segments at the level of low and high temporal receptive fields. This property enables PDAN to handle complex temporal relations between different action instances in long untrimmed videos. To corroborate the effectiveness and robustness of our method, we evaluate it on three densely annotated, multi-label datasets: Mul-tiTHUMOS, Charades and Toyota Smarthome Untrimmed (TSU) dataset. PDAN is able to outperform previous state-of-the-art methods on all these datasets.\"Time abides long enough for those who make use of it."}}
{"id": "UZIRHtqpXYsb", "cdate": 1609459200000, "mdate": 1632857997374, "content": {"title": "Joint Generative and Contrastive Learning for Unsupervised Person Re-Identification", "abstract": "Recent self-supervised contrastive learning provides an effective approach for unsupervised person re-identification (ReID) by learning invariance from different views (transformed versions) of an input. In this paper, we incorporate a Generative Adversarial Network (GAN) and a contrastive learning module into one joint training framework. While the GAN provides online data augmentation for contrastive learning, the contrastive module learns view-invariant features for generation. In this context, we propose a mesh-based view generator. Specifically, mesh projections serve as references towards generating novel views of a person. In addition, we propose a view-invariant loss to facilitate contrastive learning between original and generated views. Deviating from previous GAN-based unsupervised ReID methods involving domain adaptation, we do not rely on a labeled source dataset, which makes our method more flexible. Extensive experimental results show that our method significantly outperforms state-of-the-art methods under both, fully unsupervised and unsupervised domain adaptive settings on several large scale ReID datsets."}}
{"id": "MlgzBaDeA8JC", "cdate": 1609459200000, "mdate": 1632857997314, "content": {"title": "Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for Unsupervised Person Re-identification", "abstract": "The objective of unsupervised person re-identification (Re-ID) is to learn discriminative features without laborintensive identity annotations. State-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled images in the target domain and learn from these noisy pseudo labels. Recently introduced Mean Teacher Model is a promising way to mitigate the label noise. However, during the training, self-ensembled teacher-student networks quickly converge to a consensus which leads to a local minimum. We explore the possibility of using an asymmetric structure inside neural network to address this problem. First, asymmetric branches are proposed to extract features in different manners, which enhances the feature diversity in appearance signatures. Then, our proposed cross-branch supervision allows one branch to get supervision from the other branch, which transfers distinct knowledge and enhances the weight diversity between teacher and student networks. Extensive experiments show that our proposed method can significantly surpass the performance of previous work on both unsupervised domain adaptation and fully unsupervised Re-ID tasks."}}
