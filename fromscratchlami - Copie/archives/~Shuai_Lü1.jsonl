{"id": "N47T1xkxKRS", "cdate": 1698796800000, "mdate": 1699596399543, "content": {"title": "Guided deterministic policy optimization with gradient-free policy parameters information", "abstract": ""}}
{"id": "B71LqcydzuY", "cdate": 1680307200000, "mdate": 1681654201961, "content": {"title": "Entropy regularization methods for parameter space exploration", "abstract": ""}}
{"id": "U0QZW-FRTaw", "cdate": 1672531200000, "mdate": 1693645115467, "content": {"title": "Adaptive Estimation Q-learning with Uncertainty and Familiarity", "abstract": "One of the key problems in model-free deep reinforcement learning is how to obtain more accurate value estimations. Current most widely-used off-policy algorithms suffer from over- or underestimation bias which may lead to unstable policy. In this paper, we propose a novel method, Adaptive Estimation Q-learning (AEQ), which uses uncertainty and familiarity to control the value estimation naturally and can adaptively change for specific state-action pair. We theoretically prove the property of our familiarity term which can even keep the expected estimation bias approximate to 0, and experimentally demonstrate our dynamic estimation can improve the performance and prevent the bias continuously increasing. We evaluate AEQ on several continuous control tasks, outperforming state-of-the-art performance. Moreover, AEQ is simple to implement and can be applied in any off-policy actor-critic algorithm."}}
{"id": "yS3Lin6gxbf", "cdate": 1640995200000, "mdate": 1681654202309, "content": {"title": "Actor-critic with familiarity-based trajectory experience replay", "abstract": ""}}
{"id": "ewwBz1f3F97", "cdate": 1640995200000, "mdate": 1681654202065, "content": {"title": "Unsupervised domain adaptation via softmax-based prototype construction and adaptation", "abstract": ""}}
{"id": "MyLXBqGvRt3", "cdate": 1640995200000, "mdate": 1681654201957, "content": {"title": "NROWAN-DQN: A stable noisy network with noise reduction and online weight adjustment for exploration", "abstract": ""}}
{"id": "Acn5NS-ExS", "cdate": 1640995200000, "mdate": 1681654204472, "content": {"title": "Proximal policy optimization via enhanced exploration efficiency", "abstract": ""}}
{"id": "8MALDu4m_YU", "cdate": 1640995200000, "mdate": 1681654202067, "content": {"title": "Sampling diversity driven exploration with state difference guidance", "abstract": ""}}
{"id": "5n2xSot9xS0", "cdate": 1640995200000, "mdate": 1681654202064, "content": {"title": "Enhancing transferability and discriminability simultaneously for unsupervised domain adaptation", "abstract": ""}}
{"id": "j-rZIfmN3o", "cdate": 1609459200000, "mdate": 1681654202760, "content": {"title": "Evolutionary Generative Adversarial Networks with Crossover Based Knowledge Distillation", "abstract": ""}}
