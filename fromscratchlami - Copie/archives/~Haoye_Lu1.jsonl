{"id": "TjEzIsyEsQ6", "cdate": 1663850016546, "mdate": null, "content": {"title": "Multi-Objective Reinforcement Learning: Convexity, Stationarity and Pareto Optimality", "abstract": "In recent years, single-objective reinforcement learning (SORL) algorithms have received a significant amount of attention and seen some strong results. However, it is generally recognized that many practical problems have intrinsic multi-objective properties that cannot be easily handled by SORL algorithms. Although there have been many multi-objective reinforcement learning (MORL) algorithms proposed, there has been little recent exploration of the fundamental properties of the spaces we are learning in. In this paper, we perform a rigorous analysis of policy induced value functions and use the insights to distinguish three views of Pareto optimality. The results imply the convexity of the induced value function's range for stationary policies and suggest that any point of its Pareto front can be achieved by training a policy using linear scalarization (LS). We show the problem that leads to the suboptimal performance of LS can be solved by adding strongly concave terms to the immediate rewards, which motivates us to propose a new vector reward-based Q-learning algorithm, CAPQL. Combined with an actor-critic formulation, our algorithm achieves state-of-the-art performance on multiple MuJoCo tasks in the preference agnostic setting. Furthermore, we empirically show that, in contrast to other LS-based algorithms, our approach is significantly more stable, achieving similar results across various random seeds. "}}
{"id": "4-gBZAkF9ze", "cdate": 1621629764986, "mdate": null, "content": {"title": "Attention as Inference via Fenchel Duality", "abstract": "Attention has been widely adopted in many state-of-the-art deep learning models. While the significant performance improvements it brings have attracted great interest, attention is still poorly understood theoretically. This paper presents a new perspective to understand attention by showing that it can be seen as a solver of a family of estimation problems. In particular, we describe a convex optimization problem that arises in a family of estimation tasks commonly appearing in the design of deep learning models. Rather than directly solving the convex optimization problem, we solve its Fenchel dual and derive a closed-form approximation of the optimal solution. Remarkably, the solution gives a generalized attention structure, and its special case is equivalent to the popular dot-product attention adopted in transformer networks. We show that T5 transformer has implicitly adopted the general form of the solution by demonstrating that this expression unifies the word mask and the positional encoding functions. Finally, we discuss how the proposed attention structures can be integrated in practical models."}}
{"id": "JcB94r_Pax9z", "cdate": 1609459200000, "mdate": 1632857044184, "content": {"title": "A Spam Transformer Model for SMS Spam Detection", "abstract": "In this paper, we aim to explore the possibility of the Transformer model in detecting the spam Short Message Service (SMS) messages by proposing a modified Transformer model that is designed for detecting SMS spam messages. The evaluation of our proposed spam Transformer is performed on SMS Spam Collection v.1 dataset and UtkMl's Twitter Spam Detection Competition dataset, with the benchmark of multiple established machine learning classifiers and state-of-the-art SMS spam detection approaches. In comparison to all other candidates, our experiments on SMS spam detection show that the proposed modified spam Transformer has the optimal results on the accuracy, recall, and F1-Score with the values of 98.92%, 0.9451, and 0.9613, respectively. Besides, the proposed model also achieves good performance on the UtkMl's Twitter dataset, which indicates a promising possibility of adapting the model to other similar problems."}}
{"id": "Chy_5CvUcwi", "cdate": 1609459200000, "mdate": 1632857044719, "content": {"title": "On the Dynamics of Training Attention Models", "abstract": "The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights."}}
{"id": "1OCTOShAmqB", "cdate": 1601308057153, "mdate": null, "content": {"title": "On the Dynamics of Training Attention Models", "abstract": "The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights."}}
{"id": "IYN3LhafGYt", "cdate": 1577836800000, "mdate": 1632857044528, "content": {"title": "On the Dynamics of Training Attention Models", "abstract": "The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights."}}
{"id": "EZwcqeJxfi", "cdate": 1577836800000, "mdate": 1632857044721, "content": {"title": "A Deep Neural Network for Audio Classification with a Classifier Attention Mechanism", "abstract": "Audio classification is considered as a challenging problem in pattern recognition. Recently, many algorithms have been proposed using deep neural networks. In this paper, we introduce a new attention-based neural network architecture called Classifier-Attention-Based Convolutional Neural Network (CAB-CNN). The algorithm uses a newly designed architecture consisting of a list of simple classifiers and an attention mechanism as a classifier selector. This design significantly reduces the number of parameters required by the classifiers and thus their complexities. In this way, it becomes easier to train the classifiers and achieve a high and steady performance. Our claims are corroborated by the experimental results. Compared to the state-of-the-art algorithms, our algorithm achieves more than 10% improvements on all selected test scores."}}
{"id": "CVye4Scuy_", "cdate": 1577836800000, "mdate": 1632857044847, "content": {"title": "Periodic Time Series Data Analysis by Deep Learning Methodology", "abstract": "The detection of periodicity in a time series is considered a challenge in many research areas. The difficulty of period length extraction involves the varying noise levels among working environments. A system that performs well in one environment may not be accurate in another. Different methods, including deep neural networks, have been proposed across many applications to find suitable solutions to the period length extraction problem. This article proposes a convolutional neural network (CNN) based period classification algorithm, named PCA, to detect the dataset periods. In particular, assuming that a data stream contains periodical features, the PCA utilizes historical labeled data as training material and classifies new instances accordingly based on their periods. Its performance has been tested on both synthetic and real-world periodic time series data (PTSD) with very encouraging results. In particular, We have observed that the PCA is capable of achieving 100% accuracy in the case of low noise PTSD. Even the training of the PCA is not economical if the data do not contain much noise, it still demonstrates high performance on both synthetic and real-world datasets. Besides, we have shown that our new algorithm can capture the relationship between the shape of the waves and the target period, which is significantly different from the classical methods that mainly focus on the wave's amplitude."}}
{"id": "WDvhlNxd7xl0", "cdate": 1546300800000, "mdate": 1632857044717, "content": {"title": "A Scheme to Design Community Detection Algorithms in Various Networks", "abstract": "Network structures, consisting of nodes and edges, have applications in almost all subjects. A set of nodes is called a community if the nodes have strong interrelations. Industries (including cell phone carriers and online social media companies) need community structures to allocate network resources and provide proper and accurate services. However, most detection algorithms are derived independently, which is arduous and even unnecessary. Although recent research shows that a general detection method that serves all purposes does not exist, we believe that there is some general procedure of deriving detection algorithms. In this paper, we represent such a general scheme. We mainly focus on two types of networks: transmission networks and similarity networks. We reduce them to a unified graph model, based on which we propose a method to define and detect community structures. Finally, we also give a demonstration to show how our design scheme works."}}
{"id": "HvpsMyo3Xuvb", "cdate": 1546300800000, "mdate": 1632857044715, "content": {"title": "Periodic Time Series Data Classification By Deep Neural Network", "abstract": "It is essential for many research fields to find the period of a data set. Many algorithms have been derived for solving related problems. Recently, scholars have reported that deep neural networks can achieve a performance similar to a human on image classification. In this paper, we report a period classification algorithm based on the convolutional neural networks (CNNs). We test its performance on the randomly-generated periodic time series data sets (PTSDs) that consist of periodic and polynomial components. Our results show that the algorithm can achieve 100% out-of-sample accuracy when the polynomial component of a PTSD does not dominate."}}
