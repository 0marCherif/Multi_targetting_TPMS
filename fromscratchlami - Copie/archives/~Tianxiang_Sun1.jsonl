{"id": "GPbiCYiDVqr", "cdate": 1696601445626, "mdate": 1696601445626, "content": {"title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models", "abstract": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising results in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines."}}
{"id": "htxI8XOqdr", "cdate": 1696601378138, "mdate": 1696601378138, "content": {"title": "Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning", "abstract": "Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tun- ing for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate up- stream and downstream tasks into a unified ma- chine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tun- ing, full model tuning, and prior prompt pre- training methods in few-shot settings. In ad- dition, we demonstrate that MP2 can achieve surprisingly fast and strong adaptation to down- stream tasks by merely learning 8 parameters to combine the pre-trained modular prompts"}}
{"id": "xRtmCHKRDQD", "cdate": 1640995200000, "mdate": 1679913796676, "content": {"title": "BBTv2: Towards a Gradient-Free Future with Large Language Models", "abstract": ""}}
{"id": "u335yujDLE7", "cdate": 1640995200000, "mdate": 1679913796707, "content": {"title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation", "abstract": ""}}
{"id": "tIxAQEOvuD6", "cdate": 1640995200000, "mdate": 1679913796781, "content": {"title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation", "abstract": ""}}
{"id": "sg3GKaLm0I", "cdate": 1640995200000, "mdate": 1679913796731, "content": {"title": "BBTv2: Pure Black-Box Optimization Can Be Comparable to Gradient Descent for Few-Shot Learning", "abstract": ""}}
{"id": "sASz4y_ZtXA", "cdate": 1640995200000, "mdate": 1679913796628, "content": {"title": "Paradigm Shift in Natural Language Processing", "abstract": ""}}
{"id": "mTrEiSU5D2V", "cdate": 1640995200000, "mdate": 1679913796713, "content": {"title": "Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts", "abstract": ""}}
{"id": "iVRs5Cmx0G3", "cdate": 1640995200000, "mdate": 1679913796714, "content": {"title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation", "abstract": ""}}
{"id": "gn-x9tO84BU", "cdate": 1640995200000, "mdate": 1679913796927, "content": {"title": "Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts", "abstract": ""}}
