{"id": "hG-0UwK0-Nl", "cdate": 1672531200000, "mdate": 1680002432546, "content": {"title": "SEGA: Instructing Diffusion using Semantic Dimensions", "abstract": ""}}
{"id": "gfk8I6NZ7j", "cdate": 1672531200000, "mdate": 1694079000202, "content": {"title": "Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models", "abstract": "The field of artificial intelligence (AI) has experienced remarkable progress in recent years, driven by the widespread adoption of open-source machine learning models in both research and industry. Considering the resource-intensive nature of training on vast datasets, many applications opt for models that have already been trained. Hence, a small number of key players undertake the responsibility of training and publicly releasing large pre-trained models, providing a crucial foundation for a wide range of applications. However, the adoption of these open-source models carries inherent privacy and security risks that are often overlooked. To provide a concrete example, an inconspicuous model may conceal hidden functionalities that, when triggered by specific input patterns, can manipulate the behavior of the system, such as instructing self-driving cars to ignore the presence of other vehicles. The implications of successful privacy and security attacks encompass a broad spectrum, ranging from relatively minor damage like service interruptions to highly alarming scenarios, including physical harm or the exposure of sensitive user data. In this work, we present a comprehensive overview of common privacy and security threats associated with the use of open-source models. By raising awareness of these dangers, we strive to promote the responsible and secure use of AI systems."}}
{"id": "gY3L8qBFN7", "cdate": 1672531200000, "mdate": 1694079000202, "content": {"title": "Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability", "abstract": "Sparsely-gated Mixture of Expert (MoE) layers have been recently successfully applied for scaling large transformers, especially for language modeling tasks. An intriguing side effect of sparse MoE layers is that they convey inherent interpretability to a model via natural expert specialization. In this work, we apply sparse MoE layers to CNNs for computer vision tasks and analyze the resulting effect on model interpretability. To stabilize MoE training, we present both soft and hard constraint-based approaches. With hard constraints, the weights of certain experts are allowed to become zero, while soft constraints balance the contribution of experts with an additional auxiliary loss. As a result, soft constraints handle expert utilization better and support the expert specialization process, while hard constraints maintain more generalized experts and increase overall model performance. Our findings demonstrate that experts can implicitly focus on individual sub-domains of the input space. For example, experts trained for CIFAR-100 image classification specialize in recognizing different domains such as flowers or animals without previous data clustering. Experiments with RetinaNet and the COCO dataset further indicate that object detection experts can also specialize in detecting objects of distinct sizes."}}
{"id": "DCrpp28Mfsh", "cdate": 1672531200000, "mdate": 1680002432536, "content": {"title": "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness", "abstract": ""}}
{"id": "1grMmP-dp2", "cdate": 1672531200000, "mdate": 1680002432574, "content": {"title": "Image Classifiers Leak Sensitive Attributes About Their Classes", "abstract": ""}}
{"id": "Liuo-Bk-beq", "cdate": 1663849827334, "mdate": null, "content": {"title": "The Biased Artist: Exploiting Cultural Biases via Homoglyphs in Text-Guided Image Generation Models", "abstract": "Text-guided image generation models, such as DALL-E2 and Stable Diffusion, have recently received much attention from academia and the general public. Provided with textual descriptions, these models are capable of generating high-quality images depicting various concepts and styles. However, such models are trained on large amounts of public data and implicitly learn relationships from their training data that are not immediately apparent. We demonstrate that common multimodal models implicitly learned cultural biases that can be triggered and injected into the generated images by simply replacing single characters in the textual description with visually similar non-Latin characters. These so-called homoglyph replacements enable malicious users or service providers to induce biases into the generated images and even render the whole generation process useless. We practically illustrate such attacks on DALL-E2 and Stable Diffusion as text-guided image generation models and further show that CLIP also behaves similarly. Our results further indicate that text encoders trained on multilingual data provide a way to mitigate the effects of homoglyph replacements."}}
{"id": "slpRjbkF3cf", "cdate": 1640995200000, "mdate": 1664345230836, "content": {"title": "CLIPping Privacy: Identity Inference Attacks on Multi-Modal Machine Learning Models", "abstract": "As deep learning is now used in many real-world applications, research has focused increasingly on the privacy of deep learning models and how to prevent attackers from obtaining sensitive information about the training data. However, image-text models like CLIP have not yet been looked at in the context of privacy attacks. While membership inference attacks aim to tell whether a specific data point was used for training, we introduce a new type of privacy attack, named identity inference attack (IDIA), designed for multi-modal image-text models like CLIP. Using IDIAs, an attacker can reveal whether a particular person, was part of the training data by querying the model in a black-box fashion with different images of the same person. Letting the model choose from a wide variety of possible text labels, the attacker can probe the model whether it recognizes the person and, therefore, was used for training. Through several experiments on CLIP, we show that the attacker can identify individuals used for training with very high accuracy and that the model learns to connect the names with the depicted people. Our experiments show that a multi-modal image-text model indeed leaks sensitive information about its training data and, therefore, should be handled with care."}}
{"id": "pLYT25Ypj9", "cdate": 1640995200000, "mdate": 1680002432522, "content": {"title": "Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models", "abstract": ""}}
{"id": "kGlyj7n9AY", "cdate": 1640995200000, "mdate": 1680002432576, "content": {"title": "The Biased Artist: Exploiting Cultural Biases via Homoglyphs in Text-Guided Image Generation Models", "abstract": ""}}
{"id": "NyR_j7BZJz", "cdate": 1640995200000, "mdate": 1661159925678, "content": {"title": "Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash", "abstract": "Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system\u2019s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.1"}}
