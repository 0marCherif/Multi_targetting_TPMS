{"id": "q6CtXrZQ4k5", "cdate": 1685532017241, "mdate": null, "content": {"title": "First- and Second-Order Bounds for Adversarial Linear Contextual Bandits", "abstract": "We consider the adversarial linear contextual bandit setting, which allows for the loss functions associated with each of $K$ arms to change over time without restriction. Assuming the $d$-dimensional contexts are drawn from a fixed known distribution, the worst-case expected regret over the course of $T$ rounds is known to scale as $\\tilde O(\\sqrt{KdT})$. Under the additional assumption that the density of the contexts is log-concave, we obtain a second-order bound of order $\\tilde O(K\\sqrt{d V_T})$ in terms of the cumulative second moment of the learner's losses $V_T$, and a closely related first-order bound of order $\\tilde O(K\\sqrt{d L_T^*})$ in terms of the cumulative loss of the best policy $L_T^*$. Since $V_T$ or $L_T^*$ may be significantly smaller than $T$, these improve over the worst-case regret whenever the environment is relatively benign. Our results are obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, which we analyse by exploiting a novel connection to the linear bandit setting without contexts."}}
{"id": "H98zPlmVMQ5", "cdate": 1648669422834, "mdate": 1648669422834, "content": {"title": "Independent Policy Gradient for Large-Scale Markov Potential Games: Sharper Rates, Function Approximation, and Game-Agnostic Convergence", "abstract": "We examine global non-asymptotic convergence properties of policy gradient  methods for multi-agent reinforcement learning (RL) problems in Markov potential games (MPGs). To learn a Nash equilibrium of an MPG in which the size of state space and/or the number of players can be very large, we propose new independent policy gradient algorithms that are run by all players in tandem. When there is no uncertainty in the gradient evaluation, we show that our algorithm finds an $\\epsilon$-Nash equilibrium with $O(1/\\epsilon^2)$ iteration complexity which does not explicitly depend on the state space size. When the exact gradient is not available, we establish $O(1/\\epsilon^5)$ sample complexity bound in a potentially infinitely large state space for a sample-based algorithm that utilizes function approximation. Moreover, we identify a class of independent policy gradient algorithms that enjoys convergence for both zero-sum Markov games and Markov cooperative games with the players that are oblivious to the types of games being played. Finally, we provide computational experiments to corroborate the merits and the effectiveness of our theoretical developments."}}
{"id": "XeM4Lld0zTR", "cdate": 1621630117232, "mdate": null, "content": {"title": "Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses", "abstract": "Policy optimization is a widely-used method in reinforcement learning. Due to its local-search nature, however, theoretical guarantees on global optimality often rely on extra assumptions on the Markov Decision Processes (MDPs) that bypass the challenge of global exploration. To eliminate the need of such assumptions, in this work, we develop a general solution that adds dilated bonuses to the policy update to facilitate global exploration. To showcase the power and generality of this technique, we apply it to several episodic MDP settings with adversarial losses and bandit feedback, improving and generalizing the state-of-the-art. Specifically, in the tabular case, we obtain $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret where $T$ is the number of episodes, improving the $\\widetilde{\\mathcal{O}}({T}^{\\frac{2}{3}})$ regret bound by Shani et al. [2020]. When the number of states is infinite, under the assumption that the state-action values are linear in some low-dimensional features, we obtain $\\widetilde{\\mathcal{O}}({T}^{\\frac{2}{3}})$ regret with the help of a simulator, matching the result of Neu and Olkhovskaya [2020] while importantly removing the need of an exploratory policy that their algorithm requires. To our knowledge, this is the first algorithm with sublinear regret for linear function approximation with adversarial losses, bandit feedback, and no exploratory assumptions. Finally, we also discuss how to further improve the regret or remove the need of a simulator using dilated bonuses, when an exploratory policy is available."}}
{"id": "dx11_7vm5_r", "cdate": 1601308316146, "mdate": null, "content": {"title": "Linear Last-iterate Convergence in Constrained Saddle-point Optimization", "abstract": "Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization have received growing attention due to their favorable last-iterate convergence. However, their behaviors for simple bilinear games over the probability simplex are still not fully understood --- previous analysis lacks explicit convergence rates, only applies to an exponentially small learning rate, or requires additional assumptions such as the uniqueness of the optimal solution.\n\nIn this work, we significantly expand the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting. Specifically, for OMWU in bilinear games over the simplex, we show that when the equilibrium is unique, linear last-iterate convergence is achievable with a constant learning rate, which improves the result of (Daskalakis & Panageas, 2019) under the same assumption. We then significantly extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate. We show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption. Our condition also holds for strongly-convex-strongly-concave functions, recovering the result of (Hsieh et al., 2019). Finally, we provide experimental results to further support our theory. "}}
{"id": "HJ47qoZu-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Bandit Multiclass Linear Classification: Efficient Algorithms for the Separable Case", "abstract": "We study the problem of efficient online multiclass linear classification with bandit feedback, where all examples belong to one of $K$ classes and lie in the $d$-dimensional Euclidean space. Previ..."}}
{"id": "H1-KBnWdbr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously", "abstract": "We develop the first general semi-bandit algorithm that simultaneously achieves $\\mathcal{O}(\\log T)$ regret for stochastic environments and $\\mathcal{O}(\\sqrt{T})$ regret for adversarial environme..."}}
{"id": "Hy4vgvbuZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Efficient Online Portfolio with Logarithmic Regret", "abstract": "We study the decades-old problem of online portfolio management and propose the first algorithm with logarithmic regret that is not based on Cover's Universal Portfolio algorithm and admits much faster implementation. Specifically Universal Portfolio enjoys optimal regret $\\mathcal{O}(N\\ln T)$ for $N$ financial instruments over $T$ rounds, but requires log-concave sampling and has a large polynomial running time. Our algorithm, on the other hand, ensures a slightly larger but still logarithmic regret of $\\mathcal{O}(N^2(\\ln T)^4)$, and is based on the well-studied Online Mirror Descent framework with a novel regularizer that can be implemented via standard optimization methods in time $\\mathcal{O}(TN^{2.5})$ per round. The regret of all other existing works is either polynomial in $T$ or has a potentially unbounded factor such as the inverse of the smallest price relative."}}
{"id": "HkbG7tbOWH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Online Reinforcement Learning in Stochastic Games", "abstract": "We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the \\textsc{UCSG} algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the \\textit{diameter}, which is an intrinsic value related to the mixing property of SGs. Slightly extended, \\textsc{UCSG} finds an $\\varepsilon$-maximin stationary policy with a sample complexity of $\\tilde{\\mathcal{O}}\\left(\\text{poly}(1/\\varepsilon)\\right)$, where $\\varepsilon$ is the error parameter. To the best of our knowledge, this extended result is the first in the average-reward setting. In the analysis, we develop Markov chain's perturbation bounds for mean first passage times and techniques to deal with non-stationary opponents, which may be of interest in their own right."}}
{"id": "ryWhCLWObr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Tracking the Best Expert in Non-stationary Stochastic Environments", "abstract": "We study the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. We introduce a new parameter $\\W$, which measures the total statistical variance of the loss distributions over $T$ rounds of the process, and study how this amount affects the regret. We investigate the interaction between $\\W$ and $\\Gamma$, which counts the number of times the distributions change, as well as $\\W$ and $V$, which measures how far the distributions deviates over time. One striking result we find is that even when $\\Gamma$, $V$, and $\\Lambda$ are all restricted to constant, the regret lower bound in the bandit setting still grows with $T$. The other highlight is that in the full-information setting, a constant regret becomes achievable with constant $\\Gamma$ and $\\Lambda$, as it can be made independent of $T$, while with constant $V$ and $\\Lambda$, the regret still has a $T^{1/3}$ dependency. We not only propose algorithms with upper bound guarantee, but prove their matching lower bounds as well."}}
