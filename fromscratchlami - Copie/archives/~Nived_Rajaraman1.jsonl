{"id": "7hQIK2KTHyy", "cdate": 1684352118734, "mdate": 1684352118734, "content": {"title": "Robust Correlation Clustering", "abstract": "In this paper, we introduce and study the Robust-Correlation-Clustering problem: given a graph $G = (V,E)$ where every edge is either labeled + or - (denoting similar or dissimilar pairs of vertices), and a parameter $m$, the goal is to delete a set $D$ of $m$ vertices, and partition the remaining vertices $V \\ D$ into clusters to minimize the cost of the clustering, which is the sum of the number of + edges with end-points in different clusters and the number of - edges with end-points in the same cluster. This generalizes the classical Correlation-Clustering problem which is the special case when $m = 0$. Correlation clustering is useful when we have (only) qualitative information about the similarity or dissimilarity of pairs of points, and Robust-Correlation-Clustering equips this model with the capability to handle noise in datasets.\nIn this work, we present a constant-factor bi-criteria algorithm for Robust-Correlation-Clustering on complete graphs (where our solution is $O(1)$-approximate w.r.t the cost while however discarding $O(1)$ m points as outliers), and also complement this by showing that no finite approximation is possible if we do not violate the outlier budget. Our algorithm is very simple in that it first does a simple LP-based pre-processing to delete $O(m)$ vertices, and subsequently runs a particular Correlation-Clustering algorithm ACNAlg [Ailon et al., 2005] on the residual instance. We then consider general graphs, and show $(O(log n), O(log^2 n))$ bi-criteria algorithms while also showing a hardness of $\\alpha_MC$ on both the cost and the outlier violation, where alpha_MC is the lower bound for the Minimum-Multicut problem."}}
{"id": "dwtv4jsPSh", "cdate": 1672531200000, "mdate": 1683657780511, "content": {"title": "Beyond UCB: Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits", "abstract": "We consider the sequential decision-making problem where the mean outcome is a non-linear function of the chosen action. Compared with the linear model, two curious phenomena arise in non-linear models: first, in addition to the \"learning phase\" with a standard parametric rate for estimation or regret, there is an \"burn-in period\" with a fixed cost determined by the non-linear function; second, achieving the smallest burn-in cost requires new exploration algorithms. For a special family of non-linear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. In particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. In contrast, several classical algorithms, such as UCB and algorithms relying on regression oracles, are provably suboptimal."}}
{"id": "EgnEwwhNU-w", "cdate": 1672531200000, "mdate": 1683907561782, "content": {"title": "Sample Efficient Deep Reinforcement Learning via Local Planning", "abstract": "The focus of this work is sample-efficient deep reinforcement learning (RL) with a simulator. One useful property of simulators is that it is typically easy to reset the environment to a previously observed state. We propose an algorithmic framework, named uncertainty-first local planning (UFLP), that takes advantage of this property. Concretely, in each data collection iteration, with some probability, our meta-algorithm resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution. The agent-environment interaction then proceeds as in the standard online RL setting. We demonstrate that this simple procedure can dramatically improve the sample cost of several baseline RL algorithms on difficult exploration tasks. Notably, with our framework, we can achieve super-human performance on the notoriously hard Atari game, Montezuma's Revenge, with a simple (distributional) double DQN. Our work can be seen as an efficient approximate implementation of an existing algorithm with theoretical guarantees, which offers an interpretation of the positive empirical results."}}
{"id": "2jWd-HRyNjK", "cdate": 1672531200000, "mdate": 1682350339721, "content": {"title": "Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations", "abstract": "Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. In fact, several practical studies have shown that if a pruned model is fine-tuned with some gradient-based updates it generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper, we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem with the ground truth $U_\\star \\in \\mathbb{R}^{d \\times r}$ and the overparameterized model $U \\in \\mathbb{R}^{d \\times k}$ with $k \\gg r$. We study the approximate local minima of the mean square error, augmented with a smooth version of a group Lasso regularizer, $\\sum_{i=1}^k \\| U e_i \\|_2$. In particular, we provably show that pruning all the columns below a certain explicit $\\ell_2$-norm threshold results in a solution $U_{\\text{prune}}$ which has the minimum number of columns $r$, yet close to the ground truth in training loss. Moreover, in the subsequent fine-tuning phase, gradient descent initialized at $U_{\\text{prune}}$ converges at a linear rate to its limit. While our analysis provides insights into the role of regularization in pruning, we also show that running gradient descent in the absence of regularization results in models which {are not suitable for greedy pruning}, i.e., many columns could have their $\\ell_2$ norm comparable to that of the maximum. To the best of our knowledge, our results provide the first rigorous insights on why greedy pruning + fine-tuning leads to smaller models which also generalize well."}}
{"id": "i-UdJ6fWUFc", "cdate": 1652737592241, "mdate": null, "content": {"title": "Semi-supervised Active Linear Regression", "abstract": "Labeled data often comes at a high cost as it may require recruiting human labelers or running costly experiments. At the same time, in many practical scenarios, one already has access to a partially labeled, potentially biased dataset that can help with the learning task at hand. Motivated by such settings, we formally initiate a study of ``semi-supervised active learning'' through the frame of linear regression. Here, the learner has access to a dataset $X \\in \\mathbb{R}^{(n_{\\text{un}}+n_{\\text{lab}}) \\times d}$ composed of $n_{\\text{un}}$ unlabeled examples that a learner can actively query, and $n_{\\text{lab}}$ examples labeled a priori. Denoting the true labels by $Y \\in \\mathbb{R}^{n_{\\text{un}} + n_{\\text{lab}}}$, the learner's objective is to find $\\widehat{\\beta} \\in \\mathbb{R}^d$ such that,\n$$\n\\| X \\widehat{\\beta} - Y \\|_2^2 \\le (1 + \\epsilon) \\min_{\\beta \\in \\mathbb{R}^d} \\| X \\beta - Y \\|_2^2\n$$\nwhile querying the labels of as few unlabeled points as possible. In this paper, we introduce an instance dependent parameter called the reduced rank, denoted $\\text{R}_X$, and propose an efficient algorithm with query complexity $O(\\text{R}_X/\\epsilon)$. This result directly implies improved upper bounds for two important special cases: $(i)$ active ridge regression, and $(ii)$ active kernel ridge regression, where the reduced-rank equates to the ``statistical dimension'', $\\textsf{sd}_\\lambda$ and ``effective dimension'', $d_\\lambda$ of the problem respectively, where $\\lambda \\ge 0$ denotes the regularization parameter. Finally, we introduce a distributional version of the problem as a special case of the agnostic formulation we consider earlier; here, for every $X$, we prove a matching instance-wise lower bound of $\\Omega (\\text{R}_X / \\epsilon)$ on the query complexity of any algorithm."}}
{"id": "1mFfKXYMg5a", "cdate": 1652737585806, "mdate": null, "content": {"title": "Minimax Optimal Online Imitation Learning via Replay Estimation", "abstract": "Online imitation learning is the problem of how best to mimic expert demonstrations, given access to the environment or an accurate simulator. Prior work has shown that in the \\textit{infinite} sample regime, exact moment matching achieves value equivalence to the expert policy. However, in the \\textit{finite} sample regime, even if one has no optimization error, empirical variance can lead to a performance gap that scales with $H^2 / N_{\\text{exp}}$ for behavioral cloning and $H / N_{\\text{exp}}$ for online moment matching, where $H$ is the horizon and $N_{\\text{exp}}$ is the size of the expert dataset. We introduce the technique of ``replay estimation'' to reduce this empirical variance: by repeatedly executing cached expert actions in a stochastic simulator, we compute a smoother expert visitation distribution estimate to match. In the presence of general function approximation, we prove a meta theorem reducing the performance gap of our approach to the \\textit{parameter estimation error} for offline classification (i.e. learning the expert policy). In the tabular setting or with linear function approximation, our meta theorem shows that the performance gap incurred by our approach achieves the optimal $\\widetilde{O} \\left( \\min( H^{3/2} / N_{\\text{exp}}, H / \\sqrt{N_{\\text{exp}}} \\right)$ dependency, under significantly weaker assumptions compared to prior work. We implement multiple instantiations of our approach on several continuous control tasks and find that we are able to significantly improve policy performance across a variety of dataset sizes."}}
{"id": "ikc633VNqoE", "cdate": 1640995200000, "mdate": 1683916677149, "content": {"title": "Minimax Optimal Online Imitation Learning via Replay Estimation", "abstract": "Online imitation learning is the problem of how best to mimic expert demonstrations, given access to the environment or an accurate simulator. Prior work has shown that in the \\textit{infinite} sample regime, exact moment matching achieves value equivalence to the expert policy. However, in the \\textit{finite} sample regime, even if one has no optimization error, empirical variance can lead to a performance gap that scales with $H^2 / N_{\\text{exp}}$ for behavioral cloning and $H / N_{\\text{exp}}$ for online moment matching, where $H$ is the horizon and $N_{\\text{exp}}$ is the size of the expert dataset. We introduce the technique of ``replay estimation'' to reduce this empirical variance: by repeatedly executing cached expert actions in a stochastic simulator, we compute a smoother expert visitation distribution estimate to match. In the presence of general function approximation, we prove a meta theorem reducing the performance gap of our approach to the \\textit{parameter estimation error} for offline classification (i.e. learning the expert policy). In the tabular setting or with linear function approximation, our meta theorem shows that the performance gap incurred by our approach achieves the optimal $\\widetilde{O} \\left( \\min( H^{3/2} / N_{\\text{exp}}, H / \\sqrt{N_{\\text{exp}}} \\right)$ dependency, under significantly weaker assumptions compared to prior work. We implement multiple instantiations of our approach on several continuous control tasks and find that we are able to significantly improve policy performance across a variety of dataset sizes."}}
{"id": "44qiGmxxRU", "cdate": 1640995200000, "mdate": 1683916677089, "content": {"title": "Semi-supervised Active Linear Regression", "abstract": "Labeled data often comes at a high cost as it may require recruiting human labelers or running costly experiments. At the same time, in many practical scenarios, one already has access to a partially labeled, potentially biased dataset that can help with the learning task at hand. Motivated by such settings, we formally initiate a study of ``semi-supervised active learning'' through the frame of linear regression. Here, the learner has access to a dataset $X \\in \\mathbb{R}^{(n_{\\text{un}}+n_{\\text{lab}}) \\times d}$ composed of $n_{\\text{un}}$ unlabeled examples that a learner can actively query, and $n_{\\text{lab}}$ examples labeled a priori. Denoting the true labels by $Y \\in \\mathbb{R}^{n_{\\text{un}} + n_{\\text{lab}}}$, the learner's objective is to find $\\widehat{\\beta} \\in \\mathbb{R}^d$ such that,$$\\| X \\widehat{\\beta} - Y \\|_2^2 \\le (1 + \\epsilon) \\min_{\\beta \\in \\mathbb{R}^d} \\| X \\beta - Y \\|_2^2$$while querying the labels of as few unlabeled points as possible. In this paper, we introduce an instance dependent parameter called the reduced rank, denoted $\\text{R}_X$, and propose an efficient algorithm with query complexity $O(\\text{R}_X/\\epsilon)$. This result directly implies improved upper bounds for two important special cases: $(i)$ active ridge regression, and $(ii)$ active kernel ridge regression, where the reduced-rank equates to the ``statistical dimension'', $\\textsf{sd}_\\lambda$ and ``effective dimension'', $d_\\lambda$ of the problem respectively, where $\\lambda \\ge 0$ denotes the regularization parameter. Finally, we introduce a distributional version of the problem as a special case of the agnostic formulation we consider earlier; here, for every $X$, we prove a matching instance-wise lower bound of $\\Omega (\\text{R}_X / \\epsilon)$ on the query complexity of any algorithm."}}
{"id": "121O6X5bn_j", "cdate": 1640995200000, "mdate": 1681668108165, "content": {"title": "Minimax Optimal Online Imitation Learning via Replay Estimation", "abstract": "Online imitation learning is the problem of how best to mimic expert demonstrations, given access to the environment or an accurate simulator. Prior work has shown that in the infinite sample regime, exact moment matching achieves value equivalence to the expert policy. However, in the finite sample regime, even if one has no optimization error, empirical variance can lead to a performance gap that scales with $H^2 / N$ for behavioral cloning and $H / \\sqrt{N}$ for online moment matching, where $H$ is the horizon and $N$ is the size of the expert dataset. We introduce the technique of replay estimation to reduce this empirical variance: by repeatedly executing cached expert actions in a stochastic simulator, we compute a smoother expert visitation distribution estimate to match. In the presence of general function approximation, we prove a meta theorem reducing the performance gap of our approach to the parameter estimation error for offline classification (i.e. learning the expert policy). In the tabular setting or with linear function approximation, our meta theorem shows that the performance gap incurred by our approach achieves the optimal $\\widetilde{O} \\left( \\min({H^{3/2}} / {N}, {H} / {\\sqrt{N}} \\right)$ dependency, under significantly weaker assumptions compared to prior work. We implement multiple instantiations of our approach on several continuous control tasks and find that we are able to significantly improve policy performance across a variety of dataset sizes."}}
{"id": "Xa9Ba6NsJ6", "cdate": 1621630324133, "mdate": null, "content": {"title": "On the Value of Interaction and Function Approximation in Imitation Learning", "abstract": "We study the statistical guarantees for the Imitation Learning (IL) problem in episodic MDPs.\nRajaraman et al. (2020) show an information theoretic lower bound that in the worst case, a learner which can even actively query the expert policy suffers from a suboptimality growing quadratically in the length of the horizon, $H$. We study imitation learning under the $\\mu$-recoverability assumption of Ross et al. (2011) which assumes that the difference in the $Q$-value under the expert policy across different actions in a state do not deviate beyond $\\mu$ from the maximum. We show that the reduction proposed by Ross et al. (2010) is statistically optimal: the resulting algorithm upon interacting with the MDP for $N$ episodes results in a suboptimality bound of $\\widetilde{\\mathcal{O}} \\left( \\mu |\\mathcal{S}| H / N \\right)$ which we show is optimal up to log-factors. In contrast, we show that any algorithm which does not interact with the MDP and uses an offline dataset of $N$ expert trajectories must incur suboptimality growing as $\\gtrsim |\\mathcal{S}| H^2/N$ even under the $\\mu$-recoverability assumption. This establishes a clear and provable separation of the minimax rates between the active setting and the no-interaction setting. We also study IL with linear function approximation. When the expert plays actions according to a linear classifier of known state-action features, we use the reduction to multi-class classification to show that with high probability, the suboptimality of behavior cloning is  $\\widetilde{O}(dH^2/N)$ given $N$ rollouts from the optimal policy. This is optimal up to log-factors but can be improved to $\\widetilde{O}(dH/N)$ if we have a linear expert with parameter-sharing across time steps. In contrast, when the MDP transition structure is known to the learner such as in the case of simulators, we demonstrate fundamental differences compared to the tabular setting in terms of the performance of an optimal algorithm, Mimic-MD (Rajaraman et al. (2020)) when extended to the function approximation setting. Here, we introduce a new problem called confidence set linear classification, that can be used to construct sample-efficient IL algorithms."}}
