{"id": "K8Zgd3hoea", "cdate": 1665069644487, "mdate": null, "content": {"title": "Responsible Reasoning with Large Language Models and The Impact of Proper Nouns", "abstract": "Language models with billions of parameters have shown remarkable emergent properties, including the ability to reason on unstructured data. We show that open-science multi-lingual large language models can perform the task of spatial reasoning on two or more entities with significant accuracy. A responsible large language model would perform this spatial reasoning task with the same accuracy regardless of the choice of the names of the entities over which the spatial relationships are defined. However, we show that the accuracies of contemporary large language models are impacted by the choice of proper nouns even when the underlying task ought to be independent of the choice of proper nouns. In this context, we also observe that the conditional log probabilities or beam scores of open-science multi-lingual large language model predictions are not well-calibrated, and the scores do not discriminate between correct and wrong responses in this context."}}
{"id": "hZ2H2Ps5dp6", "cdate": 1663850552909, "mdate": null, "content": {"title": "What's in a name? The Influence of Personal Names on Spatial Reasoning in BLOOM Large Language Models", "abstract": "Large language models have been shown to exhibit reasoning capability. But the ability of these models to truly comprehend the reasoning task is not yet clear. An ideal model capable of reasoning would not be affected by the names of the entities over which the relations are defined. In this paper, we consider an algorithmically generated spatial reasoning task over the names of persons. We show that the choice of names has a significant impact on the reasoning accuracy of BLOOM large language models. Using popular names from different countries of the world, we show that BLOOM large language models are susceptible to undesirable variations in reasoning ability even though the underlying logical reasoning challenge does not depend on these names. We further identify that the conditional log probability scores characterizing the uncertainty in prediction produced by BLOOM models are not well-calibrated and cannot be used to detect such reasoning errors. We then suggest a new approach based on model self-explanations and iterative model introspection that performs better than BLOOM conditional log probability scores in detecting such errors and may help alleviate the bias exhibited by these models."}}
{"id": "jczReTpeJ0N", "cdate": 1663850513124, "mdate": null, "content": {"title": "BLOOM Large Language Models and the Chomsky Hierarchy", "abstract": "We study the performance of BLOOM large language models of different sizes on understanding 12 different languages from the Chomsky hierarchy using few-shot prompts. We investigate whether an increase in the complexity of the languages learned by the larger models can be characterized using the Chomsky hierarchy. We first show that prompting in BLOOM models enables reasoning with a good accuracy on language tasks as diverse as stack manipulation, string reversal, odds first, and interlocked pairing, when the queries are over short strings, that is, small bitwidth bit-vectors from the language. Second, we discover that the two largest models have the highest accuracy on such tasks for prompts with a fixed length, but smaller models are able to achieve similar accuracies with longer prompts. Unlike classical automata or grammar based approaches where algorithms for more complex languages in the Chomsky hierarchy can also recognize simpler languages, we find that the performance of the BLOOM large language models cannot be explained by the complexity of the languages in the Chomsky hierarchy."}}
{"id": "maUiU7edeGo", "cdate": 1654719152913, "mdate": 1654719152913, "content": {"title": "Learning Certified Control Using Contraction Metric", "abstract": "In this paper, we solve the problem of finding a certified control policy that drives a robot from any given initial state and under any bounded disturbance to the desired reference trajectory, with guarantees on the convergence or bounds on the tracking error. Such a controller is crucial in safe motion planning. We leverage the advanced theory in Control Contraction Metric and design a learning framework based on neural networks to co-synthesize the contraction metric and the controller for control-affine systems. We further provide methods to validate the convergence and bounded error guarantees. We demonstrate the performance of our method using a suite of challenging robotic models, including models with learned dynamics as neural networks. We compare our approach with leading methods using sum-of-squares programming, reinforcement learning, and model predictive control. Results show that our methods indeed can handle a broader class of systems with less tracking error and faster execution speed. Code is available at https://github.com/sundw2014/C3M.\n"}}
{"id": "TXsjU8BaibT", "cdate": 1632875729209, "mdate": null, "content": {"title": "Trigger Hunting with a Topological Prior for Trojan Detection", "abstract": "Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models \u2013 models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model\u2019s prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks."}}
{"id": "eqNpg2HMNi1", "cdate": 1632875545230, "mdate": null, "content": {"title": "Physical System Design Using Hamiltonian Monte Carlo over Learned Manifolds", "abstract": "The design of complex physical systems entails satisfying several competing performance objectives. In practice, some design requirements are often implicit in the intuition and knowledge of designers who have many years of experience working with similar designs. Designers use this experience to sample a few promising candidates in the design space and evaluate or simulate them using detailed, typically slow multiphysics models. The goal in design is usually to generate a diverse set of high-performing design configurations that allow trade-offs across different objectives and avoid early concretization. In this paper, we develop a machine learning approach to automate physical system design. We use deep generative models to learn a manifold of the valid design space, followed by Hamiltonian Monte Carlo (HMC) with simulated annealing to explore and optimize design over the learned manifold, producing a diverse set of optimal designs. Our approach is akin to partial simulated annealing restricted to the learned design manifold, where the annealing schedule is varied to trade-off different objectives. To prevent our approach from traversing off the design manifold and proposing unreliable designs, we leverage Monte Carlo dropout as a way to detect and avoid design configurations where the learned model cannot be trusted. We demonstrate the efficacy of our proposed approach using several case studies that include the design of an SAE race vehicle, propeller, and air vehicle. Across these case studies, we successfully show how our method generates high-performing and diverse designs. "}}
{"id": "bNlq-GxIrdR", "cdate": 1609459200000, "mdate": null, "content": {"title": "Online Defense of Trojaned Models using Misattributions", "abstract": "Recent studies have shown that neural networks are vulnerable to Trojan attacks, where a network is trained to respond to specially crafted trigger patterns in the inputs in specific and potentially malicious ways. This paper proposes MISA, a new online approach to detect Trojan triggers for neural networks at inference time. Our approach is based on a novel notion called misattributions, which captures the anomalous manifestation of a Trojan activation in the feature space. Given an input image and the corresponding output prediction, our algorithm first computes the model's attribution on different features. It then statistically analyzes these attributions to ascertain the presence of a Trojan trigger. Across a set of benchmarks, we show that our method can effectively detect Trojan triggers for a wide variety of trigger patterns, including several recent ones for which there are no known defenses. Our method achieves 96% AUC for detecting images that include a Trojan trigger without any assumptions on the trigger pattern."}}
{"id": "aseTe13z2m5", "cdate": 1609459200000, "mdate": null, "content": {"title": "Are all outliers alike? On Understanding the Diversity of Outliers for Detecting OODs", "abstract": "Deep neural networks (DNNs) are known to produce incorrect predictions with very high confidence on out-of-distribution (OOD) inputs. This limitation is one of the key challenges in the adoption of deep learning models in high-assurance systems such as autonomous driving, air traffic management, and medical diagnosis. This challenge has received significant attention recently, and several techniques have been developed to detect inputs where the model's prediction cannot be trusted. These techniques use different statistical, geometric, or topological signatures. This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. We demonstrate how different existing detection approaches fail to detect certain types of outliers. We utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. Our results include experiments on CIFAR10, SVHN and MNIST as in-distribution data and Imagenet, LSUN, SVHN (for CIFAR10), CIFAR10 (for SVHN), KMNIST, and F-MNIST as OOD data across different DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5."}}
{"id": "KIfbqntFnOc", "cdate": 1601308324145, "mdate": null, "content": {"title": "Robust Ensembles of Neural Networks using It\u00f4 Processes", "abstract": "Residual neural networks (ResNets) can be modeled as dynamical systems where the evolution of dynamical systems represents the inference in ResNets. We exploit this connection and the theory of stochastic dynamical systems to construct a novel ensemble of It\u00f4 processes as a new deep learning representation that is more robust than classical residual networks. An It\u00f4 process obtained by solving a suitably-formulated stochastic differential equation derived from a residual network has a probability density function that is not readily perturbed by small changes in the neural network\u2019s inputs. Our robust stochastic It\u00f4 ensemble of neural networks achieve an accuracy of 73.91% on the CIFAR-10 dataset against the PGD attack with \u03b5 = 2.0 under the L2 norm, while the accuracy of Madry\u2019s robustness toolbox on the same attack is 18.59%. Similarly, our stochastic It\u00f4 ensemble of neural networks achieves an accuracy of 79.66% on PGD attack with \u03b5 = 16/255 under the L\u221e norm, while the accuracy of Madry\u2019s robustness toolbox on the same attack is 18.13%. The It\u00f4 ensemble trained on ImageNet achieves an accuracy of 28.53% against PGD attacks under the L\u221e norm with \u03b5 = 16/255 and accuracy of 65.74% under the L2 norm with \u03b5 = 3.0, respectively. This significantly improves state-of-the-art accuracy of 5% and 35.16% for Madry\u2019s robustness tool against the same PGD attacks under the L\u221e and L2 norms, respectively. Further, our approach achieves these high robustness values without any explicit adversarial training or a significant loss of accuracy on benign inputs."}}
{"id": "zleOqnAUZzl", "cdate": 1601308259920, "mdate": null, "content": {"title": "Are all outliers alike?  On Understanding the Diversity of Outliers for Detecting OODs", "abstract": "Deep neural networks (DNNs) are known to produce incorrect predictions with very high confidence on out-of-distribution (OOD) inputs. This limitation is one of the key challenges in the adoption of deep learning models in high-assurance systems such as autonomous driving, air traffic management, and medical diagnosis. This challenge has received significant attention recently, and several techniques have been developed to detect inputs where the model's prediction cannot be trusted. These techniques use different statistical, geometric, or topological signatures. This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. We demonstrate how different existing detection approaches fail to detect certain types of outliers. We utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. Our results include experiments on CIFAR10, SVHN and MNIST as in-distribution data and Imagenet, LSUN, SVHN (for CIFAR10), CIFAR10 (for SVHN), KMNIST, and F-MNIST as OOD data across different DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5."}}
