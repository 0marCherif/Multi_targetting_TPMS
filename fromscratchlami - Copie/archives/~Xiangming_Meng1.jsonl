{"id": "OOWLRfAI_V_", "cdate": 1663850257923, "mdate": null, "content": {"title": "Quantized Compressed Sensing with Score-Based Generative Models", "abstract": "We consider the general problem of recovering a high-dimensional signal from noisy quantized measurements. Quantization, especially coarse quantization such as 1-bit sign measurements, leads to severe information loss and thus a good prior knowledge of the unknown signal is helpful for accurate recovery. Motivated by the power of score-based generative models (SGM, also known as diffusion models) in capturing the rich structure of natural signals beyond simple sparsity, we propose an unsupervised data-driven approach called quantized compressed sensing with SGM (QCS-SGM), where the prior distribution is modeled by a pre-trained SGM. To perform posterior sampling, an annealed pseudo-likelihood score called ${\\textit{noise perturbed pseudo-likelihood score}}$ is introduced and combined with the prior score of SGM. The proposed QCS-SGM applies to an arbitrary number of quantization bits. Experiments on a variety of baseline datasets demonstrate that the proposed QCS-SGM significantly outperforms existing state-of-the-art algorithms by a large margin for both in-distribution and out-of-distribution samples. Moreover, as a posterior sampling method, QCS-SGM can be easily used to obtain confidence intervals or uncertainty estimates of the reconstructed results. $\\textit{The code  is available at}$ https://github.com/mengxiangming/QCS-SGM."}}
{"id": "X6bp8ri8dV", "cdate": 1652737456646, "mdate": null, "content": {"title": "Exact Solutions of a Deep Linear Network", "abstract": "This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general."}}
{"id": "zVQGl882Xz", "cdate": 1640995200000, "mdate": 1667532969944, "content": {"title": "Exact Solutions of a Deep Linear Network", "abstract": "This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general."}}
{"id": "xbeU15G2iGW", "cdate": 1640995200000, "mdate": 1667532679968, "content": {"title": "A Unitary Transform Based Generalized Approximate Message Passing", "abstract": "We consider the problem of recovering an unknown signal ${\\mathbf x}\\in {\\mathbb R}^n$ from general nonlinear measurements obtained through a generalized linear model (GLM), i.e., ${\\mathbf y}= f\\left({\\mathbf A}{\\mathbf x}+{\\mathbf w}\\right)$, where $f(\\cdot)$ is a componentwise nonlinear function. Based on the unitary transform approximate message passing (UAMP) and expectation propagation, a unitary transform based generalized approximate message passing (GUAMP) algorithm is proposed for general measurement matrices $\\bf{A}$, in particular highly correlated matrices. Experimental results on quantized compressed sensing demonstrate that the proposed GUAMP significantly outperforms state-of-the-art GAMP and GVAMP under correlated matrices $\\bf{A}$."}}
{"id": "srRTfXvf3_1", "cdate": 1640995200000, "mdate": 1667533045924, "content": {"title": "Exact Solutions of a Deep Linear Network", "abstract": "This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general."}}
{"id": "ghtZzrH10_", "cdate": 1640995200000, "mdate": 1667532970540, "content": {"title": "Stochastic Neural Networks with Infinite Width are Deterministic", "abstract": "This work theoretically studies stochastic neural networks, a main type of neural network in use. We prove that as the width of an optimized stochastic neural network tends to infinity, its predictive variance on the training set decreases to zero. Our theory justifies the common intuition that adding stochasticity to the model can help regularize the model by introducing an averaging effect. Two common examples that our theory can be relevant to are neural networks with dropout and Bayesian latent variable models in a special limit. Our result thus helps better understand how stochasticity affects the learning of neural networks and potentially design better architectures for practical problems."}}
{"id": "b2mOYRSAkao", "cdate": 1640995200000, "mdate": 1667532679866, "content": {"title": "Stochastic Neural Networks with Infinite Width are Deterministic", "abstract": "This work theoretically studies stochastic neural networks, a main type of neural network in use. We prove that as the width of an optimized stochastic neural network tends to infinity, its predictive variance on the training set decreases to zero. Our theory justifies the common intuition that adding stochasticity to the model can help regularize the model by introducing an averaging effect. Two common examples that our theory can be relevant to are neural networks with dropout and Bayesian latent variable models in a special limit. Our result thus helps better understand how stochasticity affects the learning of neural networks and potentially design better architectures for practical problems."}}
{"id": "GPCMdxnvqx", "cdate": 1640995200000, "mdate": 1667532680020, "content": {"title": "Exact Solutions of a Deep Linear Network", "abstract": "This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general."}}
{"id": "GFRtSjvv-4", "cdate": 1640995200000, "mdate": 1667532969946, "content": {"title": "A Unitary Transform Based Generalized Approximate Message Passing", "abstract": "We consider the problem of recovering an unknown signal ${\\mathbf x}\\in {\\mathbb R}^n$ from general nonlinear measurements obtained through a generalized linear model (GLM), i.e., ${\\mathbf y}= f\\left({\\mathbf A}{\\mathbf x}+{\\mathbf w}\\right)$, where $f(\\cdot)$ is a componentwise nonlinear function. Based on the unitary transform approximate message passing (UAMP) and expectation propagation, a unitary transform based generalized approximate message passing (GUAMP) algorithm is proposed for general measurement matrices $\\bf{A}$, in particular highly correlated matrices. Experimental results on quantized compressed sensing demonstrate that the proposed GUAMP significantly outperforms state-of-the-art GAMP and GVAMP under correlated matrices $\\bf{A}$."}}
{"id": "3vutuiJhT0i", "cdate": 1640995200000, "mdate": 1667533045930, "content": {"title": "Stochastic Neural Networks with Infinite Width are Deterministic", "abstract": "This work theoretically studies stochastic neural networks, a main type of neural network in use. We prove that as the width of an optimized stochastic neural network tends to infinity, its predictive variance on the training set decreases to zero. Our theory justifies the common intuition that adding stochasticity to the model can help regularize the model by introducing an averaging effect. Two common examples that our theory can be relevant to are neural networks with dropout and Bayesian latent variable models in a special limit. Our result thus helps better understand how stochasticity affects the learning of neural networks and potentially design better architectures for practical problems."}}
