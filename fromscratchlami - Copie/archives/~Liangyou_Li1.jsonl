{"id": "4xlCI_M2nxO", "cdate": 1657339304000, "mdate": 1657339304000, "content": {"title": "Learning Multilingual Representation for Natural Language Understanding with Enhanced Cross-Lingual Supervision", "abstract": "Recently, pre-training multilingual language models has shown great potential in learning multilingual representation, a crucial topic of natural language processing. Prior works generally use a single mixed attention (MA) module, following TLM (Conneau and Lample, 2019), for attending to intra-lingual and cross-lingual contexts equivalently and simultaneously. In this paper, we propose a network named decomposed attention (DA) as a replacement of MA. The DA consists of an intra-lingual attention (IA) and a cross-lingual attention (CA), which model intralingual and cross-lingual supervisions respectively. In addition, we introduce a language-adaptive re-weighting strategy during training to further boost the model's performance. Experiments on various cross-lingual natural language understanding (NLU) tasks show that the proposed architecture and learning strategy significantly improve the model's cross-lingual transferability."}}
{"id": "v9A26Im9eV", "cdate": 1657339214280, "mdate": 1657339214280, "content": {"title": "FreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models", "abstract": "Cross-lingual transfer (CLT) is of various applications. However, labeled cross-lingual corpus is expensive or even inaccessible, especially in the fields where labels are private, such as diagnostic results of symptoms in medicine and user profiles in business. Nevertheless, there are off-the-shelf models in these sensitive fields. Instead of pursuing the original labels, a workaround for CLT is to transfer knowledge from the off-the-shelf models without labels. To this end, we define a novel CLT problem named FreeTransfer-X that aims to achieve knowledge transfer from the off-the-shelf models in rich-resource languages. To address the problem, we propose a 2-step knowledge distillation (KD, Hinton et al., 2015) framework based on multilingual pre-trained language models (mPLM). The significant improvement over strong neural machine translation (NMT) baselines demonstrates the effectiveness of the proposed method. In addition to reducing annotation cost and protecting private labels, the proposed method is compatible with different networks and easy to be deployed. Finally, a range of analyses indicate the great potential of the proposed method."}}
{"id": "HlWgwpAmjyc", "cdate": 1645063871421, "mdate": 1645063871421, "content": {"title": "Document Graph for Neural Machine Translation", "abstract": "Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English\u2013French, Chinese-English, WMT English\u2013German and Opensubtitle English\u2013Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena."}}
{"id": "7i41YF5jkK", "cdate": 1596113676232, "mdate": null, "content": {"title": "How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?", "abstract": "Despite the success of neural machine translation (NMT), simultaneous neural machine translation (SNMT), the task of translating in\nreal time before a full sentence has been observed, remains challenging due to the syntactic structure difference and simultaneity equirements. In this paper, we propose a general framework to improve simultaneous translation with a pretrained consecutive neural machine translation (CNMT) model. Our framework contains two parts: prefix translation that utilizes a pretrained CNMT model to better\ntranslate source prefixes and a stopping criterion that determines when to stop the prefix translation. Experiments on three translation\ncorpora and two language pairs show the efficacy of the proposed framework on balancing the quality and latency in simultaneous translation."}}
{"id": "beREUA3p-1d", "cdate": 1596113531915, "mdate": null, "content": {"title": "Pretrained Language Models for Document-Level Neural Machine Translation", "abstract": "Previous work on document-level NMT usually focuses on limited contexts because of degraded performance on larger contexts. In\nthis paper, we investigate on using large contexts with three main contributions: (1) Different from previous work which pertrained models on large-scale sentence-level parallel corpora, we use pretrained language models, specifically BERT (Devlin et al., 2018), which are trained on monolingual documents; (2) We propose context manipulation methods to control the influence of large contexts, which lead\nto comparable results on systems using small and large contexts; (3) We introduce a multitask training for regularization to avoid models\noverfitting our training corpora, which further improves our systems together with a deeper encoder. Experiments are conducted on the\nwidely used IWSLT data sets with three language pairs, i.e., Chinese\u2013English, French\u2013English and Spanish\u2013English. Results show that our systems are significantly better than three previously reported document-level systems.\n"}}
{"id": "qsZIVgqM7d", "cdate": 1577836800000, "mdate": null, "content": {"title": "Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption", "abstract": "We argue that the vulnerability of model parameters is of crucial value to the study of model robustness and generalization but little research has been devoted to understanding this matter. In this work, we propose an indicator to measure the robustness of neural network parameters by exploiting their vulnerability via parameter corruption. The proposed indicator describes the maximum loss variation in the non-trivial worst-case scenario under parameter corruption. For practical purposes, we give a gradient-based estimation, which is far more effective than random corruption trials that can hardly induce the worst accuracy degradation. Equipped with theoretical support and empirical validation, we are able to systematically investigate the robustness of different model parameters and reveal vulnerability of deep neural networks that has been rarely paid attention to before. Moreover, we can enhance the models accordingly with the proposed adversarial corruption-resistant training, which not only improves the parameter robustness but also translates into accuracy elevation."}}
{"id": "e6eYWVj-dty", "cdate": 1577836800000, "mdate": null, "content": {"title": "HW-TSC's Participation in the WAT 2020 Indic Languages Multilingual Task", "abstract": "Zhengzhe Yu, Zhanglin Wu, Xiaoyu Chen, Daimeng Wei, Hengchao Shang, Jiaxin Guo, Zongyao Li, Minghan Wang, Liangyou Li, Lizhi Lei, Hao Yang, Ying Qin. Proceedings of the 7th Workshop on Asian Translation. 2020."}}
{"id": "GV0KYgbemV", "cdate": 1577836800000, "mdate": null, "content": {"title": "A General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation", "abstract": "Yun Chen, Liangyou Li, Xin Jiang, Xiao Chen, Qun Liu. Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing. 2020."}}
{"id": "7_clDYJy6Uc", "cdate": 1577836800000, "mdate": null, "content": {"title": "Document Graph for Neural Machine Translation", "abstract": "Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods only consider a few number of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English--French, Chinese-English, WMT English--German and Opensubtitle English--Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena."}}
{"id": "-onXwYMD4bp", "cdate": 1546300800000, "mdate": null, "content": {"title": "Huawei's NMT Systems for the WMT 2019 Biomedical Translation Task", "abstract": "This paper describes Huawei\u2019s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering English\u2013Chinese, English\u2013French and English\u2013German language pairs. Our submitted systems achieve the best BLEU scores on English\u2013French and English\u2013German language pairs according to the official evaluation results. In the English\u2013Chinese translation task, our systems are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated models developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task."}}
