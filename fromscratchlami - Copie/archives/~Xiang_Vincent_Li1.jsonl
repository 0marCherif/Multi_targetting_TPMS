{"id": "wFymjzZEEkH", "cdate": 1652737740982, "mdate": null, "content": {"title": "Personalized Federated Learning towards Communication Efficiency, Robustness and Fairness", "abstract": "Personalized Federated Learning faces many challenges such as expensive communication costs, training-time adversarial attacks, and performance unfairness across devices. Recent developments witness a trade-off between a reference model and local models to achieve personalization. We follow the avenue and propose a personalized FL method towards the three goals. When it is time to communicate, our method projects local models into a shared-and-fixed low-dimensional random subspace and uses infimal convolution to control the deviation between the reference model and projected local models. We theoretically show our method converges for smooth objectives with square regularizers and the convergence dependence on the projection dimension is mild. We also illustrate the benefits of robustness and fairness on a class of linear problems. Finally, we conduct a large number of experiments to show the empirical superiority of our method over several state-of-the-art methods on the three aspects."}}
{"id": "wo-a8Ji6s3A", "cdate": 1652737622436, "mdate": null, "content": {"title": "Asymptotic Behaviors of Projected Stochastic Approximation: A Jump Diffusion Perspective", "abstract": "In this paper, we consider linearly constrained stochastic approximation problems with federated learning (FL) as a special case. We propose a stochastic approximation algorithm named by LPSA with probabilistic projections to ensure feasibility so that projections are performed with probability $p_n$ at the $n$-th iteration. Considering a specific family of the probability $p_n$ and step size $\\eta_n$, we analyze our algorithm from an asymptotic and continuous perspective. Using a novel jump diffusion approximation, we show that the trajectories consisting of properly rescaled last iterates weakly converge to the solution of specific SDEs. By analyzing the SDEs, we identify the asymptotic behaviors of LPSA for different choices of $(p_n, \\eta_n)$. We find the algorithm presents an intriguing asymptotic bias-variance trade-off according to the relative magnitude of $p_n$ w.r.t. $\\eta_n$. It provides insights on how to choose appropriate $\\{(p_n, \\eta_n)\\}_{n \\geq 1}$ to minimize the projection complexity."}}
{"id": "ohww-_nkAVN", "cdate": 1640995200000, "mdate": 1684336387847, "content": {"title": "Asymptotic Behaviors of Projected Stochastic Approximation: A Jump Diffusion Perspective", "abstract": "In this paper, we consider linearly constrained stochastic approximation problems with federated learning (FL) as a special case. We propose a stochastic approximation algorithm named by LPSA with probabilistic projections to ensure feasibility so that projections are performed with probability $p_n$ at the $n$-th iteration. Considering a specific family of the probability $p_n$ and step size $\\eta_n$, we analyze our algorithm from an asymptotic and continuous perspective. Using a novel jump diffusion approximation, we show that the trajectories consisting of properly rescaled last iterates weakly converge to the solution of specific SDEs. By analyzing the SDEs, we identify the asymptotic behaviors of LPSA for different choices of $(p_n, \\eta_n)$. We find the algorithm presents an intriguing asymptotic bias-variance trade-off according to the relative magnitude of $p_n$ w.r.t. $\\eta_n$. It provides insights on how to choose appropriate $\\{(p_n, \\eta_n)\\}_{n \\geq 1}$ to minimize the projection complexity."}}
{"id": "H4eBgxljz-3", "cdate": 1640995200000, "mdate": 1684336387713, "content": {"title": "Statistical Estimation and Online Inference via Local SGD", "abstract": "We analyze the novel Local SGD in federated Learning, a multi-round estimation procedure that uses intermittent communication to improve communication efficiency. Under a $2{+}\\delta$ moment condition on stochastic gradients, we first establish a {\\it functional central limit theorem} that shows the averaged iterates of Local SGD converge weakly to a rescaled Brownian motion. We next provide two iterative inference methods: the {\\it plug-in} and the {\\it random scaling}. Random scaling constructs an asymptotically pivotal statistic for inference by using the information along the whole Local SGD path. Both the methods are communication efficient and applicable to online data. Our results show that Local SGD simultaneously achieves both statistical efficiency and communication efficiency."}}
{"id": "b_crsKt2Ch", "cdate": 1609459200000, "mdate": 1684336387645, "content": {"title": "Communication-Efficient Distributed SVD via Local Power Iterations", "abstract": "We study distributed computing of the truncated singular value decomposition (SVD). We develop an algorithm that we call \\texttt{LocalPower} for improving communication efficiency. Specifically, we..."}}
{"id": "aIO3zDpvbiK", "cdate": 1577836800000, "mdate": 1684336387856, "content": {"title": "Do Subsampled Newton Methods Work for High-Dimensional Data?", "abstract": "Subsampled Newton methods approximate Hessian matrices through subsampling techniques to alleviate the per-iteration cost. Previous results require \u03a9 (d) samples to approximate Hessians, where d is the dimension of data points, making it less practical for high-dimensional data. The situation is deteriorated when d is comparably as large as the number of data points n, which requires to take the whole dataset into account, making subsampling not useful. This paper theoretically justifies the effectiveness of subsampled Newton methods on strongly convex empirical risk minimization with high dimensional data. Specifically, we provably require only \u0398\u02dc(deff\u03b3) samples for approximating the Hessian matrices, where deff\u03b3 is the \u03b3-ridge leverage and can be much smaller than d as long as n\u03b3 \u226b 1. Our theories work for three types of Newton methods: subsampled Netwon, distributed Newton, and proximal Newton."}}
{"id": "K9e3Nh23dC", "cdate": 1577836800000, "mdate": 1684336387651, "content": {"title": "On the Convergence of FedAvg on Non-IID Data", "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs..."}}
{"id": "HJxNAnVtDS", "cdate": 1569438924237, "mdate": null, "content": {"title": "On the Convergence of FedAvg on Non-IID Data", "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning.  Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal."}}
{"id": "v9-AfEQTnUx", "cdate": 1546300800000, "mdate": 1664470464895, "content": {"title": "Communication Efficient Decentralized Training with Multiple Local Updates", "abstract": "Recently, the technique of local updates is a powerful tool in centralized settings to improve communication efficiency via periodical communication. For decentralized settings, it is still unclear how to efficiently combine local updates and decentralized communication. In this work, we propose an algorithm named as LD-SGD, which incorporates arbitrary update schemes that alternate between multiple Local updates and multiple Decentralized SGDs, and provide an analytical framework for LD-SGD. Under the framework, we present a sufficient condition to guarantee the convergence. We show that LD-SGD converges to a critical point for a wide range of update schemes when the objective is non-convex and the training data are non-identically independent distributed. Moreover, our framework brings many insights into the design of update schemes for decentralized optimization. As examples, we specify two update schemes and show how they help improve communication efficiency. Specifically, the first scheme alternates the number of local and global update steps. From our analysis, the ratio of the number of local updates to that of decentralized SGD trades off communication and computation. The second scheme is to periodically shrink the length of local updates. We show that the decaying strategy helps improve communication efficiency both theoretically and empirically."}}
{"id": "DCh2J79DmV", "cdate": 1546300800000, "mdate": 1684336387625, "content": {"title": "A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning", "abstract": "We propose and study a general framework for regularized Markov decision processes (MDPs) where the goal is to find an optimal policy that maximizes the expected discounted total reward plus a policy regularization term. The extant entropy-regularized MDPs can be cast into our framework. Moreover, under our framework, many regularization terms can bring multi-modality and sparsity, which are potentially useful in reinforcement learning. In particular, we present sufficient and necessary conditions that induce a sparse optimal policy. We also conduct a full mathematical analysis of the proposed regularized MDPs, including the optimality condition, performance error, and sparseness control. We provide a generic method to devise regularization forms and propose off-policy actor critic algorithms in complex environment settings. We empirically analyze the numerical properties of optimal policies and compare the performance of different sparse regularization forms in discrete and continuous environments."}}
