{"id": "Khf6w_PMrS-", "cdate": 1672531200000, "mdate": 1681339566303, "content": {"title": "The Provable Benefits of Unsupervised Data Sharing for Offline Reinforcement Learning", "abstract": ""}}
{"id": "MTTPLcwvqTt", "cdate": 1663849983236, "mdate": null, "content": {"title": "The Provable Benefit of Unsupervised Data Sharing for Offline Reinforcement Learning", "abstract": "Self-supervised methods have become crucial for advancing deep learning by leveraging data itself to reduce the need for expensive annotations. However, the question of how to conduct self-supervised offline reinforcement learning (RL) in a principled way remains unclear.\nIn this paper, we address this issue by investigating the theoretical benefits of utilizing reward-free data in linear Markov Decision Processes (MDPs) within a semi-supervised setting. Further, we propose a novel, Provable Data Sharing algorithm (PDS) to utilize such reward-free data for offline RL. PDS uses additional penalties on the reward function learned from labeled data to prevent overestimation, ensuring a conservative algorithm. Our results on various offline RL tasks demonstrate that PDS significantly improves the performance of offline RL algorithms with reward-free data. Overall, our work provides a promising approach to leveraging the benefits of unlabeled data in offline RL while maintaining theoretical guarantees. We believe our findings will contribute to developing more robust self-supervised RL methods.\n"}}
{"id": "mSDS6cT2NM", "cdate": 1640995200000, "mdate": 1681339566355, "content": {"title": "On the Role of Discount Factor in Offline Reinforcement Learning", "abstract": ""}}
{"id": "bTjBkoQImuk", "cdate": 1640995200000, "mdate": 1681339566312, "content": {"title": "Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery", "abstract": ""}}
{"id": "GVOVwpV-Gv", "cdate": 1640995200000, "mdate": 1681339566316, "content": {"title": "Offline Reinforcement Learning with Value-based Episodic Memory", "abstract": ""}}
{"id": "0gUQc1tuPN", "cdate": 1640995200000, "mdate": 1681339566309, "content": {"title": "On the Role of Discount Factor in Offline Reinforcement Learning", "abstract": ""}}
{"id": "RCZqv9NXlZ", "cdate": 1632875614928, "mdate": null, "content": {"title": "Offline Reinforcement Learning with Value-based Episodic Memory", "abstract": "Offline reinforcement learning (RL) shows promise of applying RL to real-world problems by effectively utilizing previously collected data. Most existing offline RL algorithms use regularization or constraints to suppress extrapolation error for actions outside the dataset. In this paper, we adopt a different framework, which learns the V-function instead of the Q-function to naturally keep the learning procedure within the support of an offline dataset. To enable effective generalization while maintaining proper conservatism in offline learning, we propose Expectile V-Learning (EVL), which smoothly interpolates between the optimal value learning and behavior cloning. Further, we introduce implicit planning along offline trajectories to enhance learned V-values and accelerate convergence. Together, we present a new offline method called Value-based Episodic Memory (VEM). We provide theoretical analysis for the convergence properties of our proposed VEM method, and empirical results in the D4RL benchmark show that our method achieves superior performance in most tasks, particularly in sparse-reward tasks."}}
{"id": "yNzF41lHYV", "cdate": 1621629872647, "mdate": null, "content": {"title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning", "abstract": "Learning from datasets without interaction with environments (Offline Learning) is an essential step to apply Reinforcement Learning (RL) algorithms in real-world scenarios.\tHowever, compared with the single-agent counterpart, offline multi-agent RL introduces more agents with the larger state and action space, which is more challenging but attracts little attention. We demonstrate current offline RL algorithms are ineffective in multi-agent systems due to the accumulated extrapolation error. In this paper, we propose a novel offline RL algorithm, named Implicit Constraint Q-learning (ICQ), which effectively alleviates the extrapolation error by only trusting the state-action pairs given in the dataset for value estimation.  Moreover, we extend ICQ to multi-agent tasks by decomposing the joint-policy under the implicit constraint.  Experimental results demonstrate that the extrapolation error is successfully controlled within a reasonable range and insensitive to the number of agents. We further show that ICQ achieves the state-of-the-art performance in the challenging multi-agent offline tasks (StarCraft II). Our code is public online at https://github.com/YiqinYang/ICQ."}}
{"id": "6tM849_6RF9", "cdate": 1621629872647, "mdate": null, "content": {"title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning", "abstract": "Learning from datasets without interaction with environments (Offline Learning) is an essential step to apply Reinforcement Learning (RL) algorithms in real-world scenarios.\tHowever, compared with the single-agent counterpart, offline multi-agent RL introduces more agents with the larger state and action space, which is more challenging but attracts little attention. We demonstrate current offline RL algorithms are ineffective in multi-agent systems due to the accumulated extrapolation error. In this paper, we propose a novel offline RL algorithm, named Implicit Constraint Q-learning (ICQ), which effectively alleviates the extrapolation error by only trusting the state-action pairs given in the dataset for value estimation.  Moreover, we extend ICQ to multi-agent tasks by decomposing the joint-policy under the implicit constraint.  Experimental results demonstrate that the extrapolation error is successfully controlled within a reasonable range and insensitive to the number of agents. We further show that ICQ achieves the state-of-the-art performance in the challenging multi-agent offline tasks (StarCraft II). Our code is public online at https://github.com/YiqinYang/ICQ."}}
{"id": "h8Cqvm-z7b", "cdate": 1609459200000, "mdate": 1682089935647, "content": {"title": "Learning to Discover Task-Relevant Features for Interpretable Reinforcement Learning", "abstract": "Reinforcement Learning (RL) agents are often fed with large-dimensional observations to achieve the ideal performance in complex environments. Unfortunately, the massive observation space usually contains useless or even adverse features, which leads to low sample efficiency. Existing methods rely on domain knowledge and cross-validation to discover efficient features which are informative for decision-making. To minimize the impact of prior knowledge, we propose a temporal-adaptive feature attention algorithm (TAFA). We adopt a non-linear attention module, automatically choosing task-relevant components of hand-crafted state features without any domain knowledge. Our experiments on MuJoCo and TORCS tasks show that the agent achieves competitive performance with state-of-the-art methods while successfully identifying the most task-relevant features for free. We believe our work takes a step towards the interpretability of RL. Our code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/QiyuanZhang19/Temporal-Adaptive-Feature-Attention/tree/master</uri> ."}}
