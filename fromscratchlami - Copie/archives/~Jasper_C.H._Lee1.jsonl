{"id": "Zac0M-ckcG", "cdate": 1695400702455, "mdate": null, "content": {"title": "Finite-Sample Symmetric Mean Estimation with Fisher Information Rate", "abstract": "The mean of an unknown variance-\u03c32 distribution f can be estimated from n samples with variance \u03c32n and nearly corresponding subgaussian rate. When f is known up to translation, this can be improved asymptotically to 1nI, where I is the Fisher information of the distribution. Such an improvement is not possible for general unknown f, but [Stone, 1975] showed that this asymptotic convergence is possible if f is symmetric about its mean. Stone's bound is asymptotic, however: the n required for convergence depends in an unspecified way on the distribution f and failure probability \u03b4. In this paper we give finite-sample guarantees for symmetric mean estimation in terms of Fisher information. For every f,n,\u03b4 with n>log1\u03b4, we get convergence close to a subgaussian with variance 1nIr, where Ir is the r-smoothed Fisher information with smoothing radius r that decays polynomially in n. Such a bound essentially matches the finite-sample guarantees in the known-f setting."}}
{"id": "il4a-BqYzty", "cdate": 1672531200000, "mdate": 1681671453189, "content": {"title": "High-dimensional Location Estimation via Norm Concentration for Subgamma Vectors", "abstract": "In location estimation, we are given $n$ samples from a known distribution $f$ shifted by an unknown translation $\\lambda$, and want to estimate $\\lambda$ as precisely as possible. Asymptotically, the maximum likelihood estimate achieves the Cram\\'er-Rao bound of error $\\mathcal N(0, \\frac{1}{n\\mathcal I})$, where $\\mathcal I$ is the Fisher information of $f$. However, the $n$ required for convergence depends on $f$, and may be arbitrarily large. We build on the theory using \\emph{smoothed} estimators to bound the error for finite $n$ in terms of $\\mathcal I_r$, the Fisher information of the $r$-smoothed distribution. As $n \\to \\infty$, $r \\to 0$ at an explicit rate and this converges to the Cram\\'er-Rao bound. We (1) improve the prior work for 1-dimensional $f$ to converge for constant failure probability in addition to high probability, and (2) extend the theory to high-dimensional distributions. In the process, we prove a new bound on the norm of a high-dimensional random variable whose 1-dimensional projections are subgamma, which may be of independent interest."}}
{"id": "SoOmo_G8d_g", "cdate": 1672531200000, "mdate": 1681675539401, "content": {"title": "Branch & Learn with Post-hoc Correction for Predict+Optimize with Unknown Parameters in Constraints", "abstract": "Combining machine learning and constrained optimization, Predict+Optimize tackles optimization problems containing parameters that are unknown at the time of solving. Prior works focus on cases with unknowns only in the objectives. A new framework was recently proposed to cater for unknowns also in constraints by introducing a loss function, called Post-hoc Regret, that takes into account the cost of correcting an unsatisfiable prediction. Since Post-hoc Regret is non-differentiable, the previous work computes only its approximation. While the notion of Post-hoc Regret is general, its specific implementation is applicable to only packing and covering linear programming problems. In this paper, we first show how to compute Post-hoc Regret exactly for any optimization problem solvable by a recursive algorithm satisfying simple conditions. Experimentation demonstrates substantial improvement in the quality of solutions as compared to the earlier approximation approach. Furthermore, we show experimentally the empirical behavior of different combinations of correction and penalty functions used in the Post-hoc Regret of the same benchmarks. Results provide insights for defining the appropriate Post-hoc Regret in different application scenarios."}}
{"id": "1l5hEEK_j13", "cdate": 1652737849861, "mdate": null, "content": {"title": "Finite-Sample Maximum Likelihood Estimation of Location", "abstract": "We consider 1-dimensional location estimation, where we estimate a parameter $\\lambda$ from $n$ samples $\\lambda + \\eta_i$, with each $\\eta_i$ drawn i.i.d. from a known distribution $f$. For fixed $f$ the maximum-likelihood estimate (MLE) is well-known to be optimal in the limit as $n \\to \\infty$: it is asymptotically normal with variance matching the Cramer-Rao lower bound of $\\frac{1}{n\\mathcal{I}}$, where $\\mathcal{I}$ is the Fisher information of $f$. However, this bound does not hold for finite $n$, or when $f$ varies with $n$. We show for arbitrary $f$ and $n$ that one can recover a similar theory based on the Fisher information of a smoothed version of $f$, where the smoothing radius decays with $n$."}}
{"id": "jWgGtPmi8c", "cdate": 1652737624329, "mdate": null, "content": {"title": "Outlier-Robust Sparse Mean Estimation for Heavy-Tailed Distributions", "abstract": "We study the fundamental task of outlier-robust mean estimation  for heavy-tailed distributions in the presence of sparsity. Specifically, given a small number of corrupted samples from a high-dimensional heavy-tailed distribution whose mean $\\mu$ is guaranteed to be sparse, the goal is to efficiently compute a hypothesis that accurately approximates $\\mu$ with high probability. Prior work had obtained efficient algorithms for robust sparse mean estimation of light-tailed distributions. In this work, we give the first sample-efficient and polynomial-time robust sparse mean estimator for heavy-tailed distributions under mild moment assumptions. Our algorithm achieves the optimal asymptotic error using a number of samples scaling logarithmically with the ambient dimension. Importantly, the sample complexity of our method is optimal as a function of the failure probability $\\tau$, having an {\\em additive} $\\log(1/\\tau)$ dependence. Our algorithm leverages the stability-based approach from the algorithmic robust statistics literature, with crucial (and necessary) adaptations required in our setting. Our analysis may be of independent interest, involving the delicate design of a (non-spectral) decomposition for positive semi-definite matrices satisfying certain sparsity properties."}}
{"id": "iWg5LjFbeT_", "cdate": 1652737616553, "mdate": null, "content": {"title": "Branch & Learn for Recursively and Iteratively Solvable Problems in Predict+Optimize", "abstract": "This paper proposes Branch & Learn, a framework for Predict+Optimize to tackle optimization problems containing parameters that are unknown at the time of solving. Given an optimization problem solvable by a recursive algorithm satisfying simple conditions, we show how a corresponding learning algorithm can be constructed directly and methodically from the recursive algorithm. Our framework applies also to iterative algorithms by viewing them as a degenerate form of recursion. Extensive experimentation shows better performance for our proposal over classical and state of the art approaches."}}
{"id": "X8skzyX8hQg", "cdate": 1640995200000, "mdate": 1652485495203, "content": {"title": "Optimal Sub-Gaussian Mean Estimation in Very High Dimensions", "abstract": "We address the problem of mean estimation in very high dimensions, in the high probability regime parameterized by failure probability \u03b4. For a distribution with covariance \u03a3, let its \"effective dimension\" be d_eff = {Tr(\u03a3)}/{\u03bb_{max}(\u03a3)}. For the regime where d_eff = \u03c9(log^2 (1/\u03b4)), we show the first algorithm whose sample complexity is optimal to within 1+o(1) factor. The algorithm has a surprisingly simple structure: 1) re-center the samples using a known sub-Gaussian estimator, 2) carefully choose an easy-to-compute positive integer t and then remove the t samples farthest from the origin and 3) return the sample mean of the remaining samples. The core of the analysis relies on a novel vector Bernstein-type tail bound, showing that under general conditions, the sample mean of a bounded high-dimensional distribution is highly concentrated around a spherical shell."}}
{"id": "76R_lG6Y4TK", "cdate": 1640995200000, "mdate": 1652485495206, "content": {"title": "Branch & Learn for Recursively and Iteratively Solvable Problems in Predict+Optimize", "abstract": "This paper proposes Branch & Learn, a framework for Predict+Optimize to tackle optimization problems containing parameters that are unknown at the time of solving. Given an optimization problem solvable by a recursive algorithm satisfying simple conditions, we show how a corresponding learning algorithm can be constructed directly and methodically from the recursive algorithm. Our framework applies also to iterative algorithms by viewing them as a degenerate form of recursion. Extensive experimentation shows better performance for our proposal over classical and state-of-the-art approaches."}}
{"id": "eD0AYgjPLCi", "cdate": 1609459200000, "mdate": 1652485495204, "content": {"title": "Optimal Sub-Gaussian Mean Estimation in $\\mathbb{R}$", "abstract": "We settle the fundamental problem of estimating the mean of a real-valued distribution in the high probability regime, under the minimal (and essentially necessary) assumption that the distribution has finite but unknown variance: we propose an estimator with convergence tight up to a <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$1 +o(1)$</tex> factor. Crucially, in contrast to prior works, our estimator does not require prior knowledge of the variance, and works across the entire gamut of distributions with finite variance, including those without any higher moments. Parameterized by the sample size <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$n$</tex> , the failure probability <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\delta$</tex> , and the variance <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\sigma^{2}$</tex> , our estimator has additive accuracy within <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\sigma\\cdot(1+o(1))\\sqrt{\\frac{2\\log\\frac{1}{\\delta}}{n}}$</tex> , which is optimal up to the <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$1+o(1)$</tex> term. This asymptotically matches the convergence of the sample mean for the Gaussian distribution with the same variance. Our estimator construction and analysis gives a framework generalizable to other problems, tightly analyzing a sum of dependent random variables by viewing the sum implicitly as a 2-parameter <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\psi$</tex> -estimator, and constructing bounds using mathematical programming and duality techniques."}}
{"id": "71zdgPSk0j2", "cdate": 1609459200000, "mdate": 1652485495201, "content": {"title": "Uncertainty about Uncertainty: Optimal Adaptive Algorithms for Estimating Mixtures of Unknown Coins", "abstract": "Given a mixture between two populations of coins, \u201cpositive\u201d coins that each have|unknown and potentially different\u2014bias \u2265 \u00bd + \u0394 and \u201cnegative\u201d coins with bias \u2264 \u00bd \u2013 \u0394, we consider the task of estimating the fraction \u03c1 of positive coins to within additive error \u220a. We achieve an upper and lower bound of samples for a 1 \u2013 \u03b4 probability of success, where crucially, our lower bound applies to all fully-adaptive algorithms. Thus, our sample complexity bounds have tight dependence for every relevant problem parameter. A crucial component of our lower bound proof is a decomposition lemma (Lemma 5.2) showing how to assemble partially-adaptive bounds into a fully-adaptive bound, which may be of independent interest: though we invoke it for the special case of Bernoulli random variables (coins), it applies to general distributions. We present simulation results to demonstrate the practical efficacy of our approach for realistic problem parameters for crowdsourcing applications, focusing on the \u201crare events\u201d regime where \u03c1 is small. The fine-grained adaptive avor of both our algorithm and lower bound contrasts with much previous work in distributional testing and learning."}}
