{"id": "alT0KR1FDp", "cdate": 1617980582152, "mdate": null, "content": {"title": "Bridging Machine Learning and Mechanism Design Towards Algorithmic Fairness", "abstract": "Decision-making systems increasingly orchestrate our world: how to intervene on thealgorithmic components to build fair and equitable systems is therefore a question of ut-most importance; one that is substantially complicated by the context-dependent natureof fairness and discrimination. Modern decision-making systems that involve allocatingresources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavioror constrained allocation, concerns usually tackled in the context of mechanism design.Although both machine learning and mechanism design have developedframeworks foraddressing issues of fairness and equity, in some complex decision-making systems, nei-ther framework is individually sufficient.  In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, weargue, are inherent to each field.  Our ultimate objective is to build anencompassingframework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines."}}
{"id": "zOs07Xe6tsh", "cdate": 1577836800000, "mdate": null, "content": {"title": "Designing Evaluation Rules That Are Robust to Strategic Behavior", "abstract": "Machine learning is often used to produce decision-making rules that classify or evaluate individuals. When these individuals have incentives to be classified a certain way, they may behave strategically to influence their outcomes. We develop a model for how strategic agents can invest effort to change the outcomes they receive, and we give a tight characterization of when such agents can be incentivized to invest specified forms of effort into improving their outcomes as opposed to \u201cgaming\u201d the classifier. We show that whenever any \u201creasonable\u201d mechanism can do so, a simple linear mechanism suffices. This work is based on \u201cHow Do Classifiers Induce Agents To Invest Effort Strategically?\u201d published in Economics and Computation 2019 (Kleinberg and Raghavan 2019)."}}
{"id": "lmCYnvdUhbx", "cdate": 1577836800000, "mdate": null, "content": {"title": "Mitigating bias in algorithmic hiring: evaluating claims and practices.", "abstract": "There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law."}}
{"id": "OW64Vth1Twn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Greedy Algorithm almost Dominates in Smoothed Contextual Bandits", "abstract": "Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users in order to gain information that will lead to better decisions in the future. While necessary in the worst case, explicit exploration has a number of disadvantages compared to the greedy algorithm that always \"exploits\" by choosing an action that currently looks optimal. We ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm in the linear contextual bandits model. We improve on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\\tilde O(T^{1/3})$."}}
{"id": "w8UwmbyaKjh", "cdate": 1546300800000, "mdate": null, "content": {"title": "How Do Classifiers Induce Agents to Invest Effort Strategically?", "abstract": "Algorithms are often used to produce decision-making rules that classify or evaluate individuals. When these individuals have incentives to be classified a certain way, they may behave strategically to influence their outcomes. We develop a model for how strategic agents can invest effort in order to change the outcomes they receive, and we give a tight characterization of when such agents can be incentivized to invest specified forms of effort into improving their outcomes as opposed to \"gaming\" the classifier. We show that whenever any \"reasonable\" mechanism can do so, a simple linear mechanism suffices."}}
{"id": "uV5DwFxyd5", "cdate": 1546300800000, "mdate": null, "content": {"title": "Mitigating Bias in Algorithmic Employment Screening: Evaluating Claims and Practices.", "abstract": "There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law."}}
{"id": "egnmaLWA8kc", "cdate": 1546300800000, "mdate": null, "content": {"title": "Roles for Computing in Social Change.", "abstract": "A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems -- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own."}}
{"id": "PWWija4DDgk", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons.", "abstract": "Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established \"principal reason\" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant--and withholding others. These \"feature-highlighting explanations\" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world--and the subjective choices necessary to compensate for this--must be understood before these techniques can be usefully implemented."}}
{"id": "HyWi-hb_ZB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Hiring Under Uncertainty", "abstract": "In this paper we introduce the hiring under uncertainty problem to model the questions faced by hiring committees in large enterprises and universities alike. Given a set of $n$ eligible candidates..."}}
{"id": "WyxtsffTmj", "cdate": 1514764800000, "mdate": null, "content": {"title": "Selection Problems in the Presence of Implicit Bias", "abstract": "Over the past two decades, the notion of implicit bias has come to serve as an important com- ponent in our understanding of bias and discrimination in activities such as hiring, promotion, and school admissions. Research on implicit bias posits that when people evaluate others - for example, in a hiring context - their unconscious biases about membership in particular demo- graphic groups can have an effect on their decision-making, even when they have no deliberate intention to discriminate against members of these groups. A growing body of experimental work has demonstrated the effect that implicit bias can have in producing adverse outcomes. Here we propose a theoretical model for studying the effects of implicit bias on selection decisions, and a way of analyzing possible procedural remedies for implicit bias within this model. A canonical situation represented by our model is a hiring setting, in which recruiters are trying to evaluate the future potential of job applicants, but their estimates of potential are skewed by an unconscious bias against members of one group. In this model, we show that measures such as the Rooney Rule, a requirement that at least one member of an underrepresented group be selected, can not only improve the representation of the affected group, but also lead to higher payoffs in absolute terms for the organization performing the recruiting. However, identifying the conditions under which such measures can lead to improved payoffs involves subtle trade- offs between the extent of the bias and the underlying distribution of applicant characteristics, leading to novel theoretical questions about order statistics in the presence of probabilistic side information."}}
