{"id": "nQUzSqvJyLc", "cdate": 1609459200000, "mdate": null, "content": {"title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics", "abstract": "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate."}}
{"id": "TB5hGQKVekm", "cdate": 1609459200000, "mdate": 1636737839769, "content": {"title": "Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification", "abstract": "Cristina Garbacea, Mengtian Guo, Samuel Carton, Qiaozhu Mei. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "wYVEWj5S3q", "cdate": 1577836800000, "mdate": 1636737840240, "content": {"title": "UMSIForeseer at SemEval-2020 Task 11: Propaganda Detection by Fine-Tuning BERT with Resampling and Ensemble Learning", "abstract": "Yunzhe Jiang, Cristina Garbacea, Qiaozhu Mei. Proceedings of the Fourteenth Workshop on Semantic Evaluation. 2020."}}
{"id": "JLNnnLOdOT", "cdate": 1577836800000, "mdate": 1636737840404, "content": {"title": "Neural Language Generation: Formulation, Methods, and Evaluation", "abstract": "Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation."}}
{"id": "5aEBW630A6", "cdate": 1577836800000, "mdate": 1636737840767, "content": {"title": "An Empirical Study on Explainable Prediction of Text Complexity: Preliminaries for Text Simplification", "abstract": "Text simplification reduces the language complexity of professional content for accessibility purposes. End-to-end neural network models have been widely adopted to directly generate the simplified version of input text, usually functioning as a blackbox. We show that text simplification can be decomposed into a compact pipeline of tasks to ensure the transparency and explainability of the process. The first two steps in this pipeline are often neglected: 1) to predict whether a given piece of text needs to be simplified, and 2) if yes, to identify complex parts of the text. The two tasks can be solved separately using either lexical or deep learning methods, or solved jointly. Simply applying explainable complexity prediction as a preliminary step, the out-of-sample text simplification performance of the state-of-the-art, black-box simplification models can be improved by a large margin."}}
{"id": "zbofagAR2pZ", "cdate": 1546300800000, "mdate": 1636737840953, "content": {"title": "Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation", "abstract": "Cristina Garbacea, Samuel Carton, Shiyan Yan, Qiaozhu Mei. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
{"id": "AmNopFC3PF3", "cdate": 1546300800000, "mdate": 1636737840142, "content": {"title": "Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder", "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps."}}
{"id": "4dMpJVvjuCv", "cdate": 1546300800000, "mdate": 1636737839909, "content": {"title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps."}}
{"id": "1DH6Xk5XtR", "cdate": 1546300800000, "mdate": 1636737841224, "content": {"title": "Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation", "abstract": "We conduct a large-scale, systematic study to evaluate the existing evaluation methods for natural language generation in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the ranking of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for natural language generation. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find lexical diversity an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of natural language generation systems."}}
{"id": "4Ns3y9yXXlD", "cdate": 1483228800000, "mdate": 1636737840959, "content": {"title": "A Systematic Analysis of Sentence Update Detection for Temporal Summarization", "abstract": "Temporal summarization algorithms filter large volumes of streaming documents and emit sentences that constitute salient event updates. Systems developed typically combine in an ad-hoc fashion traditional retrieval and document summarization algorithms to filter sentences inside documents. Retrieval and summarization algorithms however have been developed to operate on static document collections. Therefore, a deep understanding of the limitations of these approaches when applied to a temporal summarization task is necessary. In this work we present a systematic analysis of the methods used for retrieval of update sentences in temporal summarization, and demonstrate the limitations and potentials of these methods by examining the retrievability and the centrality of event updates, as well as the existence of intrinsic inherent characteristics in update versus non-update sentences."}}
