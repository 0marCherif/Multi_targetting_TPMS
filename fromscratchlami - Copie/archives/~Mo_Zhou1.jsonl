{"id": "sF2Ut0fflgx", "cdate": 1663849900945, "mdate": null, "content": {"title": "On Trace of PGD-Like Adversarial Attacks", "abstract": "Adversarial attacks pose safety and security concerns to deep learning\napplications, but their characteristics are under-explored.  Yet largely\nimperceptible, a strong trace could have been left by PGD-like attacks in an\nadversarial example.  Recall that PGD-like attacks trigger the ``local\nlinearity'' of a network, which implies different extents of linearity for\nbenign or adversarial examples.  Inspired by this, we construct an Adversarial\nResponse Characteristics (ARC) feature to reflect the model's gradient\nconsistency around the input to indicate the extent of linearity.  Under\ncertain conditions, it qualitatively shows a gradually varying pattern from\nbenign example to adversarial example, as the latter leads to Sequel Attack\nEffect (SAE).  To quantitatively evaluate the effectiveness of ARC, we conduct\nexperiments on CIFAR-10 and ImageNet for attack detection and attack type\nrecognition in a challenging setting.  The results suggest that SAE is an\neffective and unique trace of PGD-like attacks reflected through the ARC\nfeature.  The ARC feature is intuitive, light-weighted, non-intrusive, and\ndata-undemanding.\n"}}
{"id": "wfel7CjOYk", "cdate": 1652737296196, "mdate": null, "content": {"title": "Resource-Adaptive Federated Learning with All-In-One Neural Composition", "abstract": "Conventional Federated Learning (FL) systems inherently assume a uniform processing capacity among clients for deployed models.  However, diverse client hardware often leads to varying computation resources in practice. Such system heterogeneity results in an inevitable trade-off between model complexity and data accessibility as a bottleneck. To avoid such a dilemma and achieve resource-adaptive federated learning, we introduce a simple yet effective mechanism, termed All-In-One Neural Composition, to systematically support training complexity-adjustable models with flexible resource adaption. It is able to efficiently construct models at various complexities using one unified neural basis shared among clients, instead of pruning the global model into local ones. The proposed mechanism endows the system with unhindered access to the full range of knowledge scattered across clients and generalizes existing pruning-based solutions by allowing soft and learnable extraction of low footprint models. Extensive experiment results on popular FL benchmarks demonstrate the effectiveness of our approach. The resulting FL system empowered by our All-In-One Neural Composition, called FLANC, manifests consistent performance gains across diverse system/data heterogeneous setups while keeping high efficiency in computation and communication. "}}
{"id": "jowVZoitZYu", "cdate": 1652737272424, "mdate": null, "content": {"title": "On Trace of PGD-Like Adversarial Attacks", "abstract": "Adversarial attacks pose safety and security concerns for deep learning applications.  Yet largely imperceptible, a strong PGD-like attack may leave strong trace in the adversarial example.  Since attack triggers the local linearity of a network, we speculate network behaves in different extents of linearity for benign examples and adversarial examples.  Thus, we construct Adversarial Response Characteristics (ARC) features to reflect the model's gradient consistency around the input to indicate the extent of linearity.  Under certain conditions, it shows a gradually varying pattern from benign example to adversarial example, as the later leads to Sequel Attack Effect (SAE).  ARC feature can be used for informed attack detection (perturbation magnitude is known) with binary classifier, or uninformed attack detection (perturbation magnitude is unknown) with ordinal regression.  Due to the uniqueness of SAE to PGD-like attacks, ARC is also capable of inferring other attack details such as loss function, or the ground-truth label as a post-processing defense.  Qualitative and quantitative evaluations manifest the effectiveness of ARC feature on CIFAR-10 w/ ResNet-18 and ImageNet w/ ResNet-152 and SwinT-B-IN1K with considerable generalization among PGD-like attacks despite domain shift.  Our method is intuitive, light-weighted, non-intrusive, and data-undemanding."}}
{"id": "VS7Mfo8KgsJ", "cdate": 1652582097131, "mdate": 1652582097131, "content": {"title": "Adversarial Ranking Attack and Defense", "abstract": "Deep Neural Network (DNN) classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in misclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two attacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by adversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation. Conversely, a defense method is also proposed to improve the ranking system robustness, which can mitigate all the proposed attacks simultaneously. Our adversarial ranking attacks and defense are evaluated on datasets including MNIST, Fashion-MNIST, and Stanford-Online-Products. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks. Meanwhile, the system robustness can be moderately improved with our defense. Furthermore, the transferable and universal properties of our adversary illustrate the possibility of realistic black-box attack."}}
{"id": "t7p9BaNbdmF", "cdate": 1652581876321, "mdate": 1652581876321, "content": {"title": "Enhancing Adversarial Robustness for Deep Metric Learning", "abstract": "Owing to security implications of adversarial vulnerability, adversarial robustness of deep metric learning models has to be improved. In order to avoid model collapse due to excessively hard examples, the existing defenses dismiss the min-max adversarial training, but instead learn from a weak adversary inefficiently. Conversely, we propose Hardness Manipulation to efficiently perturb the training triplet till a specified level of hardness for adversarial training, according to a harder benign triplet or a pseudo-hardness function. It is flexible since regular training and min-max adversarial training are its boundary cases. Besides, Gradual Adversary, a family of pseudo-hardness functions is proposed to gradually increase the specified hardness level during training for a better balance between performance and robustness. Additionally, an Intra-Class Structure loss term among benign and adversarial examples further improves model robustness and efficiency. Comprehensive experimental results suggest that the proposed method, although simple in its form, overwhelmingly outperforms the state-of-the-art defenses in terms of robustness, training efficiency, as well as performance on benign examples."}}
{"id": "DtbhwJEhJwe", "cdate": 1640995200000, "mdate": 1668614592750, "content": {"title": "On Trace of PGD-Like Adversarial Attacks", "abstract": "Adversarial attacks pose safety and security concerns to deep learning applications, but their characteristics are under-explored. Yet largely imperceptible, a strong trace could have been left by PGD-like attacks in an adversarial example. Recall that PGD-like attacks trigger the ``local linearity'' of a network, which implies different extents of linearity for benign or adversarial examples. Inspired by this, we construct an Adversarial Response Characteristics (ARC) feature to reflect the model's gradient consistency around the input to indicate the extent of linearity. Under certain conditions, it qualitatively shows a gradually varying pattern from benign example to adversarial example, as the latter leads to Sequel Attack Effect (SAE). To quantitatively evaluate the effectiveness of ARC, we conduct experiments on CIFAR-10 and ImageNet for attack detection and attack type recognition in a challenging setting. The results suggest that SAE is an effective and unique trace of PGD-like attacks reflected through the ARC feature. The ARC feature is intuitive, light-weighted, non-intrusive, and data-undemanding."}}
{"id": "ALPqYcW7N_C", "cdate": 1640995200000, "mdate": 1668614592829, "content": {"title": "Enhancing Adversarial Robustness for Deep Metric Learning", "abstract": "Owing to security implications of adversarial vulnerability, adversarial robustness of deep metric learning models has to be improved. In order to avoid model collapse due to excessively hard examples, the existing defenses dismiss the min-max adversarial training, but instead learn from a weak adversary inefficiently. Conversely, we propose Hardness Manipulation to efficiently perturb the training triplet till a specified level of hardness for adversarial training, according to a harder benign triplet or a pseudo-hardness function. It is flexible since regular training and min-max adversarial training are its boundary cases. Besides, Gradual Adversary, a family of pseudo-hardness functions is proposed to gradually increase the specified hardness level during training for a better balance between performance and robustness. Additionally, an Intra-Class Structure loss term among benign and adversarial examples further improves model robustness and efficiency. Comprehensive experimental results suggest that the proposed method, although simple in its form, overwhelmingly outperforms the state-of-the-art defenses in terms of robustness, training efficiency, as well as performance on benign examples."}}
{"id": "6t4m99NDnHR", "cdate": 1640995200000, "mdate": 1668614592828, "content": {"title": "Enhancing Adversarial Robustness for Deep Metric Learning", "abstract": "Owing to security implications of adversarial vulnerability, adversarial robustness of deep metric learning models has to be improved. In order to avoid model collapse due to excessively hard examples, the existing defenses dismiss the min-max adversarial training, but instead learn from a weak adversary inefficiently. Conversely, we propose Hardness Manipulation to efficiently perturb the training triplet till a specified level of hardness for adversarial training, according to a harder benign triplet or a pseudo-hardness function. It is flexible since regular training and min-max adversarial training are its boundary cases. Besides, Gradual Adversary, a family of pseudo-hardness functions is proposed to gradually increase the specified hardness level during training for a better balance between performance and robustness. Additionally, an Intra-Class Structure loss term among benign and adversarial examples further improves model robust-ness and efficiency. Comprehensive experimental results suggest that the proposed method, although simple in its form, overwhelmingly outperforms the state-of-the-art de-fenses in terms of robustness, training efficiency, as well as performance on benign examples."}}
{"id": "zD6lylGvAEG", "cdate": 1631112397802, "mdate": 1631112397802, "content": {"title": "Practical Relative Order Attack in Deep Ranking", "abstract": "Recent studies unveil the vulnerabilities of deep ranking\nmodels, where an imperceptible perturbation can trigger\ndramatic changes in the ranking result. While previous\nattempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order\nremains under-explored. In this paper, we formulate a new\nadversarial attack against deep ranking systems, i.e., the Order Attack, which covertly alters the relative order among a\nselected set of candidates according to an attacker-specified\npermutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style\nloss imposing an inequality chain reflecting the specified\npermutation. However, direct optimization of such white-box\nobjective is infeasible in a real-world attack scenario due\nto various black-box limitations. To cope with them, we\npropose a Short-range Ranking Correlation metric as a surrogate objective for black-box Order Attack to approximate\nthe white-box method. The Order Attack is evaluated on\nthe Fashion-MNIST and Stanford-Online-Products datasets\nunder both white-box and black-box threat models. The\nblack-box attack is also successfully implemented on a major\ne-commerce platform. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed methods,\nrevealing a new type of ranking model vulnerability."}}
{"id": "CLq_V14Dk0x", "cdate": 1631112186146, "mdate": 1631112186146, "content": {"title": "Adversarial Attack and Defense in Deep Ranking", "abstract": "Deep Neural Network classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in\nmisclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two\nattacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by\nadversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like\nobjective function is designed to obtain the optimal perturbation. Conversely, an anti-collapse triplet defense is proposed to improve the\nranking model robustness against all proposed attacks, where the model learns to prevent the positive and negative samples being pulled\nclose to each other by adversarial attack. To comprehensively measure the empirical adversarial robustness of a ranking model with our\ndefense, we propose an empirical robustness score, which involves a set of representative attacks against ranking models. Our\nadversarial ranking attacks and defenses are evaluated on MNIST, Fashion-MNIST, CUB200-2011, CARS196 and Stanford Online\nProducts datasets. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks.\nNevertheless, our defense can significantly improve the ranking system robustness, and simultaneously mitigate a wide range of attacks"}}
