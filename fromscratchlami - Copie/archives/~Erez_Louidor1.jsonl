{"id": "BQA9YW2AOl", "cdate": 1640995200000, "mdate": 1682448672815, "content": {"title": "Global Optimization Networks", "abstract": "We consider the problem of estimating a good maximizer of a black-box function given noisy examples. We propose to fit a new type of function called a global optimization network (GON), defined as ..."}}
{"id": "0cuVhIoXbE", "cdate": 1640995200000, "mdate": 1682448672817, "content": {"title": "Global Optimization Networks", "abstract": "We consider the problem of estimating a good maximizer of a black-box function given noisy examples. To solve such problems, we propose to fit a new type of function which we call a global optimization network (GON), defined as any composition of an invertible function and a unimodal function, whose unique global maximizer can be inferred in $\\mathcal{O}(D)$ time. In this paper, we show how to construct invertible and unimodal functions by using linear inequality constraints on lattice models. We also extend to \\emph{conditional} GONs that find a global maximizer conditioned on specified inputs of other dimensions. Experiments show the GON maximizers are statistically significantly better predictions than those produced by convex fits, GPR, or DNNs, and are more reasonable predictions for real-world problems."}}
{"id": "qv-lVjtMqh", "cdate": 1609459200000, "mdate": 1682448672816, "content": {"title": "Monotonic Kronecker-Factored Lattice", "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model."}}
{"id": "0pxiMpCyBtr", "cdate": 1601308117754, "mdate": null, "content": {"title": "Monotonic Kronecker-Factored Lattice", "abstract": "It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model."}}
{"id": "Lzf1kFw4zye", "cdate": 1598050243597, "mdate": null, "content": {"title": "Multidimensional Shape Constraints", "abstract": "We propose new multi-input shape constraints across four intuitive categories: complements, diminishers, dominance, and unimodality constraints. We show these shape constraints can be checked and even enforced when training machine-learned models for linear models, generalized additive models, and the nonlinear function class of multi-layer lattice models. Real-world experiments illustrate how the different shape constraints can be used to increase explainability and improve regularization, especially for non-IID train-test distribution shift."}}
{"id": "jYqWWwot_kP", "cdate": 1577836800000, "mdate": 1682448672815, "content": {"title": "Multidimensional Shape Constraints", "abstract": "We propose new multi-input shape constraints across four intuitive categories: complements, diminishers, dominance, and unimodality constraints. We show these shape constraints can be checked and e..."}}
{"id": "HyZdUh-u-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Shape Constraints for Set Functions", "abstract": "Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowl..."}}
{"id": "SA2QD6fOjKk", "cdate": 1262304000000, "mdate": null, "content": {"title": "The tradeoff function for a class of RLL(d, k) constraints", "abstract": "A reverse concatenation coding scheme for storage systems in which the information is encoded first by a modulation (constraint) code and then by a systematic error-correcting code is considered. In this scheme, the output of the modulation coding stage has certain positions left \u201cunconstrained\u201d-in the sense that any way of filling them with bits results in a sequence that satisfies the constraint. These positions are then used to store the parity-check bits of the error-correcting code so that the result is a valid constrained sequence. The tradeoff function defines the maximum overall rate of the encoding scheme for a given density of unconstrained positions. This function is determined for two families of RLL constraints: RLL(d, \u221e) and RLL(d, 2d+2). For RLL(d, 2d+2), a curious dichotomy in the shape of the tradeoff function between different ranges of values of d is shown to exist."}}
{"id": "KyANYyPEcpC", "cdate": 1262304000000, "mdate": null, "content": {"title": "The Tradeoff Function for a Class of $\\mathrm{RLL}(d, k)$ Constraints", "abstract": "A reverse concatenation coding scheme for storage systems in which the information is encoded first by a modulation (constraint) code and then by a systematic error-correcting code is considered. In this scheme, the output of the modulation coding stage has certain positions left \u201cunconstrained\u201d in the sense that any way of filling them with bits results in a sequence that satisfies the constraint. These positions are then used to store the parity-check bits of the error-correcting code so that the result is a valid constrained sequence. The tradeoff function defines the maximum overall rate of the encoding scheme for a given density of unconstrained positions. This function is determined for two families of run length limited (RLL) constraints: $\\mathrm{RLL}(d,\\infty)$ and $\\mathrm{RLL}(d,2d+2)$. For $\\mathrm{RLL}(d,2d+2)$, a curious dichotomy in the shape of the tradeoff function between different ranges of values of d is shown to exist."}}
{"id": "AbtDjPWYH_Z", "cdate": 1262304000000, "mdate": null, "content": {"title": "Improved lower bounds on capacities of symmetric 2D constraints using Rayleigh quotients", "abstract": "para xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> A method for computing lower bounds on capacities of two-dimensional (2D) constraints having a symmetric presentation in either the horizontal or the vertical direction is presented. The method is a generalization of the method of Calkin and Wilf (SIAM <emphasis emphasistype=\"italic\">J. Discrete Math.</emphasis>, 1998). Previous best lower bounds on capacities of certain constraints are improved using the method. It is also shown how this method, as well as their method for computing upper bounds on the capacity, can be applied to constraints which are not of finite-type. Additionally, capacities of two families of multidimensional constraints are given exactly. </para>"}}
