{"id": "mqSFpGgGQnv", "cdate": 1672531200000, "mdate": 1681721016060, "content": {"title": "Learning time-scales in two-layers neural networks", "abstract": "Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically `simpler' or `easier to learn' although in a way that is difficult to formalize. Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numerical simulations, we propose a scenario for the learning dynamics in this setting. In particular, the proposed evolution exhibits separation of timescales and intermittency. These behaviors arise naturally because the population gradient flow can be recast as a singularly perturbed dynamical system."}}
{"id": "rxgHtZQSxq", "cdate": 1645715836549, "mdate": 1645715836549, "content": {"title": "Acceleration of Gossip Algorithms through the Euler-Poisson-Darboux Equation", "abstract": "Gossip algorithms and their accelerated versions have been studied exclusively in discrete time on graphs. In this work, we take a different approach, and consider the scaling limit of gossip algorithms in both large graphs and large number of iterations. These limits lead to well-known partial differential equations (PDEs) with insightful properties. On lattices, we prove that the non-accelerated gossip algorithm of Boyd et al. [2006] converges to the heat equation, and the accelerated Jacobi polynomial iteration of Berthier et al. [2020] converges to the Euler-Poisson-Darboux (EPD) equation - a damped wave equation. Remarkably, with appropriate parameters, the fundamental solution of the EPD equation has the ideal gossip behaviour: a uniform density over an ellipsoid, whose radius increases at a rate proportional to t - the fastest possible rate for locally communicating gossip algorithms. This is in contrast with the heat equation where the density spreads on a typical scale of t\u221a. Additionally, we provide simulations demonstrating that the gossip algorithms are accurately approximated by their limiting PDEs."}}
{"id": "h1dL_rQ6gmh", "cdate": 1640995200000, "mdate": 1681721016149, "content": {"title": "Incremental Learning in Diagonal Linear Networks", "abstract": "Diagonal linear networks (DLNs) are a toy simplification of artificial neural networks; they consist in a quadratic reparametrization of linear regression inducing a sparse implicit regularization. In this paper, we describe the trajectory of the gradient flow of DLNs in the limit of small initialization. We show that incremental learning is effectively performed in the limit: coordinates are successively activated, while the iterate is the minimizer of the loss constrained to have support on the active coordinates only. This shows that the sparse implicit regularization of DLNs decreases with time. This work is restricted to the underparametrized regime with anti-correlated features for technical reasons."}}
{"id": "RrY7KXuy_Zy", "cdate": 1640995200000, "mdate": 1681721015982, "content": {"title": "Acceleration of Gossip Algorithms through the Euler-Poisson-Darboux Equation", "abstract": "Gossip algorithms and their accelerated versions have been studied exclusively in discrete time on graphs. In this work, we take a different approach, and consider the scaling limit of gossip algorithms in both large graphs and large number of iterations. These limits lead to well-known partial differential equations (PDEs) with insightful properties. On lattices, we prove that the non-accelerated gossip algorithm of Boyd et al. [2006] converges to the heat equation, and the accelerated Jacobi polynomial iteration of Berthier et al. [2020] converges to the Euler-Poisson-Darboux (EPD) equation - a damped wave equation. Remarkably, with appropriate parameters, the fundamental solution of the EPD equation has the ideal gossip behaviour: a uniform density over an ellipsoid, whose radius increases at a rate proportional to t - the fastest possible rate for locally communicating gossip algorithms. This is in contrast with the heat equation where the density spreads on a typical scale of $\\sqrt{t}$. Additionally, we provide simulations demonstrating that the gossip algorithms are accurately approximated by their limiting PDEs."}}
{"id": "bGfDnD7xo-v", "cdate": 1621630162780, "mdate": null, "content": {"title": "Continuized Accelerations of Deterministic and Stochastic Gradient Descents, and of Gossip Algorithms", "abstract": "We introduce the ``continuized'' Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; but a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters. \nWe provide continuized Nesterov acceleration under deterministic as well as stochastic gradients, with either additive or multiplicative noise.  Finally, using our continuized framework and expressing the gossip averaging problem as the stochastic minimization of a certain energy function, we provide the first rigorous acceleration of asynchronous gossip algorithms."}}
{"id": "cur6KU-nUah", "cdate": 1609459200000, "mdate": 1652268060894, "content": {"title": "A Continuized View on Nesterov Acceleration", "abstract": "We introduce the \"continuized\" Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; but a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters."}}
{"id": "VZfC_DQucEJ", "cdate": 1609459200000, "mdate": 1681721015983, "content": {"title": "Analysis and Acceleration of Gradient Descents and Gossip Algorithms. (Analyse et Acc\u00e9l\u00e9ration des Descentes de Gradient et des Algorithmes de Gossip)", "abstract": "Motivated by the recent interest in statistical learning and distributed computing, we study stochastic convex optimization and gossip algorithms in parallel. This joint study is enabled by rigorous relationships that are made between the structures of optimization problems and their equivalents for gossip algorithms. The strong convexity of an optimization problem corresponds to the spectral gap between the two smallest eigenvalues of the graph Laplacian for gossip algorithms. The capacity and source conditions of a least-squares problem, that describe powerlaw scalings for the eigenvalues and for the projection of the optimum against the eigenvectors, correspond to the spectral dimension of the graph for gossip algorithms."}}
{"id": "OjzCgW-f0CP", "cdate": 1609459200000, "mdate": 1652268061036, "content": {"title": "Continuized Accelerations of Deterministic and Stochastic Gradient Descents, and of Gossip Algorithms", "abstract": "We introduce the ``continuized'' Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; but a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters. We provide continuized Nesterov acceleration under deterministic as well as stochastic gradients, with either additive or multiplicative noise. Finally, using our continuized framework and expressing the gossip averaging problem as the stochastic minimization of a certain energy function, we provide the first rigorous acceleration of asynchronous gossip algorithms."}}
{"id": "5mjV8dNqQ7", "cdate": 1609459200000, "mdate": 1681721015955, "content": {"title": "Graph-based Approximate Message Passing Iterations", "abstract": "Approximate-message passing (AMP) algorithms have become an important element of high-dimensional statistical inference, mostly due to their adaptability and concentration properties, the state evolution (SE) equations. This is demonstrated by the growing number of new iterations proposed for increasingly complex problems, ranging from multi-layer inference to low-rank matrix estimation with elaborate priors. In this paper, we address the following questions: is there a structure underlying all AMP iterations that unifies them in a common framework? Can we use such a structure to give a modular proof of state evolution equations, adaptable to new AMP iterations without reproducing each time the full argument ? We propose an answer to both questions, showing that AMP instances can be generically indexed by an oriented graph. This enables to give a unified interpretation of these iterations, independent from the problem they solve, and a way of composing them arbitrarily. We then show that all AMP iterations indexed by such a graph admit rigorous SE equations, extending the reach of previous proofs, and proving a number of recent heuristic derivations of those equations. Our proof naturally includes non-separable functions and we show how existing refinements, such as spatial coupling or matrix-valued variables, can be combined with our framework."}}
{"id": "PBkP0vyT69", "cdate": 1577836800000, "mdate": 1681721016012, "content": {"title": "Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model", "abstract": "In the context of statistical supervised learning, the noiseless linear model assumes that there exists a deterministic linear relation $Y = \\langle \\theta_*, X \\rangle$ between the random output $Y$ and the random feature vector $\\Phi(U)$, a potentially non-linear transformation of the inputs $U$. We analyze the convergence of single-pass, fixed step-size stochastic gradient descent on the least-square risk under this model. The convergence of the iterates to the optimum $\\theta_*$ and the decay of the generalization error follow polynomial convergence rates with exponents that both depend on the regularities of the optimum $\\theta_*$ and of the feature vectors $\\Phi(u)$. We interpret our result in the reproducing kernel Hilbert space framework. As a special case, we analyze an online algorithm for estimating a real function on the unit interval from the noiseless observation of its value at randomly sampled points; the convergence depends on the Sobolev smoothness of the function and of a chosen kernel. Finally, we apply our analysis beyond the supervised learning setting to obtain convergence rates for the averaging process (a.k.a. gossip algorithm) on a graph depending on its spectral dimension."}}
