{"id": "ygPsVlQ9KIZ", "cdate": 1672531200000, "mdate": 1683296246819, "content": {"title": "Offline Policy Evaluation with Out-of-Sample Guarantees", "abstract": "We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finite-sample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy - including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions."}}
{"id": "0Eci00oEw5I", "cdate": 1672531200000, "mdate": 1683296246721, "content": {"title": "Regularized Linear Regression via Covariance Fitting", "abstract": "The linear minimum mean-square error estimator (LMMSE) can be viewed as a solution to a certain regularized least-squares problem formulated using model covariance matrices. However, the appropriate parameters of the model covariance matrices are unknown in many applications. This raises the question: how should we choose them using only the data? Using data-adaptive matrices obtained via the covariance fitting SPICE-methodology, we show that the empirical LMMSE is equivalent to tuned versions of various known regularized estimators \u2013 such as ridge regression, LASSO, and regularized least absolute deviation \u2013 depending on the chosen covariance structures. These theoretical results unify several important estimators under a common umbrella. Furthermore, through a number of numerical examples we show that the regularization parameters obtained via covariance fitting are close to optimal for a range of different signal conditions."}}
{"id": "nc8inihcDP", "cdate": 1640995200000, "mdate": 1683296246754, "content": {"title": "Surprises in adversarially-trained linear regression", "abstract": "State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against such examples. It is formulated as a min-max problem, searching for the best solution when the training data was corrupted by the worst-case attacks. For linear regression problems, adversarial training can be formulated as a convex problem. We use this reformulation to make two technical contributions: First, we formulate the training problem as an instance of robust regression to reveal its connection to parameter-shrinking methods, specifically that $\\ell_\\infty$-adversarial training produces sparse solutions. Secondly, we study adversarial training in the overparameterized regime, i.e. when there are more parameters than data. We prove that adversarial training with small disturbances gives the solution with the minimum-norm that interpolates the training data. Ridge regression and lasso approximate such interpolating solutions as their regularization parameter vanishes. By contrast, for adversarial training, the transition into the interpolation regime is abrupt and for non-zero values of disturbance. This result is proved and illustrated with numerical examples."}}
{"id": "I4sgkx-QR9M", "cdate": 1640995200000, "mdate": 1683296246796, "content": {"title": "Learning Pareto-Efficient Decisions with Confidence", "abstract": "The paper considers the problem of multi-objective decision support when outcomes are uncertain. We extend the concept of Pareto-efficient decisions to take into account the uncertainty of decision outcomes across varying contexts. This enables quantifying trade-offs between decisions in terms of tail outcomes that are relevant in safety-critical applications. We propose a method for learning efficient decisions with statistical confidence, building on results from the conformal prediction literature. The method adapts to weak or nonexistent context covariate overlap and its statistical guarantees are evaluated using both synthetic and real data."}}
{"id": "C9b5czl5Ku", "cdate": 1640995200000, "mdate": 1683296246832, "content": {"title": "Calibration tests beyond classification", "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression problems."}}
{"id": "6WIa8r8y_cP", "cdate": 1640995200000, "mdate": 1683296246600, "content": {"title": "Diagnostic Tool for Out-of-Sample Model Evaluation", "abstract": "Assessment of model fitness is a key part of machine learning. The standard paradigm is to learn models by minimizing a chosen loss function averaged over training data, with the aim of achieving small losses on future data. In this paper, we consider the use of a finite calibration data set to characterize the future, out-of-sample losses of a model. We propose a simple model diagnostic tool that provides finite-sample guarantees under weak assumptions. The tool is simple to compute and to interpret. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyper-parameter tuning."}}
{"id": "uLjxQYLDOHl", "cdate": 1609459200000, "mdate": 1683296246825, "content": {"title": "Scalable Belief Updating for Urban Air Quality Modeling and Prediction", "abstract": "Air pollution is one of the major concerns in global urbanization. Data science can help to understand the dynamics of air pollution and build reliable statistical models to forecast air pollution levels. To achieve these goals, one needs to learn the statistical models which can capture the dynamics from the historical data and predict air pollution in the future. Furthermore, the large size and heterogeneity of today\u2019s big urban data pose significant challenges on the scalability and flexibility of the statistical models. In this work, we present a scalable belief updating framework that is able to produce reliable predictions, using over millions of historical hourly air pollutant and meteorology records. We also present a non-parametric approach to learn the statistical model which reveals interesting periodical dynamics and correlations of the dataset. Based on the scalable belief update framework and the non-parametric model learning approach, we propose an iterative update algorithm to accelerate Gaussian process, which is notorious for its prohibitive computation with large input data. Finally, we demonstrate how to integrate information from heterogeneous data by regarding the beliefs produced by other models as the informative prior. Numerical examples and experimental results are presented to validate the proposed method."}}
{"id": "ss5iIySbe0G", "cdate": 1609459200000, "mdate": 1683296246750, "content": {"title": "Calibration tests beyond classification", "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression problems."}}
{"id": "YyXTZdW7VSQ", "cdate": 1609459200000, "mdate": 1683296246580, "content": {"title": "Inference of causal effects when control variables are unknown", "abstract": "Conventional methods in causal effect inference typically rely on specifying a valid set of control variables. When this set is unknown or misspecified, inferences will be erroneous. We propose a m..."}}
{"id": "BfTixH1jdd", "cdate": 1609459200000, "mdate": 1683296246819, "content": {"title": "Learning Pareto-Efficient Decisions with Confidence", "abstract": "The paper considers the problem of multi-objective decision support when outcomes are uncertain. We extend the concept of Pareto-efficient decisions to take into account the uncertainty of decision outcomes across varying contexts. This enables quantifying trade-offs between decisions in terms of tail outcomes that are relevant in safety-critical applications. We propose a method for learning efficient decisions with statistical confidence, building on results from the conformal prediction literature. The method adapts to weak or nonexistent context covariate overlap and its statistical guarantees are evaluated using both synthetic and real data."}}
