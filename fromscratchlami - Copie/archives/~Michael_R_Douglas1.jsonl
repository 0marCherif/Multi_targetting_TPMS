{"id": "2G_A39eKHKa", "cdate": 1623608418685, "mdate": 1623608418685, "content": {"title": "Numerical Calabi-Yau metrics from holomorphic networks", "abstract": "We propose machine learning inspired methods for computing numerical Calabi-Yau (Ricci flat\nKahler) metrics, and implement them using Tensorflow/Keras. We compare them with previous \u00a8\nwork, and find that they are far more accurate for manifolds with little or no symmetry. We also\ndiscuss issues such as overparameterization and choice of optimization methods."}}
{"id": "QZ71lPeliT", "cdate": 1609459200000, "mdate": 1681746209250, "content": {"title": "Holomorphic feedforward networks", "abstract": "A very popular model in machine learning is the feedforward neural network (FFN). The FFN can approximate general functions and mitigate the curse of dimensionality. Here we introduce FFNs which represent sections of holomorphic line bundles on complex manifolds, and ask some questions about their approximating power. We also explain formal similarities between the standard approach to supervised learning and the problem of finding numerical Ricci flat K\\\"ahler metrics, which allow carrying some ideas between the two problems."}}
{"id": "CxNu9tzrAHa", "cdate": 1609459200000, "mdate": 1681746209201, "content": {"title": "What Is Learned in Knowledge Graph Embeddings?", "abstract": "A knowledge graph (KG) is a data structure which represents entities and relations as the vertices and edges of a directed graph with edge types. KGs are an important primitive in modern machine learning and artificial intelligence. Embedding-based models, such as the seminal TransE [Bordes et al. 2013] and the recent PairRE [Chao et al. 202] are among the most popular and successful approaches for representing KGs and inferring missing edges (link completion). Their relative success is often credited in the literature to their ability to learn logical rules between the relations. In this work, we investigate whether learning rules between relations is indeed what drives the performance of embedding-based methods. We define motif learning and two alternative mechanisms, network learning (based only on the connectivity of the KG, ignoring the relation types), and unstructured statistical learning (ignoring the connectivity of the graph). Using experiments on synthetic KGs, we show that KG models can learn motifs and how this ability is degraded by non-motif (noise) edges. We propose tests to distinguish the contributions of the three mechanisms to performance, and apply them to popular KG benchmarks. We also discuss an issue with the standard performance testing protocol and suggest an improvement."}}
{"id": "XGEqHUWM1J", "cdate": 1136073600000, "mdate": null, "content": {"title": "Computational complexity of the landscape I", "abstract": "We study the computational complexity of the physical problem of finding vacua of string theory which agree with data, such as the cosmological constant, and show that such problems are typically NP hard. In particular, we prove that in the Bousso-Polchinski model, the problem is NP complete. We discuss the issues this raises and the possibility that, even if we were to find compelling evidence that some vacuum of string theory describes our universe, we might never be able to find that vacuum explicitly. In a companion paper, we apply this point of view to the question of how early cosmology might select a vacuum."}}
{"id": "L4ndUvQ194d", "cdate": 473385600000, "mdate": null, "content": {"title": "A Digital Orrery.", "abstract": "We have designed and built the Orrery, a special computer for high-speed high-precision orbital mechanics computations. On the problems the Orrery was designed to solve, it achieves approximately 10 Mflops in about 1 ft3 of space while consuming 150 W of power. The specialized parallel architecture of the Orrery, which is well matched to orbital mechanics problems, is the key to obtaining such high performance. In this paper we discuss the design, construction, and programming of the Orrery."}}
