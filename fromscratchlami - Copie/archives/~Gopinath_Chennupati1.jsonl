{"id": "3nQtMhLCBWd", "cdate": 1674501273999, "mdate": 1674501273999, "content": {"title": "General-Purpose Unsupervised Cyber Anomaly Detection via Non-Negative Tensor Factorization", "abstract": "Distinguishing malicious anomalous activities from unusual but benign activities is a fundamental challenge for cyber\ndefenders. Prior studies have shown that statistical user behavior analysis yields accurate detections by learning behavior\nproiles from observed user activity. These unsupervised models are able to generalize to unseen types of attacks by detecting\ndeviations from normal behavior, without knowledge of speciic attack signatures. However, approaches proposed to date\nbased on probabilistic matrix factorization are limited by the information conveyed in a two-dimensional space. Non-negative\ntensor factorization, on the other hand, is a powerful unsupervised machine learning method that naturally models multi-\ndimensional data, capturing complex and multi-faceted details of behavior proiles. Our new unsupervised statistical anomaly\ndetection methodology matches or surpasses state-of-the-art supervised learning baselines across several challenging and\ndiverse cyber application areas, including detection of compromised user credentials, botnets, spam e-mails, and fraudulent\ncredit card transactions."}}
{"id": "LnygZu8WJk", "cdate": 1664872119968, "mdate": null, "content": {"title": "Can Calibration Improve Sample Prioritization?", "abstract": "Calibration can reduce overconfident predictions of deep neural networks, but can calibration also accelerate training? In this paper, we show that it can when used to prioritize some examples for performing subset selection. We study the effect of popular calibration techniques in selecting better subsets of samples during training (also called sample prioritization) and observe that calibration can improve the quality of subsets, reduce the number of examples per epoch (by at least 70%), and can thereby speed up the overall training process. We further study the effect of using calibrated pre-trained models coupled with calibration during training to guide sample prioritization, which again seems to improve the quality of samples selected."}}
{"id": "eyna3N1FOBN", "cdate": 1609459200000, "mdate": 1633552879521, "content": {"title": "PPT-SASMM: Scalable Analytical Shared Memory Model: Predicting the Performance of Multicore Caches from a Single-Threaded Execution Trace", "abstract": "Performance modeling of parallel applications on multicore processors remains a challenge in computational co-design due to multicore processors' complex design. Multicores include complex private and shared memory hierarchies. We present a Scalable Analytical Shared Memory Model (SASMM). SASMM can predict the performance of parallel applications running on a multicore. SASMM uses a probabilistic and computationally-efficient method to predict the reuse distance profiles of caches in multicores. SASMM relies on a stochastic, static basic block-level analysis of reuse profiles. The profiles are calculated from the memory traces of applications that run sequentially rather than using multi-threaded traces. The experiments show that our model can predict private L1 cache hit rates with 2.12% and shared L2 cache hit rates with about 1.50% error rate."}}
{"id": "ccJwGIfRLZ", "cdate": 1609459200000, "mdate": 1633552879527, "content": {"title": "PPT-Multicore: Performance Prediction of OpenMP applications using Reuse Profiles and Analytical Modeling", "abstract": "We present PPT-Multicore, an analytical model embedded in the Performance Prediction Toolkit (PPT) to predict parallel application performance running on a multicore processor. PPT-Multicore builds upon our previous work towards a multicore cache model. We extract LLVM basic block labeled memory trace using an architecture-independent LLVM-based instrumentation tool only once in an application's lifetime. The model uses the memory trace and other parameters from an instrumented sequentially executed binary. We use a probabilistic and computationally efficient reuse profile to predict the cache hit rates and runtimes of OpenMP programs' parallel sections. We model Intel's Broadwell, Haswell, and AMD's Zen2 architectures and validate our framework using different applications from PolyBench and PARSEC benchmark suites. The results show that PPT-Multicore can predict cache hit rates with an overall average error rate of 1.23% while predicting the runtime with an error rate of 9.08%."}}
{"id": "RgXI14FTsu", "cdate": 1609459200000, "mdate": 1633552879513, "content": {"title": "Machine Learning-enabled Scalable Performance Prediction of Scientific Codes", "abstract": "Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage. PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times."}}
{"id": "Q170rb4Hm_", "cdate": 1609459200000, "mdate": 1631587055465, "content": {"title": "An Effective Baseline for Robustness to Distributional Shift", "abstract": "Refraining from confidently predicting when faced with categories of inputs different from those seen during training is an important requirement for the safe deployment of deep learning systems. While simple to state, this has been a particularly challenging problem in deep learning, where models often end up making overconfident predictions in such situations. In this work we present a simple, but highly effective approach to deal with out-of-distribution detection that uses the principle of abstention: when encountering a sample from an unseen class, the desired behavior is to abstain from predicting. Our approach uses a network with an extra abstention class and is trained on a dataset that is augmented with an uncurated set that consists of a large number of out-of-distribution (OoD) samples that are assigned the label of the abstention class; the model is then trained to learn an effective discriminator between in and out-of-distribution samples. We compare this relatively simple approach against a wide variety of more complex methods that have been proposed both for out-of-distribution detection as well as uncertainty modeling in deep learning, and empirically demonstrate its effectiveness on a wide variety of of benchmarks and deep architectures for image recognition and text classification, often outperforming existing approaches by significant margins. Given the simplicity and effectiveness of this method, we propose that this approach be used as a new additional baseline for future work in this domain."}}
{"id": "2wH3zsRtu6", "cdate": 1609459200000, "mdate": 1633552879510, "content": {"title": "Finding the Number of Latent Topics With Semantic Non-Negative Matrix Factorization", "abstract": "Topic modeling, or identifying the set of  <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">topics</i>  that occur in a collection of articles, is one of the primary objectives of text mining. One of the big challenges in topic modeling is determining the correct number of topics: underestimating the number of topics results in a loss of information, i.e., omission of topics, underfitting, while overestimating leads to noisy and unexplainable topics and overfitting. In this paper, we consider a semantic-assisted non-negative matrix factorization (NMF) topics model, which we call SeNMFk, based on Kullback-Leibler(KL) divergence and integrated with a method for determining the number of latent topics. SeNMFk involves (i) creating a random ensemble of pairs of matrices whose mean is equal to the initial words-by-documents matrix representing the text corpus and the Shifted Positive Pointwise Mutual Information (SPPMI) matrix, which encodes the context information, respectively, and (ii) jointly factorizing each of these pairs with different number of topics to acquire sets of latent topics that are stable to noise. We demonstrate the performance of our method by identifying the number of topics in several benchmark text corpora, when compared to other state-of-the-art techniques. We also show that the number of document classes in the input text corpus may differ from the number of the extracted latent topics, but these classes can be retrieved by clustering the column-vectors of one of the factor matrices. Additionally, we introduce a software called pyDNMFk to estimate the number of topics. We demonstrate that our unsupervised method, SeNMFk, not only determines the correct number of topics, but also extracts topics with a high coherence and accurately classifies the documents of the corpus."}}
{"id": "q_Q9MMGwSQu", "cdate": 1601308330473, "mdate": null, "content": {"title": "A Simple and Effective  Baseline for Out-of-Distribution Detection using Abstention", "abstract": "Refraining from confidently predicting when faced with  categories of inputs different from those seen during training is an important requirement for the safe deployment of deep learning systems. While simple to state, this has been  a particularly challenging problem in deep learning, where models often end up making overconfident predictions in such situations. In this work we present a simple, but highly effective approach to deal with out-of-distribution detection that uses the principle of abstention: when encountering a sample from an unseen class, the desired behavior is to abstain from predicting. Our  approach uses a network with an extra abstention class and is trained on a dataset that  is augmented with an uncurated  set that consists of a large number of out-of-distribution (OoD) samples that are assigned the label of the abstention class; the model is then trained to learn an effective discriminator between in and out-of-distribution samples. \n\n We  compare this relatively simple approach against a wide variety of more complex methods that have been proposed both for out-of-distribution detection as well as uncertainty modeling in deep learning, and empirically demonstrate its effectiveness on a wide variety of of benchmarks and deep architectures for image recognition and text classification, often outperforming existing approaches by significant margins. Given the simplicity and effectiveness of this method, we propose that this approach be used as a new additional baseline for future work in this domain."}}
{"id": "_eip8E5ZYy", "cdate": 1596123481271, "mdate": null, "content": {"title": "Code Characterization with Graph Convolutions and Capsule Networks", "abstract": "We propose SiCaGCN, a learning system to predict the similarity of a given software code to a set of codes that are permitted to run on a computational resource, such as a supercomputer or a cloud server. This code characterization allows us to detect abusive codes. Our system relies on a structural analysis of the control-flow graph of the software codes and two different graph similarity measures: Graph Edit Distance (GED) and a singular values based metric. SiCaGCN combines elements of Graph Convolutional Neural Networks (GCN), Capsule networks, attention mechanism, and neural tensor networks. Our experimental results include a study of the trade-offs between the two similarity metrics and two variations of our learning networks, with and without the use of capsules. Our main findings are that the use of capsules reduces mean square error significantly for both similarity metrics. Use of capsules reduces the runtime to calculate the GED while increases the runtime of singular values calculation."}}
{"id": "wyKptHtRhck", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distributed Non-Negative Tensor Train Decomposition", "abstract": "The era of exascale computing opens new venues for innovations and discoveries in many scientific, engineering, and commercial fields. However, with the exaflops also come the extra-large high-dimensional data generated by high-performance computing. High-dimensional data is presented as multidimensional arrays, aka tensors. The presence of latent (not directly observable) structures in the tensor allows a unique representation and compression of the data by classical tensor factorization techniques. However, the classical tensor methods are not always stable or they can be exponential in their memory requirements, which makes them not suitable for high-dimensional tensors. Tensor train (TT) is a state-of-the-art tensor network introduced for factorization of high-dimensional tensors. TT transforms the initial high-dimensional tensor in a network of three-dimensional tensors that requires only a linear storage. Many real-world data, such as, density, temperature, population, probability, etc., are non-negative and for an easy interpretation, the algorithms preserving non-negativity are preferred. Here, we introduce a distributed non-negative tensor-train and demonstrate its scalability and the compression on synthetic and real-world big datasets."}}
