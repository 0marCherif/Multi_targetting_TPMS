{"id": "HygVcrHlUH", "cdate": 1567802763775, "mdate": null, "content": {"title": "Exponentially convergent stochastic k-PCA without variance reduction", "abstract": "We present Matrix Krasulina, an algorithm for online k-PCA, by gen- eralizing the classic Krasulina\u2019s method (Krasulina, 1969) from vector to matrix case. We show, both theoretically and empirically, that the algorithm naturally adapts to data low-rankness and converges exponentially fast to the ground-truth principal subspace. Notably, our result suggests that despite various recent efforts to accelerate the convergence of stochastic-gradient based methods by adding a O(n)-time variance reduction step, for the k- PCA problem, a truly online SGD variant suffices to achieve exponential convergence on intrinsically low-rank data."}}
{"id": "HyiRazbRb", "cdate": 1518730158682, "mdate": null, "content": {"title": "Demystifying overcomplete nonlinear auto-encoders: fast SGD convergence towards sparse representation from random initialization", "abstract": "Auto-encoders are commonly used for unsupervised representation learning and for pre-training deeper neural networks.\nWhen its activation function is linear and the encoding dimension (width of hidden layer) is smaller than the input dimension, it is well known that auto-encoder is optimized to learn the principal components of the data distribution (Oja1982).\nHowever, when the activation is nonlinear and when the width is larger than the input dimension (overcomplete), auto-encoder behaves differently from PCA, and in fact is known to perform well empirically for sparse coding problems. \n\nWe provide a theoretical explanation for this empirically observed phenomenon, when rectified-linear unit (ReLu) is adopted as the activation function and the hidden-layer width is set to be large.\nIn this case, we show that, with significant probability, initializing the weight matrix of an auto-encoder by sampling from a spherical Gaussian distribution followed by stochastic gradient descent (SGD) training converges towards the ground-truth representation for a class of sparse dictionary learning models.\nIn addition, we can show that, conditioning on convergence, the expected convergence rate is O(1/t), where t is the number of updates.\nOur analysis quantifies how increasing hidden layer width helps the training performance when random initialization is used, and how the norm of network weights influence the speed of SGD convergence. "}}
{"id": "HJbKT_-dZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "When do random forests fail?", "abstract": "Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions. In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent.<br /> As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests."}}
