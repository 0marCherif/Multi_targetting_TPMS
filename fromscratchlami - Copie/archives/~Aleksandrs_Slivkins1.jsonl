{"id": "U8xMTXRAtLd", "cdate": 1672531200000, "mdate": 1680989577725, "content": {"title": "Bandit Social Learning: Exploration under Myopic Behavior", "abstract": ""}}
{"id": "ShCQFUwyYg", "cdate": 1672531200000, "mdate": 1681908976720, "content": {"title": "Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics", "abstract": "We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser's total value over multiple rounds of a repeated auction, subject to budget and/or return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any \"intermediate\" auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. This holds whether or not the bidding dynamics converges to an equilibrium and regardless of the correlation structure between advertiser valuations."}}
{"id": "2FZtZzY1QS", "cdate": 1672531200000, "mdate": 1682360785809, "content": {"title": "Budget Pacing in Repeated Auctions: Regret and Efficiency Without Convergence", "abstract": "Online advertising via auctions increasingly dominates the marketing landscape. A typical advertiser may participate in thousands of auctions each day with bids tailored to a variety of signals about user demographics and intent. These auctions are strategically linked through a global budget constraint. To help address the difficulty of bidding, many major online platforms now provide automated budget management via a flexible approach called budget pacing: rather than bidding directly, an advertiser specifies a global budget target and a maximum willingness-to-pay for different types of advertising opportunities. The specified maximums are then scaled down (or \"paced\") by a multiplier so that the realized total spend matches the target budget. These automated bidders are now near-universally adopted across all mature advertising platforms, raising pressing questions about market outcomes that arise when advertisers use budget pacing simultaneously. In this paper we study the aggregate welfare and individual regret guarantees of dynamic pacing algorithms in repeated auctions with budgets. We show that when agents simultaneously use a natural form of gradient-based pacing, the liquid welfare obtained over the course of the dynamics is at least half the optimal liquid welfare obtainable by any allocation rule, matching the best possible bound for static auctions even in pure Nash equilibria [Aggarwal et al., WINE 2019; Babaioff et al., ITCS 2021]. In contrast to prior work, these results hold without requiring convergence of the dynamics, circumventing known computational obstacles of finding equilibria [Chen et al., EC 2021]. Our result is robust to the correlation structure among agents' valuations and holds for any core auction, a broad class that includes first-price, second-price, and GSP auctions. We complement the aggregate guarantees by showing that an agent using such pacing algorithms achieves an O(T^{3/4}) regret relative to the value obtained by the best fixed pacing multiplier in hindsight in stochastic bidding environments. Compared to past work, this result applies to more general auctions and extends to adversarial settings with respect to dynamic regret."}}
{"id": "ITXgYOFi8b", "cdate": 1652737798202, "mdate": null, "content": {"title": "Incentivizing Combinatorial Bandit Exploration", "abstract": "Consider a bandit algorithm that recommends actions to self-interested users in a recommendation system. The users are free to choose other actions and need to be incentivized to follow the algorithm's recommendations. While the users prefer to exploit, the algorithm can incentivize them to explore by leveraging the information collected from the previous users. All published work on this problem, known as incentivized exploration, focuses on small, unstructured action sets and mainly targets the case when the users' beliefs are independent across actions. However, realistic exploration problems often feature large, structured action sets and highly correlated beliefs. We focus on a paradigmatic exploration problem with structure: combinatorial semi-bandits. We prove that Thompson Sampling, when applied to combinatorial semi-bandits, is incentive-compatible when initialized with a sufficient number of samples of each arm (where this number is determined in advance by the Bayesian prior). Moreover, we design incentive-compatible algorithms for collecting the initial samples.\n"}}
{"id": "u2dbHXhIt8-", "cdate": 1640995200000, "mdate": 1682360786083, "content": {"title": "Adversarial Bandits with Knapsacks", "abstract": "We consider Bandits with Knapsacks (henceforth, BwK), a general model for multi-armed bandits under supply/budget constraints. In particular, a bandit algorithm needs to solve a well-known knapsack problem: find an optimal packing of items into a limited-size knapsack. The BwK problem is a common generalization of numerous motivating examples, which range from dynamic pricing to repeated auctions to dynamic ad allocation to network routing and scheduling. While the prior work on BwK focused on the stochastic version, we pioneer the other extreme in which the outcomes can be chosen adversarially. This is a considerably harder problem, compared to both the stochastic version and the \u201cclassic\u201d adversarial bandits, in that regret minimization is no longer feasible. Instead, the objective is to minimize the competitive ratio: the ratio of the benchmark reward to algorithm\u2019s reward. We design an algorithm with competitive ratio O(log T) relative to the best fixed distribution over actions, where T is the time horizon; we also prove a matching lower bound. The key conceptual contribution is a new perspective on the stochastic version of the problem. We suggest a new algorithm for the stochastic version, which builds on the framework of regret minimization in repeated games and admits a substantially simpler analysis compared to prior work. We then analyze this algorithm for the adversarial version, and use it as a subroutine to solve the latter. Our algorithm is the first \u201cblack-box reduction\u201d from bandits to BwK: it takes an arbitrary bandit algorithm and uses it as a subroutine. We use this reduction to derive several extensions."}}
{"id": "tlS5f1YhA6", "cdate": 1640995200000, "mdate": 1682360785754, "content": {"title": "Incentivizing Participation in Clinical Trials", "abstract": "The difficulty of recruiting patients is a well-known issue in clinical trials which inhibits or sometimes precludes them in practice. We interpret this issue as a non-standard version of exploration-exploitation tradeoff: here, the clinical trial would like to explore as uniformly as possible, whereas each patient prefers ``exploitation\", i.e. treatments that seem best. We study how to incentivize participation by leveraging information asymmetry between the trial and the patients. We measure statistical performance via worst-case estimation error under adversarially generated outcomes, a standard objective for clinical trials. We obtain a near-optimal solution in terms of this objective. Namely, we provide an incentive-compatible mechanism with a particular guarantee, and a nearly matching impossibility result for any incentive-compatible mechanism. Our results extend to agents with heterogeneous public and private types."}}
{"id": "lLBrwJ-iHxM", "cdate": 1640995200000, "mdate": 1682360785783, "content": {"title": "Bayesian Exploration: Incentivizing Exploration in Bayesian Games", "abstract": "In a wide range of recommendation systems, self-interested individuals (\u201cagents\u201d) make decisions over time, using information revealed by other agents in the past, and producing information that ma..."}}
{"id": "_wsEha5A7S", "cdate": 1640995200000, "mdate": 1682360785912, "content": {"title": "Incentivizing Combinatorial Bandit Exploration", "abstract": "Consider a bandit algorithm that recommends actions to self-interested users in a recommendation system. The users are free to choose other actions and need to be incentivized to follow the algorithm's recommendations. While the users prefer to exploit, the algorithm can incentivize them to explore by leveraging the information collected from the previous users. All published work on this problem, known as incentivized exploration, focuses on small, unstructured action sets and mainly targets the case when the users' beliefs are independent across actions. However, realistic exploration problems often feature large, structured action sets and highly correlated beliefs. We focus on a paradigmatic exploration problem with structure: combinatorial semi-bandits. We prove that Thompson Sampling, when applied to combinatorial semi-bandits, is incentive-compatible when initialized with a sufficient number of samples of each arm (where this number is determined in advance by the Bayesian prior). Moreover, we design incentive-compatible algorithms for collecting the initial samples."}}
{"id": "WStjrwOTx-w", "cdate": 1640995200000, "mdate": 1682360785934, "content": {"title": "Budget Pacing in Repeated Auctions: Regret and Efficiency without Convergence", "abstract": "We study the aggregate welfare and individual regret guarantees of dynamic \\emph{pacing algorithms} in the context of repeated auctions with budgets. Such algorithms are commonly used as bidding agents in Internet advertising platforms. We show that when agents simultaneously apply a natural form of gradient-based pacing, the liquid welfare obtained over the course of the learning dynamics is at least half the optimal expected liquid welfare obtainable by any allocation rule. Crucially, this result holds \\emph{without requiring convergence of the dynamics}, allowing us to circumvent known complexity-theoretic obstacles of finding equilibria. This result is also robust to the correlation structure between agent valuations and holds for any \\emph{core auction}, a broad class of auctions that includes first-price, second-price, and generalized second-price auctions. For individual guarantees, we further show such pacing algorithms enjoy \\emph{dynamic regret} bounds for individual value maximization, with respect to the sequence of budget-pacing bids, for any auction satisfying a monotone bang-for-buck property."}}
{"id": "OB94Njvl0a6", "cdate": 1640995200000, "mdate": 1682353634308, "content": {"title": "Efficient Contextual Bandits with Knapsacks via Regression", "abstract": "We consider contextual bandits with linear constraints (CBwLC), a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We provide the first algorithm for CBwLC (or CBwK) that is based on regression oracles. The algorithm is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for the variant of CBwK in which the algorithm must stop once some constraint is violated. Further, we provide the first vanishing-regret guarantees for CBwLC (or CBwK) that extend beyond the stochastic environment. We side-step strong impossibility results from prior work by identifying a weaker (and, arguably, fairer) benchmark to compare against. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019), a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques."}}
