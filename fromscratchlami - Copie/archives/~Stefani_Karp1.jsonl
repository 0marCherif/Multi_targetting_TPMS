{"id": "gJ0tW43ed2", "cdate": 1640995200000, "mdate": 1682354535320, "content": {"title": "Agnostic Learnability of Halfspaces via Logistic Loss", "abstract": "We investigate approximation guarantees provided by logistic regression for the fundamental problem of agnostic learning of homogeneous halfspaces. Previously, for a certain broad class of \u201cwell-be..."}}
{"id": "oAjn5-AgSd", "cdate": 1621630098337, "mdate": null, "content": {"title": "Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels", "abstract": "Neural networks have been shown to outperform kernel methods in practice (including neural tangent kernels). Most theoretical explanations of this performance gap focus on learning a complex hypothesis class; in some cases, it is unclear whether this hypothesis class captures realistic data. In this work, we propose a related, but alternative, explanation for this performance gap in the image classification setting, based on finding a sparse signal in the presence of noise. Specifically, we prove that, for a simple data distribution with sparse signal amidst high-variance noise, a simple convolutional neural network trained using stochastic gradient descent learns to threshold out the noise and find the signal. On the other hand, the corresponding neural tangent kernel, with a fixed set of predetermined features, is unable to adapt to the signal in this manner. We supplement our theoretical results by demonstrating this phenomenon empirically: in CIFAR-10 and MNIST images with various backgrounds, as the background noise increases in intensity, a CNN's performance stays relatively robust, whereas its corresponding neural tangent kernel sees a notable drop in performance. We therefore propose the \"local signal adaptivity\" (LSA) phenomenon as one explanation for the superiority of neural networks over kernel methods."}}
{"id": "Yi5iJVjPv8", "cdate": 1609459200000, "mdate": 1684014589899, "content": {"title": "Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels", "abstract": "Neural networks have been shown to outperform kernel methods in practice (including neural tangent kernels). Most theoretical explanations of this performance gap focus on learning a complex hypothesis class; in some cases, it is unclear whether this hypothesis class captures realistic data. In this work, we propose a related, but alternative, explanation for this performance gap in the image classification setting, based on finding a sparse signal in the presence of noise. Specifically, we prove that, for a simple data distribution with sparse signal amidst high-variance noise, a simple convolutional neural network trained using stochastic gradient descent learns to threshold out the noise and find the signal. On the other hand, the corresponding neural tangent kernel, with a fixed set of predetermined features, is unable to adapt to the signal in this manner. We supplement our theoretical results by demonstrating this phenomenon empirically: in CIFAR-10 and MNIST images with various backgrounds, as the background noise increases in intensity, a CNN's performance stays relatively robust, whereas its corresponding neural tangent kernel sees a notable drop in performance. We therefore propose the \"local signal adaptivity\" (LSA) phenomenon as one explanation for the superiority of neural networks over kernel methods."}}
{"id": "y5O3c3cE-9", "cdate": 1577836800000, "mdate": null, "content": {"title": "PAC-Bayes Learning Bounds for Sample-Dependent Priors", "abstract": "We present a series of new PAC-Bayes learning guarantees for randomized algorithms with sample-dependent priors. Our most general bounds make no assumption on the priors and are given in terms of certain covering numbers under the infinite-Renyi divergence and the L1 distance. We show how to use these general bounds to derive leaning bounds in the setting where the sample-dependent priors obey an infinite-Renyi divergence or L1-distance sensitivity condition. We also provide a flexible framework for computing PAC-Bayes bounds, under certain stability assumptions on the sample-dependent priors, and show how to use this framework to give more refined bounds when the priors satisfy an infinite-Renyi divergence sensitivity condition."}}
