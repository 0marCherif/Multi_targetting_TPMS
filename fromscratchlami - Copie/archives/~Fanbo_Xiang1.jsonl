{"id": "nlprZj2Jxl", "cdate": 1668068366096, "mdate": 1668068366096, "content": {"title": "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo", "abstract": "We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF."}}
{"id": "b_CQDy9vrD1", "cdate": 1663849854555, "mdate": null, "content": {"title": "ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills", "abstract": "Generalizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers."}}
{"id": "TlM8KGnTli", "cdate": 1640995200000, "mdate": 1668163518476, "content": {"title": "Close the Visual Domain Gap by Physics-Grounded Active Stereovision Depth Sensor Simulation", "abstract": ""}}
{"id": "LBKWd5Sp6p1", "cdate": 1640995200000, "mdate": 1668163518458, "content": {"title": "OCRTOC: A Cloud-Based Competition and Benchmark for Robotic Grasping and Manipulation", "abstract": ""}}
{"id": "zQIvkXHS_U5", "cdate": 1629517571133, "mdate": null, "content": {"title": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations", "abstract": "Object manipulation from 3D visual inputs poses many challenges on building generalizable perception and policy models. However, 3D assets in existing benchmarks mostly lack the diversity of 3D shapes that align with real-world intra-class complexity in topology and geometry. Here we propose SAPIEN Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3D assets in ManiSkill include large intra-class topological and geometric variations. Tasks are carefully chosen to cover distinct types of manipulation challenges. Latest progress in 3D vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3D deep learning. To this end, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad set of researchers interested in manipulation research. Besides supporting the learning of policies from interactions,  we also support learning-from-demonstrations (LfD) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5M point cloud/RGB-D frames in total). We provide baselines using 3D deep learning and LfD algorithms. All code of our benchmark (simulator, environment, SDK, and baselines) is open-sourced (\\href{https://github.com/haosulab/ManiSkill}{Github repo}), and a challenge facing interdisciplinary researchers will be held based on the benchmark. "}}
{"id": "EougVeukEH9", "cdate": 1624097053745, "mdate": null, "content": {"title": "O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning", "abstract": "Contrary to the vast literature in modeling, perceiving, and understanding agent-object (e.g., human-object, hand-object, robot-object) interaction in computer vision and robotics, very few past works have studied the task of object-object interaction, which also plays an important role in robotic manipulation and planning tasks. There is a rich space of object-object interaction scenarios in our daily life, such as placing an object on a messy tabletop, fitting an object inside a drawer, pushing an object using a tool, etc. In this paper, we propose a unified affordance learning framework to learn object-object interaction for various tasks. By constructing four object-object interaction task environments using physical simulation (SAPIEN) and thousands of ShapeNet models with rich geometric diversity, we are able to conduct large-scale object-object affordance learning without the need for human annotations or demonstrations. At the core of technical contribution, we propose an object-kernel point convolution network to reason about detailed interaction between two objects. Experiments on large-scale synthetic data and real-world data prove the effectiveness of the proposed approach."}}
{"id": "y24j3aLcNb", "cdate": 1609459200000, "mdate": 1668163518516, "content": {"title": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations", "abstract": ""}}
{"id": "tnwaZF_HLro", "cdate": 1609459200000, "mdate": 1668163518517, "content": {"title": "NeuTex: Neural Texture Mapping for Volumetric Neural Rendering", "abstract": ""}}
{"id": "ng_2SdLyj6", "cdate": 1609459200000, "mdate": 1668163518476, "content": {"title": "NeuTex: Neural Texture Mapping for Volumetric Neural Rendering", "abstract": ""}}
{"id": "i6Jw1D3hjy", "cdate": 1609459200000, "mdate": 1668163518477, "content": {"title": "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo", "abstract": ""}}
