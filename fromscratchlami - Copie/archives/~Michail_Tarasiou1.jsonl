{"id": "F2T8qra1yib", "cdate": 1672531200000, "mdate": 1682350361705, "content": {"title": "ViTs for SITS: Vision Transformers for Satellite Image Time Series", "abstract": "In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model for general Satellite Image Time Series (SITS) processing based on the Vision Transformer (ViT). TSViT splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. We argue, that in contrast to natural images, a temporal-then-spatial factorization is more intuitive for SITS processing and present experimental evidence for this claim. Additionally, we enhance the model's discriminative power by introducing two novel mechanisms for acquisition-time-specific temporal positional encodings and multiple learnable class tokens. The effect of all novel design choices is evaluated through an extensive ablation study. Our proposed architecture achieves state-of-the-art performance, surpassing previous approaches by a significant margin in three publicly available SITS semantic segmentation and classification datasets. All model, training and evaluation codes are made publicly available to facilitate further research."}}
{"id": "X6OhbjGQRH", "cdate": 1640995200000, "mdate": 1682350361872, "content": {"title": "Embedding Earth: Self-supervised contrastive pre-training for dense land cover classification", "abstract": "In training machine learning models for land cover semantic segmentation there is a stark contrast between the availability of satellite imagery to be used as inputs and ground truth data to enable supervised learning. While thousands of new satellite images become freely available on a daily basis, getting ground truth data is still very challenging, time consuming and costly. In this paper we present Embedding Earth a self-supervised contrastive pre-training method for leveraging the large availability of satellite imagery to improve performance on downstream dense land cover classification tasks. Performing an extensive experimental evaluation spanning four countries and two continents we use models pre-trained with our proposed method as initialization points for supervised land cover semantic segmentation and observe significant improvements up to 25% absolute mIoU. In every case tested we outperform random initialization, especially so when ground truth data are scarse. Through a series of ablation studies we explore the qualities of the proposed approach and find that learnt features can generalize between disparate regions opening up the possibility of using the proposed pre-training scheme as a replacement to random initialization for Earth observation tasks. Code will be uploaded soon at https://github.com/michaeltrs/DeepSatModels."}}
{"id": "NBc4Y86oDk", "cdate": 1640995200000, "mdate": 1682350361872, "content": {"title": "Deepsatdata: Building Large Scale Datasets of Satellite Images for Training Machine Learning Models", "abstract": "This paper presents DeepSatData a free and open source pipeline for automatically generating satellite imagery datasets for training machine learning models. The implementation presented can be used to query, download and process freely available Sentinel-2 data for the generation of large scale datasets required for training deep neural networks (DNN). We discuss design considerations faced from the point of view of DNN training and evaluation such as checking the quality of ground truth data and assessing the scalability of the approach. Accompanying code is made publicly available in https://github.com/michaeltrs/DeepSatData."}}
{"id": "6T3zEb7Hj-", "cdate": 1640995200000, "mdate": 1682350361705, "content": {"title": "Context-Self Contrastive Pretraining for Crop Type Semantic Segmentation", "abstract": "In this article, we propose a fully supervised pretraining scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed context-self contrastive loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in a training sample and its local context. For crop type semantic segmentation from satellite image time series (SITS), we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, SITS dataset densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pretraining, to improve all respective baselines and present a process for semantic segmentation at greater resolution than that of the input images for obtaining crop classes at a more granular level. The code and instructions to download the data can be found in <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><uri>https://github.com/michaeltrs/DeepSatModels</uri></monospace> ."}}
{"id": "afgM44ig-p", "cdate": 1577836800000, "mdate": 1668426575548, "content": {"title": "Synthesising 3D Facial Motion from \"In-the-Wild\" Speech", "abstract": "Synthesising 3D facial motion from speech is a crucial problem manifesting in a multitude of applications such as computer games and movies. Recently proposed methods tackle this problem in controlled conditions of speech. In this paper, we introduce the first methodology for 3D facial motion synthesis from speech captured in arbitrary recording conditions (\u201cin-the-wild\u201d) and independent of the speaker. For our purposes, we captured 4D sequences of people uttering 500 words, contained in the Lip Reading in the Wild (LRW) words, a publicly available large-scale in-the-wild dataset, and built a set of 3D blendshapes appropriate for speech. We correlate the 3D shape parameters of the speech blendshapes to the LRW audio samples by means of a novel time-warping technique, named Deep Canonical Attentional Warping (DCAW), that can simultaneously learn hierarchical non-linear representations and a warping path in an end-to-end manner. We thoroughly evaluate our proposed methods, and show the ability of a deep learning model to synthesise 3D facial motion in handling different speakers and continuous speech signals in uncontrolled conditions <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> ."}}
{"id": "BTE_rwlGMQs", "cdate": 1577836800000, "mdate": 1682350361868, "content": {"title": "Extracting Deep Local Features to Detect Manipulated Images of Human Faces", "abstract": "Recent developments in computer vision and machine learning have made it possible to create realistic manipulated videos of human faces, raising the issue of ensuring adequate protection against the malevolent effects unlocked by such capabilities. In this paper we propose local image features which are shared across manipulated regions as a key element for the automatic detection of manipulated face images. We also design a lightweight architecture with the correct structural biases for extracting such features and derive a multitask training scheme that consistently outperforms image class supervision alone. The trained networks achieve state-of-the-art results in the FaceForensics++ dataset using significantly reduced number of parameters and are shown to work well in detecting fully generated face images."}}
