{"id": "c5KAMqnz_c", "cdate": 1698607677467, "mdate": 1698607677467, "content": {"title": "MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning", "abstract": "3D pose transfer is a challenging generation task that aims to transfer the pose of a source geometry onto a target geometry with the target identity preserved. Many prior methods require keypoint annotations to find correspondence between the source and target. Current pose transfer methods allow end-to-end correspondence learning but require the desired final output as ground truth for supervision. Unsupervised methods have been proposed for graph convolutional models but they require ground truth correspondence between the source and target inputs. We present a novel self-supervised framework for 3D pose transfer which can be trained in unsupervised, semi-supervised, or fully supervised settings without any correspondence labels. We introduce two contrastive learning constraints in the latent space: a mesh-level loss for disentangling global patterns including pose and identity, and a point-level loss for discriminating local semantics. We demonstrate quantitatively and qualitatively that our method achieves state-of-the-art results in supervised 3D pose transfer, with comparable results in unsupervised and semi-supervised settings. Our method is also generalisable to unseen human and animal data with complex topologies."}}
{"id": "2D8hEbPT7f", "cdate": 1672531200000, "mdate": 1681722842373, "content": {"title": "Multivariate Probabilistic Monocular 3D Object Detection", "abstract": "In autonomous driving, monocular 3D object detection is an important but challenging task. Towards accurate monocular 3D object detection, some recent methods recover the distance of objects from the physical height and visual height of objects. Such decomposition framework can introduce explicit constraints on the distance prediction, thus improving its accuracy and robustness. However, the inaccurate physical height and visual height prediction still may exacerbate the inaccuracy of the distance prediction. In this paper, we improve the framework by multivariate probabilistic modeling. We explicitly model the joint probability distribution of the physical height and visual height. This is achieved by learning a full covariance matrix of the physical height and visual height during training, with the guide of a multivariate likelihood. Such explicit joint probability distribution modeling not only leads to robust distance prediction when both the predicted physical height and visual height are inaccurate, but also brings learned covariance matrices with expected behaviors. The experimental results on the challenging Waymo Open and KITTI datasets show the effectiveness of our framework <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> ."}}
{"id": "yLpR-awXLb", "cdate": 1640995200000, "mdate": 1681722842320, "content": {"title": "Filter Pruning via Automatic Pruning Rate Search", "abstract": "Model pruning is important for deploying models on devices with limited resources. However, the searching of optimal pruned model is still a significant challenge due to the large space to be exploited. In this paper, we propose an Automatic Pruning Rate Search(APRS) method to achieve automatic pruning. We reveal the connection between the model performance and Wasserstein distance to automatic searching optimal pruning rate. To reduce the search space, we quantify the sensitivity of each filter layer by layer and reveal the connection between model performance and Wasserstein distance. We introduce an end-to-end optimization method called Pareto plane to automatically search for the pruning rate to fit the overall size of the model. APRS can obtain more compact and efficient pruning models. To verify the effectiveness of our method, we conduct extensive experiments on ResNet, VGG and DenseNet, and the results show that our method outperforms the state-of-the-art methods under different parameter settings."}}
{"id": "dnis8y5S04D", "cdate": 1640995200000, "mdate": 1681722842420, "content": {"title": "Semi-Supervised Object Detection with Object-wise Contrastive Learning and Regression Uncertainty", "abstract": ""}}
{"id": "W1vwnjPtLT", "cdate": 1640995200000, "mdate": 1672767380819, "content": {"title": "Semi-Supervised Object Detection with Object-wise Contrastive Learning and Regression Uncertainty", "abstract": ""}}
{"id": "R5AkcGaFBA", "cdate": 1640995200000, "mdate": 1681698246608, "content": {"title": "LSMD-Net: LiDAR-Stereo Fusion with Mixture Density Network for Depth Sensing", "abstract": "Depth sensing is critical to many computer vision applications but remains challenge to generate accurate dense information with single type sensor. The stereo camera sensor can provide dense depth prediction but underperforms in texture-less, repetitive and occlusion areas while the LiDAR sensor can generate accurate measurements but results in sparse map. In this paper, we advocate to fuse LiDAR and stereo camera for accurate dense depth sensing. We consider the fusion of multiple sensors as a multimodal prediction problem. We propose a novel end-to-end learning framework, dubbed as LSMD-Net to faithfully generate dense depth. The proposed method has dual-branch disparity predictor and predicts a bimodal Laplacian distribution over disparity at each pixel. This distribution has two modes which captures the information from two branches. Predictions from the branch with higher confidence is selected as the final disparity result at each specific pixel. Our fusion method can be applied for different type of LiDARs. Besides the existing dataset captured by conventional spinning LiDAR, we build a multiple sensor system with a non-repeating scanning LiDAR and a stereo camera and construct a depth prediction dataset with this system. Evaluations on both KITTI datasets and our home-made dataset demonstrate the superiority of our proposed method in terms of accuracy and computation time."}}
{"id": "_MqvJHIHMqI", "cdate": 1609459200000, "mdate": 1623572250302, "content": {"title": "Geometry-based Distance Decomposition for Monocular 3D Object Detection", "abstract": "Monocular 3D object detection is of great significance for autonomous driving but remains challenging. The core challenge is to predict the distance of objects in the absence of explicit depth information. Unlike regressing the distance as a single variable in most existing methods, we propose a novel geometry-based distance decomposition to recover the distance by its factors. The decomposition factors the distance of objects into the most representative and stable variables, i.e. the physical height and the projected visual height in the image plane. Moreover, the decomposition maintains the self-consistency between the two heights, leading to robust distance prediction when both predicted heights are inaccurate. The decomposition also enables us to trace the causes of the distance uncertainty for different scenarios. Such decomposition makes the distance prediction interpretable, accurate, and robust. Our method directly predicts 3D bounding boxes from RGB images with a compact architecture, making the training and inference simple and efficient. The experimental results show that our method achieves the state-of-the-art performance on the monocular 3D Object Detection and Birds Eye View tasks of the KITTI dataset, and can generalize to images with different camera intrinsics."}}
{"id": "X_dpdBnAdk-", "cdate": 1609459200000, "mdate": 1623572250217, "content": {"title": "Learning Feature Aggregation for Deep 3D Morphable Models", "abstract": "3D morphable models are widely used for the shape representation of an object class in computer vision and graphics applications. In this work, we focus on deep 3D morphable models that directly apply deep learning on 3D mesh data with a hierarchical structure to capture information at multiple scales. While great efforts have been made to design the convolution operator, how to best aggregate vertex features across hierarchical levels deserves further attention. In contrast to resorting to mesh decimation, we propose an attention based module to learn mapping matrices for better feature aggregation across hierarchical levels. Specifically, the mapping matrices are generated by a compatibility function of the keys and queries. The keys and queries are trainable variables, learned by optimizing the target objective, and shared by all data samples of the same object class. Our proposed module can be used as a train-only drop-in replacement for the feature aggregation in existing architectures for both downsampling and upsampling. Our experiments show that through the end-to-end training of the mapping matrices, we achieve state-of-the-art results on a variety of 3D shape datasets in comparison to existing morphable models."}}
{"id": "RYKQqrxvFfc", "cdate": 1609459200000, "mdate": 1672767380817, "content": {"title": "Learning Feature Aggregation for Deep 3D Morphable Models", "abstract": ""}}
{"id": "M6cAOk92gN", "cdate": 1609459200000, "mdate": 1666881022080, "content": {"title": "SeCGAN: Parallel Conditional Generative Adversarial Networks for Face Editing via Semantic Consistency", "abstract": "Semantically guided conditional Generative Adversarial Networks (cGANs) have become a popular approach for face editing in recent years. However, most existing methods introduce semantic masks as direct conditional inputs to the generator and often require the target masks to perform the corresponding translation in the RGB space. We propose SeCGAN, a novel label-guided cGAN for editing face images utilising semantic information without the need to specify target semantic masks. During training, SeCGAN has two branches of generators and discriminators operating in parallel, with one trained to translate RGB images and the other for semantic masks. To bridge the two branches in a mutually beneficial manner, we introduce a semantic consistency loss which constrains both branches to have consistent semantic outputs. Whilst both branches are required during training, the RGB branch is our primary network and the semantic branch is not needed for inference. Our results on CelebA and CelebA-HQ demonstrate that our approach is able to generate facial images with more accurate attributes, outperforming competitive baselines in terms of Target Attribute Recognition Rate whilst maintaining quality metrics such as self-supervised Fr\\'{e}chet Inception Distance and Inception Score."}}
