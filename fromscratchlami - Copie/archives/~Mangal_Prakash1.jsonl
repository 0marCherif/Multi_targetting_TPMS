{"id": "DfMqlB0PXjM", "cdate": 1632875549637, "mdate": null, "content": {"title": "Interpretable Unsupervised Diversity Denoising and Artefact Removal", "abstract": "Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real world applications, imaging noise and artefacts are typically hard to model, leading to unsatisfactory performance of existing unsupervised approaches. This work presents an interpretable approach for unsupervised and diverse image restoration. To this end, we introduce a capable architecture called Hierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We show that HDN learns an interpretable multi-scale representation of artefacts  and we leverage this interpretability to remove imaging artefacts commonly occurring in microscopy data. Our method achieves state-of-the-art results on twelve benchmark image denoising datasets while providing access to a whole distribution of sensibly restored solutions.\nAdditionally, we demonstrate on three real microscopy datasets that HDN removes artefacts without supervision, being the first method capable of doing so while generating multiple plausible restorations all consistent with the given corrupted image."}}
{"id": "vwL0u0d_b7X", "cdate": 1609459200000, "mdate": null, "content": {"title": "Removing Pixel Noises and Spatial Artifacts with Generative Diversity Denoising Methods", "abstract": "Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real world applications, imaging noise and artefacts are typically hard to model, leading to unsatisfactory performance of existing unsupervised approaches. This work presents an interpretable approach for unsupervised and diverse image restoration. To this end, we introduce a capable architecture called Hierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We show that HDN learns an interpretable multi-scale representation of artefacts and we leverage this interpretability to remove imaging artefacts commonly occurring in microscopy data. Our method achieves state-of-the-art results on twelve benchmark image denoising datasets while providing access to a whole distribution of sensibly restored solutions. Additionally, we demonstrate on three real microscopy datasets that HDN removes artefacts without supervision, being the first method capable of doing so while generating multiple plausible restorations all consistent with the given corrupted image."}}
{"id": "agHLCOBM5jP", "cdate": 1601308364771, "mdate": null, "content": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions."}}
{"id": "UWm7zRhPoMX", "cdate": 1595260908819, "mdate": null, "content": {"title": "DenoiSeg: Joint Denoising and Segmentation", "abstract": "Microscopy image analysis often requires the segmentation of objects, but training data for this task is typically scarce and hard to obtain. Here we propose DenoiSeg, a new method that can be trained end-to-end on only a few annotated ground truth segmentations. We achieve this by extending Noise2Void, a self-supervised denoising scheme that can be trained on noisy images alone, to also predict dense 3-class segmentations. The reason for the success of our method is that segmentation can profit from denoising, especially when performed jointly within the same network. The network becomes a denoising expert by seeing all available raw data, while co-learning to segment, even if only a few segmentation labels are available. This hypothesis is additionally fueled by our observation that the best segmentation results on high quality (very low noise) raw data are obtained when moderate amounts of synthetic noise are added. This renders the denoising-task non-trivial and unleashes the desired co-learning effect. We believe that DenoiSeg offers a viable way to circumvent the tremendous hunger for high quality training data and effectively enables learning of dense segmentations when only very limited amounts of segmentation labels are available."}}
{"id": "jKknpc6ITg", "cdate": 1580725044518, "mdate": null, "content": {"title": "A Primal-Dual Solver for Large-Scale Tracking-by-Assignment", "abstract": "We propose a fast approximate solver for the combinatorial problem known as tracking-byassignment, which we apply to cell tracking. The latter plays a key role in discovery in many life sciences, especially in cell and developmental biology. So far, in the most general setting this problem was addressed by off-theshelf solvers like Gurobi, whose run time and memory requirements rapidly grow with the size of the input. In contrast, for our method this growth is nearly linear. Our contribution consists of a new (1) decomposable compact representation of the problem; (2) dual block-coordinate ascent method for optimizing the decompositionbased dual; and (3) primal heuristics that reconstructs a feasible integer solution based on the dual information. Compared to solving the problem with Gurobi, we observe an up to 60 times speed-up, while reducing the memory footprint significantly. We demonstrate the efficacy of our method on real-world tracking problems."}}
{"id": "UXZ_oJrO4GU", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distributed Stopping Criterion for Consensus in the Presence of Delays", "abstract": "Linear consensus protocol is an iterative distributed algorithm with asymptotic convergence guarantees. This paper develops and analyzes an algorithm for agents running linear consensus iterations to detect convergence to consensus within a specified error tolerance in a distributed manner. The distributed stopping criterion allows for time-varying bounded delays in information transmission and reception between agents. The algorithm relies on distributively determining the maximum and minimum values held by the agents. This paper further develops an algorithm for average consensus that utilizes a distributive stopping criterion, based on maximum and minimum consensus, where no centralized coordination is needed on how each agent weights its neighbor's values. Here, the doubly stochastic assumption on the weight matrix is relaxed and only column stochasticity is needed. The effectiveness of the algorithms is demonstrated by simulations and a comparison with prior work in the literature. Moreover, the demonstration of the proposed algorithms on an experimental test bed of Raspberry-Pi agents communicating wirelessly validates its applicability and utility."}}
{"id": "ISUZQm8bijR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Probabilistic Noise2Void: Unsupervised Content-Aware Denoising", "abstract": "Today, Convolutional Neural Networks (CNNs) are the leading method for image denoising. They are traditionally trained on pairs of images, which are often hard to obtain for practical applications. This motivates self-supervised training methods, such as Noise2Void (N2V) that operate on single noisy images. Self-supervised methods are, unfortunately, not competitive with models trained on image pairs. Here, we present Probabilistic Noise2Void (PN2V), a method to train CNNs to predict per-pixel intensity distributions. Combining these with a suitable description of the noise, we obtain a complete probabilistic model for the noisy observations and true signal in every pixel. We evaluate PN2V on publicly available microscopy datasets, under a broad range of noise regimes, and achieve competitive results with respect to supervised state-of-the-art methods."}}
{"id": "26FF5lzi4U0", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fully Unsupervised Probabilistic Noise2Void.", "abstract": "Image denoising is the first step in many biomedical image analysis pipelines and Deep Learning (DL) based methods are currently best performing. A new category of DL methods such as Noise2Void or Noise2Self can be used fully unsupervised, requiring nothing but the noisy data. However, this comes at the price of reduced reconstruction quality. The recently proposed Probabilistic Noise2Void (PN2V) improves results, but requires an additional noise model for which calibration data needs to be acquired. Here, we present improvements to PN2V that (i) replace histogram based noise models by parametric noise models, and (ii) show how suitable noise models can be created even in the absence of calibration data. This is a major step since it actually renders PN2V fully unsupervised. We demonstrate that all proposed improvements are not only academic but indeed relevant."}}
{"id": "-MdbZY93HrN", "cdate": 1546300800000, "mdate": null, "content": {"title": "Leveraging Self-supervised Denoising for Image Segmentation.", "abstract": "Deep learning (DL) has arguably emerged as the method of choice for the detection and segmentation of biological structures in microscopy images. However, DL typically needs copious amounts of annotated training data that is for biomedical projects typically not available and excessively expensive to generate. Additionally, tasks become harder in the presence of noise, requiring even more high-quality training data. Hence, we propose to use denoising networks to improve the performance of other DL-based image segmentation methods. More specifically, we present ideas on how state-of-the-art self-supervised CARE networks can improve cell/nuclei segmentation in microscopy data. Using two state-of-the-art baseline methods, U-Net and StarDist, we show that our ideas consistently improve the quality of resulting segmentations, especially when only limited training data for noisy micrographs are available."}}
{"id": "zDERzr7WoJ", "cdate": 1420070400000, "mdate": null, "content": {"title": "Reconstruction of networks of cyclostationary processes", "abstract": "Many complex systems can be described by agents that can be modeled as a network of dynamically interacting cyclo-stationary processes. Such systems arise in areas like power systems and climate sciences. For many of these systems a key objective is to understand mutual influences between various subsystems without altering the natural behavior of the system. Such an objective translates to unveiling the interconnection of the topology of the network using only passive means. Most existing related works have emphasized correlation based methods where interdependencies over different time-instants can be missed. Recent work where dynamic influences are incorporated assuming stationary statistics cannot accommodate applications that arise in many areas such as power and climate sciences. In this article an algorithm based on Wiener filtering is devised for the reconstruction of interconnectivity of dynamically related cyclo-stationary processes. It is shown that all existing interdependencies are detected and spurious detection remains local. Application to a microgrid power network is shown to yield useful insights."}}
