{"id": "UZpWSDA3tZJ", "cdate": 1686324855367, "mdate": null, "content": {"title": "Towards General Single-Utensil Food Acquisition with Human-Informed Actions", "abstract": "Food acquisition with common general-purpose utensils is a necessary component of robot applications like in-home assistive feeding. Learning acquisition policies in this space is difficult in part because any model will need to contend with extensive state and actions spaces. Food is extremely diverse and generally difficult to simulate, and acquisition actions like skewers, scoops, wiggles, and twirls can be parameterized in myriad ways. However, food's visual diversity can belie a degree of physical homogeneity, and many foods allow flexibility in how they are acquired. Due to these facts, our key insight is that a small subset of actions is sufficient to acquire a wide variety of food items. In this work, we present a methodology for identifying such a subset from limited human trajectory data. We first develop an over-parameterized action space of robot acquisition trajectories that capture the variety of human food acquisition technique. By mapping human trajectories into this space and clustering, we construct a discrete set of 11 actions. We demonstrate that this set is capable of acquiring a variety of food items with $\\geq80\\%$ success rate, a rate that users have said is sufficient for in-home robot-assisted feeding. Furthermore, since this set is so small, we also show that we can use online learning to determine a sufficiently optimal action for a previously-unseen food item over the course of a single meal."}}
{"id": "49vN99s2bo5", "cdate": 1609459200000, "mdate": 1667375252333, "content": {"title": "VVS: Action Recognition With Virtual View Synthesis", "abstract": "Action recognition research is usually in the single-view setting. But human action is not single-view based in many cases. A lot of simple action is composed of both body movements from the third-person view, and vision guidance from the first-person view. Therefore, linking two viewpoints of data is critical for action recognition algorithms. Currently, the scale of aligned multi-view dataset is small, which limits the advancement in this direction of research. To alleviate the data limitation, we present the novel Virtual View Synthesis (VVS) module. Instead of training and testing on small scale multi-view data, VVS is first pre-trained on multi-view data to generalize the multi-view \u201csupervisory attention\u201d. Then it is incorporated into single-view action recognition model to transfer the ability of how to better observe the existing view based on experience from another view. Extensive experiments demonstrate that VVS can improve strong baselines on several single-view action recognition benchmarks."}}
{"id": "Cp6AoEVVal-", "cdate": 1590990422842, "mdate": null, "content": {"title": "Benchmarking Object-Centric Manipulation Using a Simulated Environment", "abstract": "Benchmark is of vital importance to evaluate the effectiveness of an algorithm. In recent years, the computer vision community benefits from using standard datasets and benchmarks. This trend is becoming more popular in the robotics community as well.\nHowever, if we would like to bring the same comparing approach to the robotics community, there are some challenges we have to face. Is there an alternative way to benchmark the performance of robots? We think using a simulated environment is the way out.\nHumans study the physical world from the interaction with objects, and we believe robots should study in the same way. As building home assistant robots is our final goal, we started from building a dataset of articulated objects for robots to interact with.\nFinally, with the help of the SAPIEN robotics kit that supports URDF and ROS, researchers have the possibility to train and test the robot in the simulator, and transfer it to the real world.\nThis paper also discusses the opportunities for improvement of the original paper, and proposes the new perspectives and future works since publishing."}}
{"id": "yjKQO5J4GDX", "cdate": 1577836800000, "mdate": 1668163518478, "content": {"title": "SAPIEN: A SimulAted Part-Based Interactive ENvironment", "abstract": ""}}
{"id": "HV-a5aJ923", "cdate": 1577836800000, "mdate": 1666155543843, "content": {"title": "SAPIEN: A SimulAted Part-based Interactive ENvironment", "abstract": "Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment."}}
{"id": "YXxSLyriDbj", "cdate": 1546300800000, "mdate": 1654834513965, "content": {"title": "S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered Scenes", "abstract": "Grasping is among the most fundamental and long-lasting problems in robotics study. This paper studies the problem of 6-DoF(degree of freedom) grasping by a parallel gripper in a cluttered scene captured using a commodity depth sensor from a single viewpoint. We address the problem in a learning-based framework. At the high level, we rely on a single-shot grasp proposal network, trained with synthetic data and tested in real-world scenarios. Our single-shot neural network architecture can predict amodal grasp proposal efficiently and effectively. Our training data synthesis pipeline can generate scenes of complex object configuration and leverage an innovative gripper contact model to create dense and high-quality grasp annotations. Experiments in synthetic and real environments have demonstrated that the proposed approach can outperform state-of-the-arts by a large margin."}}
{"id": "UwrmP7CoWZ", "cdate": 1546300800000, "mdate": 1666237142621, "content": {"title": "CrowdPose: Efficient Crowded Scenes Pose Estimation and a New Benchmark", "abstract": "Multi-person pose estimation is fundamental to many computer vision tasks and has made significant progress in recent years. However, few previous methods explored the problem of pose estimation in crowded scenes while it remains challenging and inevitable in many scenarios. Moreover, current benchmarks cannot provide an appropriate evaluation for such cases. In this paper, we propose a novel and efficient method to tackle the problem of pose estimation in the crowd and a new dataset to better evaluate algorithms. Our model consists of two key components: joint-candidate single person pose estimation (SPPE) and global maximum joints association. With multi-peak prediction for each joint and global association using the graph model, our method is robust to inevitable interference in crowded scenes and very efficient in inference. The proposed method surpasses the state-of-the-art methods on CrowdPose dataset by 5.2 mAP and results on MSCOCO dataset demonstrate the generalization ability of our method."}}
{"id": "SxdkY4ehBDL", "cdate": 1546300800000, "mdate": 1654834513965, "content": {"title": "S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered Scenes", "abstract": "Grasping is among the most fundamental and long-lasting problems in robotics study. This paper studies the problem of 6-DoF(degree of freedom) grasping by a parallel gripper in a cluttered scene ca..."}}
