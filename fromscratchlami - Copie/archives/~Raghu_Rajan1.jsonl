{"id": "qQghx7f6Qp", "cdate": 1661361914738, "mdate": 1661361914738, "content": {"title": "Automated Reinforcement Learning (AutoRL): A Survey and Open Problems", "abstract": "The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems, while also limits its full potential. In many other areas of machine learning, AutoML has shown it is possible to automate such design choices and has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey we seek to unify the field of AutoRL, we provide a common taxonomy, discuss each area in detail and pose open problems which would be of interest to researchers going forward."}}
{"id": "VtqyY2dvE6h", "cdate": 1623074867403, "mdate": null, "content": {"title": "MDP Playground: A Design and Debug Testbed for Reinforcement Learning", "abstract": "We present \\emph{MDP Playground}, an efficient testbed for Reinforcement Learning (RL) agents with \\textit{orthogonal} dimensions that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in generated environments. We consider and allow control over a wide variety of dimensions, including \\textit{delayed rewards}, \\textit{rewardable sequences}, \\textit{density of rewards}, \\textit{stochasticity}, \\textit{image representations}, \\textit{irrelevant features}, \\textit{time unit}, \\textit{action range} and more. We define a parameterised collection of fast-to-run toy environments in \\textit{OpenAI Gym} by varying these dimensions and propose to use these for the initial design and development of agents. We also provide wrappers that inject these dimensions into complex environments from \\textit{Atari} and \\textit{Mujoco} to allow for evaluating agent robustness. We further provide various example use-cases and instructions on how to use \\textit{MDP Playground} to design and debug agents. We believe that \\textit{MDP Playground} is a valuable testbed for researchers designing new, adaptive and intelligent RL agents and those wanting to unit test their agents."}}
{"id": "axNDkxU9-6z", "cdate": 1601308143237, "mdate": null, "content": {"title": "MDP Playground: Controlling Orthogonal Dimensions of Hardness in Toy Environments", "abstract": "We present MDP Playground, an efficient benchmark for Reinforcement Learning (RL) algorithms with various dimensions of hardness that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. We consider and allow control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action max. While it is very time consuming to run RL algorithms on standard benchmarks, we define a parameterised collection of fast-to-run toy benchmarks in OpenAI Gym by varying these dimensions. Despite their toy nature and low compute requirements, we show that these benchmarks present substantial challenges to current RL algorithms. Furthermore, since we can generate environments with a desired value for each of the dimensions, in addition to having fine-grained control over the environments' hardness, we also have the ground truth available for evaluating algorithms. Finally, we evaluate the kinds of transfer for these dimensions that may be expected from our benchmarks to more complex benchmarks. We believe that MDP Playground is a valuable testbed for researchers designing new, adaptive and intelligent RL algorithms and those wanting to unit test their algorithms.\n"}}
