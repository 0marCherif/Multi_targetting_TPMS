{"id": "jURqQx6VtB", "cdate": 1698988399009, "mdate": 1698988399009, "content": {"title": "Learning Prototype Classifiers for Long-Tailed Recognition", "abstract": "The problem of long-tailed recognition (LTR) has\nreceived attention in recent years due to the fun-\ndamental power-law distribution of objects in the\nreal-world. Most recent works in LTR use softmax\nclassifiers that are biased in that they correlate\nclassifier norm with the amount of training data for\na given class. In this work, we show that learning\nprototype classifiers addresses the biased softmax\nproblem in LTR. Prototype classifiers can deliver\npromising results simply using Nearest-Class-\nMean (NCM), a special case where prototypes\nare empirical centroids. We go one step further\nand propose to jointly learn prototypes by using\ndistances to prototypes in representation space\nas the logit scores for classification. Further, we\ntheoretically analyze the properties of Euclidean\ndistance based prototype classifiers that lead\nto stable gradient-based optimization which is\nrobust to outliers. To enable independent distance\nscales along each channel, we enhance Proto-\ntype classifiers by learning channel-dependent\ntemperature parameters. Our analysis shows that\nprototypes learned by Prototype classifiers are\nbetter separated than empirical centroids. Results\non four LTR benchmarks show that Prototype\nclassifier outperforms or is comparable to state-\nof-the-art methods. Our code is made available at\nhttps://github.com/saurabhsharma1993/prototype-\nclassifier-ltr."}}
{"id": "OM0FsMD6NG", "cdate": 1668627692646, "mdate": 1668627692646, "content": {"title": "f-vaegan-d2: A feature generating framework for any-shot learning", "abstract": "When labeled training data is scarce, a promising data augmentation approach is to generate visual features of unknown classes using their attributes. To learn the class conditional distribution of CNN features, these models rely on pairs of image features and class attributes. Hence, they can not make use of the abundance of unlabeled data samples. In this paper, we tackle any-shot learning problems ie zero-shot and few-shot, in a unified feature generating framework that operates in both inductive and transductive learning settings. We develop a conditional generative model that combines the strength of VAE and GANs and in addition, via an unconditional discriminator, learns the marginal feature distribution of unlabeled images. We empirically show that our model learns highly discriminative CNN features for five datasets, ie CUB, SUN, AWA and ImageNet, and establish a new state-of-the-art in any-shot learning, ie inductive and transductive (generalized) zero-and few-shot learning settings. We also demonstrate that our learned features are interpretable: we visualize them by inverting them back to the pixel space and we explain them by generating textual arguments of why they are associated with a certain label."}}
{"id": "ByaNEZdnx2O", "cdate": 1663850524973, "mdate": null, "content": {"title": "Learned Nearest-Class-Mean for Biased Representations in Long-Tailed Recognition", "abstract": "The problem of long-tailed recognition (LTR) has received attention in recent years due to the fundamental power-law distribution of objects in the real-world. While classifier bias in LTR has been addressed by many works, representation bias has not yet been researched. At the same time, most recent works use softmax classifiers that are unable to cope with representation bias. In this work, we address these shortcomings by firstly making the key observation that intra-class variance in representation space is negatively correlated to class frequency, leading to biased representations; our analysis reveals that high tail variance is due to spurious correlations learned by deep models. Secondly, to counter representation bias, we propose the Learned Nearest-Class-Mean (NCM), which overcomes uncertainty in empirical centroid estimates and jointly learns centroids minimizing average class-distance normalized variance. Further, we adapt the logit adjustment technique in the NCM framework to achieve higher tail class margin. Our Learned NCM with Logit Adjustment achieves 6\\% gain over state-of-the-art in tail accuracy on the benchmark CIFAR100-LT and ImageNet-LT datasets. "}}
{"id": "mjUrg0uKpQ", "cdate": 1652737352151, "mdate": null, "content": {"title": "I2DFormer: Learning Image to Document Attention for Zero-Shot Image Classification", "abstract": "Despite the tremendous progress in zero-shot learning (ZSL), the majority of existing methods still rely on human-annotated attributes, which are difficult to annotate and scale. An unsupervised alternative is to represent each class using the word embedding associated with its semantic class name. However, word embeddings extracted from pre-trained language models do not necessarily capture visual similarities, resulting in poor zero-shot performance.  In this work, we argue that online textual documents e.g., Wikipedia, contain rich visual descriptions about object classes, therefore can be used as powerful unsupervised side information for ZSL. To this end, we propose I2DFormer, a novel transformer-based ZSL framework that jointly learns to encode images and documents by aligning both modalities in a shared embedding space. In order to distill discriminative visual words from noisy documents, we introduce a new cross-modal attention module that learns fine-grained interactions between image patches and document words. Consequently, our I2DFormer not only learns highly discriminative document embeddings that capture visual similarities but also gains the ability to localize visually relevant words in image regions. Quantitatively, we demonstrate that our I2DFormer significantly outperforms previous unsupervised semantic embeddings under both zero-shot and generalized zero-shot learning settings on three public datasets. Qualitatively, we show that our method leads to highly interpretable results where document words can be grounded in the image regions. "}}
{"id": "aLRgEIeAg5N", "cdate": 1620212802750, "mdate": null, "content": {"title": "Open World Compositional Zero-Shot Learning", "abstract": "Compositional Zero-Shot learning (CZSL) requires to recognize state-object compositions unseen during training. In this work, instead of assuming prior knowledge about the unseen compositions, we operate in the open world setting, where the search space includes a large number of unseen compositions some of which might be unfeasible. In this setting, we start from the cosine similarity between visual features and compositional embeddings. After estimating the feasibility score of each composition, we use these scores to either directly mask the output space or as a margin for the cosine similarity between visual features and compositional embeddings during training. Our experiments on two standard CZSL benchmarks show that all the methods suffer severe performance degradation when applied in the open world setting. While our simple CZSL model achieves state-of-the-art performances in the closed world scenario, our feasibility scores boost the performance of our approach in the open world setting, clearly outperforming the previous state of the art."}}
{"id": "rJxm4hdH3r", "cdate": 1574435882599, "mdate": null, "content": {"title": "Semantic Projection Network for Zero- and Few-Label Semantic Segmentation", "abstract": "Semantic segmentation is one of the most fundamental problems in computer vision. As pixel-level labelling in this context is particularly expensive, there have been several attempts to reduce the annotation effort, e.g. by learning from image level labels and bounding box annotations. In this paper we take this one step further and propose zero- and few-label learning for semantic segmentation as a new task and propose a benchmark on the challenging COCO-Stuff and PASCAL VOC12 datasets. In the task of zero-label semantic image segmentation no labeled sample of that class was present during training whereas in few-label semantic segmentation only a few labeled samples were present. Solving this task requires transferring the knowledge from previously seen classes to novel classes. Our proposed semantic projection network (SPNet) achieves this by incorporating class-level semantic information into any network designed for semantic segmentation, and is trained in an end-to-end manner. Our model is effective in segmenting novel classes, i.e. alleviating expensive dense annotations, but also in adapting to novel classes without forgetting its prior knowledge, i.e. generalized zero- and few-label semantic segmentation.\n"}}
{"id": "H1l7AkrFPS", "cdate": 1569439690733, "mdate": null, "content": {"title": "Spatial Information is Overrated for Image Classification", "abstract": "Intuitively, image classification should profit from using spatial information. Recent work, however, suggests that this might be overrated in standard CNNs. In this paper, we are pushing the envelope and aim to further investigate the reliance on and necessity of spatial information. We propose and analyze three methods, namely Shuffle Conv, GAP+FC and 1x1 Conv, that destroy spatial information during both training and testing phases. We extensively evaluate these methods on several object recognition datasets (CIFAR100, Small-ImageNet, ImageNet) with a wide range of CNN architectures (VGG16, ResNet50, ResNet152, MobileNet, SqueezeNet). Interestingly, we consistently observe that spatial information can be completely deleted from a significant number of layers with no or only small performance drops."}}
{"id": "SJVtTpZuZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Feature Generating Networks for Zero-Shot Learning", "abstract": "Suffering from the extreme training data imbalance between seen and unseen classes, most of existing state-of-the-art approaches fail to achieve satisfactory results for the challenging generalized zero-shot learning task. To circumvent the need for labeled examples of unseen classes, we propose a novel generative adversarial network(GAN) that synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic descriptor of a class to a class-conditional feature distribution. Our proposed approach, pairing a Wasserstein GAN with a classification loss, is able to generate sufficiently discriminative CNN features to train softmax classifiers or any multimodal embedding method. Our experimental results demonstrate a significant boost in accuracy over the state of the art on five challenging datasets -- CUB, FLO, SUN, AWA and ImageNet -- in both the zero-shot learning and generalized zero-shot learning settings."}}
{"id": "S1EcbRW_-B", "cdate": 1483228800000, "mdate": null, "content": {"title": "Zero-Shot Learning - The Good, the Bad and the Ugly", "abstract": "Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it."}}
{"id": "HkbVEJfdZS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Latent Embeddings for Zero-Shot Classification", "abstract": "We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps."}}
