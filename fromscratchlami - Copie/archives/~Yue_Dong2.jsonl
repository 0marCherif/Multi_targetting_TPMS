{"id": "fMWpo7_HJT4", "cdate": 1609459200000, "mdate": 1634590535065, "content": {"title": "Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization", "abstract": "State-of-the-art abstractive summarization systems often generate \\emph{hallucinations}; i.e., content that is not directly inferable from the source text. Despite being assumed incorrect, we find that much hallucinated content is factual, namely consistent with world knowledge. These factual hallucinations can be beneficial in a summary by providing useful background information. In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities. Our method utilizes an entity's prior and posterior probabilities according to pre-trained and finetuned masked language models, respectively. Empirical results suggest that our approach vastly outperforms two baselines %in both accuracy and F1 scores and strongly correlates with human judgments. % on factuality classification tasks. Furthermore, we show that our detector, when used as a reward signal in an off-line reinforcement learning (RL) algorithm, significantly improves the factuality of summaries while maintaining the level of abstractiveness."}}
{"id": "WN1ZjWEMC88", "cdate": 1609459200000, "mdate": 1631128870227, "content": {"title": "On-the-Fly Attention Modulation for Neural Generation", "abstract": "Yue Dong, Chandra Bhagavatula, Ximing Lu, Jena D. Hwang, Antoine Bosselut, Jackie Chi Kit Cheung, Yejin Choi. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021."}}
{"id": "Oc9qQUoeQjz", "cdate": 1609459200000, "mdate": 1634590535066, "content": {"title": "Discourse-Aware Unsupervised Summarization for Long Scientific Documents", "abstract": "Yue Dong, Andrei Mircea, Jackie Chi Kit Cheung. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."}}
{"id": "0I_1Uh2ybrs", "cdate": 1609459200000, "mdate": 1634590535065, "content": {"title": "Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents", "abstract": "Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021."}}
{"id": "pfC7SE9BYvj", "cdate": 1577836800000, "mdate": 1631218821802, "content": {"title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles", "abstract": "Yao Lu, Yue Dong, Laurent Charlin. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "kLJZUqUjM9D", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi-Fact Correction in Abstractive Text Summarization", "abstract": "Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation."}}
{"id": "aV4a8jaWPam", "cdate": 1577836800000, "mdate": 1634590535065, "content": {"title": "Factual Error Correction for Abstractive Summarization Models", "abstract": "Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by an error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging."}}
{"id": "QgAn9H64EqM", "cdate": 1577836800000, "mdate": 1634590535070, "content": {"title": "Factual Error Correction for Abstractive Summarization Models", "abstract": "Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi Kit Cheung. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "PNdCW3d9j60", "cdate": 1577836800000, "mdate": 1631218821819, "content": {"title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles", "abstract": "Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results---using several state-of-the-art models trained on the Multi-XScience dataset---reveal that Multi-XScience is well suited for abstractive models."}}
{"id": "G5Ww44fFbtx", "cdate": 1577836800000, "mdate": 1634590535064, "content": {"title": "HipoRank: Incorporating Hierarchical and Positional Information into Graph-based Unsupervised Long Document Extractive Summarization", "abstract": "We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles."}}
