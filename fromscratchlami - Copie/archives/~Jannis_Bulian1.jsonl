{"id": "c254Nz0OoM", "cdate": 1640995200000, "mdate": 1665700863131, "content": {"title": "Scaling Up Models and Data with t5x and seqio", "abstract": "Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: $\\texttt{t5x}$ simplifies the process of building and training large language models at scale while maintaining ease of use, and $\\texttt{seqio}$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data. Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures. $\\texttt{t5x}$ and $\\texttt{seqio}$ are open source and available at https://github.com/google-research/t5x and https://github.com/google/seqio, respectively."}}
{"id": "bdiMyqWj2g", "cdate": 1640995200000, "mdate": 1681592590584, "content": {"title": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation", "abstract": ""}}
{"id": "5nBNnI9pXYn", "cdate": 1640995200000, "mdate": 1681592590609, "content": {"title": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation", "abstract": ""}}
{"id": "o1NwvL5P3JQ", "cdate": 1609459200000, "mdate": null, "content": {"title": "Fool Me Twice: Entailment from Wikipedia Gamification", "abstract": "We release FoolMeTwice (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using \"shortcuts\" compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players \"pay\" to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks. We open source the dataset and the game code."}}
{"id": "PjYm7dsqm1", "cdate": 1609459200000, "mdate": 1682674184765, "content": {"title": "Fool Me Twice: Entailment from Wikipedia Gamification", "abstract": "Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin B\u00f6rschinger, Jordan Boyd-Graber. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "BRzl2zFA9j", "cdate": 1609459200000, "mdate": 1682674184649, "content": {"title": "Fool Me Twice: Entailment from Wikipedia Gamification", "abstract": "Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin B\u00f6rschinger, Jordan Boyd-Graber. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "8vtSOa1DqJS", "cdate": 1577836800000, "mdate": 1682674184873, "content": {"title": "CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims", "abstract": "We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community."}}
{"id": "BJeypMU5wE", "cdate": 1552732855144, "mdate": null, "content": {"title": "Multi-agent query reformulation: Challenges and the role of diversity", "abstract": "We investigate methods to efficiently learn diverse strategies in reinforcement learning for a generative structured prediction problem: query reformulation. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as\nan ensemble of agents trained on the full data. We evaluate on the tasks of document retrieval and question answering. The\nimproved performance seems due to the increased diversity of reformulation strategies. This suggests that multi-agent, hierarchical approaches might play an important role in structured prediction tasks of this kind. However, we also find that it is not obvious how to characterize diversity in this context, and a first attempt based on clustering did not produce good results. Furthermore, reinforcement learning for the reformulation task is hard in high-performance regimes. At best, it only marginally improves over the state of the art, which highlights the complexity of training models in this framework for end-to-end language understanding problems."}}
{"id": "skzHeIiIryK", "cdate": 1546300800000, "mdate": 1682674184965, "content": {"title": "Meta Answering for Machine Reading", "abstract": "We investigate a framework for machine reading, inspired by real world information-seeking problems, where a meta question answering system interacts with a black box environment. The environment encapsulates a competitive machine reader based on BERT, providing candidate answers to questions, and possibly some context. To validate the realism of our formulation, we ask humans to play the role of a meta-answerer. With just a small snippet of text around an answer, humans can outperform the machine reader, improving recall. Similarly, a simple machine meta-answerer outperforms the environment, improving both precision and recall on the Natural Questions dataset. The system relies on joint training of answer scoring and the selection of conditioning information."}}
{"id": "MsZX9mzOqHh", "cdate": 1546300800000, "mdate": 1682674184985, "content": {"title": "Multi-agent query reformulation: Challenges and the role of diversity", "abstract": "We investigate methods to efficiently learn diverse strategies in reinforcement learning for a generative structured prediction problem: query reformulation. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data. We evaluate on the tasks of document retrieval and question answering. The improved performance seems due to the increased diversity of reformulation strategies. This suggests that multi-agent, hierarchical approaches might play an important role in structured prediction tasks of this kind. However, we also find that it is not obvious how to characterize diversity in this context, and a first attempt based on clustering did not produce good results. Furthermore, reinforcement learning for the reformulation task is hard in high-performance regimes. At best, it only marginally improves over the state of the art, which highlights the complexity of training models in this framework for end-to-end language understanding problems."}}
