{"id": "NQ3kRQirVw", "cdate": 1677628800000, "mdate": 1683880914630, "content": {"title": "Towards understanding quality challenges of the federated learning for neural networks: a first look from the lens of robustness", "abstract": "Federated learning (FL) is a distributed learning paradigm that preserves users\u2019 data privacy while leveraging the entire dataset of all participants. In FL, multiple models are trained independently on the clients and aggregated centrally to update a global model in an iterative process. Although this approach is excellent at preserving privacy, FL still suffers from quality issues such as attacks or byzantine faults. Recent attempts have been made to address such quality challenges on the robust aggregation techniques for FL. However, the effectiveness of state-of-the-art (SOTA) robust FL techniques is still unclear and lacks a comprehensive study. Therefore, to better understand the current quality status and challenges of these SOTA FL techniques in the presence of attacks and faults, we perform a large-scale empirical study to investigate the SOTA FL\u2019s quality from multiple angles of attacks, simulated faults (via mutation operators), and aggregation (defense) methods. In particular, we study FL\u2019s performance on the image classification tasks and use Deep Neural Networks as our model type. Furthermore, we perform our study on two generic image datasets and one real-world federated medical image dataset. We also systematically investigate the effect of the proportion of affected clients and the dataset distribution factors on the robustness of FL. After a large-scale analysis with 496 configurations, we find that most mutators on each user have a negligible effect on the final model in the generic datasets, and only one of them is effective in the medical dataset. Furthermore, we show that model poisoning attacks are more effective than data poisoning attacks. Moreover, choosing the most robust FL aggregator depends on the attacks and datasets. Finally, we illustrate that a simple ensemble of aggregators achieves a more robust solution than any single aggregator and is the best choice in 75% of the cases. The data that support the findings of this study are available in our repository at https://github.com/aminesi/federated ."}}
{"id": "WYEpJwB9sp", "cdate": 1675090080688, "mdate": 1675090080688, "content": {"title": "DeepMemory: Model-based Memorization Analysis of Deep Neural Language Models", "abstract": "The neural network model is having a significant\nimpact on many real-world applications. Unfortunately, the increasing popularity and complexity of these models also amplifies\ntheir security and privacy challenges, with privacy leakage from\ntraining data being one of the most prominent issues. In this context, prior studies proposed to analyze the abstraction behavior of\nneural network models, e.g., RNN, to understand their robustness.\nHowever, the existing research rarely addresses privacy breaches\ncaused by memorization in neural language models. To fill this\ngap, we propose a novel approach, DeepMemory, that analyzes\nmemorization behavior for a neural language model. We first\nconstruct a memorization-analysis-oriented model, taking both\ntraining data and a neural language model as input. We then\nbuild a semantic first-order Markov model to bind the constructed memorization-analysis-oriented model to the training\ndata to analyze memorization distribution. Finally, we apply\nour approach to address data leakage issues associated with\nmemorization and to assist in dememorization. We evaluate our\napproach on one of the most popular neural language models,\nthe LSTM-based language model, with three public datasets,\nnamely, WikiText-103, WMT2017, and IWSLT2016. We find that\nsentences in the studied datasets with low perplexity are more\nlikely to be memorized. Our approach achieves an average AUC\nof 0.73 in automatically identifying data leakage issues during\nassessment. We also show that with the assistance of DeepMemory,\ndata breaches due to memorization of neural language models\ncan be successfully mitigated by mutating training data without\nreducing the performance of neural language models."}}
{"id": "HEUHkBIarm3", "cdate": 1675089986352, "mdate": 1675089986352, "content": {"title": "Neural hidden markov model for machine translation", "abstract": "Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a translation, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates NMT while replacing the attention component. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the forward-backward algorithm. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 German\u2194 English and Chinese\u2192 English translation tasks.\n"}}
{"id": "Cy7zMpDX-Z", "cdate": 1672531200000, "mdate": 1683880914674, "content": {"title": "Neural Episodic Control with State Abstraction", "abstract": "Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample inefficiency. Generally, episodic control-based approaches are solutions that leverage highly-rewarded past experiences to improve sample efficiency of DRL algorithms. However, previous episodic control-based approaches fail to utilize the latent information from the historical behaviors (e.g., state transitions, topological similarities, etc.) and lack scalability during DRL training. This work introduces Neural Episodic Control with State Abstraction (NECSA), a simple but effective state abstraction-based episodic control containing a more comprehensive episodic memory, a novel state evaluation, and a multi-step state analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym domains. The experimental results indicate that NECSA achieves higher sample efficiency than the state-of-the-art episodic control-based approaches. Our data and code are available at the project website\\footnote{\\url{https://sites.google.com/view/drl-necsa}}."}}
{"id": "65BgaP_6or", "cdate": 1672531200000, "mdate": 1683880914667, "content": {"title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy", "abstract": "In recent years, diffusion models have achieved tremendous success in the field of image generation, becoming the stateof-the-art technology for AI-based image processing applications. Despite the numerous benefits brought by recent advances in diffusion models, there are also concerns about their potential misuse, specifically in terms of privacy breaches and intellectual property infringement. In particular, some of their unique characteristics open up new attack surfaces when considering the real-world deployment of such models. With a thorough investigation of the attack vectors, we develop a systematic analysis of membership inference attacks on diffusion models and propose novel attack methods tailored to each attack scenario specifically relevant to diffusion models. Our approach exploits easily obtainable quantities and is highly effective, achieving near-perfect attack performance (>0.9 AUCROC) in realistic scenarios. Our extensive experiments demonstrate the effectiveness of our method, highlighting the importance of considering privacy and intellectual property risks when using diffusion models in image generation tasks."}}
{"id": "C2fsSj3ZGiU", "cdate": 1663849917003, "mdate": null, "content": {"title": "Neural Episodic Control with State Abstraction", "abstract": "Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample inefficiency. Generally, episodic control-based approaches are solutions that leverage highly rewarded past experiences to improve sample efficiency of DRL algorithms. However, previous episodic control-based approaches fail to utilize the latent information from the historical behaviors (\\eg, state transitions, topological similarities, \\etc) and lack scalability during DRL training. This work introduces Neural Episodic Control with State Abstraction (NECSA), a simple but effective state abstraction-based episodic control containing a more comprehensive episodic memory, a novel state evaluation, and a multi-step state analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym domains. The experimental results indicate that NECSA achieves higher sample efficiency than the state-of-the-art episodic control-based approaches. Our data and code are available at the project website\\footnote{\\url{https://sites.google.com/view/drl-necsa}}. "}}
{"id": "u467lIV-j_W", "cdate": 1640995200000, "mdate": 1655082673393, "content": {"title": "Towards Understanding Quality Challenges of the Federated Learning: A First Look from the Lens of Robustness", "abstract": "Federated learning (FL) is a widely adopted distributed learning paradigm in practice, which intends to preserve users' data privacy while leveraging the entire dataset of all participants for training. In FL, multiple models are trained independently on the users and aggregated centrally to update a global model in an iterative process. Although this approach is excellent at preserving privacy by design, FL still tends to suffer from quality issues such as attacks or byzantine faults. Some recent attempts have been made to address such quality challenges on the robust aggregation techniques for FL. However, the effectiveness of state-of-the-art (SOTA) robust FL techniques is still unclear and lacks a comprehensive study. Therefore, to better understand the current quality status and challenges of these SOTA FL techniques in the presence of attacks and faults, in this paper, we perform a large-scale empirical study to investigate the SOTA FL's quality from multiple angles of attacks, simulated faults (via mutation operators), and aggregation (defense) methods. In particular, we perform our study on two generic image datasets and one real-world federated medical image dataset. We also systematically investigate the effect of the distribution of attacks/faults over users and the independent and identically distributed (IID) factors, per dataset, on the robustness results. After a large-scale analysis with 496 configurations, we find that most mutators on each individual user have a negligible effect on the final model. Moreover, choosing the most robust FL aggregator depends on the attacks and datasets. Finally, we illustrate that it is possible to achieve a generic solution that works almost as well or even better than any single aggregator on all attacks and configurations with a simple ensemble model of aggregators."}}
{"id": "ajovn2ePUsN", "cdate": 1609459200000, "mdate": 1683880914578, "content": {"title": "DeepMemory: Model-based Memorization Analysis of Deep Neural Language Models", "abstract": "The neural network model is having a significant impact on many real-world applications. Unfortunately, the increasing popularity and complexity of these models also amplifies their security and privacy challenges, with privacy leakage from training data being one of the most prominent issues. In this context, prior studies proposed to analyze the abstraction behavior of neural network models, e.g., RNN, to understand their robustness. However, the existing research rarely addresses privacy breaches caused by memorization in neural language models. To fill this gap, we propose a novel approach, DeepMemory, that analyzes memorization behavior for a neural language model. We first construct a memorization-analysis-oriented model, taking both training data and a neural language model as input. We then build a semantic first-order Markov model to bind the constructed memorization-analysis-oriented model to the training data to analyze memorization distribution. Finally, we apply our approach to address data leakage issues associated with memorization and to assist in dememorization. We evaluate our approach on one of the most popular neural language models, the LSTM-based language model, with three public datasets, namely, WikiText-103, WMT2017, and IWSLT2016. We find that sentences in the studied datasets with low perplexity are more likely to be memorized. Our approach achieves an average AUC of 0.73 in automatically identifying data leakage issues during assessment. We also show that with the assistance of DeepMemory, data breaches due to memorization of neural language models can be successfully mitigated by mutating training data without reducing the performance of neural language models."}}
{"id": "BJEcTjx_Zr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Neural Hidden Markov Model for Machine Translation", "abstract": "This paper describes a semiparametric Bayesian approach for modelling mark-recapture data. A main assumption in modelling mark-recapture data is that survival probabilities are homogeneous. We relax this assumption by modelling survival probabilities"}}
{"id": "H14-vneOZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation", "abstract": "Recently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks. Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods. This work proposes a direct HMM with neural network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm. The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0% Bleu scores on two different translation tasks."}}
