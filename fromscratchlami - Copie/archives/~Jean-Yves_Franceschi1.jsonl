{"id": "BXKN-ulI01N", "cdate": 1672531200000, "mdate": 1687879341713, "content": {"title": "Unifying GANs and Score-Based Diffusion as Generative Particle Models", "abstract": "Particle-based deep generative models, such as gradient flows and score-based diffusion models, have recently gained traction thanks to their striking performance. Their principle of displacing particle distributions by differential equations is conventionally seen as opposed to the previously widespread generative adversarial networks (GANs), which involve training a pushforward generator network. In this paper, we challenge this interpretation and propose a novel framework that unifies particle and adversarial generative models by framing generator training as a generalization of particle models. This suggests that a generator is an optional addition to any such generative model. Consequently, integrating a generator into a score-based diffusion model and training a GAN without a generator naturally emerge from our framework. We empirically test the viability of these original models as proofs of concepts of potential applications of our framework."}}
{"id": "iB3KkHR4gc", "cdate": 1664248827290, "mdate": null, "content": {"title": "Continuous PDE Dynamics Forecasting with Implicit Neural Representations", "abstract": "Effective data-driven PDE forecasting methods often rely on fixed spatial and / or temporal discretizations. This raises limitations in real-world applications like weather prediction where flexible extrapolation at arbitrary spatiotemporal locations is required. We address this problem by introducing a new data-driven approach, DINo, that models a PDE's flow with continuous-time dynamics of spatially continuous functions. This is achieved by embedding spatial observations independently of their discretization via Implicit Neural Representations in a small latent space temporally driven by a learned ODE. This separate and flexible treatment of time and space makes DINo the first data-driven model to combine the following advantages. It extrapolates at arbitrary spatial and temporal locations; it can learn from sparse irregular grids or manifolds; at test time, it generalizes to new grids or resolutions. DINo outperforms alternative neural PDE forecasters in a variety of challenging generalization scenarios on representative PDE systems."}}
{"id": "B73niNjbPs", "cdate": 1663849865203, "mdate": null, "content": {"title": "Continuous PDE Dynamics Forecasting with Implicit Neural Representations", "abstract": "Effective data-driven PDE forecasting methods often rely on fixed spatial and / or temporal discretizations. This raises limitations in real-world applications like weather prediction where flexible extrapolation at arbitrary spatiotemporal locations is required. We address this problem by introducing a new data-driven approach, DINo, that models a PDE's flow with continuous-time dynamics of spatially continuous functions. This is achieved by embedding spatial observations independently of their discretization via Implicit Neural Representations in a small latent space temporally driven by a learned ODE. This separate and flexible treatment of time and space makes DINo the first data-driven model to combine the following advantages. It extrapolates at arbitrary spatial and temporal locations; it can learn from sparse irregular grids or manifolds; at test time, it generalizes to new grids or resolutions. DINo outperforms alternative neural PDE forecasters in a variety of challenging generalization scenarios on representative PDE systems."}}
{"id": "DoK5vDUa7M", "cdate": 1640995200000, "mdate": 1667487938819, "content": {"title": "A Neural Tangent Kernel Perspective of GANs", "abstract": "We propose a novel theoretical framework of analysis for Generative Adversarial Networks (GANs). We reveal a fundamental flaw of previous analyses which, by incorrectly modeling GANs\u2019 training sche..."}}
{"id": "vLaHRtHvfFp", "cdate": 1601308025103, "mdate": null, "content": {"title": "PDE-Driven Spatiotemporal Disentanglement", "abstract": "A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets."}}
{"id": "SzMWf5j6nl5", "cdate": 1577836800000, "mdate": 1646218122305, "content": {"title": "Stochastic Latent Residual Video Prediction", "abstract": "Designing video prediction models that account for the inherent uncertainty of the future is challenging. Most works in the literature are based on stochastic image-autoregressive recurrent network..."}}
{"id": "HyeqPJHYvH", "cdate": 1569439586084, "mdate": null, "content": {"title": "Stochastic Latent Residual Video Prediction", "abstract": "Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets."}}
{"id": "rylbKErgUH", "cdate": 1567802488757, "mdate": null, "content": {"title": "Unsupervised Scalable Representation Learning for Multivariate Time Series", "abstract": "Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series."}}
{"id": "ryloJjA4d4", "cdate": 1553423074577, "mdate": null, "content": {"title": "Unsupervised Scalable Representation Learning for Multivariate Time Series", "abstract": "Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series."}}
{"id": "uLBDokLxD68", "cdate": 1514764800000, "mdate": null, "content": {"title": "Robustness of classifiers to uniform $\\ell_p$ and Gaussian noise", "abstract": "We study the robustness of classifiers to various kinds of random noise models. In particular, we consider noise drawn uniformly from the $\\ell_p$ ball for $p \u2208[1, \u221e]$ and Gaussian noise with an ar..."}}
