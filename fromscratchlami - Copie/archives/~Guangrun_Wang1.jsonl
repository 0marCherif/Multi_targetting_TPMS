{"id": "GKJS2VW2FG", "cdate": 1698913662353, "mdate": 1698913662353, "content": {"title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts", "abstract": "Thanks to the rapid development of diffusion models, un- precedented progress has been witnessed in image synthe- sis. Prior works mostly rely on pre-trained linguistic mod- els, but a text is often too abstract to properly specify all the spatial properties of an image, e.g., the layout config- uration of a scene, leading to the sub-optimal results of complex scene generation. In this paper, we achieve ac- curate complex scene generation by proposing a seman- tically controllable Layout-AWare diffusion model, termed LAW-Diffusion. Distinct from the previous Layout-to-Image generation (L2I) methods that primarily explore category- aware relationships, LAW-Diffusion introduces a spatial de- pendency parser to encode the location-aware semantic co- herence across objects as a layout embedding and produces a scene with perceptually harmonious object styles and con- textual relations. To be specific, we delicately instantiate each object\u2019s regional semantics as an object region map and leverage a location-aware cross-object attention mod- ule to capture the spatial dependencies among those dis- entangled representations. We further propose an adap- tive guidance schedule for our layout guidance to mitigate the trade-off between the regional semantic alignment and the texture fidelity of generated objects. Moreover, LAW- Diffusion allows for instance reconfiguration while main- taining the other regions in a synthesized image by introduc- ing a layout-aware latent grafting mechanism to recompose its local regional semantics. To better verify the plausibil- ity of generated scenes, we propose a new evaluation metric for the L2I task, dubbed Scene Relation Score (SRS) to mea- sure how the images preserve the rational and harmonious relations among contextual objects. Comprehensive ex- periments on COCO-Stuff and Visual-Genome demonstrate that our LAW-Diffusion yields the state-of-the-art genera- tive performance, especially with coherent object relations."}}
{"id": "AxE00qNIRL4", "cdate": 1668592141464, "mdate": 1668592141464, "content": {"title": "Solving Inefficiency of Self-supervised Representation Learning", "abstract": "Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, eg, taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (eg, ImageNet, SYSU-30k, and COCO). The results demonstrate our model's \u2026"}}
{"id": "Y3Ca1RhPB1D", "cdate": 1668591855492, "mdate": null, "content": {"title": "Semantic-Aware Auto-Encoders for Self-Supervised Representation Learning", "abstract": "The resurgence of unsupervised learning can be attributed to the remarkable progress of self-supervised learning, which includes generative (G) and discriminative (D) models. In computer vision, the mainstream self-supervised learning algorithms are D models. However, designing a D model could be over-complicated; also, some studies hinted that a D model might not be as general and interpretable as a G model. In this paper, we switch from D models to G models using the classical auto-encoder (AE). Note that a vanilla G model was far less efficient than a D model in self-supervised computer vision tasks, as it wastes model capability on overfitting semantic-agnostic high-frequency details. Inspired by perceptual learning that could use cross-view learning to perceive concepts and semantics, we propose a novel AE that could learn semantic-aware representation via cross-view image reconstruction. We use one view of an image as the input and another view of the same image as the reconstruction target. This kind of AE has rarely been studied before, and the optimization is very difficult. To enhance learning ability and find a feasible solution, we propose a semantic aligner that uses geometric transformation knowledge to align the hidden code of AE to help optimization. These techniques significantly improve the representation learning ability of AE and make self-supervised learning with G models possible. Extensive experiments on many large-scale benchmarks (eg, ImageNet, COCO 2017, and SYSU-30k) demonstrate the effectiveness of our methods."}}
{"id": "2XLRBjY46O6", "cdate": 1663850114516, "mdate": null, "content": {"title": "ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency", "abstract": "Recently, great success has been made in learning visual representations from text supervision, facilitating the emergence of text-supervised semantic segmentation. However, existing works focus on pixel grouping and cross-modal semantic alignment, while ignoring the correspondence among multiple augmented views of the same image. To overcome such limitation, we propose multi-View Consistent learning (ViewCo) for text-supervised semantic segmentation. Specifically, we first propose text-to-views consistency modeling to learn correspondence for multiple views of the same input image. Additionally, we propose cross-view segmentation consistency modeling to address the ambiguity issue of text supervision by contrasting the segment features of Siamese visual encoders.  The text-to-views consistency benefits dense assignment of the visual features by encouraging different crops to align with the same text, while the cross-view segmentation consistency modeling provides additional self-supervision, overcoming the limitation of ambiguous text supervision for segmentation masks. Trained with large-scale image-text data, our model can directly segment objects of arbitrary categories in a zero-shot manner. Extensive experiments show that ViewCo outperforms state-of-the-art methods on average by up to 2.9%, 1.6%, and 2.4% mIoU on PASCAL VOC2012, PASCAL Context, and COCO, respectively."}}
{"id": "mkEPog9HiV", "cdate": 1652737438818, "mdate": null, "content": {"title": "Structure-Preserving 3D Garment Modeling with Neural Sewing Machines", "abstract": "3D Garment modeling is a critical and challenging topic in the area of computer vision and graphics, with increasing attention focused on garment representation learning, garment reconstruction, and controllable garment manipulation, whereas existing methods were constrained to model garments under specific categories or with relatively simple topologies. In this paper, we propose a novel Neural Sewing Machine (NSM), a learning-based framework for structure-preserving 3D garment modeling, which is capable of learning representations for garments with diverse shapes and topologies and is successfully applied to 3D garment reconstruction and controllable manipulation. To model generic garments, we first obtain sewing pattern embedding via a unified sewing pattern encoding module, as the sewing pattern can accurately describe the intrinsic structure and the topology of the 3D garment. Then we use a 3D garment decoder to decode the sewing pattern embedding into a 3D garment using the UV-position maps with masks. To preserve the intrinsic structure of the predicted 3D garment, we introduce an inner-panel structure-preserving loss, an inter-panel structure-preserving loss, and a surface-normal loss in the learning process of our framework. We evaluate NSM on the public 3D garment dataset with sewing patterns with diverse garment shapes and categories. Extensive experiments demonstrate that the proposed NSM is capable of representing 3D garments under diverse garment shapes and topologies, realistically reconstructing 3D garments from 2D images with the preserved structure, and accurately manipulating the 3D garment categories, shapes, and topologies, outperforming the state-of-the-art methods by a clear margin."}}
{"id": "7xArdn_FKtV", "cdate": 1601308112385, "mdate": null, "content": {"title": "Heterogeneous Model Transfer between Different Neural Networks", "abstract": "We propose an effective heterogeneous model transfer (HMT) method that can transfer the knowledge from one pretrained neural network to another neural network. Most of the existing deep learning methods depend much on a pretraining-finetuning strategy, i.e., pretraining a deep model on a large task-related (source) dataset and finetuning it on a small target dataset. Pretraining provides a universal feature representation for the target learning task and thus reduces the overfitting on a small target dataset. However, it is often assumed that the pretrained model and the target model share an identical backbone, which significantly limits the scalability of pretrained deep models. This paper relaxes this limitation and generalizes to heterogeneous model transfer between two different neural networks. Specifically, we select the longest chain from the source model and transfer it to the longest chain of the target model. Motivated by one-shot neural architecture search methods, the longest chain inherits merits from the source model and also serves as a weight-sharing path of the target model, thus provides a good initialization. With the longest chains, the layer-to-layer weight transfer is then transformed by bilinear interpolation and cyclic stack. HMT opens a new window for the pretraining-finetuning strategy and significantly improves the reuse efficiency of pretrained models without re-pretraining on the large source dataset. Experiments on several datasets show the effectiveness of HMT. Anonymous code is at: https://anonymous.4open.science/r/6ab184dc-3c64-4fdd-ba6d-1e5097623dfd/"}}
{"id": "j9O6V6KWdvq", "cdate": 1598889036663, "mdate": null, "content": {"title": "Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking", "abstract": "The success of DNNs has driven the extensive appli- cations of person re-identification (ReID) into a new era. However, whether ReID inherits the vulnerability of DNNs remains unexplored. To examine the robustness of ReID systems is rather important because the insecurity of ReID systems may cause severe losses, e.g., the criminals may use the adversarial perturbations to cheat the CCTV systems.\n\nIn this work, we examine the insecurity of current best- performing ReID models by proposing a learning-to-mis- rank formulation to perturb the ranking of the system out- put. As the cross-dataset transferability is crucial in the ReID domain, we also perform a back-box attack by devel- oping a novel multi-stage network architecture that pyra- mids the features of different levels to extract general and transferable features for the adversarial perturbations. Our method can control the number of malicious pixels by using differentiable multi-shot sampling. To guarantee the incon- spicuousness of the attack, we also propose a new percep- tion loss to achieve better visual quality.\n\nExtensive experiments on four of the largest ReID benchmarks (i.e., Market1501 [45], CUHK03 [17], DukeMTMC [33], and MSMT17 [40]) not only show the effectiveness of our method, but also provides directions of the future improvement in the robustness of ReID systems. For example, the accuracy of one of the best-performing ReID systems drops sharply from 91.8% to 1.4% after being attacked by our method. Some attack results are shown in Fig. 1. The code is available at https://github. com/whj363636/Adversarial-attack-on- Person-ReID-With-Deep-Mis-Ranking.\n\n\n"}}
{"id": "yI-gy8ibKGH", "cdate": 1598888035308, "mdate": null, "content": {"title": "Block-wisely Supervised Neural Architecture Search with Knowledge Distillation", "abstract": "Neural Architecture Search (NAS), aiming at automati- cally designing network architectures by machines, is ex- pected to bring about a new revolution in machine learn- ing. Despite these high expectation, the effectiveness and efficiency of existing NAS solutions are unclear, with some recent works going so far as to suggest that many existing NAS solutions are no better than random architecture selec- tion. The ineffectiveness of NAS solutions may be attributed to inaccurate architecture evaluation. Specifically, to speed up NAS, recent works have proposed under-training differ- ent candidate architectures in a large search space concur- rently by using shared network parameters; however, this has resulted in incorrect architecture ratings and furthered the ineffectiveness of NAS.\n\nIn this work, we propose to modularize the large search space of NAS into blocks to ensure that the potential candi- date architectures are fully trained; this reduces the repre- sentation shift caused by the shared parameters and leads to the correct rating of the candidates. Thanks to the block- wise search, we can also evaluate all of the candidate ar- chitectures within each block. Moreover, we find that the knowledge of a network model lies not only in the network parameters but also in the network architecture. Therefore, we propose to distill the neural architecture (DNA) knowl- edge from a teacher model to supervise our block-wise ar- chitecture search, which significantly improves the effective- ness of NAS. Remarkably, the performance of our searched architectures has exceeded the teacher model, demonstrat- ing the practicability of our method. Finally, our method achieves a state-of-the-art 78.4% top-1 accuracy on Im- ageNet in a mobile setting. All of our searched models along with the evaluation code are available at https: //github.com/changlin31/DNA.\n"}}
{"id": "rJeUPlrYvr", "cdate": 1569439837933, "mdate": null, "content": {"title": "FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization", "abstract": "Finding out the computational redundant part of a trained Deep Neural Network (DNN) is the key question that pruning algorithms target on. Many algorithms try to predict model performance of the pruned sub-nets by introducing various evaluation methods. But they are either inaccurate or very complicated for general application. In this work, we present a pruning method called Fast Neural Network Pruning (FNNP), in which a simple yet efficient evaluation component called ABN-based evaluation is applied to unveil a strong correlation between different pruned DNN structures and their final settled accuracy. This strong correlation allows us to fast spot the pruned candidates with highest potential accuracy without actually fine tuning them. FNNP does not require any extra regularization or supervision introduced to a common DNN training pipeline but still can achieve better accuracy than many carefully-designed pruning methods. In the experiments of pruning MobileNet V1 and ResNet-50, FNNP outperforms all compared methods by up to 3.8%. Even in the more challenging experiments of pruning the compact model of MobileNet V1, our FNNP achieves the highest accuracy of 70.7% with an overall 50% operations (FLOPs) pruned. All accuracy data are Top-1 ImageNet classification accuracy. Source code and models are accessible to open-source community."}}
{"id": "rJgCOySYwH", "cdate": 1569439606255, "mdate": null, "content": {"title": "Function Feature Learning of Neural Networks", "abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method."}}
