{"id": "nWUvkpYG-aV", "cdate": 1672531200000, "mdate": 1682335263608, "content": {"title": "Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses", "abstract": "As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space."}}
{"id": "0rfEDZmOlf3", "cdate": 1663939399239, "mdate": null, "content": {"title": "Improving Vertical Federated Learning by Efficient Communication with ADMM", "abstract": "Vertical Federated learning (VFL) allows each client to collect partial features and jointly train the shared model. In this paper, we identified two challenges in VFL: (1) some works directly average the learned feature embeddings and therefore might lose the unique properties of each local feature set;  (2) the server needs to communicate gradients with the clients for each training step, incurring high communication cost.\nWe aim to address the above challenges and propose an efficient VFL with multiple heads (VIM) framework, where each head corresponds to local clients by taking the separate contribution of each client into account. In addition, we propose an Alternating Direction Method of Multipliers (ADMM)-based method to solve our optimization problem, which reduces the communication cost by allowing multiple local updates in each step.\nWe show that VIM achieves significantly higher accuracy and faster convergence compared with state-of-the-arts on four datasets, and the weights of learned heads reflect the importance of local clients."}}
{"id": "9bVBH1GD5sr", "cdate": 1663850177211, "mdate": null, "content": {"title": "FOCUS: Fairness via Agent-Awareness for Federated Learning on Heterogeneous Data", "abstract": "Federated learning (FL) provides an effective collaborative training paradigm, allowing local agents to train a global model jointly without sharing their local data to protect privacy.\nOn the other hand, due to the heterogeneous nature of local agents, it is challenging to optimize or even define the fairness for agents, which may discourage valuable participation. For instance, the trained global model may sacrifice the performance of a minority user with high-quality data based on loss optimization over all users.\nExisting work usually considers accuracy equity as fairness for different users in FL, which is limited especially under the heterogeneous setting, since it is intuitively \"unfair\" that agents with low-quality data would achieve similar accuracy.\nIn this work, we aim to address such limitations and propose a formal fairness definition in FL, fairness via agent-awareness (FAA), which takes the heterogeneous data contributions of local agents into account. In addition, we propose a fair FL training algorithm based on agent clustering (FOCUS) to achieve FAA. Theoretically, we prove the convergence and optimality of  FOCUS under mild conditions for linear and general convex loss functions with bounded smoothness. We also prove that FOCUS always achieves higher fairness measured by FAA compared with standard FedAvg protocol under both linear and general convex loss functions. Empirically, we evaluate FOCUS on four datasets, including synthetic data, images, and texts under different settings, and we show that FOCUS achieves significantly higher fairness based on FAA while maintaining similar or even higher prediction accuracy compared with FedAvg and other existing fair FL algorithms.\n"}}
{"id": "r5rzV51GZx", "cdate": 1652737581571, "mdate": null, "content": {"title": "CoPur: Certifiably Robust Collaborative Inference via Feature Purification", "abstract": "Collaborative inference leverages diverse features provided by different agents (e.g., sensors) for more accurate inference. A common setup is where each agent sends its embedded features instead of the raw data to the Fusion Center (FC) for joint prediction. In this setting, we consider the inference-time attacks when a small fraction of agents are compromised. The compromised agent either does not send embedded features to the FC, or sends arbitrarily embedded features. To address this, we propose a certifiably robust COllaborative inference framework via feature PURification (CoPur), by leveraging the block-sparse nature of adversarial perturbations on the feature vector, as well as exploring the underlying redundancy across the embedded features (by assuming the overall features lie on an underlying lower dimensional manifold). We theoretically show that the proposed feature purification method can robustly recover the true feature vector, despite adversarial corruptions and/or incomplete observations. We also propose and test an untargeted distributed feature-flipping attack, which is agnostic to the model, training data, label, as well as the features held by other agents, and is shown to be effective in attacking state-of-the-art defenses. Experiments on ExtraSensory and NUS-WIDE datasets show that CoPur significantly outperforms existing defenses in terms of robustness against targeted and untargeted adversarial attacks."}}
{"id": "a_ASZbWsQp_", "cdate": 1632875736887, "mdate": null, "content": {"title": "RVFR: Robust Vertical Federated Learning via Feature Subspace Recovery", "abstract": "Vertical Federated Learning (VFL) is a distributed learning paradigm that allows multiple agents to jointly train a global model when each agent holds a different subset of features for the same sample(s). VFL is known to be vulnerable to backdoor attacks, where data from malicious agents are manipulated during training, and vulnerable to test-time attacks, where malicious agents manipulate the test data. However, unlike the standard horizontal federated learning, improving the robustness of robust VFL remains challenging. To this end, we propose RVFR, a novel robust VFL training and inference framework. The key to our approach is to ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions, RVFR recovers the underlying uncorrupted features with guarantees, thus sanitizes the model against a vast range of backdoor attacks. Further, RVFR also defends against test-time adversarial and missing feature attacks. We conduct extensive experiments on several datasets and show that the robustness of RVFR outperforms different baselines against diverse types of attacks."}}
{"id": "qrdbsZEZPZ", "cdate": 1632875463236, "mdate": null, "content": {"title": "Certified Robustness for Free in Differentially Private Federated Learning", "abstract": "Federated learning (FL)  provides an efficient training paradigm to jointly train a global model leveraging data from distributed users.\nAs the local training data comes from different users who may not be trustworthy, several studies have shown that FL is vulnerable to poisoning attacks where adversaries add malicious data during training. On the other hand,  to protect the privacy of users, FL is usually trained in a differentially private way (DPFL). Given these properties of FL, in this paper, we aim to ask: Can we leverage the innate privacy property of  DPFL to provide robustness certification against poisoning attacks? Can we further improve the privacy of FL to improve such certification?\nTo this end, we first investigate both the user-level and instance-level privacy of FL, and propose novel randomization mechanisms and analysis to achieve improved differential privacy.\nWe then provide two robustness certification criteria: certified prediction and certified attack cost for DPFL on both levels. Theoretically, given different privacy properties of DPFL, we prove their certified robustness under a bounded number of adversarial users or instances. \nEmpirically, we conduct extensive experiments to verify our theories under different attacks on a range of datasets. We show that the global model with a tighter privacy guarantee always provides stronger robustness certification in terms of the certified attack cost, while may exhibit tradeoffs regarding the certified prediction.\nWe believe our work will inspire future research of developing certifiably robust DPFL based on its inherent properties."}}
{"id": "qH89ZF22cD", "cdate": 1609459200000, "mdate": 1681490806731, "content": {"title": "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks", "abstract": ""}}
{"id": "JUS8Z_HkIj4", "cdate": 1609459200000, "mdate": 1668553710880, "content": {"title": "Style-Based Point Generator With Adversarial Rendering for Point Cloud Completion", "abstract": "In this paper, we proposed a novel Style-based Point Generator with Adversarial Rendering (SpareNet) for point cloud completion. Firstly, we present the channel-attentive EdgeConv to fully exploit the local structures as well as the global shape in point features. Secondly, we observe that the concatenation manner used by vanilla foldings limits its potential of generating a complex and faithful shape. Enlightened by the success of StyleGAN, we regard the shape feature as style code that modulates the normalization layers during the folding, which considerably enhances its capability. Thirdly, we realize that existing point supervisions, e.g., Chamfer Distance or Earth Mover's Distance, cannot faithfully reflect the perceptual quality of the reconstructed points. To address this, we propose to project the completed points to depth maps with a differentiable renderer and apply adversarial training to advocate the perceptual realism under different viewpoints. Comprehensive experiments on ShapeNet and KITTI prove the effectiveness of our method, which achieves state-of-the-art quantitative performance while offering superior visual quality."}}
{"id": "434qJPI3_Ti", "cdate": 1577836800000, "mdate": 1681490807794, "content": {"title": "DBA: Distributed Backdoor Attacks against Federated Learning", "abstract": ""}}
{"id": "HkgAJxrYwr", "cdate": 1569439718284, "mdate": null, "content": {"title": "Attack-Resistant Federated Learning with Residual-based Reweighting", "abstract": "Federated learning has a variety of applications in multiple domains by utilizing private training data stored on different devices. However, the aggregation process in federated learning is highly vulnerable to adversarial attacks so that the global model may behave abnormally under attacks. To tackle this challenge, we present a novel aggregation algorithm with residual-based reweighting to defend federated learning. Our aggregation algorithm combines repeated median regression with the reweighting scheme in iteratively reweighted least squares. Our experiments show that our aggression algorithm outperforms other alternative algorithms in the presence of label-flipping, backdoor, and Gaussian noise attacks. We also provide theoretical guarantees for our aggregation algorithm.\n"}}
