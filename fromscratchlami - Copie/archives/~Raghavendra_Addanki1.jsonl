{"id": "FUnBJIPQp-F", "cdate": 1672531200000, "mdate": 1683583756268, "content": {"title": "Anytime-Valid Confidence Sequences in an Enterprise A/B Testing Platform", "abstract": "A/B tests are the gold standard for evaluating digital experiences on the web. However, traditional \u201cfixed-horizon\u201d statistical methods are often incompatible with the needs of modern industry practitioners as they do not permit continuous monitoring of experiments. Frequent evaluation of fixed-horizon tests (\u201cpeeking\u201d) leads to inflated type-I error and can result in erroneous conclusions. We have released an experimentation service on the Adobe Experience Platform based on anytime-valid confidence sequences, allowing for continuous monitoring of the A/B test and data-dependent stopping. We demonstrate how we adapted and deployed asymptotic confidence sequences in a full featured A/B testing platform, describe how sample size calculations can be performed, and how alternate test statistics like \u201clift\u201d can be analyzed. On both simulated data and thousands of real experiments, we show the desirable properties of using anytime-valid methods instead of traditional approaches."}}
{"id": "4X0q4uJ1fR", "cdate": 1652737659736, "mdate": null, "content": {"title": "Sample Constrained Treatment Effect Estimation", "abstract": "Treatment effect estimation is a fundamental problem in causal inference. We focus on designing efficient randomized controlled trials, to accurately estimate the effect of some treatment on a population of $n$ individuals. In particular, we study \\textit{sample-constrained treatment effect estimation}, where we must select a subset of $s \\ll n$ individuals from the population to experiment on. This subset must be further partitioned into treatment and control groups. Algorithms for partitioning the entire population into treatment and control groups, or for choosing a single representative subset, have been well-studied. The key challenge in our setting is jointly choosing a representative subset and a partition for that set.\n\n  We focus on both individual and average treatment effect estimation, under a linear effects model. We give provably efficient experimental designs and corresponding estimators, by identifying connections to discrepancy minimization and leverage-score-based sampling used in randomized numerical linear algebra. Our theoretical results obtain a smooth transition to known guarantees when $s$ equals the population size. We also empirically demonstrate the performance of our algorithms.\n"}}
{"id": "aGswpBZpFv", "cdate": 1640995200000, "mdate": 1681739323317, "content": {"title": "Non-Adaptive Edge Counting and Sampling via Bipartite Independent Set Queries", "abstract": "We study the problem of estimating the number of edges in an n-vertex graph, accessed via the Bipartite Independent Set query model introduced by Beame et al. (TALG '20). In this model, each query returns a Boolean, indicating the existence of at least one edge between two specified sets of nodes. We present a non-adaptive algorithm that returns a (1\u00b1 \u03b5) relative error approximation to the number of edges, with query complexity O\u0303(\u03b5^{-5}log\u2075 n), where O\u0303(\u22c5) hides poly(log log n) dependencies. This is the first non-adaptive algorithm in this setting achieving poly(1/\u03b5,log n) query complexity. Prior work requires \u03a9(log\u00b2 n) rounds of adaptivity. We avoid this by taking a fundamentally different approach, inspired by work on single-pass streaming algorithms. Moreover, for constant \u03b5, our query complexity significantly improves on the best known adaptive algorithm due to Bhattacharya et al. (STACS '22), which requires O(\u03b5^{-2} log^{11} n) queries. Building on our edge estimation result, we give the first {non-adaptive} algorithm for outputting a nearly uniformly sampled edge with query complexity O\u0303(\u03b5^{-6} log\u2076 n), improving on the works of Dell et al. (SODA '20) and Bhattacharya et al. (STACS '22), which require \u03a9(log\u00b3 n) rounds of adaptivity. Finally, as a consequence of our edge sampling algorithm, we obtain a O\u0303(n log^8 n) query algorithm for connectivity, using two rounds of adaptivity. This improves on a three-round algorithm of Assadi et al. (ESA '21) and is tight; there is no non-adaptive algorithm for connectivity making o(n\u00b2) queries."}}
{"id": "U38k8Jq-vzv", "cdate": 1640995200000, "mdate": 1681739323717, "content": {"title": "Non-Adaptive Edge Counting and Sampling via Bipartite Independent Set Queries", "abstract": "We study the problem of estimating the number of edges in an $n$-vertex graph, accessed via the Bipartite Independent Set query model introduced by Beame et al. (ITCS '18). In this model, each query returns a Boolean, indicating the existence of at least one edge between two specified sets of nodes. We present a non-adaptive algorithm that returns a $(1\\pm \\epsilon)$ relative error approximation to the number of edges, with query complexity $\\tilde O(\\epsilon^{-5}\\log^{5} n )$, where $\\tilde O(\\cdot)$ hides $\\textrm{poly}(\\log \\log n)$ dependencies. This is the first non-adaptive algorithm in this setting achieving $\\textrm{poly}(1/\\epsilon,\\log n)$ query complexity. Prior work requires $\\Omega(\\log^2 n)$ rounds of adaptivity. We avoid this by taking a fundamentally different approach, inspired by work on single-pass streaming algorithms. Moreover, for constant $\\epsilon$, our query complexity significantly improves on the best known adaptive algorithm due to Bhattacharya et al. (STACS '22), which requires $O(\\epsilon^{-2} \\log^{11} n)$ queries. Building on our edge estimation result, we give the first non-adaptive algorithm for outputting a nearly uniformly sampled edge with query complexity $\\tilde O(\\epsilon^{-6} \\log^{6} n)$, improving on the works of Dell et al. (SODA '20) and Bhattacharya et al. (STACS '22), which require $\\Omega(\\log^3 n)$ rounds of adaptivity. Finally, as a consequence of our edge sampling algorithm, we obtain a $\\tilde O(n\\log^ 8 n)$ query algorithm for connectivity, using two rounds of adaptivity. This improves on a three-round algorithm of Assadi et al. (ESA '21) and is tight; there is no non-adaptive algorithm for connectivity making $o(n^2)$ queries."}}
{"id": "R7Q99ROD-S", "cdate": 1640995200000, "mdate": 1684167707934, "content": {"title": "Sample Constrained Treatment Effect Estimation", "abstract": "Treatment effect estimation is a fundamental problem in causal inference. We focus on designing efficient randomized controlled trials, to accurately estimate the effect of some treatment on a population of $n$ individuals. In particular, we study \\textit{sample-constrained treatment effect estimation}, where we must select a subset of $s \\ll n$ individuals from the population to experiment on. This subset must be further partitioned into treatment and control groups. Algorithms for partitioning the entire population into treatment and control groups, or for choosing a single representative subset, have been well-studied. The key challenge in our setting is jointly choosing a representative subset and a partition for that set. We focus on both individual and average treatment effect estimation, under a linear effects model. We give provably efficient experimental designs and corresponding estimators, by identifying connections to discrepancy minimization and leverage-score-based sampling used in randomized numerical linear algebra. Our theoretical results obtain a smooth transition to known guarantees when $s$ equals the population size. We also empirically demonstrate the performance of our algorithms."}}
{"id": "IpWke5Bwcv", "cdate": 1640995200000, "mdate": 1682376451855, "content": {"title": "Improved Approximation and Scalability for Fair Max-Min Diversification", "abstract": "Given an $n$-point metric space $(\\mathcal{X},d)$ where each point belongs to one of $m=O(1)$ different categories or groups and a set of integers $k_1, \\ldots, k_m$, the fair Max-Min diversification problem is to select $k_i$ points belonging to category $i\\in [m]$, such that the minimum pairwise distance between selected points is maximized. The problem was introduced by Moumoulidou et al. [ICDT 2021] and is motivated by the need to down-sample large data sets in various applications so that the derived sample achieves a balance over diversity, i.e., the minimum distance between a pair of selected points, and fairness, i.e., ensuring enough points of each category are included. We prove the following results: 1. We first consider general metric spaces. We present a randomized polynomial time algorithm that returns a factor $2$-approximation to the diversity but only satisfies the fairness constraints in expectation. Building upon this result, we present a $6$-approximation that is guaranteed to satisfy the fairness constraints up to a factor $1-\\epsilon$ for any constant $\\epsilon$. We also present a linear time algorithm returning an $m+1$ approximation with exact fairness. The best previous result was a $3m-1$ approximation. 2. We then focus on Euclidean metrics. We first show that the problem can be solved exactly in one dimension. For constant dimensions, categories and any constant $\\epsilon>0$, we present a $1+\\epsilon$ approximation algorithm that runs in $O(nk) + 2^{O(k)}$ time where $k=k_1+\\ldots+k_m$. We can improve the running time to $O(nk)+ poly(k)$ at the expense of only picking $(1-\\epsilon) k_i$ points from category $i\\in [m]$. Finally, we present algorithms suitable to processing massive data sets including single-pass data stream algorithms and composable coresets for the distributed processing."}}
{"id": "FTkw3Z_1bcg", "cdate": 1640995200000, "mdate": 1652806066114, "content": {"title": "Improved Approximation and Scalability for Fair Max-Min Diversification", "abstract": "Given an n-point metric space ({\ud835\udcb3},d) where each point belongs to one of m = O(1) different categories or groups and a set of integers k\u2081, \u2026, k_m, the fair Max-Min diversification problem is to select k_i points belonging to category i \u2208 [m], such that the minimum pairwise distance between selected points is maximized. The problem was introduced by Moumoulidou et al. [ICDT 2021] and is motivated by the need to down-sample large data sets in various applications so that the derived sample achieves a balance over diversity, i.e., the minimum distance between a pair of selected points, and fairness, i.e., ensuring enough points of each category are included. We prove the following results: 1) We first consider general metric spaces. We present a randomized polynomial time algorithm that returns a factor 2-approximation to the diversity but only satisfies the fairness constraints in expectation. Building upon this result, we present a 6-approximation that is guaranteed to satisfy the fairness constraints up to a factor 1-\u03b5 for any constant \u03b5. We also present a linear time algorithm returning an m+1 approximation with exact fairness. The best previous result was a 3m-1 approximation. 2) We then focus on Euclidean metrics. We first show that the problem can be solved exactly in one dimension. {For constant dimensions, categories and any constant \u03b5 > 0, we present a 1+\u03b5 approximation algorithm that runs in O(nk) + 2^{O(k)} time where k = k\u2081+\u2026+k_m.} We can improve the running time to O(nk)+poly(k) at the expense of only picking (1-\u03b5) k_i points from category i \u2208 [m]. Finally, we present algorithms suitable to processing massive data sets including single-pass data stream algorithms and composable coresets for the distributed processing."}}
{"id": "23jhhEISkt", "cdate": 1640995200000, "mdate": 1682376451861, "content": {"title": "Sample Constrained Treatment Effect Estimation", "abstract": "Treatment effect estimation is a fundamental problem in causal inference. We focus on designing efficient randomized controlled trials, to accurately estimate the effect of some treatment on a population of $n$ individuals. In particular, we study sample-constrained treatment effect estimation, where we must select a subset of $s \\ll n$ individuals from the population to experiment on. This subset must be further partitioned into treatment and control groups. Algorithms for partitioning the entire population into treatment and control groups, or for choosing a single representative subset, have been well-studied. The key challenge in our setting is jointly choosing a representative subset and a partition for that set. We focus on both individual and average treatment effect estimation, under a linear effects model. We give provably efficient experimental designs and corresponding estimators, by identifying connections to discrepancy minimization and leverage-score-based sampling used in randomized numerical linear algebra. Our theoretical results obtain a smooth transition to known guarantees when $s$ equals the population size. We also empirically demonstrate the performance of our algorithms."}}
{"id": "35wwc2nc1a4", "cdate": 1621630044539, "mdate": null, "content": {"title": "Collaborative Causal Discovery with Atomic Interventions", "abstract": "We introduce a new Collaborative Causal Discovery problem, through which we model a common scenario in which we have multiple independent entities each with their own causal graph, and the goal is to simultaneously learn all these causal graphs. We study this problem without the causal sufficiency assumption, using Maximal Ancestral Graphs (MAG) to model the causal graphs, and assuming that we have the ability to actively perform independent single vertex (or atomic) interventions on the entities. If the $M$ underlying (unknown) causal graphs of the entities satisfy a natural notion of clustering, we give algorithms that leverage this property and recovers all the causal graphs using roughly logarithmic in $M$ number of atomic interventions per entity. These are significantly fewer than $n$ atomic interventions per entity required to learn each causal graph separately, where $n$ is the number of observable nodes in the causal graph. We complement our results with a lower bound and discuss various extensions of our collaborative setting."}}
{"id": "vWiHKURCpLP", "cdate": 1609459200000, "mdate": 1652806066118, "content": {"title": "Intervention Efficient Algorithms for Approximate Learning of Causal Graphs", "abstract": "We study the problem of learning the causal relationships between a set of observed variables in the presence of latents, while minimizing the cost of interventions on the observed variables. We as..."}}
