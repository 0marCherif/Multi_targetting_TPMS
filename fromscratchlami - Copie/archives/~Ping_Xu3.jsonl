{"id": "n8ZpWKP9w36", "cdate": 1640995200000, "mdate": 1682361734041, "content": {"title": "Deep Kernel Learning Networks with Multiple Learning Paths", "abstract": "This paper proposes deep kernel learning networks with multiple learning paths (DKL-MLP) for nonlinear function approximation. Leveraging the random feature (RF) mapping technique, kernel methods can be implemented as a two-layer neural network, at drastically reduced workload on weight training. Motivated by the representation power of the deep architecture in deep neural networks, we devise a vanilla deep kernel learning network (DKL) by applying RF mapping at each layer and learn the last layer only. To improve the learning performance of DKL, we add multiple trainable paths to DKL and develop the DKL-MLP method so that some implicit information from earlier hidden layers to the output layer can be learned. We prove that both DKL and DKL-MLP permit universal representation of a wide variety of interesting functions with arbitrarily small error and have no bad local minimum. Numerical experiments on both regression and classification tasks are provided to demonstrate the learning performance and computational efficiency of the proposed methods."}}
{"id": "dGsVkWw-Bs", "cdate": 1640995200000, "mdate": 1682361734074, "content": {"title": "QC-ODKLA: Quantized and Communication-Censored Online Decentralized Kernel Learning via Linearized ADMM", "abstract": "This paper focuses on online kernel learning over a decentralized network. Each agent in the network receives continuous streaming data locally and works collaboratively to learn a nonlinear prediction function that is globally optimal in the reproducing kernel Hilbert space with respect to the total instantaneous costs of all agents. In order to circumvent the curse of dimensionality issue in traditional online kernel learning, we utilize random feature (RF) mapping to convert the non-parametric kernel learning problem into a fixed-length parametric one in the RF space. We then propose a novel learning framework named Online Decentralized Kernel learning via Linearized ADMM (ODKLA) to efficiently solve the online decentralized kernel learning problem. To further improve the communication efficiency, we add the quantization and censoring strategies in the communication stage and develop the Quantized and Communication-censored ODKLA (QC-ODKLA) algorithm. We theoretically prove that both ODKLA and QC-ODKLA can achieve the optimal sublinear regret $\\mathcal{O}(\\sqrt{T})$ over $T$ time slots. Through numerical experiments, we evaluate the learning effectiveness, communication, and computation efficiencies of the proposed methods."}}
{"id": "8Vj3U4L-77", "cdate": 1640995200000, "mdate": 1682361734244, "content": {"title": "A class of distributed event-triggered average consensus algorithms for multi-agent systems", "abstract": "This paper proposes a class of distributed event-triggered algorithms that solve the average consensus problem in multi-agent systems. By designing events such that a specifically chosen Lyapunov f..."}}
{"id": "86EnlMBP6Mz", "cdate": 1640995200000, "mdate": 1682361734032, "content": {"title": "Robust Distributed Learning Against Both Distributional Shifts and Byzantine Attacks", "abstract": "In distributed learning systems, robustness issues may arise from two sources. On one hand, due to distributional shifts between training data and test data, the trained model could exhibit poor out-of-sample performance. On the other hand, a portion of working nodes might be subject to byzantine attacks which could invalidate the learning result. Existing works mostly deal with these two issues separately. In this paper, we propose a new algorithm that equips distributed learning with robustness measures against both distributional shifts and byzantine attacks. Our algorithm is built on recent advances in distributionally robust optimization as well as norm-based screening (NBS), a robust aggregation scheme against byzantine attacks. We provide convergence proofs in three cases of the learning model being nonconvex, convex, and strongly convex for the proposed algorithm, shedding light on its convergence behaviors and endurability against byzantine attacks. In particular, we deduce that any algorithm employing NBS (including ours) cannot converge when the percentage of byzantine nodes is 1/3 or higher, instead of 1/2, which is the common belief in current literature. The experimental results demonstrate the effectiveness of our algorithm against both robustness issues. To the best of our knowledge, this is the first work to address distributional shifts and byzantine attacks simultaneously."}}
{"id": "n3YYXTvdUtn", "cdate": 1609459200000, "mdate": 1682361734169, "content": {"title": "COKE: Communication-Censored Decentralized Kernel Learning", "abstract": "This paper studies the decentralized optimization and learning problem where multiple interconnected agents aim to learn an optimal decision function defined over a reproducing kernel Hilbert space by jointly minimizing a global objective function, with access to their own locally observed dataset. As a non-parametric approach, kernel learning faces a major challenge in distributed implementation: the decision variables of local objective functions are data-dependent and thus cannot be optimized under the decentralized consensus framework without any raw data exchange among agents. To circumvent this major challenge, we leverage the random feature (RF) approximation approach to enable consensus on the function modeled in the RF space by data-independent parameters across different agents. We then design an iterative algorithm, termed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we further develop a communication-censored kernel learning (COKE) algorithm that reduces the communication load of DKLA by preventing an agent from transmitting at every iteration unless its local updates are deemed informative. Theoretical results in terms of linear convergence guarantee and generalization performance analysis of DKLA and COKE are provided. Comprehensive tests on both synthetic and real datasets are conducted to verify the communication efficiency and learning effectiveness of COKE."}}
{"id": "tdyIaJwqEI0", "cdate": 1577836800000, "mdate": 1682361734076, "content": {"title": "DC-CNN: Computational Flow Redefinition for Efficient CNN through Structural Decoupling", "abstract": "Recently Convolutional Neural Networks (CNNs) are widely applied into novel intelligent applications and systems. However, the CNN computation performance is significantly hindered by its computation flow, which computes the model structure sequentially by layers with massive convolution operations. Such a layer-wise sequential computation flow can cause certain performance issues, such as resource under-utilization, huge memory overhead, etc. To solve these problems, we propose a novel CNN structural decoupling method, which could decouple CNN models into \"critical paths\" and eliminate the inter-layer data dependency. Based on this method, we redefine the CNN computation flow into parallel and cascade computing paradigms, which can significantly enhance the CNN computation performance with both multi-core and single-core CPU processors. Experiments show that, our DC-CNN framework could reduce 24% to 33% latency on multi-core CPUs for CIFAR and ImageNet. On small-capacity mobile platforms, cascade computing could reduce the latency by average 24% on ImageNet and 42% on CIFAR10. Meanwhile, the memory reduction could also reach average 21% and 64%, respectively."}}
{"id": "sc1RbZ7Nbe", "cdate": 1546300800000, "mdate": 1682361734109, "content": {"title": "Coke: Communication-Censored Kernel Learning Via Random Features", "abstract": "Distributed kernel-based methods are attractive in nonlinear learning tasks where either a dataset is too large to be processed on a single machine or the data are only locally available to geographically-located sites. For the first case, we propose to split the large dataset into multiple mini-batches and distribute them to distinct sites for parallel learning through the alternating direction method of multipliers (ADMM). For the second case, we develop a decentralized ADMM so that each site can solve the learning task collaboratively through one-hop communications. To circumvent the curse of dimensionality in kernel-based methods, we leverage the random feature approximation to map the large-volume data into a smaller feature space. This also results in a common set of decision parameters that can be exchanged among sites. Motivated by the need to conserve energy and reduce communication overheads, we apply a censoring strategy to evaluate the updated parameter at each site and decide if this update is worth transmitting. The proposed COmmunication-censored KErnel learning (COKE) algorithms are corroborated to be communication-efficient and learning-effective by simulations on both synthetic and real datasets."}}
{"id": "DT782XHLvZg", "cdate": 1546300800000, "mdate": 1682361734154, "content": {"title": "A Class of Distributed Event-Triggered Average Consensus Algorithms for Multi-Agent Systems", "abstract": "This paper proposes a class of distributed event-triggered algorithms that solve the average consensus problem in multi-agent systems. By designing events such that a specifically chosen Lyapunov function is monotonically decreasing, event-triggered algorithms succeed in reducing communications among agents while still ensuring that the entire system converges to the desired state. However, depending on the chosen Lyapunov function the transient behaviors can be very different. Moreover, performance requirements also vary from application to application. Consequently, we are instead interested in considering a class of Lyapunov functions such that each Lyapunov function produces a different event-triggered coordination algorithm to solve the multi-agent average consensus problem. The proposed class of algorithms all guarantee exponential convergence of the resulting system and exclusion of Zeno behaviors. This allows us to easily implement different algorithms that all guarantee correctness to meet varying performance needs. We show that our findings can be applied to the practical clock synchronization problem in wireless sensor networks (WSNs) and further corroborate their effectiveness with simulation results."}}
{"id": "uF_WYQlaZ6o", "cdate": 1514764800000, "mdate": 1682361734053, "content": {"title": "An Energy-Efficient Distributed Average Consensus Scheme via Infrequent Communication", "abstract": "Distributed average consensus finds extensive applications in multi-agent systems where a group of agents are required to agree upon a common value. Motivated by the need to conserve energy for large-scale multi-agent systems, we propose an infrequent communication scheme that allows the system to reach average consensus at low energy consumption. The proposed scheme divides the original large network into two smaller sub-networks with some overlapping nodes. Agents in the two sub-networks take turns to broadcast and update their local estimates, until reaching consensus. We prove that the proposed scheme guarantees asymptotic convergence when both subnetworks are connected and have overlapping nodes. Simulations corroborate the energy saving capability of the proposed scheme."}}
{"id": "35dg_3fAmn", "cdate": 1514764800000, "mdate": 1682361734107, "content": {"title": "A class of event-triggered coordination algorithms for multi-agent systems on weight-balanced digraphs", "abstract": "This paper revisits the multi-agent average consensus problem on weight-balanced directed graphs. In order to reduce communication among the agents, many recent works have considered event-triggered communication and control as a method to reduce communication while still ensuring that the entire network converges to the desired state. One common way to do this is to design events such that a specifically chosen Lyapunov function is monotonically decreasing; however, depending on the chosen Lyapunov function the transient behaviors can be very different. Consequently, we are instead interested in considering a class of Lyapunov functions such that each Lyapunov function produces a different event-triggered coordination algorithm to solve the multi-agent average consensus problem. The proposed class of algorithms all guarantee exponential convergence of the resulting network and exclusion of Zeno behavior. This allows us to easily consider the implementation of different algorithms that all guarantee correctness to be able to meet varying performance needs. Simulations are provided to illustrate our findings."}}
