{"id": "tqnkPOd_7h", "cdate": 1676827065156, "mdate": null, "content": {"title": "Pessimistic Model Selection for Deep Reinforcement Learning", "abstract": "Deep Reinforcement Learning (DRL) has demonstrated great potentials in solving sequential decision making problems in many applications. Despite its promising performance, practical gaps exist when deploying DRL in real-world scenarios. One main barrier is the over-fitting issue that leads to poor generalizability of the policy learned by DRL. In particular, for offline DRL with observational data, model selection is a challenging task as there is no ground truth available for performance demonstration, in contrast with the online setting with simulated environments. In this work, we propose a pessimistic model selection (PMS) approach for offline DRL with a theoretical guarantee, which features a tuning-free framework for finding the best policy among a set of candidate models. Two refined approaches are also proposed to address the potential bias of DRL model in identifying the optimal policy. Numerical studies demonstrated the superior performance of our approach over existing methods."}}
{"id": "cLOpzW3shh7", "cdate": 1672531200000, "mdate": 1682147434231, "content": {"title": "Proximal Causal Learning of Heterogeneous Treatment Effects", "abstract": "Efficiently and flexibly estimating treatment effect heterogeneity is an important task in a wide variety of settings ranging from medicine to marketing, and there are a considerable number of promising conditional average treatment effect estimators currently available. These, however, typically rely on the assumption that the measured covariates are enough to justify conditional exchangeability. We propose the P-learner, motivated by the R- and DR-learner, a tailored two-stage loss function for learning heterogeneous treatment effects in settings where exchangeability given observed covariates is an implausible assumption, and we wish to rely on proxy variables for causal inference. Our proposed estimator can be implemented by off-the-shelf loss-minimizing machine learning methods, which in the case of kernel regression satisfies an oracle bound on the estimated error as long as the nuisance components are estimated reasonably well."}}
{"id": "YqHoOSz18M", "cdate": 1672531200000, "mdate": 1695961967453, "content": {"title": "Proximal Causal Learning of Conditional Average Treatment Effects", "abstract": "Efficiently and flexibly estimating treatment effect heterogeneity is an important task in a wide variety of settings ranging from medicine to marketing, and there are a considerable number of prom..."}}
{"id": "BXgezkJtrix", "cdate": 1672531200000, "mdate": 1695961967450, "content": {"title": "Pessimistic Model Selection for Offline Deep Reinforcement Learning", "abstract": "Deep Reinforcement Learning (DRL) has demonstrated great potentials in solving sequential decision making problems in many applications. Despite its promising performance, practical gaps exist when..."}}
{"id": "UaicS9LPKmZ", "cdate": 1664582400000, "mdate": 1695961967452, "content": {"title": "Dimension Reduction Forests: Local Variable Importance Using Structured Random Forests", "abstract": "Random forests are one of the most popular machine learning methods due to their accuracy and variable importance assessment. However, random forests only provide variable importance in a global se..."}}
{"id": "bYfk8y7BXS", "cdate": 1632875431198, "mdate": null, "content": {"title": "Pessimistic Model Selection for Offline Deep Reinforcement Learning", "abstract": "Deep Reinforcement Learning (DRL) has demonstrated great potentials in solving sequential decision making problems in many applications. Despite its promising performance,  practical gaps exist when deploying DRL in real-world scenarios. One main barrier is the over-fitting issue that leads to poor generalizability of the policy learned by DRL. In particular, for offline DRL with observational data, model selection is a challenging task as there is no ground truth available for performance demonstration, in contrast with the online setting with simulated environments. In this work, we propose a pessimistic model selection (PMS) approach for offline DRL with a theoretical guarantee, which features a tuning-free framework for finding the best policy among a set of candidate models. Two refined approaches are also proposed to address the potential bias of DRL model in identifying the optimal policy. Numerical studies demonstrated the superior performance of our approach over existing methods."}}
{"id": "or5h_z6xL1", "cdate": 1609459200000, "mdate": 1681490808471, "content": {"title": "Pessimistic Model Selection for Offline Deep Reinforcement Learning", "abstract": ""}}
{"id": "G_G3HqyPEVd", "cdate": 1577836800000, "mdate": 1682147434368, "content": {"title": "Estimating heterogeneous treatment effects with right-censored data via causal survival forests", "abstract": "Forest-based methods have recently gained in popularity for non-parametric treatment effect estimation. Building on this line of work, we introduce causal survival forests, which can be used to estimate heterogeneous treatment effects in a survival and observational setting where outcomes may be right-censored. Our approach relies on orthogonal estimating equations to robustly adjust for both censoring and selection effects under unconfoundedness. In our experiments, we find our approach to perform well relative to a number of baselines."}}
