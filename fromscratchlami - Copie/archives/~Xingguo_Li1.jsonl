{"id": "BJl-5pNKDB", "cdate": 1569439113020, "mdate": null, "content": {"title": "On Computation and Generalization of Generative Adversarial Imitation Learning", "abstract": "Generative Adversarial Imitation Learning (GAIL) is a powerful and practical approach for learning sequential decision-making policies. Different from Reinforcement Learning (RL), GAIL takes advantage of demonstration data by experts (e.g., human), and learns both the policy and reward function of the unknown environment. Despite the significant empirical progresses, the theory behind GAIL is still largely unknown. The major difficulty comes from the underlying temporal dependency of the demonstration data and the minimax computational formulation of GAIL without convex-concave structure. To bridge such a gap between theory and practice, this paper investigates the theoretical properties of GAIL. Specifically, we show: (1) For GAIL with general reward parameterization, the generalization can be guaranteed as long as the class of the reward functions is properly controlled; (2) For GAIL, where the reward is parameterized as a reproducing kernel function, GAIL can be efficiently solved by stochastic first order optimization algorithms, which attain sublinear convergence to a stationary solution. To the best of our knowledge, these are the first results on statistical and computational guarantees of imitation learning with reward/policy function ap- proximation. Numerical experiments are provided to support our analysis.\n"}}
{"id": "HJeu43ActQ", "cdate": 1538087983886, "mdate": null, "content": {"title": "NOODL: Provable Online Dictionary Learning and Sparse Coding", "abstract": "We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. Our algorithm, NOODL, is also scalable and amenable for large scale distributed implementations in neural architectures, by which we mean that it only involves simple linear and non-linear operations. Finally, we corroborate these theoretical results via experimental evaluation of the proposed algorithm with the current state-of-the-art techniques."}}
{"id": "Skf-oo0qt7", "cdate": 1538087832960, "mdate": null, "content": {"title": "On Generalization Bounds of a Family of Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) have been widely applied to sequential data analysis. Due to their complicated modeling structures, however, the theory behind is still largely missing. To connect theory and practice, we study the generalization properties of vanilla RNNs as well as their variants, including Minimal Gated Unit (MGU) and Long Short Term Memory (LSTM) RNNs. Specifically, our theory is established under the PAC-Learning framework. The generalization bound is presented in terms of the spectral norms of the weight matrices and the total number of parameters. We also establish refined generalization bounds with additional norm assumptions, and draw a comparison among these bounds. We remark: (1) Our generalization bound for vanilla RNNs is significantly tighter than the best of existing results; (2) We are not aware of any other generalization bounds for MGU and LSTM in the exiting literature; (3) We demonstrate the advantages of these variants in generalization."}}
{"id": "SJzwvoCqF7", "cdate": 1538087775070, "mdate": null, "content": {"title": "On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond", "abstract": "We propose a generalization error bound for a general family of deep neural networks based on the depth and width of the networks, as well as the spectral norm of weight matrices. Through introducing a novel characterization of the Lipschitz properties of neural network family, we achieve a tighter generalization error bound. We further obtain a result that is free of linear dependence on norms for bounded losses. Besides the general deep neural networks, our results can be applied to derive new bounds for several popular architectures, including convolutional neural networks (CNNs), residual networks (ResNets), and hyperspherical networks (SphereNets).  When achieving same generalization errors with previous arts, our bounds allow for the choice of much larger parameter spaces of weight matrices, inducing potentially stronger expressive ability for neural networks."}}
{"id": "ByEh0qZOWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Towards Black-box Iterative Machine Teaching", "abstract": "In this paper, we make an important step towards the black-box machine teaching by considering the cross-space machine teaching, where the teacher and the learner use different feature representati..."}}
{"id": "rk4wXdWO-B", "cdate": 1483228800000, "mdate": null, "content": {"title": "On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning", "abstract": "We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions. Our proposed algorithm integrates the proximal newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming, and enjoys both strong computational and statistical guarantees. Specifically, by leveraging a sophisticated characterization of sparse modeling structures (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations. Numerical experiments are provided to support our theory."}}
{"id": "HJbrEOW_br", "cdate": 1483228800000, "mdate": null, "content": {"title": "Deep Hyperspherical Learning", "abstract": "Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions."}}
{"id": "BJ4sawWdbS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Near Optimal Sketching of Low-Rank Tensor Regression", "abstract": "We study the least squares regression problem $\\min_{\\Theta \\in \\RR^{p_1 \\times \\cdots \\times p_D}} \\| \\cA(\\Theta) - b \\|_2^2$, where $\\Theta$ is a low-rank tensor, defined as $\\Theta = \\sum_{r=1}^{R} \\theta_1^{(r)} \\circ \\cdots \\circ \\theta_D^{(r)}$, for vectors $\\theta_d^{(r)} \\in \\mathbb{R}^{p_d}$ for all $r \\in [R]$ and $d \\in [D]$. %$R$ is small compared with $p_1,\\ldots,p_D$, Here, $\\circ$ denotes the outer product of vectors, and $\\cA(\\Theta)$ is a linear function on $\\Theta$. This problem is motivated by the fact that the number of parameters in $\\Theta$ is only $R \\cdot \\sum_{d=1}^D p_D$, which is significantly smaller than the $\\prod_{d=1}^{D} p_d$ number of parameters in ordinary least squares regression. We consider the above CP decomposition model of tensors $\\Theta$, as well as the Tucker decomposition. For both models we show how to apply data dimensionality reduction techniques based on {\\it sparse} random projections $\\Phi \\in \\RR^{m \\times n}$, with $m \\ll n$, to reduce the problem to a much smaller problem $\\min_{\\Theta} \\|\\Phi \\cA(\\Theta) - \\Phi b\\|_2^2$, for which $\\|\\Phi \\cA(\\Theta) - \\Phi b\\|_2^2 = (1 \\pm \\varepsilon) \\| \\cA(\\Theta) - b \\|_2^2$ holds simultaneously for all $\\Theta$. We obtain a significantly smaller dimension and sparsity in the randomized linear mapping $\\Phi$ than is possible for ordinary least squares regression. Finally, we give a number of numerical simulations supporting our theory."}}
{"id": "rJW0P2-dbr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning", "abstract": "We propose a stochastic variance reduced optimization algorithm for solving a class of large-scale nonconvex optimization problems with cardinality constraints, and provide sufficient conditions un..."}}
