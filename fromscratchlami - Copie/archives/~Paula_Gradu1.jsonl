{"id": "ek2vusIDdTy", "cdate": 1673376176039, "mdate": 1673376176039, "content": {"title": "Non-Stochastic Control with Bandit Feedback", "abstract": "We study the problem of controlling a linear dynamical system with adversarial perturbations where the only feedback available to the controller is the scalar loss, and the loss function itself is unknown. For this problem, with either a known or unknown system, we give an efficient sublinear regret algorithm. The main algorithmic difficulty is the dependence of the loss on past controls. To overcome this issue, we propose an efficient algorithm for the general setting of bandit convex optimization for loss functions with memory, which may be of independent interest."}}
{"id": "jLX7Rr_olo", "cdate": 1673375504035, "mdate": 1673375504035, "content": {"title": "Projection-free Adaptive Regret with Membership Oracles", "abstract": "In the framework of online convex optimization, most iterative algorithms require the computation of projections onto convex sets, which can be computationally expensive. To tackle this problem Hazan and Kale (2012) proposed the study of projection-free methods that replace projections with less expensive computations. The most common approach is based on the Frank-Wolfe method, that uses linear optimization computation in lieu of projections. Recent work by Garber and Kretzu (2022) gave sublinear adaptive regret guarantees with projection free algorithms based on the Frank Wolfe approach.\nIn this work we give projection-free algorithms that are based on a different technique, inspired by Mhammedi (2022), that replaces projections by set-membership computations. We propose a simple lazy gradient-based algorithm with a Minkowski regularization that attains near-optimal adaptive regret bounds. For general convex loss functions we improve previous adaptive regret bounds from $O(T^{3/4})$ to $O(\\sqrt{T})$, and further to tight interval dependent bound $\\tilde{O}(\\sqrt{I})$ where I denotes the interval length. For strongly convex functions we obtain the first poly-logarithmic adaptive regret bounds using a projection-free algorithm."}}
{"id": "-H6LDOXqzN_", "cdate": 1673374661292, "mdate": 1673374661292, "content": {"title": "Adaptive Regret for Control of Time-Varying Dynamics", "abstract": "We consider the problem of online control of systems with time-varying linear dynamics. This is a general formulation that is motivated by the use of local linearization in control of nonlinear dynamical systems. To state meaningful guarantees over changing environments, we introduce the metric of {\\it adaptive regret} to the field of control. This metric, originally studied in online learning, measures performance in terms of regret against the best policy in hindsight on {\\it any interval in time}, and thus captures the adaptation of the controller to changing dynamics.\nOur main contribution is a novel efficient meta-algorithm: it converts a controller with sublinear regret bounds into one with sublinear {\\it adaptive regret} bounds in the setting of time-varying linear dynamical systems. The main technical innovation is the first adaptive regret bound for the more general framework of online convex optimization with memory. Furthermore, we give a lower bound showing that our attained adaptive regret bound is nearly tight for this general framework."}}
{"id": "Z3l8qyKqKSl", "cdate": 1664815578186, "mdate": null, "content": {"title": "Valid Inference after Causal Discovery", "abstract": "Causal graph discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to \"double dipping,\" invalidating coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. One key contribution is a randomized version of the greedy equivalence search (GES) algorithm, which permits a valid, distribution-free correction of classical confidence intervals. We show that a naive combination of causal discovery and subsequent inference algorithms typically leads to highly inflated miscoverage rates; at the same time, our noisy GES method provides reliable coverage control while achieving more accurate causal graph recovery than data splitting."}}
{"id": "Ao2METZY4n", "cdate": 1621629869149, "mdate": null, "content": {"title": "Online Control of Unknown Time-Varying Dynamical Systems", "abstract": "We study online control of time-varying linear systems with unknown dynamics in the nonstochastic control model. At a high level, we demonstrate that this setting is \\emph{qualitatively harder} than that of either unknown time-invariant or known time-varying dynamics, and complement our negative results with algorithmic upper bounds in regimes where sublinear regret is possible. More specifically, we study regret bounds with respect to common classes of policies: Disturbance Action (SLS), Disturbance Response (Youla), and linear feedback policies. While these three classes are essentially equivalent for LTI systems, we demonstrate that these equivalences break down for time-varying systems. \n\nWe prove a lower bound that no algorithm can obtain sublinear regret with respect to the first two classes unless a certain measure of system variability also scales sublinearly in the horizon. Furthermore, we show that offline planning over the state linear feedback policies is NP-hard, suggesting hardness of the online learning problem. \n\nOn the positive side, we give an efficient algorithm that attains a sublinear regret bound against the class of Disturbance Response policies up to the aforementioned system variability term. In fact, our algorithm enjoys sublinear \\emph{adaptive} regret bounds, which is a strictly stronger metric than standard regret and is more appropriate for time-varying systems. We sketch extensions to Disturbance Action policies and partial observation, and propose an inefficient algorithm for regret against linear state feedback policies."}}
