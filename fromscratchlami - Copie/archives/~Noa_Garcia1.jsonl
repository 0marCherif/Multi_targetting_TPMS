{"id": "OgK9XuZhO6", "cdate": 1668387739223, "mdate": 1668387739223, "content": {"title": "Transferring Domain-Agnostic Knowledge in Video Question Answering", "abstract": "Video question answering (VideoQA) is designed to answer a given question based\non a relevant video clip. The current available large-scale datasets have made it possible\nto formulate VideoQA as the joint understanding of visual and language information.\nHowever, this training procedure is costly and still less competent with human performance. In this paper, we investigate a transfer learning method by the introduction of\ndomain-agnostic knowledge and domain-specific knowledge. First, we develop a novel\ntransfer learning framework, which finetunes the pre-trained model by applying domainagnostic knowledge as the medium. Second, we construct a new VideoQA dataset with\n21,412 human-generated question-answer samples for comparable transfer of knowledge. Our experiments show that: (i) domain-agnostic knowledge is transferable and (ii)\nour proposed transfer learning framework can boost VideoQA performance effectively."}}
{"id": "fbs4D_0f40", "cdate": 1668387602984, "mdate": 1668387602984, "content": {"title": "Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions", "abstract": "To understand movies, humans constantly reason over the dialogues and actions shown in specific scenes and relate them to the overall storyline already seen. Inspired by this behaviour, we design ROLL,\na model for knowledge-based video story question answering that leverages three crucial aspects of movie understanding: dialog comprehension,\nscene reasoning, and storyline recalling. In ROLL, each of these tasks is in\ncharge of extracting rich and diverse information by 1) processing scene\ndialogues, 2) generating unsupervised video scene descriptions, and 3)\nobtaining external knowledge in a weakly supervised fashion. To answer\na given question correctly, the information generated by each inspiredcognitive task is encoded via Transformers and fused through a modality\nweighting mechanism, which balances the information from the different sources. Exhaustive evaluation demonstrates the effectiveness of our\napproach, which yields a new state-of-the-art on two challenging video\nquestion answering datasets: KnowIT VQA and TVQA+."}}
{"id": "kXlZCD9Z7B", "cdate": 1668387558845, "mdate": 1668387558845, "content": {"title": "BERT Representations for Video Question Answering", "abstract": "Visual question answering (VQA) aims at answering\nquestions about the visual content of an image or a video.\nCurrently, most work on VQA is focused on image-based\nquestion answering, and less attention has been paid into\nanswering questions about videos. However, VQA in video\npresents some unique challenges that are worth studying:\nit not only requires to model a sequence of visual features\nover time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a\nsequential modelling technique based on Transformers, to\nencode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments,\nwe exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two\nwell-known video VQA datasets: TVQA and Pororo."}}
{"id": "uWnl1lEXbo5", "cdate": 1668387386996, "mdate": 1668387386996, "content": {"title": "Gender and Racial Bias in Visual Question Answering Datasets", "abstract": "Vision-and-language tasks have increasingly drawn more attention\nas a means to evaluate human-like reasoning in machine learning\nmodels. A popular task in the field is visual question answering\n(VQA), which aims to answer questions about images. However,\nVQA models have been shown to exploit language bias by learning\nthe statistical correlations between questions and answers without looking into the image content: e.g., questions about the color\nof a banana are answered with yellow, even if the banana in the\nimage is green. If societal bias (e.g., sexism, racism, ableism, etc.)\nis present in the training data, this problem may be causing VQA\nmodels to learn harmful stereotypes. For this reason, we investigate\ngender and racial bias in five VQA datasets. In our analysis, we\nfind that the distribution of answers is highly different between\nquestions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that\nspecific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets.\nOur findings suggest that there are dangers associated to using\nVQA datasets without considering and dealing with the potentially\nharmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset\ncollection process."}}
{"id": "9OZZPJbF0Qv", "cdate": 1648712992319, "mdate": 1648712992319, "content": {"title": "Quantifying Societal Bias Amplification in Image Captioning", "abstract": "We study societal bias amplification in image captioning. Image captioning models have been shown to perpetuate gender and racial biases, however, metrics to measure, quantify, and evaluate the societal bias in captions are not yet standardized. We provide a comprehensive study on the strengths and limitations of each metric, and propose LIC, a metric to study captioning bias amplification. We argue that, for image captioning, it is not enough to focus on the correct prediction of the protected attribute, and the whole context should be taken into account. We conduct extensive evaluation on traditional and state-of-the-art image captioning models, and surprisingly find that, by only focusing on the protected attribute prediction, bias mitigation models are unexpectedly amplifying bias.\n"}}
{"id": "U_kE8PYg9Bs", "cdate": 1648712426966, "mdate": 1648712426966, "content": {"title": "Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation", "abstract": "Have you ever looked at a painting and wondered what is the story behind it? This work presents a framework to bring art closer to people by generating comprehensive descriptions of fine-art paintings. Generating informative descriptions for artworks, however, is extremely challenging, as it requires to 1) describe multiple aspects of the image such as its style, content, or composition, and 2) provide background and contextual knowledge about the artist, their influences, or the historical period. To address these challenges, we introduce a multi-topic and knowledgeable art description framework, which modules the generated sentences according to three artistic topics and, additionally, enhances each description with external knowledge. The framework is validated through an exhaustive analysis, both quantitative and qualitative, as well as a comparative human evaluation, demonstrating outstanding results in terms of both topic diversity and information veracity."}}
{"id": "ylDvyC9xRH", "cdate": 1640995200000, "mdate": 1668560184520, "content": {"title": "Learning More May Not Be Better: Knowledge Transferability in Vision and Language Tasks", "abstract": "Is more data always better to train vision-and-language models? We study knowledge transferability in multi-modal tasks. The current tendency in machine learning is to assume that by joining multiple datasets from different tasks their overall performance will improve. However, we show that not all the knowledge transfers well or has a positive impact on related tasks, even when they share a common goal. We conduct an exhaustive analysis based on hundreds of cross-experiments on 12 vision-and-language tasks categorized in 4 groups. Whereas tasks in the same group are prone to improve each other, results show that this is not always the case. Other factors such as dataset size or pre-training stage have also a great impact on how well the knowledge is transferred."}}
{"id": "oAy6nLs1-4y", "cdate": 1640995200000, "mdate": 1668560184296, "content": {"title": "Gender and Racial Bias in Visual Question Answering Datasets", "abstract": "Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process."}}
{"id": "PBQdf8ROqK", "cdate": 1640995200000, "mdate": 1668560184513, "content": {"title": "Quantifying Societal Bias Amplification in Image Captioning", "abstract": "We study societal bias amplification in image captioning. Image captioning models have been shown to perpetuate gender and racial biases, however, metrics to measure, quantify, and evaluate the societal bias in captions are not yet standardized. We provide a comprehensive study on the strengths and limitations of each metric, and propose LIC, a metric to study captioning bias amplification. We argue that, for image captioning, it is not enough to focus on the correct prediction of the protected attribute, and the whole context should be taken into account. We conduct extensive evaluation on traditional and state-of-the-art image captioning models, and surprisingly find that, by only focusing on the protected attribute prediction, bias mitigation models are unexpectedly amplifying bias."}}
{"id": "AjJFG0-Ejj", "cdate": 1640995200000, "mdate": 1668560184547, "content": {"title": "The semantic typology of visually grounded paraphrases", "abstract": ""}}
