{"id": "lyXUbh4adh", "cdate": 1690848000000, "mdate": 1699893896218, "content": {"title": "A Vision-Based Method for Real-Time Traffic Flow Estimation on Edge Devices", "abstract": "Traffic flow estimation is an essential task in modern intelligent transportation systems. Many types of information, including vehicle type, vehicle totals, and movement direction, are vital for mitigating transportation-related tasks and effective traffic control strategies. With the development of embedded devices, systems can process captured video at the edge instead of transferring data to centralized processing servers. This paper proposes a real-time and edge-based traffic flow estimation system. The proposed system follows a detect-and-track mechanism where lightweight deep learning models perform vehicle detection. A novel scenario-based tracking and counting technique is developed to provide multi-class, multi-movement vehicle counting. The method uses predefined regions to assign the movement for each vehicle initially detected. It then performs spatial-temporal trajectory matching between the vehicle trajectory and the movement path throughout the whole video. Extensive experiments have shown that the proposed method achieves high effectiveness with multiple camera types and viewpoints."}}
{"id": "_YbVn3IkTTV", "cdate": 1672531200000, "mdate": 1699893896218, "content": {"title": "Robust Automatic Motorcycle Helmet Violation Detection for an Intelligent Transportation System", "abstract": "Video surveillance-based automatic detection of motorcycle helmet usage can enhance the effectiveness of educational and enforcement initiatives aimed at boosting road safety. Current detection methods, however, have room for enhancement, such as the inability to pinpoint individual motorcycles or differentiate between drivers and passengers in terms of helmet usage. This paper introduces a framework designed to detect and identify individual motorcycles while recording specific helmet usage for riders. The proposed classification approach for helmet usage demonstrates increased efficiency in comparison to previous research. Our findings highlight the exceptional accuracy of deep learning, with our method achieving a score of 0.7754 on the AI City 2023 Challenge Track 5 public leaderboard."}}
{"id": "ViC0qGrqHf", "cdate": 1672531200000, "mdate": 1699893896240, "content": {"title": "Improving Deep Learning-based Automatic Checkout System Using Image Enhancement Techniques", "abstract": "The retail sector has experienced significant growth in artificial intelligence and computer vision applications, particularly with the emergence of automatic checkout (ACO) systems in stores and supermarkets. ACO systems encounter challenges such as object occlusion, motion blur, and similarity between scanned items while acquiring accurate training images for realistic checkout scenarios is difficult due to constant product updates. This paper improves existing deep learning-based ACO solutions by incorporating several image enhancement techniques in the data pre-processing step. The proposed ACO system employs a detect-and-track strategy, which involves: (1) detecting objects in areas of interest; (2) tracking objects in consecutive frames; and (3) counting objects using a track management pipeline. Several data generation techniques\u2014including copy-and-paste, random placement, and augmentation\u2014are employed to create diverse training data. Additionally, the proposed solution is designed as an open-ended framework that can be easily expanded to accommodate multiple tasks. The system has been evaluated on the AI City Challenge 2023 Track 4 dataset, showcasing outstanding performance by achieving a top-1 ranking on test-set A with an F1 score of 0.9792."}}
{"id": "wUC3tWgqNh", "cdate": 1640995200000, "mdate": 1667528261563, "content": {"title": "A Robust Traffic-Aware City-Scale Multi-Camera Vehicle Tracking Of Vehicles", "abstract": "Multi-Target Multi-Camera Tracking (MTMC) has an immense domain of Intelligent Traffic Surveillance System applications. Multifarious tasks manage to apply MTMC trackings, such as crowd analysis and city-scale traffic management. This paper describes our framework using spatial constraints for the Task of the Track 1 multi-camera vehicle tracking in the 2022 AI City Challenge. The framework includes single-camera detection and tracking, vehicle re-identification, and multi-camera track matching. To improve the system\u2019s accuracy, we proposed Region-Aware for the precision of vehicle detection and tracking, leading to the effective service of vehicle re-identification models to extract targets and appearance features. We use Crossing-Aware for a tracker to utilize the rich feature to find the tracklets and operate trajectory matching for multi-camera tracklets connection. Finally, the Inter-Camera Matching generated the global IDs for vehicle trajectory. Our method acquired an IDF1 score of 0.8129 on the AI City 2022 Challenge Track 1 public leaderboard."}}
{"id": "fQLC-ntflB", "cdate": 1640995200000, "mdate": 1667528261580, "content": {"title": "DeepACO: A Robust Deep Learning-based Automatic Checkout System", "abstract": "The retail industry has seen an increasing growth of artificial intelligence and computer vision applications. Of the various topics, automatic checkout (ACO) in retail stores or supermarkets has emerged as one of the critical tasks in this area. Several problems stem from real-world scenarios such as object occlusion, blurring from scanning motion, and similarity in scanned items. Moreover, the challenge also comes from the difficulty of collecting training images that reflect the realistic checkout scenarios due to continuous updates of the products. This paper proposes a deep learning-based automatic checkout system (DeepACO) to recognize, localize, track, and count products as they move along a retail check-out conveyor belt. The DeepACO follows the detect-and-track approach, i.e., applying trackers on detected bounding boxes. It also provides a completed pipeline for generating large training datasets under various environments from synthetic data. The proposed system has been evaluated on the 2022 AI City Challenge Track 4 benchmark. Compared to other state-of-the-art solutions, it has shown outstanding results, achieving top-2 on the test-set A with the F1 score of 0.4783."}}
{"id": "fOijPT4uG4", "cdate": 1640995200000, "mdate": 1667528261572, "content": {"title": "Robust Uncalibrated Rectification With Low Geometric Distortion Under Unbalanced Field of View Circumstances", "abstract": "Rectification is a standard process in every system that requires multiviews. Existing algorithms largely work on similar field of view (FoV) cases where the two views are mostly identical. Dissimilarities between different FoVs can generate unexpected errors during the optimization process, resulting in a large amount of rectification errors and unwanted geometric distortion. In this study, we present a full pipeline to rectify uncalibrated images captured by cameras that have dissimilar FoVs under the constraints of geometric distortion. The proposed method contains two main parts: Field of View Neutralization and Rectification with Adaptive Geometric Constraints. The Field of View Neutralization module estimates the transformation matrix to compensate for the imbalance between different views. In addition, this module improves the overall quality of correspondences by removing misleading feature matching pairs. Finally, by applying an adaptive optimization process with the geometric constraints involved, our method addresses the overdistortion issue while maintaining small rectification errors. Extensive experiments are conducted to demonstrate the robust performance of the proposed method. Besides the existing datasets, we provide our dissimilar FoVs dataset with multiple baselines to examine the performance. Our method outperforms the existing algorithms in terms of both rectification errors and geometric distortion rates."}}
{"id": "K7QdcH7Ch9F", "cdate": 1640995200000, "mdate": 1667528261563, "content": {"title": "Universal Detection-Based Driving Assistance Using a Mono Camera With Jetson Devices", "abstract": "Advanced Driver Assistance Systems (ADAS) are a collection of intelligent solutions integrated into next-generation vehicles to assist in safe driving. When building ADAS systems, the main goals are that they are stable, flexible, easy to maintain, and allow for error tracing. If a driving assistance algorithm is designed to be implemented on one machine or in one model, there is a potential disadvantage that if one component fails, then the entire system would stop. We work on modularizing the ADAS system to be flexible to accommodate any changes or improvements based on up-to-date requirements. Using advanced current edge (or network) devices, we propose a Detection-based Driving Assistance algorithm, which can collaborate or integrate with an existing system in a vehicle. The core of any process is to ensure that the system has a predictable level of functionality and that any misbehavior can be easily traced to the root cause. The proposed system shows fast, real-time performance on edge devices with limited computing power."}}
{"id": "-XQI2Ki7_wt", "cdate": 1640995200000, "mdate": 1699893896246, "content": {"title": "Camera-wise Training for Enhanced Omni-directional 2D Object Detection", "abstract": "In this paper, we propose a method to perform training and inference with multiple instances of the same deep neural network architecture on images taken from cameras of different directions. Across multiple cameras, depending on each of their directional characteristics, objects viewed from the cameras can form slightly different distributions in visual features. Regarding this, we emphasize the importance of camera-wise training on multiple instances of a given deep neural network for object detection. Given the Waymo Open Perception Dataset, we used multiple instances of the YOLOv5x6 architecture and trained each of them per camera. Such a training scheme on the Training Set achieves better training progression, and the inference results are shown to have AP/L1 as high as 0.6679 on the Testing Set."}}
{"id": "hhJdfBbv3P", "cdate": 1609459200000, "mdate": 1667528261575, "content": {"title": "A Region-and-Trajectory Movement Matching for Multiple Turn-Counts at Road Intersection on Edge Device", "abstract": "In intelligent traffic systems, vehicle detection and counting have become an important task. The counting information is essential for reducing traffic congestion and improving traffic signal capability. Traditional methods have been focusing on counting vehicles in a single frame or consecutive frames. However, they have not yet considered the movement of interest (MOI) of the vehicles moving in different lanes and directions. This paper proposes a region-and-trajectory movement matching method that aims to detect and count vehicles for each movement on the road. First, the YOLOv5 detection model is used to detect candidate vehicles in the region of interest (ROI). Second, the SORT tracking method associates vehicles of the same instance in consecutive images to create tracked trajectories. Then, the counting method using the combination of MOI regions and predefined movement tracks. Each tracked trajectory is assigned to the corresponding movement id and is outputted to the result file. The efficiency and effectiveness of the proposed method have been evaluated and ranked 3rd on AI City Challenge 2021 Track 1 leaderboard. Further experiments showed that the method could achieve around 120 fps on an NVIDIA Quadro RTX 8000 and 20 fps on an NVIDIA Jetson Xavier AGX."}}
{"id": "LEWXbqHUIcf", "cdate": 1609459200000, "mdate": 1667528261577, "content": {"title": "TSS-Net: Time-based Semantic Segmentation Neural Network for Road Scene Understanding", "abstract": "In this research, a multitask convolutional neural network that can do end-to-end road scene classification and semantic segmentation, which are the two crucial tasks for advanced driver assistance systems (ADAS), is proposed. We name the network TSS which means time-based semantic segmentation. The network contains three main modules: an image encoder, a scene classifier, and two time-based segmentation decoders. For each road scene image, the encoder extracts image features which will be used for classifier and decoders. Next, the image features are fed to the classifier to predict the scene type (in this case a day or a night scene). Then, based on the predicted scene type, the same extracted features are fed to a corresponding segmentation decoder to produce the final semantic segmentation result. By using this classification-driven decoder approach, we can improve the accuracy of the segmentation model, even when the model has been trained excessively earlier. Through the experiment, the validity of our proposed method has been proven. Our approach can be considered as stacking multiple segmentation modules on top of the classification module with all of them share the same image encoder. With this approach, we can utilize the result from classification to gain more accuracy in segmentation in one feed forward only."}}
