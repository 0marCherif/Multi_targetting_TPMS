{"id": "etyCRmK3S0", "cdate": 1640995200000, "mdate": 1667551492260, "content": {"title": "ML4CO-KIDA: Knowledge Inheritance in Data Aggregation", "abstract": "The Machine Learning for Combinatorial Optimization (ML4CO) NeurIPS 2021 competition aims to improve state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning models. On the dual task, we design models to make branching decisions to promote the dual bound increase faster. We propose a knowledge inheritance method to generalize knowledge of different models from the dataset aggregation process, named KIDA. Our improvement overcomes some defects of the baseline graph-neural-networks-based methods. Further, we won the $1$\\textsuperscript{st} Place on the dual task. We hope this report can provide useful experience for developers and researchers. The code is available at https://github.com/megvii-research/NeurIPS2021-ML4CO-KIDA."}}
{"id": "X3wfy5o1fp", "cdate": 1640995200000, "mdate": 1667551492262, "content": {"title": "Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer", "abstract": "This paper reports our solution for ACM Multimedia ViCo 2022 Conversational Head Generation Challenge, which aims to generate vivid face-to-face conversation videos based on audio and reference images. Our solution focuses on training a generalized audio-to-head driver using regularization and assembling a high-visual quality renderer. We carefully tweak the audio-to-behavior model and post-process the generated video using our foreground-background fusion module. We get first place in the listening head generation track and second place in the talking head generation track on the official leaderboard. Our code is available at https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration."}}
{"id": "WdiiCGcxDG", "cdate": 1640995200000, "mdate": 1667551492250, "content": {"title": "Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer", "abstract": "This paper reports our solution for ACM Multimedia ViCo 2022 Conversational Head Generation Challenge, which aims to generate vivid face-to-face conversation videos based on audio and reference images. Our solution focuses on training a generalized audio-to-head driver using regularization and assembling a high visual quality renderer. We carefully tweak the audio-to-behavior model and post-process the generated video using our foreground-background fusion module. We get first place in the listening head generation track and second place in the talking head generation track in the official leaderboard. Our code is available at https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration."}}
{"id": "KksQtWbGIdi", "cdate": 1640995200000, "mdate": 1667551492258, "content": {"title": "Real-Time Intermediate Flow Estimation for Video Frame Interpolation", "abstract": ""}}
{"id": "G2D5IJMNJf", "cdate": 1640995200000, "mdate": 1667551492263, "content": {"title": "Collaborative Neural Rendering using Anime Character Sheets", "abstract": "Drawing images of characters with desired poses is an essential but laborious task in anime production. In this paper, we present the Collaborative Neural Rendering (CoNR) method, which creates new images for specified poses from a few reference images (AKA Character Sheets). In general, the high diversity of body shapes of anime characters defies the employment of universal body models like SMPL, which are developed from real-world humans. To overcome this difficulty, CoNR uses a compact and easy-to-obtain landmark encoding to avoid creating a unified UV mapping in the pipeline. In addition, the performance of CoNR can be significantly improved when referring to multiple reference images, thanks to feature space cross-view warping in a carefully designed neural network. Moreover, we have collected a character sheet dataset containing over 700,000 hand-drawn and synthesized images of diverse poses to facilitate research in this area. Our code and demo are available at https://github.com/megvii-research/CoNR."}}
{"id": "uRmUKLqJ7FT", "cdate": 1617670671490, "mdate": null, "content": {"title": "Learning to Paint With Model-based Deep Reinforcement Learning", "abstract": "We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at this https URL. "}}
{"id": "i7PTCvA3hxX", "cdate": 1609459200000, "mdate": 1667551492250, "content": {"title": "The Machine Learning for Combinatorial Optimization Competition (ML4CO): Results and Insights", "abstract": "Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that ..."}}
{"id": "rDXPJtFT-D", "cdate": 1546300800000, "mdate": 1667551492259, "content": {"title": "Learning to Paint With Model-Based Deep Reinforcement Learning", "abstract": "We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint."}}
{"id": "idK_d9NMpu", "cdate": 1483228800000, "mdate": 1667551492251, "content": {"title": "Learning to Run with Actor-Critic Ensemble", "abstract": "We introduce an Actor-Critic Ensemble(ACE) method for improving the performance of Deep Deterministic Policy Gradient(DDPG) algorithm. At inference time, our method uses a critic ensemble to select the best action from proposals of multiple actors running in parallel. By having a larger candidate set, our method can avoid actions that have fatal consequences, while staying deterministic. Using ACE, we have won the 2nd place in NIPS'17 Learning to Run competition, under the name of \"Megvii-hzwer\"."}}
