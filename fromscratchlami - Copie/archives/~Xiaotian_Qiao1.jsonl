{"id": "uU7_9OEjNaC", "cdate": 1672531200000, "mdate": 1695953516187, "content": {"title": "Design Order Guided Visual Note Layout Optimization", "abstract": "With the goal of making contents easy to understand, memorize and share, a clear and easy-to-follow layout is important for visual notes. Unfortunately, since visual notes are often taken by the designers in real time while watching a video or listening to a presentation, the contents are usually not carefully structured, resulting in layouts that may be difficult for others to follow. In this article, we address this problem by proposing a novel approach to automatically optimize the layouts of visual notes. Our approach predicts the design order of a visual note and then warps the contents along the predicted design order such that the visual note can be easier to follow and understand. At the core of our approach is a learning-based framework to reason about the element-wise design orders of visual notes. In particular, we first propose a hierarchical LSTM-based architecture to predict a grid-based design order of the visual note, based on the graphical and textual information. We then derive the element-wise order from the grid-based prediction. Such an idea allows our network to be weakly-supervised, i.e., making it possible to predict dense grid-based orders from visual notes with only coarse annotations. We evaluate the effectiveness of our approach on visual notes with diverse content densities and layouts. The results show that our network can predict plausible design orders for various types of visual notes and our approach can effectively optimize their layouts in order for them to be easier to follow."}}
{"id": "yrPLh1XyKAe", "cdate": 1640995200000, "mdate": 1667350635481, "content": {"title": "Learning Object Context for Novel-view Scene Layout Generation", "abstract": "Novel-view prediction of a scene has many applications. Existing works mainly focus on generating novel-view images via pixel-wise prediction in the image space, often resulting in severe ghosting and blurry artifacts. In this paper, we make the first attempt to explore novel-view prediction in the layout space, and introduce the new problem of novel-view scene layout generation. Given a single scene layout and the camera transformation as inputs, our goal is to generate a plausible scene layout for a specified viewpoint. Such a problem is challenging as it involves accurate understanding of the 3D geometry and semantics of the scene from as little as a single 2D scene layout. To tackle this challenging problem, we propose a deep model to capture contextualized object representation by explicitly modeling the object context transformation in the scene. The contextualized object representation is essential in generating geometrically and semantically consistent scene layouts of different views. Experiments show that our model outperforms several strong baselines on many indoor and outdoor scenes, both qualitatively and quantitatively. We also show that our model enables a wide range of applications, including novel-view image synthesis, novel-view image editing, and amodal object estimation."}}
{"id": "b3AH8aep8B", "cdate": 1640995200000, "mdate": 1667350635477, "content": {"title": "Instance-Aware Scene Layout Forecasting", "abstract": "Forecasting scene layout is of vital importance in many vision applications, e.g., enabling autonomous vehicles to plan actions early. It is a challenging problem as it involves understanding of the past scene layouts and the diverse object interactions in the scene, and then forecasting what the scene will look like at a future time. Prior works learn a direct mapping from past pixels to future pixel-wise labels and ignore the underlying object interactions in the scene, resulting in temporally incoherent and averaged predictions. In this paper, we propose a learning framework to forecast semantic scene layouts (represented by instance maps) from an instance-aware perspective. Specifically, our framework explicitly models the dynamics of individual instances and captures their interactions in a scene. Under this formulation, we are able to enforce instance-level constraints to forecast scene layouts by effectively reasoning about their spatial and semantic relations. Experimental results show that our model can predict sharper and more accurate future instance maps than the baselines and prior methods, yielding state-of-the-art performances on short-term, mid-term and long-term scene layout forecasting."}}
{"id": "_ly445pdu4l", "cdate": 1640995200000, "mdate": 1667350635480, "content": {"title": "Object-Level Scene Context Prediction", "abstract": "Contextual information plays an important role in solving various image and scene understanding tasks. Prior works have focused on the extraction of contextual information from an image and use it to infer the properties of some object(s) in the image or understand the scene behind the image, e.g., context-based object detection, recognition and semantic segmentation. In this paper, we consider an inverse problem, i.e., how to hallucinate the missing contextual information from the properties of standalone objects. We refer to it as object-level scene context prediction. This problem is difficult, as it requires extensive knowledge of the complex and diverse relationships among objects in the scene. We propose a deep neural network, which takes as input the properties (i.e., category, shape, and position) of a few standalone objects to predict an object-level scene layout that compactly encodes the semantics and structure of the scene context where the given objects are. Quantitative experiments and user studies demonstrate that our model can generate more plausible scene contexts than the baselines. Our model also enables the synthesis of realistic scene images from partial scene layouts. Finally, we validate that our model internally learns useful features for scene recognition and fake scene detection."}}
{"id": "_2gZyq6ffw", "cdate": 1609459200000, "mdate": 1667350635484, "content": {"title": "Distilling Reflection Dynamics for Single-Image Reflection Removal", "abstract": "Single-image reflection removal (SIRR) aims to restore the transmitted image given a single image shot through glass or window. Existing methods rely mainly on information extracted from a single image along with some predefined priors, and fail to give satisfying results on real-world images, due to inherent ambiguity and lack of large and diverse real-world training data. In this paper, instead of reasoning about a single image only, we propose to distill a representation of reflection dynamics from multi-view images (i.e., the motions of reflection and transmission layers over time), and transfer the learned knowledge for the SIRR problem. In particular, we propose a teacher-student framework where the teacher network learns a representation of reflection dynamics by watching a sequence of multi-view images of a scene captured by a moving camera and teaches a student network to remove reflection from a single input image. In addition, we collect a large real-world multi-view reflection image dataset for reflection dynamics knowledge distillation. Extensive experiments show that our model yields state-of-the-art performances."}}
{"id": "46oEWLGBoHT", "cdate": 1609459200000, "mdate": 1667350635491, "content": {"title": "Light Source Guided Single-Image Flare Removal from Unpaired Data", "abstract": "Causally-taken images often suffer from flare artifacts, due to the unintended reflections and scattering of light inside the camera. However, as flares may appear in a variety of shapes, positions, and colors, detecting and removing them entirely from an image is very challenging. Existing methods rely on predefined intensity and geometry priors of flares, and may fail to distinguish the difference between light sources and flare artifacts. We observe that the conditions of the light source in the image play an important role in the resulting flares. In this paper, we present a deep framework with light source aware guidance for single-image flare removal (SIFR). In particular, we first detect the light source regions and the flare regions separately, and then remove the flare artifacts based on the light source aware guidance. By learning the underlying relationships between the two types of regions, our approach can remove different kinds of flares from the image. In addition, instead of using paired training data which are difficult to collect, we propose the first unpaired flare removal dataset and new cycle-consistency constraints to obtain more diverse examples and avoid manual annotations. Extensive experiments demonstrate that our method outperforms the baselines qualitatively and quantitatively. We also show that our model can be applied to flare effect manipulation (e.g., adding or changing image flares)."}}
{"id": "QRcj4pXTqTH", "cdate": 1546300800000, "mdate": 1667350635502, "content": {"title": "Distraction-Aware Shadow Detection", "abstract": "Shadow detection is an important and challenging task for scene understanding. Despite promising results from recent deep learning based methods. Existing works still struggle with ambiguous cases where the visual appearances of shadow and non-shadow regions are similar (referred to as distraction in our context). In this paper, we propose a Distraction-aware Shadow Detection Network (DSDNet) by explicitly learning and integrating the semantics of visual distraction regions in an end-to-end framework. At the core of our framework is a novel standalone, differentiable Distraction-aware Shadow (DS) module, which allows us to learn distraction-aware, discriminative features for robust shadow detection, by explicitly predicting false positives and false negatives. We conduct extensive experiments on three public shadow detection datasets, SBU, UCF and ISTD, to evaluate our method. Experimental results demonstrate that our model can boost shadow detection performance, by effectively suppressing the detection of false positives and false negatives, achieving state-of-the-art results."}}
{"id": "GHd6Qx-u6pW", "cdate": 1546300800000, "mdate": 1667350635518, "content": {"title": "Tell Me Where I Am: Object-Level Scene Context Prediction", "abstract": "Contextual information has been shown to be effective in helping solve various image understanding tasks. Previous works have focused on the extraction of contextual information from an image and use it to infer the properties of some object(s) in the image. In this paper, we consider an inverse problem of how to hallucinate missing contextual information from the properties of a few standalone objects. We refer to it as scene context prediction. This problem is difficult as it requires an extensive knowledge of complex and diverse relationships among different objects in natural scenes. We propose a convolutional neural network, which takes as input the properties (i.e., category, shape, and position) of a few standalone objects to predict an object-level scene layout that compactly encodes the semantics and structure of the scene context where the given objects are. Our quantitative experiments and user studies show that our model can generate more plausible scene context than the baseline approach. We demonstrate that our model allows for the synthesis of realistic scene images from just partial scene layouts and internally learns useful features for scene recognition."}}
{"id": "1He8xIAHfhF", "cdate": 1546300800000, "mdate": 1667350635516, "content": {"title": "Content-aware generative modeling of graphic design layouts", "abstract": "Layout is fundamental to graphic designs. For visual attractiveness and efficient communication of messages and ideas, graphic design layouts often have great variation, driven by the contents to be presented. In this paper, we study the problem of content-aware graphic design layout generation. We propose a deep generative model for graphic design layouts that is able to synthesize layout designs based on the visual and textual semantics of user inputs. Unlike previous approaches that are oblivious to the input contents and rely on heuristic criteria, our model captures the effect of visual and textual contents on layouts, and implicitly learns complex layout structure variations from data without the use of any heuristic rules. To train our model, we build a large-scale magazine layout dataset with fine-grained layout annotations and keyword labeling. Experimental results show that our model can synthesize high-quality layouts based on the visual semantics of input images and keyword-based summary of input text. We also demonstrate that our model internally learns powerful features that capture the subtle interaction between contents and layouts, which are useful for layout-aware design retrieval."}}
{"id": "1AvXRv9L9vk", "cdate": 1420070400000, "mdate": 1695953516186, "content": {"title": "Undersampled Dynamic MRI Reconstruction by Double Sparse Spatiotemporal Dictionary", "abstract": "Dynamic magnetic resonance imaging (dMRI) is widely used in human motion organ and functional imaging. But it requires reducing the imaging time to obtain high spatial and temporal resolution. This paper proposes a double sparse spatiotemporal dictionary model for compressed sensing reconstruction of dMRI from undersampled data. The model extends the ordinary 2-D dictionary to 3-D spatiotemporal dictionary by sparse representation of both the signals and dictionary atoms. Specifically, the first level sparse representation of dictionary atoms is learned with K-SVD algorithm. The second level sparse representation of spatiotemporal patches is obtained by OMP algorithm. An alternate iterative optimization is applied to solve the problem. Experiment results demonstrate that comparing with the state of the art method k-t FOCUSS and single level dictionary learning \u2013 DLMRI, the proposed method performs better in removing aliasing artifacts and in capturing temporal variations as well."}}
