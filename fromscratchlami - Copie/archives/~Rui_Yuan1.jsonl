{"id": "Rh429iw7d_C", "cdate": 1685532017749, "mdate": null, "content": {"title": "A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence", "abstract": "Modern policy optimization methods in reinforcement learning, such as Trust Region Policy Optimization and Proximal Policy Optimization, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parametrization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parametrizations. The policy class induced by our scheme recovers known classes, e.g.\\ softmax, and generates new ones depending on the choice of mirror map. For our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parametrization. To demonstrate the ability of our framework to accommodate general parametrization schemes, we obtain its sample complexity when using shallow neural networks and show that it represents an improvement upon the previous best results."}}
{"id": "-Iy4wy7SuO7", "cdate": 1672531200000, "mdate": 1683882227655, "content": {"title": "A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence", "abstract": "Modern policy optimization methods in applied reinforcement learning, such as Trust Region Policy Optimization and Policy Mirror Descent, are often based on the policy gradient framework. While theoretical guarantees have been established for this class of algorithms, particularly in the tabular setting, the use of a general parametrization scheme remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parametrizations. The policy class induced by our scheme recovers known classes, e.g. softmax, and it generates new ones, depending on the choice of the mirror map. For a general mirror map and parametrization class, we establish the quasi-monotonicity of the updates in value function, global linear convergence rates, and we bound the total expected Bregman divergence of the algorithm along its path. To showcase the ability of our framework to accommodate general parametrization schemes, we present a case study involving shallow neural networks."}}
{"id": "-z9hdsyUwVQ", "cdate": 1663849857447, "mdate": null, "content": {"title": "Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies", "abstract": "We consider infinite-horizon discounted Markov decision processes and study the convergence rates of the natural policy gradient (NPG) and the Q-NPG methods with the log-linear policy class. Using the compatible function approximation framework, both methods with log-linear policies can be written as approximate versions of the policy mirror descent (PMD) method. We show that both methods attain linear convergence rates and $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexities using a simple, non-adaptive geometrically increasing step size, without resorting to entropy or other strongly convex regularization. Lastly, as a byproduct, we obtain sublinear convergence rates for both methods with arbitrary constant step size."}}
{"id": "kWeMzs7uCU", "cdate": 1640995200000, "mdate": 1683882227823, "content": {"title": "Sketched Newton-Raphson", "abstract": ""}}
{"id": "iGNjFvEeSw", "cdate": 1640995200000, "mdate": 1683882227892, "content": {"title": "A general sample complexity analysis of vanilla policy gradient", "abstract": "We adapt recent tools developed for the analysis of Stochastic Gradient Descent (SGD) in non-convex optimization to obtain convergence and sample complexity guarantees for the vanilla policy gradient (PG). Our only assumptions are that the expected return is smooth w.r.t. the policy parameters, that its $H$-step truncated gradient is close to the exact gradient, and a certain ABC assumption. This assumption requires the second moment of the estimated gradient to be bounded by $A \\geq 0$ times the suboptimality gap, $B \\geq 0$ times the norm of the full batch gradient and an additive constant $C \\geq 0$, or any combination of aforementioned. We show that the ABC assumption is more general than the commonly used assumptions on the policy space to prove convergence to a stationary point. We provide a single convergence theorem that recovers the $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$ sample complexity of PG. Our results also affords greater flexibility in the choice of hyper parameters such as the step size and places no restriction on the batch size $m$, including the single trajectory case (i.e., $m=1$). We then instantiate our theorem in different settings, where we both recover existing results and obtained improved sample complexity, e.g., for convergence to the global optimum for Fisher-non-degenerated parameterized policies."}}
{"id": "RQEBTPVqt1k", "cdate": 1640995200000, "mdate": 1683882227702, "content": {"title": "SAN: Stochastic Average Newton Algorithm for Minimizing Finite Sums", "abstract": "We present a principled approach for designing stochastic Newton methods for solving finite sum optimization problems. Our approach has two steps. First, we re-write the stationarity conditions as a system of nonlinear equations that associates each data point to a new row. Second, we apply a Subsampled Newton Raphson method to solve this system of nonlinear equations. Using our approach, we develop a new Stochastic Average Newton (SAN) method, which is incremental by design, in that it requires only a single data point per iteration. It is also cheap to implement when solving regularized generalized linear models, with a cost per iteration of the order of the number of the parameters. We show through extensive numerical experiments that SAN requires no knowledge about the problem, neither parameter tuning, while remaining competitive as compared to classical variance reduced gradient methods (e.g. SAG and SVRG), incremental Newton and quasi-Newton methods (e.g. SNM, IQN)."}}
{"id": "C6rniFSgQT", "cdate": 1640995200000, "mdate": 1683882227883, "content": {"title": "Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies", "abstract": "We consider infinite-horizon discounted Markov decision processes and study the convergence rates of the natural policy gradient (NPG) and the Q-NPG methods with the log-linear policy class. Using the compatible function approximation framework, both methods with log-linear policies can be written as inexact versions of the policy mirror descent (PMD) method. We show that both methods attain linear convergence rates and $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexities using a simple, non-adaptive geometrically increasing step size, without resorting to entropy or other strongly convex regularization. Lastly, as a byproduct, we obtain sublinear convergence rates for both methods with arbitrary constant step size."}}
{"id": "9rr7pFeIEuq", "cdate": 1632875465901, "mdate": null, "content": {"title": "A general sample complexity analysis of vanilla policy gradient", "abstract": "We adapt recent tools developed for the analysis of Stochastic Gradient Descent (SGD) in non-convex optimization to obtain convergence guarantees and sample complexities for the vanilla policy gradient (PG) -- REINFORCE and GPOMDP. Our only assumptions are that the expected return is smooth w.r.t. the policy parameters and that the second moment of its gradient satisfies a certain ABC assumption. The ABC assumption allows for the second moment of the gradient to be bounded by $A\\geq 0$ times the suboptimality gap,  $B \\geq 0$ times the norm of the full batch gradient and an additive constant $C \\geq 0$, or any combination of aforementioned. We show that the ABC assumption is more general than the commonly used assumptions on the policy space to prove convergence to a stationary point. We provide a single convergence theorem under the ABC assumption, and show that, despite the generality of the ABC assumption, we recover the $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$ sample complexity of PG. Our convergence theorem also affords greater flexibility in the choice of hyper parameters such as the step size and places no restriction on the batch size $m$. Even the  single trajectory case (i.e., $m=1$) fits within our analysis. We believe that the generality of the ABC assumption may provide theoretical guarantees for PG to a much broader range of problems that have not been previously considered."}}
{"id": "cOgMPzdJDDd", "cdate": 1609459200000, "mdate": 1683882227763, "content": {"title": "A general sample complexity analysis of vanilla policy gradient", "abstract": "We adapt recent tools developed for the analysis of Stochastic Gradient Descent (SGD) in non-convex optimization to obtain convergence and sample complexity guarantees for the vanilla policy gradient (PG). Our only assumptions are that the expected return is smooth w.r.t. the policy parameters, that its $H$-step truncated gradient is close to the exact gradient, and a certain ABC assumption. This assumption requires the second moment of the estimated gradient to be bounded by $A\\geq 0$ times the suboptimality gap, $B \\geq 0$ times the norm of the full batch gradient and an additive constant $C \\geq 0$, or any combination of aforementioned. We show that the ABC assumption is more general than the commonly used assumptions on the policy space to prove convergence to a stationary point. We provide a single convergence theorem that recovers the $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$ sample complexity of PG to a stationary point. Our results also affords greater flexibility in the choice of hyper parameters such as the step size and the batch size $m$, including the single trajectory case (i.e., $m=1$). When an additional relaxed weak gradient domination assumption is available, we establish a novel global optimum convergence theory of PG with $\\widetilde{\\mathcal{O}}(\\epsilon^{-3})$ sample complexity. We then instantiate our theorems in different settings, where we both recover existing results and obtain improved sample complexity, e.g., $\\widetilde{\\mathcal{O}}(\\epsilon^{-3})$ sample complexity for the convergence to the global optimum for Fisher-non-degenerated parametrized policies."}}
{"id": "9pD47jLzFi", "cdate": 1609459200000, "mdate": 1683882227666, "content": {"title": "SAN: Stochastic Average Newton Algorithm for Minimizing Finite Sums", "abstract": "We present a principled approach for designing stochastic Newton methods for solving finite sum optimization problems. Our approach has two steps. First, we re-write the stationarity conditions as a system of nonlinear equations that associates each data point to a new row. Second, we apply a Subsampled Newton Raphson method to solve this system of nonlinear equations. Using our approach, we develop a new Stochastic Average Newton (SAN) method, which is incremental by design, in that it requires only a single data point per iteration. It is also cheap to implement when solving regularized generalized linear models, with a cost per iteration of the order of the number of the parameters. We show through extensive numerical experiments that SAN requires no knowledge about the problem, neither parameter tuning, while remaining competitive as compared to classical variance reduced gradient methods (e.g. SAG and SVRG), incremental Newton and quasi-Newton methods (e.g. SNM, IQN)."}}
