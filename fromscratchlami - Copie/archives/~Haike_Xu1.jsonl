{"id": "cG1JR4Pa2dM", "cdate": 1680531864877, "mdate": 1680531864877, "content": {"title": "A Universal Discriminator for Zero-Shot Generalization ", "abstract": "Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0\\%, 7.8\\%, and 11.5\\% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods.Meanwhile, our approach requires minimal prompting efforts, which largely improves robustness and is essential for real-world applications. Furthermore, we also jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discriminative tasks and simultaneously works on generative tasks."}}
{"id": "MwqHSFHS8tW", "cdate": 1671549047269, "mdate": 1671549047269, "content": {"title": "Embeddings and labeling schemes for A", "abstract": "A* is a classic and popular method for graphs search and path finding. It assumes the existence of a heuristic function $h(u,t)$ that estimates the shortest distance from any input node $u$ to the destination $t$. Traditionally, heuristics have been handcrafted by domain experts.  However, over the last few years, there has been a growing interest in learning heuristic functions. Such learned heuristics estimate the distance between given nodes based on ``features'' of those nodes.\n\nIn this paper we formalize and initiate the study of such feature-based heuristics. In particular, we consider heuristics induced by norm embeddings and distance labeling schemes, and provide lower bounds for the tradeoffs between the number of dimensions or bits used to represent each graph node, and the running time of the A* algorithm. We also show that, under natural assumptions, our lower bounds are almost optimal."}}
{"id": "-hTaLC2VL-y", "cdate": 1671548932401, "mdate": 1671548932401, "content": {"title": "Fine-grained gap-dependent bounds for tabular mdps via adaptive multi-step bootstrap", "abstract": "This paper presents a new model-free algorithm for episodic finite-horizon Markov Decision Processes (MDP), \\textbf{A}daptive \\textbf{M}ulti-step \\textbf{B}ootstrap~(\\textsf{AMB}),  which enjoys a stronger gap-dependent regret bound.\nThe first innovation is to estimate the optimal $Q$-function by combining an optimistic bootstrap with an \\textbf{adaptive} multi-step Monte Carlo rollout.\nThe second innovation is to select the action with the largest confidence interval length among admissible actions that are not dominated by any other actions. \nWe show when each state has a unique optimal action, \\textsf{AMB}~achieves a gap-dependent regret bound that only scales with the sum of the inverse of the sub-optimality gaps.\nIn contrast, Simchowitz and Jamieson (2019) showed  all upper-confidence-bound (UCB) algorithms suffer an additional $\\Omega\\left(\\frac{S}{\\mathrm{\\Delta}_{\\mathrm{min}}}\\right)$ regret due to over-exploration where $\\mathrm{\\Delta}_{\\mathrm{min}}$ is the minimum sub-optimality gap and $S$ is the number of states.\nWe further show that for general MDPs, \\textsf{AMB}~suffers an additional $\\frac{\\abs{Z_{\\mathrm{mul}}}}{\\mathrm{\\Delta}_{\\mathrm{min}}}$  regret, where $Z_{\\mathrm{mul}}$ is the set of state-action pairs $(s,a)$'s satisfying $a$ is a \\emph{non-unique optimal action} for $s$.\nWe complement our upper bound with a lower bound showing the dependency on $\\frac{\\abs{Z_{\\mathrm{mul}}}}{\\mathrm{\\Delta}_{\\mathrm{min}}}$ is unavoidable for any consistent algorithm.\nThis lower bound also implies a separation between reinforcement learning and contextual bandits.\n"}}
{"id": "a5bDgzSxKL", "cdate": 1671548598017, "mdate": 1671548598017, "content": {"title": "Simple combinatorial algorithms for combinatorial bandits: corruptions and approximations", "abstract": "We consider the stochastic combinatorial semi-bandit problem with adversarial corruptions. We provide a simple combinatorial algorithm that can achieve a regret of $\\tilde{O}\\left(C+d^2K/\\Delta_{min}\\right)$ where $C$ is the total amount of corruptions, $d$ is the maximal number of arms one can play in each round, $K$ is the number of arms. If one selects only one arm in each round, we achieves a regret of $\\tilde{O}\\left(C+\\sum_{\\Delta_i>0}1/\\Delta_i\\right)$. Our algorithm is combinatorial and improves on the previous \ncombinatorial algorithm by [Gupta et al., COLT2019] (their bound is \n$\\tilde{O}\\left(KC+\\sum_{\\Delta_i>0}1/\\Delta_i\\right)$\n), and almost\nmatches the best known bounds obtained by  [Zimmert et al., ICML2019] and [Zimmert and Seldin, AISTATS2019] (up to logarithmic factor). Note that the algorithms in  [Zimmert et al., ICML2019] and [Zimmert and Seldin, AISTATS2019]  require one to solve complex convex programs while our algorithm is combinatorial, very easy to implement, requires weaker assumptions and has very low oracle complexity and running time.\nWe also study the setting where we only get access to an approximation oracle for the stochastic combinatorial semi-bandit problem. \nOur algorithm achieves an (approximation) regret bound of $\\tilde{O}\\left(d\\sqrt{KT}\\right)$. Our algorithm is very simple, only worse than the best known regret bound by $\\sqrt{d}$, and has much lower oracle complexity than previous work."}}
{"id": "V4OEsd1BxOm", "cdate": 1640995200000, "mdate": 1671549173953, "content": {"title": "Embeddings and Labeling Schemes for A", "abstract": "A* is a classic and popular method for graphs search and path finding. It assumes the existence of a heuristic function h(u,t) that estimates the shortest distance from any input node u to the destination t. Traditionally, heuristics have been handcrafted by domain experts. However, over the last few years, there has been a growing interest in learning heuristic functions. Such learned heuristics estimate the distance between given nodes based on \"features\" of those nodes. In this paper we formalize and initiate the study of such feature-based heuristics. In particular, we consider heuristics induced by norm embeddings and distance labeling schemes, and provide lower bounds for the tradeoffs between the number of dimensions or bits used to represent each graph node, and the running time of the A* algorithm. We also show that, under natural assumptions, our lower bounds are almost optimal."}}
{"id": "wS-r9pIXxtM", "cdate": 1609459200000, "mdate": 1671549173958, "content": {"title": "Simple combinatorial algorithms for combinatorial bandits: corruptions and approximations", "abstract": "We consider the stochastic combinatorial semi-bandit problem with adversarial corruptions. We provide a simple combinatorial algorithm that can achieve a regret of $\\tilde{O}\\left(C+d^2K/\\Delta_{mi..."}}
{"id": "pEVCmvwaNj", "cdate": 1609459200000, "mdate": 1671549173957, "content": {"title": "Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap", "abstract": "This paper presents a new model-free algorithm for episodic finite-horizon Markov Decision Processes (MDP), Adaptive Multi-step Bootstrap (AMB), which enjoys a stronger gap-dependent regret bound. The first innovation is to estimate the optimal $Q$-function by combining an optimistic bootstrap with an adaptive multi-step Monte Carlo rollout. The second innovation is to select the action with the largest confidence interval length among admissible actions that are not dominated by any other actions. We show when each state has a unique optimal action, AMB achieves a gap-dependent regret bound that only scales with the sum of the inverse of the sub-optimality gaps. In contrast, Simchowitz and Jamieson (2019) showed all upper-confidence-bound (UCB) algorithms suffer an additional $\\Omega\\left(\\frac{S}{\\Delta_{min}}\\right)$ regret due to over-exploration where $\\Delta_{min}$ is the minimum sub-optimality gap and $S$ is the number of states. We further show that for general MDPs, AMB suffers an additional $\\frac{|Z_{mul}|}{\\Delta_{min}}$ regret, where $Z_{mul}$ is the set of state-action pairs $(s,a)$s satisfying $a$ is a non-unique optimal action for $s$. We complement our upper bound with a lower bound showing the dependency on $\\frac{|Z_{mul}|}{\\Delta_{min}}$ is unavoidable for any consistent algorithm. This lower bound also implies a separation between reinforcement learning and contextual bandits."}}
