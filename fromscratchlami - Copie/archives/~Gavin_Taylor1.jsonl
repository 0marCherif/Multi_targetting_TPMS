{"id": "01olnfLIbD", "cdate": 1601308407164, "mdate": null, "content": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems."}}
{"id": "hJmtwocEqzc", "cdate": 1601308255422, "mdate": null, "content": {"title": "LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition", "abstract": "Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike.  These systems are typically built by scraping social media profiles for user images.  Adversarial perturbations have been proposed for bypassing facial recognition systems.  However, existing methods fail on full-scale systems and commercial APIs.  We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases.  Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%."}}
{"id": "mj7WsaHYxj", "cdate": 1601308024496, "mdate": null, "content": {"title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks", "abstract": "Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets.  "}}
{"id": "SyliaANtwH", "cdate": 1569439427314, "mdate": null, "content": {"title": "MetaPoison:   Learning to craft adversarial poisoning examples via meta-learning", "abstract": " We consider a new class of \\emph{data poisoning} attacks on neural networks, in which the attacker takes control of a model by making small perturbations to a subset of its training data.  We formulate the task of finding poisons as a bi-level optimization problem, which can be solved using methods borrowed from the meta-learning community.  Unlike previous poisoning strategies, the meta-poisoning can poison networks that are trained from scratch using an initialization unknown to the attacker and transfer across hyperparameters. Further we show that our attacks are more versatile: they can cause misclassification of the target image into an arbitrarily chosen class. Our results show above 50% attack success rate when poisoning just 3-10% of the training dataset."}}
{"id": "S1bi7j-ubS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Transferable Clean-Label Poisoning Attacks on Deep Neural Nets", "abstract": "In this paper, we explore clean-label poisoning attacks on deep convolutional networks with access to neither the network\u2019s output nor its architecture or parameters. Our goal is to ensure that aft..."}}
{"id": "HkmaTz-0W", "cdate": 1518730158821, "mdate": null, "content": {"title": "Visualizing the Loss Landscape of Neural Nets", "abstract": "Neural network training relies on our ability to find ````````\"good\" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.\n\nIn this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ``\"filter normalization\" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers."}}
{"id": "rJZKgtZdZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Visualizing the Loss Landscape of Neural Nets", "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers."}}
{"id": "rkZ3UpldZr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Scalable Classifiers with ADMM and Transpose Reduction", "abstract": "As datasets for machine learning grow larger, parallelization strategies become more and more important. Recent approaches to distributed modelfitting rely heavily either on consensus ADMM, where each node solves smallsub-problems using only local data, or on stochastic gradient methods thatdon't scale well to large numbers of cores in a cluster setting. For this reason, GPU clusters have become common prerequisites to large-scale machinelearning. This paper describes an unconventional training method that uses alternating direction methods and Bregman iteration to train a variety of machine learning models on CPUs while avoiding the drawbacks of consensus methods and without gradient descent steps. Using transpose reduction strategies, the proposed method reduces the optimization problems to a sequence of minimization sub-steps that can each be solved globally in closed form. The method provides strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores."}}
{"id": "SyZ_4sWOWB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Adaptive Consensus ADMM for Distributed Optimization", "abstract": "The alternating direction method of multipliers (ADMM) is commonly used for distributed model fitting problems, but its performance and reliability depend strongly on user-defined penalty parameter..."}}
{"id": "rJbH1j-ubH", "cdate": 1451606400000, "mdate": null, "content": {"title": "Training Neural Networks Without Gradients: A Scalable ADMM Approach", "abstract": "With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimizati..."}}
