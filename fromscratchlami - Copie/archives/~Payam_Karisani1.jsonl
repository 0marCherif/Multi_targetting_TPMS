{"id": "6GKMnbtCGdh", "cdate": 1672531200000, "mdate": 1689381041898, "content": {"title": "Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets", "abstract": ""}}
{"id": "JHyhmqyW7Xg", "cdate": 1640995200000, "mdate": 1661357624827, "content": {"title": "Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers", "abstract": "We present a novel multiple-source unsupervised model for text classification under domain shift. Our model exploits the update rates in document representations to dynamically integrate domain encoders. It also employs a probabilistic heuristic to infer the error rate in the target domain in order to pair source classifiers. Our heuristic exploits data transformation cost and the classifier accuracy in the target feature space. We have used real world scenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We also used pretrained multi-layer transformers as the document encoder in the experiments to demonstrate whether the improvement achieved by domain adaptation models can be delivered by out-of-the-box language model pretraining. The experiments testify that our model is the top performing approach in this setting."}}
{"id": "74CgsPDwFa7", "cdate": 1640995200000, "mdate": 1675997013761, "content": {"title": "Multi-View Active Learning for Short Text Classification in User-Generated Data", "abstract": ""}}
{"id": "Nso-BzfNWOi", "cdate": 1609459200000, "mdate": 1661357624936, "content": {"title": "Semi-Supervised Text Classification via Self-Pretraining", "abstract": "We present a neural semi-supervised learning model termed Self-Pretraining. Our model is inspired by the classic self-training algorithm. However, as opposed to self-training, Self-Pretraining is threshold-free, it can potentially update its belief about previously labeled documents, and can cope with the semantic drift problem. Self-Pretraining is iterative and consists of two classifiers. In each iteration, one classifier draws a random set of unlabeled documents and labels them. This set is used to initialize the second classifier, to be further trained by the set of labeled documents. The algorithm proceeds to the next iteration and the classifiers' roles are reversed. To improve the flow of information across the iterations and also to cope with the semantic drift problem, Self-Pretraining employs an iterative distillation process, transfers hypotheses across the iterations, utilizes a two-stage training model, uses an efficient learning rate schedule, and employs a pseudo-label transformation heuristic. We have evaluated our model in three publicly available social media datasets. Our experiments show that Self-Pretraining outperforms the existing state-of-the-art semi-supervised classifiers across multiple settings. Our code is available at https://github.com/p-karisani/self-pretraining ."}}
{"id": "nT7bno-HiJf", "cdate": 1577836800000, "mdate": 1661357624937, "content": {"title": "Domain-Guided Task Decomposition with Self-Training for Detecting Personal Events in Social Media", "abstract": "Mining social media content for tasks such as detecting personal experiences or events, suffer from lexical sparsity, insufficient training data, and inventive lexicons. To reduce the burden of creating extensive labeled data and improve classification performance, we propose to perform these tasks in two steps: 1. Decomposing the task into domain-specific sub-tasks by identifying key concepts, thus utilizing human domain understanding; and 2. Combining the results of learners for each key concept using co-training to reduce the requirements for labeled training data. We empirically show the effectiveness and generality of our approach, Co-Decomp, using three representative social media mining tasks, namely Personal Health Mention detection, Crisis Report detection, and Adverse Drug Reaction monitoring. The experiments show that our model is able to outperform the state-of-the-art text classification models\u2013including those using the recently introduced BERT model\u2013when small amounts of training data are available."}}
{"id": "IYJk8JmYS2A", "cdate": 1514764800000, "mdate": 1661357624897, "content": {"title": "Did You Really Just Have a Heart Attack?: Towards Robust Detection of Personal Health Mentions in Social Media", "abstract": "Millions of users share their experiences on social media sites, such as Twitter, which in turn generate valuable data for public health monitoring, digital epidemiology, and other analyses of population health at global scale. The first, critical, task for these applications is classifying whether a personal health event was mentioned, which we call the (PHM) problem. This task is challenging for many reasons, including typically short length of social media posts, inventive spelling and lexicons, and figurative language, including hyperbole using diseases like \"heart attack\u00bb\u00bb or \"cancer\u00bb\u00bb for emphasis, and not as a health self-report. This problem is even more challenging for rarely reported, or frequent but ambiguously expressed conditions, such as \"stroke\u00bb\u00bb. To address this problem, we propose a general, robust method for detecting PHMs in social media, which we call WESPAD, that combines lexical, syntactic, word embedding-based, and context-based features. WESPAD is able to generalize from few examples by automatically distorting the word embedding space to most effectively detect the true health mentions. Unlike previously proposed state-of-the-art supervised and deep-learning techniques, WESPAD requires relatively little training data, which makes it possible to adapt, with minimal effort, to each new disease and condition. We evaluate WESPAD on both an established publicly available Flu detection benchmark, and on a new dataset that we have constructed with mentions of multiple health conditions. Our experiments show that WESPAD outperforms the baselines and state-of-the-art methods, especially in cases when the number and proportion of true health mentions in the training data is small."}}
