{"id": "AlpR6dzKjfy", "cdate": 1652737770060, "mdate": null, "content": {"title": "Robustness to Label Noise Depends on the Shape of the Noise Distribution", "abstract": "Machine learning classifiers have been demonstrated, both empirically and theoretically, to be robust to label noise under certain conditions --- notably the typical assumption is that label noise is independent of the features given the class label. We provide a theoretical framework that generalizes beyond this typical assumption by modeling label noise as a distribution over feature space. We show that both the scale and the \\emph{shape} of the noise distribution influence the posterior likelihood; and the shape of the noise distribution has a stronger impact on classification performance if the noise is concentrated in feature space where the decision boundary can be moved. For the special case of uniform label noise (independent of features and the class label), we show that the Bayes optimal classifier for $c$ classes is robust to label noise until the ratio of noisy samples goes above $\\frac{c-1}{c}$ (e.g. 90\\% for 10 classes), which we call the \\emph{tipping point}. However, for the special case of class-dependent label noise (independent of features given the class label), the tipping point can be as low as 50\\%. Most importantly, we show that when the noise distribution targets decision boundaries (label noise is directly dependent on feature space), classification robustness can drop off even at a small scale of noise. Even when evaluating recent label-noise mitigation methods we see reduced accuracy when label noise is dependent on features. These findings explain why machine learning often handles label noise well if the noise distribution is uniform in feature-space; yet it also points to the difficulty of overcoming label noise when it is concentrated in a region of feature space where a decision boundary can move. "}}
{"id": "WH-TzXPS3U", "cdate": 1642636241636, "mdate": 1642636241636, "content": {"title": "Learning Diverse Gaussian Graphical Models and Interpreting Edges", "abstract": "Gaussian graphical models are used to discover patterns of variable dependencies in many scientific applications, yet there is no guarantee that the assumption of identically distributed (IID) samples is correct. In many cases, the non-IID nature of the data is due to the fact that some observations are produced by a different underlying process than the other samples. Therefore, it is informative to the analyst who is trying to understand patterns in their data to explore different graphical models produced by various subsets of the data. Learning a graphical model for every one of a combinatoric number of data subsets is intractable. We solve the problem with an interactive machine learning approach, by first learning a Gaussian graphical model from data, then finding a different subset of the data that would produce the most different Gaussian graphical model, allowing the user to explore the most diverse Gaussian graphical models that fit various subsets of their data. To find the most different Gaussian graphical model, we define an optimization problem that can be solved by gradient-based algorithms. Furthermore, to gain insight into the learned Gaussian graphical model, we explain an edge in the graph by finding the subset of observations in the dataset that are critical for defining the correlation that the edge represents. That is, we interpret edges by finding subsets of data such that their removal from the dataset will lead to eliminating an edge in the graph. This relational information enables analysts to interpret each edge in terms of its robustness and relationship to observations in the data. Our method finds patterns in data from the Mars rover and online recipes. By bringing transparency and interpretability, we enable the practitioners to create and use models that they are confident and insightful about."}}
{"id": "tkqpdHibGBw", "cdate": 1632817578764, "mdate": null, "content": {"title": "Transfer learning with fewer ImageNet classes", "abstract": "  Though much previous work tried to uncover the best practices for transfer learning, much is left unexplored. Our preliminary work explores the effect of removing a portion of the ImageNet classes with low per-class validation accuracy on the accuracy of the remaining classes. Furthermore, we explore if models trained with a reduced number of classes are suitable for transfer learning."}}
{"id": "Hj8-M77g_pH", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Novel Algorithm for Skeleton Extraction From Images Using Topological Graph Analysis.", "abstract": "Skeletonization, also called thinning, is an important pre-processing step in computer vision and image processing tasks such as shape analysis and vectorization. It is a morphological process that generates a skeleton from an input image. Many thinning algorithms have been proposed, but accurate and fast algorithms are still in demand. In this paper, we propose a novel algorithm using embedded topological graphs and computational geometry that can extract skeletons from input binary images. We compare three well-known thinning algorithms with our method, with the experimental results showing effectiveness of the proposed method and algorithms."}}
{"id": "B1-8cx-u-H", "cdate": 1451606400000, "mdate": null, "content": {"title": "Bayesian Networks with Prior Knowledge for Malware Phylogenetics", "abstract": "Malware phylogenetics help cybersecurity experts to quickly understand a new malware sample by placing the new sample in the context of similar samples that have been previously reverse engineered. Recently, researchers have begun using malware code as data to infer directed acyclic graphs (DAG) that model the evolutionary relationships among samples of malware. A DAG is the ideal model for a phylogenetic graph because it includes the merges and branches that are often present in malware evolution. We present a novel Bayesian network discovery algorithm for learning a DAG via statistical inference of conditional dependencies from observed data with an informative prior on the partial ordering of variables. Our approach leverages the information on edge direction that a human can provide and the edge presence inference which data can provide. We give an efficient implementation of the partial-order prior in a Bayesian structure discovery learning algorithm, as well as a related structure prior, showing that both priors meet the local modularity requirement necessary for the efficient Bayesian discovery algorithm. We apply our algorithm to learn phylogenetic graphs on three malicious families and two benign families where the ground truth is known; and show that compared to competing algorithms, our algorithm more accurately identifies directed edges."}}
{"id": "B1EUn0xubH", "cdate": 1325376000000, "mdate": null, "content": {"title": "Leveraging Domain Knowledge in Multitask Bayesian Network Structure Learning", "abstract": "Network structure learning algorithms have aided network discovery in fields such as bioinformatics, neuroscience, ecology and social science. However, challenges remain in learning informative networks for related sets of tasks because the search space of Bayesian network structures is characterized by large basins of approximately equivalent solutions. Multitask algorithms select a set of networks that are near each other in the search space, rather than a score-equivalent set of networks chosen from independent regions of the space. This selection preference allows a domain expert to see only differences supported by the data. However, the usefulness of these algorithms for scientific datasets is limited because existing algorithms naively assume that all pairs of tasks are equally related. We introduce a framework that relaxes this assumption by incorporating domain knowledge about task-relatedness into the learning objective. Using our framework, we introduce the first multitask Bayesian network algorithm that leverages domain knowledge about the relatedness of tasks. We use our algorithm to explore the effect of task-relatedness on network discovery and show that our algorithm learns networks that are closer to ground truth than naive algorithms and that our algorithm discovers patterns that are interesting."}}
