{"id": "zAuMvl7Wan2", "cdate": 1668324966689, "mdate": 1668324966689, "content": {"title": "Single Image Super-Resolution via Perceptual Loss Guided by Denoising Auto-Encoder", "abstract": "Image restoration is a difficult task due to its non-uniqueness of solution. Owing to the power of Convolution Neural Networks (CNNs), we can generate images with high PSNR (Peak Signal to Noise Ratio) and SSIM (Structural SIMilarity index) by using a per-pixel loss between the outputs and the ground-truth images. Unfortunately, these images usually suffer from over-smoothing problem. The previous perceptual loss based on high-level features was then proposed to replace the traditional per-pixel loss. But the low-level information such as color, shape and texture will be lost. In order to alleviate this problem, we design a new perceptual loss extracted from a pre-trained denoising auto-encoder with symmetric skip connections (SDAE). The encoder in SDAE is extracted as a perceptual function. We carry out the experiments on single image super-resolution to validate the proposed method. The results show that the images generated by our method have both better visual quality and higher PSNR and SSIM than the state-of-the-art methods."}}
{"id": "I6FX2VuP5I", "cdate": 1668324895213, "mdate": 1668324895213, "content": {"title": "A Novel Attention Enhanced Dense Network for Image Super-Resolution", "abstract": "Deep convolutional neural networks (CNNs) have recently achieved impressive performance in image super-resolution (SR). However, they usually treat the spatial features and channel-wise features indiscriminatingly and fail to take full advantage of hierarchical features, restricting adaptive ability. To address these issues, we propose a novel attention enhanced dense network (AEDN) to adaptively recalibrate each kernel and feature for different inputs, by integrating both spatial attention (SA) and channel attention (CA) modules in the proposed network. In experiments, we explore the effect of attention mechanism and present quantitative and qualitative evaluations, where the results show that the proposed AEDN outperforms state-of-the-art methods by effectively suppressing the artifacts and faithfully recovering more high-frequency image details."}}
{"id": "6CuAYuPzZc", "cdate": 1668324668053, "mdate": 1668324668053, "content": {"title": "Lightweight and Accurate Single Image Super-Resolution with Channel Segregation Network", "abstract": "Deep neural networks have witnessed great success in Single Image Super-Resolution (SISR). However, current improvements are mainly contributed by much deeper networks, which leads to huge computation cost and limited application for mobile devices. Moreover, most existing methods propagate the basic content of low-resolution images forward to deeper layers iteratively. Such duplicate computations inevitably result in inefficient reconstruction. To address this issue, we propose an efficient channel segregation block containing multiple branches with different depths, enabling the model to preserve basic content, and focusing on optimizing the detail content with fewer parameters. By merging the output of segregated branches, the block covers a large range of receptive fields. Experimental results demonstrate that the proposed method outperforms other state-of-the-art methods even with fewer parameters and lower computational complexity, which is more applicable to lightweight scenarios. "}}
{"id": "2OdAggzzF3z", "cdate": 1652737299146, "mdate": null, "content": {"title": "ResT V2: Simpler, Faster and Stronger", "abstract": "This paper proposes ResTv2, a simpler, faster, and stronger multi-scale vision Transformer for visual recognition. ResTv2 simplifies the EMSA structure in ResTv1 (i.e., eliminating the multi-head interaction part) and employs an upsample operation to reconstruct the lost medium- and high-frequency information caused by the downsampling operation. In addition, we explore different techniques for better applying ResTv2 backbones to downstream tasks. We find that although combining EMSAv2 and window attention can greatly reduce the theoretical matrix multiply FLOPs, it may significantly decrease the computation density, thus causing lower actual speed. We comprehensively validate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic segmentation. Experimental results show that the proposed ResTv2 can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResTv2 as solid backbones. The code and models will be made publicly available at \\url{https://github.com/wofmanaf/ResT}."}}
{"id": "tWtgk3rjr5", "cdate": 1640995200000, "mdate": 1668601606494, "content": {"title": "Unsupervised Unpaired Super-Resolution Using an Active Sampling Strategy Based on Edge Detection", "abstract": "Most existing super-resolution (SR) methods rely on pairs of low resolution (LR) and high resolution (HR) images and predetermined degradation operations (e.g., bicubic), usually trained by supervised learning. However, they often fail in real-world scenarios because of the occurrence of noise and blur. The key reason is that the degradation process is unknown, and no HR-LR pairs can be obtained directly. To address the above issues, this paper explores the optimization of an unsupervised unpaired SR method inspired by generative models such as Generative Adversarial Networks (GAN) and Cycle-Consistent Adversarial Networks (CycleGAN). We propose an active sampling strategy based on edge detection, and introduce a denoising network to construct a novel unsupervised unpaired SR framework. The active sampling strategy can perform image-level feature alignment by sampling image patches actively, thus optimizing the learning direction of the generator. At the same time, the denoising network can reduce the learning difficulty of the generator by preprocessing real-world LR images. Extensive experiments indicate that our method obtains better performance over other existing solutions to the unsupervised unpaired SR challenge,"}}
{"id": "P1Roaq782f", "cdate": 1640995200000, "mdate": 1668601606945, "content": {"title": "ResT V2: Simpler, Faster and Stronger", "abstract": "This paper proposes ResTv2, a simpler, faster, and stronger multi-scale vision Transformer for visual recognition. ResTv2 simplifies the EMSA structure in ResTv1 (i.e., eliminating the multi-head interaction part) and employs an upsample operation to reconstruct the lost medium- and high-frequency information caused by the downsampling operation. In addition, we explore different techniques for better apply ResTv2 backbones to downstream tasks. We found that although combining EMSAv2 and window attention can greatly reduce the theoretical matrix multiply FLOPs, it may significantly decrease the computation density, thus causing lower actual speed. We comprehensively validate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic segmentation. Experimental results show that the proposed ResTv2 can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResTv2 as solid backbones. The code and models will be made publicly available at \\url{https://github.com/wofmanaf/ResT}"}}
{"id": "L9gOkFsE4S", "cdate": 1640995200000, "mdate": 1668601606944, "content": {"title": "From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering", "abstract": "Multi-hop question answering (QA) is a challenging task requiring QA systems to perform complex reasoning over multiple documents and provide supporting facts together with the exact answer. Existing works tend to utilize graph-based reasoning and question decomposition to obtain the reasoning chain, which inevitably introduces additional complexity and cumulative error to the system. To address the above issue, we propose a simple yet effective novel framework, From Easy to Hard (FE2H), to remove distracting information and obtain better contextual representations for the multi-hop QA task. Inspired by the iterative document selection process and the progressive learning custom of humans, FE2H divides both the document selector and reader into two stages following an easy-to-hard manner. Specifically, we first select the document most relevant to the question and then utilize the question together with this document to select other pertinent documents. As for the QA phase, our reader is first trained on a single-hop QA dataset and then transferred into the multi-hop QA task. We comprehensively evaluate our model on the popular multi-hop QA benchmark HotpotQA. Experimental results demonstrate that our method ourperforms all other methods in the leaderboard of HotpotQA (distractor setting)."}}
{"id": "6Ab68Ip4Mu", "cdate": 1621629730780, "mdate": null, "content": {"title": "ResT: An Efficient Transformer for Visual Recognition", "abstract": "This paper presents an efficient multi-scale vision Transformer, called ResT, that capably served as a general-purpose backbone for image recognition. Unlike existing Transformer methods, which employ standard Transformer blocks to tackle raw images with a fixed resolution, our ResT have several advantages: (1) A memory-efficient multi-head self-attention is built, which compresses the memory by a simple depth-wise convolution, and projects the interaction across the attention-heads dimension while keeping the diversity ability of multi-heads; (2) Positional encoding is constructed as spatial attention, which is more flexible and can tackle with input images of arbitrary size without interpolation or fine-tune; (3) Instead of the straightforward tokenization at the beginning of each stage, we design the patch embedding as a stack of overlapping convolution operation with stride on the token map. We comprehensively validate ResT on image classification and downstream tasks. Experimental results show that the proposed ResT can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResT as strong backbones. The code and models will be made publicly available at https://github.com/wofmanaf/ResT."}}
{"id": "zCvqBMGqGH4", "cdate": 1609459200000, "mdate": 1668601607356, "content": {"title": "Group-CAM: Group Score-Weighted Visual Explanations for Deep Convolutional Networks", "abstract": "In this paper, we propose an efficient saliency map generation method, called Group score-weighted Class Activation Mapping (Group-CAM), which adopts the \"split-transform-merge\" strategy to generate saliency maps. Specifically, for an input image, the class activations are firstly split into groups. In each group, the sub-activations are summed and de-noised as an initial mask. After that, the initial masks are transformed with meaningful perturbations and then applied to preserve sub-pixels of the input (i.e., masked inputs), which are then fed into the network to calculate the confidence scores. Finally, the initial masks are weighted summed to form the final saliency map, where the weights are confidence scores produced by the masked inputs. Group-CAM is efficient yet effective, which only requires dozens of queries to the network while producing target-related saliency maps. As a result, Group-CAM can be served as an effective data augment trick for fine-tuning the networks. We comprehensively evaluate the performance of Group-CAM on common-used benchmarks, including deletion and insertion tests on ImageNet-1k, and pointing game tests on COCO2017. Extensive experimental results demonstrate that Group-CAM achieves better visual performance than the current state-of-the-art explanation approaches. The code is available at https://github.com/wofmanaf/Group-CAM."}}
{"id": "sY3xz7xLRy", "cdate": 1609459200000, "mdate": 1668601606944, "content": {"title": "SA-Net: Shuffle Attention for Deep Convolutional Neural Networks", "abstract": "Attention mechanisms, which enable a neural network to accurately focus on all the relevant elements of the input, have become an essential component to improve the performance of deep neural networks. There are mainly two attention mechanisms widely used in computer vision studies, spatial attention and channel attention, which aim to capture the pixel-level pairwise relationship and channel dependency, respectively. Although fusing them together may achieve better performance than their individual implementations, it will inevitably increase the computational overhead. In this paper, we propose an efficient Shuffle Attention (SA) module to address this issue, which adopts Shuffle Units to combine two types of attention mechanisms effectively. Specifically, SA first groups channel dimensions into multiple sub-features before processing them in parallel. Then, for each sub-feature, SA utilizes a Shuffle Unit to depict feature dependencies in both spatial and channel dimensions. After that, all sub-features are aggregated and a \"channel shuffle\" operator is adopted to enable information communication between different sub-features. The proposed SA module is efficient yet effective, e.g., the parameters and computations of SA against the backbone ResNet50 are 300 vs. 25.56M and 2.76e-3 GFLOPs vs. 4.12 GFLOPs, respectively, and the performance boost is more than 1.34% in terms of Top-1 accuracy. Extensive experimental results on common-used benchmarks, including ImageNet-1k for classification, MS COCO for object detection, and instance segmentation, demonstrate that the proposed SA outperforms the current SOTA methods significantly by achieving higher accuracy while having lower model complexity."}}
