{"id": "47KG_AvNqeZ", "cdate": 1663850280353, "mdate": null, "content": {"title": "Online Low Rank Matrix Completion", "abstract": "  We study the problem of online low-rank matrix completion with $\\mathsf{M}$ users, $\\mathsf{N}$ items and $\\mathsf{T}$ rounds. In each round, the algorithm recommends one item per user, for which it gets a (noisy) reward sampled from a low-rank user-item preference matrix. The goal is to design a method with sub-linear regret (in $\\mathsf{T}$) and nearly optimal dependence on $\\mathsf{M}$ and $\\mathsf{N}$. The problem can be easily mapped to the standard multi-armed bandit problem where each item is an independent arm, but that leads to poor regret as the correlation between arms and users is not exploited. On the other hand, exploiting the low-rank structure of reward matrix is challenging due to non-convexity of the low-rank manifold. We first demonstrate that the low-rank structure can be exploited  using a simple  explore-then-commit (ETC) approach that ensures a regret of $O(\\mathsf{polylog} (\\mathsf{M}+\\mathsf{N}) \\mathsf{T}^{2/3})$. That is, roughly only $\\mathsf{polylog} (\\mathsf{M}+\\mathsf{N})$ item recommendations are required per user to get a non-trivial solution. We then improve our result for the  rank-$1$ setting which in itself is quite challenging and encapsulates some of the key issues. Here, we propose OCTAL (Online Collaborative filTering using iterAtive user cLustering) that guarantees nearly optimal regret of $O(\\mathsf{polylog} (\\mathsf{M}+\\mathsf{N}) \\mathsf{T}^{1/2})$. OCTAL is based on a novel technique of clustering users that allows iterative elimination of items and leads to a nearly optimal minimax rate. "}}
{"id": "GIZg_kOXqyG", "cdate": 1663850277424, "mdate": null, "content": {"title": "Private and Efficient Meta-Learning with Low Rank and Sparse decomposition", "abstract": "Meta-learning is critical for a variety of practical ML systems -- like personalized recommendations systems -- that are  required to generalize to new tasks despite a small number of task-specific training points. Existing meta-learning techniques use two complementary approaches of either learning a low-dimensional representation of points for all tasks, or task-specific fine-tuning of a global model trained using all the tasks. In this work, we propose a novel meta-learning framework that combines both the techniques to enable handling of a large number of data-starved tasks. Our framework models network weights as a sum of low-rank  and sparse matrices. This allows us to capture information from multiple domains together in the low-rank part while still allowing task specific personalization using the sparse part.  We instantiate and study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a small number of linear measurements. We propose an alternating minimization method with hard thresholding -- AMHT-LRS -- to learn the low-rank and sparse part effectively and efficiently. For the realizable, Gaussian data setting, we show that AMHT-LRS indeed solves the problem efficiently with nearly optimal samples.  We extend  AMHT-LRS  to ensure that it preserves privacy of each individual user in the dataset, while still ensuring strong  generalization with nearly optimal number of samples. Finally, on multiple datasets, we demonstrate that the framework allows personalized models to obtain superior performance in the data-scarce regime."}}
{"id": "wchZL_2g2d", "cdate": 1640995200000, "mdate": 1669707463844, "content": {"title": "Improved Support Recovery in Universal One-bit Compressed Sensing", "abstract": "One-bit compressed sensing (1bCS) is an extremely quantized signal acquisition method that has been proposed and studied rigorously in the past decade. In 1bCS, linear samples of a high dimensional signal are quantized to only one bit per sample (sign of the measurement). Assuming the original signal vector to be sparse, existing results in 1bCS either aim to find the support of the vector, or approximate the signal allowing a small error. The focus of this paper is support recovery, which often also computationally facilitate approximate signal recovery. A {\\em universal} measurement matrix for 1bCS refers to one set of measurements that work for all sparse signals. With universality, it is known that $\\tilde{\\Theta}(k^2)$ 1bCS measurements are necessary and sufficient for support recovery (where $k$ denotes the sparsity). To improve the dependence on sparsity from quadratic to linear, in this work we propose approximate support recovery (allowing $\\epsilon>0$ proportion of errors), and superset recovery (allowing $\\epsilon$ proportion of false positives). We show that the first type of recovery is possible with $\\tilde{O}(k/\\epsilon)$ measurements, while the later type of recovery, more challenging, is possible with $\\tilde{O}(\\max\\{k/\\epsilon,k^{3/2}\\})$ measurements. We also show that in both cases $\\Omega(k/\\epsilon)$ measurements would be necessary for universal recovery. Improved results are possible if we consider universal recovery within a restricted class of signals, such as rational signals, or signals with bounded dynamic range. In both cases superset recovery is possible with only $\\tilde{O}(k/\\epsilon)$ measurements. Other results on universal but approximate support recovery are also provided in this paper. All of our main recovery algorithms are simple and polynomial-time."}}
{"id": "uUtSp8oITo4", "cdate": 1640995200000, "mdate": 1668593305487, "content": {"title": "Private and Efficient Meta-Learning with Low Rank and Sparse Decomposition", "abstract": "Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation style systems. Standard personalization approaches involve learning a user/domain specific embedding that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain -- a.k.a meta-learning -- has high storage/infrastructure cost. We propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse matrices. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. Furthermore, the framework is up to two orders of magnitude more scalable (in terms of storage/infrastructure cost) than user-specific finetuning of model. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a small number of linear measurements. We propose an alternating minimization method with iterative hard thresholding -- AMHT-LRS -- to learn the low-rank and sparse part. For the realizable, Gaussian data setting, we show that AMHT-LRS solves the problem efficiently with nearly optimal samples. A significant challenge in personalization is ensuring privacy of each user's sensitive data. We alleviate this problem by proposing a differentially private variant of our method that also is equipped with strong generalization guarantees. Finally, on multiple standard recommendation datasets, we demonstrate that our approach allows personalized models to obtain superior performance in sparse data regime."}}
{"id": "oAP7gDd2HK", "cdate": 1640995200000, "mdate": 1669707463843, "content": {"title": "Community Recovery in the Geometric Block Model", "abstract": "To capture inherent geometric features of many community detection problems, we propose to use a new random graph model of communities that we call a \\emph{Geometric Block Model}. The geometric block model builds on the \\emph{random geometric graphs} (Gilbert, 1961), one of the basic models of random graphs for spatial networks, in the same way that the well-studied stochastic block model builds on the Erd\\H{o}s-R\\'{en}yi random graphs. It is also a natural extension of random community models inspired by the recent theoretical and practical advancements in community detection. To analyze the geometric block model, we first provide new connectivity results for \\emph{random annulus graphs} which are generalizations of random geometric graphs. The connectivity properties of geometric graphs have been studied since their introduction, and analyzing them has been difficult due to correlated edge formation. We then use the connectivity results of random annulus graphs to provide necessary and sufficient conditions for efficient recovery of communities for the geometric block model. We show that a simple triangle-counting algorithm to detect communities in the geometric block model is near-optimal. For this we consider two regimes of graph density. In the regime where the average degree of the graph grows logarithmically with number of vertices, we show that our algorithm performs extremely well, both theoretically and practically. In contrast, the triangle-counting algorithm is far from being optimum for the stochastic block model in the logarithmic degree regime. We also look at the regime where the average degree of the graph grows linearly with the number of vertices $n$, and hence to store the graph one needs $\\Theta(n^2)$ memory. We show that our algorithm needs to store only $O(n \\log n)$ edges in this regime to recover the latent communities."}}
{"id": "o0KFnfN_RQ", "cdate": 1640995200000, "mdate": 1669707463844, "content": {"title": "On Learning Mixture of Linear Regressions in the Non-Realizable Setting", "abstract": "While mixture of linear regressions (MLR) is a well-studied topic, prior works usually do not analyze such models for prediction error. In fact, <em>prediction</em> and <em>loss</em> are not well-d..."}}
{"id": "l55ruGK_Lei", "cdate": 1640995200000, "mdate": 1652615671745, "content": {"title": "Support Recovery in Universal One-Bit Compressed Sensing", "abstract": "One-bit compressed sensing (1bCS) is an extreme-quantized signal acquisition method that has been intermittently studied in the past decade. In 1bCS, linear samples of a high dimensional signal are quantized to only one bit per sample (sign of the measurement). The extreme quantization makes it an interesting case study of the more general single-index or generalized linear models. At the same time it can also be thought of as a \"design\" version of learning a binary linear classifier or halfspace-learning. Assuming the original signal vector to be sparse, existing results in 1bCS either aim to find the support of the vector, or approximate the signal within an \u03b5-ball. The focus of this paper is support recovery, which often also computationally facilitate approximate signal recovery. A universal measurement matrix for 1bCS refers to one set of measurements that work for all sparse signals. With universality, it is known that \u0398\u0303(k\u00b2) 1bCS measurements are necessary and sufficient for support recovery (where k denotes the sparsity). In this work, we show that it is possible to universally recover the support with a small number of false positives with O\u0303(k^{3/2}) measurements. If the dynamic range of the signal vector is known, then with a different technique, this result can be improved to only O\u0303(k) measurements. Other results on universal but approximate support recovery are also provided in this paper. All of our main recovery algorithms are simple and polynomial-time."}}
{"id": "ZYs46qcjE7", "cdate": 1640995200000, "mdate": 1669707463845, "content": {"title": "Online Low Rank Matrix Completion", "abstract": "We study the problem of {\\em online} low-rank matrix completion with $\\mathsf{M}$ users, $\\mathsf{N}$ items and $\\mathsf{T}$ rounds. In each round, the algorithm recommends one item per user, for which it gets a (noisy) reward sampled from a low-rank user-item preference matrix. The goal is to design a method with sub-linear regret (in $\\mathsf{T}$) and nearly optimal dependence on $\\mathsf{M}$ and $\\mathsf{N}$. The problem can be easily mapped to the standard multi-armed bandit problem where each item is an {\\em independent} arm, but that leads to poor regret as the correlation between arms and users is not exploited. On the other hand, exploiting the low-rank structure of reward matrix is challenging due to non-convexity of the low-rank manifold. We first demonstrate that the low-rank structure can be exploited using a simple explore-then-commit (ETC) approach that ensures a regret of $O(\\mathsf{polylog} (\\mathsf{M}+\\mathsf{N}) \\mathsf{T}^{2/3})$. That is, roughly only $\\mathsf{polylog} (\\mathsf{M}+\\mathsf{N})$ item recommendations are required per user to get a non-trivial solution. We then improve our result for the rank-$1$ setting which in itself is quite challenging and encapsulates some of the key issues. Here, we propose \\textsc{OCTAL} (Online Collaborative filTering using iterAtive user cLustering) that guarantees nearly optimal regret of $O(\\mathsf{polylog} (\\mathsf{M}+\\mathsf{N}) \\mathsf{T}^{1/2})$. OCTAL is based on a novel technique of clustering users that allows iterative elimination of items and leads to a nearly optimal minimax rate."}}
{"id": "UwdQGY2xL3_", "cdate": 1640995200000, "mdate": 1652615671491, "content": {"title": "On Learning Mixture Models with Sparse Parameters", "abstract": "Mixture models are widely used to fit complex and multimodal datasets. In this paper we study mixtures with high dimensional sparse latent parameter vectors and consider the problem of support recovery of those vectors. While parameter learning in mixture models is well-studied, the sparsity constraint remains relatively unexplored. Sparsity of parameter vectors is a natural constraint in variety of settings, and support recovery is a major step towards parameter estimation. We provide efficient algorithms for support recovery that have a logarithmic sample complexity dependence on the dimensionality of the latent space. Our algorithms are quite general, namely they are applicable to 1) mixtures of many different canonical distributions including Uniform, Poisson, Laplace, Gaussians, etc. 2) Mixtures of linear regressions and linear classifiers with Gaussian covariates under different assumptions on the unknown parameters. In most of these settings, our results are the first guarantees on the problem while in the rest, our results provide improvements on existing works."}}
{"id": "IAPHhRm8wDz", "cdate": 1640995200000, "mdate": 1652615671491, "content": {"title": "Lower Bounds on the Total Variation Distance Between Mixtures of Two Gaussians", "abstract": "Mixtures of high dimensional Gaussian distributions have been studied extensively in statistics and learning theory. While the total variation distance appears naturally in the sample complexity of distribution learning, it is analytically difficult to obtain tight lower bounds for mixtures. Exploiting a connection between total variation distance and the characteristic function of the mixture, we provide fairly tight functional approximations. This enables us to derive new lower bounds on the total variation distance between two-component Gaussian mixtures with a shared covariance matrix."}}
