{"id": "-FWocM016N", "cdate": 1621527408892, "mdate": null, "content": {"title": "Solving Common-Payoff Games with Approximate Policy Iteration", "abstract": "For    artificially intelligent learning systems to have widespread applicability in real-world settings, it is important that they be able to operate decentrally. Unfortunately, decentralized control is difficult\u2014computing even an epsilon-optimal  joint  policy  is  a  NEXP  complete  problem.Nevertheless, a recently rediscovered insight\u2014that a team of agents  can  coordinate  via  common  knowledge\u2014has  given rise to algorithms capable of finding optimal joint policies in small common-payoff games. The Bayesian action decoder(BAD) leverages this insight and deep reinforcement learning to scale to games as large as two-player Hanabi. However, the approximations it uses to do so prevent it from discovering optimal joint policies even in games small enough to brute force optimal solutions. This work proposes CAPI, a novel algorithm  which,  like  BAD,  combines  common  knowledge with  deep  reinforcement  learning.  However,  unlike  BAD, CAPI  prioritizes  the  propensity  to  discover  optimal  joint policies over scalability. While this choice precludes CAPI from scaling to games as large as Hanabi, empirical results demonstrate  that,  on  the  games  to  which  CAPI  does  scale, it is capable of discovering optimal joint policies even when other modern multi-agent reinforcement learning algorithms are unable to do so."}}
{"id": "YMsbeG6FqBU", "cdate": 1601308409359, "mdate": null, "content": {"title": "The Advantage Regret-Matching Actor-Critic", "abstract": "Regret minimization has played a key role in online learning, equilibrium computation in games, and reinforcement learning (RL). In this paper, we describe a general model-free RL method for no-regret learning based on repeated reconsideration of past behavior: Advantage Regret-Matching Actor-Critic (ARMAC). Rather than saving past state-action data, ARMAC saves a buffer of past policies, replaying through them to reconstruct hindsight assessments of past behavior. These retrospective value estimates are used to predict conditional advantages which, combined with regret matching, produces a new policy. In particular, ARMAC learns from sampled trajectories in a centralized training setting, without requiring the application of importance sampling commonly used in Monte Carlo counterfactual regret (CFR) minimization; hence, it does not suffer from excessive variance in large environments. In the single-agent setting, ARMAC shows an interesting form of exploration by keeping past policies intact. In the multiagent setting, ARMAC in self-play approaches Nash equilibria on some partially-observable zero-sum benchmarks. We provide exploitability estimates in the significantly larger game of betting-abstracted no-limit Texas Hold'em."}}
{"id": "31Xq5v51we4", "cdate": 1596657844898, "mdate": null, "content": {"title": "Variance Reduction in Monte Carlo Counterfactual Regret Minimization (VR-MCCFR) for Extensive Form Games using Baselines", "abstract": "Learning strategies for imperfect information games from samples of interaction is a challenging problem. A common method for this setting, Monte Carlo Counterfactual Regret Minimization (MCCFR), can have slow long-term convergence rates due to high variance. In this paper, we introduce a variance reduction technique (VR-MCCFR) that applies to any sampling variant of MCCFR. Using this technique, per-iteration estimated values and updates are reformulated as a function of sampled values and state-action baselines, similar to their use in policy gradient reinforcement learning. The new formulation allows estimates to be bootstrapped from other estimates within the same episode, propagating the benefits of baselines along the sampled trajectory; the estimates remain unbiased even when bootstrapping from other estimates. Finally, we show that given a perfect baseline, the variance of the value estimates can be reduced to zero. Experimental evaluation shows that VR-MCCFR brings an order of magnitude speedup, while the empirical variance decreases by three orders of magnitude. The decreased variance allows for the first time CFR+ to be used with sampling, increasing the speedup to two orders of magnitude."}}
{"id": "HyWXVlbu-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "AIVAT: A New Variance Reduction Technique for Agent Evaluation in Imperfect Information Games", "abstract": "Evaluating agent performance when outcomes are stochastic and agents use randomized strategies can be challenging when there is limited data available. The variance of sampled outcomes may make the simple approach of Monte Carlo sampling inadequate. This is the case for agents playing heads-up no-limit Texas hold'em poker, whereman-machine competitions typically involve multiple days of consistent play by multiple players, but still can (and sometimes did) result in statistically insignificant conclusions. In this paper, we introduce AIVAT, a low variance, provably unbiased value assessment tool that exploits an arbitrary heuristic estimate of state value, as well as the explicit strategy of a subset of the agents. Unlike existing techniques which reduce the variance from chance events, or only consider game ending actions, AIVAT reduces the variance both from choices by nature and by players with a known strategy. The resulting estimator produces results that significantly outperform previous state of the art techniques. It was able to reduce the standard deviation of a Texas hold'em poker man-machine match by 85\\% and consequently requires 44 times fewer games to draw the same statistical conclusion. AIVAT enabled the first statistically significant AI victory against professional poker players in no-limit hold'em.Furthermore, the technique was powerful enough to produce statistically significant results versus individual players, not just an aggregate pool of the players. We also used AIVAT to analyze a short series of AI vs human poker tournaments,producing statistical significant results with as few as 28 matches."}}
{"id": "SyNr26ldbB", "cdate": 1483228800000, "mdate": null, "content": {"title": "AIVAT: A New Variance Reduction Technique for Agent Evaluation in Imperfect Information Games", "abstract": "Evaluating agent performance when outcomes are stochastic and agents use randomized strategies can be challenging when there is limited data available. The variance of sampled outcomes may make the simple approach of Monte Carlo sampling inadequate. This is the case for agents playing heads-up no-limit Texas hold'em poker, where man-machine competitions have involved multiple days of consistent play and still not resulted in statistically significant conclusions even when the winner's margin is substantial. In this paper, we introduce AIVAT, a low variance, provably unbiased value assessment tool that uses an arbitrary heuristic estimate of state value, as well as the explicit strategy of a subset of the agents. Unlike existing techniques which reduce the variance from chance events, or only consider game ending actions, AIVAT reduces the variance both from choices by nature and by players with a known strategy. The resulting estimator in no-limit poker can reduce the number of hands needed to draw statistical conclusions by more than a factor of 10."}}
{"id": "rJbj7RxubB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Refining Subgames in Large Imperfect Information Games", "abstract": "The leading approach to solving large imperfect information games is to pre-calculate an approximate solution using a simplified abstraction of the full game; that solution is then used to play the original, full-scale game. The abstraction step is necessitated by the size of the game tree. However, as the original game progresses, the remaining portion of the tree (the subgame) becomes smaller. An appealing idea is to use the simplified abstraction to play the early parts of the game and then, once the subgame becomes tractable, to calculate a solution using a finer-grained abstraction in real time, creating a combined final strategy. While this approach is straightforward for perfect information games, it is a much more complex problem for imperfect information games. If the subgame is solved locally, the opponent can alter his play in prior to this subgame to exploit our combined strategy. To prevent this, we introduce the notion of subgame margin, a simple value with appealing properties. If any best response reaches the subgame, the improvement of exploitability of the combined strategy is (at least) proportional to the sub-game margin. This motivates subgame refinements resulting in large positive margins. Unfortunately, current techniques either neglect subgame margin (potentially leading to a large negative subgame margin and drastically more exploitable strategies), or guarantee only non-negative subgame margin (possibly producing the original, unrefined strategy, even if much stronger strategies are possible). Our technique remedies this problem by maximizing the subgame margin and is guaranteed to find the optimal solution. We evaluate our technique using one of the top participants of the AAAI-14 Computer Poker Competition, the leading playground for agents in imperfect information settings."}}
{"id": "Hk-EVngO-S", "cdate": 1451606400000, "mdate": null, "content": {"title": "Text Understanding with the Attention Sum Reader Network", "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets."}}
{"id": "H1bOXAxd-r", "cdate": 1420070400000, "mdate": null, "content": {"title": "Automatic Public State Space Abstraction in Imperfect Information Games", "abstract": "Although techniques for finding Nash equilibria in extensive form games have become more powerful in recent years, many games that model real world interactions remain too large to be solved directly. The current approach is to create a smaller abstracted game, allowing the computation of an optimal solution. The strategy can then be used in the original game. Considering public information to create the abstraction can be strategically important, yet very few of the previous abstraction algorithms specifically consider public information or use an expert approach. In this paper, we show that the public information can be crucial, and we present a new, automatic technique for abstracting the public state space. We also present an experimental evaluation in the domain of Texas Hold\u2019em poker and show that it outperforms state-of-the-art abstraction algorithms."}}
{"id": "HJNvICx_bS", "cdate": 1388534400000, "mdate": null, "content": {"title": "Bounding the Support Size in Extensive Form Games with Imperfect Information", "abstract": "It is a well known fact that in extensive form games with perfect information, there is a Nash equilibrium with support of size one. This doesn't hold for games with imperfect information, where the size of minimal support can be larger. We present a dependency between the level of uncertainty and the minimum support size. For many games, there is a big disproportion between the game uncertainty and the number of actions available. In Bayesian extensive games with perfect information, the only uncertainty is about the type of players. In card games, the uncertainty comes from dealing the deck. In these games, we can significantly reduce the support size. Our result applies to general-sum extensive form games with any finite number of players."}}
{"id": "Qn8lxPngJFkB2l8pUYxg", "cdate": null, "mdate": null, "content": {"title": "Neural Text Understanding with Attention Sum Reader", "abstract": "Two large-scale cloze-style context-question-answer datasets have been introduced recently: i) the CNN and Daily Mail news data and ii) the Children's Book Test.\nThanks to the size of these datasets, the associated task is well suited for deep-learning techniques that seem to outperform all alternative approaches.\nWe present a new, simple model that is tailor made for such question-answering problems.\nOur model directly sums attention over candidate answer words in the document instead of using it to compute weighted sum of word embeddings.\nOur model outperforms models previously proposed for these tasks by a large margin."}}
