{"id": "Nd-pDyMB4X", "cdate": 1640995200000, "mdate": 1668059099750, "content": {"title": "ViA: View-invariant Skeleton Action Representation Learning via Motion Retargeting", "abstract": "Current self-supervised approaches for skeleton action representation learning often focus on constrained scenarios, where videos and skeleton data are recorded in laboratory settings. When dealing with estimated skeleton data in real-world videos, such methods perform poorly due to the large variations across subjects and camera viewpoints. To address this issue, we introduce ViA, a novel View-Invariant Autoencoder for self-supervised skeleton action representation learning. ViA leverages motion retargeting between different human performers as a pretext task, in order to disentangle the latent action-specific `Motion' features on top of the visual representation of a 2D or 3D skeleton sequence. Such `Motion' features are invariant to skeleton geometry and camera view and allow ViA to facilitate both, cross-subject and cross-view action classification tasks. We conduct a study focusing on transfer-learning for skeleton-based action recognition with self-supervised pre-training on real-world data (e.g., Posetics). Our results showcase that skeleton representations learned from ViA are generic enough to improve upon state-of-the-art action classification accuracy, not only on 3D laboratory datasets such as NTU-RGB+D 60 and NTU-RGB+D 120, but also on real-world datasets where only 2D data are accurately estimated, e.g., Toyota Smarthome, UAV-Human and Penn Action."}}
{"id": "7r6kDq0mK_", "cdate": 1632875497348, "mdate": null, "content": {"title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation", "abstract": "Due to the remarkable progress of deep generative models, animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation is instrumental in transferring motion from driving videos to still images. However, such approaches fail in case the source image and driving video encompass large appearance variation. Moreover, the extraction of structure information requires additional modules that endow the animation-model with increased complexity. Deviating from such models, we here introduce the Latent Image Animator (LIA), a self-supervised autoencoder that evades need for structure representation. LIA is streamlined to animate images by linear navigation in the latent space. Specifically, motion in generated video is constructed by linear displacement of codes in the latent space. Towards this, we learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement in the latent space. Extensive quantitative and qualitative analysis suggests that our model systematically and significantly outperforms state-of-art methods on VoxCeleb, Taichi and TED-talk datasets w.r.t. generated quality."}}
{"id": "wuDI17v9hXb", "cdate": 1609459200000, "mdate": 1668059099765, "content": {"title": "Self-Supervised Video Pose Representation Learning for Occlusion- Robust Action Recognition", "abstract": ""}}
{"id": "mXtohqdSE2", "cdate": 1609459200000, "mdate": 1668059099811, "content": {"title": "UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition", "abstract": ""}}
{"id": "V87VQ3VUYif", "cdate": 1609459200000, "mdate": 1668059099775, "content": {"title": "Joint Generative and Contrastive Learning for Unsupervised Person Re-Identification", "abstract": "Recent self-supervised contrastive learning provides an effective approach for unsupervised person re-identification (ReID) by learning invariance from different views (transformed versions) of an input. In this paper, we incorporate a Generative Adversarial Network (GAN) and a contrastive learning module into one joint training framework. While the GAN provides online data augmentation for contrastive learning, the contrastive module learns view-invariant features for generation. In this context, we propose a mesh-based view generator. Specifically, mesh projections serve as references towards generating novel views of a person. In addition, we propose a view-invariant loss to facilitate contrastive learning between original and generated views. Deviating from previous GAN-based unsupervised ReID methods involving domain adaptation, we do not rely on a labeled source dataset, which makes our method more flexible. Extensive experimental results show that our method significantly outperforms state-of-the-art methods under both, fully unsupervised and unsupervised domain adaptive settings on several large scale ReID datsets."}}
{"id": "Qcdzvl1hpYf", "cdate": 1609459200000, "mdate": 1668059099746, "content": {"title": "InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation", "abstract": "In this work, we introduce an unconditional video generative model, InMoDeGAN, targeted to (a) generate high quality videos, as well as to (b) allow for interpretation of the latent space. For the latter, we place emphasis on interpreting and manipulating motion. Towards this, we decompose motion into semantic sub-spaces, which allow for control of generated samples. We design the architecture of InMoDeGAN-generator in accordance to proposed Linear Motion Decomposition, which carries the assumption that motion can be represented by a dictionary, with related vectors forming an orthogonal basis in the latent space. Each vector in the basis represents a semantic sub-space. In addition, a Temporal Pyramid Discriminator analyzes videos at different temporal resolutions. Extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the VoxCeleb2-mini and BAIR-robot datasets w.r.t. video quality related to (a). Towards (b) we present experimental results, confirming that decomposed sub-spaces are interpretable and moreover, generated motion is controllable."}}
{"id": "KzqD-vhL6QJ", "cdate": 1609459200000, "mdate": 1668059099835, "content": {"title": "Selective Spatio-Temporal Aggregation Based Pose Refinement System: Towards Understanding Human Activities in Real-World Videos", "abstract": ""}}
{"id": "O2P8gXclQytJ", "cdate": 1599588744986, "mdate": null, "content": {"title": "G3AN: Disentangling Appearance and Motion for Video Generation", "abstract": "Creating realistic human videos entails the challenge of being able to simultaneously generate both appearance, as well as motion. To tackle this challenge, we introduce G3AN, a novel spatio-temporal generative model, which seeks to capture the distribution of high dimensional video data and to model appearance and motion in disentangled manner. The latter is achieved by decomposing appearance and motion in a three-stream Generator, where the main stream aims to model spatio-temporal consistency, whereas the two auxiliary streams augment the main stream with multi-scale appearance and motion features, respectively. An extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the facial expression datasets MUG and UvA-NEMO, as well as the Weizmann and UCF101 datasets on human action. Additional analysis on the learned latent representations confirms the successful decomposition of appearance and motion."}}
{"id": "OVZiSOJult", "cdate": 1577836800000, "mdate": 1668059099730, "content": {"title": "ImaGINator: Conditional Spatio-Temporal GAN for Video Generation", "abstract": ""}}
{"id": "2V3H2I9NtW", "cdate": 1577836800000, "mdate": 1668059099751, "content": {"title": "A video is worth more than 1000 lies. Comparing 3DCNN approaches for detecting deepfakes", "abstract": ""}}
