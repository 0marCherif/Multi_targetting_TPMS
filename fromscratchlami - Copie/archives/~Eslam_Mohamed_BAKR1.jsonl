{"id": "-AxpnEv1f1", "cdate": 1652737452821, "mdate": null, "content": {"title": "Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding", "abstract": "3D visual grounding task has been explored with visual and language streams to comprehend referential language for identifying targeted objects in 3D scenes.\nHowever, most existing methods devote the visual stream to capture the 3D visual clues using off-the-shelf point clouds encoders. The main question we address is \u201ccan we consolidate the 3D visual stream by 2D clues and efficiently utilize them in both training and testing phases?\u201d. The main idea is to assist the 3D encoder by incorporating rich 2D object representations without requiring extra 2D inputs. \nTo this end, we leverage 2D clues, synthetically generated from 3D point clouds, that empirically show their aptitude to boost the quality of the learned visual representations. We validate our approach through comprehensive experiments on Nr3D, Sr3D, and ScanRefer datasets. Our experiments show consistent performance gains against counterparts, where our proposed module, dubbed as LAR, significantly outperforms state-of-the-art 3D visual grounding techniques on three benchmarks.\nOur code will be made publicly available."}}
{"id": "nEEWKsdHqTQ", "cdate": 1640995200000, "mdate": 1668510789304, "content": {"title": "EMCA: Efficient Multiscale Channel Attention Module", "abstract": "Attention mechanisms have been explored with CNNs across the spatial and channel dimensions. However, all the existing methods devote the attention modules to capture local interactions from a uni-scale. This paper tackles the following question: can one consolidate multi-scale aggregation while learning channel attention more efficiently? To this end, we avail channel-wise attention over multiple feature scales, which empirically shows its aptitude to replace the limited local and uni-scale attention modules. EMCA is lightweight and can efficiently model the global context further; it is easily integrated into any feed-forward CNN architectures and trained in an end-to-end fashion. We validate our novel architecture through comprehensive experiments on image classification, object detection, and instance segmentation with different backbones. Our experiments show consistent gains in performances against their counterparts, where our proposed module, named EMCA, outperforms other channel attention techniques in accuracy and latency trade-off. More specifically, compared to SENet, we boost the accuracy by 0.8 %, 0.6 %, and 1 % on ImageNet benchmark for ResNet-18, 34, and 50, respectively. For detection and segmentation tasks, MS-COCO are for benchmarking, Our EMCA module boost the accuracy by 0.5 % and 0.3 %, respectively. We also conduct experiments that probe the robustness of the learned representations. Our code will be published once the paper is accepted."}}
{"id": "X3WxnuzAYyE", "cdate": 1632875745423, "mdate": null, "content": {"title": "PKCAM: Previous Knowledge Channel Attention Module", "abstract": "Attention mechanisms have been explored with CNNs, both across the spatial and channel dimensions. \nHowever, all the existing methods devote the attention modules to capture local interactions from the current feature map only, disregarded the valuable previous knowledge that is acquired by the earlier layers. \nThis paper tackles the following question: Can one incorporate previous knowledge aggregation while learning channel attention more efficiently? To this end, we propose a Previous Knowledge Channel Attention Module( PKCAM), that captures channel-wise relations across different layers to model the global context. \nOur proposed module PKCAM is easily integrated into any feed-forward CNN architectures and trained in an end-to-end fashion with a negligible footprint due to its lightweight property. We validate our novel architecture through extensive experiments on image classification and object detection tasks with different backbones. \nOur experiments show consistent improvements in performances against their counterparts. We also conduct experiments that probe the robustness of the learned representations."}}
