{"id": "DKGSg8yNUqf", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Tunable Loss Function for Classification", "abstract": "We introduce a tunable loss function called $\\alpha$-loss, parameterized by $\\alpha \\in (0,\\infty]$, which interpolates between the exponential loss ($\\alpha = 1/2$), the log-loss ($\\alpha = 1$), and the 0-1 loss ($\\alpha = \\infty$), for the machine learning setting of classification. Theoretically, we illustrate a fundamental connection between $\\alpha$-loss and Arimoto conditional entropy, verify the classification-calibration of $\\alpha$-loss in order to demonstrate asymptotic optimality via Rademacher complexity generalization techniques, and build-upon a notion called strictly local quasi-convexity in order to quantitatively characterize the optimization landscape of $\\alpha$-loss. Practically, we perform class imbalance, robustness, and classification experiments on benchmark image datasets using convolutional-neural-networks. Our main practical conclusion is that certain tasks may benefit from tuning $\\alpha$-loss away from log-loss ($\\alpha = 1$), and to this end we provide simple heuristics for the practitioner. In particular, navigating the $\\alpha$ hyperparameter can readily provide superior model robustness to label flips ($\\alpha > 1$) and sensitivity to imbalanced classes ($\\alpha < 1$)."}}
{"id": "AMZfyq13PxH", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Tunable Loss Function for Binary Classification", "abstract": "We present $\\alpha$-loss, $\\alpha \\in [1,\\infty]$, a tunable loss function for binary classification that bridges log-loss ($\\alpha=1$) and $0$-$1$ loss ($\\alpha = \\infty$). We prove that $\\alpha$-loss has an equivalent margin-based form and is classification-calibrated, two desirable properties for a good surrogate loss function for the ideal yet intractable $0$-$1$ loss. For logistic regression-based classification, we provide an upper bound on the difference between the empirical and expected risk at the empirical risk minimizers for $\\alpha$-loss by exploiting its Lipschitzianity along with recent results on the landscape features of empirical risk functions. Finally, we show that $\\alpha$-loss with $\\alpha = 2$ performs better than log-loss on MNIST for logistic regression."}}
