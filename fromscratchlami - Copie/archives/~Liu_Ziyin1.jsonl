{"id": "gt22v5enJ1", "cdate": 1672531200000, "mdate": 1681651242855, "content": {"title": "The Probabilistic Stability of Stochastic Gradient Descent", "abstract": ""}}
{"id": "0eOsvQW3YH", "cdate": 1672531200000, "mdate": 1681651242855, "content": {"title": "On the stepwise nature of self-supervised learning", "abstract": ""}}
{"id": "J9EyxEpWYVj", "cdate": 1664194165568, "mdate": null, "content": {"title": "On Rotational Symmetry in the Loss landscape of Self-Supervised Learning", "abstract": "We derive an analytically tractable theory of SSL landscape and show that it accurately captures an array of collapse phenomena and identifies their causes. "}}
{"id": "idY99Ugd5ek", "cdate": 1663850059148, "mdate": null, "content": {"title": "Sparsity by Redundancy: Solving $L_1$ with a Simple Reparametrization", "abstract": "We identify and prove a general principle: $L_1$ sparsity can be achieved using a redundant parametrization plus $L_2$ penalty. Our results lead to a simple algorithm, \\textit{spred}, that seamlessly integrates $L_1$ regularization into any modern deep learning framework. Practically, we demonstrate (1) the efficiency of \\textit{spred} in optimizing conventional tasks such as lasso and sparse coding, (2) benchmark our method for nonlinear feature selection of six gene selection tasks, and (3) illustrate the usage of the method for achieving structured and unstructured sparsity in deep learning in an end-to-end manner. Conceptually, our result bridges the gap in understanding the inductive bias of the redundant parametrization common in deep learning and conventional statistical learning."}}
{"id": "3zSn48RUO8M", "cdate": 1663849858413, "mdate": null, "content": {"title": "What shapes the loss landscape of self supervised learning?", "abstract": "Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance."}}
{"id": "X6bp8ri8dV", "cdate": 1652737456646, "mdate": null, "content": {"title": "Exact Solutions of a Deep Linear Network", "abstract": "This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general."}}
{"id": "zAc2a6_0aHb", "cdate": 1652737304402, "mdate": null, "content": {"title": "Posterior Collapse of a Linear Latent Variable Model", "abstract": "This work identifies the existence and cause of a type of posterior collapse that frequently occurs in the Bayesian deep learning practice. For a general linear latent variable model that includes linear variational autoencoders as a special case, we precisely identify the nature of posterior collapse to be the competition between the likelihood and the regularization of the mean due to the prior. Our result also suggests that posterior collapse may be a general problem of learning for deeper architectures and deepens our understanding of Bayesian deep learning."}}
{"id": "zBcX_XOcSk", "cdate": 1640995200000, "mdate": 1671730156091, "content": {"title": "Sparsity by Redundancy: Solving L1 with a Simple Reparametrization", "abstract": "We propose to minimize a generic differentiable objective with $L_1$ constraint using a simple reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \\textit{spred}, is an exact differentiable solver of $L_1$ and that the reparametrization trick is completely ``benign\" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning."}}
{"id": "rrlb4oYyUlc", "cdate": 1640995200000, "mdate": 1645767068084, "content": {"title": "Stochastic Neural Networks with Infinite Width are Deterministic", "abstract": "This work theoretically studies stochastic neural networks, a main type of neural network in use. We prove that as the width of an optimized stochastic neural network tends to infinity, its predictive variance on the training set decreases to zero. Our theory justifies the common intuition that adding stochasticity to the model can help regularize the model by introducing an averaging effect. Two common examples that our theory can be relevant to are neural networks with dropout and Bayesian latent variable models in a special limit. Our result thus helps better understand how stochasticity affects the learning of neural networks and potentially design better architectures for practical problems."}}
{"id": "XesuHidrag", "cdate": 1640995200000, "mdate": 1681651242815, "content": {"title": "SGD Can Converge to Local Maxima", "abstract": ""}}
