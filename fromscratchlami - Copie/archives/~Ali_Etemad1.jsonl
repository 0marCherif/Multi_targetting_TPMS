{"id": "puhCOv5Y-a", "cdate": 1672531200000, "mdate": 1681657336270, "content": {"title": "Partial Label Learning for Emotion Recognition from EEG", "abstract": "Fully supervised learning has recently achieved promising performance in various electroencephalography (EEG) learning tasks by training on large datasets with ground truth labels. However, labeling EEG data for affective experiments is challenging, as it can be difficult for participants to accurately distinguish between similar emotions, resulting in ambiguous labeling (reporting multiple emotions for one EEG instance). This notion could cause model performance degradation, as the ground truth is hidden within multiple candidate labels. To address this issue, Partial Label Learning (PLL) has been proposed to identify the ground truth from candidate labels during the training phase, and has shown good performance in the computer vision domain. However, PLL methods have not yet been adopted for EEG representation learning or implemented for emotion recognition tasks. In this paper, we adapt and re-implement six state-of-the-art PLL approaches for emotion recognition from EEG on a large emotion dataset (SEED-V, containing five emotion classes). We evaluate the performance of all methods in classical and real-world experiments. The results show that PLL methods can achieve strong results in affective computing from EEG and achieve comparable performance to fully supervised learning. We also investigate the effect of label disambiguation, a key step in many PLL methods. The results show that in most cases, label disambiguation would benefit the model when the candidate labels are generated based on their similarities to the ground truth rather than obeying a uniform distribution. This finding suggests the potential of using label disambiguation-based PLL methods for real-world affective tasks. We make the source code of this paper publicly available at: https://github.com/guangyizhangbci/PLL-Emotion-EEG."}}
{"id": "E9NW9J8fdx", "cdate": 1672531200000, "mdate": 1681657336708, "content": {"title": "Audio Representation Learning by Distilling Video as Privileged Information", "abstract": "Deep audio representation learning using multi-modal audio-visual data often leads to a better performance compared to uni-modal approaches. However, in real-world scenarios both modalities are not always available at the time of inference, leading to performance degradation by models trained for multi-modal inference. In this work, we propose a novel approach for deep audio representation learning using audio-visual data when the video modality is absent at inference. For this purpose, we adopt teacher-student knowledge distillation under the framework of learning using privileged information (LUPI). While the previous methods proposed for LUPI use soft-labels generated by the teacher, in our proposed method we use embeddings learned by the teacher to train the student network. We integrate our method in two different settings: sequential data where the features are divided into multiple segments throughout time, and non-sequential data where the entire features are treated as one whole segment. In the non-sequential setting both the teacher and student networks are comprised of an encoder component and a task header. We use the embeddings produced by the encoder component of the teacher to train the encoder of the student, while the task header of the student is trained using ground-truth labels. In the sequential setting, the networks have an additional aggregation component that is placed between the encoder and task header. We use two sets of embeddings produced by the encoder and aggregation component of the teacher to train the student. Similar to the non-sequential setting, the task header of the student network is trained using ground-truth labels. We test our framework on two different audio-visual tasks, namely speaker recognition and speech emotion recognition and show considerable improvements over sole audio-based recognition as well as prior works that use LUPI."}}
{"id": "84nHWbNwSE", "cdate": 1672531200000, "mdate": 1681657335991, "content": {"title": "Deep Gait Recognition: A Survey", "abstract": "Gait recognition is an appealing biometric modality which aims to identify individuals based on the way they walk. Deep learning has reshaped the research landscape in this area since 2015 through the ability to automatically learn discriminative representations. Gait recognition methods based on deep learning now dominate the state-of-the-art in the field and have fostered real-world applications. In this paper, we present a comprehensive overview of breakthroughs and recent developments in gait recognition with deep learning, and cover broad topics including datasets, test protocols, state-of-the-art solutions, challenges, and future research directions. We first review the commonly used gait datasets along with the principles designed for evaluating them. We then propose a novel taxonomy made up of four separate dimensions namely body representation, temporal representation, feature representation, and neural architecture, to help characterize and organize the research landscape and literature in this area. Following our proposed taxonomy, a comprehensive survey of gait recognition methods using deep learning is presented with discussions on their performances, characteristics, advantages, and limitations. We conclude this survey with a discussion on current challenges and mention a number of promising directions for future research in gait recognition."}}
{"id": "5cgEaZYXIn", "cdate": 1672531200000, "mdate": 1681657336359, "content": {"title": "Human Pose Estimation from Ambiguous Pressure Recordings with Spatio-temporal Masked Transformers", "abstract": "Despite the impressive performance of vision-based pose estimators, they generally fail to perform well under adverse vision conditions and often don't satisfy the privacy demands of customers. As a result, researchers have begun to study tactile sensing systems as an alternative. However, these systems suffer from noisy and ambiguous recordings. To tackle this problem, we propose a novel solution for pose estimation from ambiguous pressure data. Our method comprises a spatio-temporal vision transformer with an encoder-decoder architecture. Detailed experiments on two popular public datasets reveal that our model outperforms existing solutions in the area. Moreover, we observe that increasing the number of temporal crops in the early stages of the network positively impacts the performance while pre-training the network in a self-supervised setting using a masked auto-encoder approach also further improves the results."}}
{"id": "14DR2fGa84", "cdate": 1672531200000, "mdate": 1681657336755, "content": {"title": "A Study on Bias and Fairness In Deep Speaker Recognition", "abstract": "With the ubiquity of smart devices that use speaker recognition (SR) systems as a means of authenticating individuals and personalizing their services, fairness of SR systems has becomes an important point of focus. In this paper we study the notion of fairness in recent SR systems based on 3 popular and relevant definitions, namely Statistical Parity, Equalized Odds, and Equal Opportunity. We examine 5 popular neural architectures and 5 commonly used loss functions in training SR systems, while evaluating their fairness against gender and nationality groups. Our detailed experiments shed light on this concept and demonstrate that more sophisticated encoder architectures better align with the definitions of fairness. Additionally, we find that the choice of loss functions can significantly impact the bias of SR models."}}
{"id": "5gSmWrtspt", "cdate": 1667354341441, "mdate": null, "content": {"title": "Vote from the center: 6 dof pose estimation in rgb-d images by radial keypoint voting", "abstract": "Abstract. We propose a novel keypoint voting scheme based on intersecting spheres, that is more accurate than existing schemes and allows\nfor fewer, more disperse keypoints. The scheme is based upon the distance\nbetween points, which as a 1D quantity can be regressed more accurately\nthan the 2D and 3D vector and offset quantities regressed in previous\nwork, yielding more accurate keypoint localization. The scheme forms\nthe basis of the proposed RCVPose method for 6 DoF pose estimation\nof 3D objects in RGB-D data, which is particularly effective at handling\nocclusions. A CNN is trained to estimate the distance between the 3D\npoint corresponding to the depth mode of each RGB pixel, and a set of\n3 disperse keypoints defined in the object frame. At inference, a sphere\ncentered at each 3D point is generated, of radius equal to this estimated\ndistance. The surfaces of these spheres vote to increment a 3D accumulator space, the peaks of which indicate keypoint locations. The pro\u0002posed radial voting scheme is more accurate than previous vector or offset\nschemes, and is robust to disperse keypoints. Experiments demonstrate\nRCVPose to be highly accurate and competitive, achieving state-of-the-art results on the LINEMOD (99.7%) and YCB-Video (97.2%) datasets,\nnotably scoring +4.9% higher (71.1%) than previous methods on the\nchallenging Occlusion LINEMOD dataset, and on average outperforming\nall other published results from the BOP benchmark for these 3 datasets.\nOur code is available at http://www.github.com/aaronwool/rcvpose.\n"}}
{"id": "XMe8Qa0reVz", "cdate": 1662013900020, "mdate": 1662013900020, "content": {"title": "Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity", "abstract": "We present CrissCross, a self-supervised framework for learning audio-visual representations. A novel notion is introduced in our framework whereby in addition to learning the intra-modal and standard \u2018synchronous\u2019 cross-modal relations, CrissCross also learns \u2018asynchronous\u2019 cross-modal relationships. We show that by relaxing the temporal synchronicity between the audio and visual modalities, the network learns strong generalized representations. Our experiments show that strong augmentations for both audio and visual modalities with relaxation of cross-modal temporal synchronicity optimize performance. To pretrain our proposed framework, we use 3 different datasets with varying sizes, Kinetics-Sound, Kinetics400, and AudioSet. The learned representations are evaluated on a number of downstream tasks namely action recognition, sound classification, and retrieval. CrissCross shows state-of-the-art performances on action recognition (UCF101 and HMDB51) and sound classification (ESC50 and DCASE). The codes and pretrained models will be made publicly available.\n"}}
{"id": "BvI3NvnOOLT", "cdate": 1662013837074, "mdate": 1662013837074, "content": {"title": "AVCaffe: A Large Scale Audio-Visual Dataset of Cognitive Load and Affect for Remote Work  ", "abstract": "We introduce AVCAffe, the first Audio-Visual dataset consisting of Cognitive load and Affect attributes. We record AVCAffe by simulating remote work scenarios over a video-conferencing platform, where subjects collaborate to complete a number of cognitively engaging tasks. AVCAffe is the largest originally collected (not collected from the Internet) affective dataset in English language. We recruit 106 participants from 18 different countries of origin, spanning an age range of 18 to 57 years old, with a balanced male-female ratio. AVCAffe comprises a total of 108 hours of video, equivalent to more than 58, 000 clips along with task-based self-reported ground truth labels for arousal, valence, and cognitive load attributes such as mental demand, temporal demand, effort, and a few others. We believe AVCAffe would be a challenging benchmark for the deep learning research community given the inherent difficulty of classifying affect and cognitive load in particular. Moreover, our dataset fills an existing timely gap by facilitating the creation of learning systems for better self-management of remote work meetings, and further study of hypotheses regarding the impact of remote work on cognitive load and affective states."}}
{"id": "4nKaoJSyI4", "cdate": 1662013711632, "mdate": 1662013711632, "content": {"title": "PARSE: Pairwise Alignment of Representations in Semi-Supervised EEG Learning for Emotion Recognition", "abstract": "We propose PARSE, a novel semi-supervised architecture for learning strong EEG representations for emotion recognition. To reduce the potential distribution mismatch between the large amounts of unlabeled data and the limited amount of labeled data, PARSE uses pairwise representation alignment. First, our model performs data augmentation followed by label guessing for large amounts of original and augmented unlabeled data. This is then followed by sharpening of the guessed labels and convex combinations of the unlabeled and labeled data. Finally, representation alignment and emotion classification are performed. To rigorously test our model, we compare PARSE to several state-of-the-art semi-supervised approaches which we implement and adapt for EEG learning. We perform these experiments on four public EEG-based emotion recognition datasets, SEED, SEED-IV, SEED-V and AMIGOS (valence and arousal). The experiments show that our pro- posed framework achieves the overall best results with varying amounts of limited labeled samples in SEED, SEED-IV and AMIGOS (valence), while approaching the overall best result (reaching the second-best) in SEED-V and AMIGOS (arousal). The analysis shows that our pairwise representation alignment considerably improves the performance by reducing the distribution alignment between unlabeled and labeled data, especially when only 1 sample per class is labeled."}}
{"id": "RZlQzdFjGr-", "cdate": 1662013579045, "mdate": 1662013579045, "content": {"title": "Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation", "abstract": "3D hand pose estimation (HPE) is the process of locating the joints of the hand in 3D from any visual input. HPE has recently received an increased amount of attention due to its key role in a variety of human-computer interaction applications. Recent HPE methods have demonstrated the advantages of employing videos or multi-view images, allowing for more robust HPE systems. Accordingly, in this study, we propose a new method to perform Sequential learning with Transformer for Hand Pose (SeTHPose) estimation. Our SeTHPose pipeline begins by extracting visual embeddings from individual hand images. We then use a transformer encoder to learn the sequential context along time or viewing angles and generate accurate 2D hand joint locations. Then, a graph convolutional neural network with a U- Net configuration is used to convert the 2D hand joint locations to 3D poses. Our experiments show that SeTHPose performs well on both hand sequence varieties, temporal and angular. Also, SeTHPose outperforms other methods in the field to achieve new state-of-the-art results on two public available sequential datasets, STB and MuViHand."}}
