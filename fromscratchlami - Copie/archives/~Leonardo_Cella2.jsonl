{"id": "d6-TYNU3oR", "cdate": 1683880362689, "mdate": null, "content": {"title": "Multi-task representation learning with stochastic linear bandits", "abstract": "We study the problem of transfer-learning in the setting of stochastic linear contextual bandit tasks. We consider that a low dimensional linear representation is shared across the tasks, and study the benefit of learning the tasks jointly. Following recent results to design Lasso stochastic bandit policies, we propose an efficient greedy policy based on trace norm regularization. It implicitly learns a low dimensional representation by encouraging the matrix formed by the task regression vectors to be of low rank. Unlike previous work in the literature, our policy does not need to know the rank of the underlying matrix, nor {does} it requires the covariance of the arms distribution to be invertible. We derive an upper bound on the multi-task regret of our policy, which is, up to logarithmic factors, of order $T\\sqrt{rN} + \\sqrt{rNTd}$ , where $T$ is the number of tasks, $r$ the rank, $d$ the number of variables and $N$ the number of rounds per task. We show the benefit of our strategy over an independent task learning baseline, which has a worse regret of order $T\\sqrt{dN}$. We also argue that our policy {is minimax optimal} and, when $T\\geq d$, has a multi-task regret which is comparable to the regret of an oracle policy which knows the true underlying representation."}}
{"id": "lfe1CdzuXBJ", "cdate": 1652737753139, "mdate": null, "content": {"title": "Group Meritocratic Fairness in Linear Contextual Bandits", "abstract": "We study the linear contextual bandit problem where an agent has to select one candidate from a pool and each candidate belongs to a sensitive group. In this setting, candidates' rewards may not be directly comparable between groups, for example when the agent is an employer hiring candidates from different ethnic groups and some groups have a lower reward due to discriminatory bias and/or social injustice. We propose a notion of fairness that states that the agent's policy is fair when it selects a candidate with highest relative rank, \nwhich measures how good the reward is when compared to candidates from the same group. This is a very strong notion of fairness, since the relative rank is not directly observed by the agent and depends on the underlying reward model and on the distribution of rewards. Thus we study the problem of learning a policy which approximates a fair policy under the condition that the contexts are independent between groups and the distribution of rewards of each group is absolutely continuous. In particular, we design a greedy policy which at each round constructs a ridge regression estimate from the observed context-reward pairs, and then computes an estimate of the relative rank of each candidate using the empirical cumulative distribution function. We prove that, despite its simplicity and the lack of an initial exploration phase, the greedy policy achieves, up to log factors and with high probability, a fair pseudo-regret of order $\\sqrt{dT}$ after $T$ rounds, where $d$ is the dimension of the context vectors. The policy also satisfies demographic parity at each round when averaged over all possible information available before the selection. Finally, we use simulated settings and experiments on the US census data to show that our policy achieves sub-linear fair pseudo-regret also in practice."}}
{"id": "rYifgyfPulR", "cdate": 1621501283702, "mdate": null, "content": {"title": "Meta-learning with stochastic linear bandits", "abstract": "We investigate meta-learning procedures in the setting of stochastic linear bandits tasks. The goal is to select a learning algorithm which works well on average over a class of bandits tasks, that are sampled from a task-distribution. Inspired by recent work on learning-to-learn linear regression, we consider a class of bandit algorithms that implement a regularized version of the well-known OFUL algorithm, where the regularization is a square euclidean distance to a bias vector. We first study the benefit of the biased OFUL algorithm in terms of regret minimization. We then propose two strategies to estimate the bias within the learning-to-learn setting. We show both theoretically and experimentally, that when the number of tasks grows and the variance of the task-distribution is small, our strategies have a significant advantage over learning the tasks in isolation."}}
{"id": "WmyaTpXhb8U", "cdate": 1546300800000, "mdate": null, "content": {"title": "Stochastic Bandits with Delay-Dependent Payoffs.", "abstract": "Motivated by recommendation problems in music streaming platforms, we propose a nonstationary stochastic bandit model in which the expected reward of an arm depends on the number of rounds that have passed since the arm was last pulled. After proving that finding an optimal policy is NP-hard even when all model parameters are known, we introduce a class of ranking policies provably approximating, to within a constant factor, the expected reward of the optimal policy. We show an algorithm whose regret with respect to the best ranking policy is bounded by $\\widetilde{\\mathcal{O}}\\big(\\!\\sqrt{kT}\\big)$, where $k$ is the number of arms and $T$ is time. Our algorithm uses only $\\mathcal{O}\\big(k\\ln\\ln T\\big)$ switches, which helps when switching between policies is costly. As constructing the class of learning policies requires ordering the arms according to their expectations, we also bound the number of pulls required to do so. Finally, we run experiments to compare our algorithm against UCB on different problem instances."}}
{"id": "4ahOAkIgGq", "cdate": 1546300800000, "mdate": null, "content": {"title": "Efficient Linear Bandits through Matrix Sketching.", "abstract": "We prove that two popular linear contextual bandit algorithms, OFUL and Thompson Sampling, can be made efficient using Frequent Directions, a deterministic online sketching technique. More precisel..."}}
{"id": "LjLnvVJ5cIp", "cdate": 1514764800000, "mdate": null, "content": {"title": "Efficient Linear Bandits through Matrix Sketching.", "abstract": "We prove that two popular linear contextual bandit algorithms, OFUL and Thompson Sampling, can be made efficient using Frequent Directions, a deterministic online sketching technique. More precisely, we show that a sketch of size $m$ allows a $\\mathcal{O}(md)$ update time for both algorithms, as opposed to $\\Omega(d^2)$ required by their non-sketched versions in general (where $d$ is the dimension of context vectors). This computational speedup is accompanied by regret bounds of order $(1+\\varepsilon_m)^{3/2}d\\sqrt{T}$ for OFUL and of order $\\big((1+\\varepsilon_m)d\\big)^{3/2}\\sqrt{T}$ for Thompson Sampling, where $\\varepsilon_m$ is bounded by the sum of the tail eigenvalues not covered by the sketch. In particular, when the selected contexts span a subspace of dimension at most $m$, our algorithms have a regret bound matching that of their slower, non-sketched counterparts. Experiments on real-world datasets corroborate our theoretical results."}}
{"id": "D-Ffc2rc8l", "cdate": 1514764800000, "mdate": null, "content": {"title": "Efficient Context-Aware Sequential Recommender System.", "abstract": "Traditional collaborative filtering, and content-based approaches attempt to learn a static recommendation model in a batch fashion. These approaches are not suitable in highly dynamic recommendation scenarios, like news recommendation and computational advertisement. Due to this well-known limitation, in the last decade a lot of efforts have been spent over the study of online learning techniques. Currently, a lot of attention has been devoted to improvements on the theoretical guarantees, without caring too much about computational cost and memory footprint. However, in the era of big-data content features tend to be high-dimensional, which leads to a direct challenge for traditional on-line learning algorithms (e.g., multi-armed bandits) since these are mostly designed for low-dimensional feature spaces. In this work we face the aforementioned problem, investigating an approximated context-aware bandit learner. Our model takes into account the problem of finding the actual low-dimensional manifold spanned by data content-features. In particular, we propose to store the covariance matrix of the previously seen contexts in a compressed space, without losing too much in terms of recommendation quality. With this work we provide an overview over the main properties, describe the adopted techniques, and report on preliminary experimental results on a synthetic dataset. We also discuss a drawback of the proposed method that may appear in typical scenarios and suggest future research avenues."}}
{"id": "qSEXNWqENQ_", "cdate": 1483228800000, "mdate": null, "content": {"title": "Deriving Item Features Relevance from Past User Interactions.", "abstract": "Item-based recommender systems suggest products based on the similarities between items computed either from past user preferences (collaborative filtering) or from item content features (content-based filtering). Collaborative filtering has been proven to outperform content-based filtering in a variety of scenarios. However, in item cold-start, collaborative filtering cannot be used directly since past user interactions are not available for the newly added items. Hence, content-based filtering is usually the only viable option left."}}
{"id": "mPjYOosQ-50", "cdate": 1483228800000, "mdate": null, "content": {"title": "Estimate Features Relevance for Groups of Users.", "abstract": ""}}
{"id": "hF_JowVDtec", "cdate": 1483228800000, "mdate": null, "content": {"title": "Modelling User Behaviors with Evolving Users and Catalogs of Evolving Items.", "abstract": "Recommender systems are used to suggest users products that they would not be able to find by themselves. State of the art algorithms assume that items have static features, however this assumption does not always correspond to reality. There are challenging and still unexplored domains, where not only users but also items have properties that evolve continuously over time. In this research we aim to overcome these limitations by suggesting to model evolution of users and items as a reinforcement learning problem. As use case we will refer to the recommendation problem applied to the financial domain, where items' (contracts) features evolve continuously according to \"market laws\"."}}
