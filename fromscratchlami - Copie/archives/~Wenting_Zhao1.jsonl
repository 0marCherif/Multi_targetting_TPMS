{"id": "h9KrUP1BEP", "cdate": 1672531200000, "mdate": 1687833884421, "content": {"title": "HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision", "abstract": "Explainable multi-hop question answering (QA) not only predicts answers but also identifies rationales, i. e. subsets of input sentences used to derive the answers. This problem has been extensively studied under the supervised setting, where both answer and rationale annotations are given. Because rationale annotations are expensive to collect and not always available, recent efforts have been devoted to developing methods that do not rely on supervision for rationales. However, such methods have limited capacities in modeling interactions between sentences, let alone reasoning across multiple documents. This work proposes a principled, probabilistic approach for training explainable multi-hop QA systems without rationale supervision. Our approach performs multi-hop reasoning by explicitly modeling rationales as sets, enabling the model to capture interactions between documents and sentences within a document. Experimental results show that our approach is more accurate at selecting rationales than the previous methods, while maintaining similar accuracy in predicting answers."}}
{"id": "7qiUwsMyMnn", "cdate": 1672531200000, "mdate": 1687833884417, "content": {"title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations", "abstract": "Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmented zero-shot methods."}}
{"id": "PkHSHZLig5H", "cdate": 1654191669797, "mdate": null, "content": {"title": "Modeling Perspective-Dependent Ambiguity in Collaborative Dialogue", "abstract": "Errors in reference generation and resolution can occur if a dialogue agent reasons incorrectly about their partner's perspective. We present, in a collaborative and visually grounded setting, a dialogue planner that infers its partner's perspective to produce referring expressions that the partner resolves correctly. The dialogue planner models the partner perspective as a latent variable, embedded in a partner model that is used for both model-based planning and incorporating evidence from partner responses. We validate our approach on \\textsc{OneCommon}, a challenging dialogue game where players have large differences in their perspectives. In symbolic selfplay, where agents partner with a copy of themselves using symbolic communication, the dialogue planner improves over a planning baseline that does not reason about the partner's full perspective."}}
{"id": "rg-zrfteOZc", "cdate": 1646950669411, "mdate": null, "content": {"title": "Commonsense Reasoning for Question Answering with Explanations", "abstract": "Commonsense reasoning is an important capability for a range of AI applications such as text understanding. Neural models for commonsense reasoning QA often directly predict answers based on learned representations of language. In this work, we consider the challenge of producing an explicit reasoning step for a commonsense QA system. We propose a latent-variable model that identifies what type of knowledge from an external knowledge base may be relevant to answering the question, computes the commonsense inferences, and predicts the answer. Our method can therefore learn to provide posterior rationales for why a certain answer was chosen. Experimental results show that the model can identify the correct reasoning step in twice as many examples compared to an existing unsupervised approach for producing explanations, while still maintaining comparable accuracy to end-to-end pretrained models."}}
{"id": "RReyvaT-Ni", "cdate": 1640995200000, "mdate": 1687833884470, "content": {"title": "Compositional Task-Oriented Parsing as Abstractive Question Answering", "abstract": "Task-oriented parsing (TOP) aims to convert natural language into machine-readable representations of specific tasks, such as setting an alarm. A popular approach to TOP is to apply seq2seq models to generate linearized parse trees. A more recent line of work argues that pretrained seq2seq models are better at generating outputs that are themselves natural language, so they replace linearized parse trees with canonical natural-language paraphrases that can then be easily translated into parse trees, resulting in so-called naturalized parsers. In this work we continue to explore naturalized semantic parsing by presenting a general reduction of TOP to abstractive question answering that overcomes some limitations of canonical paraphrasing. Experimental results show that our QA-based technique outperforms state-of-the-art methods in full-data settings while achieving dramatic improvements in few-shot settings."}}
{"id": "rXa4PSGXJjd", "cdate": 1620346825312, "mdate": null, "content": {"title": "Zero Training Overhead Portfolios for Learning to Solve Combinatorial Problems", "abstract": "There has been an increasing interest in harnessing deep learning to tackle combinatorial optimization (CO) problems in recent years. Typical CO deep learning approaches leverage the problem structure in the model architecture. Nevertheless, the model selection is still mainly based on the conventional machine learning setting. Due to the discrete nature of CO problems, a single model is unlikely to learn the problem entirely. We introduce ZTop, which stands for Zero Training Overhead Portfolio, a simple yet effective model selection and ensemble mechanism for learning to solve combinatorial problems. ZTop is inspired by algorithm portfolios, a popular CO ensembling strategy, particularly restart portfolios, which periodically restart a randomized CO algorithm, de facto exploring the search space with different heuristics. We have observed that well-trained models acquired in the same training trajectory, with similar top validation performance, perform well on very different validation instances. Following this observation, ZTop ensembles a set of well-trained models, each providing a unique heuristic with zero training overhead, and applies them, sequentially or in parallel, to solve the test instances. We show how ZTopping, i.e., using a ZTop ensemble strategy with a given deep learning approach, can significantly improve the performance of the current state-of-the-art deep learning approaches on three prototypical CO domains, the hardest unique-solution Sudoku instances, challenging routing problems, and the graph maximum cut problem, as well as on multi-label classification, a machine learning task with a large combinatorial label space."}}
{"id": "HkehD3VtvS", "cdate": 1569438820510, "mdate": null, "content": {"title": "Deep Reasoning Networks:  Thinking Fast and Slow, for Pattern De-mixing", "abstract": "We introduce Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with reasoning for solving pattern de-mixing problems, typically in an unsupervised or weakly-supervised setting.  DRNets exploit problem structure and prior knowledge by tightly combining logic and constraint reasoning with stochastic-gradient-based neural network optimization.  We illustrate the power of DRNets on de-mixing overlapping hand-written Sudokus (Multi-MNIST-Sudoku) and on a substantially more complex task in scientific discovery that concerns inferring crystal structures of materials from X-ray diffraction data (Crystal-Structure-Phase-Mapping). DRNets significantly outperform the state of the art and experts' capabilities on Crystal-Structure-Phase-Mapping, recovering more precise and physically meaningful crystal structures. On Multi-MNIST-Sudoku, DRNets perfectly recovered the mixed Sudokus' digits, with 100% digit accuracy, outperforming the supervised state-of-the-art MNIST de-mixing models."}}
