{"id": "EoblE5EsYG", "cdate": 1698555986580, "mdate": 1698555986580, "content": {"title": "Unsupervised Open-Vocabulary Object Localization in Videos", "abstract": "In this paper, we show that recent advances in video representation learning and pre-trained vision-language models allow for substantial improvements in self-supervised video object localization. We propose a method that first localizes objects in videos via a slot attention approach and then assigns text to the obtained slots. The latter is achieved by an unsupervised way to read localized semantic information from the pre-trained CLIP model. The resulting video object localization is entirely unsupervised apart from the implicit annotation contained in CLIP, and it is effectively the first unsupervised approach that yields good results on regular video benchmarks."}}
{"id": "IyBlj_JCtwQ", "cdate": 1684250514641, "mdate": 1684250514641, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a\nsafety-critical challenge inevitably faced by all autonomous\ndriving systems. Existing image and video driving datasets,\nhowever, fall short of capturing the mutable nature of the\nreal world. In this paper, we introduce the largest multitask synthetic dataset for autonomous driving, SHIFT. It\npresents discrete and continuous shifts in cloudiness, rain\nand fog intensity, time of day, and vehicle and pedestrian\ndensity. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT\nallows investigating the degradation of a perception system performance at increasing levels of domain shift, fostering the development of continuous adaptation strategies\nto mitigate this problem and assess model robustness and\ngenerality. Our dataset and benchmark toolkit are publicly\navailable at www.vis.xyz/shift."}}
{"id": "MosSgoAdSmQ", "cdate": 1668678400597, "mdate": 1668678400597, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous-driving systems. Existing image- and video-based driving datasets, however, fall short of capturing the mutable nature of the real world. In this paper, we introduce the largest multi-task synthetic dataset for autonomous driving, SHIFT. It presents discrete and continuous shifts in cloudiness, rain and fog intensity, time of day, and vehicle and pedestrian density. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT allows to investigate how a perception systems' performance degrades at increasing levels of domain shift, fostering the development of continuous adaptation strategies to mitigate this problem and assessing the robustness and generality of a model. Our dataset and benchmark toolkit are publicly available at www.vis.xyz/shift."}}
{"id": "OM0FsMD6NG", "cdate": 1668627692646, "mdate": 1668627692646, "content": {"title": "f-vaegan-d2: A feature generating framework for any-shot learning", "abstract": "When labeled training data is scarce, a promising data augmentation approach is to generate visual features of unknown classes using their attributes. To learn the class conditional distribution of CNN features, these models rely on pairs of image features and class attributes. Hence, they can not make use of the abundance of unlabeled data samples. In this paper, we tackle any-shot learning problems ie zero-shot and few-shot, in a unified feature generating framework that operates in both inductive and transductive learning settings. We develop a conditional generative model that combines the strength of VAE and GANs and in addition, via an unconditional discriminator, learns the marginal feature distribution of unlabeled images. We empirically show that our model learns highly discriminative CNN features for five datasets, ie CUB, SUN, AWA and ImageNet, and establish a new state-of-the-art in any-shot learning, ie inductive and transductive (generalized) zero-and few-shot learning settings. We also demonstrate that our learned features are interpretable: we visualize them by inverting them back to the pixel space and we explain them by generating textual arguments of why they are associated with a certain label."}}
{"id": "RM0ho4YqDYE", "cdate": 1668627400189, "mdate": 1668627400189, "content": {"title": "Long-tailed recognition using class-balanced experts", "abstract": "Deeplearningenablesimpressiveperformanceinimagerecog- nition using large-scale artificially-balanced datasets. However, real-world datasets exhibit highly class-imbalanced distributions, yielding two main challenges: relative imbalance amongst the classes and data scarcity for mediumshot or fewshot classes. In this work, we address the problem of long-tailed recognition wherein the training set is highly imbalanced and the test set is kept balanced. Differently from existing paradigms relying on data-resampling, cost-sensitive learning, online hard exam- ple mining, loss objective reshaping, and/or memory-based modeling, we propose an ensemble of class-balanced experts that combines the strength of diverse classifiers. Our ensemble of class-balanced experts reaches results close to state-of-the-art and an extended ensemble estab- lishes a new state-of-the-art on two benchmarks for long-tailed recogni- tion. We conduct extensive experiments to analyse the performance of the ensembles, and discover that in modern large-scale datasets, relative imbalance is a harder problem than data scarcity. The training and eval- uation code is available at https://github.com/ssfootball04/ class-balanced-experts."}}
{"id": "CBYxLztZuq", "cdate": 1666700534644, "mdate": 1666700534644, "content": {"title": "Convolutional Dynamic Alignment Networks for Interpretable Classifications", "abstract": "We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which linearly transform their input with weight vectors that dynamically align with task-relevant patterns. As a result, CoDA-Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA-Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet."}}
{"id": "ju9B0jGIMNw", "cdate": 1666700418221, "mdate": 1666700418221, "content": {"title": "Towards Better Understanding Attribution Methods", "abstract": "Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models\u2019 decisions. Evaluating such methods is chal- lenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make compar- isons between them more fair, and to make visual inspec- tion more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are ap- plied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantita- tive metrics. For more systematic visualizations, we pro- pose a scheme (AggAtt) to qualitatively evaluate the meth- ods on complete datasets. We use these evaluation schemes to study strengths and shortcomings of some widely used attribution methods. Finally, we propose a post-processing smoothing step that significantly improves the performance of some attribution methods, and discuss its applicability."}}
{"id": "PeWqtulPJw", "cdate": 1666700305566, "mdate": 1666700305566, "content": {"title": "B-cos Networks: Alignment is All We Need for Interpretability", "abstract": "We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faithfully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced linear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common models such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability. Code available at this https URL"}}
{"id": "PDrUPTXJI_A", "cdate": 1663850200880, "mdate": null, "content": {"title": "FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning", "abstract": "Semi-supervised Learning (SSL) has witnessed great success owing to the impressive performances brought by various methods based on pseudo labeling and consistency regularization. However, we argue that existing methods might fail to utilize the unlabeled data more effectively since they either use a pre-defined / fixed threshold or an ad-hoc threshold adjusting scheme, resulting in inferior performance and slow convergence. We first analyze a motivating example to obtain intuitions on the relationship between the desirable threshold and model's learning status. Based on the analysis, we hence propose FreeMatch to adjust the confidence threshold in a self-adaptive manner according to the model's learning status. We further introduce a self-adaptive class fairness regularization penalty to encourage the model for diverse predictions during the early training stage. Extensive experiments indicate the superiority of FreeMatch especially when the labeled data are extremely rare. FreeMatch achieves 5.78%, 13.59%, and 1.28% error rate reduction over the latest state-of-the-art method FlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class, and ImageNet with 100 labels per class, respectively. Moreover, FreeMatch can also boost the performance of imbalanced SSL. The codes can be found at https://github.com/microsoft/Semi-supervised-learning.\n"}}
{"id": "ymt1zQXBDiF", "cdate": 1663850199461, "mdate": null, "content": {"title": "SoftMatch: Addressing the Quantity-Quality Tradeoff in Semi-supervised Learning", "abstract": "The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification."}}
