{"id": "gtQZaa2HKpZ", "cdate": 1667336385423, "mdate": 1667336385423, "content": {"title": "Towards extremely compact rnns for video recognition with fully decomposed hierarchical tucker structure", "abstract": "Recurrent Neural Networks (RNNs) have been widely\nused in sequence analysis and modeling. However, when\nprocessing high-dimensional data, RNNs typically require\nvery large model sizes, thereby bringing a series of deployment challenges. Although various prior works have been\nproposed to reduce the RNN model sizes, executing RNN\nmodels in the resource-restricted environments is still a very\nchallenging problem. In this paper, we propose to develop\nextremely compact RNN models with fully decomposed hierarchical Tucker (FDHT) structure. The HT decomposition does not only provide much higher storage cost reduction than the other tensor decomposition approaches, but\nalso brings better accuracy performance improvement for\nthe compact RNN models. Meanwhile, unlike the existing\ntensor decomposition-based methods that can only decompose the input-to-hidden layer of RNNs, our proposed fully\ndecomposition approach enables the comprehensive compression for the entire RNN models with maintaining very\nhigh accuracy. Our experimental results on several popular\nvideo recognition datasets show that, our proposed fully decomposed hierarchical tucker-based LSTM (FDHT-LSTM)\nis extremely compact and highly efficient. To the best of\nour knowledge, FDHT-LSTM, for the first time, consistently\nachieves very high accuracy with only few thousand parameters (3,132 to 8,808) on different datasets. Compared\nwith the state-of-the-art compressed RNN models, such as\nTT-LSTM, TR-LSTM and BT-LSTM, our FDHT-LSTM simultaneously enjoys both order-of-magnitude (3,985\u02c6 to\n10,711\u02c6) fewer parameters and significant accuracy improvement (0.6% to 12.7%)."}}
{"id": "4JqP0yhXAkS", "cdate": 1667336310453, "mdate": 1667336310453, "content": {"title": "HODEC: Towards Efficient High-Order DEcomposed Convolutional Neural Networks", "abstract": "High-order decomposition is a widely used model compression approach towards compact convolutional neural\nnetworks (CNNs). However, many of the existing solutions,\nthough can efficiently reduce CNN model sizes, are very difficult to bring considerable saving for computational costs,\nespecially when the compression ratio is not huge, thereby\ncausing the severe computation inefficiency problem. To\novercome this challenge, in this paper we propose efficient\nHigh-Order DEcomposed Convolution (HODEC). By performing systematic explorations on the underlying reason\nand mitigation strategy for the computation inefficiency, we\ndevelop a new decomposition and computation-efficient execution scheme, enabling simultaneous reductions in computational and storage costs.\nTo demonstrate the effectiveness of HODEC, we perform empirical evaluations for various CNN models on different datasets. HODEC shows consistently outstanding\ncompression and acceleration performance. For compressing ResNet-56 on CIFAR-10 dataset, HODEC brings 67%\nfewer parameters and 62% fewer FLOPs with 1.17% accuracy increase than the baseline model. For compressing ResNet-50 on ImageNet dataset, HODEC achieves 63%\nFLOPs reduction with 0.31% accuracy increase than the\nuncompressed model."}}
{"id": "PrRWSVT2htx", "cdate": 1663850026198, "mdate": null, "content": {"title": "CEPD: Co-Exploring Pruning and Decomposition for Compact DNN Models", "abstract": "Pruning and decomposition are two important techniques to compress deep neural network (DNN) models. To date, these two popular yet distinct approaches are typically used in a separate way; while their efficient integration for better compression performance is little explored. In this paper, we perform systematic co-exploration on pruning and decomposition toward compact DNN models. We first investigate and analyze several important design factors for joint pruning and decomposition, including operational sequence, decomposition format, and optimization procedure. Based on the observations from our analysis, we then propose CEPD, a unified DNN compression framework that can simultaneously capture the benefits of pruning and decomposition in an efficient way. Empirical experiments demonstrate the promising performance of our proposed solution. Notably, on CIFAR-10 dataset, CEPD brings 0.72% and 0.45% accuracy increase over the baseline ResNet-56 and MobileNetV2 models, respectively, and meanwhile the computational costs are reduced by 43.0% and 44.2%, respectively. On the ImageNet dataset, our approach can enable 0.10% and 1.39% accuracy increase over the baseline ResNet-18 and ResNet-50 models with 59.4% and 54.6% fewer parameters, respectively. "}}
{"id": "TC39w69m8bB", "cdate": 1663850025700, "mdate": null, "content": {"title": "ELRT: Towards Efficient Low-Rank Training for Compact Neural Networks", "abstract": "Low-rank compression, a popular model compression technique that produces compact convolutional neural networks (CNNs) with low rankness, has been well studied in the literature. On the other hand, low-rank training, as an alternative way to train low-rank CNNs from scratch, is little exploited yet. Unlike low-rank compression, low-rank training does not need pre-trained full-rank models and the entire training phase is always performed on the low-rank structure, bringing attractive benefits for practical applications. However, the existing low-rank training solutions are still very limited and do not demonstrate their effectiveness for training modern low-rank CNN models in the large-scale dataset from scratch. In this paper, we perform a systematic investigation on low-rank CNN training. By identifying the proper low-rank format and performance-improving strategy, we propose ELRT, an efficient low-rank training solution for high-accuracy high-compactness low-rank CNN models. Our extensive evaluation results for training various CNNs on different datasets demonstrate the effectiveness of ELRT."}}
{"id": "BYZxX9NNfX5", "cdate": 1648669834591, "mdate": 1648669834591, "content": {"title": "Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework", "abstract": "Advanced tensor decomposition, such as Tensor train (TT) and Tensor ring (TR), has been widely studied for deep neural network (DNN) model compression, especially for recurrent neural networks (RNNs). However, compressing convolutional neural networks (CNNs) using TT/TR always suffers significant accuracy loss. In this paper, we propose a systematic framework for tensor decomposition-based model compression using Alternating Direction Method of Multipliers (ADMM). By formulating TT decomposition-based model compression to an optimization problem with constraints on tensor ranks, we leverage ADMM technique to systemically solve this optimization problem in an iterative way. During this procedure, the entire DNN model is trained in the original structure instead of TT format, but gradually enjoys the desired low tensor rank characteristics. We then decompose this uncompressed model to TT format and fine-tune it to finally obtain a high-accuracy TT-format DNN model. Our framework is very general, and it works for both CNNs and RNNs, and can be easily modified to fit other tensor decomposition approaches. We evaluate our proposed framework on different DNN models for image classification and video recognition tasks. Experimental results show that our ADMM-based TT-format models demonstrate very high compression performance with high accuracy. Notably, on CIFAR-100, with 2.3X and 2.4X compression ratios, our models have 1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and ResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model achieves 2.47X FLOPs reduction without accuracy loss."}}
{"id": "uBR8WKhfGK", "cdate": 1640995200000, "mdate": 1668567238940, "content": {"title": "BATUDE: Budget-Aware Neural Network Compression Based on Tucker Decomposition", "abstract": "Model compression is very important for the efficient deployment of deep neural network (DNN) models on resource-constrained devices. Among various model compression approaches, high-order tensor decomposition is particularly attractive and useful because the decomposed model is very small and fully structured. For this category of approaches, tensor ranks are the most important hyper-parameters that directly determine the architecture and task performance of the compressed DNN models. However, as an NP-hard problem, selecting optimal tensor ranks under the desired budget is very challenging and the state-of-the-art studies suffer from unsatisfied compression performance and timing-consuming search procedures. To systematically address this fundamental problem, in this paper we propose BATUDE, a Budget-Aware TUcker DEcomposition-based compression approach that can efficiently calculate optimal tensor ranks via one-shot training. By integrating the rank selecting procedure to the DNN training process with a specified compression budget, the tensor ranks of the DNN models are learned from the data and thereby bringing very significant improvement on both compression ratio and classification accuracy for the compressed models. The experimental results on ImageNet dataset show that our method enjoys 0.33% top-5 higher accuracy with 2.52X less computational cost as compared to the uncompressed ResNet-18 model. For ResNet-50, the proposed approach enables 0.37% and 0.55% top-5 accuracy increase with 2.97X and 2.04X computational cost reduction, respectively, over the uncompressed model."}}
{"id": "d0GZbFWa0V1", "cdate": 1640995200000, "mdate": 1668567238932, "content": {"title": "HODEC: Towards Efficient High-Order DEcomposed Convolutional Neural Networks", "abstract": "High-order decomposition is a widely used model compression approach towards compact convolutional neural networks (CNNs). However, many of the existing solutions, though can efficiently reduce CNN model sizes, are very difficult to bring considerable saving for computational costs, especially when the compression ratio is not huge, thereby causing the severe computation inefficiency problem. To overcome this challenge, in this paper we propose efficient High-Order DEcomposed Convolution (HODEC). By performing systematic explorations on the underlying reason and mitigation strategy for the computation inefficiency, we develop a new decomposition and computation-efficient execution scheme, enabling simultaneous reductions in computational and storage costs. To demonstrate the effectiveness of HODEC, we perform empirical evaluations for various CNN models on different datasets. HODEC shows consistently outstanding compression and acceleration performance. For compressing ResNet-56 on CIFAR-10 dataset, HODEC brings 67% fewer parameters and 62% fewer FLOPs with 1.17% accuracy increase than the baseline model. For compressing ResNet-50 on ImageNet dataset, HODEC achieves 63% FLOPs reduction with 0.31% accuracy increase than the uncompressed model."}}
{"id": "cTmvXu-iVDN", "cdate": 1640995200000, "mdate": 1668567239041, "content": {"title": "Robot Motion Planning as Video Prediction: A Spatio-Temporal Neural Network-based Motion Planner", "abstract": "Neural network (NN)-based methods have emerged as an attractive approach for robot motion planning due to strong learning capabilities of NN models and their inherently high parallelism. Despite the current development in this direction, the efficient capture and processing of important sequential and spatial information, in a direct and simultaneous way, is still relatively under-explored. To overcome the challenge and unlock the potentials of neural networks for motion planning tasks, in this paper, we propose STP-Net, an end-to-end learning framework that can fully extract and leverage important spatio-temporal information to form an efficient neural motion planner. By interpreting the movement of the robot as a video clip, robot motion planning is transformed to a video prediction task that can be performed by STP-Net in both spatially and temporally efficient ways. Empirical evaluations across different seen and unseen environments show that, with nearly 100% accuracy (aka, success rate), STP-Net demonstrates very promising performance with respect to both planning speed and path cost. Compared with existing NN-based motion planners, STP-Net achieves at least 5x, 2.6x and 1.8x faster speed with lower path cost on 2D Random Forest, 2D Maze and 3D Random Forest environments, respectively. Furthermore, STP-Net can quickly and simultaneously compute multiple near-optimal paths in multi-robot motion planning tasks"}}
{"id": "RD8wI2XbyE", "cdate": 1640995200000, "mdate": 1668567239069, "content": {"title": "A Graph Convolutional Network for Point Cloud Completion", "abstract": "In this paper, we propose a graph convolutional network for point cloud completion (GCNC), predicting the missing region. The local information is embedded into the global features by a graph neural network module, which comprehensively captures relations among points through a graph-based context aggregation. On the other hand, the skip connection effectively utilizes the local structural details of incomplete point clouds during the inference of multi-scale missing parts, which preserves the detailed structure of the complete point cloud. Extensive experiments on the ShapeNet and ModelNet40 benchmark show that the proposed approach outperforms the previous baselines, highlighting its effectiveness. Our quantitative and qualitative analysis confirms the effectiveness of each part of the method."}}
{"id": "CQ8azCO-b1r", "cdate": 1640995200000, "mdate": 1668567239017, "content": {"title": "TDC: Towards Extremely Efficient CNNs on GPUs via Hardware-Aware Tucker Decomposition", "abstract": "Tucker decomposition is one of the SOTA CNN model compression techniques. However, unlike the FLOPs reduction, we observe very limited inference time reduction with Tucker-compressed models using existing GPU software such as cuDNN. To this end, we propose an efficient end-to-end framework that can generate highly accurate and compact CNN models via Tucker decomposition and optimized inference code on GPUs. Specifically, we propose an ADMM-based training algorithm that can achieve highly accurate Tucker-format models. We also develop a high-performance kernel for Tucker-format convolutions and analytical performance models to guide the selection of execution parameters. We further propose a co-design framework to determine the proper Tucker ranks driven by practical inference time (rather than FLOPs). Our evaluation on five modern CNNs with A100 demonstrates that our compressed models with our optimized code achieve up to 2.21X speedup over cuDNN, 1.12X speedup over TVM, and 3.27X over the original models using cuDNN with at most 0.05% accuracy loss."}}
