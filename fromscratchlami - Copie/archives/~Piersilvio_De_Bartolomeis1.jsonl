{"id": "sbabGqakIc", "cdate": 1668734781415, "mdate": null, "content": {"title": "Certified defences hurt generalisation", "abstract": "In recent years, much work has been devoted to designing certified\ndefences for neural networks, i.e., methods for learning neural\nnetworks that are provably robust to certain adversarial\nperturbations. Due to the non-convexity of the problem, dominant\napproaches in this area rely on convex approximations, which are\ninherently loose. In this paper, we question the effectiveness of such\napproaches for realistic computer vision tasks. First, we provide\nextensive empirical evidence to show that certified defences suffer\nnot only worse accuracy but also worse robustness and fairness than\nempirical defences. We hypothesise that the reason for why certified\ndefences suffer in generalisation is (i) the large number of\nrelaxed non-convex constraints and (ii) strong alignment between the\nadversarial perturbations and the \"signal\" direction. We provide a\ncombination of theoretical and experimental evidence to support these\nhypotheses."}}
{"id": "h1j5I0WVxoI", "cdate": 1664725483013, "mdate": null, "content": {"title": "Certified defences hurt generalisation", "abstract": "In recent years, much work has been devoted to designing certified defences for neural networks, i.e., methods for learning neural networks that are provably robust to certain adversarial perturbations. Due to the non-convexity of the problem, dominant approaches in this area rely on convex approximations, which are inherently loose. In this paper, we question the effectiveness of such approaches for realistic computer vision tasks. First, we provide extensive empirical evidence to show that certified defences suffer not only worse accuracy but also worse robustness and fairness than empirical defences. We hypothesise that the reason for why certified defences suffer in generalisation is (i) the large number of relaxed non-convex constraints and (ii) strong alignment between the adversarial perturbations and the \"signal\" direction. We provide a combination of theoretical and experimental evidence to support these hypotheses."}}
{"id": "-XVMGVmjNLs", "cdate": 1653750180805, "mdate": null, "content": {"title": "Enhancing Unit-tests for Invariance Discovery", "abstract": "Recently, Aubin et al. (2021) proposed a set of linear low-dimensional problems to precisely evaluate different types of out-of-distribution generalization. In this paper, we show that one of these problems can already be solved by established algorithms, simply by better hyper-parameter tuning.  We then propose an enhanced version of the linear unit-tests. To the best of our hyper-parameter search and within the set of algorithms evaluated, AND-mask is the best performing algorithm on this new suite of tests. Our findings on synthetic data are further reinforced by experiments on an image classification task where we introduce spurious correlations.\n\n"}}
{"id": "13S0tUMqynI", "cdate": 1652737691551, "mdate": null, "content": {"title": "Challenging Common Assumptions in Convex Reinforcement Learning", "abstract": "The classic Reinforcement Learning (RL) formulation concerns the maximization of a scalar reward function. More recently, convex RL has been introduced to extend the RL formulation to all the objectives that are convex functions of the state distribution induced by a policy. Notably, convex RL covers several relevant applications that do not fall into the scalar formulation, including imitation learning, risk-averse RL, and pure exploration. In classic RL, it is common to optimize an infinite trials objective, which accounts for the state distribution instead of the empirical state visitation frequencies, even though the actual number of trajectories is always finite in practice. This is theoretically sound since the infinite trials and finite trials objectives are equivalent and thus lead to the same optimal policy. In this paper, we show that this hidden assumption does not hold in convex RL. In particular, we prove that erroneously optimizing the infinite trials objective in place of the actual finite trials one, as it is usually done, can lead to a significant approximation error. Since the finite trials setting is the default in both simulated and real-world RL, we believe shedding light on this issue will lead to better approaches and methodologies for convex RL, impacting relevant research areas such as imitation learning, risk-averse RL, and pure exploration among others. "}}
