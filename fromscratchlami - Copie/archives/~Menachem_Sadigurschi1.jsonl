{"id": "Kzuys6WghCV", "cdate": 1621629894602, "mdate": null, "content": {"title": "On the Sample Complexity of Privately Learning Axis-Aligned Rectangles", "abstract": "We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a finite grid $X^d\\subseteq\\mathbb{R}^d$ with differential privacy. Existing results show that the sample complexity of this problem is at most $\\min\\left\\{ d{\\cdot}\\log|X| \\;,\\; d^{1.5}{\\cdot}\\left(\\log^*|X| \\right)^{1.5}\\right\\}$. That is, existing constructions either require sample complexity that grows linearly with $\\log|X|$, or else it grows super linearly with the dimension $d$.  We present a novel algorithm that reduces the sample complexity to only $\\tilde{O}\\left\\{d{\\cdot}\\left(\\log^*|X|\\right)^{1.5}\\right\\}$,  attaining a dimensionality optimal dependency without requiring the sample complexity to grow with $\\log|X|$. The technique used in order to attain this improvement involves the deletion of \"exposed\" data-points on the go, in a fashion designed to avoid the cost of the adaptive composition theorems.\nThe core of this technique may be of individual interest, introducing a new method for constructing statistically-efficient private algorithms.\n"}}
{"id": "mQnjmyqQkQ9", "cdate": 1546300800000, "mdate": null, "content": {"title": "Sample Compression for Real-Valued Learners.", "abstract": "We give an algorithmically efficient version of the learner-to-compression scheme conversion in Moran and Yehudayoff (2016). We further extend this technique to real-valued hypotheses, to obtain a bounded-size sample compression scheme via an efficient reduction to a certain generic real-valued learning strategy. To our knowledge, this is the first general compressed regression result (regardless of efficiency or boundedness) guaranteeing uniform approximate reconstruction. Along the way, we develop a generic procedure for constructing weak real-valued learners out of abstract regressors; this result is also of independent interest. In particular, this result sheds new light on an open question of H. Simon (1997). We show applications to two regression problems: learning Lipschitz and bounded-variation functions."}}
{"id": "icX84h8fjOB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Agnostic Sample Compression for Linear Regression.", "abstract": "We obtain the first positive results for bounded sample compression in the agnostic regression setting. We show that for p in {1,infinity}, agnostic linear regression with $\\ell_p$ loss admits a bounded sample compression scheme. Specifically, we exhibit efficient sample compression schemes for agnostic linear regression in $R^d$ of size $d+1$ under the $\\ell_1$ loss and size $d+2$ under the $\\ell_\\infty$ loss. We further show that for every other $\\ell_p$ loss (1 < p < infinity), there does not exist an agnostic compression scheme of bounded size. This refines and generalizes a negative result of David, Moran, and Yehudayoff (2016) for the $\\ell_2$ loss. We close by posing a general open question: for agnostic regression with $\\ell_1$ loss, does every function class admit a compression scheme of size equal to its pseudo-dimension? This question generalizes Warmuth's classic sample compression conjecture for realizable-case classification (Warmuth, 2003)."}}
