{"id": "Cf9SdqYINS", "cdate": 1705927539161, "mdate": null, "content": {"title": "C\u00b2A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding", "abstract": "Spoken language understanding (SLU) is a critical task in taskoriented dialogue systems. However, automatic speech recognition (ASR) errors often impair the understanding performance. Despite many previous models have obtained promising results for improving ASR robustness in SLU, most of them treat clean manual transcripts and ASR transcripts equally during the finetuning stage. To tackle this issue, in this paper, we propose a novel method termed C2A-SLU. Specifically speaking, we add calculated cross attention to the original hidden states and apply contrastive attention to compare the input transcript with clean manual transcripts to distill the contrastive information, which can better capture distinctive features of ASR transcripts. Experiments on three datasets show that C2A-SLU surpasses existing models and achieves a new state-of-the-art performance, with a relative improvement of 3.4% in terms of accuracy over the previous best model on SLURP dataset."}}
{"id": "CtNwWtXdM0", "cdate": 1705927070211, "mdate": null, "content": {"title": "FC-MTLF: a fine-and coarse-grained multi-task learning framework for cross-lingual spoken language understanding", "abstract": "Currently, zero-shot cross-lingual spoken language understanding (SLU) attracts increasing attention. Most of existing methods construct a mixed-language context via the code-switching approach. However, due to the different syntactic structures of each language, code-switching might fail to perform well and result in the loss of semantics. To address this issue, we propose a novel framework termed FC-MTLF, which applies a multitask learning by introducing an auxiliary multilingual neural machine translation (NMT) task to compensate for the shortcomings of code-switching. In addition, we also adopt the curriculum learning strategy to further improve the performance. Experimental results show that our framework achieves the new state-of-the-art performance on the MultiATIS++ dataset. Further analysis verifies that our FC-MTLF can effectively transfer knowledge from source languages to target languages."}}
{"id": "eVz1K4cuQ_f", "cdate": 1702815983482, "mdate": 1702815983482, "content": {"title": "C2A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding", "abstract": "Spoken language understanding (SLU) is a critical task in task-oriented dialogue systems. However, automatic speech recognition (ASR) errors often impair the understanding performance. Despite many previous models have obtained promising results for improving ASR robustness in SLU, most of them treat clean manual transcripts and ASR transcripts equally during the finetuning stage. To tackle this issue, in this paper, we propose a novel method termed C2A-SLU. Specifically speaking, we add calculated cross attention to the original hidden states and apply contrastive attention to compare the input transcript with clean manual transcripts to distill the contrastive information, which can better capture distinctive features of ASR transcripts. Experiments on three datasets show that C2A-SLU surpasses existing models and achieves a new state-of-the-art performance, with a relative improvement of 3.4% in terms of accuracy over the previous best model on SLURP dataset."}}
{"id": "3KOz73S_h7I", "cdate": 1702815742436, "mdate": 1702815742436, "content": {"title": "FC-MTLF: A Fine-and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding", "abstract": "Currently, zero-shot cross-lingual spoken language understanding (SLU) attracts increasing attention. Most of existing methods construct a mixed-language context via the code-switching approach. However, due to the different syntactic structures of each language, code-switching might fail to perform well and result in the loss of semantics. To address this issue, we propose a novel framework termed FC-MTLF, which applies a multitask learning by introducing an auxiliary multilingual neural machine translation (NMT) task to compensate for the shortcomings of code-switching. In addition, we also adopt the curriculum learning strategy to further improve the performance. Experimental results show that our framework achieves the new state-of-the-art performance on the MultiATIS++ dataset. Further analysis verifies that our FC-MTLF can effectively transfer knowledge from source languages to target languages."}}
{"id": "a96rJpERen", "cdate": 1702815477681, "mdate": 1702815477681, "content": {"title": "GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering", "abstract": "Spoken question answering (SQA) aims to identify the correct answer to the given the question from a spoken passage. Most conventional SQA frameworks combine an automatic speech recognition (ASR) module and a text question answering (TQA) module in a cascaded manner, which might suffer from error propagation and high latency. To tackle these issues, several end-to-end SQA frameworks based on Textless NLP are proposed. However, existing end-to-end models still fail to outperform the cascade models with the similar number of parameters. In this paper, to improve textless SQA, we propose GhostT5, which generates more features from the remaining features with very cheap operations for stronger performance. Experiment results and further analysis show that our GhostT5 achieves the new state-of-the-art performance on NMSQA dataset and surpasses cascaded SQA models. More encouragingly, GhostT5 surpasses the previous best end-to-end SQA model with less than half of the parameters."}}
