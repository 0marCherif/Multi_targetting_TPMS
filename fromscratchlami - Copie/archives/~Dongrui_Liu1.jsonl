{"id": "rH_5aICeWmP", "cdate": 1672531200000, "mdate": 1683886361143, "content": {"title": "Concept-Level Explanation for the Generalization of a DNN", "abstract": "This paper explains the generalization power of a deep neural network (DNN) from the perspective of interactive concepts. Many recent studies have quantified a clear emergence of interactive concepts encoded by the DNN, which have been observed on different DNNs during the learning process. Therefore, in this paper, we investigate the generalization power of each interactive concept, and we use the generalization power of different interactive concepts to explain the generalization power of the entire DNN. Specifically, we define the complexity of each interactive concept. We find that simple concepts can be better generalized to testing data than complex concepts. The DNN with strong generalization power usually learns simple concepts more quickly and encodes fewer complex concepts. More crucially, we discover the detouring dynamics of learning complex concepts, which explain both the high learning difficulty and the low generalization power of complex concepts."}}
{"id": "FLMvYXMucWk", "cdate": 1663849860563, "mdate": null, "content": {"title": "Temporary feature collapse phenomenon in early learning of MLPs", "abstract": "In this paper, we focus on a typical two-phase phenomenon in the learning of multi-layer perceptrons (MLPs). We discover and explain the reason for the feature collapse phenomenon in the first phase, i.e., the diversity of features over different samples keeps decreasing in the first phase, until samples of different categories share almost the same feature, which hurts the optimization of MLPs. We explain such a phenomenon in terms of the learning dynamics of MLPs. Furthermore, we theoretically analyze the reason why four typical operations can alleviate the feature collapse. The code has been attached with the submission."}}
{"id": "pG9RSmBrY3", "cdate": 1663849853084, "mdate": null, "content": {"title": "Formulating and Proving the Trend of DNNs Learning Simple Concepts", "abstract": "This paper theoretically explains the intuition that simple concepts are more likely to be learned by deep neural networks (DNNs) than complex concepts. Beyond empirical studies, our research first specifies an exact definition of the complexity of the concept that boosts the learning difficulty. Specifically, it is proven that the inference logic of a neural network can be represented as a causal graph. In this way, causal patterns in the causal graph can be used to formulate interactive concepts learned by the neural network. Based on such formulation, we explain the reason why simple interactive concepts in the data are more likely to be learned than complex interactive concepts. More crucially, we discover that our research provides a new perspective to explain previous understandings of the conceptual complexity. The code will be released when the paper is accepted."}}
{"id": "wn22mhf-1SG", "cdate": 1640995200000, "mdate": 1683889106129, "content": {"title": "Self-supervised Point Cloud Registration with Deep Versatile Descriptors", "abstract": "As a fundamental yet challenging problem in intelligent transportation systems, point cloud registration attracts vast attention and has been attained with various deep learning-based algorithms. The unsupervised registration algorithms take advantage of deep neural network-enabled novel representation learning while requiring no human annotations, making them applicable to industrial applications. However, unsupervised methods mainly depend on global descriptors, which ignore the high-level representations of local geometries. In this paper, we propose to jointly use both global and local descriptors to register point clouds in a self-supervised manner, which is motivated by a critical observation that all local geometries of point clouds are transformed consistently under the same transformation. Therefore, local geometries can be employed to enhance the representation ability of the feature extraction module. Moreover, the proposed local descriptor is flexible and can be integrated into most existing registration methods and improve their performance. Besides, we also utilize point cloud reconstruction and normal estimation to enhance the transformation awareness of global and local descriptors. Lastly, extensive experimental results on one synthetic and three real-world datasets demonstrate that our method outperforms existing state-of-art unsupervised registration methods and even surpasses supervised ones in some cases. Robustness and computational efficiency evaluations also indicate that the proposed method applies to intelligent vehicles."}}
{"id": "rjpu1l4r6MN", "cdate": 1640995200000, "mdate": 1683889106123, "content": {"title": "PointFP: A Feature-Preserving Point Cloud Sampling", "abstract": "Many 3D perception applications (e.g., detection, classification, and segmentation) that directly process point cloud have made great progress with the development of 3D sensors in recent years. However, computational costs and storage demands of these applications grow significantly with the increase of point cloud size. Thus, it is necessary to sam-ple the point cloud while preserving semantic features and uniform density. Deep learning-based sampling methods are task-specific and fail to generalize to different tasks. Hence, we propose a task agnostic point cloud simplification method, called point cloud feature-preserving. The proposed method preserves semantic features from the original point cloud by generating representative nodes. The sampling rate could be controlled by adjusting the number of representative nodes. Qualitative and quantitative experimental results on synthetic and real-world remote sensing point cloud datasets demon-strate the effectiveness of the proposed method."}}
{"id": "jbrinpmQcK", "cdate": 1640995200000, "mdate": 1683889106026, "content": {"title": "PFMixer: Point Cloud Frequency Mixing", "abstract": "Convolutional Neural Networks and Transformer are popu-lar for various computer vision tasks. Recently, a modern multi-layer perceptrons (MLPs) architecture, MLP-Mixer, has achieved remarkable performance in visual recognition tasks. MLP-Mixer includes two types of MLPs: channel-mixing and token-mixing MLPs. Inspired by its simplicity and success, we introduce the application of MLP-Mixer for point cloud processing. We design token-mixing operations to allow interaction between high-frequency (edge) regions and low-frequency (smooth) regions. Furthermore, channel-mixing operations are proposed to allow interaction between different channels of local structures and global shapes. Ex-tensive experiments demonstrate the superiority of PFMixer: POINT CLOUD FREQUENCY MIXING"}}
{"id": "emASIfTV40", "cdate": 1640995200000, "mdate": 1683889105969, "content": {"title": "SGCNN for 3D Point Cloud Classification", "abstract": "3D point cloud processing is challenging, as the points in the point cloud are disordered and irregularly distributed. Graph-based networks leverage the underlying topological relationship between points and achieve satisfactory performance in point cloud classification task. However, we observe that traditional graph construction and aggregation methods limit their efficiency. To address this problem, we propose a Sparse Graph Convolution Neural Network (SGCNN). Specifically, we apply a Sparse Graph Convolution (SGC) module to reduce the computation complexity of graph convolution and a Sparse Feature Encoding (SFE) module to enrich the representation of the point cloud in terms of sparse neighbor. The classification performances on synthetic and real-world benchmarks demonstrate the superiority and effectiveness of the proposed method. Compared with state-of-the-art methods, our approach balances accuracy and efficiency."}}
{"id": "Kna-gixYoS5", "cdate": 1640995200000, "mdate": 1667576805918, "content": {"title": "A Robust and Reliable Point Cloud Recognition Network Under Rigid Transformation", "abstract": "Point cloud recognition is an essential task in industrial robotics and autonomous driving. Recently, several point cloud processing models have achieved state-of-the-art performances. However, these methods lack rotation robustness, and their performances degrade severely under random rotations, failing to extend to real-world scenarios with varying orientations. To this end, we propose a method named self-contour-based transformation (SCT), which can be flexibly integrated into various existing point cloud recognition models against arbitrary rotations. SCT provides efficient rotation and translation invariance by introducing contour-aware transformation (CAT), which linearly transforms the Cartesian coordinates of points to translation and rotation-invariant representations. We prove that CAT is a rotation- and translation-invariant transformation based on the theoretical analysis. Furthermore, the frame alignment module is proposed to enhance the discriminative feature extraction by capturing contours and transforming self-contour-based frames into intraclass frames. Extensive experimental results show that SCT outperforms the state-of-the-art approaches under arbitrary rotations in effectiveness and efficiency on synthetic and real-world benchmarks. Furthermore, the robustness and generality evaluations indicate that SCT is robust and is applicable to various point cloud processing models, which highlights the superiority of SCT in industrial applications."}}
{"id": "4LGGvUZorN", "cdate": 1640995200000, "mdate": 1683889105878, "content": {"title": "Point Clouds Downsampling Based on Complementary Attention and Contrastive Learning", "abstract": "This paper presents a novel method for point clouds down-sampling. We formulate the sampling task as an optimal permutation problem and develop two techniques, the com-plementary attention module and contrastive learning mech-anism, to enable it. The complementary attention module assigns large weights to point-wise features to emphasize fea-tures related to selected and discarded points. We optimize the network by introducing the contrastive learning mecha-nism, which minimizes feature discrepancy of the discarded points while maximizing feature separation between the se-lected points. It is evaluated on ModelNet40 and ShapeNet-Core datasets for classification and reconstruction tasks and achieves promising results."}}
{"id": "QMG2bzvk5HV", "cdate": 1621629774706, "mdate": null, "content": {"title": "Interpreting Representation Quality of DNNs for 3D Point Cloud Processing", "abstract": "In this paper, we evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. We propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, we also propose metrics to evaluate the spatial smoothness of encoding 3D structures, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training. The code will be released when this paper is accepted."}}
