{"id": "R4nMRio6XL", "cdate": 1690872610870, "mdate": 1690872610870, "content": {"title": "Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning", "abstract": "Pretrained Language Models (PLMs) have improved the performance of natural language understanding in recent years. Such models are pretrained on large corpora, which encode the general prior knowledge of natural languages but are agnostic to information characteristic of downstream tasks. This often results in overfitting when fine-tuned with low resource datasets where task-specific information is limited. In this paper, we integrate label information as a task-specific prior into the self-attention component of pretrained BERT models. Experiments on several benchmarks and real-word datasets suggest that the proposed approach can largely improve the performance of pretrained models when fine-tuning with small datasets."}}
{"id": "9no8H5jM9S", "cdate": 1690872454280, "mdate": 1690872454280, "content": {"title": "Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets", "abstract": "Federated learning involves collaborative training with private data from multiple platforms, while not violating data privacy. We study the problem of federated domain adaptation for Named Entity Recognition (NER), where we seek to transfer knowledge across different platforms with data of multiple domains. In addition, we consider a practical and challenging scenario, where NER datasets of different platforms of federated learning are annotated with heterogeneous tag sets, ie, different sets of entity types. The goal is to train a global model with federated learning, such that it can predict with a complete tag set, ie, with all the occurring entity types for data across all platforms. To cope with the heterogeneous tag sets in a multi-domain setting, we propose a distillation approach along with a mechanism of instance weighting to facilitate knowledge transfer across platforms. Besides, we release two re-annotated clinic NER datasets, for testing the proposed method in the clinic domain. Our method shows superior empirical performance for NER with federated learning."}}
{"id": "ByQo5YAK8M", "cdate": 1690872317704, "mdate": 1690872317704, "content": {"title": "Few-Shot Composition Learning for Image Retrieval with Prompt Tuning", "abstract": "We study the problem of composition learning for image retrieval, for which we learn to retrieve target images with search queries in the form of a composition of a reference image and a modification text that describes desired modifications of the image. Existing models of composition learning for image retrieval are generally built with large-scale datasets, demanding extensive training samples, ie, query-target pairs, as supervision, which restricts their application for the scenario of few-shot learning with only few query-target pairs available. Recently, prompt tuning with frozen pretrained language models has shown remarkable performance when the amount of training data is limited. Inspired by this, we propose a prompt tuning mechanism with the pretrained CLIP model for the task of few-shot composition learning for image retrieval. Specifically, we regard the representation of the reference image as a trainable visual prompt, prefixed to the embedding of the text sequence. One challenge is to efficiently train visual prompt with few-shot samples. To deal with this issue, we further propose a self-upervised auxiliary task via ensuring that the reference image can retrieve itself when no modification information is given from the text, which facilitates training for the visual prompt, while not requiring additional annotations for query-target pairs. Experiments on multiple benchmarks show that our proposed model can yield superior performance when trained with only few query-target pairs."}}
{"id": "U-irVmiKy3", "cdate": 1684350014204, "mdate": 1684350014204, "content": {"title": "Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling", "abstract": "Supervised training of existing deep learning models for sequence labeling relies on large scale labeled datasets. Such datasets are generally created with crowd-source labeling. However, crowd-source labeling for tasks of sequence labeling can be expensive and time-consuming. Further, crowd-source labeling by external annotators may not be appropriate for data that contains user private information. Considering the above limitations of crowd-source labeling, we study interactive sequence labeling that allows training directly with the user feedback, which alleviates the annotation cost and maintains the user privacy. We identify two bias, namely, context bias and feedback bias, by formulating interactive sequence labeling via a Structural Causal Model (SCM). To alleviate the context and feedback bias based on the SCM, we identify the frequent context tokens as confounders in the backdoor adjustment and further propose an entropy-based modulation that is inspired by information theory. entities more sample-efficiently. With extensive experiments, we validate that our approach can effectively alleviate the biases and our models can be efficiently learnt with the user feedback."}}
{"id": "oijssSX3sfL", "cdate": 1684349755760, "mdate": 1684349755760, "content": {"title": "Discriminative clustering for robust unsupervised domain adaptation", "abstract": "Unsupervised domain adaptation seeks to learn an invariant and discriminative representation for an unlabeled target domain by leveraging the information of a labeled source dataset. We propose to improve the discriminative ability of the target domain representation by simultaneously learning tightly clustered target representations while encouraging that each cluster is assigned to a unique and different class from the source. This strategy alleviates the effects of negative transfer when combined with adversarial domain matching between source and target representations. Our approach is robust to differences in the source and target label distributions and thus applicable to both balanced and imbalanced domain adaptation tasks, and with a simple extension, it can also be used for partial domain adaptation. Experiments on several benchmark datasets for domain adaptation demonstrate that our approach can achieve state-of-the-art performance in all three scenarios, namely, balanced, imbalanced and partial domain adaptation."}}
{"id": "aRxODETLWF", "cdate": 1684349487045, "mdate": 1684349487045, "content": {"title": "Unsupervised Paraphrasing Consistency Training for Low Resource Named Entity Recognition", "abstract": "Unsupervised consistency training is a way of semi-supervised learning that encourages consistency in model predictions between the original and augmented data. For Named Entity Recognition (NER), existing approaches augment the input sequence with token replacement, assuming annotations on the replaced positions unchanged. In this paper, we explore the use of paraphrasing as a more principled data augmentation scheme for NER unsupervised consistency training. Specifically, we convert Conditional Random Field (CRF) into a multi-label classification module and encourage consistency on the entity appearance between the original and paraphrased sequences. Experiments show that our method is especially effective when annotations are limited."}}
{"id": "cxuCPhPdozk", "cdate": 1683906998665, "mdate": 1683906998665, "content": {"title": "Toward Fairness in Text Generation via Mutual Information Minimizationbased on Importance Sampling", "abstract": "Pretrained language models (PLMs), such as GPT-2,  have  achieved  remarkable  empirical  perfor-mance in text generation tasks.   However,  pre-trained on large-scale natural language corpora,the generated text from PLMs may exhibitsocialbiasagainst disadvantaged demographic groups.To improve the fairness of PLMs in text genera-tion, we propose to minimize the mutual informa-tion between the semantics in the generated textsentences  and  theirdemographic  polarity,i.e.,the demographic group to which the sentence isreferring. In this way, the mentioning of a demo-graphic group (e.g.,male or female) is encouragedto be independent from how it is described in thegenerated text, thus effectively alleviating the so-cial bias.   Moreover,  we propose to efficientlyestimate the upper bound of the above mutual in-formation via importance sampling, leveraging anatural language corpus. We also propose a dis-tillation mechanism that preserves the languagemodeling ability of the PLMs after debiasing. Em-pirical results on real-world benchmarks demon-strate that the proposed method yields superiorperformance in term of both fairness and languagemodeling ability."}}
{"id": "6venG64mwf", "cdate": 1683906851820, "mdate": 1683906851820, "content": {"title": "Few-Shot Class-Incremental Learning for Named Entity Recognition", "abstract": "Previous  work  of  class-incremental  learning for  Named  Entity  Recognition  (NER)  relies on the assumption that there exists abundance of labeled data for the training of new classes.In  this  work,  we  study  a  more  challenging but  practical  problem,i.e.,  few-shot  class-incremental learning for NER, where an NER model is trained with only few labeled samples of the new classes, without forgetting knowledge of the old ones.   To alleviate the problem of catastrophic forgetting in few-shot class-incremental  learning,  we  generate  synthetic data of the old classes using the trained NER model, augmenting the training of new classes.We further develop a framework that distills from the NER model from previous steps with both synthetic data, and real data from the cur-rent training set.   Experimental results show that our approach achieves significant improvements over existing baselines."}}
{"id": "lTxJYzoOpWs", "cdate": 1680307200000, "mdate": 1682375579369, "content": {"title": "Calibration and Uncertainty in Neural Time-to-Event Modeling", "abstract": "Models for predicting the time of a future event are crucial for risk assessment, across a diverse range of applications. Existing time-to-event (survival) models have focused primarily on preserving pairwise ordering of estimated event times (i.e., relative risk). We propose neural time-to-event models that account for calibration and uncertainty while predicting accurate absolute event times. Specifically, an adversarial nonparametric model is introduced for estimating matched time-to-event distributions for probabilistically concentrated and accurate predictions. We also consider replacing the discriminator of the adversarial nonparametric model with a survival-function matching estimator that accounts for model calibration. The proposed estimator can be used as a means of estimating and comparing conditional survival distributions while accounting for the predictive uncertainty of probabilistic models. Extensive experiments show that the distribution matching methods outperform existing approaches in terms of both calibration and concentration of time-to-event distributions."}}
{"id": "NBMD0FTNn2", "cdate": 1672531200000, "mdate": 1682844657155, "content": {"title": "Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling", "abstract": "Pretrained language models (PLMs), such as GPT2, have achieved remarkable empirical performance in text generation tasks. However, pretrained on large-scale natural language corpora, the generated text from PLMs may exhibit social bias against disadvantaged demographic groups. To improve the fairness of PLMs in text generation, we propose to minimize the mutual information between the semantics in the generated text sentences and their demographic polarity, i.e., the demographic group to which the sentence is referring. In this way, the mentioning of a demographic group (e.g., male or female) is encouraged to be independent from how it is described in the generated text, thus effectively alleviating the social bias. Moreover, we propose to efficiently estimate the upper bound of the above mutual information via importance sampling, leveraging a natural language corpus. We also propose a distillation mechanism that preserves the language modeling ability of the PLMs after debiasing. Empirical results on real-world benchmarks demonstrate that the proposed method yields superior performance in term of both fairness and language modeling ability."}}
