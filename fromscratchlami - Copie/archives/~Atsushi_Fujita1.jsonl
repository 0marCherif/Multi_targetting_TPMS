{"id": "zNxfpVWlmj8", "cdate": 1609459200000, "mdate": 1639631473201, "content": {"title": "Understanding Pre-Editing for Black-Box Neural Machine Translation", "abstract": "Rei Miyata, Atsushi Fujita. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."}}
{"id": "z9iHwbyLf1", "cdate": 1609459200000, "mdate": 1639631473398, "content": {"title": "Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers", "abstract": "This paper presents the first large-scale meta-evaluation of machine translation (MT). We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020. Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends. An increasing number of MT evaluations exclusively rely on differences between BLEU scores to draw conclusions, without performing any kind of statistical significance testing nor human evaluation, while at least 108 metrics claiming to be better than BLEU have been proposed. MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an algorithm without confirming neither exactly the same training, validating, and testing data have been used nor the metric scores are comparable. Furthermore, tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility."}}
{"id": "x-VIe0iglUA", "cdate": 1609459200000, "mdate": 1639631464754, "content": {"title": "Understanding Pre-Editing for Black-Box Neural Machine Translation", "abstract": "Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), pre-editing has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of pre-editing methods for particular settings, thus far, a deep understanding of what pre-editing is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a protocol to incrementally record the minimum edits for each ST and collected 6,652 instances of pre-editing across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives: the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following: (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types."}}
{"id": "guKJvdtI3o5", "cdate": 1609459200000, "mdate": 1639631464754, "content": {"title": "Synthesizing Monolingual Data for Neural Machine Translation", "abstract": "In neural machine translation (NMT), monolingual data in the target language are usually exploited through a method so-called \"back-translation\" to synthesize additional training parallel data. The synthetic data have been shown helpful to train better NMT, especially for low-resource language pairs and domains. Nonetheless, large monolingual data in the target domains or languages are not always available to generate large synthetic parallel data. In this work, we propose a new method to generate large synthetic parallel data leveraging very small monolingual data in a specific domain. We fine-tune a pre-trained GPT-2 model on such small in-domain monolingual data and use the resulting model to generate a large amount of synthetic in-domain monolingual data. Then, we perform back-translation, or forward translation, to generate synthetic in-domain parallel data. Our preliminary experiments on three language pairs and five domains show the effectiveness of our method to generate fully synthetic but useful in-domain parallel data for improving NMT in all configurations. We also show promising results in extreme adaptation for personalized NMT."}}
{"id": "_DQMxhkwKFx", "cdate": 1609459200000, "mdate": 1639631473399, "content": {"title": "Investigating Softmax Tempering for Training Neural Machine Translation Models", "abstract": "Raj Dabre, Atsushi Fujita. Proceedings of Machine Translation Summit XVIII: Research Track. 2021."}}
{"id": "UxD1s6s9edJ", "cdate": 1609459200000, "mdate": 1639631473199, "content": {"title": "Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation", "abstract": "In deep neural network modeling, the most common practice is to stack a number of recurrent, convolutional, or feed-forward layers in order to obtain high-quality continuous space representations which in turn improves the quality of the network's prediction. Conventionally, each layer in the stack has its own parameters which leads to a significant increase in the number of model parameters. In this paper, we propose to share parameters across all layers thereby leading to a recurrently stacked neural network model. We report on an extensive case study on neural machine translation (NMT), where we apply our proposed method to an encoder-decoder based neural network model, i.e., the Transformer model, and experiment with three Japanese--English translation datasets. We empirically demonstrate that the translation quality of a model that recurrently stacks a single layer 6 times, despite having significantly fewer parameters, approaches that of a model that stacks 6 layers where each layer has different parameters. We also explore the limits of recurrent stacking where we train extremely deep NMT models. This paper also examines the utility of our recurrently stacked model as a student model through transfer learning via leveraging pre-trained parameters and knowledge distillation, and shows that it compensates for the performance drops in translation quality that the direct training of recurrently stacked model brings. We also show how transfer learning helps in faster decoding on top of the already reduced number of parameters due to recurrent stacking. Finally, we analyze the effects of recurrently stacked layers by visualizing the attentions of models that use recurrently stacked layers and models that do not."}}
{"id": "PBnxrdXUPSn", "cdate": 1609459200000, "mdate": null, "content": {"title": "Understanding Pre-Editing for Black-Box Neural Machine Translation", "abstract": "Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), pre-editing has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of pre-editing methods for particular settings, thus far, a deep understanding of what pre-editing is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a protocol to incrementally record the minimum edits for each ST and collected 6,652 instances of pre-editing across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives: the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following: (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types."}}
{"id": "HlSz6hM9z1h", "cdate": 1609459200000, "mdate": 1639631467688, "content": {"title": "Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers", "abstract": "Benjamin Marie, Atsushi Fujita, Raphael Rubino. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "Hbv0FegOv5", "cdate": 1609459200000, "mdate": 1639631464754, "content": {"title": "Attainable Text-to-Text Machine Translation vs. Translation: Issues Beyond Linguistic Processing", "abstract": "Atsushi Fujita. Proceedings of Machine Translation Summit XVIII: Research Track. 2021."}}
{"id": "16jxscj6IqI", "cdate": 1609459200000, "mdate": null, "content": {"title": "Synthesizing Monolingual Data for Neural Machine Translation", "abstract": "In neural machine translation (NMT), monolingual data in the target language are usually exploited through a method so-called \"back-translation\" to synthesize additional training parallel data. The synthetic data have been shown helpful to train better NMT, especially for low-resource language pairs and domains. Nonetheless, large monolingual data in the target domains or languages are not always available to generate large synthetic parallel data. In this work, we propose a new method to generate large synthetic parallel data leveraging very small monolingual data in a specific domain. We fine-tune a pre-trained GPT-2 model on such small in-domain monolingual data and use the resulting model to generate a large amount of synthetic in-domain monolingual data. Then, we perform back-translation, or forward translation, to generate synthetic in-domain parallel data. Our preliminary experiments on three language pairs and five domains show the effectiveness of our method to generate fully synthetic but useful in-domain parallel data for improving NMT in all configurations. We also show promising results in extreme adaptation for personalized NMT."}}
