{"id": "WwZbupAKWo", "cdate": 1621630065121, "mdate": null, "content": {"title": "Fast and Memory Efficient Differentially Private-SGD via JL Projections", "abstract": "Differentially Private-SGD (DP-SGD) of Abadi et al. and its variations are the only known algorithms for private training of large scale neural networks. This algorithm requires computation of per-sample gradients norms which is extremely slow and memory intensive in practice. In this paper, we present a new framework to design differentially private optimizers called DP-SGD-JL and DP-Adam-JL. Our approach uses Johnson\u2013Lindenstrauss (JL) projections to quickly approximate the per-sample gradient norms without exactly computing them, thus making the training time and memory requirements of our optimizers closer to that of their non-DP versions. Unlike previous attempts to make DP-SGD faster which work only on a subset of network architectures or use compiler techniques, we propose an algorithmic solution which works for any network in a black-box manner which is the main contribution of this paper. To illustrate this, on IMDb dataset, we train a Recurrent Neural Network (RNN) to achieve good privacy-vs-accuracy tradeoff, while being significantly faster than DP-SGD and with a similar memory footprint as non-private SGD. "}}
{"id": "0Jr4rjA6glk", "cdate": 1601308225523, "mdate": null, "content": {"title": "FAST DIFFERENTIALLY PRIVATE-SGD VIA JL PROJECTIONS", "abstract": "Differentially Private-SGD (DP-SGD) of Abadi et al. (2016) and its variations are the only known\nalgorithms for private training of large scale neural networks. This algorithm requires computation\nof per-sample gradients norms which is extremely slow and memory intensive in practice. In this\npaper, we present a new framework to design differentially private optimizers called DP-SGD-JL and\nDP-Adam-JL. Our approach uses Johnson\u2013Lindenstrauss (JL) projections to quickly approximate\nthe per-sample gradient norms without exactly computing them, thus making the training time and\nmemory requirements of our optimizers closer to that of their non-DP versions.\nOur algorithms achieve state-of-the-art privacy-vs-accuracy tradeoffs on MNIST and CIFAR10\ndatasets while being significantly faster. Unlike previous attempts to make DP-SGD faster which\nwork only on fully-connected or convolutional layers, our algorithms work for any network in a\nblack-box manner which is the main contribution of this paper. To illustrate this, on IMDb\ndataset, we train a Recurrent Neural Network (RNN) to achieve good privacy-vs-accuracy tradeoff,\nwhereas existing DP optimizers are either inefficient or inapplicable. On RNNs, our algorithms are\norders of magnitude faster than DP-SGD for large batch sizes.\nThe privacy analysis of our algorithms is more involved than DP-SGD, we use the recently proposed\nf-DP framework of Dong et al. (2019). In summary, we design new differentially private training\nalgorithms which are fast, achieve state-of-the-art privacy-vs-accuracy tradeoffs and generalize to all\nnetwork architectures."}}
{"id": "HygFxxrFvB", "cdate": 1569439728795, "mdate": null, "content": {"title": "Differentially Private Mixed-Type Data Generation For Unsupervised Learning", "abstract": "In this work we introduce the DP-auto-GAN framework for synthetic data generation, which combines the low dimensional representation of autoencoders with the flexibility of GANs.  This framework can be used to take in raw sensitive data, and privately train a model for generating synthetic data that should satisfy the same statistical properties as the original data.  This learned model can be used to generate arbitrary amounts of publicly available synthetic data, which can then be freely shared due to the post-processing guarantees of differential privacy.  Our framework is applicable to unlabled \\emph{mixed-type data}, that may include binary, categorical, and real-valued data.  We implement this framework on both unlabeled binary data (MIMIC-III) and unlabeled mixed-type data (ADULT).  We also introduce new metrics for evaluating the quality of synthetic mixed-type data, particularly in unsupervised settings."}}
{"id": "rkE7hIWO-B", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Price of Fair PCA: One Extra dimension", "abstract": "We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations. We show on several real-world data sets, PCA has higher reconstruction error on population A than on B (for example, women versus men or lower- versus higher-educated individuals). This can happen even when the data set has a similar number of samples from A and B. This motivates our study of dimensionality reduction techniques which maintain similar fidelity for A and B. We define the notion of Fair PCA and give a polynomial-time algorithm for finding a low dimensional representation of the data which is nearly-optimal with respect to this measure. Finally, we show on real-world data sets that our algorithm can be used to efficiently generate a fair low dimensional representation of the data."}}
{"id": "HyNY7DWdZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Differential Privacy for Growing Databases", "abstract": "The large majority of differentially private algorithms focus on the static setting, where queries are made on an unchanging database. This is unsuitable for the myriad applications involving databases that grow over time. To address this gap in the literature, we consider the dynamic setting, in which new data arrive over time. Previous results in this setting have been limited to answering a single non-adaptive query repeatedly as the database grows. In contrast, we provide tools for richer and more adaptive analysis of growing databases. Our first contribution is a novel modification of the private multiplicative weights algorithm, which provides accurate analysis of exponentially many adaptive linear queries (an expressive query class including all counting queries) for a static database. Our modification maintains the accuracy guarantee of the static setting even as the database grows without bound. Our second contribution is a set of general results which show that many other private and accurate algorithms can be immediately extended to the dynamic setting by rerunning them at appropriate points of data growth with minimal loss of accuracy, even when data growth is unbounded."}}
