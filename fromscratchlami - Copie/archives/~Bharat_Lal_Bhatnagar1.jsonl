{"id": "-RiPLDxP4L", "cdate": 1689666699924, "mdate": 1689666699924, "content": {"title": "Visibility Aware Human-Object Interaction Tracking from Single RGB Camera", "abstract": "Capturing the interactions between humans and their\nenvironment in 3D is important for many applications in\nrobotics, graphics, and vision. Recent works to reconstruct\nthe 3D human and object from a single RGB image do not\nhave consistent relative translation across frames because\nthey assume a fixed depth. Moreover, their performance\ndrops significantly when the object is occluded. In this\nwork, we propose a novel method to track the 3D human,\nobject, contacts, and relative translation across frames from\na single RGB camera, while being robust to heavy occlusions. Our method is built on two key insights. First, we\ncondition our neural field reconstructions for human and\nobject on per-frame SMPL model estimates obtained by\npre-fitting SMPL to a video sequence. This improves neural reconstruction accuracy and produces coherent relative\ntranslation across frames. Second, human and object motion from visible frames provides valuable information to\ninfer the occluded object. We propose a novel transformerbased neural network that explicitly uses object visibility and human motion to leverage neighboring frames to\nmake predictions for the occluded frames. Building on\nthese insights, our method is able to track both human\nand object robustly even under occlusions. Experiments\non two datasets show that our method significantly improves over the state-of-the-art methods. Our code and pretrained models are available at: https://virtualhumans.mpiinf.mpg.de/VisTracker."}}
{"id": "DoqBGUjD3Mq", "cdate": 1667628972390, "mdate": 1667628972390, "content": {"title": "SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing", "abstract": "While models of 3D clothing learned from real data exist, no method can predict clothing deformation as a function of garment size. In this paper, we introduce SizerNet to predict 3D clothing conditioned on human body shape and garment size parameters, and ParserNet to infer garment meshes and shape under clothing with personal details in a single pass from an input mesh. SizerNet allows to estimate and visualize the dressing effect of a garment in various sizes, and ParserNet allows to edit clothing of an input mesh directly, removing the need for scan segmentation, which is a challenging problem in itself. To learn these models, we introduce the SIZER dataset of clothing size variation which includes 100 different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. Our experiments show better parsing accuracy and size prediction than baseline methods trained on SIZER. The code, model and dataset will be released for research purposes."}}
{"id": "u23Sp4VC6x", "cdate": 1667384980827, "mdate": 1667384980827, "content": {"title": "COUCH: Towards Controllable Human-Chair Interactions", "abstract": "   Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework \\model~that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the \\model~Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts."}}
{"id": "umCnqpdkbm", "cdate": 1640995200000, "mdate": 1667455768947, "content": {"title": "BEHAVE: Dataset and Method for Tracking Human Object Interactions", "abstract": "Modelling interactions between humans and objects in natural environments is central to many applications including gaming, virtual and mixed reality, as well as human behavior analysis and human-robot collaboration. This challenging operation scenario requires generalization to vast number of objects, scenes, and human actions. Unfortunately, there exist no such dataset. Moreover, this data needs to be acquired in diverse natural environments, which rules out 4D scanners and marker based capture systems. We present BEHAVE dataset, the first full body human-object interaction dataset with multi-view RGBD frames and corresponding 3D SMPL and object fits along with the annotated contacts between them. We record ~15k frames at 5 locations with 8 subjects performing a wide range of interactions with 20 common objects. We use this data to learn a model that can jointly track humans and objects in natural environments with an easy-to-use portable multi-camera setup. Our key insight is to predict correspondences from the human and the object to a statistical body model to obtain human-object contacts during interactions. Our approach can record and track not just the humans and objects but also their interactions, modeled as surface contacts, in 3D. Our code and data can be found at: http://virtualhumans.mpi-inf.mpg.de/behave."}}
{"id": "sHe6HIx8U7O", "cdate": 1640995200000, "mdate": 1667455769019, "content": {"title": "TOCH: Spatio-Temporal Object Correspondence to Hand for Motion Refinement", "abstract": "We present TOCH, a method for refining incorrect 3D hand-object interaction sequences using a data prior. Existing hand trackers, especially those that rely on very few cameras, often produce visually unrealistic results with hand-object intersection or missing contacts. Although correcting such errors requires reasoning about temporal aspects of interaction, most previous works focus on static grasps and contacts. The core of our method are TOCH fields, a novel spatio-temporal representation for modeling correspondences between hands and objects during interaction. TOCH fields are a point-wise, object-centric representation, which encode the hand position relative to the object. Leveraging this novel representation, we learn a latent manifold of plausible TOCH fields with a temporal denoising auto-encoder. Experiments demonstrate that TOCH outperforms state-of-the-art 3D hand-object interaction models, which are limited to static grasps and contacts. More importantly, our method produces smooth interactions even before and after contact. Using a single trained TOCH model, we quantitatively and qualitatively demonstrate its usefulness for correcting erroneous sequences from off-the-shelf RGB/RGB-D hand-object reconstruction methods and transferring grasps across objects."}}
{"id": "YWU_VT-9l9", "cdate": 1640995200000, "mdate": 1667455769068, "content": {"title": "BEHAVE: Dataset and Method for Tracking Human Object Interactions", "abstract": "Modelling interactions between humans and objects in natural environments is central to many applications including gaming, virtual and mixed reality, as well as human behavior analysis and human-robot collaboration. This challenging operation scenario requires generalization to vast number of objects, scenes, and human actions. Unfortunately, there exist no such dataset. Moreover, this data needs to be acquired in diverse natural environments, which rules out 4D scanners and marker based capture systems. We present BEHAVE dataset, the first full body human- object interaction dataset with multi-view RGBD frames and corresponding 3D SMPL and object fits along with the annotated contacts between them. We record around 15k frames at 5 locations with 8 subjects performing a wide range of interactions with 20 common objects. We use this data to learn a model that can jointly track humans and objects in natural environments with an easy-to-use portable multi-camera setup. Our key insight is to predict correspondences from the human and the object to a statistical body model to obtain human-object contacts during interactions. Our approach can record and track not just the humans and objects but also their interactions, modeled as surface contacts, in 3D. Our code and data can be found at: http://virtualhumans.mpi-inf.mpg.de/behave"}}
{"id": "XlnAs0XjpG5", "cdate": 1640995200000, "mdate": 1667455769071, "content": {"title": "CHORE: Contact, Human and Object REconstruction from a single RGB image", "abstract": "Most prior works in perceiving 3D humans from images reason human in isolation without their surroundings. However, humans are constantly interacting with the surrounding objects, thus calling for models that can reason about not only the human but also the object and their interaction. The problem is extremely challenging due to heavy occlusions between humans and objects, diverse interaction types and depth ambiguity. In this paper, we introduce CHORE, a novel method that learns to jointly reconstruct the human and the object from a single RGB image. CHORE takes inspiration from recent advances in implicit surface learning and classical model-based fitting. We compute a neural reconstruction of human and object represented implicitly with two unsigned distance fields, a correspondence field to a parametric body and an object pose field. This allows us to robustly fit a parametric body model and a 3D object template, while reasoning about interactions. Furthermore, prior pixel-aligned implicit learning methods use synthetic data and make assumptions that are not met in the real data. We propose a elegant depth-aware scaling that allows more efficient shape learning on real data. Experiments show that our joint reconstruction learned with the proposed strategy significantly outperforms the SOTA. Our code and models are available at https://virtualhumans.mpi-inf.mpg.de/chore"}}
{"id": "Gc_xtqmr0D", "cdate": 1640995200000, "mdate": 1667455769017, "content": {"title": "COUCH: Towards Controllable Human-Chair Interactions", "abstract": "Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts."}}
{"id": "fNaK5kHGNjv", "cdate": 1617645322871, "mdate": null, "content": {"title": "Multi-Garment Net: Learning to Dress 3D People from Images", "abstract": "We present Multi-Garment Network (MGN), a method\nto predict body shape and clothing, layered on top of the\nSMPL [40] model from a few frames (1-8) of a video. Several experiments demonstrate that this representation allows higher level of control when compared to single mesh\nor voxel representations of shape. Our model allows to\npredict garment geometry, relate it to the body shape, and\ntransfer it to new body shapes and poses. To train MGN,\nwe leverage a digital wardrobe containing 712 digital garments in correspondence, obtained with a novel method\nto register a set of clothing templates to a dataset of real\n3D scans of people in different clothing and poses. Garments from the digital wardrobe, or predicted by MGN, can\nbe used to dress any body shape in arbitrary poses. We\nwill make publicly available the digital wardrobe, the MGN\nmodel, and code to dress SMPL with the garments."}}
{"id": "0duDaQOdHhh", "cdate": 1617645226871, "mdate": null, "content": {"title": "LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration", "abstract": "We address the problem of fitting 3D human models to 3D scans of dressed humans.\nClassical methods optimize both the data-to-model correspondences and the human\nmodel parameters (pose and shape), but are reliable only when initialized close to\nthe solution. Some methods initialize the optimization based on fully supervised\ncorrespondence predictors, which is not differentiable end-to-end, and can only\nprocess a single scan at a time. Our main contribution is LoopReg, an end-to-end\nlearning framework to register a corpus of scans to a common 3D human model.\nThe key idea is to create a self-supervised loop. A backward map, parameterized by\na Neural Network, predicts the correspondence from every scan point to the surface\nof the human model. A forward map, parameterized by a human model, transforms\nthe corresponding points back to the scan based on the model parameters (pose and\nshape), thus closing the loop. Formulating this closed loop is not straightforward\nbecause it is not trivial to force the output of the NN to be on the surface of the\nhuman model \u2013 outside this surface the human model is not even defined. To\nthis end, we propose two key innovations. First, we define the canonical surface\nimplicitly as the zero level set of a distance field in R3, which in contrast to more\ncommon UV parameterizations \u2126 \u2282 R2, does not require cutting the surface, does\nnot have discontinuities, and does not induce distortion. Second, we diffuse the\nhuman model to the 3D domain R3. This allows to map the NN predictions forward,\neven when they slightly deviate from the zero level set. Results demonstrate that we\ncan train LoopReg mainly self-supervised \u2013 following a supervised warm-start, the\nmodel becomes increasingly more accurate as additional unlabelled raw scans are\nprocessed. Our code and pre-trained models can be downloaded for research."}}
