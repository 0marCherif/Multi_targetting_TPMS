{"id": "n1dC5l-bqrd", "cdate": 1672531200000, "mdate": 1684331148312, "content": {"title": "The Bit Complexity of Efficient Continuous Optimization", "abstract": "We analyze the bit complexity of efficient algorithms for fundamental optimization problems, such as linear regression, $p$-norm regression, and linear programming (LP). State-of-the-art algorithms are iterative, and in terms of the number of arithmetic operations, they match the current time complexity of multiplying two $n$-by-$n$ matrices (up to polylogarithmic factors). However, previous work has typically assumed infinite precision arithmetic, and due to complicated inverse maintenance techniques, the actual running times of these algorithms are unknown. To settle the running time and bit complexity of these algorithms, we demonstrate that a core common subroutine, known as \\emph{inverse maintenance}, is backward-stable. Additionally, we show that iterative approaches for solving constrained weighted regression problems can be accomplished with bounded-error pre-conditioners. Specifically, we prove that linear programs can be solved approximately in matrix multiplication time multiplied by polylog factors that depend on the condition number $\\kappa$ of the matrix and the inner and outer radius of the LP problem. $p$-norm regression can be solved approximately in matrix multiplication time multiplied by polylog factors in $\\kappa$. Lastly, linear regression can be solved approximately in input-sparsity time multiplied by polylog factors in $\\kappa$. Furthermore, we present results for achieving lower than matrix multiplication time for $p$-norm regression by utilizing faster solvers for sparse linear systems."}}
{"id": "k_ACuCOuKv", "cdate": 1672531200000, "mdate": 1681657223189, "content": {"title": "Approximately Optimal Core Shapes for Tensor Decompositions", "abstract": "This work studies the combinatorial optimization problem of finding an optimal core tensor shape, also called multilinear rank, for a size-constrained Tucker decomposition. We give an algorithm with provable approximation guarantees for its reconstruction error via connections to higher-order singular values. Specifically, we introduce a novel Tucker packing problem, which we prove is NP-hard, and give a polynomial-time approximation scheme based on a reduction to the 2-dimensional knapsack problem with a matroid constraint. We also generalize our techniques to tree tensor network decompositions. We implement our algorithm using an integer programming solver, and show that its solution quality is competitive with (and sometimes better than) the greedy algorithm that uses the true Tucker decomposition loss at each step, while also running up to 1000x faster."}}
{"id": "nMAbvsQo5YY", "cdate": 1663850401190, "mdate": null, "content": {"title": "Constant-Factor Approximation Algorithms for Socially Fair $k$-Clustering", "abstract": "We study approximation algorithms for the socially fair $(\\ell_p, k)$-clustering problem with $m$ groups which include the socially fair $k$-median ($p=1$) and $k$-means ($p=2$). We present (1) a polynomial-time $(5+2\\sqrt{6})^p$-approximation with at most $k+m$ centers (2) a $(5+2\\sqrt{6}+\\epsilon)^p$-approximation with $k$ centers in time $(nk)^{{2^{O(p)} m^2}/\\epsilon}$, and (3) a $(15+6\\sqrt{6})^p$ approximation with $k$ centers in time $k^{m}\\cdot\\text{poly}(n)$. The former is obtained by a refinement of the iterative rounding method via a sequence of linear programs. The latter two are obtained by converting a solution with up to $k+m$ centers to one with $k$ centers by sparsification methods for (2) and via an exhaustive search for (3). We also compare the performance of our algorithms with existing approximation algorithms on benchmark datasets, and find that our algorithms outperform existing methods."}}
{"id": "lbQTJN42uea", "cdate": 1652737725194, "mdate": null, "content": {"title": "Subquadratic Kronecker Regression with Applications to Tensor Decomposition", "abstract": "Kronecker regression is a highly-structured least squares problem $\\min_{\\mathbf{x}} \\lVert \\mathbf{K}\\mathbf{x} - \\mathbf{b} \\rVert_{2}^2$, where the design matrix $\\mathbf{K} = \\mathbf{A}^{(1)} \\otimes \\cdots \\otimes \\mathbf{A}^{(N)}$ is a Kronecker product of factor matrices. This regression problem arises in each step of the widely-used alternating least squares (ALS) algorithm for computing the Tucker decomposition of a tensor. We present the first subquadratic-time algorithm for solving Kronecker regression to a $(1+\\varepsilon)$-approximation that avoids the exponential term $O(\\varepsilon^{-N})$ in the running time. Our techniques combine leverage score sampling and iterative methods. By extending our approach to block-design matrices where one block is a Kronecker product, we also achieve subquadratic-time algorithms for (1) Kronecker ridge regression and (2) updating the factor matrix of a Tucker decomposition in ALS, which is not a pure Kronecker regression problem, thereby improving the running time of all steps of Tucker ALS. We demonstrate the speed and accuracy of this Kronecker regression algorithm on synthetic data and real-world image tensors."}}
{"id": "ruJZcFexbF4", "cdate": 1640995200000, "mdate": 1684331148604, "content": {"title": "Constant-Factor Approximation Algorithms for Socially Fair k-Clustering", "abstract": "We study approximation algorithms for the socially fair $(\\ell_p, k)$-clustering problem with $m$ groups, whose special cases include the socially fair $k$-median ($p=1$) and socially fair $k$-means ($p=2$) problems. We present (1) a polynomial-time $(5+2\\sqrt{6})^p$-approximation with at most $k+m$ centers (2) a $(5+2\\sqrt{6}+\\epsilon)^p$-approximation with $k$ centers in time $n^{2^{O(p)}\\cdot m^2}$, and (3) a $(15+6\\sqrt{6})^p$ approximation with $k$ centers in time $k^{m}\\cdot\\text{poly}(n)$. The first result is obtained via a refinement of the iterative rounding method using a sequence of linear programs. The latter two results are obtained by converting a solution with up to $k+m$ centers to one with $k$ centers using sparsification methods for (2) and via an exhaustive search for (3). We also compare the performance of our algorithms with existing bicriteria algorithms as well as exactly $k$ center approximation algorithms on benchmark datasets, and find that our algorithms also outperform existing methods in practice."}}
{"id": "qF6l0wrJwf", "cdate": 1640995200000, "mdate": 1684206764517, "content": {"title": "Subquadratic Kronecker Regression with Applications to Tensor Decomposition", "abstract": "Kronecker regression is a highly-structured least squares problem $\\min_{\\mathbf{x}} \\lVert \\mathbf{K}\\mathbf{x} - \\mathbf{b} \\rVert_{2}^2$, where the design matrix $\\mathbf{K} = \\mathbf{A}^{(1)} \\otimes \\cdots \\otimes \\mathbf{A}^{(N)}$ is a Kronecker product of factor matrices. This regression problem arises in each step of the widely-used alternating least squares (ALS) algorithm for computing the Tucker decomposition of a tensor. We present the first subquadratic-time algorithm for solving Kronecker regression to a $(1+\\varepsilon)$-approximation that avoids the exponential term $O(\\varepsilon^{-N})$ in the running time. Our techniques combine leverage score sampling and iterative methods. By extending our approach to block-design matrices where one block is a Kronecker product, we also achieve subquadratic-time algorithms for (1) Kronecker ridge regression and (2) updating the factor matrix of a Tucker decomposition in ALS, which is not a pure Kronecker regression problem, thereby improving the running time of all steps of Tucker ALS. We demonstrate the speed and accuracy of this Kronecker regression algorithm on synthetic data and real-world image tensors."}}
{"id": "PArQjxpSzix", "cdate": 1640995200000, "mdate": 1681657223511, "content": {"title": "Subquadratic Kronecker Regression with Applications to Tensor Decomposition", "abstract": "Kronecker regression is a highly-structured least squares problem $\\min_{\\mathbf{x}} \\lVert \\mathbf{K}\\mathbf{x} - \\mathbf{b} \\rVert_{2}^2$, where the design matrix $\\mathbf{K} = \\mathbf{A}^{(1)} \\otimes \\cdots \\otimes \\mathbf{A}^{(N)}$ is a Kronecker product of factor matrices. This regression problem arises in each step of the widely-used alternating least squares (ALS) algorithm for computing the Tucker decomposition of a tensor. We present the first subquadratic-time algorithm for solving Kronecker regression to a $(1+\\varepsilon)$-approximation that avoids the exponential term $O(\\varepsilon^{-N})$ in the running time. Our techniques combine leverage score sampling and iterative methods. By extending our approach to block-design matrices where one block is a Kronecker product, we also achieve subquadratic-time algorithms for (1) Kronecker ridge regression and (2) updating the factor matrices of a Tucker decomposition in ALS, which is not a pure Kronecker regression problem, thereby improving the running time of all steps of Tucker ALS. We demonstrate the speed and accuracy of this Kronecker regression algorithm on synthetic data and real-world image tensors."}}
{"id": "M5oTlSbMe0d", "cdate": 1640995200000, "mdate": 1681494432333, "content": {"title": "Amortized Rejection Sampling in Universal Probabilistic Programming", "abstract": ""}}
{"id": "pFQ8yH-F7i", "cdate": 1609459200000, "mdate": 1684331148349, "content": {"title": "Socially Fair k-Means Clustering", "abstract": ""}}
{"id": "mHq_bM2BA8", "cdate": 1609459200000, "mdate": 1684331148582, "content": {"title": "Beyond Submodular Maximization via One-Sided Smoothness", "abstract": "The multilinear framework was developed to achieve the breakthrough 1 \u2013 1/e approximation for maximizing a monotone submodular function subject to a matroid constraint, which includes the submodular welfare problem as special case. This framework has a continuous optimization part (solving the multilinear extension of a submodular set function) and a rounding part (rounding a fractional solution to an integral one). We extend both parts so that the resulting generalized framework may be used on a wider array of problems. In particular, we make a conceptual contribution by identifying a family of parameterized functions and their applications. As a running example we focus on solving diversity problems max , where \u2133 is matroid. These diversity functions have Aij \u2265 0 as a measure of dissimilarity of i, j, and A has 0-diagonal. This family of problems ranges from intractable problems such as densest k-subgraph, to \u00bd-approximable metric diversity problems. The multilinear extension F of such diversity functions satisfies \u25bf2F(x) = A \u2265 0 and hence the original multilinear framework (which assumes non-positive Hessians) does not directly apply. Instead we introduce a new parameter for functions F \u220a C2 which measures the approximability of the associated problem max{F(x) : x \u220a P}, for solvable downwards-closed polytopes P. A function F is called one-sided \u03c3-smooth if for all u, x \u2265 0, x = 0. For \u03c3 = 0 this class includes previously studied classes such as continuous DR-submodular functions, and much more. For the multlinear extension of a diversity function, we show that it is one-sided \u03c3-smooth whenever Aij forms a \u03c3-semi-metric. We give an \u03a9(1/\u03c3)-approximation for the continuous maximization problem of monotone, normalized one-sided \u03c3-smooth F with an additional property: non-positive third order partial derivatives. Since the multilinear extension of a diversity function has this additional property we can apply the extended multilinear framework to this family of discrete problems. This requires new matroid rounding techniques for quadratic objectives. The result is an \u03a9(1/\u03c33/2)-approximation for maximizing a \u03c3-semi-metric diversity function subject to matroid constraint. This improves upon the previous best bound of \u03a9(1/\u03c3) and we give evidence that it may be tight. For general one-sided smooth functions, we show the continuous process gives an \u03a9(1/32\u03c3)-approximation, independent of n. In this setting, by discretizing, we present a concrete poly-time algorithm for multilinear functions that satisfy the one-sided \u03c3-smoothness condition."}}
