{"id": "HLcgsgKEpMq", "cdate": 1648343283242, "mdate": null, "content": {"title": "It\u2019s Over There: Designing an Intelligent Virtual Agent That Can Point Accurately into the Real World", "abstract": "It is challenging to design an intelligent virtual agent (IVA) that can point from the virtual to the real world and have users accurately recognize where it is pointing due to differences in perceptual cues between the two spaces. We designed an IVA with factors including: a situated display, appearance, and pointing gesture strategy to establish whether it is possible to have an IVA point accurately into the real world. With a real person pointing as a baseline, we performed an empirical study using our designed IVA and demonstrated that participants perceived the IVA's pointing to a physical location with comparable accuracy to a real person baseline. Specifically, we found that when the IVA is 230 cm away from the targets on average, the IVA outperformed the real person in the vertical dimension (10.22 cm, 28.8% less error) and achieved the same level of accuracy (11.58 cm) horizontally. Our integrated design choices provide a foundation for design factors to consider when designing IVAs for pointing and pave the way for future studies and systems in providing accurate pointing perception."}}
{"id": "brggMSDG2B", "cdate": 1609459200000, "mdate": null, "content": {"title": "SPEAK WITH YOUR HANDS - Using Continuous Hand Gestures to control Articulatory Speech Synthesizer", "abstract": "This work presents our advancements in controlling an articulatory speech synthesis engine, \\textit{viz.}, Pink Trombone, with hand gestures. Our interface translates continuous finger movements and wrist flexion into continuous speech using vocal tract area-function based articulatory speech synthesis. We use Cyberglove II with 18 sensors to capture the kinematic information of the wrist and the individual fingers, in order to control a virtual tongue. The coordinates and the bending values of the sensors are then utilized to fit a spline tongue model that smoothens out the noisy values and outliers. Considering the upper palate as fixed and the spline model as the dynamically moving lower surface (tongue) of the vocal tract, we compute 1D area functional values that are fed to the Pink Trombone, generating continuous speech sounds. Therefore, by learning to manipulate one's wrist and fingers, one can learn to produce speech sounds just through one's hands, without the need for using the vocal tract."}}
{"id": "WZqn62UhuwI", "cdate": 1609459200000, "mdate": null, "content": {"title": "A comparative study of two-dimensional vocal tract acoustic modeling based on Finite-Difference Time-Domain methods", "abstract": "The two-dimensional (2D) numerical approaches for vocal tract (VT) modelling can afford a better balance between the low computational cost and accurate rendering of acoustic wave propagation. However, they require a high spatio-temporal resolution in the numerical scheme for a precise estimation of acoustic formants at the simulation run-time expense. We have recently proposed a new VT acoustic modelling technique, known as the 2.5D Finite-Difference Time-Domain (2.5D FDTD), which extends the existing 2D FDTD approach by adding tube depth to its acoustic wave solver. In this work, first, the simulated acoustic outputs of our new model are shown to be comparable with the 2D FDTD and a realistic 3D FEM VT model at a low spatio-temporal resolution. Next, a radiation model is developed by including a circular baffle around the VT as head geometry. The transfer functions of the radiation model are analyzed using five different vocal tract shapes for vowel sounds /a/, /e/, /i/, /o/ and /u/."}}
{"id": "B9xyqeaVvZY", "cdate": 1609459200000, "mdate": null, "content": {"title": "Active learning with online video: The impact of learning context on engagement", "abstract": "Highlights \u2022 This study investigated the impact of learning context on students' engagement with video in online and blended courses. \u2022 Focus on students' video activity with ViDeX, a hypervideo player that supports in situ annotating and highlighting. \u2022 A survey identified eight engagement goals: Reflect, Flag, Remember, Clarify, Skim, Search, Orient, and Take a break. \u2022 Analysis of clickstream data found that students use video much more adaptively in online courses than in blended courses. \u2022 Limitations of clickstream data for identifying higher level engagement are discussed. Abstract Learning with online video is pervasive in higher education. Recent research has explored the importance of student engagement when learning with video in online and blended courses. However, little is known about students' goals and intents when engaging with video. Furthermore, there is limited empirical evidence on the impact of learning context on engagement with video, which limits our understanding of how students learn from video. To address this gap, we identify a set of engagement goals for learning with video, and study associated student activity in relation to learning context (course week, exam, and rewatch). In Study 1, we conducted a survey (n\u00a0=\u00a0116) that maps students' video viewing activities to their engagement goals and intents. We identified a variety of engagement goals, specifically Reflect, Flag, Remember, Clarify, Skim, Search, Orient, and Take a break. In Study 2, we analyzed clickstream data generated by 387 students enrolled in three semester-long courses. We examined the impact of learning context on students\u2019 engagement with video. A multilevel model showed different patterns for online and blended courses. Students in the online course showed much more strategic and adaptive use of video. As the semester progressed, students in the online courses performed fewer Reflect and Search. During exam weeks and when rewatching videos, online students performed more Search within the video. The only trend that was found for blended learning students was an increase in Skim with course week. These findings have implications for video players that adapt to context, such as helping students easily locate important in-video information during the exam week or when rewatching previously watched videos."}}
{"id": "twMABpNZRgY", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Joint Articulatory-Acoustic Representations with Normalizing Flows", "abstract": "The articulatory geometric configurations of the vocal tract and the acoustic properties of the resultant speech sound are considered to have a strong causal relationship. This paper aims at finding a joint latent representation between the articulatory and acoustic domain for vowel sounds via invertible neural network models, while simultaneously preserving the respective domain-specific features. Our model utilizes a convolutional autoencoder architecture and normalizing flow-based models to allow both forward and inverse mappings in a semi-supervised manner, between the mid-sagittal vocal tract geometry of a two degrees-of-freedom articulatory synthesizer with 1D acoustic wave model and the Mel-spectrogram representation of the synthesized speech sounds. Our approach achieves satisfactory performance in achieving both articulatory-to-acoustic as well as acoustic-to-articulatory mapping, thereby demonstrating our success in achieving a joint encoding of both the domains."}}
{"id": "ZIc5Cpa4Z0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Ultra2Speech - A Deep Learning Framework for Formant Frequency Estimation and Tracking from Ultrasound Tongue Images", "abstract": "Thousands of individuals need surgical removal of their larynx due to critical diseases every year and therefore, require an alternative form of communication to articulate speech sounds after the loss of their voice box. This work addresses the articulatory-to-acoustic mapping problem based on ultrasound (US) tongue images for the development of a silent-speech interface (SSI) that can provide them with an assistance in their daily interactions. Our approach targets automatically extracting tongue movement information by selecting an optimal feature set from US images and mapping these features to the acoustic space. We use a novel deep learning architecture to map US tongue images from the US probe placed beneath a subject's chin to formants that we call, Ultrasound2Formant (U2F) Net. It uses hybrid spatio-temporal 3D convolutions followed by feature shuffling, for the estimation and tracking of vowel formants from US images. The formant values are then utilized to synthesize continuous time-varying vowel trajectories, via Klatt Synthesizer. Our best model achieves R-squared (R^2) measure of 99.96% for the regression task. Our network lays the foundation for an SSI as it successfully tracks the tongue contour automatically as an internal representation without any explicit annotation."}}
{"id": "Rm6yEcwJ2Dn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Joint Articulatory-Acoustic Representations with Normalizing Flows", "abstract": "The articulatory geometric configurations of the vocal tract and the acoustic properties of the resultant speech sound are considered to have a strong causal relationship. This paper aims at finding a joint latent representation between the articulatory and acoustic domain for vowel sounds via invertible neural network models, while simultaneously preserving the respective domain-specific features. Our model utilizes a convolutional autoencoder architecture and normalizing flow-based models to allow both forward and inverse mappings in a semi-supervised manner, between the mid-sagittal vocal tract geometry of a two degrees-of-freedom articulatory synthesizer with 1D acoustic wave model and the Mel-spectrogram representation of the synthesized speech sounds. Our approach achieves satisfactory performance in achieving both articulatory-to-acoustic as well as acoustic-to-articulatory mapping, thereby demonstrating our success in achieving a joint encoding of both the domains."}}
{"id": "LYrVRyhNcCe", "cdate": 1577836800000, "mdate": null, "content": {"title": "Estimating tongue deformation during laryngoscopy using hybrid FEM-multibody model and intraoperative tracking: a cadaver pilot study", "abstract": "Minimally invasive approaches to treating tumors of the pharynx and larynx like trans-oral surgery have improved patient outcome, but challenges remain in localizing tumors for margin control. Introducing necessary retractors and scopes deforms the anatomy and the tumor, rendering preoperative imaging inaccurate and making tumor localization difficult. This paper describes a pipeline that uses preoperative imaging to generate a hybrid FEM-multibody model and then dynamically simulates tongue deformation due to insertion of an electromagnetically-tracked laryngoscope. We hypothesize that the simulation output will be a sufficient estimate of the final intraoperative state and thus provide the surgeon with more accurate guidance during surgical resection. This pipeline was trialed on a cadaver head. The skull, mandible, and laryngoscope were tracked, and fiducial clips were embedded in the tongue for calculating target localization error (TLE) between the simulated and real tongue deformation. Registration accuracies were 1.1, 1.3, and 0.8 mm, respectively, for the tracked skull, mandible, and laryngoscope, and tracking and segmentation validation between the last tracked frame and the ground-truth intraoperative CT was 0.8, 0.9, and 1.2 mm, respectively. TLE of 6.4&plusmn;2.5 mm was achieved for the full pipeline, in contrast to the total tongue deformation of 37.2&plusmn;11.4 mm (via tongue clips) between the preoperative and intraoperative CT. Use of tracking and deformation modeling is viable to estimate deformation of the tongue during laryngoscopy. Future work involves additional intraoperative data streams to help further refine model parameters and improve localization."}}
{"id": "D078rczVJJ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Ultra2Speech - A Deep Learning Framework for Formant Frequency Estimation and Tracking from Ultrasound Tongue Images", "abstract": "Thousands of individuals need surgical removal of their larynx due to critical diseases every year and therefore, require an alternative form of communication to articulate speech sounds after the loss of their voice box. This work addresses the articulatory-to-acoustic mapping problem based on ultrasound (US) tongue images for the development of a silent-speech interface (SSI) that can provide them with an assistance in their daily interactions. Our approach targets automatically extracting tongue movement information by selecting an optimal feature set from US images and mapping these features to the acoustic space. We use a novel deep learning architecture to map US tongue images from the US probe placed beneath a subject\u2019s chin to formants that we call, Ultrasound2Formant (U2F) Net. It uses hybrid spatio-temporal 3D convolutions followed by feature shuffling, for the estimation and tracking of vowel formants from US images. The formant values are then utilized to synthesize continuous time-varying vowel trajectories, via Klatt Synthesizer. Our best model achieves R-squared ( $$R^2$$ ) measure of 99.96% for the regression task. Our network lays the foundation for an SSI as it successfully tracks the tongue contour automatically as an internal representation without any explicit annotation."}}
{"id": "Big_jNJlONX", "cdate": 1577836800000, "mdate": null, "content": {"title": "Artificial Intelligence for Video-based Learning at Scale", "abstract": "Video-based learning (VBL) is widespread; however, there are numerous challenges when teaching and learning with video. For instructors, creating effective instructional videos takes considerable time and effort. For students, watching videos can be a passive learning activity. Artificial intelligence (AI) has the potential to improve the VBL experience for students and teachers. This half-day workshop will bring together multi-disciplinary researchers and practitioners to collaboratively envision the future of VBL enhanced by AI. This workshop will be comprised of a group discussion followed by a presentation session. The goal of the workshop is to facilitate the cross-pollination of design ideas and critical assessments of AI approaches to VBL."}}
