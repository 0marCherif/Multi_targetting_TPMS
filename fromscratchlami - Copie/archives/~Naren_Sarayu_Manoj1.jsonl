{"id": "aUChMBWeyWt", "cdate": 1701816672866, "mdate": 1701816672866, "content": {"title": "Dueling Optimization with a Monotone Adversary", "abstract": "We introduce and study the problem of \\textit{dueling optimization with a monotone adversary}, which is a generalization of (noiseless) dueling convex optimization. The goal is to design an online algorithm to find a minimizer $\\bm{x}^{\\star}$ for a function $f\\colon \\mathcal{X} \\to \\mathbb{R}$, where $\\mathcal{X} \\subseteq \\mathbb{R}^d$. In each round, the algorithm submits a pair of guesses, i.e., $\\bm{x}^{(1)}$ and $\\bm{x}^{(2)}$, and the adversary responds with \\textit{any} point in the space that is at least as good as both guesses. The cost of each query is the suboptimality of the worse of the two guesses; i.e., ${\\max} \\left( f(\\bm{x}^{(1)}), f(\\bm{x}^{(2)}) \\right) - f(\\bm{x}^{\\star})$. The goal is to minimize the number of iterations required to find an $\\eps$-optimal point and to minimize the total cost (regret) of the guesses over many rounds. Our main result is an efficient randomized algorithm for several natural choices of the function $f$ and set $\\mathcal{X}$ that incurs cost $O(d)$ and iteration complexity $O(d\\log(1/\\varepsilon)^2)$. Moreover, our dependence on $d$ is asymptotically optimal, as we show examples in which any randomized algorithm for this problem must incur $\\Omega(d)$ cost and iteration complexity."}}
{"id": "yUs-Evk1Vh", "cdate": 1672531200000, "mdate": 1684085113941, "content": {"title": "An Optimal Algorithm for Certifying Monotone Functions", "abstract": "Given query access to a monotone function f: {0,1}n \u2192 {0,1} with certificate complexity C(f) and an input x*, we design an algorithm that outputs a size-C (f) subset of x* certifying the value of f(x*). Our algorithm makes O (C(f) \u00b7 log n) queries to f, which matches the information-theoretic lower bound for this problem and resolves the main open question posed in the STOC 2022 paper of Blanc, Koch, Lange, and Tan [BKLT22]. We extend this result to an algorithm that finds a size-2C(f) certificate for a real-valued monotone function with O (C(f) \u00b7 logn) queries. We also complement our algorithms with a hardness result, in which we show that finding the shortest possible certificate for x* may require queries in the worst case."}}
{"id": "PKYQ3oFOMV", "cdate": 1672531200000, "mdate": 1684237353200, "content": {"title": "Interpolation Learning With Minimum Description Length", "abstract": "We prove that the Minimum Description Length learning rule exhibits tempered overfitting. We obtain tempered agnostic finite sample learning guarantees and characterize the asymptotic behavior in the presence of random label noise."}}
{"id": "qn0ekD1_J-R", "cdate": 1640995200000, "mdate": 1684085113943, "content": {"title": "Streaming Algorithms for Ellipsoidal Approximation of Convex Polytopes", "abstract": "We give efficient deterministic one-pass streaming algorithms for finding an ellipsoidal approximation of a symmetric convex polytope. The algorithms are near-optimal in that their approximation fa..."}}
{"id": "8kk8a_zvWua", "cdate": 1621629928006, "mdate": null, "content": {"title": "Excess Capacity and Backdoor Poisoning", "abstract": "A backdoor data poisoning attack is an adversarial attack wherein the attacker injects several watermarked, mislabeled training examples into a training set. The watermark does not impact the test-time performance of the model on typical data; however, the model reliably errs on watermarked examples.\n\nTo gain a better foundational understanding of backdoor data poisoning attacks, we present a formal theoretical framework within which one can discuss backdoor data poisoning attacks for classification problems. We then use this to analyze important statistical and computational issues surrounding these attacks.\n\nOn the statistical front, we identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows us to argue about the robustness of several natural learning problems to backdoor attacks. Our results favoring the attacker involve presenting explicit constructions of backdoor attacks, and our robustness results show that some natural problem settings cannot yield successful backdoor attacks.\n\nFrom a computational standpoint, we show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. We then show that under similar assumptions, two closely related problems we call backdoor filtering and robust generalization are nearly equivalent. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors."}}
{"id": "Ee8Qj6cddic", "cdate": 1577836800000, "mdate": 1684085113951, "content": {"title": "Random Smoothing Might be Unable to Certify L\u221e Robustness for High-Dimensional Images", "abstract": "We show a hardness result for random smoothing to achieve certified adversarial robustness against attacks in the $\\ell_p$ ball of radius $\\epsilon$ when $p>2$. Although random smoothing has been well understood for the $\\ell_2$ case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of $p>2$. This has been posed as an open problem by Cohen et al. (2019) and includes many significant paradigms such as the $\\ell_\\infty$ threat model. In this work, we show that any noise distribution $\\mathcal{D}$ over $\\mathbb{R}^d$ that provides $\\ell_p$ robustness for all base classifiers with $p>2$ must satisfy $\\mathbb{E} \\eta_i^2=\\Omega(d^{1-2/p}\\epsilon^2(1-\\delta)/\\delta^2)$ for 99% of the features (pixels) of vector $\\eta\\sim\\mathcal{D}$, where $\\epsilon$ is the robust radius and $\\delta$ is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in $[0,255]$, the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classifiers."}}
{"id": "kxAG1KeUz29", "cdate": 1546300800000, "mdate": 1684085113940, "content": {"title": "Quantifying Perceptual Distortion of Adversarial Examples", "abstract": "Recent work has shown that additive threat models, which only permit the addition of bounded noise to the pixels of an image, are insufficient for fully capturing the space of imperceivable adversarial examples. For example, small rotations and spatial transformations can fool classifiers, remain imperceivable to humans, but have large additive distance from the original images. In this work, we leverage quantitative perceptual metrics like LPIPS and SSIM to define a novel threat model for adversarial attacks. To demonstrate the value of quantifying the perceptual distortion of adversarial examples, we present and employ a unifying framework fusing different attack styles. We first prove that our framework results in images that are unattainable by attack styles in isolation. We then perform adversarial training using attacks generated by our framework to demonstrate that networks are only robust to classes of adversarial perturbations they have been trained against, and combination attacks are stronger than any of their individual components. Finally, we experimentally demonstrate that our combined attacks retain the same perceptual distortion but induce far higher misclassification rates when compared against individual attacks."}}
