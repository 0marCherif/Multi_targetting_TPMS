{"id": "JqOQ5rP59SR", "cdate": 1681650813344, "mdate": null, "content": {"title": "Fat-shattering dimension of k-fold maxima", "abstract": "We provide improved estimates on the fat-shattering dimension of the k-fold maximum of\nreal-valued function classes. The latter consists of all ways of choosing k functions, one\nfrom each of the k classes, and computing their pointwise maximum. The bound is stated\nin terms of the fat-shattering dimensions of the component classes. For linear and affine\nfunction classes, we provide a considerably sharper upper bound and a matching lower\nbound, achieving, in particular, an optimal dependence on k. Along the way, we point out\nand correct a number of erroneous claims in the literature."}}
{"id": "uFuUlpRSa2", "cdate": 1681650772940, "mdate": null, "content": {"title": "Adversarially Robust Learning of Real-Valued Functions", "abstract": "We study robustness to test-time adversarial attacks in the regression setting with $\\ell_p$ losses and\narbitrary perturbation sets. We address the question of which function classes are PAC learnable in\nthis setting. We show that classes of finite fat-shattering dimension are learnable. Moreover, for convex function classes, they are even properly learnable. In contrast, some non-convex function classes\nprovably require improper learning algorithms. We also discuss extensions to agnostic learning. Our\nmain technique is based on a construction of an adversarially robust sample compression scheme of\na size determined by the fat-shattering dimension."}}
{"id": "uQ7fbEkzqlH", "cdate": 1681650690869, "mdate": null, "content": {"title": "Learning Revenue Maximization using Posted Prices for Stochastic Strategic Patient Buyers", "abstract": "We consider a seller faced with buyers which have the ability to delay their decision, which we\ncall patience. Each buyer\u2019s type is composed of value and patience, and it is sampled i.i.d. from a\ndistribution. The seller, using posted prices, would like to maximize her revenue from selling to the\nbuyer. In this paper, we formalize this setting and characterize the resulting Stackelberg equilibrium,\nwhere the seller first commits to her strategy, and then the buyers best respond. Following this, we\nshow how to compute both the optimal pure and mixed strategies. We then consider a learning\nsetting, where the seller does not have access to the distribution over buyer\u2019s types. Our main\nresults are the following. We derive a sample complexity bound for the learning of an approximate\noptimal pure strategy, by computing the fat-shattering dimension of this setting. Moreover, we\nprovide a general sample complexity bound for the approximate optimal mixed strategy. We also\nconsider an online setting and derive a vanishing regret bound with respect to both the optimal pure\nstrategy and the optimal mixed strategy."}}
{"id": "B7Q2mbIFa6Q", "cdate": 1652737710158, "mdate": null, "content": {"title": "A Characterization of Semi-Supervised Adversarially Robust PAC Learnability", "abstract": "We study the problem of learning an adversarially robust predictor to test time attacks in the semi-supervised PAC model.\nWe address the question of how many labeled and unlabeled examples are required to ensure learning.\nWe show that having enough unlabeled data (the size of a labeled sample that a fully-supervised method would require),\nthe labeled sample complexity can be arbitrarily smaller compared to previous works, and is sharply characterized by a different complexity measure. We prove nearly matching upper and lower bounds on this sample complexity.\nThis shows that there is a significant benefit in semi-supervised robust learning even in the worst-case distribution-free model, and establishes a gap between supervised and semi-supervised label complexities which is known not to hold in standard non-robust PAC learning."}}
{"id": "bUAdXW8wN6", "cdate": 1632875720496, "mdate": null, "content": {"title": "Domain Invariant Adversarial Learning", "abstract": "The phenomenon of adversarial examples illustrates one of the most basic vulnerabilities of deep neural networks. Among the variety of techniques introduced to surmount this inherent weakness, adversarial training has emerged as the most effective strategy to achieve robustness. Typically, this is achieved by balancing robust and natural objectives. In this work, we aim to further reduce the trade-off between robust and standard accuracy by enforcing a domain-invariant feature representation. We present a new adversarial training method, Domain Invariant Adversarial Learning (DIAL), which learns a feature representation which is both robust and domain invariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on the natural domain and its corresponding adversarial domain. In a case where the source domain consists of natural examples and the target domain is the adversarially perturbed examples, our method learns a feature representation constrained not to discriminate between the natural and adversarial examples, and can therefore achieve a more robust representation. Our experiments indicate that our method improves both robustness and standard accuracy, when compared to other state-of-the-art adversarial training methods."}}
{"id": "89CsJP5oVQw", "cdate": 1577836800000, "mdate": null, "content": {"title": "Prediction with Corrupted Expert Advice", "abstract": "We revisit the fundamental problem of prediction with expert advice, in a setting where the environment is benign and generates losses stochastically, but the feedback observed by the learner is subject to a moderate adversarial corruption. We prove that a variant of the classical Multiplicative Weights algorithm with decreasing step sizes achieves constant regret in this setting and performs optimally in a wide range of environments, regardless of the magnitude of the injected corruption. Our results reveal a surprising disparity between the often comparable Follow the Regularized Leader (FTRL) and Online Mirror Descent (OMD) frameworks: we show that for experts in the corrupted stochastic regime, the regret performance of OMD is in fact strictly inferior to that of FTRL."}}
{"id": "APZHZv6CHZQv", "cdate": 1514764800000, "mdate": null, "content": {"title": "Improved generalization bounds for robust learning.", "abstract": "We consider a model of robust learning in an adversarial environment. The learner gets uncorrupted training data with access to possible corruptions that may be affected by the adversary during testing. The learner's goal is to build a robust classifier, which will be tested on future adversarial examples. The adversary is limited to $k$ possible corruptions for each input. We model the learner-adversary interaction as a zero-sum game. This model is closely related to the adversarial examples model of Schmidt et al. (2018); Madry et al. (2017). Our main results consist of generalization bounds for the binary and multiclass classification, as well as the real-valued case (regression). For the binary classification setting, we both tighten the generalization bound of Feige et al. (2015), and are also able to handle infinite hypothesis classes. The sample complexity is improved from $O(\\frac{1}{\\epsilon^4}\\log(\\frac{|H|}{\\delta}))$ to $O\\big(\\frac{1}{\\epsilon^2}(kVC(H)\\log^{\\frac{3}{2}+\\alpha}(kVC(H))+\\log(\\frac{1}{\\delta})\\big)$ for any $\\alpha > 0$. Additionally, we extend the algorithm and generalization bound from the binary to the multiclass and real-valued cases. Along the way, we obtain results on fat-shattering dimension and Rademacher complexity of $k$-fold maxima over function classes; these may be of independent interest. For binary classification, the algorithm of Feige et al. (2015) uses a regret minimization algorithm and an ERM oracle as a black box; we adapt it for the multiclass and regression settings. The algorithm provides us with near-optimal policies for the players on a given training sample."}}
