{"id": "vdv6CmGksr0", "cdate": 1663850027574, "mdate": null, "content": {"title": "Learning differentiable solvers for systems with hard constraints", "abstract": "We introduce a practical method to enforce partial differential equation (PDE) constraints for functions defined by neural networks (NNs), with a high degree of accuracy and up to a desired tolerance. We develop a differentiable PDE-constrained layer that can be incorporated into any NN architecture. Our method leverages differentiable optimization and the implicit function theorem to effectively enforce physical constraints. Inspired by dictionary learning, our model learns a family of functions, each of which defines a mapping from PDE parameters to PDE solutions. At inference time, the model finds an optimal linear combination of the functions in the learned family by solving a PDE-constrained optimization problem. Our method provides continuous solutions over the domain of interest that accurately satisfy desired physical constraints. Our results show that incorporating hard constraints directly into the NN architecture achieves much lower test error when compared to training on an unconstrained objective."}}
{"id": "T1O67-Ou6Y", "cdate": 1640995200000, "mdate": 1681099582874, "content": {"title": "Learning differentiable solvers for systems with hard constraints", "abstract": ""}}
{"id": "J0PFkzJ6WgM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Stochastic Frank-Wolfe for Constrained Finite-Sum Minimization", "abstract": "We propose a novel Stochastic Frank-Wolfe (a. k. a. conditional gradient) algorithm for constrained smooth finite-sum minimization with a generalized linear prediction/structure. This class of prob..."}}
{"id": "GFicedEhmj2", "cdate": 1577836800000, "mdate": null, "content": {"title": "Linearly Convergent Frank-Wolfe without Line-Search", "abstract": "Structured constraints in Machine Learning have recently brought the Frank-Wolfe (FW) family of algorithms back in the spotlight. While the classical FW algorithm has poor local convergence propert..."}}
{"id": "60RkXM8KPp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Stochastic Frank-Wolfe for Constrained Finite-Sum Minimization", "abstract": "We propose a novel Stochastic Frank-Wolfe (a.k.a. conditional gradient) algorithm for constrained smooth finite-sum minimization with a generalized linear prediction/structure. This class of problems includes empirical risk minimization with sparse, low-rank, or other structured constraints. The proposed method is simple to implement, does not require step-size tuning, and has a constant per-iteration cost that is independent of the dataset size. Furthermore, as a byproduct of the method we obtain a stochastic estimator of the Frank-Wolfe gap that can be used as a stopping criterion. Depending on the setting, the proposed method matches or improves on the best computational guarantees for Stochastic Frank-Wolfe algorithms. Benchmarks on several datasets highlight different regimes in which the proposed method exhibits a faster empirical convergence than related methods. Finally, we provide an implementation of all considered methods in an open-source package."}}
{"id": "MU1ivpu4aHu", "cdate": 1514764800000, "mdate": null, "content": {"title": "Lifted Neural Networks", "abstract": "We describe a novel family of models of multi- layer feedforward neural networks in which the activation functions are encoded via penalties in the training problem. Our approach is based on representing a non-decreasing activation function as the argmin of an appropriate convex optimiza- tion problem. The new framework allows for algo- rithms such as block-coordinate descent methods to be applied, in which each step is composed of a simple (no hidden layer) supervised learning problem that is parallelizable across data points and/or layers. Experiments indicate that the pro- posed models provide excellent initial guesses for weights for standard neural networks. In addi- tion, the model provides avenues for interesting extensions, such as robustness against noisy in- puts and optimizing over parameters in activation functions."}}
