{"id": "S5KslIBXt_", "cdate": 1676827065698, "mdate": null, "content": {"title": "RDM-DC: Poisoning Resilient Dataset Condensation with Robust Distribution Matching", "abstract": "Dataset condensation aims to condense the original training dataset into a small synthetic dataset for data-efficient learning. The recently proposed dataset condensation techniques allow the model trainers with limited resources to learn acceptable deep learning models on a small amount of synthetic data. However, in an adversarial environment, given the original dataset as a poisoned dataset, dataset condensation may encode the poisoning information into the condensed synthetic dataset. To explore the vulnerability of dataset condensation to data poisoning, we revisit the state-of-the-art targeted data poisoning method and customize a targeted data poisoning algorithm for dataset condensation. By executing the two poisoning methods, we demonstrate that, when the synthetic dataset is condensed from a poisoned dataset, the models trained on the synthetic dataset may predict the targeted sample as the attack-targeted label. To defend against data poisoning, we introduce the concept of poisoned deviation to quantify the poisoning effect. We further propose a poisoning-resilient dataset condensation algorithm with a calibration method to reduce poisoned deviation. Extensive evaluations demonstrate that our proposed algorithm can protect the synthetic dataset from data poisoning with minor performance drop."}}
{"id": "ZRkHGPMY3dd", "cdate": 1663850144706, "mdate": null, "content": {"title": "Fairness of Federated Learning with Dynamic Participants", "abstract": "The concept of fairness has widely caught the attention of Federated Learning (FL). While there are tremendous studies about various notations of fairness in FL in recent years, all of them only consider the case where the training process starts and ends at the time point for all participants. Actually, participants could be dynamic and they may join and leave the training process at different time points. However, participants who join the training process at different time points receive similar incentive benefits can be seen as a signal of unfairness. In this paper, we provide the first study on such fairness of FL for dynamic participants. First, we propose a new mathematical definition of the above fairness namely $\\textit{dynamic fairness}$. Briefly speaking, an algorithm is dynamically fair and satisfies that local agents who participate in the model training longer should receive more benefits than those who participate in the process shorter. Second, we develop a simple but novel method, which could be seen as a normalized version of $\\textit{Fedavg}$, and theoretically show that it is fairer than $\\textit{Fedavg}$.  Moreover, we can combine our method with the previous methods in fair FL for static participants to additionally guarantee fair treatment for local agents who join the training process at the same time point by minimizing the discrepancy of benefits they receive. Finally, empirically we propose a measure for $\\textit{dynamic fairness}$ and demonstrate that our method can achieve a fairer performance under our definition of fairness through intensive experiments on three benchmark datasets. "}}
{"id": "H8XpqEkbua_", "cdate": 1663849941051, "mdate": null, "content": {"title": "Differentially Private Dataset Condensation", "abstract": "Recent work in ICML'22 builds a theoretical connection between dataset condensation (DC) and differential privacy (DP) and claims that DC can provide privacy protection for free. However, the connection is problematic because of two controversial assumptions. In this paper, we revisit the ICML'22 work and elucidate the issues in the two controversial assumptions. To correctly connect DC and DP, we propose two differentially private dataset condensation (DPDC) algorithms---LDPDC and NDPDC. Through extensive evaluations on multiple datasets, we demonstrate that LDPDC has comparable performance to recent DP generative methods despite its simplicity. NDPDC provides acceptable DP guarantees with a mild utility loss, compared to the state-of-the-art DC method. Additionally, NDPDC allows a flexible trade-off between the synthetic data utility and DP budget."}}
{"id": "lifRwnIuAv0", "cdate": 1632875442117, "mdate": null, "content": {"title": "PGD-2 can be better than FGSM + GradAlign", "abstract": "One major issue of adversarial training (AT) with the fast gradient sign method (FGSM AT) is the phenomenon of catastrophic overfitting, meaning that the trained model suddenly loses its robustness over a single epoch. In addition to FGSM AT, Andriushchenko & Flammarion (2020) observed that two-step projected gradient descent adversarial training (PGD-2 AT) also suffers from catastrophic overfitting for large $\\ell_\\infty$ perturbations. To prevent catastrophic overfitting, Andriushchenko & Flammarion (2020) proposed a gradient alignment regularization method (GradAlign) and claimed that GradAlign can prevent catastrophic overfitting in FGSM AT and PGD-2 AT. In this paper, we show that PGD-2 AT with random initialization (PGD-2-RS AT) and attack step size $\\alpha=1.25\\epsilon/2$ only needs approximately a half computational cost of FGSM + GradAlign AT and actually can avoid catastrophic overfitting for large $\\ell_\\infty$ perturbations. We hypothesize that, if FGSM-RS AT with $\\alpha=1.25\\epsilon/2$ can avoid catastrophic overfitting for $\\ell_\\infty$ perturbation size $\\epsilon/2$, then PGD-2-RS AT with $\\alpha=1.25\\epsilon/2$ may be able to avoid catastrophic overfitting for $\\ell_\\infty$ perturbation size $\\epsilon$. Our intuitions to justify this empirical hypothesis induce a more unexpected finding: If we apply random noise from the uniform distribution $\\mathcal{U}(-\\epsilon/2, \\epsilon/2)$ to the perturbations before each step of PGD-2 with $\\alpha=1.25\\epsilon/2$, instead of initializing the perturbations with random noise from $\\mathcal{U}(-\\epsilon, \\epsilon)$ at the beginning ({\\em i.e.,} the conventional random initialization scheme), the corresponding AT method can also avoid catastrophic overfitting and even achieve better robust accuracy in most cases. We refer to this AT method as Qusai-PGD-2-RS AT. Extensive evaluations demonstrate that PGD-2-RS AT and Qusai-PGD-2-RS AT with $\\alpha=1.25\\epsilon/2$ achieve better performance and efficiency than FGSM + GradAlign AT. Notably, Qusai-PGD-2-RS AT achieves comparable robust accuracy against PGD-50-10 as PGD-3-RS AT on CIFAR10 and SVHN, and it also achieves approximately $18\\%$ top-1 and $38\\%$ top-5 robust accuracy against PGD-50-10 at $\\epsilon=8/255$ on ImageNet."}}
{"id": "ryl71a4YPB", "cdate": 1569438939231, "mdate": null, "content": {"title": "A Unified framework for randomized smoothing based certified defenses", "abstract": "Randomized smoothing, which was recently proved to be a certified defensive technique, has received considerable attention due to its scalability to large datasets and neural networks. However, several important questions still remain unanswered in the existing frameworks, such as (i) whether Gaussian mechanism is an optimal choice for certifying $\\ell_2$-normed robustness, and (ii) whether randomized smoothing can certify $\\ell_\\infty$-normed robustness (on high-dimensional datasets like ImageNet). To answer these questions, we introduce a {\\em  unified} and {\\em self-contained} framework to study randomized smoothing-based certified defenses, where we mainly focus on the two most popular norms in adversarial machine learning, {\\em i.e.,} $\\ell_2$ and $\\ell_\\infty$ norm. We answer the above two questions by first demonstrating that Gaussian mechanism and  Exponential mechanism are the (near) optimal options to certify the $\\ell_2$ and $\\ell_\\infty$-normed robustness. We further show that the largest $\\ell_\\infty$ radius certified by randomized smoothing is upper bounded by $O(1/\\sqrt{d})$, where $d$ is the dimensionality of the data. This theoretical finding suggests that certifying $\\ell_\\infty$-normed robustness by randomized smoothing may not be scalable to high-dimensional data. The veracity of our framework and analysis is verified by extensive evaluations on CIFAR10 and ImageNet."}}
