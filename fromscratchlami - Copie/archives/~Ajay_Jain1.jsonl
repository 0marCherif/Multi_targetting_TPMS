{"id": "j4-a3SNyaY", "cdate": 1664310941193, "mdate": null, "content": {"title": "Journey to the BAOAB-limit: finding effective MCMC samplers for score-based models", "abstract": "Diffusion and score-based generative models have achieved remarkable sample quality on difficult image synthesis tasks. Many works have proposed samplers for pretrained diffusion models, including ancestral samplers, SDE and ODE integrators and annealed MCMC approaches. So far, the best sample quality has been achieved with samplers that use time-conditional score functions and move between several noise levels. However, estimating an accurate score function at many noise levels can be challenging and requires an architecture that is more expressive than would be needed for a single noise level. In this work, we explore MCMC sampling algorithms that operate at a single noise level, yet synthesize images with acceptable sample quality. We show that while na\u00efve application of Langevin dynamics and a related noise-denoise sampler produces poor samples, methods built on integrators of underdamped Langevin dynamics using splitting methods can perform well. Our samplers also have great diversity, allowing many samples to be generated in a single long-run MCMC chain. Further, by combining MCMC methods with existing multiscale samplers, we begin to approach competitive sample quality without using scores at large noise levels. Find videos and code at https://ajayj.com/journey."}}
{"id": "FjNys5c7VyY", "cdate": 1663850414208, "mdate": null, "content": {"title": "DreamFusion: Text-to-3D using 2D Diffusion", "abstract": "Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D or multiview data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors."}}
{"id": "HMzzPOLs9l5", "cdate": 1646077550900, "mdate": null, "content": {"title": "AdaCat: Adaptive Categorical Discretization for Autoregressive Models", "abstract": "Autoregressive generative models can estimate complex continuous data distributions, like trajectory rollouts in an RL environment, image intensities, and audio. Most state-of-the-art models discretize continuous data into several bins and use categorical distributions over the bins to approximate the continuous data distribution. The advantage is that the categorical distribution can easily express multiple modes and are straightforward to optimize. However, such approximation cannot express sharp changes in density without using significantly more bins, which makes it parameter inefficient. We propose an efficient, expressive, multimodal parameterization called Adaptive Categorical Discretization (AdaCat). AdaCat discretizes each dimension of an autoregressive model adaptively, which allows the model to allocate density to fine intervals of interest, improving parameter efficiency. AdaCat generalizes both categoricals and quantile-based regression. AdaCat is a simple add-on to any discretization-based distribution estimator. In experiments, AdaCat improves density estimation for real-world tabular data, images, audio, and trajectories, and improves planning in model-based offline RL.\n"}}
{"id": "uV7hcsjqM-", "cdate": 1601308275172, "mdate": null, "content": {"title": "Contrastive Code Representation Learning", "abstract": "Machine-aided programming tools such as automated type predictors and autocomplete are increasingly learning-based. However, current approaches predominantly rely on supervised learning with task-specific datasets. We propose Contrastive Code Representation Learning (ContraCode), a self-supervised algorithm for learning task-agnostic semantic representations of programs via contrastive learning. Our approach uses no human-provided labels, only the raw text of programs. ContraCode optimizes for a representation that is invariant to semantic-preserving code transformations. We develop an automated source-to-source compiler that generates textually divergent variants of source programs. We then train a neural network to identify variants of anchor programs within a large batch of non-equivalent negatives. To solve this task, the network must extract features representing the functionality, not form, of the program. In experiments, we pre-train ContraCode with 1.8M unannotated JavaScript methods mined from GitHub, then transfer to downstream tasks by fine-tuning. Pre-training with ContraCode consistently improves the F1 score of code summarization baselines and top-1 accuracy of type inference baselines by 2% to 13%. ContraCode achieves 9% higher top-1 accuracy than the current state-of-the-art static type analyzer for TypeScript. Finally, representations learned through a hybrid contrastive and reconstruction objective transfer in zero-shot to code clone detection with +10% AUROC over a static text similarity measure and +5% over reconstruction alone."}}
{"id": "HJGsj13qTE", "cdate": 1559048099484, "mdate": null, "content": {"title": "Using effective dimension to analyze feature transformations in deep neural networks", "abstract": "In a typical deep learning approach to a computer vision task, Convolutional Neural Networks (CNNs) are used to extract features at varying levels of abstraction from an image and compress a high dimensional input into a lower dimensional decision space through a series of transformations. In this paper, we investigate how a class of input images is eventually compressed over the course of these transformations. In particular, we use singular value decomposition to analyze the relevant variations in feature space. These variations are formalized as the effective dimension of the embedding. We consider how the effective dimension varies across layers within class. We show that across datasets and architectures, the effective dimension of a class increases before decreasing further into the network, suggesting some sort of initial whitening transformation. Further, the decrease rate of the effective dimension deeper in the network corresponds with training performance of the model."}}
{"id": "BJLSGcywG", "cdate": 1518473790125, "mdate": null, "content": {"title": "An Analysis of the Delayed Gradients Problem in Asynchronous SGD", "abstract": "Stochastic Gradient Descent can be effectively paralellized to many workers via the use of minibatches. Yet, parameter synchronization requires that each batch waits for the slowest worker to finish. There is a fully asynchronous method, detailed by Dean et. al (2012), called Downpour SGD, or ASGD, which minimizes worker idle time by allowing gradients which are computed on stale parameters to be sent to the parameter server. In practice, direct usage of ASGD is not recommended due to the added noise from stale gradients (referred as the \"delayed gradient problem\"), and therefore some form of delay compensation is required, as is detailed in Zheng et al (2017). In this paper, we present a detailed analysis of the failure modes of asynchronous SGD due to delayed gradients under various hyperparameter selections in order to better inform in what cases ASGD is best applied. On the MNIST digit recognition task with the LeNet5 model we find that delayed gradients significantly reduce test accuracy with large batch sizes and large learning rates. This limits the applicability of asynchronous gradient methods (without delay compensation) in cases where the learning rate is scaled linearly with the batch size, or with adaptive methods that may select large learning rates at times. Finally, we provide discussion on possible delay compensation methods."}}
