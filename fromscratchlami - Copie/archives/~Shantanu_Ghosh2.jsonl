{"id": "e0bEejCTLS", "cdate": 1672531200000, "mdate": 1697341090054, "content": {"title": "Distilling BlackBox to Interpretable Models for Efficient Transfer Learning", "abstract": "Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (e.g., scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a mixture of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Further, we use the pseudo-labeling technique from semi-supervised learning (SSL) to learn the concept classifier in the target domain, followed by fine-tuning the interpretable models in the target domain. We evaluate our model using a real-life large-scale chest-X-ray (CXR) classification dataset. The code is available at: https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs ."}}
{"id": "KAQDKr8Fbb", "cdate": 1672531200000, "mdate": 1697341090055, "content": {"title": "Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance", "abstract": "Discovering a high-performing sparse network within a massive neural network is advantageous for deploying them on devices with limited storage, such as mobile phones. Additionally, model explainability is essential to fostering trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep network with comparable or superior performance to the original model. However, limited study has been conducted on the success or failure of LTH in terms of explainability. In this work, we examine why the performance of the pruned networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept bottleneck models (PCBMs), respectively, we investigate the explainability of pruned networks in terms of pixels and high-level concepts. We perform extensive experiments across vision and medical imaging datasets. As more weights are pruned, the performance of the network degrades. The discovered concepts and pixels from the pruned networks are inconsistent with the original network -- a possible reason for the drop in performance."}}
{"id": "jVDhQl8mPpx", "cdate": 1663850238191, "mdate": null, "content": {"title": "Route, Interpret, Repeat: Blurring the Line Between Posthoc Explainability and Interpretable Models ", "abstract": "The current approach to ML model design is either to choose a flexible Blackbox model and explain it post hoc or to start with an interpretable model. Blackbox models are flexible but difficult to explain, whereas interpretable models are designed to be explainable. However, developing interpretable models necessitates extensive ML knowledge, and the resulting models tend to be less flexible, offering potentially subpar performance compared to their Blackbox equivalents. This paper aims to blur the distinction between a post hoc explanation of a BlackBox and constructing interpretable models. We propose beginning with a flexible BlackBox model and gradually carving out a mixture of interpretable models and a residual network. Our design identifies a subset of samples and routes them through the interpretable models. The remaining samples are routed through a flexible residual network. We adopt First Order Logic (FOL) as the interpretable model's backbone, which provides basic reasoning on the concept retrieved from the BlackBox model. On the residual network, we repeat the method until the proportion of data explained by the residual network falls below a desired threshold. \nOur approach offers several advantages. First, the mixture of interpretable and flexible residual networks results in almost no compromise in performance. Second, the rout, interpret, and repeat approach yields a highly flexible interpretable model. Our extensive experiment demonstrates the performance of the model on various datasets. We show that by editing the FOL model, we can fix the shortcut learned by the original BlackBox model. Finally, our method provides a framework for a hybrid symbolic-connectionist network that is simple to train and adaptable to many applications."}}
{"id": "wk_YLhUJAtM", "cdate": 1640995200000, "mdate": 1685078763554, "content": {"title": "DR-VIDAL - Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation", "abstract": ""}}
{"id": "lPkOoDyac4s", "cdate": 1640995200000, "mdate": 1678252584024, "content": {"title": "Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays", "abstract": ""}}
{"id": "YVbWbbmOcq1", "cdate": 1609459200000, "mdate": 1685078763567, "content": {"title": "Deep propensity network using a sparse autoencoder for estimation of treatment effects", "abstract": ""}}
{"id": "UKpjKlahrsr", "cdate": 1609459200000, "mdate": 1685078763560, "content": {"title": "Causal AI with Real World Data: Do Statins Protect from Alzheimer's Disease Onset?", "abstract": ""}}
