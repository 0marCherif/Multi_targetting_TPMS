{"id": "p0yrSRbN5Bu", "cdate": 1663850019650, "mdate": null, "content": {"title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning", "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks with abundant training samples. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and  propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of ensemble methods and simultaneously captures the sample-specific competence of each source prompt.  We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-\\{base, large, XL\\}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin."}}
{"id": "xw1j6ZC-d2", "cdate": 1640995200000, "mdate": 1682320907246, "content": {"title": "Modeling Document-level Temporal Structures for Building Temporal Dependency Graphs", "abstract": "Prafulla Kumar Choubey, Ruihong Huang. Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2022."}}
{"id": "xqz_o07B7d", "cdate": 1640995200000, "mdate": 1682320907227, "content": {"title": "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts", "abstract": "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (the \"experts\") and select one to query the LLM. These require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. P-Adapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of just using natural language queries alone. Finally, we investigate what makes P-Adapters successful and conclude that a significant factor is access to the LLM's embeddings of the original natural language prompt, particularly the subject of the entity pair being queried."}}
{"id": "fB8RF-enUiQ", "cdate": 1640995200000, "mdate": 1682320907088, "content": {"title": "Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure", "abstract": ""}}
{"id": "XHSuhkYkIE", "cdate": 1640995200000, "mdate": 1681497515792, "content": {"title": "Conformal Predictor for Improving Zero-Shot Text Classification Efficiency", "abstract": ""}}
{"id": "D0P2I-SR3D", "cdate": 1640995200000, "mdate": 1682320907158, "content": {"title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning", "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of model ensemble approaches and simultaneously captures the sample-specific competence of each source prompt. We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin."}}
{"id": "4lLUr3QQ_uc", "cdate": 1640995200000, "mdate": 1682320907336, "content": {"title": "Improving Factual Consistency in Summarization with Compression-Based Post-Editing", "abstract": ""}}
{"id": "_qjEae4op-", "cdate": 1632875571965, "mdate": null, "content": {"title": "MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization", "abstract": "Neural abstractive summarization models are susceptible to generating factually inconsistent content, a phenomenon known as hallucination. This limits the usability and adoption of these systems in real-world applications. To reduce the presence of hallucination,  we propose the Mixture of Factual Experts (MoFE) model, which combines multiple summarization experts that each target a specific type of error. We train our experts using reinforcement learning (RL) to minimize the error defined by two factual consistency metrics: entity overlap and dependency arc entailment. We construct MoFE by combining the experts using two ensembling strategies (weights and logits) and evaluate them on two summarization datasets (XSUM and CNN/DM). Our experiments on BART models show that the MoFE improves performance according to both entity overlap and dependency arc entailment, without a significant performance drop on standard ROUGE metrics. The performance improvement also transfers to unseen factual consistency metrics, such as question answer-based factuality evaluation metric and BERTScore precision with respect to the source document."}}
{"id": "DhzIU48OcZh", "cdate": 1632875550964, "mdate": null, "content": {"title": "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts", "abstract": "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (the \"experts\") and select one to query the LLM. These require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. P-Adapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of just using natural language queries alone. Finally, we investigate what makes P-Adapters successful and conclude that a significant factor is access to the LLM's embeddings of the original natural language prompt, particularly the subject of the entity pair being queried."}}
{"id": "reqnNXQIFPN", "cdate": 1609459200000, "mdate": 1639450142350, "content": {"title": "Profiling News Discourse Structure Using Explicit Subtopic Structures Guided Critics", "abstract": "Prafulla Kumar Choubey, Ruihong Huang. Findings of the Association for Computational Linguistics: EMNLP 2021. 2021."}}
