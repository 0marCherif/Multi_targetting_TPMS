{"id": "fY5xpJszW2-", "cdate": 1664924966384, "mdate": null, "content": {"title": "Meta-Learning via Classifier(-free) Guidance", "abstract": "We aim to develop meta-learning techniques that achieve higher zero-shot performance than the state of the art on unseen tasks. To do so, we take inspiration from recent advances in generative modeling and language-conditioned image synthesis to propose meta-learning techniques that use natural language guidance for zero-shot task adaptation. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second \"guidance\" model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance: \"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing meta-learning methods with zero-shot learning experiments on our Meta-VQA dataset."}}
{"id": "8NLta1E_BPR", "cdate": 1663850356321, "mdate": null, "content": {"title": "Meta-Learning via Classifier(-free) Guidance", "abstract": "State-of-the-art meta-learning techniques do not optimize for zero-shot adaptation to unseen tasks, a setting in which humans excel. On the contrary, meta-learning algorithms learn hyperparameters and weight initializations that explicitly optimize for few-shot learning performance. In this work, we take inspiration from recent advances in generative modeling and language-conditioned image synthesis to propose meta-learning techniques that use natural language guidance to achieve higher zero-shot performance compared to the state-of-the-art. We do so by recasting the meta-learning problem as a multi-modal generative modeling problem: given a task, we consider its adapted neural network weights and its natural language description as equivalent multi-modal task representations. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second \"guidance\" model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance: \"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing meta-learning methods with zero-shot learning experiments on our Meta-VQA dataset, which we specifically constructed to reflect the multi-modal meta-learning setting."}}
{"id": "xgqi6mkthA3", "cdate": 1640995200000, "mdate": 1681284256948, "content": {"title": "Fast Aquatic Swimmer Optimization with Differentiable Projective Dynamics and Neural Network Hydrodynamic Models", "abstract": ""}}
{"id": "aUsL3Gr7nb", "cdate": 1640995200000, "mdate": 1681284256951, "content": {"title": "Diversified Sampling for Batched Bayesian Optimization with Determinantal Point Processes", "abstract": ""}}
{"id": "_Dw0RojMtyq", "cdate": 1640995200000, "mdate": 1681284256952, "content": {"title": "Sim2Real for Soft Robotic Fish via Differentiable Simulation", "abstract": ""}}
