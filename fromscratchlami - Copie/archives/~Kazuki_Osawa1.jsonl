{"id": "iN5pHYUN-i", "cdate": 1681833044731, "mdate": null, "content": {"title": "Improving Continual Learning by Accurate Gradient Reconstructions of the Past", "abstract": "Regularization and experience replay are two popular continual-learning strategies with complementary strengths: while regularization requires less memory, replay can more accurately mimic batch training. But can we combine them to get provably better methods? Despite the simplicity of the question, little is known or done to find optimal combination methods that give provable improvements. In this paper, we present such a method by using a recently proposed principle of adaptation that relies on a faithful reconstruction of the gradients of the past data. Using this principle, we design a prior which combines two types of replay methods with a quadratic Bayesian weight-regularizer and achieves provably better gradient reconstructions. The combination improves performance on standard benchmarks such as Split CIFAR, Split TinyImageNet, and ImageNet-1000, often achieving >80% of the batch performance by simply utilizing a memory of <10% of the past data. Our work shows that a good combination of replay and regularizer-based methods can be very effective in reducing forgetting, and can sometimes even completely eliminate it."}}
{"id": "BVaytYu5Yj", "cdate": 1663850266491, "mdate": null, "content": {"title": "Improving Continual Learning by Accurate Gradient Reconstructions of the Past", "abstract": "Knowledge reuse is essential for continual learning, and current methods attempt to realize it through regularization or experience replay. These two strategies have complementary strengths, e.g., regularization methods are compact, but replay methods can mimic batch training more accurately. At present, little has been done to find principled ways to combine the two methods and current heuristics can give suboptimal performance. Here, we provide a principled approach to combine and improve them by using a recently proposed principle of adaptation, where the goal is to reconstruct the \u201cgradients of the past\u201d, i.e., to mimic batch training by estimating gradients from past data. Using this principle, we design a prior that provably gives better gradient reconstructions by utilizing two types of replay and a quadratic weight-regularizer. This improves performance on standard benchmarks such as Split CIFAR, Split TinyImageNet, and ImageNet-1000. Our work shows that a good combination of replay and regularizer-based methods can be very effective in reducing forgetting, and can sometimes even completely eliminate it."}}
{"id": "gHi_bIxFdDZ", "cdate": 1663849990618, "mdate": null, "content": {"title": "Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias", "abstract": "Gradient regularization (GR) is a method that penalizes the gradient norm of the training loss during training. Although some studies have reported that GR improves generalization performance in deep learning, little attention has been paid to it from the algorithmic perspective, that is, the algorithms of GR that efficiently improve performance. In this study, we first reveal that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational cost for GR. In addition, this computation empirically achieves better generalization performance. Next, we theoretically analyze a solvable model, a diagonal linear network, and clarify that GR has a desirable implicit bias. Learning with GR chooses better minima in a certain problem, and the finite-difference GR chooses even better ones as the ascent step size becomes larger. Finally, we demonstrate that finite-difference GR is closely related to some other algorithms based on iterative ascent and descent steps for exploring flat minima: sharpness-aware minimization and the flooding method. In particular, we reveal that flooding performs finite-difference GR in an implicit way. Thus, this work broadens our understanding of GR in both practice and theory."}}
{"id": "p0sMj8oH2O", "cdate": 1662812648821, "mdate": null, "content": {"title": "Neural Graph Databases", "abstract": "Graph databases (GDBs) enable processing and analysis of unstructured, complex,\nrich, and usually vast graph datasets. Despite the large significance of GDBs\nin both academia and industry, little effort has been made into integrating\nthem with the predictive power of graph neural networks (GNNs).  In this work,\nwe show how to seamlessly combine nearly any GNN model with the computational\ncapabilities of GDBs. For this, we observe that the majority of these systems\nare based on a graph data model called the Labeled Property Graph (LPG), where\nvertices and edges can have arbitrarily complex sets of labels and properties.\nWe then develop LPG2vec, an encoder that transforms an arbitrary LPG dataset\ninto a representation that can be directly used with a broad class of GNNs,\nincluding convolutional, attentional, message-passing, and even higher-order or\nspectral models.  In our evaluation, we show that the rich information\nrepresented as LPG labels and properties is properly preserved by LPG2vec, and\nit increases the accuracy of predictions regardless of the targeted learning\ntask or the used GNN model, by up to 34% compared to graphs with no LPG\nlabels/properties.  In general, LPG2vec enables combining predictive power of\nthe most powerful GNNs with the full scope of information encoded in the LPG\nmodel, paving the way for neural graph databases, a class of systems where the\nvast complexity of maintained data will benefit from modern and future graph\nmachine learning methods."}}
{"id": "n2TXffiKEz", "cdate": 1577836800000, "mdate": null, "content": {"title": "Understanding Approximate Fisher Information for Fast Convergence of Natural Gradient Descent in Wide Neural Networks", "abstract": "Natural Gradient Descent (NGD) helps to accelerate the convergence of gradient descent dynamics, but it requires approximations in large-scale deep neural networks because of its high computational cost. Empirical studies have confirmed that some NGD methods with approximate Fisher information converge sufficiently fast in practice. Nevertheless, it remains unclear from the theoretical perspective why and under what conditions such heuristic approximations work well. In this work, we reveal that, under specific conditions, NGD with approximate Fisher information achieves the same fast convergence to global minima as exact NGD. We consider deep neural networks in the infinite-width limit, and analyze the asymptotic training dynamics of NGD in function space via the neural tangent kernel. In the function space, the training dynamics with the approximate Fisher information are identical to those with the exact Fisher information, and they converge quickly. The fast convergence holds in layer-wise approximations; for instance, in block diagonal approximation where each block corresponds to a layer as well as in block tri-diagonal and K-FAC approximations. We also find that a unit-wise approximation achieves the same fast convergence under some assumptions. All of these different approximations have an isotropic gradient in the function space, and this plays a fundamental role in achieving the same convergence properties in training. Thus, the current study gives a novel and unified theoretical foundation with which to understand NGD methods in deep learning."}}
{"id": "WanFoNRE9el", "cdate": 1577836800000, "mdate": null, "content": {"title": "Rich Information is Affordable: A Systematic Performance Analysis of Second-order Optimization Using K-FAC", "abstract": "Rich information matrices from first and second-order derivatives have many potential applications in both theoretical and practical problems in deep learning. However, computing these information matrices is extremely expensive and this enormous cost is currently limiting its application to important problems regarding generalization, hyperparameter tuning, and optimization of deep neural networks. One of the most challenging use cases of information matrices is their use as a preconditioner for the optimizers, since the information matrices need to be updated every step. In this work, we conduct a step-by-step performance analysis when computing the Fisher information matrix during training of ResNet-50 on ImageNet, and show that the overhead can be reduced to the same amount as the cost of performing a single SGD step. We also show that the resulting Fisher preconditioned optimizer can converge in 1/3 the number of epochs compared to SGD, while achieving the same Top-1 validation accuracy. This is the first work to achieve such accuracy with K-FAC while reducing the training time to match that of SGD."}}
{"id": "AebEJ4tO70B", "cdate": 1577836800000, "mdate": null, "content": {"title": "Scalable and Practical Natural Gradient for Large-Scale Deep Learning", "abstract": "Large-scale distributed training of deep neural networks results in models with worse generalization performance as a result of the increase in the effective mini-batch size. Previous approaches attempt to address this problem by varying the learning rate and batch size over epochs and layers, or ad hoc modifications of batch normalization. We propose Scalable and Practical Natural Gradient Descent (SP-NGD), a principled approach for training models that allows them to attain similar generalization performance to models trained with first-order optimization methods, but with accelerated convergence. Furthermore, SP-NGD scales to large mini-batch sizes with a negligible computational overhead as compared to first-order methods. We evaluated SP-NGD on a benchmark task where highly optimized first-order methods are available as references: training a ResNet-50 model for image classification on ImageNet. We demonstrate convergence to a top-1 validation accuracy of 75.4% in 5.5 minutes using a mini-batch size of 32,768 with 1,024 GPUs, as well as an accuracy of 74.9% with an extremely large mini-batch size of 131,072 in 873 steps of SP-NGD."}}
{"id": "SygcdNrgUS", "cdate": 1567802482006, "mdate": null, "content": {"title": "Practical Deep Learning with Bayesian Principles", "abstract": "Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated and uncertainties on out-of-distribution data are improved. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation will be available as a plug-and-play optimiser."}}
{"id": "rZ6xryH_lOI", "cdate": 1546300800000, "mdate": null, "content": {"title": "Performance Optimizations and Analysis of Distributed Deep Learning with Approximated Second-Order Optimization Method", "abstract": "Faster training of deep neural networks is desired to speed up the research and development cycle in deep learning. Distributed deep learning and second-order optimization methods are two different techniques to accelerate the training of deep neural networks. In the previous work, researchers show that an approximated second-order optimization method, called K-FAC, can mitigate each other drawbacks of the two techniques. However, there was no detailed discussion on the performance, which is critical for the usage in practice. In this work, we propose several performance optimization techniques to reduce the overheads of K-FAC and to accelerate the overall training. Applying all performance optimizations, we are able to speed up the training 1.64 times per iteration compared to a baseline. Additional to the performance optimizations, we construct a simple performance model to predict model training performance to help the users to determine whether distributed K-FAC is appropriate or not for their training in terms of wall-time."}}
{"id": "KFNvRWo-60D", "cdate": 1514764800000, "mdate": null, "content": {"title": "Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs", "abstract": "Large-scale distributed training of deep neural networks suffer from the generalization gap caused by the increase in the effective mini-batch size. Previous approaches try to solve this problem by varying the learning rate and batch size over epochs and layers, or some ad hoc modification of the batch normalization. We propose an alternative approach using a second-order optimization method that shows similar generalization capability to first-order methods, but converges faster and can handle larger mini-batches. To test our method on a benchmark where highly optimized first-order methods are available as references, we train ResNet-50 on ImageNet. We converged to 75% Top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75% even with a mini-batch size of 131,072, which took only 978 iterations."}}
