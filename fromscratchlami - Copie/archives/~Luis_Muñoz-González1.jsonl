{"id": "3zATGVZZR4", "cdate": 1685577600000, "mdate": 1696238118679, "content": {"title": "SparSFA: Towards robust and communication-efficient peer-to-peer federated learning", "abstract": ""}}
{"id": "nbtV6wuDPi", "cdate": 1680307200000, "mdate": 1681671347765, "content": {"title": "Redundancy Planning for Cost Efficient Resilience to Cyber Attacks", "abstract": "We investigate the extent to which redundancy (including with diversity) can help mitigate the impact of cyber attacks that aim to reduce system performance. Using analytical techniques, we estimate impacts, in terms of monetary costs, of penalties from breaching Service Level Agreements (SLAs), and find optimal resource allocations to minimize the overall costs arising from attacks. Our approach combines attack impact analysis, based on performance modeling using queueing networks, with an attack model based on attack graphs. We evaluate our approach using a case study of a website, and show how resource redundancy and diversity can improve the resilience of a system by reducing the likelihood of a fully disruptive attack. We find that the cost-effectiveness of redundancy depends on the SLA terms, the probability of attack detection, the time to recover, and the cost of maintenance. In our case study, redundancy with diversity achieved a saving of up to around 50 percent in expected attack costs relative to no redundancy. The overall benefit over time depends on how the saving during attacks compares to the added maintenance costs due to redundancy."}}
{"id": "cOg8k885dUp", "cdate": 1672531200000, "mdate": 1696238118686, "content": {"title": "Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization", "abstract": "Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to deliberately degrade the algorithms' performance. Optimal attacks can be formulated as bilevel optimization problems and help to assess their robustness in worst-case scenarios. We show that current approaches, which typically assume that hyperparameters remain constant, lead to an overly pessimistic view of the algorithms' robustness and of the impact of regularization. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters and models the attack as a multiobjective bilevel optimization problem. This allows to formulate optimal attacks, learn hyperparameters and evaluate robustness under worst-case conditions. We apply this attack formulation to several ML classifiers using $L_2$ and $L_1$ regularization. Our evaluation on multiple datasets confirms the limitations of previous strategies and evidences the benefits of using $L_2$ and $L_1$ regularization to dampen the effect of poisoning attacks."}}
{"id": "jjn7LCv9TsK", "cdate": 1640995200000, "mdate": 1681671347795, "content": {"title": "Privacy-Preserving Technologies for Trusted Data Spaces", "abstract": "The quality of a machine learning model depends on the volume of data used during the training process. To prevent low accuracy models, one needs to generate more training data or add external data sources of the same kind. If the first option is not feasible, the second one requires the adoption of a federated learning approach, where different devices can collaboratively learn a shared prediction model. However, access to data can be hindered by privacy restrictions. Training machine learning algorithms using data collected from different data providers while mitigating privacy concerns is a challenging problem. In this chapter, we first introduce the general approach of federated machine learning and the H2020 MUSKETEER project, which aims to create a federated, privacy-preserving machine learning Industrial Data Platform. Then, we describe the Privacy Operations Modes designed in MUSKETEER as an answer for more privacy before looking at the platform and its operation using these different Privacy Operations Modes. We eventually present an efficiency assessment of the federated approach using the MUSKETEER platform. This chapter concludes with the description of a real use case of MUSKETEER in the manufacturing domain."}}
{"id": "CCxPzYVLqR", "cdate": 1640995200000, "mdate": 1681671347779, "content": {"title": "HA-Grid: Security Aware Hazard Analysis for Smart Grids", "abstract": "Attacks targeting smart grid infrastructures can result in the disruptions of power supply as well as damages to costly equipment, with significant impact on safety as well as on end-consumers. It is therefore of essence to identify attack paths in the infrastructure that lead to safety violations and to determine critical components that must be protected. In this paper, we introduce a methodology (HA-Grid) that incorporates both safety and security modelling of smart grid infrastructure to analyse the impact of cyber threats on the safety of smart grid infrastructures. HA-Grid is applied on a smart grid test-bed to identify attack paths that lead to safety hazards, and to determine the common nodes in these attack paths as critical components that must be protected."}}
{"id": "A1OU67GJ7bV", "cdate": 1640995200000, "mdate": 1681671347810, "content": {"title": "Security and Robustness in Federated Learning", "abstract": "Federated learning (FL) has emerged as a powerful approach to decentralize the training of machine learning algorithms, allowing the training of collaborative models while preserving the privacy of the datasets provided by different parties. Despite the benefits, FL is also vulnerable to adversaries, similar to other machine learning (ML) algorithms in centralized settings. For example, just a single malicious or faulty participant in an FL task can entirely compromise the performance of the model when using unsecure implementations. In this chapter, we provide a comprehensive analysis of the vulnerabilities of FL algorithms to different attacks that can compromise their performance. We describe a taxonomy of attacks comparing the similarities and differences with respect to centralized ML algorithms. Then, we describe and analyze different families of existing defenses that can be applied to mitigate these threats. Finally, we review a set of comprehensive attacks that aim to compromise the performance and convergence of FL."}}
{"id": "i63pMoDi1Z", "cdate": 1621781418474, "mdate": null, "content": {"title": "Universal Adversarial Perturbations for Malware", "abstract": "Machine learning classification models are vulnerable to\nadversarial examples\u2014effective input-specific perturbations\nthat can manipulate the model\u2019s output. Universal Adversarial Perturbations (UAPs), which identify noisy patterns\nthat generalize across the input space, allow the attacker to\ngreatly scale up the generation of these adversarial examples.\nAlthough UAPs have been explored in application domains\nbeyond computer vision, little is known about their properties\nand implications in the specific context of realizable attacks,\nsuch as malware, where attackers must reason about satisfying\nchallenging problem-space constraints.\n\nIn this paper, we explore the challenges and strengths of\nUAPs in the context of malware classification. We generate sequences \nof problem-space transformations that induce\nUAPs in the corresponding feature-space embedding and evaluate \ntheir effectiveness across threat models that consider a\nvarying degree of realistic attacker knowledge. Additionally, \nwe propose adversarial training-based mitigations using\nknowledge derived from the problem-space transformations,\nand compare against alternative feature-space defenses. Our\nexperiments limit the effectiveness of a white box Android\nevasion attack to ~20% at the cost of ~3% TPR at 1% FPR.\n\nWe additionally show how our method can be adapted to more\nrestrictive application domains such as Windows malware.\nWe observe that while adversarial training in the feature\nspace must deal with large and often unconstrained regions,\nUAPs in the problem space identify specific vulnerabilities\nthat allow us to harden a classifier more effectively, shifting\nthe challenges and associated cost of identifying new universal\nadversarial transformations back to the attacker."}}
{"id": "ybPiSDw8h8", "cdate": 1609459200000, "mdate": 1681671347821, "content": {"title": "Shadow-Catcher: Looking into Shadows to Detect Ghost Objects in Autonomous Vehicle 3D Sensing", "abstract": "LiDAR-driven 3D sensing allows new generations of vehicles to achieve advanced levels of situation awareness. However, recent works have demonstrated that physical adversaries can spoof LiDAR return signals and deceive 3D object detectors to erroneously detect \u201cghost\" objects. Existing defenses are either impractical or focus only on vehicles. Unfortunately, it is easier to spoof smaller objects such as pedestrians and cyclists, but harder to defend against and can have worse safety implications. To address this gap, we introduce Shadow-Catcher, a set of new techniques embodied in an end-to-end prototype to detect both large and small ghost object attacks on 3D detectors. We characterize a new semantically meaningful physical invariant (3D shadows) which Shadow-Catcher leverages for validating objects. Our evaluation on the KITTI dataset shows that Shadow-Catcher consistently achieves more than 94% accuracy in identifying anomalous shadows for vehicles, pedestrians, and cyclists, while it remains robust to a novel class of strong \u201cinvalidation\u201d attacks targeting the defense system. Shadow-Catcher can achieve real-time detection, requiring only between 0.003\u00a0s\u20130.021\u00a0s on average to process an object in a 3D point cloud on commodity hardware and achieves a 2.17x speedup compared to prior work."}}
{"id": "k9qkMEkJI72", "cdate": 1609459200000, "mdate": 1681671347837, "content": {"title": "Universal Adversarial Robustness of Texture and Shape-Biased Models", "abstract": "Increasing shape-bias in deep neural networks has been shown to improve robustness to common corruptions and noise. In this paper we analyze the adversarial robustness of texture and shape-biased models to Universal Adversarial Perturbations (UAPs). We use UAPs to evaluate the robustness of DNN models with varying degrees of shape-based training. We find that shape-biased models do not markedly improve adversarial robustness, and we show that ensembles of texture and shape-biased models can improve universal adversarial robustness while maintaining strong performance."}}
{"id": "UKcDo4ZDkB", "cdate": 1609459200000, "mdate": 1681671347875, "content": {"title": "FedRAD: Federated Robust Adaptive Distillation", "abstract": "The robustness of federated learning (FL) is vital for the distributed training of an accurate global model that is shared among large number of clients. The collaborative learning framework by typically aggregating model updates is vulnerable to model poisoning attacks from adversarial clients. Since the shared information between the global server and participants are only limited to model parameters, it is challenging to detect bad model updates. Moreover, real-world datasets are usually heterogeneous and not independent and identically distributed (Non-IID) among participants, which makes the design of such robust FL pipeline more difficult. In this work, we propose a novel robust aggregation method, Federated Robust Adaptive Distillation (FedRAD), to detect adversaries and robustly aggregate local models based on properties of the median statistic, and then performing an adapted version of ensemble Knowledge Distillation. We run extensive experiments to evaluate the proposed method against recently published works. The results show that FedRAD outperforms all other aggregators in the presence of adversaries, as well as in heterogeneous data distributions."}}
