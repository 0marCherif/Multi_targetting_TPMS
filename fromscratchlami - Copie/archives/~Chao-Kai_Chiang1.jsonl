{"id": "KGee1aOL7jt", "cdate": 1483228800000, "mdate": null, "content": {"title": "Online Learning Problems against Dynamic Strategies in Gradually Evolving Worlds", "abstract": ""}}
{"id": "HyV85dbu-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "Federated Multi-Task Learning", "abstract": "Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets."}}
{"id": "Xigu1SLnSpc", "cdate": 1451606400000, "mdate": null, "content": {"title": "An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits", "abstract": "We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is O(K\\sqrtn \\log n) and against ..."}}
{"id": "S7heFOXO6mJ", "cdate": 1451606400000, "mdate": null, "content": {"title": "Pareto Front Identification from Stochastic Bandit Feedback", "abstract": "We consider the problem of identifying the Pareto front for multiple objectives from a finite set of operating points. Sampling an operating point gives a random vector where each coordinate corres..."}}
{"id": "8fMHc_I4exB", "cdate": 1451606400000, "mdate": null, "content": {"title": "An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits", "abstract": "We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is $O(K\\sqrt{n \\log n})$ and against stochastic bandits the pseudo-regret is $O(\\sum_i (\\log n)/\\Delta_i)$. We also show that no algorithm with $O(\\log n)$ pseudo-regret against stochastic bandits can achieve $\\tilde{O}(\\sqrt{n})$ expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show $\\tilde{O}(\\sqrt{n})$ expected adversarial regret with $O((\\log n)^2)$ stochastic pseudo-regret."}}
{"id": "JUR02r97k5Q", "cdate": 1420070400000, "mdate": null, "content": {"title": "Resisting Dynamic Strategies in Gradually Evolving Worlds", "abstract": "We study the online linear optimization problem, in which a player has to make repeated online decisions with linear loss functions and hopes to achieve a small regret. We consider a natural restriction of this problem in which the loss functions have a small deviation, measured by the sum of the distances between every two consecutive loss functions. At the same time, we also consider a natural generalization, in which the regret is measured against a dynamic offline algorithm which can play different strategies in different rounds, but under the constraint that their deviation is small. We show that in this new setting, an online algorithm modified from the gradient descent algorithm can still achieve a small regret, which can be characterized in terms of the deviation of loss functions and the deviation of the offline algorithm. For the closely related online decision problem, we show that an online algorithm modified from the Hedge algorithm can also achieve a small regret in this new setting."}}
{"id": "lECchagyDQ", "cdate": 1388534400000, "mdate": null, "content": {"title": "Pseudo-reward Algorithms for Contextual Bandits with Linear Payoff Functions", "abstract": "We study the contextual bandit problem with linear payoff functions, which is a generalization of the traditional multi-armed bandit problem. In the contextual bandit problem, the learner needs to ..."}}
{"id": "696AgS3jwjQ", "cdate": 1356998400000, "mdate": null, "content": {"title": "Beating Bandits in Gradually Evolving Worlds", "abstract": "Consider the online convex optimization problem, in which a player has to choose actions iteratively and suffers corresponding losses according to some convex loss functions, and the goal is to min..."}}
{"id": "T0OllPPH7c", "cdate": 1325376000000, "mdate": null, "content": {"title": "Online Optimization with Gradual Variations", "abstract": ""}}
{"id": "sqpapJR1-v8", "cdate": 1262304000000, "mdate": null, "content": {"title": "Online Learning with Queries", "abstract": "The online learning problem requires a player to iteratively choose an action in an unknown and changing environment. In the standard setting of this problem, the player has to choose an action in each round before knowing anything about the corresponding loss. However, there are situations in which it seems possible for the player to spend efforts or resources to collect some prior information before her actions. This motivates us to study a variant of the online learning problem, in which the player is allowed to query B bits from the loss vector in each round before choosing her action. Suppose each loss value is represented by K bits and distinct loss values differ by at least some amount \u03b4, and suppose there are N actions to choose and T rounds to play. We provide an algorithm for this problem which achieves a regret of the following form. Before B approaching B1 = NK/2, the regret stays at , and after B exceeding B1 but before approaching B2 = NK/2 + 3K/2\u20131, the regret drops slightly to , while after B exceeding B2, the regret takes a dramatic drop to (N ln N)/\u03b4. Our algorithm is in fact close to optimal as we also provide regret lower bounds which almost match the regret upper bounds achieved by our algorithm."}}
