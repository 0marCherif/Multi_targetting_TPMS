{"id": "1j47BxncxKY", "cdate": 1687903933193, "mdate": null, "content": {"title": "A comparison of online automatic speech recognition systems", "abstract": "High-quality transcription systems are required for conversational\nanalysis systems. We compared two manual transcribers with five automatic transcription systems using video-conferences from a medical domain and found that\n(1) manual transcriptions significantly outperformed the automatic services, and\n(2) the automatic transcription of YouTube Captions significantly outperformed the other ASR services"}}
{"id": "cwrqI-qt1q-", "cdate": 1629976122644, "mdate": 1629976122644, "content": {"title": "COVID-19 detection from audio: seven grains of salt", "abstract": "Digital mass testing for COVID-19 via a mobile phone application may be possible through machine learning\u2019s (ML) ability to identify patterns in data. COVID-19  appears to confer unique features in the audio produced by infected individuals and ML COVID-19 detection from breath, cough and speech audio recordings has yielded promising results. In this critique, we present seven major issues with this research and argue that further investigation is needed before conclusions about the detectability of COVID-19 from audio can be made. Many of these issues relate to a single question: are the learnt audio representations, which correlate with COVID-19 in the various collected datasets, truly audio biomarkers caused by COVID-19?"}}
{"id": "x20Xi9DBfwr", "cdate": 1620493835175, "mdate": null, "content": {"title": "A survey of multimodal sentiment analysis", "abstract": "Sentiment analysis aims to automatically uncover the underlying attitude that we hold towards an entity. The aggregation of these sentiments over a population represents opinion polling and has numerous applications. Current text-based sentiment analysis relies on the construction of dictionaries and machine learning models that learn sentiment from large text corpora. Sentiment analysis from text is currently widely used for customer satisfaction assessment and brand perception analysis, among others. With the proliferation of social media, multimodal sentiment analysis is set to bring new opportunities with the arrival of complementary data streams for improving and going beyond text-based sentiment analysis. Since sentiment can be detected through affective traces it leaves, such as facial and vocal displays, multimodal sentiment analysis offers promising avenues for analyzing facial and vocal expressions in addition to the transcript or textual content. These approaches leverage emotion recognition and context inference to determine the underlying polarity and scope of an individual\u2019s sentiment. In this survey, we define sentiment and the problem of multimodal sentiment analysis and review recent developments in multimodal sentiment analysis in different domains, including spoken reviews, images, video blogs, human\u2013machine and human\u2013human interactions. Challenges and opportunities of this emerging field are also discussed, leading to our thesis that multimodal sentiment analysis holds a significant untapped potential."}}
{"id": "roxev5pGgdpH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Personalized Estimation of Engagement From Videos Using Active Learning With Deep Reinforcement Learning.", "abstract": "Perceiving users' engagement accurately is important for technologies that need to respond to learners in a natural and intelligent way. In this paper, we address the problem of automated estimation of engagement from videos of child-robot interactions recorded in unconstrained environments (kindergartens). This is challenging due to diverse and person-specific styles of engagement expressions through facial and body gestures, as well as because of illumination changes, partial occlusion, and a changing background in the classroom as each child is active. To tackle these difficult challenges, we propose a novel deep reinforcement learning architecture for active learning and estimation of engagement from video data. The key to our approach is the learning of a personalized policy that enables the model to decide whether to estimate the child's engagement level (low, medium, high) or, when uncertain, to query a human for a video label. Queried videos are labeled by a human expert in an offline manner, and used to personalize the policy and engagement classifier to a target child over time. We show on a database of 43 children involved in robot-assisted learning activities (8 sessions over 3 months), that this combined human-AI approach can easily adapt its interpretations of engagement to the target child using only a handful of labeled videos, while being robust to the many complex influences on the data. The results show large improvements over a non-personalized approach and over traditional active learning methods."}}
{"id": "r1BRfhiab", "cdate": 1518730190624, "mdate": null, "content": {"title": "The Principle of Logit Separation", "abstract": "We consider neural network training, in applications in which there are many possible classes, but at test-time, the task is to identify only whether the given example belongs to a specific class, which can be different in different applications of the classifier. For instance, this is the case in an image search engine. We consider the Single Logit Classification (SLC) task: training the network so that at test-time, it would be possible to accurately identify if the example belongs to a given class, based only on the output logit for this class. \nWe propose a natural principle, the Principle of Logit Separation, as a guideline for choosing and designing losses suitable for the SLC. \nWe show that the cross-entropy loss function is not aligned with the Principle of Logit Separation. In contrast, there are known loss functions, as well as novel batch loss functions that we propose, which are aligned with this principle. In total, we study seven loss functions. \nOur experiments show that indeed in almost all cases, losses that are aligned with Principle of Logit Separation obtain a 20%-35% relative performance improvement in the SLC task, compared to losses that are not aligned with it. We therefore conclude that the Principle of Logit Separation sheds light on an important property of the most common loss functions used by neural network classifiers. "}}
{"id": "SjLqeVBgdaS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep Canonical Time Warping for Simultaneous Alignment and Representation Learning of Sequences.", "abstract": "Machine learning algorithms for the analysis of time-series often depend on the assumption that utilised data are temporally aligned. Any temporal discrepancies arising in the data is certain to lead to ill-generalisable models, which in turn fail to correctly capture properties of the task at hand. The temporal alignment of time-series is thus a crucial challenge manifesting in a multitude of applications. Nevertheless, the vast majority of algorithms oriented towards temporal alignment are either applied directly on the observation space or simply utilise linear projections-thus failing to capture complex, hierarchical non-linear representations that may prove beneficial, especially when dealing with multi-modal data (e.g., visual and acoustic information). To this end, we present Deep Canonical Time Warping (DCTW), a method that automatically learns non-linear representations of multiple time-series that are (i) maximally correlated in a shared subspace, and (ii) temporally aligned. Furthermore, we extend DCTW to a supervised setting, where during training, available labels can be utilised towards enhancing the alignment process. By means of experiments on four datasets, we show that the representations learnt significantly outperform state-of-the-art methods in temporal alignment, elegantly handling scenarios with heterogeneous feature sets, such as the temporal alignment of acoustic and visual information."}}
{"id": "BJ4KvHzdZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Affective Image Content Analysis: A Comprehensive Survey", "abstract": "Images can convey rich semantics and induce strong emotions in viewers. Recently, with the explosive growth of visual data, extensive research efforts have been dedicated to affective image content analysis (AICA). In this paper, we review the state-of-the-art methods comprehensively with respect to two main challenges -- affective gap and perception subjectivity. We begin with an introduction to the key emotion representation models that have been widely employed in AICA. Available existing datasets for performing evaluation are briefly described. We then summarize and compare the representative approaches on emotion feature extraction, personalized emotion prediction, and emotion distribution learning. Finally, we discuss some future research directions."}}
{"id": "roiP9qEg_6H", "cdate": 1483228800000, "mdate": null, "content": {"title": "A Deep Matrix Factorization Method for Learning Attribute Representations.", "abstract": "Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies cannot interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants."}}
{"id": "S1N1mCbu-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "Deep Structured Learning for Facial Action Unit Intensity Estimation", "abstract": "We consider the task of automated estimation of facial expression intensity. This involves estimation of multiple output variables (facial action units - AUs) that are structurally dependent. Their structure arises from statistically induced co-occurrence patterns of AU intensity levels. Modeling this structure is critical for improving the estimation performance, however, this performance is bounded by the quality of the input features extracted from face images. The goal of this paper is to model these structures and estimate complex feature representations simultaneously by combining conditional random field (CRF) encoded AU dependencies with deep learning. To this end, we propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables. Our model accounts for ordinal structure in output variables and their non-linear dependencies via copula functions modeled as cliques of a CRF. These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm. We demonstrate the effectiveness of our approach on the task of AU intensity estimation on two benchmark datasets. We show that joint learning of the deep features and the target output structure results in significant performance gains compared to existing structured deep models and deep models for analysis of facial expressions."}}
{"id": "Hk-H6k-dWH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Tunable Sensitivity to Large Errors in Neural Network Training", "abstract": "When humans learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. We propose incorporating this idea of tunable sensitivity for hard examples in neural network learning, using a new generalization of the cross-entropy gradient step, which can be used in place of the gradient in any gradient-based training method. The generalized gradient is parameterized by a value that controls the sensitivity of the training process to harder training examples. We tested our method on several benchmark datasets. We propose, and corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the network. Moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross-entropy gradient learner. We therefore conclude that tunable sensitivity can be helpful for neural network learning."}}
