{"id": "ckqh1fFK4_", "cdate": 1609459200000, "mdate": 1681800801767, "content": {"title": "Reliable Counterfactual Explanations for Autoencoder based Anomalies", "abstract": "Autoencoders have been used successfully for tackling the problem of anomaly detection in an unsupervised setting, and are often known to give better results than traditional approaches such as clustering and subspace-based (linear) methods. A data point is flagged as anomalous by an autoencoder if its reconstruction loss is higher than an appropriate threshold. However, as with other deep learning models, the increased accuracy offered by autoencoders comes at the cost of interpretability. Explaining an autoencoder\u2019s decision to flag a particular data point as an anomaly is greatly important, since a human-friendly explanation would be necessary for a domain expert tasked with evaluating the model\u2019s decisions. We consider the problem of finding counterfactual explanations for autoencoder anomalies, which address the question of what needs to be minimally changed in a given anomalous data point to make it non-anomalous. We present an algorithm that generates a diverse set of proximate counterfactual explanations for a given autoencoder anomaly. We also introduce the notion of reliability of a counterfactual, and present techniques to find reliable counterfactual explanations."}}
{"id": "-BKqZ0xbfv", "cdate": 1577836800000, "mdate": 1626448430345, "content": {"title": "Verifying Individual Fairness in Machine Learning Models", "abstract": "We consider the problem of whether a given decision model, working with structured data, has individual fairness. Following the work of Dwork, a model is individually biased (or unfair) if there is..."}}
