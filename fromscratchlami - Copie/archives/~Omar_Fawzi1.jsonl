{"id": "UDe_F-4EeHd", "cdate": 1621629977603, "mdate": null, "content": {"title": "Sequential Algorithms for Testing Closeness of Distributions", "abstract": "  What advantage do sequential procedures provide over batch algorithms for testing properties of unknown distributions? Focusing on the problem of testing whether two distributions $\\mathcal{D}_1$ and $\\mathcal{D}_2$ on $\\{1,\\dots, n\\}$ are equal or $\\epsilon$-far, we give several answers to this question. We show that for a small alphabet size $n$, there is a sequential algorithm that outperforms any batch algorithm by a factor of at least $4$ in terms sample complexity. For a general alphabet size $n$, we give a sequential algorithm that uses no more samples than its batch counterpart, and possibly fewer if the actual distance between $\\mathcal{D}_1$ and $\\mathcal{D}_2$ is larger than $\\epsilon$. As a corollary, letting $\\epsilon$ go to $0$, we obtain a sequential algorithm for testing closeness (with no a priori bound on the distance between $\\mathcal{D}_1$ and $\\mathcal{D}_2$) with a sample complexity $\\tilde{\\mathcal{O}}(\\frac{n^{2/3}}{TV(\\mathcal{D}_1, \\mathcal{D}_2)^{4/3}})$: this improves over the $\\tilde{\\mathcal{O}}(\\frac{n/\\log n}{TV(\\mathcal{D}_1, \\mathcal{D}_2)^{2} })$ tester of [Daskalakis and Kawase 2017]  and is optimal up to multiplicative constants. We also establish limitations of sequential algorithms for the problem of testing closeness: they can improve the worst case number of samples by at most a constant factor. "}}
{"id": "ByrZyglCb", "cdate": 1518730185480, "mdate": null, "content": {"title": "Robustness of Classifiers to Universal Perturbations: A Geometric Perspective", "abstract": "Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties."}}
{"id": "rkZpZtZd-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Adversarial vulnerability for any classifier", "abstract": "Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets."}}
{"id": "SkbXn2ZO-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "Universal Adversarial Perturbations", "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."}}
