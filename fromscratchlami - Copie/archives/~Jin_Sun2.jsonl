{"id": "Sy-n32bOZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Faces", "abstract": "Lighting estimation from faces is an important task and has applications in many areas such as image editing, intrinsic image decomposition, and image forgery detection. We propose to train a deep Convolutional Neural Network (CNN) to regress lighting parameters from a single face image. Lacking massive ground truth lighting labels for face images in the wild, we use an existing method to estimate lighting parameters, which are treated as ground truth with noise. To alleviate the effect of such noise, we utilize the idea of Generative Adversarial Networks (GAN) and propose a Label Denoising Adversarial Network (LDAN). LDAN makes use of synthetic data with accurate ground truth to help train a deep CNN for lighting regression on real face images. Experiments show that our network outperforms existing methods in producing consistent lighting parameters of different faces under similar lighting conditions. To further evaluate the proposed method, we also apply it to regress object 2D key points where ground truth labels are available. Our experiments demonstrate its effectiveness on this application."}}
{"id": "HkEjmyMdWr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Generating Holistic 3D Scene Abstractions for Text-Based Image Retrieval", "abstract": "Spatial relationships between objects provide important information for text-based image retrieval. As users are more likely to describe a scene from a real world perspective, using 3D spatial relationships rather than 2D relationships that assume a particular viewing direction, one of the main challenges is to infer the 3D structure that bridges images with users text descriptions. However, direct inference of 3D structure from images requires learning from large scale annotated data. Since interactions between objects can be reduced to a limited set of atomic spatial relations in 3D, we study the possibility of inferring 3D structure from a text description rather than an image, applying physical relation models to synthesize holistic 3D abstract object layouts satisfying the spatial constraints present in a textual description. We present a generic framework for retrieving images from a textual description of a scene by matching images with these generated abstract object layouts. Images are ranked by matching object detection outputs (bounding boxes) to 2D layout candidates (also represented by bounding boxes) which are obtained by projecting the 3D scenes with sampled camera directions. We validate our approach using public indoor scene datasets and show that our method outperforms baselines built upon object occurrence histograms and learned 2D pairwise relations."}}
{"id": "ByW8qRWO-H", "cdate": 1483228800000, "mdate": null, "content": {"title": "Seeing What is Not There: Learning Context to Determine Where Objects are Missing", "abstract": "Most of computer vision focuses on what is in an image. We propose to train a standalone object-centric context representation to perform the opposite task: seeing what is not there. Given an image, our context model can predict where objects should exist, even when no object instances are present. Combined with object detection results, we can perform a novel vision task: finding where objects are missing in an image. Our model is based on a convolutional neural network structure. With a specially designed training strategy, the model learns to ignore objects and focus on context only. It is fully convolutional thus highly efficient. Experiments show the effectiveness of the proposed approach in one important accessibility task: finding city street regions where curb ramps are missing, which could help millions of people with mobility disabilities."}}
{"id": "rillE-i8gOar", "cdate": 1356998400000, "mdate": null, "content": {"title": "Scale and Object Aware Image Thumbnailing.", "abstract": "In this paper we study effective approaches to create thumbnails from input images. Since a thumbnail will eventually be presented to and perceived by a human visual system, a thumbnailing algorithm should consider several important issues in the process including thumbnail scale, object completeness and local structure smoothness. To address these issues, we propose a new thumbnailing framework named scale and object aware thumbnailing (SOAT), which contains two components focusing respectively on saliency measure and thumbnail warping/cropping. The first component, named scale and object aware saliency (SOAS), models the human perception of thumbnails using visual acuity theory, which takes thumbnail scale into consideration. In addition, the \u201cobjectness\u201d measurement (Alexe et al. 2012) is integrated in SOAS, as to preserve object completeness. The second component uses SOAS to guide the thumbnailing based on either retargeting or cropping. The retargeting version uses the thin-plate-spline (TPS) warping for preserving structure smoothness. An extended seam carving algorithm is developed to sample control points used for TPS model estimation. The cropping version searches a cropping window that balances the spatial efficiency and SOAS-based content preservation. The proposed algorithms were evaluated in three experiments: a quantitative user study to evaluate thumbnail browsing efficiency, a quantitative user study for subject preference, and a qualitative study on the RetargetMe dataset. In all studies, SOAT demonstrated promising performances in comparison with state-of-the-art algorithms."}}
{"id": "r1EcLbGd-S", "cdate": 1293840000000, "mdate": null, "content": {"title": "Scale and object aware image retargeting for thumbnail browsing", "abstract": "Many image retargeting algorithms, despite aesthetically carving images smaller, pay limited attention to image browsing tasks where tiny thumbnails are presented. When applying traditional retargeting methods for generating thumbnails, several important issues frequently arise, including thumbnail scales, object completeness and local structure smoothness. To address these issues, we propose a novel image retargeting algorithm, Scale and Object Aware Retargeting (SOAR), which has four components: (1) a scale dependent saliency map to integrate size information of thumbnails, (2) objectness (Alexe et al. 2010) for preserving object completeness, (3) a cyclic seam carving algorithm to guide continuous retarget warping, and (4) a thin-plate-spline (TPS) retarget warping algorithm that champions local structure smoothness. The effectiveness of the proposed algorithm is evaluated both quantitatively and qualitatively. The quantitative evaluation is conducted through an image browsing user study to measure the effectiveness of different thumbnail generating algorithms, followed by the ANOVA analysis. The qualitative study is performed on the RetargetMe benchmark dataset. In both studies, SOAR generates very promising performance, in comparison with state-of-the-art retargeting algorithms."}}
