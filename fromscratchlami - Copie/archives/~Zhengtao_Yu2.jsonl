{"id": "jU25bRxqlG", "cdate": 1708177602961, "mdate": 1708177602961, "content": {"title": "A Cross-Lingual Summarization method based on cross-lingual Fact-relationship Graph Generation", "abstract": "The aim of cross-lingual summarization (CLS) is to condense the content of a document in one language into a summary in another language. In essence, a CLS model requires both translation and summarization capabilities, which presents a unique challenge, as the model must effectively tackle the difficulties associated with both tasks simultaneously (e.g., semantic alignment, information compression and factual inconsistency). Graph-based semantic representation can model important text information in a structured manner, which may alleviate these challenges. Therefore, in this paper, we propose a Cross-Lingual Summarization method based on cross-lingual Fact-relationship Graph Generation (FGGCLS). Specifically, we first construct fact-relationship graphs for source language documents and target language summaries. Then, we introduce a cross-lingual fact-relationship graph generation method, which converts the CLS problem into a cross-lingual fact-relationship graph generation problem. This approach simplifies semantic alignment and information compression through the generation of graphs and leads to improved fact consistency. Finally, the generated fact-relationship graph of the target language summary serves as a draft for generating the summary, which enhances the quality of the generated summary. We conduct systematic experiments on the Zh2EnSum and En2ZhSum datasets, and the results demonstrate that our method can effectively improve the performance of CLS and alleviate factual inconsistency."}}
{"id": "l9QWAJJu2dd", "cdate": 1668659333788, "mdate": 1668659333788, "content": {"title": "Dual-stream Reciprocal Disentanglement Learning for domain adaptation person re-identification", "abstract": "Since human-labeled samples are free for the target set, unsupervised person re-identification (Re-ID) has attracted much attention in recent years, by additionally exploiting the source set. However, due to the differences on camera styles, illumination and backgrounds, there exists a large gap between source domain and target domain, introducing a great challenge on cross-domain matching. To tackle this problem, in this paper we propose a novel method named Dual-stream Reciprocal Disentanglement Learning (DRDL), which is quite efficient in learning domain-invariant features. In DRDL, two encoders are first constructed for id-related and id-unrelated feature extractions, which are respectively measured by their associated classifiers. Furthermore, followed by an adversarial learning strategy, both streams reciprocally and positively effect each other, so that the id-related features and id-unrelated features are completely disentangled from a given image, allowing the encoder to be powerful enough to obtain the discriminative but domain-invariant features. In contrast to existing approaches, our proposed method is free from image generation, which not only reduces the computational complexity remarkably, but also removes redundant information from id-related features. Extensive experiments substantiate the superiority of our proposed method compared with the state-of-the-arts. The source code has been released in https://github.com/lhf12278/DRDL."}}
{"id": "lNQAXpf7rGu", "cdate": 1663850073896, "mdate": null, "content": {"title": "The Progressive Alignment-aware Multimodal Fusion with Easy2hard Strategy for Multimodal Neural Machine Translation", "abstract": "Multimodal neural machine translation (MNMT) aims to improve textual level machine translation performance in the presence of text-related images. Most of the previous works on MNMT have only focused on either multimodal feature fusion or noise multi-modal representations based on full visual and textual features, however, the degree of multi-modal alignment is often ignored. Generally, the fine-grained multi-modal information, such as visual object, textual entity, is easy to align, but the global-level semantic alignment is always difficult. In order to alleviate the challenging problem of multi-modal alignment, this paper proposes a novel progressive multimodal fusion approach with the easy-to-hard (easy2hard) cross-model alignment strategy by fully exploiting visual information for MNMT. We first extract both visual and textual features with modal-specific pre-trained models, respectively, and the fine-grained features (e.g., the regional visual features, the entity features) are roughly aligned as multi-modal anchors based on cross-modal interactive module. Then a progressive multi-modal fusion framework is employed for MNMT by gradually narrowing the global-level multi-modal semantic gap based on the roughly aligned anchors. We validate our method on the Multi30K dataset. The experimental results show the superiority of our proposed model, and achieve the state-of-the-art (SOTA) scores in all En-De, En-Fr and En-Cs translation tasks."}}
{"id": "H87NVDpH-fq", "cdate": 1647562076137, "mdate": null, "content": {"title": "Ontology-guided and Text-enhanced Representation for  Knowledge Graph Zero-shot Relational Learning", "abstract": "Knowledge graph embedding (KGE) have been proposed and utilized to knowledge graph completion (KGC), but most KGE methods struggle in unseen relations. Previous studies focus on complete zero-shot relational learning by incorporating text-features and proximity relations, which are difficult to accurately represent the complete semantic of relations. To overcome the above-mentioned issues in zero-shot relation learning, we propose an ontology-guided and text-enhanced representation, which could improve the effect of current KGE for unseen relations. In fact, each KG contain ontology and text descriptions that describe the meta-information of knowledge. To combine text-embedding space and graph-embedding space, we design TR-GCN to obtain the meta-representation of relations based on the ontology structure and their textual descriptions. It will be used directly to guide previous KGE methods such as TransE and RotatE on zero-shot relation learning. The experimental results on multiple public datasets demonstrate that the proposed ontology-guided and text-enhanced representation can enrich KGs embedding, and significantly improves the KGC performance on unseen relations."}}
{"id": "47CX_LRrEEr", "cdate": 1633656965047, "mdate": 1633656965047, "content": {"title": "Element graph-augmented abstractive summarization for legal public opinion news with graph transformer", "abstract": "Automatic summarization for legal public opinion news has been an attractive research problem in recent years. Compared with the open-domain, summarization for legal public opinion news has two essential constraints: (1) the key information (e.g., the case elements) of the news should be summarized; (2) the factual errors should be avoided in the generated summary. To address these challenges, the summarizer should learn a structured representation of the news (event plan), making it better to understand the event information implied in the news. This paper proposes a novel element graph-augmented abstractive summarization model, which first constructs the structural graph by extracting elements from the source document and then produces graph representation via graph transformer network. Finally, the structural representation is taken as an essential complementary component of the conventional sequence-to-sequence model to guide the decoding process simultaneously. Furthermore, the pre-trained language model is introduced to enhance the sequential and structural encoder, which further promotes the summarization model\u2019s performance. For evaluation, we build a large-scale legal public opinion news (LPO-news) corpus. Experimental results on LPO-news and another news-oriented CNN/Daily mail dataset show that our model significantly outperforms other baselines in terms of both ROUGE scores and Bert scores. We also perform a human evaluation to demonstrate our model\u2019s effectiveness by evaluating the generated summary using several subjective metrics."}}
