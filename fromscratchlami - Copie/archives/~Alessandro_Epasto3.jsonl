{"id": "gdPiOBu99Y", "cdate": 1665069631602, "mdate": null, "content": {"title": "Scalable and Improved Algorithms for Individually Fair Clustering", "abstract": "We present scalable and improved algorithms for the individually fair ($p$, $k$)-clustering problem \nintroduced by Jung et al and Mahabadi et al.\nGiven $n$ points $P$ in a metric space, \nlet $\\delta(x)$ for $x\\in P$ be the radius of the smallest ball around $x$ \ncontaining at least $\\nicefrac nk$ points.  \n\nIn this work, we present two main contributions.\nWe first present local-search algorithms improving prior work along cost and maximum fairness violation.\nThen we design a fast local-search algorithm\nthat runs in $\\tO(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. \nFinally we show empirically that not only is our algorithm much faster than prior work, but it also produces lower-cost solutions.\n"}}
{"id": "dzVZGSe0NoJ", "cdate": 1664046166016, "mdate": null, "content": {"title": "Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank", "abstract": "Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of graph representations such as node ranking, labeling, and graph embedding. However, while data privacy is one of the most important recent concerns, existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges: the difference of only one edge may cause a big change in the PPR vector, potentially leaking private user data.\n\nIn this work, we propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. In addition, we prove that our algorithm achieves  similar accuracy to non-private algorithms when the input graph has large degrees. Our sensitivity-bounded PPR directly implies private algorithms for several tools of graph learning, such as, differentially private (DP) PPR ranking, DP node classification, and DP node embedding. To complement our theoretical analysis, we also empirically verify the practical performances of our algorithms.\n"}}
{"id": "Fhty8PgFkDo", "cdate": 1652737816863, "mdate": null, "content": {"title": "Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank", "abstract": "Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of graph representations such as node ranking, labeling, and graph embedding. However, while data privacy is one of the most important recent concerns, existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges: the difference of only one edge may cause a big change in the PPR vector, potentially leaking private user data.\n\nIn this work, we propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. In addition, we prove that our algorithm achieves  similar accuracy to non-private algorithms when the input graph has large degrees. Our sensitivity-bounded PPR directly implies private algorithms for several tools of graph learning, such as, differentially private (DP) PPR ranking, DP node classification, and DP node embedding. To complement our theoretical analysis, we also empirically verify the practical performances of our algorithms.\n"}}
{"id": "9lQmaKMxIUD", "cdate": 1652737595021, "mdate": null, "content": {"title": "Near-Optimal Private and Scalable $k$-Clustering", "abstract": "  We study the differentially private (DP) $k$-means and $k$-median clustering problems of $n$ points in $d$-dimensional Euclidean space in the massively parallel computation (MPC) model. We provide two near-optimal algorithms where the near-optimality is in three aspects: they both achieve (1). $O(1)$ parallel computation rounds, (2). near-linear in $n$ and polynomial in $k$ total computational work (i.e., near-linear running time when $n$ is a sufficient polynomial in $k$), (3). $O(1)$ relative approximation and $\\text{poly}(k, d)$ additive error. Note that $\\Omega(1)$ relative approximation is provably necessary even for any polynomial-time non-private algorithm, and $\\Omega(k)$ additive error is a provable lower bound for any polynomial-time DP $k$-means/median algorithm. Our two algorithms provide a tradeoff between the relative approximation and the additive error: the first has $O(1)$ relative approximation and $\\sim (k^{2.5} + k^{1.01} \\sqrt{d})$ additive error, and the second one achieves $(1+\\gamma)$ relative approximation to the optimal non-private algorithm for an arbitrary small constant $\\gamma>0$ and with $\\text{poly}(k, d)$ additive error for a larger polynomial dependence on $k$ and $d$.\n  \n  To achieve our result, we develop a general framework which partitions the data and reduces the DP clustering problem for the entire dataset to the DP clustering problem for each part. To control the blow-up of the additive error introduced by each part, we develop a novel charging argument which might be of independent interest."}}
{"id": "9t6OZi-PrR", "cdate": 1622132518036, "mdate": null, "content": {"title": "Fair Hierarchical Clustering", "abstract": "As machine learning has become more prevalent, researchers have begun to recog-nize the necessity of ensuring machine learning systems are fair. Recently, there hasbeen an interest in defining a notion of fairness that mitigates over-representationin traditional clustering.\n\nIn this paper we extend this notion to hierarchical clustering, where the goal is torecursively partition the data to optimize a specific objective. For various naturalobjectives, we obtain simple, efficient algorithms to find a provably good fairhierarchical clustering. Empirically, we show that our algorithms can find a fairhierarchical clustering, with only a negligible loss in the objective."}}
{"id": "bHa7j8dHFBr", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bisect and Conquer: Hierarchical Clustering via Max-Uncut Bisection", "abstract": "Hierarchical Clustering is an unsupervised data analysis method which has been widely used for decades. Despite its popularity, it had an underdeveloped analytical foundation and to address this, Dasgupta recently introduced an optimization viewpoint of hierarchical clustering with pairwise similarity information that spurred a line of work shedding light on old algorithms (e.g., Average-Linkage), but also designing new algorithms. Here, for the maximization dual of Dasgupta\u2019s objective (introduced by Moseley-Wang), we present polynomial-time 42.46% approximation algorithms that use Max-Uncut Bisection as a subroutine. The previous best worst-case approximation factor in polynomial time was 33.6%, improving only slightly over Average-Linkage which achieves 33.3%. Finally, we complement our positive results by providing APX-hardness (even for 0-1 similarities), under the Small Set Expansion hypothesis."}}
{"id": "TNy6GJ2q8V4", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimal Approximation - Smoothness Tradeoffs for Soft-Max Functions", "abstract": "A soft-max function has two main efficiency measures: (1) approximation - which corresponds to how well it approximates the maximum function, (2) smoothness - which shows how sensitive it is to changes of its input. Our goal is to identify the optimal approximation-smoothness tradeoffs for different measures of approximation and smoothness. This leads to novel soft-max functions, each of which is optimal for a different application. The most commonly used soft-max function, called exponential mechanism, has optimal tradeoff between approximation measured in terms of expected additive approximation and smoothness measured with respect to R\\'enyi Divergence. We introduce a soft-max function, called \"piecewise linear soft-max\", with optimal tradeoff between approximation, measured in terms of worst-case additive approximation and smoothness, measured with respect to $\\ell_q$-norm. The worst-case approximation guarantee of the piecewise linear mechanism enforces sparsity in the output of our soft-max function, a property that is known to be important in Machine Learning applications [Martins et al. '16, Laha et al. '18] and is not satisfied by the exponential mechanism. Moreover, the $\\ell_q$-smoothness is suitable for applications in Mechanism Design and Game Theory where the piecewise linear mechanism outperforms the exponential mechanism. Finally, we investigate another soft-max function, called power mechanism, with optimal tradeoff between expected \\textit{multiplicative} approximation and smoothness with respect to the R\\'enyi Divergence, which provides improved theoretical and practical results in differentially private submodular optimization."}}
{"id": "T5uxZmX55qZ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimal Approximation - Smoothness Tradeoffs for Soft-Max Functions", "abstract": "pre><code>A soft-max function has two main efficiency measures: (1) approximation - which corresponds to how well it approximates the maximum function, (2) smoothness - which shows how sensitive it is to changes of its input. Our goal is to identify the optimal approximation-smoothness tradeoffs for different measures of approximation and smoothness. This leads to novel soft-max functions, each of which is optimal for a different application. The most commonly used soft-max function, called exponential mechanism, has optimal tradeoff between approximation measured in terms of expected additive approximation and smoothness measured with respect to Renyi Divergence. We introduce a soft-max function, called piece-wise linear soft-max, with optimal tradeoff between approximation, measured in terms of worst-case additive approximation and smoothness, measured with respect to l_q-norm. The worst-case approximation guarantee of the piece-wise linear mechanism enforces sparsity in the output of our soft-max function, a property that is known to be important in Machine Learning applications Martins et al. 16, Laha et al. 18 and is not satisfied by the exponential mechanism. Moreover, the l_q-smoothness is suitable for applications in Mechanism Design and Game Theory where the piece-wise linear mechanism outperforms the exponential mechanism. Finally, we investigate another soft-max function, called power mechanism, with optimal tradeoff between expected multiplicative approximation and smoothness with respect to the Renyi Divergence, which provides improved theoretical and practical results in differentially private submodular optimization."}}
{"id": "LSBMprFaTuI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Smoothly Bounding User Contributions in Differential Privacy", "abstract": "A differentially private algorithm guarantees that the input of a single user won\u2019t significantly change the output distribution of the algorithm. When a user contributes more data points, more information can be collected to improve the algorithm\u2019s performance. But at the same time, more noise might need to be added to the algorithm in order to keep the algorithm differentially private and this might hurt the algorithm\u2019s performance. Amin et al. (2019) initiates the study on bounding user contributions and proposes a very natural algorithm which limits the number of samples each user can contribute by a threshold. For a better trade-off between utility and privacy guarantee, we propose a method which smoothly bounds user contributions by setting appropriate weights on data points and apply it to estimating the mean/quantiles, linear regression, and empirical risk minimization. We show that our algorithm provably outperforms the sample limiting algorithm. We conclude with experimental evaluations which validate our theoretical results."}}
{"id": "nHkcttFZu1Z", "cdate": 1546300800000, "mdate": null, "content": {"title": "Clustering without Over-Representation", "abstract": "In this paper we consider clustering problems in which each point is endowed with a color. The goal is to cluster the points to minimize the classical clustering cost but with the additional constraint that no color is over-represented in any cluster. This problem is motivated by practical clustering settings, e.g., in clustering news articles where the color of an article is its source, it is preferable that no single news source dominates any cluster.   For the most general version of this problem, we obtain an algorithm that has provable guarantees of performance; our algorithm is based on finding a fractional solution using a linear program and rounding the solution subsequently. For the special case of the problem where no color has an absolute majority in any cluster, we obtain a simpler combinatorial algorithm also with provable guarantees. Experiments on real-world data shows that our algorithms are effective in finding good clustering without over-representation."}}
