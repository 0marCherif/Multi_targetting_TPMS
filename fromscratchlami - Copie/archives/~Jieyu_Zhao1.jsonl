{"id": "yko59Buew6", "cdate": 1640995200000, "mdate": 1683231234680, "content": {"title": "Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN", "abstract": "Auditing machine learning-based (ML) healthcare tools for bias is critical to preventing patient harm, especially in communities that disproportionately face health inequities. General frameworks are becoming increasingly available to measure ML fairness gaps between groups. However, ML for health (ML4H) auditing principles call for a contextual, patient-centered approach to model assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H auditing principles and (2) able to illuminate and characterize communities vulnerable to the most harm. To address this gap, we propose supplementing ML4H auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs detectioN), an automatic tool for capturing local biases in a clinical prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs detectioN), by contextualizing group bias detection in patient illness severity and past medical history. We investigate and compare SLOGAN's bias detection capabilities to LOGAN and other clustering techniques across patient subgroups in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness disparities in over 75% of patient groups than LOGAN while maintaining clustering quality. Furthermore, in a diabetes case study, health disparity literature corroborates the characterizations of the most biased clusters identified by SLOGAN. Our results contribute to the broader discussion of how machine learning biases may perpetuate existing healthcare disparities."}}
{"id": "wYnSCubo1B5", "cdate": 1640995200000, "mdate": 1683231234745, "content": {"title": "On Measures of Biases and Harms in NLP", "abstract": "Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, Kai-Wei Chang. Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022. 2022."}}
{"id": "v0OlIiAa8uF", "cdate": 1640995200000, "mdate": 1683231234639, "content": {"title": "Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers", "abstract": "Large pre-trained language models have shown remarkable performance over the past few years. These models, however, sometimes learn superficial features from the dataset and cannot generalize to the distributions that are dissimilar to the training scenario. There have been several approaches proposed to reduce model's reliance on these bias features which can improve model robustness in the out-of-distribution setting. However, existing methods usually use a fixed low-capacity model to deal with various bias features, which ignore the learnability of those features. In this paper, we analyze a set of existing bias features and demonstrate there is no single model that works best for all the cases. We further show that by choosing an appropriate bias model, we can obtain a better robustness result than baselines with a more sophisticated model design."}}
{"id": "L369BwTNKfI", "cdate": 1640995200000, "mdate": 1667336800415, "content": {"title": "DisinfoMeme: A Multimodal Dataset for Detecting Meme Intentionally Spreading Out Disinformation", "abstract": "Disinformation has become a serious problem on social media. In particular, given their short format, visual attraction, and humorous nature, memes have a significant advantage in dissemination among online communities, making them an effective vehicle for the spread of disinformation. We present DisinfoMeme to help detect disinformation memes. The dataset contains memes mined from Reddit covering three current topics: the COVID-19 pandemic, the Black Lives Matter movement, and veganism/vegetarianism. The dataset poses multiple unique challenges: limited data and label imbalance, reliance on external knowledge, multimodal reasoning, layout dependency, and noise from OCR. We test multiple widely-used unimodal and multimodal models on this dataset. The experiments show that the room for improvement is still huge for current models."}}
{"id": "6A0FIvh5ox", "cdate": 1640995200000, "mdate": 1683231234648, "content": {"title": "Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers", "abstract": ""}}
{"id": "4ulZGHB760V", "cdate": 1640995200000, "mdate": 1683231234867, "content": {"title": "SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models", "abstract": "A common limitation of diagnostic tests for detecting social biases in NLP models is that they may only detect stereotypic associations that are pre-specified by the designer of the test. Since enumerating all possible problematic associations is infeasible, it is likely these tests fail to detect biases that are present in a model but not pre-specified by the designer. To address this limitation, we propose SODAPOP (SOcial bias Discovery from Answers about PeOPle) in social commonsense question-answering. Our pipeline generates modified instances from the Social IQa dataset (Sap et al., 2019) by (1) substituting names associated with different demographic groups, and (2) generating many distractor answers from a masked language model. By using a social commonsense model to score the generated distractors, we are able to uncover the model's stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations of multiple state-of-the-art debiasing algorithms."}}
{"id": "pB8y9cab4Z", "cdate": 1609459200000, "mdate": 1631298143423, "content": {"title": "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation", "abstract": "Chong Zhang, Jieyu Zhao, Huan Zhang, Kai-Wei Chang, Cho-Jui Hsieh. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "lMDXREZMskW", "cdate": 1609459200000, "mdate": 1683231234634, "content": {"title": "Building Accountable Natural Language Processing Models: on Social Bias Detection and Mitigation", "abstract": "Author(s): Zhao, Jieyu | Advisor(s): Chang, Kai-Wei | Abstract: Natural Language Processing (NLP) plays an important role in many applications, including resume filtering, text analysis, and information retrieval. Despite the remarkable accuracy enabled by the advances of machine learning methods, recent studies show that these techniques also capture and generalize the societal biases in the data.For example, an automatic resume filtering system may unconsciously select candidates based on their gender and race due to implicit associations between applicant names and job titles, causing the societal disparity as indicated in [BCZ16]. Various laws and policies have been designed and created to ensure societal equality and diversity. However, there is a lack of such a mechanism to restrict machine learning models from making bias predictions in sensitive applications. My research goal is to analyze potential stereotypes exhibited in various machine learning models and to develop computational approaches to enhance fairness in a wide range of NLP applications. The broader impact of my research aligns well with the goal of fairness in machine learning -- in recognizing the value of diversity and underrepresented groups."}}
{"id": "l0tmdjyqjZ2", "cdate": 1609459200000, "mdate": 1631298143657, "content": {"title": "Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?", "abstract": "Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way? We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes. Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a question-answering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it. To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions. Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences. Few-shot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization. Our new task thus poses a novel language understanding challenge for the community."}}
{"id": "ID5paIP9iCV", "cdate": 1609459200000, "mdate": 1631298144060, "content": {"title": "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation", "abstract": "Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a \"double perturbation\" framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models' robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack."}}
