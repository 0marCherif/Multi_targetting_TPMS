{"id": "pFl4zhssd58", "cdate": 1514764800000, "mdate": null, "content": {"title": "The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics With Mobile Devices", "abstract": "Scientific advances build on reproducible researches which need publicly available benchmark data sets. The computer vision and speech recognition communities have led the way in establishing benchmark data sets. There are much less data sets available in mobile computing, especially for rich locomotion and transportation analytics. This paper presents a highly versatile and precisely annotated large-scale data set of smartphone sensor data for multimodal locomotion and transportation analytics of mobile users. The data set comprises seven months of measurements, collected from all sensors of four smartphones carried at typical body locations, including the images of a body-worn camera, while three participants used eight different modes of transportation in the south-east of the U.K., including in London. In total, 28 context labels were annotated, including transportation mode, participant's posture, inside/outside location, road conditions, traffic conditions, presence in tunnels, social interactions, and having meals. The total amount of collected data exceed 950 GB of sensor data, which corresponds to 2812 h of labeled data and 17 562 km of traveled distance. We present how we set up the data collection, including the equipment used and the experimental protocol. We discuss the data set, including the data curation process, the analysis of the annotations, and of the sensor data. We discuss the challenges encountered and present the lessons learned and some of the best practices we developed to ensure high quality data collection and annotation. We discuss the potential applications which can be developed using this large-scale data set. In particular, we present how a machine-learning system can use this data set to automatically recognize modes of transportations. Many other research questions related to transportation analytics, activity recognition, radio signal propagation and mobility modeling can be addressed through this data set. The full data set is being made available to the community, and a thorough preview is already published."}}
{"id": "esoGYgA9Zz", "cdate": 1483228800000, "mdate": null, "content": {"title": "High reliability Android application for multidevice multimodal mobile data acquisition and annotation", "abstract": "We have completed the collection of one of the richest accurately annotated mobile dataset of modes of transportation and locomotion. To do this, we developed a highly reliable Android application called DataLogger capable of recording multisensor data from multiple synchronized smartphones simultaneously. The application allows real-time data annotation. We explain how we designed the app to achieve high reliability and ease of use. We also present an evaluation of the application in a big-data collection (750 hours, 950 GB of data, 17 different sensor modalities), analysing the data loss (less than 0.4%) and battery consumption (\u2248 6% on average per hour). The application is available as open source."}}
{"id": "UBONgOPNrex", "cdate": 1483228800000, "mdate": null, "content": {"title": "A Versatile Annotated Dataset for Multimodal Locomotion Analytics with Mobile Devices", "abstract": "We explain how to obtain a highly versatile and precisely annotated dataset for the multimodal locomotion of mobile users. After presenting the experimental setup, data management challenges and potential applications, we conclude with the best practices for assuring data quality and reducing loss. The dataset currently comprises 7 months of measurements, collected by smartphone's sensors and a body-worn camera, while the 3 participants used 8 different modes of transportation. It comprises 950 GB of sensor data, which corresponds to 750 hours of labelled data. The obtained data will be useful for a wide range of research questions related to activity recognition, and will be made available to the community1."}}
{"id": "xIaxlE-p7Fy", "cdate": 1451606400000, "mdate": null, "content": {"title": "Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition", "abstract": "Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4% on average; outperforming some of the previous reported results by up to 9%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters\u2019 influence on performance to provide insights about their optimisation."}}
{"id": "n4tbWqVSL5", "cdate": 1451606400000, "mdate": null, "content": {"title": "Exploring glass as a novel method for hands-free data entry in flexible cystoscopy", "abstract": "We present a way to annotate cystoscopy finding on Google Glass in a reproducible and hands free manner for use by surgeons during operations in the sterile environment inspired by the current practice of hand-drawn sketches. We developed three data entry variants based on speech and head movements. We assessed the feasibility, benefits and drawbacks of the system with 8 surgeons and Foundation Doctors having up to 30 years' cystoscopy experience at a UK hospital in laboratory trials. We report data entry speed and error rate of input modalities and contrast it with the participants' feedback on their perception of usability, acceptance, and suitability for deployment. The results are supportive of new data entry technologies and point out directions for future improvement of eyewear computers. The findings can be generalised to other endoscopic procedures (e.g. OGD/laryngoscopy) and could be included within hospital IT in the future."}}
{"id": "clZMW8MHIrr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Exploring human activity annotation using a privacy preserving 3D model", "abstract": "Annotating activity recognition datasets is a very time consuming process. Using lay annotators (e.g. using crowd-sourcing) has been suggested to speed this up. However, this requires to preserve privacy of users and may preclude relying on video for annotation. We investigate to which extent using a 3D human model animated from the data of inertial sensors placed on the limbs allows for annotation of human activities. We animate the upper body of the 3D model with the data from 5 inertial measurement sensors obtained from the OPPORTUNITY dataset. The animated model is shown to 6 people in a suite of experiments in order to understand to which extent it can be used for labelling. We present 3 experiments where we investigate the use of a 3D model for i) activity segmentation, ii) for \"open-ended\" annotation where users freely describe the activity they see on screen, and iii) traditional annotation, where users pick one activity among a pre-defined list of activities. In the latter case, results show that users recognise the model's activities with 56% accuracy when picking from 11 possible activities."}}
{"id": "ZKIwqb3BaJN", "cdate": 1451606400000, "mdate": null, "content": {"title": "Electric field phase sensing for wearable orientation and localisation applications", "abstract": "We show how to sense the phase of the ambient electric field from a body-worn sensor with respect to a reference and discuss how phase information could contribute to relative orientation sensing and indoor localisation. Our system uses 7mW and can be enclosed in a plastic case which makes it suitable for new wearable devices."}}
{"id": "NFyoYYlawwu", "cdate": 1451606400000, "mdate": null, "content": {"title": "Deep convolutional feature transfer across mobile activity recognition domains, sensor modalities and locations", "abstract": "Kernels in the convolutional layers of deep convolutional networks are believed to act as feature extractors, progressively highlighting more domain-specific features in the upper network layers. Thus lower-level features might be suitable for transfer. We analyse this in wearable activity recognition by reusing kernels learned on a source domain on another target domain. We consider transfer between users, application domains, sensor modalities and sensor locations. We characterize the trade-offs of transferring various convolutional layers along model size, learning speed, recognition performance and training data. Through novel kernel visualisations and comparative evaluations we identify what kernels are predominantly sensitive to, amongst sensor characteristics, motion dynamics and on-body placement. Kernel transfer reduces training time by ~17% without additional complexity. We derive recommendations on when transfer is most suitable."}}
{"id": "FrQxat_vdGM", "cdate": 1451606400000, "mdate": null, "content": {"title": "Beach volleyball serve type recognition", "abstract": "We present results on beach volleyball serve recognition and classification from a wrist-worn gyroscope deployed with semi-professional beach volleyball players. We trained a template-based recognition system based on a Warping Longest Common Subsequence algorithm to spot serves, and potentially distinguish among 4 common serve types. This shows the potential of wearable technologies in beach volleyball, which could offer precise sport analytics."}}
{"id": "4dsJooRajMs", "cdate": 1420070400000, "mdate": null, "content": {"title": "Sensor-based Bayesian detection of anomalous living patterns in a home setting", "abstract": "In this paper, we present an automated behavior analysis system developed to assist the elderly and individuals with disabilities who live alone, by learning and predicting standard behaviors to improve the efficiency of their healthcare. Established behavioral patterns have been recorded using wireless sensor networks composed by several event-based sensors that captured raw measures of the actions of each user. Using these data, behavioral patterns of the residents were extracted using Bayesian statistics. The behavior was statistically estimated based on three probabilistic features we introduce, namely sensor activation likelihood, sensor sequence likelihood, and sensor event duration likelihood. Real data obtained from different home environments were used to verify the proposed method in the individual analysis. The results suggest that the monitoring system can be used to detect anomalous behavior signs which could reflect changes in health status of the user, thus offering an opportunity to intervene if required."}}
