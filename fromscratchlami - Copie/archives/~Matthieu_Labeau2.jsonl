{"id": "XPM_PWwWeeM", "cdate": 1615996188859, "mdate": null, "content": {"title": "Improving Multimodal fusion via Mutual Dependency Maximisation", "abstract": "Multimodal sentiment analysis is a trending area of research, and the multimodal fusion is one of is most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating these different unimodal representations into one synthetic representation. So far, a consequent effort has been made on developing complex architectures allowing the fusion of these modalities. However, such systems are mainly trained by minimising simple losses such as $L1$ or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to $4.3$ on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: \\texttt{CMU-MOSI} and \\texttt{CMU-MOSEI}. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model."}}
{"id": "c1oDhu_hagR", "cdate": 1615996044479, "mdate": null, "content": {"title": "Cross-Lingual Pretraining Methods for Spoken Dialog", "abstract": "There has been an increasing interest among NLP researchers towards learning generic representations. However, in the field of multilingual spoken dialogue systems, this problem remains overlooked. Indeed most of the pre-training methods focus on learning representations for written and non-conversational data or are restricted to the monolingual setting. In this work we (1) generalise existing losses to the multilingual setting, (2) develop a new set of losses to leverage parallel conversations when available.  These losses improve the learning of representations by fostering the deep encoder to better learn contextual dependencies. The pre-training relies on \\texttt{OpenSubtitles}, a huge multilingual corpus that is composed of $24.3$G tokens; a by-product of the pre-processing includes multilingual aligned conversations. We also introduce two new multilingual tasks and a new benchmark on multilingual dialogue act labels called \\texttt{MIAM}. We validate our pre-training on the three aforementioned tasks and show that our model using our newly designed losses achieves better performances than existing models. Our implementation will be available on \\url{github.com} and preprocessed data will be available in Datasets"}}
{"id": "PNbaQ4R3yZ8", "cdate": 1609459200000, "mdate": 1633361409272, "content": {"title": "M\u00e9ta-apprentissage : classification de messages en cat\u00e9gories \u00e9motionnelles inconnues en entra\u00eenement (Meta-learning : Classifying Messages into Unseen Emotional Categories)", "abstract": "Ga\u00ebl Guibon, Matthieu Labeau, H\u00e9l\u00e8ne Flamein, Luce Lefeuvre, Chlo\u00e9 Clavel. Actes de la 28e Conf\u00e9rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf\u00e9rence principale. 2021."}}
{"id": "UzA9uSK6KpN", "cdate": 1577836800000, "mdate": 1633361409319, "content": {"title": "Compositional languages emerge in a neural iterated learning model", "abstract": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents via grounded language learning. Inspired by the iterated learning framework, which simulates the process of language evolution, we propose an effective neural iterated learning algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide specific advantages to neural agents during training, which translates as a larger posterior probability, which is then incrementally amplified via the iterated learning procedure. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalization of the neural agent communication."}}
{"id": "KzqeUIygM_", "cdate": 1577836800000, "mdate": 1633361409316, "content": {"title": "The importance of fillers for text representations of speech transcripts", "abstract": "Tanvi Dinkar, Pierre Colombo, Matthieu Labeau, Chlo\u00e9 Clavel. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "FlbsfcvbdaU", "cdate": 1577836800000, "mdate": 1633361409322, "content": {"title": "Hierarchical Pre-training for Sequence Labelling in Spoken Dialog", "abstract": "Emile Chapuis, Pierre Colombo, Matteo Manica, Matthieu Labeau, Chlo\u00e9 Clavel. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
{"id": "HkePNpVKPB", "cdate": 1569439022930, "mdate": null, "content": {"title": "Compositional languages emerge in a neural iterated learning model", "abstract": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents via grounded language learning. Inspired by the iterated learning framework, which simulates the process of language evolution, we propose an effective neural iterated learning algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide specific advantages to neural agents during training, which translates as a larger posterior probability, which is then incrementally amplified via the iterated learning procedure. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalization of the neural agent communication."}}
{"id": "4v8mEOkyFph", "cdate": 1546300800000, "mdate": 1633361409267, "content": {"title": "Experimenting with Power Divergences for Language Modeling", "abstract": "Matthieu Labeau, Shay B. Cohen. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
{"id": "nFjqlVGrrql", "cdate": 1514764800000, "mdate": 1633361409277, "content": {"title": "Neural language models: Dealing with large vocabularies. (Mod\u00e8les de langue neuronaux: Gestion des grands vocabulaires)", "abstract": "This work investigates practical methods to ease training and improve performances of neural language models with large vocabularies. The main limitation of neural language models is their expensive computational cost: it depends on the size of the vocabulary, with which it grows linearly. Despite several training tricks, the most straightforward way to limit computation time is to limit the vocabulary size, which is not a satisfactory solution for numerous tasks. Most of the existing methods used to train large-vocabulary language models revolve around avoiding the computation of the partition function, ensuring that output scores are normalized into a probability distribution. Here, we focus on sampling-based approaches, including importance sampling and noise contrastive estimation. These methods allow an approximate computation of the partition function. After examining the mechanism of self-normalization in noise-contrastive estimation, we first propose to improve its efficiency with solutions that are adapted to the inner workings of the method and experimentally show that they considerably ease training. Our second contribution is to expand on a generalization of several sampling based objectives as Bregman divergences, in order to experiment with new objectives. We use Beta divergences to derive a set of objectives from which noise contrastive estimation is a particular case. Finally, we aim at improving performances on full vocabulary language models, by augmenting output words representation with subwords. We experiment on a Czech dataset and show that using character-based representations besides word embeddings for output representations gives better results. We also show that reducing the size of the output look-up table improves results even more."}}
{"id": "NsxzisX3Tzr", "cdate": 1514764800000, "mdate": 1633361409278, "content": {"title": "Learning with Noise-Contrastive Estimation: Easing training by learning to scale", "abstract": "Matthieu Labeau, Alexandre Allauzen. Proceedings of the 27th International Conference on Computational Linguistics. 2018."}}
