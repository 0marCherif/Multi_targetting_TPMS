{"id": "7BhcDa7IOM", "cdate": 1672531200000, "mdate": 1696036078819, "content": {"title": "Mitigating Communication Costs in Neural Networks: The Role of Dendritic Nonlinearity", "abstract": "Our comprehension of biological neuronal networks has profoundly influenced the evolution of artificial neural networks (ANNs). However, the neurons employed in ANNs exhibit remarkable deviations from their biological analogs, mainly due to the absence of complex dendritic trees encompassing local nonlinearity. Despite such disparities, previous investigations have demonstrated that point neurons can functionally substitute dendritic neurons in executing computational tasks. In this study, we scrutinized the importance of nonlinear dendrites within neural networks. By employing machine-learning methodologies, we assessed the impact of dendritic structure nonlinearity on neural network performance. Our findings reveal that integrating dendritic structures can substantially enhance model capacity and performance while keeping signal communication costs effectively restrained. This investigation offers pivotal insights that hold considerable implications for the development of future neural network accelerators."}}
{"id": "zfonbDkNyoh", "cdate": 1620339268837, "mdate": null, "content": {"title": "Improved Expressivity Through Dendritic Neural Networks", "abstract": "A typical biological neuron, such as a pyramidal neuron of the neocortex, receives thousands of afferent synaptic inputs on its dendrite tree and sends the efferent axonal output downstream. In typical artificial neural networks, dendrite trees are modeled as linear structures that funnel weighted synaptic inputs to the cell bodies. However, numerous experimental and theoretical studies have shown that dendritic arbors are far more than simple linear accumulators. That is, synaptic inputs can actively modulate their neighboring synaptic activities; therefore, the dendritic structures are highly nonlinear. In this study, we model such local nonlinearity of dendritic trees with our dendritic neural network (DENN) structure and apply this structure to typical machine learning tasks. Equipped with localized nonlinearities, DENNs can attain greater model expressivity than regular neural networks while maintaining efficient network inference. Such strength is evidenced by the increased fitting power when we train DENNs with supervised machine learning tasks. We also empirically show that the locality structure of DENNs can improve the generalization performance, as exemplified by DENNs outranking naive deep neural network architectures when tested on classification tasks from the UCI machine learning repository."}}
{"id": "JxQ2jMOfNn-", "cdate": 1609459200000, "mdate": null, "content": {"title": "Two-Stage Point Cloud Super Resolution with Local Interpolation and Readjustment via Outer-Product Neural Network", "abstract": "This paper proposes a two-stage point cloud super resolution framework that combines local interpolation and deep neural network based readjustment. For the first stage, the authors apply a local interpolation method to increase the density and uniformity of the target point cloud. For the second stage, the authors employ an outer-product neural network to readjust the position of points that are inserted at the first stage. Comparison examples are given to demonstrate that the proposed framework achieves a better accuracy than existing state-of-art approaches, such as PU-Net, PointNet and DGCNN (Source code is available at https://github.com/qwerty1319/PC-SR )."}}
{"id": "HCgZT_Tmqx5", "cdate": 1609459200000, "mdate": 1646046581055, "content": {"title": "Intragroup sparsity for efficient inference", "abstract": "This work studies intragroup sparsity, a fine-grained structural constraint on network weight parameters. It eliminates the computational inefficiency of fine-grained sparsity due to irregular dataflow, while at the same time achieving high inference accuracy. We present theoretical analysis on how weight group sizes affect sparsification error, and on how the performance of pruned networks changes with sparsity level. Further, we analyze inference-time I/O cost of two different strategies for achieving intragroup sparsity and how the choice of strategies affect I/O cost under mild assumptions on accelerator architecture. Moreover, we present a novel training algorithm that yield models of improved accuracies over the standard training approach under the intragroup sparsity constraint."}}
{"id": "NYLvNv8q4i", "cdate": 1601308354083, "mdate": null, "content": {"title": "Intragroup sparsity for efficient inference", "abstract": "This work studies intragroup sparsity, a fine-grained structural constraint on network weight parameters. It eliminates the computational inefficiency of fine-grained sparsity due to irregular dataflow, while at the same time achieving high inference accuracy. We present theoretical analysis on how weight group sizes affect sparsification error, and on how the performance of pruned networks changes with sparsity level. Further, we analyze inference-time I/O cost of two different strategies for achieving intragroup sparsity and how the choice of strategies affect I/O cost under mild assumptions on accelerator architecture. Moreover, we present a novel training algorithm that yield models of improved accuracies over the standard training approach under the intragroup sparsity constraint."}}
{"id": "ITdfkUB_iEp", "cdate": 1577836800000, "mdate": null, "content": {"title": "ECG signal classification with binarized convolutional neural network", "abstract": "Highlights \u2022 Real-time ECG monitoring can be an important life-saving tool. \u2022 A binarized 1-D convolutional neural network is a good candidate for the task. \u2022 Na\u00efve binarized models suffer from significant performance loss. \u2022 With a proper strategy, binarized models can achieve good performance for the task. Abstract Arrhythmias are a group of common conditions associated with irregular heart rhythms. Some of these conditions, for instance, atrial fibrillation (AF), might develop into serious syndromes if not treated in time. Therefore, for high-risk patients, early detection of arrhythmias is crucial. In this study, we propose employing deep convolutional neural network (CNN)-based algorithms for real-time arrhythmia detection. We first build a full-precision deep convolutional network model. With our proposed construction, we are able to achieve state-of-the-art level performance on the PhysioNet/CinC AF Classification Challenge 2017 dataset with our full-precision model. It is desirable to employ models with low computing resource requirements. It has been shown that a binarized model requires much less computing power and memory space than a full-precision model. We proceed to verify the feasibility of binarization in our neural network model. Network binarization can cause significant model performance degradation. Therefore, we propose employing a full-precision model as the teacher to regularize the training of the binarized model through knowledge distillation. With our proposed approach, we observe that network binarization only causes a small performance loss (the F1 score decreases from 0.88 to 0.87 for the validation set). Given that binarized convolutional networks can achieve favorable model performance while dramatically reducing computing cost, they are ideal for deployment on long-term cardiac condition monitoring devices. (Source code is available at https://github.com/yangfansun/bnn-ecg)."}}
{"id": "E_uusgcEaa0", "cdate": 1577836800000, "mdate": null, "content": {"title": "SBNN: Slimming binarized neural network", "abstract": "With the rapid developments of deep neural networks related applications, approaches for accelerating computationally intensive convolutional neural networks, such as network quantization, pruning, knowledge distillation, have attracted ever-increasing attention. Network binarization is an extreme form of network quantization technique, which binarizes the network weights and/or activation values to save computational resources. However, it often introduces noises into the network, and requires larger model size (more parameters) to compensate for the loss of representation capacity. To address the model complexity reduction challenges and further improve the network performance, this paper proposes an approach: slimming binarized neural networks (SBNN), which reduces complexity of binarized networks with acceptable accuracy loss. SBNN prunes the convolutional layers and fully-connected layer in a binarized network. Then it is refined by the proposed SoftSign function, knowledge distillation and full-precision computation to enhance the network accuracy. The proposed SBNN can be also conveniently applied to a pre-trained binarized network. We demonstrate the effectiveness of our approach through several state-of-the-art binarized models. For AlexNet and ResNet-18 on ILSVRC-2012 dataset, SBNN obtains negligible accuracy loss but even a better accuracy than the pre-pruning model while using only 75% of original filters."}}
{"id": "42H42b-EvK1", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving the Accuracy of Binarized Neural Networks and Application on Remote Sensing Data", "abstract": "Deep neural networks are well known to achieve outstanding results in many domains. Recently, many researchers have introduced deep neural networks into remote sensing (RS) data processing. However, typical RS data usually possesses enormous scale. Processing RS data with deep neural networks requires a rather demanding computing hardware. Most high-performance deep neural networks are associated with highly complex network structures with many parameters. This restricts their deployment for real-time processing in satellites. Many researchers have attempted overcoming this obstacle by reducing network complexity. One of the promising approaches able to reduce network computational complexity and memory usage dramatically is network binarization. In this letter, through analyzing the learning behavior of binarized neural networks (BNNs), we propose several novel strategies for improving the performance of BNNs. Empirical experiments prove these strategies to be effective in improving BNN performance for image classification tasks on both small- and large-scale data sets. We also test BNN on a remote sense data set with positive results. A detailed discussion and preliminary analysis of the strategies used in the training are provided."}}
{"id": "Ddk9Q6cPj19", "cdate": 1546300800000, "mdate": null, "content": {"title": "Human body shape reconstruction from binary silhouette images", "abstract": "3D content creation is referred to as one of the most fundamental tasks of computer graphics. And many 3D modeling algorithms from 2D images or curves have been developed over the past several decades. Designers are allowed to align some conceptual images or sketch some suggestive curves, from front, side, and top views, and then use them as references in constructing a 3D model manually or semi-automatically. In this paper, we propose a deep learning based reconstruction of 3D human body shape from 2D orthographic silhouette images. A CNN-based regression network, with two branches corresponding to frontal and lateral views respectively, is designed for estimating 3D human body shape from binary silhouette images. We train our networks separately to decouple the feature descriptors which encode the body parameters from different views, and fuse them to estimate an accurate human body shape. In addition, to overcome the shortage of training data required for this purpose, we propose some significantly data augmentation schemes for 3D human body shapes, which can be used to promote further research on this topic. Extensive experimental results demonstrate that visually realistic and accurate reconstructions can be achieved effectively using our algorithm. Requiring only one or two silhouette images, our method can help users create their own digital avatars quickly, and also make it easy to create digital human body for 3D game, virtual reality, online fashion shopping."}}
{"id": "3EUdH9dyxRe", "cdate": 1546300800000, "mdate": null, "content": {"title": "How Dendrites Affect Online Recognition Memory", "abstract": "Author summary Humans can effortlessly recognize a pattern as familiar even after a single presentation and a long delay, and our capacity to do so even with complex stimuli such as images has been called \"almost limitless\". How is the information needed to support familiarity judgements stored so rapidly and held so reliably for such a long time? Most theoretical work aimed at understanding the brain's one-shot learning mechanisms has been based on drastically simplified neuron models which omit any representation of the most visually prominent features of neurons\u2014their extensive dendritic arbors. Given recent evidence that individual dendritic branches generate local spikes, and function as separately thresholded learning/responding units inside neurons, we set out to capture mathematically how the numerous parameters needed to describe a dendrite-based neural learning system interact to determine the memory's storage capacity. Using the model, we show that having dendrite-sized learning units provides a large capacity boost compared to a memory based on simplified (dendriteless) neurons, attesting to the importance of dendrites for optimal memory function. Our mathematical model may also prove useful in future efforts to understand how disruptions to dendritic structure and function lead to reduced memory capacity in aging and disease."}}
