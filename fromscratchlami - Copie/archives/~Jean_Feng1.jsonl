{"id": "rKb8gLLs5g5", "cdate": 1646077512162, "mdate": null, "content": {"title": "Sequential algorithmic modification with test data reuse", "abstract": "After initial release of a machine learning algorithm, the model can be fine-tuned by retraining on subsequently gathered data, adding newly discovered features, or more. Each modification introduces a risk of deteriorating performance and must be validated on a test dataset. It may not always be practical to assemble a new dataset for testing each modification, especially when most modifications are minor or are implemented in rapid succession. Recent work has shown how one can repeatedly test modifications on the same dataset and protect against overfitting by (i) discretizing test results along a grid and (ii) applying a Bonferroni correction to adjust for the total number of modifications considered by an adaptive developer. However, the standard Bonferroni correction is overly conservative when most modifications are beneficial and/or highly correlated. This work investigates more powerful approaches using alpha-recycling and sequentially-rejective graphical procedures (SRGPs). We introduce two novel extensions that account for correlation between adaptively chosen algorithmic modifications: the first leverages the correlation between consecutive modifications using flexible fixed sequence tests, and the second leverages the correlation between the proposed modifications and those generated by a hypothetical prespecified model updating procedure. In empirical analyses, both SRGPs control the error rate of approving deleterious modifications and approve significantly more beneficial modifications than previous approaches."}}
{"id": "TfSpXaSSrT", "cdate": 1600131179394, "mdate": null, "content": {"title": "Approval policies for modifications to machine learning-based software as a medical device: A study of bio-creep", "abstract": "Successful deployment of machine learning algorithms in healthcare requires careful assessments of their\nperformance and safety. To date, the FDA approves locked algorithms prior to marketing and requires future updates to\nundergo separate premarket reviews. However, this negates a key feature of machine learning\u2013the ability to learn from a\ngrowing dataset and improve over time. This paper frames the design of an approval policy, which we refer to as an automatic\nalgorithmic change protocol (aACP), as an online hypothesis testing problem. As this process has obvious analogy with\nnoninferiority testing of new drugs, we investigate how repeated testing and adoption of modifications might lead to gradual\ndeterioration in prediction accuracy, also known as \u201cbiocreep\u201d in the drug development literature. We consider simple policies\nthat one might consider but do not necessarily offer any error-rate guarantees, as well as policies that do provide error-rate\ncontrol. For the latter, we define two online error-rates appropriate for this context: Bad Approval Count (BAC) and Bad\nApproval and Benchmark Ratios (BABR). We control these rates in the simple setting of a constant population and data source\nusing policies aACP-BAC and aACP-BABR, which combine alpha-investing, group-sequential, and gate-keeping methods. In\nsimulation studies, bio-creep regularly occurred when using policies with no error-rate guarantees, whereas aACP-BAC and\n-BABR controlled the rate of bio-creep without substantially impacting our ability to approve beneficial modifications."}}
{"id": "qyIFi8bCS5y", "cdate": 1600131031531, "mdate": null, "content": {"title": "Efficient nonparametric statistical inference on population feature importance using Shapley values", "abstract": "The true population-level importance of a variable in a prediction task provides useful knowledge about the underlying data-generating mechanism and can help in deciding which measurements to collect in subsequent experiments. Valid statistical inference on this importance is a key component in understanding the population of interest. We present a computationally efficient procedure for estimating and obtaining valid statistical inference on the Shapley Population Variable Importance Measure (SPVIM). Although the computational complexity of the true SPVIM scales exponentially with the number of variables, we propose an estimator based on randomly sampling only \u0398(n) feature subsets given n observations. We prove that our estimator converges at an asymptotically optimal rate. Moreover, by deriving the asymptotic distribution of our estimator, we construct valid confidence intervals and hypothesis tests. Our procedure has good finite-sample performance in simulations, and for an in-hospital mortality prediction task produces similar variable importance estimates when different machine learning algorithms are applied."}}
{"id": "ByZZr3bdbS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Nonparametric variable importance using an augmented neural network with multi-task learning", "abstract": "In predictive modeling applications, it is often of interest to determine the relative contribution of subsets of features in explaining the variability of an outcome. It is useful to consider this..."}}
