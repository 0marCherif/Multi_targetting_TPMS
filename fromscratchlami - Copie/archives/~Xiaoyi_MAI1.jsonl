{"id": "_UlpBAtHRT", "cdate": 1640995200000, "mdate": 1681906845351, "content": {"title": "On The Effectiveness of Active Learning by Uncertainty Sampling in Classification of High-Dimensional Gaussian Mixture Data", "abstract": "Active learning aims to reduce the cost of labeling through selective sampling. Despite reported empirical success over passive learning, many popular active learning heuristics such as uncertainty sampling still lack satisfying theoretical guarantees. Towards closing the gap between practical use and theoretical understanding in active learning, we propose to characterize the exact behavior of uncertainty sampling for high-dimensional Gaussian mixture data, in a modern regime of big data where the numbers of samples and features are commensurately large. Through a sharp characterization of the learning results, our analysis sheds light on the important question of when uncertainty sampling works better than passive learning. Our results show that the effectiveness of uncertainty sampling is not always ensured. In fact it depends crucially on the choice of i) an adequate initial classifier used to start the active sampling process and ii) a proper loss function that allows an adaptive treatment of samples queried at various steps."}}
{"id": "4iFsmUEmbna", "cdate": 1609459200000, "mdate": 1681906845349, "content": {"title": "Consistent Semi-Supervised Graph Regularization for High Dimensional Data", "abstract": "Semi-supervised Laplacian regularization, a standard graph-based approach for learning from both labelled and unlabelled data, was recently demonstrated to have an insignificant high dimensional learning efficiency with respect to unlabelled data, causing it to be outperformed by its unsupervised counterpart, spectral clustering, given sufficient unlabelled data. Following a detailed discussion on the origin of this inconsistency problem, a novel regularization approach involving centering operation is proposed as solution, supported by both theoretical analysis and empirical results."}}
{"id": "e6YFEy7jETM", "cdate": 1577836800000, "mdate": 1681906845343, "content": {"title": "Consistent Semi-Supervised Graph Regularization for High Dimensional Data", "abstract": "Semi-supervised Laplacian regularization, a standard graph-based approach for learning from both labelled and unlabelled data, was recently demonstrated to have an insignificant high dimensional learning efficiency with respect to unlabelled data (Mai and Couillet 2018), causing it to be outperformed by its unsupervised counterpart, spectral clustering, given sufficient unlabelled data. Following a detailed discussion on the origin of this inconsistency problem, a novel regularization approach involving centering operation is proposed as solution, supported by both theoretical analysis and empirical results."}}
{"id": "fVPoLE4MYgC", "cdate": 1546300800000, "mdate": 1681906845352, "content": {"title": "A Large Scale Analysis of Logistic Regression: Asymptotic Performance and New Insights", "abstract": "Logistic regression, one of the most popular machine learning binary classification methods, has been long believed to be unbiased. In this paper, we consider the \"hard\" classification problem of separating high dimensional Gaussian vectors, where the data dimension p and the sample size n are both large. Based on recent advances in random matrix theory (RMT) and high dimensional statistics, we evaluate the asymptotic distribution of the logistic regression classifier and consequently, provide the associated classification performance. This brings new insights into the internal mechanism of logistic regression classifier, including a possible bias in the separating hyperplane, as well as on practical issues such as hyper-parameter tuning, thereby opening the door to novel RMT-inspired improvements."}}
{"id": "6phHOmAtfy", "cdate": 1546300800000, "mdate": 1681906845348, "content": {"title": "High Dimensional Classification via Empirical Risk Minimization: Improvements and Optimality", "abstract": "This article provides, through theoretical analysis, an in-depth understanding of the classification performance of the empirical risk minimization framework, in both ridge-regularized and unregularized cases, when high dimensional data are considered. Focusing on the fundamental problem of separating a two-class Gaussian mixture, the proposed analysis allows for a precise prediction of the classification error for a set of numerous data vectors $\\mathbf{x} \\in \\mathbb R^p$ of sufficiently large dimension $p$. This precise error depends on the loss function, the number of training samples, and the statistics of the mixture data model. It is shown to hold beyond Gaussian distribution under some additional non-sparsity condition of the data statistics. Building upon this quantitative error analysis, we identify the simple square loss as the optimal choice for high dimensional classification in both ridge-regularized and unregularized cases, regardless of the number of training samples."}}
{"id": "3NAPwmSRQb", "cdate": 1546300800000, "mdate": 1681906845349, "content": {"title": "Revisiting and Improving Semi-supervised Learning: A Large Dimensional Approach", "abstract": "The recent work [1] shows that in the big data regime (i.e., numerous high dimensional data), the popular semi-supervised graph regularization, known as semi-supervised Laplacian regularization, fails to effectively extract information from unlabelled data. In response to this problem, we propose in this article an improved approach based on a simple yet fundamental update of the classical method. The effectiveness of the former is supported by both asymptotic results and simulations on finite data samples."}}
{"id": "k6QrQtOsr7q", "cdate": 1514764800000, "mdate": 1681906845352, "content": {"title": "Classification Asymptotics in the Random Matrix Regime", "abstract": "This article discusses the asymptotic performance of classical machine learning classification methods (from discriminant analysis to neural networks) for simultaneously large and numerous Gaussian mixture modelled data. We first provide theoretical bounds on the minimally discriminable class means and covariances under an oracle setting, which are then compared to recent theoretical findings on the performance of machine learning. Non-obvious phenomena are discussed, among which surprising phase transitions in the optimal performance rates for specific hyperparameter settings."}}
{"id": "arLtKKVP0n", "cdate": 1514764800000, "mdate": 1681906845362, "content": {"title": "A Random Matrix Analysis and Improvement of Semi-Supervised Learning for Large Dimensional Data", "abstract": "This article provides an original understanding of the behavior of a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data. It is demonstrated that the intuition at the root of these methods collapses in this limit and that, as a result, most of them become inconsistent. Corrective measures and a new data-driven parametrization scheme are proposed along with a theoretical analysis of the asymptotic performances of the resulting approach. A surprisingly close behavior between theoretical performances on Gaussian mixture models and on real data sets is also illustrated throughout the article, thereby suggesting the importance of the proposed analysis for dealing with practical data. As a result, significant performance gains are observed on practical data classification using the proposed parametrization."}}
{"id": "9_WuxKJLvD", "cdate": 1514764800000, "mdate": 1681906845458, "content": {"title": "Semi-Supervised Spectral Clustering", "abstract": "In this article, we propose a semi-supervised version of spectral clustering, a widespread graph-based unsupervised learning method. The semi-supervised spectral clustering has the advantage of producing consistent classification of data with sufficiently large number of labelled or unlabelled data, un-like classical graph-based semi-supervised methods which are only consistent on labelled data. Theoretical arguments are provided to support the proposition of this novel approach, as well as empirical evidence to confirm the theoretical claims and demonstrate its superiority over other graph-based semi-supervised methods."}}
{"id": "lTGgBSjuf4", "cdate": 1483228800000, "mdate": 1681906845350, "content": {"title": "The counterintuitive mechanism of graph-based semi-supervised learning in the big data regime", "abstract": ""}}
