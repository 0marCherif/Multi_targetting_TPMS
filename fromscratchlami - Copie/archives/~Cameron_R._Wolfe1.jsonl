{"id": "-XsiFMpdSJz", "cdate": 1664046170192, "mdate": null, "content": {"title": "GIST: Distributed Training for Large-Scale Graph Convolutional Networks", "abstract": "The graph convolutional network (GCN) is a go-to solution for machine learning on graphs, but its training is notoriously difficult to scale both in terms of graph size and the number of model parameters. Although some work has explored training on large-scale graphs, we pioneer efficient training of large-scale GCN models with the proposal of a novel, distributed training framework, called \\texttt{GIST}. \\texttt{GIST} disjointly partitions the parameters of a GCN model into several, smaller sub-GCNs that are trained independently and in parallel. Compatible with all GCN architectures and existing sampling techniques, \\texttt{GIST} $i)$ improves model performance, $ii)$ scales to training on arbitrarily large graphs, $iii)$ decreases wall-clock training time, and $iv)$ enables the training of markedly overparameterized GCN models. Remarkably, with \\texttt{GIST}, we train an astonishgly-wide $32,\\!768$-dimensional GraphSAGE model, which exceeds the capacity of a single GPU by a factor of $8\\times$, to SOTA performance on the Amazon2M dataset. "}}
{"id": "rq4v8Ujcec", "cdate": 1646077519190, "mdate": null, "content": {"title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training", "abstract": "We propose ResIST, a novel distributed training protocol for Residual Networks (ResNets). ResIST randomly decomposes a global ResNet into several shallow sub-ResNets that are trained independently in a distributed manner for several local iterations, before having their updates synchronized and aggregated into the global model. In the next round, new sub-ResNets are randomly generated and the process repeats until convergence. By construction, per iteration, ResIST communicates only a small portion of network parameters to each machine and never uses the full model during training. Thus, ResIST reduces the per-iteration communication, memory, and time requirements of ResNet training to only a fraction of the requirements of full-model training. In comparison to common protocols, like data-parallel training and data-parallel training with local SGD, ResIST yields a decrease in communication and compute requirements, while being competitive with respect to model performance."}}
{"id": "kSwqMH0zn1F", "cdate": 1632875608302, "mdate": null, "content": {"title": "PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication", "abstract": "Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature gradients among partitions for every GCN layer during each training iteration, limiting the achievable training efficiency and model scalability. To this end, we propose PipeGCN, a simple yet effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation. It is non-trivial to pipeline for efficient GCN training, as communicated node features/gradients will become stale and thus can harm the convergence, negating the pipeline benefit. Notably, little is known regarding the convergence rate of GCN training with both stale features and stale feature gradients. This work not only provides a theoretical convergence analysis but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without any staleness. Furthermore, we develop a smoothing method to further improve PipeGCN's convergence. Extensive experiments show that PipeGCN can largely boost the training throughput (1.7\u00d7~28.5\u00d7) while achieving the same accuracy as its vanilla counterpart and existing full-graph training methods. The code is available at https://github.com/RICE-EIC/PipeGCN."}}
{"id": "GFRq2JxiI7d", "cdate": 1632875505951, "mdate": null, "content": {"title": "How much pre-training is enough to discover a good subnetwork?", "abstract": "Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. However, more often than not, it involves a three-step process\u2014pre-training, pruning, and re-training\u2014that is computationally expensive, as the dense model must be fully pre-trained. Luckily, several works have empirically shown that high-performing subnetworks can be discovered via pruning without fully pre-training the dense network. Aiming to theoretically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a theoretical bound in the number of SGD pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection (Ye et al., 2020) yields a subnetwork that achieves good training error. This threshold is shown to be logarithmically dependent upon the size of the dataset, meaning that experiments with larger datasets require more pre-training for subnetworks obtained via pruning to perform well. We empirically demonstrate the validity of our theoretical results across a variety of architectures and datasets, including fully-connected networks trained on MNIST and several deep convolutional neural network (CNN) architectures trained on CIFAR10 and ImageNet."}}
{"id": "4pgt90ydves", "cdate": 1609459200000, "mdate": null, "content": {"title": "GIST: Distributed Training for Large-Scale Graph Convolutional Networks", "abstract": "The graph convolutional network (GCN) is a go-to solution for machine learning on graphs, but its training is notoriously difficult to scale both in terms of graph size and the number of model parameters. Although some work has explored training on large-scale graphs (e.g., GraphSAGE, ClusterGCN, etc.), we pioneer efficient training of large-scale GCN models (i.e., ultra-wide, overparameterized models) with the proposal of a novel, distributed training framework. Our proposed training methodology, called GIST, disjointly partitions the parameters of a GCN model into several, smaller sub-GCNs that are trained independently and in parallel. In addition to being compatible with all GCN architectures and existing sampling techniques for efficient GCN training, GIST i) improves model performance, ii) scales to training on arbitrarily large graphs, iii) decreases wall-clock training time, and iv) enables the training of markedly overparameterized GCN models. Remarkably, with GIST, we train an astonishgly-wide 32,768-dimensional GraphSAGE model, which exceeds the capacity of a single GPU by a factor of 8x, to SOTA performance on the Amazon2M dataset."}}
{"id": "yNFwsrcEtO0", "cdate": 1601308116609, "mdate": null, "content": {"title": "Demon: Momentum Decay for Improved Neural Network Training", "abstract": "Momentum is a popular technique in deep learning for gradient-based optimizers. We propose a decaying momentum (Demon) rule, motivated by decaying the total contribution of a gradient to all future updates. Applying Demon to Adam leads to significantly improved training, notably competitive to momentum SGD with learning rate decay, even in settings in which adaptive methods are typically non-competitive. Similarly, applying Demon to momentum SGD improves over momentum SGD with learning rate decay in most cases. Notably, Demon momentum SGD is observed to be significantly less sensitive to parameter tuning than momentum SGD with learning rate decay schedule, critical to training neural networks in practice. Results are demonstrated across a variety of settings and architectures, including image classification, generative models, and language models. Demon is easy to implement and tune, and incurs limited extra computational overhead, compared to the vanilla counterparts. Code is readily available."}}
{"id": "sgxgBbX8tAJ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Data Augmentation for Deep Transfer Learning", "abstract": "In this work, we propose data augmentation methods for embeddings from pre-trained deep learning models that take a weighted combination of a pair of input embeddings, as inspired by Mixup, and combine such augmentation with extra label softening. These methods are shown to significantly increase classification accuracy, reduce training time, and improve confidence calibration of a downstream model that is trained with them. As a result of such improved confidence calibration, the model output can be more intuitively interpreted and used to accurately identify out-of-distribution data by applying an appropriate confidence threshold to model predictions. The identified out-of-distribution data can then be prioritized for labeling, thus focusing labeling effort on data that is more likely to boost model performance. These findings, we believe, lay a solid foundation for improving the classification performance and calibration of models that use pre-trained embeddings as input and provide several benefits that prove extremely useful in a production-level deep learning system."}}
{"id": "odOmDX_lGrq", "cdate": 1546300800000, "mdate": null, "content": {"title": "Functional Generative Design of Mechanisms with Recurrent Neural Networks and Novelty Search", "abstract": "Consumer-grade 3D printers have made it easier to fabricate aesthetic objects and static assemblies, opening the door to automated design of such objects. However, while static designs are easily produced with 3D printing, functional designs with moving parts are more difficult to generate: The search space is too high-dimensional, the resolution of the 3D-printed parts is not adequate, and it is difficult to predict the physical behavior of imperfect 3D-printed mechanisms. An example challenge is to produce a diverse set of reliable and effective gear mechanisms that could be used after production without extensive post-processing. To meet this challenge, an indirect encoding based on a Recurrent Neural Network (RNN) is created and evolved using novelty search. The elite solutions of each generation are 3D printed to evaluate their functional performance on a physical test platform. The system is able to discover sequential design rules that are difficult to discover with other methods. Compared to direct encoding evolved with Genetic Algorithms (GAs), its designs are geometrically more diverse and functionally more effective. It therefore forms a promising foundation for the generative design of 3D-printed, functional mechanisms."}}
{"id": "cFnUUXPNeGh", "cdate": 1546300800000, "mdate": null, "content": {"title": "Functional generative design of mechanisms with recurrent neural networks and novelty search", "abstract": "Consumer-grade 3D printers have made the fabrication of aesthetic objects and static assemblies easier, opening the door to automate the design of such objects. However, while static designs are easily produced with 3D printing, functional designs, with moving parts, are more difficult to generate: The search space is high-dimensional, the resolution of the 3D-printed parts is not adequate, and it is difficult to predict the physical behavior of imperfect, 3D-printed mechanisms. An example challenge for automating the design of functional, 3D-printed mechanisms is producing a diverse set of reliable and effective gear mechanisms that could be used after production without extensive post-processing. To meet this challenge, an indirect encoding based on a Recurrent Neural Network (RNN) is proposed and evolved using Novelty Search. The elite solutions of each generation are 3D printed to evaluate their functional performance in a physical test platform. The proposed RNN model successfully discovers sequential design rules that are difficult to discover with other methods. Compared to a direct encoding of gear mechanisms evolved with Genetic Algorithms (GAs), the designs produced by the RNN are geometrically more diverse and functionally more effective, thus forming a promising foundation for the generative design of 3D-printed, functional mechanisms."}}
