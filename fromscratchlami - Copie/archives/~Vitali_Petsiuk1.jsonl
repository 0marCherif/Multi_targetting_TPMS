{"id": "x-XKh_LaB3", "cdate": 1640995200000, "mdate": 1682001888915, "content": {"title": "Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark", "abstract": "We provide a new multi-task benchmark for evaluating text-to-image models. We perform a human evaluation comparing the most common open-source (Stable Diffusion) and commercial (DALL-E 2) models. Twenty computer science AI graduate students evaluated the two models, on three tasks, at three difficulty levels, across ten prompts each, providing 3,600 ratings. Text-to-image generation has seen rapid progress to the point that many recent models have demonstrated their ability to create realistic high-resolution images for various prompts. However, current text-to-image methods and the broader body of research in vision-language understanding still struggle with intricate text prompts that contain many objects with multiple attributes and relationships. We introduce a new text-to-image benchmark that contains a suite of thirty-two tasks over multiple applications that capture a model's ability to handle different features of a text prompt. For example, asking a model to generate a varying number of the same object to measure its ability to count or providing a text prompt with several objects that each have a different attribute to identify its ability to match objects and attributes correctly. Rather than subjectively evaluating text-to-image results on a set of prompts, our new multi-task benchmark consists of challenge tasks at three difficulty levels (easy, medium, and hard) and human ratings for each generated image."}}
{"id": "pawEkTJ8dB", "cdate": 1609459200000, "mdate": 1667681940651, "content": {"title": "Black-Box Explanation of Object Detectors via Saliency Maps", "abstract": "We propose D-RISE, a method for generating visual explanations for the predictions of object detectors. Utilizing the proposed similarity metric that accounts for both localization and categorization aspects of object detection allows our method to produce saliency maps that show image areas that most affect the prediction. D-RISE can be considered \"black-box\" in the software testing sense, as it only needs access to the inputs and outputs of an object detector. Compared to gradient-based methods, D-RISE is more general and agnostic to the particular type of object detector being tested, and does not need knowledge of the inner workings of the model. We show that D-RISE can be easily applied to different object detectors including one-stage detectors such as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed analysis of the generated visual explanations to highlight the utilization of context and possible biases learned by object detectors."}}
{"id": "fElqh3n1I-i", "cdate": 1609459200000, "mdate": 1667681940660, "content": {"title": "Guided Zoom: Zooming into Network Evidence to Refine Fine-Grained Model Decisions", "abstract": "In state-of-the-art deep single-label classification models, the top- <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula> <inline-formula><tex-math notation=\"LaTeX\">$(k=2,3,4, \\dots)$</tex-math></inline-formula> accuracy is usually significantly higher than the top-1 accuracy. This is more evident in fine-grained datasets, where differences between classes are quite subtle. Exploiting the information provided in the top <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula> predicted classes boosts the final prediction of a model. We propose Guided Zoom, a novel way in which explainability could be used to improve model performance. We do so by making sure the model has \u201cthe right reasons\u201d for a prediction. The reason/evidence upon which a deep neural network makes a prediction is defined to be the grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable the evidence used to make each of the top- <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula> predictions is. Test time evidence is deemed reasonable if it is coherent with evidence used to make similar correct decisions at training time. This leads to better informed predictions. We explore a variety of grounding techniques and study their complementarity for computing evidence. We show that Guided Zoom results in an improvement of a model's classification accuracy and achieves state-of-the-art classification performance on four fine-grained classification datasets. Our code is available at <uri>https://github.com/andreazuna89/Guided-Zoom</uri> ."}}
{"id": "e0hOxQjvznU", "cdate": 1577836800000, "mdate": 1667681940694, "content": {"title": "Black-box Explanation of Object Detectors via Saliency Maps", "abstract": "We propose D-RISE, a method for generating visual explanations for the predictions of object detectors. Utilizing the proposed similarity metric that accounts for both localization and categorization aspects of object detection allows our method to produce saliency maps that show image areas that most affect the prediction. D-RISE can be considered \"black-box\" in the software testing sense, as it only needs access to the inputs and outputs of an object detector. Compared to gradient-based methods, D-RISE is more general and agnostic to the particular type of object detector being tested, and does not need knowledge of the inner workings of the model. We show that D-RISE can be easily applied to different object detectors including one-stage detectors such as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed analysis of the generated visual explanations to highlight the utilization of context and possible biases learned by object detectors."}}
{"id": "V8NZiNbMBSa", "cdate": 1577836800000, "mdate": 1667681940715, "content": {"title": "Beyond the Visual Analysis of Deep Model Saliency", "abstract": "Increased explainability in machine learning is traditionally associated with lower performance, e.g.\u00a0a decision tree is more explainable, but less accurate than a deep neural network. We argue that, in fact, increasing the explainability of a deep classifier can improve its generalization. In this chapter, we survey a line of our published work that demonstrates how spatial and spatiotemporal visual explainability can be obtained, and how such explainability can be used to train models that generalize better on unseen in-domain and out-of-domain samples, refine fine-grained classification predictions, better utilize network capacity, and are more robust to network compression."}}
{"id": "PyJW1z7H9t", "cdate": 1577836800000, "mdate": 1667681940667, "content": {"title": "Why Do These Match? Explaining the Behavior of Image Similarity Models", "abstract": "Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering. In this paper, we introduce Salient Attributes for Network Explanation (SANE) to explain image similarity models, where a model\u2019s output is a score measuring the similarity of two inputs rather than a classification score. In this task, an explanation depends on both of the input images, so standard methods do not apply. Our SANE explanations pairs a saliency map identifying important image regions with an attribute that best explains the match. We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach\u2019s ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2. Code available at: https://github.com/VisionLearningGroup/SANE ."}}
{"id": "S1l_ZlrFvS", "cdate": 1569439744240, "mdate": null, "content": {"title": "Why do These Match? Explaining the Behavior of Image Similarity Models", "abstract": "Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering.  In this paper, we introduce an explanation approach for image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classification.  In this task, an explanation depends on both of the input images, so standard methods do not apply. We propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match.  We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach's ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2."}}
{"id": "smIymFJ0nl", "cdate": 1546300800000, "mdate": 1667681940704, "content": {"title": "Guided Zoom: Questioning Network Evidence for Fine-grained Classification", "abstract": ""}}
{"id": "J5h4lGyOglT", "cdate": 1546300800000, "mdate": 1667681940741, "content": {"title": "Why do These Match? Explaining the Behavior of Image Similarity Models", "abstract": "Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering. In this paper, we introduce Salient Attributes for Network Explanation (SANE) to explain image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classification score. In this task, an explanation depends on both of the input images, so standard methods do not apply. Our SANE explanations pairs a saliency map identifying important image regions with an attribute that best explains the match. We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach's ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2. Code available at: https://github.com/VisionLearningGroup/SANE"}}
{"id": "CxpUsNQWGcx", "cdate": 1546300800000, "mdate": 1667681940723, "content": {"title": "Are CNN Predictions based on Reasonable Evidence?", "abstract": "We propose Guided Zoom, an approach that utilizes spatial grounding to make more informed predictions. It does so by making sure the model has \"the right reasons\" for a prediction, being defined as reasons that are coherent with those used to make similar correct decisions at training time. The reason/evidence upon which a deep neural network makes a prediction is defined to be the spatial grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom question show reasonable the evidence used to make a prediction is. We show that Guided Zoom results in the refinement of a model's classification accuracy on two fine-grained classification datasets."}}
