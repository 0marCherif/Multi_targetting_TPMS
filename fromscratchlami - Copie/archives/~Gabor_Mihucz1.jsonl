{"id": "vPS7yxt6oNE", "cdate": 1663850195548, "mdate": null, "content": {"title": "Goal-Space Planning with Subgoal Models", "abstract": "This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can learn significantly faster than a Double DQN baseline in a variety of situations."}}
{"id": "43wm0POPxQv", "cdate": 1640995200000, "mdate": 1681494441645, "content": {"title": "Goal-Space Planning with Subgoal Models", "abstract": ""}}
{"id": "nz2iUi-iZLQ", "cdate": 1621630035063, "mdate": null, "content": {"title": "Structural Credit Assignment in Neural Networks using Reinforcement Learning", "abstract": "Structural credit assignment in neural networks is a long-standing problem, with a variety of alternatives to backpropagation proposed to allow for local training of nodes. One of the early strategies was to treat each node as an agent and use a reinforcement learning method called REINFORCE to update each node locally with only a global reward signal. In this work, we revisit this approach and investigate if we can leverage other reinforcement learning approaches to improve learning. We first formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. We show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions. We introduce an off-policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. We conclude by showing that these networks of agents can be more robust to correlated samples when learning online."}}
{"id": "virfwDC0vgN", "cdate": 1609459200000, "mdate": 1681494441645, "content": {"title": "Structural Credit Assignment in Neural Networks using Reinforcement Learning", "abstract": ""}}
