{"id": "sUmIB5XXrO", "cdate": 1672531200000, "mdate": 1686170855029, "content": {"title": "Hierarchical Video-Moment Retrieval and Step-Captioning", "abstract": "There is growing interest in searching for information from large video corpora. Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HiREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus. HiREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans relevant to text query and breakdown of each moment into key instruction steps with caption and timestamps (totaling 8.6K step captions). Our hierarchical benchmark consists of video retrieval, moment retrieval, and two novel moment segmentation and step captioning tasks. In moment segmentation, models break down a video moment into instruction steps and identify start-end boundaries. In step captioning, models generate a textual summary for each step. We also present starting point task-specific and end-to-end joint baseline models for our new benchmark. While the baseline models show some promising results, there still exists large room for future improvement by the community. Project website: https://hirest-cvpr2023.github.io"}}
{"id": "jv5vndJ-iu", "cdate": 1672531200000, "mdate": 1686170855025, "content": {"title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval", "abstract": "Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our dense retriever trained with diverse augmentation, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++)."}}
{"id": "DYRkAozMRw", "cdate": 1672531200000, "mdate": 1686170855030, "content": {"title": "Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis", "abstract": "Few-shot learning for open domain multi-hop question answering typically relies on large language models (LLMs). While powerful, LLMs are inefficient at the inference time. We propose a data synthesis framework for multi-hop question answering that allows for improving smaller language models with less than 10 human-annotated question answer pairs. The framework is built upon the data generation functions parameterized by LLMs and prompts, which requires minimal hand-crafted features. Empirically, we synthesize millions of multi-hop questions and claims. After finetuning language models on the synthetic data, we evaluate the models on popular benchmarks on multi-hop question answering and fact verification. Our experimental results show that finetuning on the synthetic data improves model performance significantly, allowing our finetuned models to be competitive with prior models while being almost one-third the size in terms of parameter counts."}}
{"id": "AIvnwQk757", "cdate": 1672531200000, "mdate": 1686170855027, "content": {"title": "VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation", "abstract": "We propose a new two-stage pre-training framework for video-to-text generation tasks such as video captioning and video question answering: A generative encoder-decoder model is first jointly pre-trained on massive image-text data to learn fundamental vision-language concepts, and then adapted to video data in an intermediate video-text pre-training stage to learn video-specific skills such as spatio-temporal reasoning. As a result, our VideoOFA model achieves new state-of-the-art performance on four Video Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr score. It also outperforms existing models on two open-ended Video Question Answering datasets, showcasing its generalization capability as a universal video-to-text model."}}
{"id": "rdFPgLzFjdV", "cdate": 1640995200000, "mdate": 1673646518593, "content": {"title": "UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering", "abstract": ""}}
{"id": "g2-mb7ZvLS", "cdate": 1640995200000, "mdate": 1681776150782, "content": {"title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?", "abstract": "Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, Wen-tau Yih. Findings of the Association for Computational Linguistics: EMNLP 2022. 2022."}}
{"id": "dPC0dyUiIG", "cdate": 1640995200000, "mdate": 1673646518114, "content": {"title": "CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training", "abstract": ""}}
{"id": "NGl3LWwtLmd", "cdate": 1640995200000, "mdate": 1673646518560, "content": {"title": "CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval", "abstract": ""}}
{"id": "Mm1Uge1Fn0", "cdate": 1640995200000, "mdate": 1673646518770, "content": {"title": "Nonparametric Masked Language Modeling", "abstract": ""}}
{"id": "L8zJ2iv3h-", "cdate": 1640995200000, "mdate": 1665516089517, "content": {"title": "Simple Local Attentions Remain Competitive for Long-Context Tasks", "abstract": "Wenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Scott Yih, Yashar Mehdad. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
