{"id": "ZsLZo01LUl", "cdate": 1620939231912, "mdate": null, "content": {"title": "Robust high-dimensional factor models with applications to statistical machine learning", "abstract": "Factor models are a class of powerful statistical models that have been widely used to deal with dependent measurements that arise frequently from various applications from genomics and neuroscience to economics and finance. As data are collected at an ever-growing scale, statistical machine learning faces some new challenges: high dimensionality, strong dependence among observed variables, heavy-tailed variables and heterogeneity. High-dimensional robust factor analysis serves as a powerful toolkit to conquer these challenges.\n\nThis paper gives a selective overview on recent advance on high- dimensional factor models and their applications to statistics including Factor-Adjusted Robust Model selection (FarmSelect) and Factor-Adjusted Robust Multiple testing (FarmTest). We show that classical methods, especially principal component analysis (PCA), can be tailored to many new problems and provide powerful tools for statistical estimation and inference. We highlight PCA and its connections to matrix perturbation theory, robust statistics, random projection, false discovery rate, etc., and illustrate through several applications how insights from these fields yield solutions to modern challenges. We also present far-reaching connections between factor models and popular statistical learning problems, including network analysis and low-rank matrix recovery."}}
{"id": "V5AY0DU1Xt", "cdate": 1620938968446, "mdate": null, "content": {"title": "Entrywise Eigenvector Analysis of Random Matrices with Low Expected Rank", "abstract": "Recovering low-rank structures via eigenvector perturbation analysis is a common problem in statistical machine learning, such as in factor analysis, community detection, ranking, matrix completion, among others.  While a large variety of bounds are available for average errors between empirical and population statistics of eigenvectors, few results are tight for entrywise analyses, which are critical for a number of problems such as community detection.\n\nThis paper investigates entrywise behaviors of eigenvectors for a large class of random matrices whose expectations are low-rank, which helps settle the conjecture in \\cite{abh_arxiv} that the spectral algorithm achieves exact recovery in the stochastic block model without any trimming or cleaning steps. The key is a first-order approximation of eigenvectors under the $\\ell_\\infty$ norm:\n$$u_k \\approx \\frac{A u_k^*}{\\lambda_k^*},$$\nwhere $\\{u_k\\}$ and $\\{u_k^*\\}$ are eigenvectors of a random matrix $A$ and its expectation $\\E A$, respectively. The fact that the approximation is both tight and linear in $A$ facilitates sharp comparisons between $u_k$ and $u_k^*$. In particular, it allows for comparing the signs of $u_k$ and $u_k^*$ even if $\\| u_k - u_k^*\\|_{\\infty}$ is large. The results are further extended to perturbations of eigenspaces, yielding new $\\ell_\\infty$-type bounds for synchronization ($\\mathbb{Z}_2$-spiked Wigner model) and noisy matrix completion."}}
{"id": "o3cEh2d87rE", "cdate": 1609459200000, "mdate": 1682364669684, "content": {"title": "Tractability from overparametrization: The example of the negative perceptron", "abstract": "In the negative perceptron problem we are given $n$ data points $({\\boldsymbol x}_i,y_i)$, where ${\\boldsymbol x}_i$ is a $d$-dimensional vector and $y_i\\in\\{+1,-1\\}$ is a binary label. The data are not linearly separable and hence we content ourselves to find a linear classifier with the largest possible \\emph{negative} margin. In other words, we want to find a unit norm vector ${\\boldsymbol \\theta}$ that maximizes $\\min_{i\\le n}y_i\\langle {\\boldsymbol \\theta},{\\boldsymbol x}_i\\rangle$. This is a non-convex optimization problem (it is equivalent to finding a maximum norm vector in a polytope), and we study its typical properties under two random models for the data. We consider the proportional asymptotics in which $n,d\\to \\infty$ with $n/d\\to\\delta$, and prove upper and lower bounds on the maximum margin $\\kappa_{\\text{s}}(\\delta)$ or -- equivalently -- on its inverse function $\\delta_{\\text{s}}(\\kappa)$. In other words, $\\delta_{\\text{s}}(\\kappa)$ is the overparametrization threshold: for $n/d\\le \\delta_{\\text{s}}(\\kappa)-\\varepsilon$ a classifier achieving vanishing training error exists with high probability, while for $n/d\\ge \\delta_{\\text{s}}(\\kappa)+\\varepsilon$ it does not. Our bounds on $\\delta_{\\text{s}}(\\kappa)$ match to the leading order as $\\kappa\\to -\\infty$. We then analyze a linear programming algorithm to find a solution, and characterize the corresponding threshold $\\delta_{\\text{lin}}(\\kappa)$. We observe a gap between the interpolation threshold $\\delta_{\\text{s}}(\\kappa)$ and the linear programming threshold $\\delta_{\\text{lin}}(\\kappa)$, raising the question of the behavior of other algorithms."}}
{"id": "Xy4aI_jeSQP", "cdate": 1577836800000, "mdate": 1682364669684, "content": {"title": "The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training", "abstract": "Modern neural networks are often operated in a strongly overparametrized regime: they comprise so many parameters that they can interpolate the training set, even if actual labels are replaced by purely random ones. Despite this, they achieve good prediction error on unseen data: interpolating the training set does not lead to a large generalization error. Further, overparametrization appears to be beneficial in that it simplifies the optimization landscape. Here we study these phenomena in the context of two-layers neural networks in the neural tangent (NT) regime. We consider a simple data model, with isotropic covariates vectors in $d$ dimensions, and $N$ hidden neurons. We assume that both the sample size $n$ and the dimension $d$ are large, and they are polynomially related. Our first main result is a characterization of the eigenstructure of the empirical NT kernel in the overparametrized regime $Nd\\gg n$. This characterization implies as a corollary that the minimum eigenvalue of the empirical NT kernel is bounded away from zero as soon as $Nd\\gg n$, and therefore the network can exactly interpolate arbitrary labels in the same regime. Our second main result is a characterization of the generalization error of NT ridge regression including, as a special case, min-$\\ell_2$ norm interpolation. We prove that, as soon as $Nd\\gg n$, the test error is well approximated by the one of kernel ridge regression with respect to the infinite-width kernel. The latter is in turn well approximated by the error of polynomial ridge regression, whereby the regularization parameter is increased by a `self-induced' term related to the high-degree components of the activation function. The polynomial degree depends on the sample size and the dimension (in particular on $\\log n/\\log d$)."}}
{"id": "zmN6ZOZx8wD", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Selective Overview of Deep Learning", "abstract": "Deep learning has arguably achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, we introduce common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new characteristics of deep learning (including depth and over-parametrization) and explain their practical and theoretical benefits. We also sample recent results on theories of deep learning, many of which are only suggestive. While a complete understanding of deep learning remains elusive, we hope that our perspectives and discussions serve as a stimulus for new statistical research."}}
{"id": "3uMSA7sW1ep", "cdate": 1514764800000, "mdate": 1682364669682, "content": {"title": "Near-Optimal Bounds for Phase Synchronization", "abstract": ""}}
{"id": "TR7ZHVvQpGV", "cdate": 1483228800000, "mdate": null, "content": {"title": "An $\\ell_{\\infty}$ Eigenvector Perturbation Bound and Its Application", "abstract": "In statistics and machine learning, we are interested in the eigenvectors (or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc). However, those matrices are usually perturbed by noises or statistical errors, either from random sampling or structural patterns. The Davis- Kahan $\\sin \\theta$ theorem is often used to bound the difference between the eigenvectors of a matrix $A$ and those of a perturbed matrix $\\widetilde{A} = A + E$, in terms of $\\ell_2$ norm. In this paper, we prove that when $A$ is a low-rank and incoherent matrix, the $\\ell_{\\infty}$ norm perturbation bound of singular vectors (or eigenvectors in the symmetric case) is smaller by a factor of $\\sqrt{d_1}$ or $\\sqrt{d_2}$ for left and right vectors, where $d_1$ and $d_2$ are the matrix dimensions. The power of this new perturbation result is shown in robust covariance estimation, particularly when random variables have heavy tails. There, we propose new robust covariance estimators and establish their asymptotic properties using the newly developed perturbation bound. Our theoretical results are verified through extensive numerical experiments."}}
{"id": "2-UU-b9jNDV", "cdate": 1451606400000, "mdate": null, "content": {"title": "Differentially Private Data Releasing for Smooth Queries", "abstract": "In the past few years, differential privacy has become a standard concept in the area of privacy. One of the most important problems in this field is to answer queries while preserving differential privacy. In spite of extensive studies, most existing work on differentially private query answering assumes the data are discrete (i.e., in $\\{0,1\\}^d$) and focuses on queries induced by \\emph{Boolean} functions. In real applications however, continuous data are at least as common as binary data. Thus, in this work we explore a less studied topic, namely, differential privately query answering for continuous data with continuous function. As a first step towards the continuous case, we study a natural class of linear queries on continuous data which we refer to as smooth queries. A linear query is said to be $K$-smooth if it is specified by a function defined on $[-1,1]^d$ whose partial derivatives up to order $K$ are all bounded. We develop two $\\epsilon$-differentially private mechanisms which are able to answer all smooth queries. The first mechanism outputs a summary of the database and can then give answers to the queries. The second mechanism is an improvement of the first one and it outputs a synthetic database. The two mechanisms both achieve an accuracy of $O (n^{-\\frac{K}{2d+K}}/\\epsilon )$. Here we assume that the dimension $d$ is a constant. It turns out that even in this parameter setting (which is almost trivial in the discrete case), using existing discrete mechanisms to answer the smooth queries is difficult and requires more noise. Our mechanisms are based on $L_{\\infty}$-approximation of (transformed) smooth functions by low-degree even trigonometric polynomials with uniformly bounded coefficients. We also develop practically efficient variants of the mechanisms with promising experimental results."}}
{"id": "_D0bL8yTZgh", "cdate": 1388534400000, "mdate": null, "content": {"title": "Differentially Private Data Releasing for Smooth Queries with Synthetic Database Output", "abstract": "We consider accurately answering smooth queries while preserving differential privacy. A query is said to be $K$-smooth if it is specified by a function defined on $[-1,1]^d$ whose partial derivatives up to order $K$ are all bounded. We develop an $\\epsilon$-differentially private mechanism for the class of $K$-smooth queries. The major advantage of the algorithm is that it outputs a synthetic database. In real applications, a synthetic database output is appealing. Our mechanism achieves an accuracy of $O (n^{-\\frac{K}{2d+K}}/\\epsilon )$, and runs in polynomial time. We also generalize the mechanism to preserve $(\\epsilon, \\delta)$-differential privacy with slightly improved accuracy. Extensive experiments on benchmark datasets demonstrate that the mechanisms have good accuracy and are efficient."}}
