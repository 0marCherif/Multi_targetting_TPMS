{"id": "50MVMMduuVT", "cdate": 1672531200000, "mdate": 1681652160045, "content": {"title": "Approximation of a Pareto Set Segment Using a Linear Model with Sharing Variables", "abstract": ""}}
{"id": "a_yFkJ4-uEK", "cdate": 1663850050303, "mdate": null, "content": {"title": "Data-efficient Supervised Learning is Powerful for Neural Combinatorial Optimization", "abstract": "Neural combinatorial optimization (NCO) is a promising learning-based approach to solve difficult combinatorial optimization problems. However, how to efficiently train a powerful NCO solver remains challenging. The widely-used reinforcement learning method suffers from sparse rewards and low data efficiency, while the supervised learning approach requires a large number of high-quality solutions. In this work, we develop efficient methods to extract sufficient supervised information from limited labeled data, which can significantly overcome the main shortcoming of supervised learning. To be specific, we propose a set of efficient data augmentation methods and a novel bidirectional loss to better leverage the equivalent properties of problem instances, which finally lead to a promising supervised learning approach. The thorough experimental studies demonstrate our proposed method can achieve state-of-the-art performance on the traveling salesman problem (TSP) only with a small set of 50,000 labeled instances, while it also enjoys better generalization performance. We believe this somewhat surprising finding could lead to valuable rethinking on the value of efficient supervised learning for NCO."}}
{"id": "vriLTB2-O0G", "cdate": 1652737536055, "mdate": null, "content": {"title": "Pareto Set Learning for Expensive Multi-Objective Optimization", "abstract": "Expensive multi-objective optimization problems can be found in many real-world applications, where their objective function evaluations involve expensive computations or physical experiments. It is desirable to obtain an approximate Pareto front with a limited evaluation budget. Multi-objective Bayesian optimization (MOBO) has been widely used for finding a finite set of Pareto optimal solutions. However, it is well-known that the whole Pareto set is on a continuous manifold and can contain infinite solutions. The structural properties of the Pareto set are not well exploited in existing MOBO methods, and the finite-set approximation may not contain the most preferred solution(s) for decision-makers. This paper develops a novel learning-based method to approximate the whole Pareto set for MOBO, which generalizes the decomposition-based multi-objective optimization algorithm (MOEA/D) from finite populations to models. We design a simple and powerful acquisition search method based on the learned Pareto set, which naturally supports batch evaluation. In addition, with our proposed model, decision-makers can readily explore any trade-off area in the approximate Pareto set for flexible decision-making. This work represents the first attempt to model the Pareto set for expensive multi-objective optimization. Experimental results on different synthetic and real-world problems demonstrate the effectiveness of our proposed method. "}}
{"id": "T2P9YAx5LMs", "cdate": 1650104026033, "mdate": 1650104026033, "content": {"title": "Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization", "abstract": "Multiobjective combinatorial optimization (MOCO) problems can be found in many real-world applications. However, exactly solving these problems would be very challenging, particularly when they are NP-hard. Many handcrafted heuristic methods have been proposed to tackle different MOCO problems over the past decades. In this work, we generalize the idea of neural combinatorial optimization, and develop a learning-based approach to approximate the whole Pareto set for a given MOCO problem without further search procedure. We propose a single preference-conditioned model to directly generate approximate Pareto solutions for any trade-off preference, and design an efficient multiobjective reinforcement learning algorithm to train this model. Our proposed method can be treated as a learning-based extension for the widely-used decomposition-based multiobjective evolutionary algorithm (MOEA/D). It uses a single model to accommodate all the possible preferences, whereas other methods use a finite number of solution to approximate the Pareto set. Experimental results show that our proposed method significantly outperforms some other methods on the multiobjective traveling salesman problem, multiobjective vehicle routing problem and multiobjective knapsack problem in terms of solution quality, speed, and model efficiency."}}
{"id": "J_WNtHoU7h", "cdate": 1640995200000, "mdate": 1668754887195, "content": {"title": "Pareto Set Learning for Expensive Multi-Objective Optimization", "abstract": "Expensive multi-objective optimization problems can be found in many real-world applications, where their objective function evaluations involve expensive computations or physical experiments. It is desirable to obtain an approximate Pareto front with a limited evaluation budget. Multi-objective Bayesian optimization (MOBO) has been widely used for finding a finite set of Pareto optimal solutions. However, it is well-known that the whole Pareto set is on a continuous manifold and can contain infinite solutions. The structural properties of the Pareto set are not well exploited in existing MOBO methods, and the finite-set approximation may not contain the most preferred solution(s) for decision-makers. This paper develops a novel learning-based method to approximate the whole Pareto set for MOBO, which generalizes the decomposition-based multi-objective optimization algorithm (MOEA/D) from finite populations to models. We design a simple and powerful acquisition search method based on the learned Pareto set, which naturally supports batch evaluation. In addition, with our proposed model, decision-makers can readily explore any trade-off area in the approximate Pareto set for flexible decision-making. This work represents the first attempt to model the Pareto set for expensive multi-objective optimization. Experimental results on different synthetic and real-world problems demonstrate the effectiveness of our proposed method."}}
{"id": "JMscM6S_tmz", "cdate": 1640995200000, "mdate": 1665907743222, "content": {"title": "Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization", "abstract": "Multiobjective combinatorial optimization (MOCO) problems can be found in many real-world applications. However, exactly solving these problems would be very challenging, particularly when they are..."}}
{"id": "QuObT9BTWo", "cdate": 1632875624632, "mdate": null, "content": {"title": "Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization", "abstract": "Multiobjective combinatorial optimization (MOCO) problems can be found in many real-world applications. However, exactly solving these problems would be very challenging, particularly when they are NP-hard. Many handcrafted heuristic methods have been proposed to tackle different MOCO problems over the past decades. In this work, we generalize the idea of neural combinatorial optimization, and develop a learning-based approach to approximate the whole Pareto set for a given MOCO problem without further search procedure. We propose a single preference-conditioned model to directly generate approximate Pareto solutions for any trade-off preference, and design an efficient multiobjective reinforcement learning algorithm to train this model. Our proposed method can be treated as a learning-based extension for the widely-used decomposition-based multiobjective evolutionary algorithm (MOEA/D). It uses a single model to accommodate all the possible preferences, whereas other methods use a finite number of solutions to approximate the Pareto set. Experimental results show that our proposed method significantly outperforms some other methods on the multiobjective traveling salesman problem, multiobjective vehicle routing problem, and multiobjective knapsack problem in terms of solution quality, speed, and model efficiency."}}
{"id": "Dc4lxJzndA", "cdate": 1609459200000, "mdate": 1665907743079, "content": {"title": "Template NeRF: Towards Modeling Dense Shape Correspondences from Category-Specific Object Images", "abstract": "We present neural radiance fields (NeRF) with templates, dubbed Template-NeRF, for modeling appearance and geometry and generating dense shape correspondences simultaneously among objects of the same category from only multi-view posed images, without the need of either 3D supervision or ground-truth correspondence knowledge. The learned dense correspondences can be readily used for various image-based tasks such as keypoint detection, part segmentation, and texture transfer that previously require specific model designs. Our method can also accommodate annotation transfer in a one or few-shot manner, given only one or a few instances of the category. Using periodic activation and feature-wise linear modulation (FiLM) conditioning, we introduce deep implicit templates on 3D data into the 3D-aware image synthesis pipeline NeRF. By representing object instances within the same category as shape and appearance variation of a shared NeRF template, our proposed method can achieve dense shape correspondences reasoning on images for a wide range of object classes. We demonstrate the results and applications on both synthetic and real-world data with competitive results compared with other methods based on 3D information."}}
{"id": "5mhViEOQxaV", "cdate": 1601308157343, "mdate": null, "content": {"title": "Controllable Pareto Multi-Task Learning", "abstract": "A multi-task learning (MTL) system aims at solving multiple related tasks at the same time.  With a fixed model capacity, the tasks would be conflicted with each other, and the system usually has to make a trade-off among learning all of them together. Multiple models with different preferences over tasks have to be trained and stored for many real-world applications where the trade-off has to be made online. This work proposes a novel controllable Pareto multi-task learning framework, to enable the system to make real-time trade-off switch among different tasks with a single model. To be specific, we formulate the MTL as a preference-conditioned multiobjective optimization problem, for which there is a parametric mapping from the preferences to the Pareto stationary solutions. A single hypernetwork-based multi-task neural network is built to learn all tasks with different trade-off preferences among them, where the hypernetwork generates the model parameters conditioned on the preference. At the inference time, MTL practitioners can easily control the model performance based on different trade-off preferences in real-time. Experiments on different applications demonstrate that the proposed model is efficient for solving various multi-task learning problems. "}}
{"id": "nXbWevR4_2E", "cdate": 1577836800000, "mdate": 1665907743107, "content": {"title": "Fast Covariance Matrix Adaptation for Large-Scale Black-Box Optimization", "abstract": "Covariance matrix adaptation evolution strategy (CMA-ES) is a successful gradient-free optimization algorithm. Yet, it can hardly scale to handle high-dimensional problems. In this paper, we propose a fast variant of CMA-ES (Fast CMA-ES) to handle large-scale black-box optimization problems. We approximate the covariance matrix by a low-rank matrix with a few vectors and use two of them to generate each new solution. The algorithm achieves linear internal complexity on the dimension of search space. We illustrate that the covariance matrix of the underlying distribution can be considered as an ensemble of simple models constructed by two vectors. We experimentally investigate the algorithm\u2019s behaviors and performances. It is more efficient than the CMA-ES in terms of running time. It outperforms or performs comparatively to the variant limited memory CMA-ES on large-scale problems. Finally, we evaluate the algorithm\u2019s performance with a restart strategy on the CEC\u20192010 large-scale global optimization benchmarks, and it shows remarkable performance and outperforms the large-scale variants of the CMA-ES."}}
