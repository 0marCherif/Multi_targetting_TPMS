{"id": "yukdkyeIFUB", "cdate": 1640995200000, "mdate": 1675523834955, "content": {"title": "Split-word Architecture in Recurrent Neural Networks POS-Tagging", "abstract": "We analyze Recurrent Neural Network (RNN) architectures to handle the problem of Part-of-Speech (POS) Tagging. When linguistic rules are inserted ad-hoc into the decision algorithm, there is a difficulty in understanding the role of prior information and learning. The real potential of recurrent networks is demonstrated in this paper on the Italian language in a purely data-driven approach, where we can reach the state-of-the-art on the UD_Italian-ISTD (Italian Stanford Dependency Treebank) dataset in comparison to TINT. We propose a methodology for splitting words that are mapped to embedding spaces and fed to forward-backward networks."}}
{"id": "TaN1I7wOvKj", "cdate": 1640995200000, "mdate": 1675523834955, "content": {"title": "A Unifying View of Estimation and Control Using Belief Propagation With Application to Path Planning", "abstract": "The use of estimation techniques on stochastic models to solve control problems is an emerging paradigm that falls under the rubric of Active Inference (AI) and Control as Inference (CAI). In this work, we use probability propagation on factor graphs to show that various algorithms proposed in the literature can be seen as specific composition rules in a factor graph. We show how this unified approach, presented both in probability space and in log of the probability space, provides a very general framework that includes the Sum-product, the Max-product, Dynamic programming and mixed Reward/Entropy criteria-based algorithms. The framework also expands algorithmic design options that lead to new smoother or sharper policy distributions. We propose original recursions such as: a generalized Sum/Max-product algorithm, a Smooth Dynamic programming algorithm and a modified versions of the Reward/Entropy algorithm. The discussion is carried over with reference to a path planning problem where the recursions that arise from various cost functions, although they may appear similar in scope, bear noticeable differences. We provide a comprehensive table of composition rules and a comparison through simulations, first on a synthetic small grid with a single goal with obstacles, and then on a grid extrapolated from a real-world scene with multiple goals and a semantic map."}}
{"id": "du-FxbkbDj3", "cdate": 1609459200000, "mdate": 1626514127663, "content": {"title": "Optimized realization of Bayesian networks in reduced normal form using latent variable model", "abstract": "Bayesian networks in their Factor Graph Reduced Normal Form are a powerful paradigm for implementing inference graphs. Unfortunately, the computational and memory costs of these networks may be considerable even for relatively small networks, and this is one of the main reasons why these structures have often been underused in practice. In this work, through a detailed algorithmic and structural analysis, various solutions for cost reduction are proposed. Moreover, an online version of the classic batch learning algorithm is also analysed, showing very similar results in an unsupervised context but with much better performance; which may be essential if multi-level structures are to be built. The solutions proposed, together with the possible online learning algorithm, are included in a C++ library that is quite efficient, especially if compared to the direct use of the well-known sum-product and Maximum Likelihood algorithms. The results obtained are discussed with particular reference to a Latent Variable Model structure."}}
{"id": "_1vtH8ing3", "cdate": 1609459200000, "mdate": 1675523834953, "content": {"title": "Considerations about learning Word2Vec", "abstract": "Despite the large diffusion and use of embedding generated through Word2Vec, there are still many open questions about the reasons for its results and about its real capabilities. In particular, to our knowledge, no author seems to have analysed in detail how learning may be affected by the various choices of hyperparameters. In this work, we try to shed some light on various issues focusing on a typical dataset. It is shown that the learning rate prevents the exact mapping of the co-occurrence matrix, that Word2Vec is unable to learn syntactic relationships, and that it does not suffer from the problem of overfitting. Furthermore, through the creation of an ad-hoc network, it is also shown how it is possible to improve Word2Vec directly on the analogies, obtaining very high accuracy without damaging the pre-existing embedding. This analogy-enhanced Word2Vec may be convenient in various NLP scenarios, but it is used here as an optimal starting point to evaluate the limits of Word2Vec."}}
{"id": "UxVEZ6H2hLh", "cdate": 1609459200000, "mdate": 1626514127638, "content": {"title": "A Unified View of Algorithms for Path Planning Using Probabilistic Inference on Factor Graphs", "abstract": "Even if path planning can be solved using standard techniques from dynamic programming and control, the problem can also be approached using probabilistic inference. The algorithms that emerge using the latter framework bear some appealing characteristics that qualify the probabilistic approach as a powerful alternative to the more traditional control formulations. The idea of using estimation on stochastic models to solve control problems is not new and the inference approach considered here falls under the rubric of Active Inference (AI) and Control as Inference (CAI). In this work, we look at the specific recursions that arise from various cost functions that, although they may appear similar in scope, bear noticeable differences, at least when applied to typical path planning problems. We start by posing the path planning problem on a probabilistic factor graph, and show how the various algorithms translate into specific message composition rules. We then show how this unified approach, presented both in probability space and in log space, provides a very general framework that includes the Sum-product, the Max-product, Dynamic programming and mixed Reward/Entropy criteria-based algorithms. The framework also expands algorithmic design options for smoother or sharper policy distributions, including generalized Sum/Max-product algorithm, a Smooth Dynamic programming algorithm and modified versions of the Reward/Entropy recursions. We provide a comprehensive table of recursions and a comparison through simulations, first on a synthetic small grid with a single goal with obstacles, and then on a grid extrapolated from a real-world scene with multiple goals and a semantic map."}}
{"id": "eiWAi4ENues", "cdate": 1577836800000, "mdate": 1626514127638, "content": {"title": "Intent Classification in Question-Answering Using LSTM Architectures", "abstract": "Question-answering (QA) is certainly the best known and probably also one of the most complex problem within Natural Language Processing (NLP) and artificial intelligence (AI). Since the complete solution to the problem of finding a generic answer still seems far away, the wisest thing to do is to break down the problem by solving single simpler parts. Assuming a modular approach to the problem, we confine our research to intent classification for an answer, given a question. Through the use of an LSTM network, we show how this type of classification can be approached effectively and efficiently, and how it can be properly used within a basic prototype responder."}}
{"id": "XOBvt3r5REH", "cdate": 1577836800000, "mdate": 1626514127640, "content": {"title": "An Analysis of Word2Vec for the Italian Language", "abstract": "Word representation is fundamental in NLP tasks, because it is precisely from the coding of semantic closeness between words that it is possible to think of teaching a machine to understand text. Despite the spread of word embedding concepts, still few are the achievements in linguistic contexts other than English. In this work, analysing the semantic capacity of the Word2Vec algorithm, an embedding for the Italian language is produced. Parameter setting such as the number of epochs, the size of the context window and the number of negatively backpropagated samples is explored."}}
{"id": "UZkmpxLfJGh", "cdate": 1577836800000, "mdate": 1626514127519, "content": {"title": "Path Planning Using Probability Tensor Flows", "abstract": "Probability models have been proposed in the literature to account for \"intelligent\" behavior in many contexts. In this paper, probability propagation is applied to model agent's motion in potentially complex scenarios that include goals and obstacles. The backward flow provides precious background information to the agent's behavior, viz., inferences coming from the future determine the agent's actions. Probability tensors are layered in time in both directions in a manner similar to convolutional neural networks. The discussion is carried out with reference to a set of simulated grids where, despite the apparent task complexity, a solution, if feasible, is always found. The original model proposed by Attias has been extended to include non-absorbing obstacles, multiple goals and multiple agents. The emerging behaviors are very realistic and demonstrate great potentials of the application of this framework to real environments."}}
{"id": "TPLmzRap4Kv", "cdate": 1577836800000, "mdate": 1626514127525, "content": {"title": "Context Analysis Using a Bayesian Normal Graph", "abstract": "Contextual information can be used to help object detection in video and images, or to categorize text. In this work we demonstrate how the Latent Variable Model, expressed as a Factor Graph in Reduced Normal Form, can manage contextual information to support a scene understanding task. In an unsupervised scenario our model learns how various objects can coexist, by associating object variables to a latent Bayesian cluster. The model, that is implemented using probabilistic message propagation, can be used to correct or to assign labels to new images."}}
{"id": "2NlZ0jM-5Fp", "cdate": 1577836800000, "mdate": 1626514127638, "content": {"title": "Probing a Deep Neural Network", "abstract": "We report a number of experimentsPalmieri, Francesco A. N.\u00a0on a deep convolutional network in order to gain a better understanding of the transformations thatBaldi, Mario\u00a0emerge from learning at the various layers. We analyze the backward flow and the reconstructed images, using an adaptive maskingBuonanno, Amedeo\u00a0approach in which pooling and nonlinearities at the various layers are represented by data-dependent binary masks. We focus on the field of view of specific neurons, also using random parameters, inGennaro, Giovanni Di\u00a0order to understand the nature of the information that flows through the activation\u2019s\u201choles\" that emerge in the multi-layer structure when an image is presented at the input. We show how the peculiarityOspedale, Francesco\u00a0of the multi-layer structure is not so much in the learned parameters, but in the patterns of connectivity that are partly imposed and then learned. Furthermore, a deep network appears to focus more on statistics, such as gradient-like transformations, rather than on filters matched to image patterns. Our probes seem to explain why classical image processing algorithms, such as the famous SIFT, have provided robust, although limited, solutions to image recognition tasks."}}
