{"id": "vJHPos_G2jT", "cdate": 1609459200000, "mdate": 1634394533780, "content": {"title": "Minimax and Neyman-Pearson Meta-Learning for Outlier Languages", "abstract": "Edoardo Maria Ponti, Rahul Aralikatte, Disha Shrivastava, Siva Reddy, Anders S\u00f8gaard. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021."}}
{"id": "nuV8HhpT9X3", "cdate": 1609459200000, "mdate": 1634230129043, "content": {"title": "Joint Semantic Analysis with Document-Level Cross-Task Coherence Rewards", "abstract": "Coreference resolution and semantic role labeling are NLP tasks that capture different aspects of semantics, indicating respectively, which expressions refer to the same entity, and what semantic roles expressions serve in the sentence. However, they are often closely interdependent, and both generally necessitate natural language understanding. Do they form a coherent abstract representation of documents? We present a neural network architecture for joint coreference resolution and semantic role labeling for English, and train graph neural networks to model the 'coherence' of the combined shallow semantic graph. Using the resulting coherence score as a reward for our joint semantic analyzer, we use reinforcement learning to encourage global coherence over the document and between semantic annotations. This leads to improvements on both tasks in multiple datasets from different domains, and across a range of encoders of different expressivity, calling, we believe, for a more holistic approach for semantics in NLP."}}
{"id": "jmebUSD-gb", "cdate": 1609459200000, "mdate": 1634394534281, "content": {"title": "Itihasa: A large-scale corpus for Sanskrit to English translation", "abstract": "This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset."}}
{"id": "iWYWF9suBdH", "cdate": 1609459200000, "mdate": 1634394535199, "content": {"title": "Minimax and Neyman-Pearson Meta-Learning for Outlier Languages", "abstract": "Model-agnostic meta-learning (MAML) has been recently put forth as a strategy to learn resource-poor languages in a sample-efficient fashion. Nevertheless, the properties of these languages are often not well represented by those available during training. Hence, we argue that the i.i.d. assumption ingrained in MAML makes it ill-suited for cross-lingual NLP. In fact, under a decision-theoretic framework, MAML can be interpreted as minimising the expected risk across training languages (with a uniform prior), which is known as Bayes criterion. To increase its robustness to outlier languages, we create two variants of MAML based on alternative criteria: Minimax MAML reduces the maximum risk across languages, while Neyman-Pearson MAML constrains the risk in each language to a maximum threshold. Both criteria constitute fully differentiable two-player games. In light of this, we propose a new adaptive optimiser solving for a local approximation to their Nash equilibrium. We evaluate both model variants on two popular NLP tasks, part-of-speech tagging and question answering. We report gains for their average and minimum performance across low-resource languages in zero- and few-shot settings, compared to joint multi-source transfer and vanilla MAML."}}
{"id": "hbEb5pkeXC2", "cdate": 1609459200000, "mdate": 1634230128909, "content": {"title": "Multilingual Compositional Wikidata Questions", "abstract": "Semantic parsing (SP) allows humans to leverage vast knowledge resources through natural interaction. However, parsers are mostly designed for and evaluated on English resources, such as CFQ (Keysers et al., 2020), the current standard benchmark based on English data generated from grammar rules and oriented towards Freebase, an outdated knowledge base. We propose a method for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata. We introduce such a dataset, which we call Multilingual Compositional Wikidata Questions (MCWQ), and use it to analyze the compositional generalization of semantic parsers in Hebrew, Kannada, Chinese and English. While within-language generalization is comparable across languages, experiments on zero-shot cross-lingual transfer demonstrate that cross-lingual compositional generalization fails, even with state-of-the-art pretrained multilingual encoders. Furthermore, our methodology, dataset and results will facilitate future research on SP in more realistic and diverse settings than has been possible with existing resources."}}
{"id": "esaszGM3_In", "cdate": 1609459200000, "mdate": 1634394533911, "content": {"title": "Ellipsis Resolution as Question Answering: An Evaluation", "abstract": "Rahul Aralikatte, Matthew Lamm, Daniel Hardt, Anders S\u00f8gaard. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."}}
{"id": "XYZY7JMjrgX", "cdate": 1609459200000, "mdate": 1634402253662, "content": {"title": "Focus Attention: Promoting Faithfulness and Diversity in Summarization", "abstract": "Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, Ryan McDonald. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "MP7Sik0JmFz", "cdate": 1609459200000, "mdate": 1634402253884, "content": {"title": "Focus Attention: Promoting Faithfulness and Diversity in Summarization", "abstract": "Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on \\rouge and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-$k$ or nucleus sampling-based decoding methods."}}
{"id": "Kwc0JTEuSbf", "cdate": 1609459200000, "mdate": 1634394533779, "content": {"title": "Itihasa: A large-scale corpus for Sanskrit to English translation", "abstract": "Rahul Aralikatte, Miryam de Lhoneux, Anoop Kunchukuttan, Anders S\u00f8gaard. Proceedings of the 8th Workshop on Asian Translation (WAT2021). 2021."}}
{"id": "4ujkOevOmxX", "cdate": 1609459200000, "mdate": 1629976965081, "content": {"title": "How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task", "abstract": "Rahul Aralikatte, H\u00e9ctor Ricardo Murrieta Bello, Miryam de Lhoneux, Daniel Hershcovich, Marcel Bollmann, Anders S\u00f8gaard. Proceedings of the 8th Workshop on Asian Translation (WAT2021). 2021."}}
