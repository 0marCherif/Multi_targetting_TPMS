{"id": "EGAJFa1KcPD", "cdate": 1672531200000, "mdate": 1681652002476, "content": {"title": "Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate Policies", "abstract": ""}}
{"id": "4FSfANJp8Qx", "cdate": 1652737665579, "mdate": null, "content": {"title": "Sharp Analysis of Stochastic Optimization under Global Kurdyka-Lojasiewicz Inequality", "abstract": "We study the complexity of finding the global solution to stochastic nonconvex optimization when the objective function satisfies global Kurdyka-{\\L}ojasiewicz (KL) inequality and the queries from stochastic gradient oracles satisfy mild expected smoothness assumption.  We first introduce a general framework to analyze Stochastic Gradient Descent (SGD) and its associated nonlinear dynamics under the setting.  As a byproduct of our analysis, we obtain a sample complexity of  $\\mathcal{O}(\\epsilon^{-(4-\\alpha)/\\alpha})$ for SGD when the objective satisfies the so called $\\alpha$-P{\\L} condition, where $\\alpha$ is the degree of gradient domination. Furthermore, we show that a modified SGD with variance reduction and restarting (PAGER) achieves an improved sample complexity of $\\mathcal{O}(\\epsilon^{-2/\\alpha})$ when the objective satisfies the average smoothness assumption. This leads to the first optimal algorithm for the important case of $\\alpha=1$ which appears in applications such as policy optimization in reinforcement learning. "}}
{"id": "lerqLAE8NKc", "cdate": 1652457276661, "mdate": 1652457276661, "content": {"title": "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation", "abstract": "We propose and study a new class of gradient communication mechanisms for communication- efficient training\u2014three point compressors (3PC)\u2014as well as efficient distributed nonconvex optimization algorithms that can take advantage of them. Unlike most established approaches, which rely on a static compressor choice (e.g., Top-\ud835\udc3e), our class allows the compressors to evolve throughout the training process, with the aim of improving the theoretical communication complexity and practical efficiency of the underly- ing methods. We show that our general approach can recover the recently proposed state-of-the-art error feedback mechanism EF21 (Richta \u0301rik et al., 2021) and its theoretical properties as a special case, but also leads to a number of new efficient methods. Notably, our approach allows us to improve upon the state of the art in the algorithmic and theoretical foundations of the lazy aggregation literature (Chen et al., 2018). As a by-product that may be of independent interest, we provide a new and fundamental link between the lazy aggregation and error feedback literature. A special feature of our work is that we do not require the compressors to be unbiased."}}
{"id": "j_6qFnOdjdT", "cdate": 1640995200000, "mdate": 1682322271301, "content": {"title": "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation", "abstract": "We propose and study a new class of gradient compressors for communication-efficient training\u2014three point compressors (3PC)\u2014as well as efficient distributed nonconvex optimization algorithms that c..."}}
{"id": "gsKFGxIe5VG", "cdate": 1640995200000, "mdate": 1648811519302, "content": {"title": "3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation", "abstract": "We propose and study a new class of gradient communication mechanisms for communication-efficient training -- three point compressors (3PC) -- as well as efficient distributed nonconvex optimization algorithms that can take advantage of them. Unlike most established approaches, which rely on a static compressor choice (e.g., Top-$K$), our class allows the compressors to {\\em evolve} throughout the training process, with the aim of improving the theoretical communication complexity and practical efficiency of the underlying methods. We show that our general approach can recover the recently proposed state-of-the-art error feedback mechanism EF21 (Richt\\'arik et al., 2021) and its theoretical properties as a special case, but also leads to a number of new efficient methods. Notably, our approach allows us to improve upon the state of the art in the algorithmic and theoretical foundations of the {\\em lazy aggregation} literature (Chen et al., 2018). As a by-product that may be of independent interest, we provide a new and fundamental link between the lazy aggregation and error feedback literature. A special feature of our work is that we do not require the compressors to be unbiased."}}
{"id": "9QVL5oEzvB", "cdate": 1640995200000, "mdate": 1683878875617, "content": {"title": "Sharp Analysis of Stochastic Optimization under Global Kurdyka-Lojasiewicz Inequality", "abstract": "We study the complexity of finding the global solution to stochastic nonconvex optimization when the objective function satisfies global Kurdyka-{\\L}ojasiewicz (KL) inequality and the queries from stochastic gradient oracles satisfy mild expected smoothness assumption. We first introduce a general framework to analyze Stochastic Gradient Descent (SGD) and its associated nonlinear dynamics under the setting. As a byproduct of our analysis, we obtain a sample complexity of $\\mathcal{O}(\\epsilon^{-(4-\\alpha)/\\alpha})$ for SGD when the objective satisfies the so called $\\alpha$-P{\\L} condition, where $\\alpha$ is the degree of gradient domination. Furthermore, we show that a modified SGD with variance reduction and restarting (PAGER) achieves an improved sample complexity of $\\mathcal{O}(\\epsilon^{-2/\\alpha})$ when the objective satisfies the average smoothness assumption. This leads to the first optimal algorithm for the important case of $\\alpha=1$ which appears in applications such as policy optimization in reinforcement learning."}}
{"id": "miA4AkGK00R", "cdate": 1632875440308, "mdate": null, "content": {"title": "EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback", "abstract": "First proposed by Seide et al (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is $O(1/T^{2/3})$, the rate of gradient descent in the same regime is $O(1/T)$). Recently, Richt\\'{a}rik et al (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21: partial participation, stochastic approximation, variance reduction, proximal setting, momentum and bidirectional compression. Our extensions are supported by strong convergence theory in the smooth nonconvex and also Polyak-\u0141ojasiewicz regimes. Several of these techniques were never analyzed in conjunction with EF before, and in cases where they were (e.g., bidirectional compression), our rates are vastly superior."}}
{"id": "8ygF02Zm51q", "cdate": 1621629762448, "mdate": null, "content": {"title": "EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback", "abstract": "Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of supervised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-$k$. First proposed by Seide et al [2014] as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. While these early breakthroughs were followed by a steady stream of works offering various improvements and generalizations, the current theoretical understanding of EF is still very limited. Indeed, to the best of our knowledge, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such as global boundedness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the introduction of additional unbiased compressors, which increase the communication cost. In this work we fix all these deficiencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms EF in practice. Moreover, our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast $\\mathcal{O}(1/T)$  convergence rate for smooth nonconvex problems, beating the previous bound of $\\mathcal{O}(1/T^{2/3})$, which was shown under a strong bounded gradients assumption. We further improve this to a fast linear rate for Polyak-Lojasiewicz functions, which is the first linear convergence result for an error feedback method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, will have a large impact on the practice of communication efficient distributed learning.\t"}}
{"id": "uyrTgJIngSE", "cdate": 1609459200000, "mdate": 1683881816782, "content": {"title": "Optimizing Static Linear Feedback: Gradient Method", "abstract": ""}}
{"id": "rzhZH8FGrhq", "cdate": 1609459200000, "mdate": 1653601130145, "content": {"title": "EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback", "abstract": "Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of supervised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-$k$. First proposed by Seide et al [2014] as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. While these early breakthroughs were followed by a steady stream of works offering various improvements and generalizations, the current theoretical understanding of EF is still very limited. Indeed, to the best of our knowledge, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such as global boundedness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the introduction of additional unbiased compressors, which increase the communication cost. In this work we fix all these deficiencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms EF in practice. Moreover, our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast $\\mathcal{O}(1/T)$ convergence rate for smooth nonconvex problems, beating the previous bound of $\\mathcal{O}(1/T^{2/3})$, which was shown under a strong bounded gradients assumption. We further improve this to a fast linear rate for Polyak-Lojasiewicz functions, which is the first linear convergence result for an error feedback method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, will have a large impact on the practice of communication efficient distributed learning."}}
