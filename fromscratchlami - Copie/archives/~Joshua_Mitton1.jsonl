{"id": "0zlLhfG6rxI", "cdate": 1652737406928, "mdate": null, "content": {"title": "Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres", "abstract": "We develop a new type of model for solving the task of inverting the transmission effects of multi-mode optical fibres through the construction of an $\\mathrm{SO}^{+}(2,1)$-equivariant neural network. This model takes advantage of the of the azimuthal correlations known to exist in fibre speckle patterns and naturally accounts for the difference in spatial arrangement between input and speckle patterns. In addition, we use a second post-processing network to remove circular artifacts, fill gaps, and sharpen the images, which is required due to the nature of optical fibre transmission. This two stage approach allows for the inspection of the predicted images produced by the more robust physically motivated equivariant model, which could be useful in a safety-critical application, or by the output of both models, which produces high quality images. Further, this model can scale to previously unachievable resolutions of imaging with multi-mode optical fibres and is demonstrated on $256 \\times 256$ pixel images. This is a result of improving the trainable parameter requirement from $\\mathcal{O}(N^4)$ to $\\mathcal{O}(m)$, where $N$ is pixel size and $m$ is number of fibre modes. Finally, this model generalises to new images, outside of the set of training data classes, better than previous models."}}
{"id": "7oyVOECcrt", "cdate": 1632875535828, "mdate": null, "content": {"title": "Local Permutation Equivariance For Graph Neural Networks", "abstract": "In this work we develop a new method, named {\\it locally permutation-equivariant graph neural networks}, which provides a framework for building graph neural networks that operate on local node neighbourhoods, through sub-graphs, while using permutation equivariant update functions. The potential benefits of learning on graph-structured data are vast, and relevant to many application domains. However, one of the challenges, is that graphs are not always of the same size, and often each node in a graph does not have the same connectivity. This necessitates that the update function must be flexible to the input size, which is not the case in most other domains.\n\nUsing our locally permutation-equivariant graph neural networks ensures an expressive update function through using permutation representations, while operating on a lower-dimensional space than that utilised in global permutation equivariance. Furthermore, the use of local update functions offers a significant improvement in GPU memory over global methods. We demonstrate that our method can outperform competing methods on a set of widely used graph benchmark classification tasks."}}
