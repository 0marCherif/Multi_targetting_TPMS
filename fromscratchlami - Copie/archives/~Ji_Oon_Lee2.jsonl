{"id": "XEtSPr06Zhb", "cdate": 1609459200000, "mdate": 1649827223036, "content": {"title": "Detection of Signal in the Spiked Rectangular Models", "abstract": "We consider the problem of detecting signals in the rank-one signal-plus-noise data matrix models that generalize the spiked Wishart matrices. We show that the principal component analysis can be improved by pre-transforming the matrix entries if the noise is non-Gaussian. As an intermediate step, we prove a sharp phase transition of the largest eigenvalues of spiked rectangular matrices, which extends the Baik-Ben Arous-P\\'ech\\'e (BBP) transition. We also propose a hypothesis test to detect the presence of signal with low computational complexity, based on the linear spectral statistics, which minimizes the sum of the Type-I and Type-II errors when the noise is Gaussian."}}
{"id": "Fc4eu8ETtiK", "cdate": 1609459200000, "mdate": 1649827223015, "content": {"title": "Detection of Signal in the Spiked Rectangular Models", "abstract": "We consider the problem of detecting signals in the rank-one signal-plus-noise data matrix models that generalize the spiked Wishart matrices. We show that the principal component analysis can be i..."}}
{"id": "L5jQnqi_nk5", "cdate": 1577836800000, "mdate": 1649827223016, "content": {"title": "Weak Detection in the Spiked Wigner Model with General Rank", "abstract": "We study the statistical decision process of detecting the signal from a `signal+noise' type matrix model with an additive Wigner noise. We propose a hypothesis test based on the linear spectral statistics of the data matrix, which does not depend on the distribution of the signal or the noise. The test is optimal under the Gaussian noise if the signal-to-noise ratio is small, as it minimizes the sum of the Type-I and Type-II errors. Under the non-Gaussian noise, the test can be improved with an entrywise transformation to the data matrix. We also introduce an algorithm that estimates the rank of the signal when it is not known a priori."}}
{"id": "6fZwa7SEcr", "cdate": 1577836800000, "mdate": null, "content": {"title": "Weak Detection in the Spiked Wigner Model with General Rank.", "abstract": "We study the statistical decision process of detecting the signal from a `signal+noise' type matrix model with an additive Wigner noise. We propose a hypothesis test based on the linear spectral statistics of the data matrix, which does not depend on the distribution of the signal or the noise. The test is optimal under the Gaussian noise if the signal-to-noise ratio is small, as it minimizes the sum of the Type-I and Type-II errors. Under the non-Gaussian noise, the test can be improved with an entrywise transformation to the data matrix. We also introduce an algorithm that estimates the rank of the signal when it is not known a priori."}}
{"id": "bBoyuJbN1ga", "cdate": 1546300800000, "mdate": null, "content": {"title": "Weak Detection of Signal in the Spiked Wigner Model.", "abstract": "We consider the problem of detecting the presence of the signal in a rank-one signal-plus-noise data matrix. In case the signal-to-noise ratio is under the threshold below which a reliable detectio..."}}
{"id": "vlXcV0j-Sv", "cdate": 1514764800000, "mdate": null, "content": {"title": "Trade-offs Between Query Difficulty and Sample Complexity in Crowdsourced Data Acquisition.", "abstract": "Consider a crowdsourcing system whose task is to classify k objects in a database into two groups depending on the binary attributes of the objects. Here we propose a parity response model: the worker is asked to check whether the number of objects having a given attribute in the chosen subset is even or odd. A worker either responds with a correct binary answer or declines to respond. We propose a method for designing the sequence of subsets of objects to be queried so that the attributes of the objects can be identified with high probability using few (n) answers. The method is based on an analogy to the design of Fountain codes for erasure channels. We define the query difficulty d\u0305 as the average size of the query subsets and we define the sample complexity n as the minimum number of collected answers required to attain a given recovery accuracy. We obtain fundamental tradeoffs between recovery accuracy, query difficulty, and sample complexity. In particular, the necessary and sufficient sample complexity required for recovering all k attributes with high probability is n = c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> max{k, (k log k)/ d\u0305} and the sample complexity for recovering a fixed proportion (1 - \u03b4)k of the attributes for \u03b4 = o(1) is n = c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> max{k, (klog(1/\u03b4))/ d\u0305}, where c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> , c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> > 0."}}
{"id": "sSq5voVlGQA", "cdate": 1514764800000, "mdate": 1649827223009, "content": {"title": "Trade-offs Between Query Difficulty and Sample Complexity in Crowdsourced Data Acquisition", "abstract": "Consider a crowdsourcing system whose task is to classify k objects in a database into two groups depending on the binary attributes of the objects. Here we propose a parity response model: the worker is asked to check whether the number of objects having a given attribute in the chosen subset is even or odd. A worker either responds with a correct binary answer or declines to respond. We propose a method for designing the sequence of subsets of objects to be queried so that the attributes of the objects can be identified with high probability using few (n) answers. The method is based on an analogy to the design of Fountain codes for erasure channels. We define the query difficulty d\u0305 as the average size of the query subsets and we define the sample complexity n as the minimum number of collected answers required to attain a given recovery accuracy. We obtain fundamental tradeoffs between recovery accuracy, query difficulty, and sample complexity. In particular, the necessary and sufficient sample complexity required for recovering all k attributes with high probability is n = c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> max{k, (k log k)/ d\u0305} and the sample complexity for recovering a fixed proportion (1 - \u03b4)k of the attributes for \u03b4 = o(1) is n = c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> max{k, (klog(1/\u03b4))/ d\u0305}, where c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> , c <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> > 0."}}
{"id": "YLdfwSbR9d", "cdate": 1514764800000, "mdate": 1649827223009, "content": {"title": "Parity Crowdsourcing for Cooperative Labeling", "abstract": "Consider a query-based data acquisition problem that aims to recover the values of $k$ binary variables from parity (XOR) measurements of chosen subsets of the variables. Assume the response model where only a randomly selected subset of the measurements is received. We propose a method for designing a sequence of queries so that the variables can be identified with high probability using as few ($n$) measurements as possible. We define the query difficulty $\\bar{d}$ as the average size of the query subsets and the sample complexity $n$ as the minimum number of measurements required to attain a given recovery accuracy. We obtain fundamental trade-offs between recovery accuracy, query difficulty, and sample complexity. In particular, the necessary and sufficient sample complexity required for recovering all $k$ variables with high probability is $n = c_0 \\max\\{k, (k \\log k)/\\bar{d}\\}$ and the sample complexity for recovering a fixed proportion $(1-\\delta)k$ of the variables for $\\delta=o(1)$ is $n = c_1\\max\\{k, (k \\log(1/\\delta))/\\bar{d}\\}$, where $c_0, c_1>0$."}}
{"id": "JlvB1Br-lY", "cdate": 1514764800000, "mdate": null, "content": {"title": "Parity Crowdsourcing for Cooperative Labeling.", "abstract": "Consider a query-based data acquisition problem that aims to recover the values of $k$ binary variables from parity (XOR) measurements of chosen subsets of the variables. Assume the response model where only a randomly selected subset of the measurements is received. We propose a method for designing a sequence of queries so that the variables can be identified with high probability using as few ($n$) measurements as possible. We define the query difficulty $\\bar{d}$ as the average size of the query subsets and the sample complexity $n$ as the minimum number of measurements required to attain a given recovery accuracy. We obtain fundamental trade-offs between recovery accuracy, query difficulty, and sample complexity. In particular, the necessary and sufficient sample complexity required for recovering all $k$ variables with high probability is $n = c_0 \\max\\{k, (k \\log k)/\\bar{d}\\}$ and the sample complexity for recovering a fixed proportion $(1-\\delta)k$ of the variables for $\\delta=o(1)$ is $n = c_1\\max\\{k, (k \\log(1/\\delta))/\\bar{d}\\}$, where $c_0, c_1>0$."}}
{"id": "-bkXqAHdYaq", "cdate": 1514764800000, "mdate": 1649827223011, "content": {"title": "Fundamental Limits on Data Acquisition: Trade-offs Between Sample Complexity and Query Difficulty", "abstract": "We consider query-based data acquisition and the corresponding information recovery problem, where the goal is to recover k binary variables (information bits) from parity measurements of those variables. The queries and the corresponding parity measurements are designed using the encoding rule of Fountain codes. By using Fountain codes, we can design potentially limitless number of queries, and corresponding parity measurements, and guarantee that the original k information bits can be recovered with high probability from any sufficiently large set of measurements of size n. In the query design, the average number of information bits that is associated with one parity measurement is called query difficulty (d\u0305) and the minimum number of measurements required to recover the k information bits for a fixed d\u0305 is called sample complexity (n). We analyze the fundamental trade-offs between the query difficulty and the sample complexity, and show that the sample complexity of n = c max{k,(k log k)/d\u0305} for some constant c > 0 is necessary and sufficient to recover k information bits with high probability as k\u2192\u221e."}}
