{"id": "wwpobuQQuXM", "cdate": 1672531200000, "mdate": 1681651747757, "content": {"title": "Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering", "abstract": ""}}
{"id": "vymVIjeh5i", "cdate": 1672531200000, "mdate": 1681651747546, "content": {"title": "Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks", "abstract": ""}}
{"id": "vmSVIVHYNF", "cdate": 1672531200000, "mdate": 1675107068308, "content": {"title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense", "abstract": "Deep Learning backdoor attacks have a threat model similar to traditional cyber attacks. Attack forensics, a critical counter-measure for traditional cyber attacks, is hence of importance for defending model backdoor attacks. In this paper, we propose a novel model backdoor forensics technique. Given a few attack samples such as inputs with backdoor triggers, which may represent different types of backdoors, our technique automatically decomposes them to clean inputs and the corresponding triggers. It then clusters the triggers based on their properties to allow automatic attack categorization and summarization. Backdoor scanners can then be automatically synthesized to find other instances of the same type of backdoor in other models. Our evaluation on 2,532 pre-trained models, 10 popular attacks, and comparison with 9 baselines show that our technique is highly effective. The decomposed clean inputs and triggers closely resemble the ground truth. The synthesized scanners substantially outperform the vanilla versions of existing scanners that can hardly generalize to different kinds of attacks."}}
{"id": "E39mJ-6cwm", "cdate": 1672531200000, "mdate": 1681586542382, "content": {"title": "Detecting Backdoors in Pre-trained Encoders", "abstract": ""}}
{"id": "1L2LPjz2yqH", "cdate": 1672531200000, "mdate": 1680903477154, "content": {"title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense", "abstract": ""}}
{"id": "Xo2E217_M4n", "cdate": 1663850525684, "mdate": null, "content": {"title": "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning", "abstract": "Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP."}}
{"id": "qHcR93949op", "cdate": 1663850514949, "mdate": null, "content": {"title": "MEDIC: Model Backdoor Removal by Importance Driven Cloning", "abstract": "We develop a novel method to remove injected backdoors in Deep Learning models. It works by cloning the benign behaviors of a trojaned model  to a new model of the same structure. It trains the clone model from scratch on a very small subset of samples and aims to minimize a cloning loss that denotes the differences between the activations of important neurons across the two models. The set of important neurons varies for each input, depending on their magnitude of activations and their impact on the classification result.\nOur experiments show that our technique can effectively remove nine different types of backdoors with minor benign accuracy degradation, outperforming the state-of-the-art backdoor removal techniques that are based on fine-tuning, knowledge distillation, and neuron pruning."}}
{"id": "oxIbD0j-GGo", "cdate": 1663850465200, "mdate": null, "content": {"title": "Dynamics Model Based Adversarial Training For Competitive Reinforcement Learning", "abstract": "Adversarial perturbations substantially degrade the performance of Deep Reinforcement Learning (DRL) agents, reducing the applicability of DRL in practice. Existing adversarial training for robustifying DRL uses the information of agent at the current step to minimize the loss upper bound introduced by adversarial input perturbations. It however only works well for single-agent tasks. The enhanced controversy in two-agent games introduces more dynamics and makes existing methods less effective. Inspired by model-based RL that builds a model for the environment transition probability, we propose a dynamics model-based adversarial training framework for modeling multi-step state transitions. Our dynamics model transitively predicts future states, which can provide more precise back-propagated future information during adversarial perturbation generation, and hence improve the agent\u2019s empirical robustness substantially under different attacks. Our experiments on four two-agent competitive MuJoCo games show that our method consistently outperforms state-of-the-art adversarial training techniques in terms of empirical robustness and normal functionalities of DRL agents."}}
{"id": "LfdEuhjR5GV", "cdate": 1663850132611, "mdate": null, "content": {"title": "Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks", "abstract": "Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems.  Traditional adversarial training method requires ground-truth labels and hence cannot be directly applied to self-supervised MDE that does not have depth ground truth. Some self-supervised model hardening technique (e.g., contrastive learning) ignores the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using the depth ground truth. We improve adversarial robustness against physical-world attacks using $L_0$-norm-bounded perturbation in training. We compare our method with supervised learning-based and contrastive learning-based methods that are tailored for MDE. Results on two representative MDE networks show that we achieve better robustness against various adversarial attacks with nearly no benign performance degradation."}}
{"id": "wqh--rbhk_", "cdate": 1640995200000, "mdate": 1681651747584, "content": {"title": "Bounded Adversarial Attack on Deep Content Features", "abstract": ""}}
