{"id": "kjPLodRa0n", "cdate": 1663850012044, "mdate": null, "content": {"title": "Revisiting Pretraining Objectives for Tabular Deep Learning", "abstract": "Recent deep learning models for tabular data currently compete with the traditional ML models based on decision trees (GBDT). Unlike GBDT, deep models can additionally benefit from pretraining, which is a workhorse of DL for vision and NLP. \nFor tabular problems, several pretraining methods were proposed, but it is not entirely clear if pretraining provides consistent noticeable improvements and what method should be used, since the methods are often not compared to each other or comparison is limited to the simplest MLP architectures.\n\nIn this work, we aim to identify the best practices to pretrain tabular DL models that can be universally applied to different datasets and architectures. \nAmong our findings, we show that using the object target labels during the pretraining stage is beneficial for the downstream performance and advocate several target-aware pretraining objectives. \nOverall, our experiments demonstrate that properly performed pretraining significantly increases the performance of tabular DL models, which often leads to their superiority over GBDTs."}}
{"id": "EJka_dVXEcr", "cdate": 1663850003049, "mdate": null, "content": {"title": "TabDDPM: Modelling Tabular Data with Diffusion Models", "abstract": "Denoising diffusion probabilistic models are currently becoming the leading paradigm of generative modeling for many important data modalities. Being the most prevalent in the computer vision community, diffusion models have also recently gained some attention for other domains, including speech, NLP, and graph-like data. In this work, we investigate if the framework of diffusion models can be advantageous for general tabular problems, where datapoints are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data makes it quite challenging for accurate modeling, since the individual features can be of completely different nature, i.e., some of them can be continuous and some of them can be discrete. To address such data types, we introduce TabDDPM --- a diffusion model that can be universally applied to any tabular dataset and handles any types of features. We extensively evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority over existing GAN/VAE alternatives, which is consistent with the advantage of diffusion models in other fields. Additionally, we show that TabDDPM can be successfully used in privacy-oriented setups, where the original datapoints cannot be shared."}}
{"id": "pfI7u0eJAIr", "cdate": 1652737537744, "mdate": null, "content": {"title": "On Embeddings for Numerical Features in Tabular Deep Learning", "abstract": "Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with gradient boosted decision trees (GBDT) on some GBDT-friendly benchmarks (that is, where GBDT outperforms conventional DL models). We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial for many backbones, not only for Transformers. Specifically, after proper embeddings, simple MLP-like models can perform on par with the attention-based architectures. Overall, we highlight embeddings for numerical features as an important design aspect with good potential for further improvements in tabular DL. The source code is available at https://github.com/Yura52/tabular-dl-num-embeddings"}}
{"id": "uVNtMSkYR-w", "cdate": 1640995200000, "mdate": 1681202638494, "content": {"title": "TabDDPM: Modelling Tabular Data with Diffusion Models", "abstract": ""}}
{"id": "pCeo8F1HUUN", "cdate": 1640995200000, "mdate": 1681202638495, "content": {"title": "Label-Efficient Semantic Segmentation with Diffusion Models", "abstract": ""}}
{"id": "hGffrcPFP2", "cdate": 1640995200000, "mdate": 1681202638380, "content": {"title": "On Embeddings for Numerical Features in Tabular Deep Learning", "abstract": ""}}
{"id": "82pWFzIsgmO", "cdate": 1640995200000, "mdate": 1681202638381, "content": {"title": "Revisiting Pretraining Objectives for Tabular Deep Learning", "abstract": ""}}
{"id": "SlxSY2UZQT", "cdate": 1632875626167, "mdate": null, "content": {"title": "Label-Efficient Semantic Segmentation with Diffusion Models", "abstract": "Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision. "}}
{"id": "i_Q1yrOegLY", "cdate": 1621630184361, "mdate": null, "content": {"title": "Revisiting Deep Learning Models for Tabular Data", "abstract": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.\n\nIn this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution. The source code is available at https://github.com/yandex-research/rtdl."}}
{"id": "UFV3r0S6zis", "cdate": 1609459200000, "mdate": 1681202638380, "content": {"title": "Revisiting Deep Learning Models for Tabular Data", "abstract": ""}}
