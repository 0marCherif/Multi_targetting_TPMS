{"id": "DvxH_RCnSj3", "cdate": 1621630097270, "mdate": null, "content": {"title": "Implicit Task-Driven Probability Discrepancy Measure for Unsupervised Domain Adaptation", "abstract": "Probability discrepancy measure is a fundamental construct for numerous machine learning models such as weakly supervised learning and generative modeling.  However, most measures overlook the fact that the distributions are not the end-product of learning, but are the basis of downstream predictor.  Therefore it is important to warp the probability discrepancy measure towards the end tasks, and we hence propose a new bi-level optimization based approach so that the two distributions are compared not uniformly against the entire hypothesis space, but only with respect to the optimal predictor for the downstream end task.  When applied to margin disparity discrepancy and contrastive domain discrepancy, our method significantly improves the performance in unsupervised domain adaptation, and enjoys a much more principled training process."}}
{"id": "Dd7IUaRSIYl", "cdate": 1620327403848, "mdate": null, "content": {"title": "CONTRASTIVE UNSUPERVISED LEARNING FOR SPEECH EMOTION RECOGNITION", "abstract": "Speech emotion recognition (SER) is a key technology to enable more natural human-machine communication. How- ever, SER has long suffered from a lack of public large-scale labeled datasets. To circumvent this problem, we investi- gate how unsupervised representation learning on unlabeled datasets can benefit SER. We show that the contrastive pre- dictive coding (CPC) method can learn salient representations from unlabeled datasets, which improves emotion recogni- tion performance. In our experiments, this method achieved state-of-the-art concordance correlation coefficient (CCC) performance for all emotion primitives (activation, valence, and dominance) on IEMOCAP. Additionally, on the MSP- Podcast dataset, our method obtained considerable perfor- mance improvements compared to baselines."}}
{"id": "rAx2hQRp3km", "cdate": 1620327268186, "mdate": null, "content": {"title": "Proximal Mapping for Deep Regularization", "abstract": "Underpinning the success of deep learning is effective regularizations that allow a variety of priors in data to be modeled. For example, robustness to adversarial perturbations, and correlations between multiple modalities. However, most regu- larizers are specified in terms of hidden layer outputs, which are not themselves optimization variables. In contrast to prevalent methods that optimize them indi- rectly through model weights, we propose inserting proximal mapping as a new layer to the deep network, which directly and explicitly produces well regularized hidden layer outputs. The resulting technique is shown well connected to kernel warping and dropout, and novel algorithms were developed for robust temporal learning and multiview modeling, both outperforming state-of-the-art methods"}}
{"id": "H1edV6VKvS", "cdate": 1569439024210, "mdate": null, "content": {"title": "ProxNet: End-to-End Learning of  Structured Representation by Proximal Mapping", "abstract": "Underpinning the success of deep learning is the effective regularization that allows a broad range of structures in data to be compactly modeled in a deep architecture.  Examples include transformation invariances, robustness to adversarial/random perturbations, and correlations between multiple modalities. However, most existing methods incorporate such priors either by auto-encoders, whose result is used to initialize supervised learning, or by augmenting the data with exemplifications of the transformations which, despite the improved performance of  supervised learning, leaves it unclear whether the learned latent representation does encode the desired regularities. To address these issues, this work proposes an \\emph{end-to-end} representation learning framework that allows prior structures to be encoded \\emph{explicitly} in the hidden layers, and to be trained efficiently in conjunction with the supervised target. Our approach is based on proximal mapping in a reproducing kernel Hilbert space, and leverages differentiable optimization. The resulting technique is applied to generalize dropout and invariant kernel warping, and to develop novel algorithms for multiview modeling and robust temporal learning."}}
