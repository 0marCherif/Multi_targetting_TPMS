{"id": "Q0HBcFnOR4", "cdate": 1679902904669, "mdate": 1679902904669, "content": {"title": "Nearly Optimal Algorithms for Level Set Estimation", "abstract": "The level set estimation problem seeks to find all points in a domain $\\X$ where the value of an unknown function $f:\\X\\rightarrow \\mathbb{R}$ exceeds a threshold $\\alpha$. The estimation is based on noisy function evaluations that may be acquired at sequentially and adaptively chosen locations in $\\X$. The threshold value $\\alpha$ can either be \\emph{explicit} and provided a priori, or \\emph{implicit} and defined relative to the optimal function value, i.e.  $\\alpha = (1-\\epsilon)f(\\bx_\\ast)$ for a given $\\epsilon > 0$ where $f(\\bx_\\ast)$ is the maximal function value and is unknown.  In this work we provide a new approach to the level set estimation problem by relating it to recent adaptive experimental design methods for linear bandits in the Reproducing Kernel Hilbert Space (RKHS) setting. We assume that $f$ can be approximated by a function in the RKHS up to an unknown misspecification and provide novel algorithms for both the implicit and explicit cases in this setting with strong theoretical guarantees. Moreover, in the linear (kernel) setting, we show that our bounds are nearly optimal, namely, our upper bounds match existing lower bounds for threshold linear bandits. To our knowledge this work provides the first instance-dependent, non-asymptotic upper bounds on sample complexity of level-set estimation that match information theoretic lower bounds."}}
{"id": "R1fj6401HJF", "cdate": 1652737837547, "mdate": null, "content": {"title": "Instance-optimal PAC Algorithms for Contextual Bandits", "abstract": "In the stochastic contextual bandit setting, regret-minimizing algorithms have been extensively researched, but their instance-minimizing best-arm identification counterparts remain seldom studied. In this work, we focus on the stochastic bandit problem in the $(\\epsilon,\\delta)$-PAC setting: given a policy class $\\Pi$ the goal of the learner is to return a policy $\\pi\\in \\Pi$ whose expected reward is within $\\epsilon$ of the optimal policy with probability greater than $1-\\delta$. We characterize the first instance-dependent PAC sample complexity of contextual bandits through a quantity $\\rho_{\\Pi}$, and provide matching upper and lower bounds in terms of $\\rho_{\\Pi}$ for the agnostic and linear contextual best-arm identification settings. We show that no algorithm can be simultaneously minimax-optimal for regret minimization and instance-dependent PAC for best-arm identification. Our main result is a new instance-optimal and computationally efficient algorithm that relies on a polynomial number of calls to a cost-sensitive classification oracle. "}}
{"id": "fdyxLGHE6bU", "cdate": 1652737765046, "mdate": null, "content": {"title": "Active Learning with Safety Constraints", "abstract": "Active learning methods have shown great promise in reducing the number of samples necessary for learning. As automated learning systems are adopted into real-time, real-world decision-making pipelines, it is increasingly important that such algorithms are designed with safety in mind. In this work we investigate the complexity of learning the best safe decision in interactive environments. We reduce this problem to a safe linear bandits problem, where our goal is to find the best arm satisfying certain (unknown) safety constraints. We propose an adaptive experimental design-based algorithm, which we show efficiently trades off between the difficulty of showing an arm is unsafe vs suboptimal. To our knowledge, our results are the first on best-arm identification in linear bandits with safety constraints. In  practice, we demonstrate that this approach performs well on synthetic and real world datasets."}}
{"id": "8apIRxHxZC", "cdate": 1632875663955, "mdate": null, "content": {"title": "Learning to Actively Learn: A Robust Approach", "abstract": "This work proposes a procedure for designing algorithms for specific adaptive data collection tasks like active learning and pure-exploration multi-armed bandits. Unlike the design of traditional adaptive algorithms that rely on concentration of measure and careful analysis to justify the correctness and sample complexity of the procedure, our adaptive algorithm is learned via adversarial training over equivalence classes of problems derived from information theoretic lower bounds. In particular, a single adaptive learning algorithm is learned that competes with the best adaptive algorithm learned for each equivalence class. Our procedure takes as input just the available queries, set of hypotheses, loss function, and total query budget. This is in contrast to existing meta-learning work that learns an adaptive algorithm relative to an explicit, user-defined subset or prior distribution over problems which can be challenging to define and be mismatched to the instance encountered at test time. This work is particularly focused on the regime when the total query budget is very small, such as a few dozen, which is much smaller than those budgets typically considered by theoretically derived algorithms. We perform synthetic experiments to justify the stability and effectiveness of the training procedure, and then evaluate the method on tasks derived from real data including a noisy 20 Questions game and a joke recommendation task."}}
{"id": "aoXERVeC7cC", "cdate": 1621630098570, "mdate": null, "content": {"title": "Selective Sampling for Online Best-arm Identification", "abstract": "This work considers the problem of selective-sampling for best-arm identification. Given a set of potential options $\\mathcal{Z}\\subset\\mathbb{R}^d$, a learner aims to compute with probability greater than $1-\\delta$, $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$ where $\\theta_{\\ast}$ is unknown. At each time step, a potential measurement $x_t\\in \\mathcal{X}\\subset\\mathbb{R}^d$ is drawn IID and the learner can either choose to take the measurement, in which case they observe a noisy measurement of $x^{\\top}\\theta_{\\ast}$, or to abstain from taking the measurement and wait for a potentially more informative point to arrive in the stream. Hence the learner faces a fundamental trade-off between the number of labeled samples they take and when they have collected enough evidence to declare the best arm and stop sampling. The main results of this work precisely characterize this trade-off between labeled samples and stopping time and provide an algorithm that nearly-optimally achieves the minimal label complexity given a desired stopping time. In addition, we show that the optimal decision rule has a simple geometric form based on deciding whether a point is in an ellipse or not. Finally, our framework is general enough to capture binary classification improving upon previous works. "}}
{"id": "2MOQimZvOz", "cdate": 1620887215371, "mdate": null, "content": {"title": "Finding all epsilon good arms in stochastic bandits", "abstract": "The pure-exploration problem in stochastic multi-armed bandits aims to find one or more arms with the largest (or near largest) means. Examples include finding an\u2208-good arm, best-arm identification, top-k arm identification, and finding all arms with means above a specified threshold. However, the problem of finding all\u2208-good arms has been overlooked in past work, although arguably this may be the most natural objective in many applications. For example, a virologist may conduct preliminary laboratory experiments on a large candidate set of treatments and move all\u2208-good treatments into more expensive clinical trials. Since the ultimate clinical efficacy is uncertain, it is important to identify all\u2208-good candidates. Mathematically, the all-\u2208-good arm identification problem presents significant new challenges and surprises that do not arise in the pure-exploration objectives studied in the past. We introduce two algorithms to overcome these and demonstrate their great empirical performance on a large-scale crowd-sourced dataset of 2.2 M ratings collected by the New Yorker Caption Contest as well as a dataset testing hundreds of possible cancer drugs."}}
{"id": "YWOvz5ugKIX", "cdate": 1620887123795, "mdate": null, "content": {"title": "Improved Confidence Bounds for the Linear Logistic Model and Applications to Linear Bandits", "abstract": "We propose improved fixed-design confidence bounds for the linear logistic model. Our bounds significantly improve upon the state-of-the-art bound by Li et al. (2017) via recent developments of the self-concordant analysis of the logistic loss (Faury et al., 2020). Specifically, our confidence bound avoids a direct dependence on , where  is the minimal variance over all arms' reward distributions. In general,  scales exponentially with the norm of the unknown linear parameter . Instead of relying on this worst-case quantity, our confidence bound for the reward of any given arm depends directly on the variance of that arm's reward distribution. We present two applications of our novel bounds to pure exploration and regret minimization logistic bandits improving upon state-of-the-art performance guarantees. For pure exploration, we also provide a lower bound highlighting a dependence on  for a family of instances."}}
{"id": "Hyef2BSgLr", "cdate": 1567802793965, "mdate": null, "content": {"title": "A New Perspective on Pool-Based Active Classification and False-Discovery Control", "abstract": "In many scientific settings there is a need for adaptive experimental design to guide the process of identifying regions of the search space that contain as many true positives as possible subject to a low rate of false discoveries (i.e. false alarms). Such regions of the search space could differ drastically from a predicted set that minimizes 0/1 error and accurate identification could require very different sampling strategies. Like active learning for binary classification, this experimental design cannot be optimally chosen a priori, but rather the data must be taken sequentially and adaptively in a closed loop. However, unlike classification with 0/1 error, collecting data adaptively to find a set with high true positive rate and low false discovery rate (FDR) is not as well understood. In this paper, we provide the first provably sample efficient adaptive algorithm for this problem. Along the way, we highlight connections between classification, combinatorial bandits, and FDR control making contributions to each."}}
{"id": "rkM7lrrgIr", "cdate": 1567802603434, "mdate": null, "content": {"title": "Sequential Experimental Design for Transductive Linear Bandits", "abstract": "In this paper we introduce the \\emph{transductive linear bandit problem}: given a set of measurement vectors $\\mathcal{X}\\subset\\mathbb{R}^d$, a set of items $\\mathcal{Z}\\subset \\mathbb{R}^d$, a fixed confidence $\\delta$, and an unknown vector $\\theta^{\\ast}\\in \\mathbb{R}^d$, the goal is to infer $\\argmax_{z\\in \\mathcal{Z}} z^\\top\\theta^\\ast$ with probability $1-\\delta$ by making as few sequentially chosen noisy measurements of the form $x^\\top\\theta^{\\ast}$ as possible. When $\\mathcal{X}=\\mathcal{Z}$, this setting generalizes \\emph{linear bandits}, and \\emph{combinatorial bandits} when $\\mathcal{X}$ is the standard basis vectors and $\\mathcalZ\\subset \\{0,1\\}^d$. Such a transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages $\\mathcal{X}$ a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages $\\mathcal{Z}$ that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books a user is queried about $\\mathcal{X}$ may be restricted to well known best-sellers even though the goal might be to recommend more esoteric titles $\\mathcal{Z}$. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we provide the first non-asymptotic algorithm for linear bandits that nearly achieves the information theoretic lower bound."}}
{"id": "ByZ7Js-ObS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Firing Bandits: Optimizing Crowdfunding", "abstract": "In this paper, we model the problem of optimizing crowdfunding platforms, such as the non-profit Kiva or for-profit KickStarter, as a variant of the multi-armed bandit problem. In our setting, Bern..."}}
