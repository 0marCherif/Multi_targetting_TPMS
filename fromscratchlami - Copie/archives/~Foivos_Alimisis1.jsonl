{"id": "j4WyOz-LhYi", "cdate": 1652656890403, "mdate": 1652656890403, "content": {"title": "Distributed Principal Component Analysis with Limited Communication", "abstract": "We study efficient distributed algorithms for the fundamental problem of principal component analysis and leading eigenvector computation on the sphere, when the data are randomly distributed among a set of computational nodes. We propose a new quantized variant of Riemannian gradient descent to solve this problem, and prove that the algorithm converges with high probability under a set of necessary spherical-convexity properties. We give bounds on the number of bits transmitted by the algorithm under common initialization schemes, and investigate the dependency on the problem dimension in each case.\n\n"}}
{"id": "pDPGIhYYyjZ", "cdate": 1631450007202, "mdate": 1631450007202, "content": {"title": "Momentum Improves Optimization on Riemannian Manifolds", "abstract": "We develop a new Riemannian descent algorithm that relies on momentum to improve over existing first-order methods for geodesically convex optimization. In contrast, accelerated convergence rates proved in prior work have only been shown to hold for geodesically strongly-convex objective functions. We further extend our algorithm to geodesically weakly-quasi-convex objectives. Our proofs of convergence rely on a novel estimate sequence that illustrates the dependency of the convergence rate on the curvature of the manifold. We validate our theoretical results empirically on several optimization problems defined on the sphere and on the manifold of positive definite matrices."}}
{"id": "edCFRvlWqV", "cdate": 1621629812888, "mdate": null, "content": {"title": "Distributed Principal Component Analysis with Limited Communication", "abstract": "We study efficient distributed algorithms for the fundamental problem of principal component analysis and leading eigenvector computation on the sphere, when the data are randomly distributed among a set of computational nodes. \nWe propose a new quantized variant of Riemannian gradient descent to solve this problem, and prove that the algorithm converges with high probability under a set of necessary spherical-convexity properties. We give bounds on the number of bits transmitted by the algorithm under common initialization schemes, and investigate the dependency on the problem dimension in each case."}}
{"id": "SShtcjeElZh", "cdate": 1609459200000, "mdate": null, "content": {"title": "Communication-Efficient Distributed Optimization with Quantized Preconditioners", "abstract": "We investigate fast and communication-efficient algorithms for the classic problem of minimizing a sum of strongly convex and smooth functions that are distributed among $n$ different nodes, which can communicate using a limited number of bits. Most previous communication-efficient approaches for this problem are limited to first-order optimization, and therefore have \\emph{linear} dependence on the condition number in their communication complexity. We show that this dependence is not inherent: communication-efficient methods can in fact have sublinear dependence on the condition number. For this, we design and analyze the first communication-efficient distributed variants of preconditioned gradient descent for Generalized Linear Models, and for Newton's method. Our results rely on a new technique for quantizing both the preconditioner and the descent direction at each step of the algorithms, while controlling their convergence rate. We also validate our findings experimentally, showing fast convergence and reduced communication."}}
{"id": "0p3781bGgj", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Continuous-time Perspective for Modeling Acceleration in Riemannian Optimization", "abstract": "We propose a novel second-order ODE as the continuous-time limit of a Riemannian accelerated gradient-based method on a manifold with curvature bounded from below. This ODE can be seen as a general..."}}
