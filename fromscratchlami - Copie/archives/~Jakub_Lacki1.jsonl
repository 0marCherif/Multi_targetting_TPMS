{"id": "LpgG0C6Y75", "cdate": 1652737772496, "mdate": null, "content": {"title": "Hierarchical Agglomerative Graph Clustering in Poly-Logarithmic Depth ", "abstract": "Obtaining scalable algorithms for \\emph{hierarchical agglomerative clustering} (HAC) is of significant interest due to the massive size of real-world datasets. At the same time, efficiently parallelizing HAC is difficult due to the seemingly sequential nature of the algorithm. In this paper, we address this issue and present ParHAC, the first efficient parallel HAC algorithm with sublinear depth for the widely-used average-linkage function. In particular, we provide a $(1+\\epsilon)$-approximation algorithm for this problem on $m$ edge graphs using $\\tilde{O}(m)$ work and poly-logarithmic depth. Moreover, we show that obtaining similar bounds for \\emph{exact} average-linkage HAC is not possible under standard complexity-theoretic assumptions.\n\nWe complement our theoretical results with a comprehensive study of the ParHAC algorithm in terms of its scalability, performance, and quality, and compare with several state-of-the-art sequential and parallel baselines. On a broad set of large publicly-available real-world datasets, we find that ParHAC obtains a 50.1x speedup on average over the best sequential baseline, while achieving quality similar to the exact HAC algorithm. We also show that ParHAC can cluster one of the largest publicly available graph datasets with 124 billion edges in a little over three hours using a commodity multicore machine."}}
{"id": "qNbBjFlUPp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Faster DBSCAN via subsampled similarity queries", "abstract": "DBSCAN is a popular density-based clustering algorithm. It computes the $\\epsilon$-neighborhood graph of a dataset and uses the connected components of the high-degree nodes to decide the clusters. However, the full neighborhood graph may be too costly to compute with a worst-case complexity of $O(n^2)$. In this paper, we propose a simple variant called SNG-DBSCAN, which clusters based on a subsampled $\\epsilon$-neighborhood graph, only requires access to similarity queries for pairs of points and in particular avoids any complex data structures which need the embeddings of the data points themselves. The runtime of the procedure is $O(sn^2)$, where $s$ is the sampling rate. We show under some natural theoretical assumptions that $s \\approx \\log n/n$ is sufficient for statistical cluster recovery guarantees leading to an $O(n\\log n)$ complexity. We provide an extensive experimental analysis showing that on large datasets, one can subsample as little as $0.1\\%$ of the neighborhood graph, leading to as much as over 200x speedup and 250x reduction in RAM consumption compared to scikit-learn's implementation of DBSCAN, while still maintaining competitive clustering performance."}}
{"id": "offKjHVNsSp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fully Dynamic Matching: Beating 2-Approximation in \u0394 Update Time.\u03f5", "abstract": "In fully dynamic graphs, we know how to maintain a 2-approximation of maximum matching extremely fast, that is, in polylogarithmic update time or better. In a sharp contrast and despite extensive studies, all known algorithms that maintain a 2 \u2013 \u03a9(1) approximate matching are much slower. Understanding this gap and, in particular, determining the best possible update time for algorithms providing a better-than-2 approximate matching is a major open question. In this paper, we show that for any constant \u03f5 > 0, there is a randomized algorithm that with high probability maintains a 2 \u2013 \u03a9(1) approximate maximum matching of a fully-dynamic general graph in worst-case update time O(\u0394\u03f5 + polylog n), where \u0394 is the maximum degree. Previously, the fastest fully dynamic matching algorithm providing a better-than-2 approximation had O(m1/4) update-time [Bernstein and Stein, SODA 2016]. A faster algorithm with update-time O(n\u03f5) was known, but worked only for maintaining the size (and not the edges) of the matching in bipartite graphs [Bhattacharya, Henzinger, and Nanongkai, STOC 2016]."}}
{"id": "aEx5OWUDPOI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Near-Optimal Decremental Approximate Multi-Source Shortest Paths", "abstract": "Given a weighted undirected graph $G=(V,E,w)$, a hopset $H$ of hopbound $\\beta$ and stretch $(1+\\epsilon)$ is a set of edges such that for any pair of nodes $u, v \\in V$, there is a path in $G \\cup H$ of at most $\\beta$ hops, whose length is within a $(1+\\epsilon)$ factor from the distance between $u$ and $v$ in $G$. We show the first efficient decremental algorithm for maintaining hopsets with a polylogarithmic hopbound. The update time of our algorithm matches the best known static algorithm up to polylogarithmic factors. All the previous decremental hopset constructions had a superpolylogarithmic (but subpolynomial) hopbound of $2^{\\log^{\\Omega(1)} n}$ [Bernstein, FOCS'09; HKN, FOCS'14; Chechik, FOCS'18]. By applying our decremental hopset construction, we get improved or near optimal bounds for several distance problems. Most importantly, we show how to decrementally maintain $(2k-1)(1+\\epsilon)$-approximate all-pairs shortest paths (for any constant $k \\geq 2)$, in $\\tilde{O}(n^{1/k})$ amortized update time and $O(k)$ query time. This improves (by a polynomial factor) over the update-time of the best previously known decremental algorithm in the constant query time regime. Moreover, it improves over the result of [Chechik, FOCS'18] that has a query time of $O(\\log \\log(nW))$, where $W$ is the aspect ratio, and the amortized update time is $n^{1/k}\\cdot(\\frac{1}{\\epsilon})^{\\tilde{O}(\\sqrt{\\log n})}$. For sparse graphs our construction nearly matches the best known static running time / query time tradeoff. We also obtain near-optimal bounds for maintaining approximate multi-source shortest paths and distance sketches, and get improved bounds for approximate single-source shortest paths. Our algorithms are randomized and our bounds hold with high probability against an oblivious adversary."}}
{"id": "RrFmM7BnNdP", "cdate": 1577836800000, "mdate": null, "content": {"title": "Faster DBSCAN via subsampled similarity queries", "abstract": "DBSCAN is a popular density-based clustering algorithm. It computes the $\\epsilon$-neighborhood graph of a dataset and uses the connected components of the high-degree nodes to decide the clusters. However, the full neighborhood graph may be too costly to compute with a worst-case complexity of $O(n^2)$. In this paper, we propose a simple variant called SNG-DBSCAN, which clusters based on a subsampled $\\epsilon$-neighborhood graph, only requires access to similarity queries for pairs of points and in particular avoids any complex data structures which need the embeddings of the data points themselves. The runtime of the procedure is $O(sn^2)$, where $s$ is the sampling rate. We show under some natural theoretical assumptions that $s \\approx \\log n/n$ is sufficient for statistical cluster recovery guarantees leading to an $O(n\\log n)$ complexity. We provide an extensive experimental analysis showing that on large datasets, one can subsample as little as $0.1\\%$ of the neighborhood graph, leading to as much as over 200x speedup and 250x reduction in RAM consumption compared to scikit-learn's implementation of DBSCAN, while still maintaining competitive clustering performance."}}
{"id": "OFOEXGLD87", "cdate": 1577836800000, "mdate": null, "content": {"title": "Parallel Graph Algorithms in Constant Adaptive Rounds: Theory meets Practice", "abstract": "We study fundamental graph problems such as graph connectivity, minimum spanning forest (MSF), and approximate maximum (weight) matching in a distributed setting. In particular, we focus on the Adaptive Massively Parallel Computation (AMPC) model, which is a theoretical model that captures MapReduce-like computation augmented with a distributed hash table. We show the first AMPC algorithms for all of the studied problems that run in a constant number of rounds and use only $O(n^\\epsilon)$ space per machine, where $0 < \\epsilon < 1$. Our results improve both upon the previous results in the AMPC model, as well as the best-known results in the MPC model, which is the theoretical model underpinning many popular distributed computation frameworks, such as MapReduce, Hadoop, Beam, Pregel and Giraph. Finally, we provide an empirical comparison of the algorithms in the MPC and AMPC models in a fault-tolerant distriubted computation environment. We empirically evaluate our algorithms on a set of large real-world graphs and show that our AMPC algorithms can achieve improvements in both running time and round-complexity over optimized MPC baselines."}}
{"id": "Ka7DDyAzBj", "cdate": 1577836800000, "mdate": null, "content": {"title": "Walking randomly, massively, and efficiently", "abstract": "We introduce a set of techniques that allow for efficiently generating many independent random walks in the Massively Parallel Computation (MPC) model with space per machine strongly sublinear in the number of vertices. In this space-per-machine regime, many natural approaches to graph problems struggle to overcome the \u0398(log n) MPC round complexity barrier, where n is the number of vertices. Our techniques enable achieving this for PageRank\u2014one of the most important applications of random walks\u2014even in more challenging directed graphs, as well as for approximate bipartiteness and expansion testing. In the undirected case, we start our random walks from the stationary distribution, which implies that we approximately know the empirical distribution of their next steps. This allows for preparing continuations of random walks in advance and applying a doubling approach. As a result we can generate multiple random walks of length l in \u0398(log l) rounds on MPC. Moreover, we show that under the popular 1-vs.-2-Cycles conjecture, this round complexity is asymptotically tight. For directed graphs, our approach stems from our treatment of the PageRank Markov chain. We first compute the PageRank for the undirected version of the input graph and then slowly transition towards the directed case, considering convex combinations of the transition matrices in the process. For PageRank, we achieve the following round complexities for damping factor equal to 1 \u2212 \u0454: in O(log log n + log 1 / \u0454) rounds for undirected graphs (with \u00d5(m / \u04542) total space), in \u00d5(log2 log n + log2 1/\u0454) rounds for directed graphs (with \u00d5((m+n 1+o(1)) / poly(\u0454)) total space). The round complexity of our result for computing PageRank has only logarithmic dependence on 1/\u0454. We use this to show that our PageRank algorithm can be used to construct directed length-l random walks in O(log2 log n + log2 l) rounds with \u00d5((m+n 1+o(1)) poly(l)) total space. More specifically, by setting \u0454 = \u0398(1 / l), a length-l PageRank walk with constant probability contains no random jump, and hence is a directed random walk."}}
{"id": "JVIV29CMiMI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Simple Label-Correcting Algorithms for Partially Dynamic Approximate Shortest Paths in Directed Graphs", "abstract": "Classical single source shortest paths algorithms work by maintaining distance estimates d : V \u2192 \u211d and performing so-called edge relaxations. We call an edge uv of weight w(uv) relaxed if d(v) \u2264 d(u) + w(uv), and tense otherwise. To relax a tense edge uv means to set d(v) to d(u)+w(uv). It is known that starting from d(s) = 0, and d(v) = \u221e for all v \u2260 s, and performing edge relaxations in arbitrary order until there are no more tense edges leads to d being equal to the distances from the source s. This overall idea can be extended to a very simple incremental algorithm for maintaining shortest paths. We consider an operation which can be seen as a dual of a relaxation and study an approximate version of both operations. We show that by repeating the respective operation until convergence one obtains very simple incremental and decremental deterministic algorithms for (1 + \u220a)-approximate shortest paths in directed graphs. Specifically, we give an algorithm maintaining all-pairs approximate shortest paths in O(n3 log n log (nW)/\u220a) total update time, where the graph's edge weights come from the interval [1,W]. This is two log-factors faster than the known folklore solution obtained by combining King's decremental transitive closure algorithm [King, FOCS'99] and the h-SSSP algorithm [Bernstein, SICOMP'16] for h = 2. In addition, we give an algorithm for approximating single source shortest paths of hop-length at most h in O(mhlog(nW)/\u220a) total time. The obtained algorithm is simpler and more efficient than Bernstein's h-SSSP algorithm [Bernstein, SICOMP'16]."}}
{"id": "BEk3MIbxC0j", "cdate": 1577836800000, "mdate": null, "content": {"title": "Parallel Graph Algorithms in Constant Adaptive Rounds: Theory meets Practice", "abstract": ""}}
{"id": "BklpA5RrDV", "cdate": 1552440021083, "mdate": null, "content": {"title": "Round Compression for Parallel Matching Algorithms", "abstract": ""}}
