{"id": "VFGgG8XpFLu", "cdate": 1665069633982, "mdate": null, "content": {"title": "Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation", "abstract": "Adversarial patch attacks are an emerging security threat for real world deep learning applications. We present Demasked Smoothing, the first approach (up to our knowledge) to certify the robustness of semantic segmentation models against this threat model. Previous work on certifiably defending against patch attacks has mostly focused on image classification task and often required changes in the model architecture and additional training which is undesirable and computationally expensive. In Demasked Smoothing, any segmentation model can be applied without particular training, fine-tuning, or restriction of the architecture. Using different masking strategies, Demasked Smoothing can be applied both for certified detection and certified recovery. In extensive experiments we show that Demasked Smoothing can on average certify 63% of the pixel predictions for a 1% patch in the detection task and 46% against a 0.5% patch for the recovery task on the ADE20K dataset. "}}
{"id": "b0JxQC7JLWh", "cdate": 1663850031124, "mdate": null, "content": {"title": "Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation", "abstract": "Adversarial patch attacks are an emerging security threat for real world deep learning applications. We present Demasked Smoothing, the first approach (up to our knowledge) to certify the robustness of semantic segmentation models against this threat model. Previous work on certifiably defending against patch attacks has mostly focused on image classification task and often required changes in the model architecture and additional training which is undesirable and computationally expensive. In Demasked Smoothing, any segmentation model can be applied without particular training, fine-tuning, or restriction of the architecture. Using different masking strategies, Demasked Smoothing can be applied both for certified detection and certified recovery. In extensive experiments we show that Demasked Smoothing can on average certify 63% of the pixel predictions for a 1% patch in the detection task and 46% against a 0.5% patch for the recovery task on the ADE20K dataset. "}}
{"id": "wNSqWOGgAB", "cdate": 1640995200000, "mdate": 1681653098876, "content": {"title": "Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation", "abstract": ""}}
{"id": "a9WXj5XV5mK", "cdate": 1621630213627, "mdate": null, "content": {"title": "Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks", "abstract": "Adversarial attacks based on randomized search schemes have obtained state-of-the-art results in black-box robustness evaluation recently. However, as we demonstrate in this work, their efficiency in different query budget regimes depends on manual design and heuristic tuning of the underlying proposal distributions. We study how this issue can be addressed by adapting the proposal distribution online based on the information obtained during the attack. We consider Square Attack, which is a state-of-the-art score-based black-box attack, and demonstrate how its performance can be improved by a learned controller that adjusts the parameters of the proposal distribution online during the attack. We train the controller using gradient-based end-to-end training on a CIFAR10 model with white box access. We demonstrate that plugging the learned controller into the attack consistently improves its black-box robustness estimate in different query regimes by up to 20% for a wide range of different models with black-box access. We further show that the learned adaptation principle transfers well to the other data distributions such as CIFAR100 or ImageNet and to the targeted attack setting. "}}
{"id": "lb87f2ITxp0", "cdate": 1609459200000, "mdate": 1681653098863, "content": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "abstract": ""}}
{"id": "iQ5OjJDp70", "cdate": 1609459200000, "mdate": 1681653098875, "content": {"title": "Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks", "abstract": ""}}
{"id": "aw6U_3cCyU", "cdate": 1609459200000, "mdate": 1681653098869, "content": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "abstract": ""}}
{"id": "LB9UpnLA9B", "cdate": 1609459200000, "mdate": 1681653098876, "content": {"title": "Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks", "abstract": ""}}
{"id": "hr-3PMvDpil", "cdate": 1601308054774, "mdate": null, "content": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches."}}
