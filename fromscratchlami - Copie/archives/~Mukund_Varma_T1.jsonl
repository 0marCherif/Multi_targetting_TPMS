{"id": "xE-LtsE-xx", "cdate": 1663850153673, "mdate": null, "content": {"title": "Is Attention All That NeRF Needs?", "abstract": "We present Generalizable NeRF Transformer (GNT), a transformer-based architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to render novel views on the fly from source views. While prior works on NeRFs optimize a scene representation by inverting a handcrafted rendering equation, GNT achieves neural representation and rendering that generalizes across scenes using transformers at two stages. (1) The view transformer leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. (2) The ray transformer renders novel views using attention to decode the features from the view transformer along the sampled points during ray marching. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct NeRF without an explicit rendering formula due to the learned ray renderer. When trained on multiple scenes, GNT consistently achieves state-of-the-art performance when transferring to unseen scenes and outperform all other methods by ~10% on average. Our analysis of the learned attention maps to infer depth and occlusion indicate that attention enables learning a physically-grounded rendering. Our results show the promise of transformers as a universal modeling tool for graphics. Please refer to our project page for video results: https://vita-group.github.io/GNT/"}}
{"id": "wfKbtSjHA6F", "cdate": 1652737534381, "mdate": null, "content": {"title": "Sparse Winning Tickets are Data-Efficient Image Recognizers", "abstract": "Improving the performance of deep networks in data-limited regimes has warranted much attention. In this work, we empirically show that \u201cwinning tickets\u201d (small sub-networks) obtained via magnitude pruning based on the lottery ticket hypothesis, apart from being sparse are also effective recognizers in data-limited regimes. Based on extensive experiments, we find that in low data regimes (datasets of 50-100 examples per class), sparse winning tickets substantially outperform the original dense networks. This approach, when combined with augmentations or fine-tuning from a self-supervised backbone network, shows further improvements in performance by as much as 16% (absolute) on low-sample datasets and long-tailed classification. Further, sparse winning tickets are more robust to synthetic noise and distribution shifts compared to their dense counterparts. Our analysis of winning tickets on small datasets indicates that, though sparse, the networks retain density in the initial layers and their representations are more generalizable. Code is available at https://github.com/VITA-Group/DataEfficientLTH."}}
{"id": "4hm5ufX69jo", "cdate": 1612102784503, "mdate": null, "content": {"title": "[Re]: On the Relationship between Self-Attention and Convolutional Layers", "abstract": "\\subsection*{Scope of Reproducibility}\nIn this report, we perform a detailed study on the paper \"On the Relationship between Self-Attention and Convolutional Layers\" \\cite{attncnn} which provides theoretical and experimental evidence that self-attention layers can behave like convolutional layers. The proposed method does not obtain state-of-the-art performance but rather answers an interesting question - \\textit{do self-attention layers process images in a similar manner to convolutional layers?}. This has inspired many recent works like \\cite{zhao2020exploring, dosovitskiy2020image} which propose a fully-attentional model for image recognition. We focus on experimentally validating the claims of the original paper, highlight differences with other similar works, and propose a new variant of the attention operation - {\\em Hierarchical Attention} which shows improved performance with significantly lesser parameters. To facilitate further study, all the code used in our experiments are publicly available here\\footnote{Code available in the supplementary material during the review period.}. \n\n\\subsection*{Methodology}\nWe implement the original paper \\cite{attncnn} from scratch in Pytorch and refer to the author's source code\\footnote{\\url{https://github.com/epfml/attention-cnn}} for verification. In our experiments involving SAN \\cite{zhao2020exploring}, we utilize the official implementation\\footnote{\\url{https://github.com/hszhao/SAN}} due to the available faster CUDA kernels while we implement VIT \\cite{dosovitskiy2020image} from scratch referring to the author's source code\\footnote{\\url{https://github.com/google-research/vision_transformer}}. We then incorporate our proposed hierarchical operation in all three methods for comparison. For all our experiments mentioned in this report, we use the CIFAR10 dataset to benchmark the performance of the model in the image classification task. Each training run required around 20 hours while the corresponding hierarchical versions took around 10-12 hours for convergence in an Nvidia RTX 2060 GPU. \n\n\\subsection*{Results}\nWe were able to reproduce all the results from the paper within 1\\% of the reported value, hence validating the claims of the original paper. However, there seem to be some differences in the attention figures which lead to interesting insights and the proposed Hierarchical Attention. In the case of VIT and SAN, we do not have a comparative baseline as the corresponding papers do not evaluate performance on the CIFAR10 dataset (without pre-training). \n\n\\subsection*{What was easy}\nWe did not face any major challenges in reproducing the results in the paper. \n\n\\subsection*{What was difficult}\nMost of the code in the official implementation seems to be borrowed from HuggingFace's repository\\footnote{\\url{https://github.com/huggingface/transformers}} which also brought along a lot of unnecessary code making it difficult to read and understand quickly. Further, the training time for each run is quite significant making it difficult for us to experiment with multiple datasets and hyperparameter settings. \n\n\\subsection*{Communication with original authors}\nWe have tried contacting the authors regarding the differences in the attention figures since the code for the same was not available on the repository for verification. However, we have not received any response regarding the same. "}}
