{"id": "e8ocvO-2DH", "cdate": 1672531200000, "mdate": 1681491079634, "content": {"title": "ELF: Federated Langevin Algorithms with Primal, Dual and Bidirectional Compression", "abstract": ""}}
{"id": "Btsb0yEquoj", "cdate": 1640995200000, "mdate": 1681491087920, "content": {"title": "Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition", "abstract": ""}}
{"id": "TkhWRbYIca5", "cdate": 1577836800000, "mdate": 1652916936078, "content": {"title": "Penalized Langevin dynamics with vanishing penalty for smooth and log-concave targets", "abstract": "We study the problem of sampling from a probability distribution on $\\mathbb R^p$ defined via a convex and smooth potential function. We first consider a continuous-time diffusion-type process, termed Penalized Langevin dynamics (PLD), the drift of which is the negative gradient of the potential plus a linear penalty that vanishes when time goes to infinity. An upper bound on the Wasserstein-2 distance between the distribution of the PLD at time $t$ and the target is established. This upper bound highlights the influence of the speed of decay of the penalty on the accuracy of approximation. As a consequence, in the case of low-temperature limit we infer a new result on the convergence of the penalized gradient flow for the optimization problem."}}
{"id": "wvR0DbP7CTb", "cdate": 1546300800000, "mdate": 1652916936079, "content": {"title": "Bounding the error of discretized Langevin algorithms for non-strongly log-concave targets", "abstract": "In this paper, we provide non-asymptotic upper bounds on the error of sampling from a target density using three schemes of discretized Langevin diffusions. The first scheme is the Langevin Monte Carlo (LMC) algorithm, the Euler discretization of the Langevin diffusion. The second and the third schemes are, respectively, the kinetic Langevin Monte Carlo (KLMC) for differentiable potentials and the kinetic Langevin Monte Carlo for twice-differentiable potentials (KLMC2). The main focus is on the target densities that are smooth and log-concave on $\\mathbb R^p$, but not necessarily strongly log-concave. Bounds on the computational complexity are obtained under two types of smoothness assumption: the potential has a Lipschitz-continuous gradient and the potential has a Lipschitz-continuous Hessian matrix. The error of sampling is measured by Wasserstein-$q$ distances. We advocate for the use of a new dimension-adapted scaling in the definition of the computational complexity, when Wasserstein-$q$ distances are considered. The obtained results show that the number of iterations to achieve a scaled-error smaller than a prescribed value depends only polynomially in the dimension."}}
{"id": "N8urA-h__eQ", "cdate": 1483228800000, "mdate": 1652916936085, "content": {"title": "User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient", "abstract": "In this paper, we study the problem of sampling from a given probability density function that is known to be smooth and strongly log-concave. We analyze several methods of approximate sampling based on discretizations of the (highly overdamped) Langevin diffusion and establish guarantees on its error measured in the Wasserstein-2 distance. Our guarantees improve or extend the state-of-the-art results in three directions. First, we provide an upper bound on the error of the first-order Langevin Monte Carlo (LMC) algorithm with optimized varying step-size. This result has the advantage of being horizon free (we do not need to know in advance the target precision) and to improve by a logarithmic factor the corresponding result for the constant step-size. Second, we study the case where accurate evaluations of the gradient of the log-density are unavailable, but one can have access to approximations of the aforementioned gradient. In such a situation, we consider both deterministic and stochastic approximations of the gradient and provide an upper bound on the sampling error of the first-order LMC that quantifies the impact of the gradient evaluation inaccuracies. Third, we establish upper bounds for two versions of the second-order LMC, which leverage the Hessian of the log-density. We nonasymptotic guarantees on the sampling error of these second-order LMCs. These guarantees reveal that the second-order LMC algorithms improve on the first-order LMC in ill-conditioned settings."}}
