{"id": "rncUUL4vApq", "cdate": 1672531200000, "mdate": 1681650336895, "content": {"title": "Co-learning Planning and Control Policies Using Differentiable Formal Task Constraints", "abstract": ""}}
{"id": "LUOSN8opID1", "cdate": 1663849966006, "mdate": null, "content": {"title": "Constrained Hierarchical Deep Reinforcement Learning with Differentiable Formal Specifications", "abstract": "Formal logic specifications are a useful tool to describe desired agent behavior and have been explored as a means to shape rewards in Deep Reinforcement Learning (DRL) systems over a variety of problems and domains. Prior work, however, has failed to consider the possibility of making these specifications differentiable, which would yield a more informative signal of the objective via the specification gradient. This paper examines precisely such an approach by exploring a Lagrangian method to constrain policy updates using a differentiable style of temporal logic specifications that associates logic formulae with real-valued quantitative semantics. This constrained learning mechanism is then used in a hierarchical setting where a high-level specification-guided neural network path planner works with a low-level control policy to navigate through planned waypoints. The effectiveness of our approach is demonstrated over four robot dynamics with five different types of Linear Temporal Logic (LTL) specifications. Our demo videos are collected at https://sites.google.com/view/schrl."}}
{"id": "MNarazRfRK", "cdate": 1640995200000, "mdate": 1681650336890, "content": {"title": "Defending Observation Attacks in Deep Reinforcement Learning via Detection and Denoising", "abstract": ""}}
{"id": "AlL4U82lRbg", "cdate": 1640995200000, "mdate": 1675868791598, "content": {"title": "Model-free Neural Lyapunov Control for Safe Robot Navigation", "abstract": "Model-free Deep Reinforcement Learning (DRL) controllers have demonstrated promising results on various challenging non-linear control tasks. While a model-free DRL algorithm can solve unknown dynamics and high-dimensional problems, it lacks safety assurance. Although safety constraints can be encoded as part of a reward function, there still exists a large gap between an RL controller trained with this modified reward and a safe controller. In contrast, instead of implicitly encoding safety constraints with rewards, we explicitly colearn a Twin Neural Lyapunov Function (TNLF) with the control policy in the DRL training loop and use the learned TNLF to build a runtime monitor. Combined with the path generated from a planner, the monitor chooses appropriate waypoints that guide the learned controller to provide collision-free control trajectories. Our approach inherits the scalability advantages from DRL while enhancing safety guarantees. Our experimental evaluation demonstrates the effectiveness of our approach compared to DRL with augmented rewards and constrained DRL methods over a range of high-dimensional safety-sensitive navigation tasks."}}
{"id": "2rAirZMQ1Yp", "cdate": 1640995200000, "mdate": 1681650336913, "content": {"title": "DistSPECTRL: Distributing Specifications in Multi-Agent Reinforcement Learning Systems", "abstract": ""}}
{"id": "mXQAGP2rhC", "cdate": 1577836800000, "mdate": 1649297704417, "content": {"title": "Robustness to Adversarial Attacks in Learning-Enabled Controllers", "abstract": "Learning-enabled controllers used in cyber-physical systems (CPS) are known to be susceptible to adversarial attacks. Such attacks manifest as perturbations to the states generated by the controller's environment in response to its actions. We consider state perturbations that encompass a wide variety of adversarial attacks and describe an attack scheme for discovering adversarial states. To be useful, these attacks need to be natural, yielding states in which the controller can be reasonably expected to generate a meaningful response. We consider shield-based defenses as a means to improve controller robustness in the face of such perturbations. Our defense strategy allows us to treat the controller and environment as black-boxes with unknown dynamics. We provide a two-stage approach to construct this defense and show its effectiveness through a range of experiments on realistic continuous control domains such as the navigation control-loop of an F16 aircraft and the motion control system of humanoid robots."}}
