{"id": "2jr4h1-2tjI", "cdate": 1680280448683, "mdate": 1680280448683, "content": {"title": "Benchmarking saliency methods for chest X-ray interpretation", "abstract": "Saliency methods, which produce heat maps that highlight the areas of the medical image that influence model prediction, are often presented to clinicians as an aid in diagnostic decision-making. However, rigorous investigation of the accuracy and reliability of these strategies is necessary before they are integrated into the clinical setting. In this work, we quantitatively evaluate seven saliency methods, including Grad-CAM, across multiple neural network architectures using two evaluation metrics. We establish the first human benchmark for chest X-ray segmentation in a multilabel classification set-up, and examine under what clinical conditions saliency maps might be more prone to failure in localizing important pathologies compared with a human expert benchmark. We find that (1) while Grad-CAM generally localized pathologies better than the other evaluated saliency methods, all seven performed significantly worse compared with the human benchmark, (2) the gap in localization performance between Grad-CAM and the human benchmark was largest for pathologies that were smaller in size and had shapes that were more complex, and (3) model confidence was positively correlated with Grad-CAM localization performance. Our work demonstrates that several important limitations of saliency methods must be addressed before we can rely on them for deep learning explainability in medical imaging."}}
{"id": "KSh6BueUBJ", "cdate": 1672531200000, "mdate": 1680280062601, "content": {"title": "Don't be fooled: label leakage in explanation methods and the importance of their quantitative evaluation", "abstract": ""}}
{"id": "PjYQJEqSVJ", "cdate": 1640995200000, "mdate": 1680280062583, "content": {"title": "Learning Invariant Representations with Missing Data", "abstract": ""}}
{"id": "YrADFoblKrX", "cdate": 1635261626077, "mdate": null, "content": {"title": "Learning Invariant Representations with Missing Data", "abstract": "Spurious correlations, or *shortcuts*, allow flexible models to predict well during training but poorly on related test populations. Recent work has shown that models that satisfy particular independencies involving the correlation-inducing *nuisance* variable have guarantees on their test performance. However, enforcing such independencies requires nuisances to be observed during training.  But nuisances such as demographics or image background labels are often missing. Enforcing independence on just the observed data does not imply independence on the entire population. In this work, we derive the missing-mmd estimator used for invariance objectives under missing nuisances. On simulations and clinical data, missing-mmds enable improvements in test performance similar to those achieved by using fully-observed data."}}
{"id": "tO_6s92BaDZ", "cdate": 1633790970195, "mdate": null, "content": {"title": "Learning Invariant Representations with Missing Data", "abstract": "Spurious correlations allow flexible models to predict well during training but poorly on related test populations. Recent work has shown that models that satisfy particular independencies involving correlation-inducing nuisance variables have guarantees on their test performance. Enforcing such independencies requires nuisances to be observed during training. However, nuisances, such as demographics or image background labels, are often missing. Enforcing independence on just the observed data does not imply independence on the entire population. Here we derive MMD estimators used for invariance objectives under missing nuisances. On simulations and clinical data, optimizing through these estimates achieves test performance similar to using estimators that make use of the full data."}}
{"id": "pMWtc5NKd7V", "cdate": 1623121650467, "mdate": null, "content": {"title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports", "abstract": "Extracting structured clinical information from free-text radiology reports can enable the use of radiology report information for a variety of critical healthcare applications. In our work, we present RadGraph, a dataset of entities and relations in full-text chest X-ray radiology reports based on a novel information extraction schema we designed to structure radiology reports. We release a development dataset, which contains board-certified radiologist annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579 entities and 10,889 relations), and a test dataset, which contains two independent sets of board-certified radiologist annotations for 100 radiology reports split equally across the MIMIC-CXR and CheXpert datasets. Using these datasets, we train and test a deep learning model, RadGraph Benchmark, that achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR and CheXpert test sets respectively. Additionally, we release an inference dataset, which contains annotations automatically generated by RadGraph Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4 million relations) and 500 CheXpert reports (13,783 entities and 9,908 relations) with mappings to associated chest radiographs. Our freely available dataset can facilitate a wide range of research in medical natural language processing, as well as computer vision and multi-modal learning when linked to chest radiographs."}}
{"id": "Ud1K-l71AI2", "cdate": 1623101948420, "mdate": null, "content": {"title": "Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management", "abstract": "Recent advances in Natural Language Processing (NLP), and specifically automated Question Answering (QA) systems, have demonstrated both impressive linguistic fluency and a pernicious tendency to reflect social biases. In this study, we introduce Q-Pain, a dataset for assessing bias in medical QA in the context of pain management, one of the most challenging forms of clinical decision-making. Along with the dataset, we propose a new, rigorous framework, including a sample experimental design, to measure the potential biases present when making treatment decisions. We demonstrate its use by assessing two reference Question-Answering systems, GPT-2 and GPT-3, and find statistically significant differences in treatment between intersectional race-gender subgroups, thus reaffirming the risks posed by AI in medical settings, and the need for datasets like ours to ensure safety before medical AI applications are deployed."}}
{"id": "atRaQOIl9SY", "cdate": 1609459200000, "mdate": 1680280062588, "content": {"title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports", "abstract": ""}}
{"id": "ZsLH3krVTCe", "cdate": 1609459200000, "mdate": 1680280062580, "content": {"title": "Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management", "abstract": ""}}
{"id": "NErW-zjJ_lE", "cdate": 1609459200000, "mdate": 1680280062590, "content": {"title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports", "abstract": ""}}
