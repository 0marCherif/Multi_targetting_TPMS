{"id": "n9q6BNnDmUH", "cdate": 1677628800000, "mdate": 1681718963218, "content": {"title": "Regional Attention Network (RAN) for Head Pose and Fine-Grained Gesture Recognition", "abstract": "Affect is often expressed via non-verbal body language such as actions/gestures, which are vital indicators for human behaviors. Recent studies on recognition of fine-grained actions/gestures in monocular images have mainly focused on modeling spatial configuration of body parts representing body pose, human-objects interactions and variations in local appearance. The results show that this is a brittle approach since it relies on accurate body parts/objects detection. In this work, we argue that there exist local discriminative semantic regions, whose \u201cinformativeness\u201d can be evaluated by the attention mechanism for inferring fine-grained gestures/actions. To this end, we propose a novel end-to-end <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">regional attention network (RAN)</i> , which is a fully convolutional neural network (CNN) to combine multiple contextual regions through attention mechanism, focusing on parts of the images that are most relevant to a given task. Our regions consist of one or more consecutive cells and are adapted from the strategies used in computing HOG (Histogram of Oriented Gradient) descriptor. The model is extensively evaluated on ten datasets belonging to 3 different scenarios: 1) head pose recognition, 2) drivers state recognition, and 3) human action and facial expression recognition. The proposed approach outperforms the state-of-the-art by a considerable margin in different metrics."}}
{"id": "CqT4GN3lNXj", "cdate": 1667572275352, "mdate": 1667572275352, "content": {"title": "Unsupervised Multi-View CNN for Salient View Selection and 3D Interest Point Detection", "abstract": "We present an unsupervised 3D deep learning framework based on a ubiquitously true proposition named by us\nview-object consistency as it states that a 3D object and\nits projected 2D views always belong to the same object\nclass. To validate its effectiveness, we design a multi-view\nCNN instantiating it for salient view selection and interest\npoint detection of 3D objects, which quintessentially cannot be handled by supervised learning due to the difficulty\nof collecting sufficient and consistent training data. Our unsupervised multi-view CNN, namely UMVCNN, branches\noff two channels which encode the knowledge within each\n2D view and the 3D object respectively and also exploits\nboth intra-view and inter-view knowledge of the object. It\nends with a new loss layer which formulates the view-object\nconsistency by impelling the two channels to generate consistent classification outcomes. The UMVCNN is then integrated with a global distinction adjustment scheme to incorporate global cues into salient view selection. We evaluate our method for salient view section both qualitatively and quantitatively, demonstrating its superiority over several\nstate-of-the-art methods. In addition, we showcase that our\nmethod can be used to select salient views of 3D scenes containing multiple objects. We also develop a method based on\nthe UMVCNN for 3D interest point detection and conduct\ncomparative evaluations on a publicly available benchmark,\nwhich shows that the UMVCNN is amenable to different 3D\nshape understanding tasks."}}
{"id": "x565vaeQgf", "cdate": 1667334389372, "mdate": null, "content": {"title": "SR-GNN: Spatial Relation-Aware Graph Neural Network for Fine-Grained Image Categorization", "abstract": "Over the past few years, a significant progress has been made in deep convolutional neural networks (CNNs)-based image recognition. This is mainly due to the strong ability of such networks in mining discriminative object pose and parts information from texture and shape. This is often inappropriate for fine-grained visual classification (FGVC) since it exhibits high intra-class and low inter-class variances due to occlusions, deformation, illuminations, etc. Thus, an expressive feature representation describing global structural information is a key to characterize an object/scene. To this end, we propose a method that effectively captures subtle changes by aggregating context-aware features from most relevant image-regions and their importance in discriminating fine-grained categories avoiding the bounding-box and/or distinguishable part annotations. Our approach is inspired by the recent advancement in self-attention and graph neural networks (GNNs) approaches to include a simple yet effective relation-aware feature transformation and its refinement using a context-aware attention mechanism to boost the discriminability of the transformed feature in an end-to-end learning process. Our model is evaluated on eight benchmark datasets consisting of fine-grained objects and human-object interactions. It outperforms the state-of-the-art approaches by a significant margin in recognition accuracy."}}
{"id": "znwGru6i7e", "cdate": 1640995200000, "mdate": 1672741466932, "content": {"title": "Retinal Structure Detection in OCTA Image via Voting-Based Multitask Learning", "abstract": ""}}
{"id": "s1ECA4qvKz", "cdate": 1640995200000, "mdate": 1681718963222, "content": {"title": "Uncertainty-guided graph attention network for parapneumonic effusion diagnosis", "abstract": ""}}
{"id": "j0Jt77Zv7X", "cdate": 1640995200000, "mdate": 1681718963227, "content": {"title": "SR-GNN: Spatial Relation-Aware Graph Neural Network for Fine-Grained Image Categorization", "abstract": "Over the past few years, a significant progress has been made in deep convolutional neural networks (CNNs)-based image recognition. This is mainly due to the strong ability of such networks in mining discriminative object pose and parts information from texture and shape. This is often inappropriate for fine-grained visual classification (FGVC) since it exhibits high intra-class and low inter-class variances due to occlusions, deformation, illuminations, etc. Thus, an expressive feature representation describing global structural information is a key to characterize an object/scene. To this end, we propose a method that effectively captures subtle changes by aggregating context-aware features from most relevant image-regions and their importance in discriminating fine-grained categories avoiding the bounding-box and/or distinguishable part annotations. Our approach is inspired by the recent advancement in self-attention and graph neural networks (GNNs) approaches to include a simple yet effective relation-aware feature transformation and its refinement using a context-aware attention mechanism to boost the discriminability of the transformed feature in an end-to-end learning process. Our model is evaluated on eight benchmark datasets consisting of fine-grained objects and human-object interactions. It outperforms the state-of-the-art approaches by a significant margin in recognition accuracy."}}
{"id": "dFAyjcdI80A", "cdate": 1640995200000, "mdate": 1667572864931, "content": {"title": "Retinal Structure Detection in OCTA Image via Voting-based Multi-task Learning", "abstract": "Automated detection of retinal structures, such as retinal vessels (RV), the foveal avascular zone (FAZ), and retinal vascular junctions (RVJ), are of great importance for understanding diseases of the eye and clinical decision-making. In this paper, we propose a novel Voting-based Adaptive Feature Fusion multi-task network (VAFF-Net) for joint segmentation, detection, and classification of RV, FAZ, and RVJ in optical coherence tomography angiography (OCTA). A task-specific voting gate module is proposed to adaptively extract and fuse different features for specific tasks at two levels: features at different spatial positions from a single encoder, and features from multiple encoders. In particular, since the complexity of the microvasculature in OCTA images makes simultaneous precise localization and classification of retinal vascular junctions into bifurcation/crossing a challenging task, we specifically design a task head by combining the heatmap regression and grid classification. We take advantage of three different \\textit{en face} angiograms from various retinal layers, rather than following existing methods that use only a single \\textit{en face}. To facilitate further research, part of these datasets with the source code and evaluation benchmark have been released for public access:https://github.com/iMED-Lab/VAFF-Net."}}
{"id": "ZXXKB_kYNzq", "cdate": 1640995200000, "mdate": 1667572865061, "content": {"title": "Key Landmarks Detection of Cleft Lip-Repaired Partially Occluded Facial Images for Aesthetics Outcome Assessment", "abstract": "This paper proposes a novel method for the detection of the symmetrical axis of the cropped face required for the aesthetic outcome estimation from the facial images of patients after their cleft treatment. It firstly applies the Gaussian filter to smooth the images in order to compress noise on the subsequent tasks, then the bilateral semantic segmentation network is applied to segment the facial components out and each region is assigned a distinct colour, thirdly the Canny edge detector is applied to detect the facial feature points and all the contours are further detected and classified into three thirds according to their height. Fourthly, the centres of mass of detected feature points on the contours and the average of all these centres are used to estimate four potential symmetrical axes of the face, the one with minimum Manhattan distance from all the detected feature points is finally selected as the optimal one and used to estimate the aesthetic numerical score through the shape analysis in structural similarity measure. The experimental results based on a publicly accessible dataset shows that it performs well and better than one existing method."}}
{"id": "D5eNrr-RZoG", "cdate": 1640995200000, "mdate": 1681718963284, "content": {"title": "SR-GNN: Spatial Relation-aware Graph Neural Network for Fine-Grained Image Categorization", "abstract": "Over the past few years, a significant progress has been made in deep convolutional neural networks (CNNs)-based image recognition. This is mainly due to the strong ability of such networks in mining discriminative object pose and parts information from texture and shape. This is often inappropriate for fine-grained visual classification (FGVC) since it exhibits high intra-class and low inter-class variances due to occlusions, deformation, illuminations, etc. Thus, an expressive feature representation describing global structural information is a key to characterize an object/ scene. To this end, we propose a method that effectively captures subtle changes by aggregating context-aware features from most relevant image-regions and their importance in discriminating fine-grained categories avoiding the bounding-box and/or distinguishable part annotations. Our approach is inspired by the recent advancement in self-attention and graph neural networks (GNNs) approaches to include a simple yet effective relation-aware feature transformation and its refinement using a context-aware attention mechanism to boost the discriminability of the transformed feature in an end-to-end learning process. Our model is evaluated on eight benchmark datasets consisting of fine-grained objects and human-object interactions. It outperforms the state-of-the-art approaches by a significant margin in recognition accuracy."}}
{"id": "BbFv6rnn8E3", "cdate": 1640995200000, "mdate": 1681650213924, "content": {"title": "Unsupervised Multi-View CNN for Salient View Selection and 3D Interest Point Detection", "abstract": ""}}
