{"id": "eCyISpGXsp", "cdate": 1672531200000, "mdate": 1695965036716, "content": {"title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings", "abstract": ""}}
{"id": "--5_yWkCC1", "cdate": 1672531200000, "mdate": 1695965036775, "content": {"title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis", "abstract": ""}}
{"id": "hXzOqPlXDwm", "cdate": 1652737576470, "mdate": null, "content": {"title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation", "abstract": "Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\\url{https://github.com/chijames/KERPLE.git}."}}
{"id": "sonJCw_Qdq", "cdate": 1640995200000, "mdate": 1668484620385, "content": {"title": "PowerGym: A Reinforcement Learning Environment for Volt-Var Control in Power Distribution Systems", "abstract": "Reinforcement learning for power distribution systems has so far been studied using customized environments due to the proprietary nature of the power industry. To encourage researchers to benchmar..."}}
{"id": "qoWf9_rNsBd", "cdate": 1640995200000, "mdate": 1695965036783, "content": {"title": "Soft Actor-Critic With Integer Actions", "abstract": "Reinforcement learning is well-studied under discrete actions. Integer actions setting is popular in the industry yet still challenging due to its high dimensionality. To this end, we study reinforcement learning under integer actions by incorporating the Soft Actor-Critic (SAC) algorithm with an integer reparameterization. Our key observation for integer actions is that their discrete structure can be simplified using their comparability property. Hence, the proposed integer reparameterization does not need one-hot encoding and is of low dimensionality. Experiments show that the proposed SAC under integer actions is as good as the continuous action version on robot control tasks and outperforms Proximal Policy Optimization on power distribution systems control tasks."}}
{"id": "WBKmWXwrIW", "cdate": 1640995200000, "mdate": 1695965036775, "content": {"title": "Training Discrete Deep Generative Models via Gapped Straight-Through Estimator", "abstract": "While deep generative models have succeeded in image processing, natural language processing, and reinforcement learning, training that involves discrete random variables remains challenging due to..."}}
{"id": "ZAA0Ol4z2i4", "cdate": 1632875552692, "mdate": null, "content": {"title": "Explaining Off-Policy Actor-Critic From A Bias-Variance Perspective", "abstract": "Off-policy Actor-Critic algorithms have demonstrated phenomenal experimental performance but still require better explanations. To this end, we show its policy evaluation error on the distribution of transitions decomposes into: a Bellman error, a bias from policy mismatch, and a variance term from sampling. By comparing the magnitude of bias and variance, we explain the success of the Emphasizing Recent Experience sampling and 1/age weighted sampling. Both sampling strategies yield smaller bias and variance and are hence preferable to uniform sampling."}}
{"id": "pVZVrVHT5r1", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Contraction Approach to Model-based Reinforcement Learning", "abstract": "Despite its experimental success, Model-based Reinforcement Learning still lacks a complete theoretical understanding. To this end, we analyze the error in the cumulative reward using a contraction approach. We consider both stochastic and deterministic state transitions for continuous (non-discrete) state and action spaces. This approach doesn\u2019t require strong assumptions and can recover the typical quadratic error to the horizon. We prove that branched rollouts can reduce this error and are essential for deterministic transitions to have a Bellman contraction. Our analysis of policy mismatch error also applies to Imitation Learning. In this case, we show that GAN-type learning has an advantage over Behavioral Cloning when its discriminator is well-trained."}}
{"id": "S1lJv0VYDr", "cdate": 1569439318565, "mdate": null, "content": {"title": "Model Imitation for Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) aims to learn a dynamic model to reduce the number of interactions with real-world environments. However, due to estimation error, rollouts in the learned model, especially those of long horizon, fail to match the ones in real-world environments. This mismatching has seriously impacted the sample complexity of MBRL. The phenomenon can be attributed to the fact that previous works employ supervised learning to learn the one-step transition models, which has inherent difficulty ensuring the matching of distributions from multi-step rollouts. Based on the claim, we propose to learn the synthesized model by matching the distributions of multi-step rollouts sampled from the synthesized model and the real ones via WGAN. We theoretically show that matching the two can minimize the difference of cumulative rewards between the real transition and the learned one. Our experiments also show that the proposed model imitation method outperforms the state-of-the-art in terms of sample complexity and average return."}}
{"id": "-MEjAB_gm3O", "cdate": 1514764800000, "mdate": null, "content": {"title": "Rumor Source Detection: A Probabilistic Perspective", "abstract": "In this paper we consider the problem of rumor source detection in a network. Our main contribution is an efficient Belief-Propagation-based (BP) algorithm to compute the joint likelihood function of the source location and the spreading time for the general continuous-time Susceptible-Infected epidemic model on trees. As a result, many probabilistic detection algorithms, including the joint maximum likelihood estimator, can be implemented with time complexity being nearly linear in the product of the size of the graph and the effective range of the spreading time. This is in sharp contrast to the widely employed discrete-time epidemic models where the complexity in computing the likelihood function of the source location is exponential. To extend the BP algorithm to general graphs, we propose a \u201cGamma Generated Tree\u201d heuristic to convert the original graph to a tree with heterogeneous infection rates over edges. Compared to state-of-the-art methods, simulation results show that our algorithm provides better estimates of the source when the graph topology is similar to trees. As a byproduct, the spreading time can also be estimated, which is useful in some applications."}}
