{"id": "brXFKoT6ox", "cdate": 1672531200000, "mdate": 1695960791305, "content": {"title": "Discovering Object-Centric Generalized Value Functions From Pixels", "abstract": "Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent \"question\" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptation."}}
{"id": "8AQrtiuujE", "cdate": 1672531200000, "mdate": 1695960791285, "content": {"title": "Discovering Object-Centric Generalized Value Functions From Pixels", "abstract": "Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatica..."}}
{"id": "WfsYwoltD2", "cdate": 1664902718289, "mdate": null, "content": {"title": "The Paradox of Choice: On the Role of Attention in Hierarchical Reinforcement Learning", "abstract": "Decision-making AI agents are often faced with two important challenges: the depth of the planning horizon, and the branching factor due to having many choices. Hierarchical reinforcement learning methods aim to solve the first problem, by providing shortcuts that skip over multiple time steps. To cope with the breadth, it is desirable to restrict the agent's attention at each step to a reasonable number of possible choices. The concept of affordances (Gibson, 1977) suggests that only certain actions are feasible in certain states. In this work, we first characterize \"affordances\" as a \"hard\" attention mechanism that strictly limits the available choices of temporally extended options. We then investigate the role of hard versus soft attention in training data collection, abstract value learning in long-horizon tasks, and handling a growing number of choices. To this end, we present an online, model-free algorithm to learn affordances that can be used to further learn subgoal options. Finally, we identify and empirically demonstrate the settings in which the \"paradox of choice\" arises, i.e. when having fewer but more meaningful choices improves the learning speed and performance of a reinforcement learning agent."}}
{"id": "rSeZdKsOozq", "cdate": 1640995200000, "mdate": 1648229248122, "content": {"title": "The Paradox of Choice: Using Attention in Hierarchical Reinforcement Learning", "abstract": "Decision-making AI agents are often faced with two important challenges: the depth of the planning horizon, and the branching factor due to having many choices. Hierarchical reinforcement learning methods aim to solve the first problem, by providing shortcuts that skip over multiple time steps. To cope with the breadth, it is desirable to restrict the agent's attention at each step to a reasonable number of possible choices. The concept of affordances (Gibson, 1977) suggests that only certain actions are feasible in certain states. In this work, we model \"affordances\" through an attention mechanism that limits the available choices of temporally extended options. We present an online, model-free algorithm to learn affordances that can be used to further learn subgoal options. We investigate the role of hard versus soft attention in training data collection, abstract value learning in long-horizon tasks, and handling a growing number of choices. We identify and empirically illustrate the settings in which the paradox of choice arises, i.e. when having fewer but more meaningful choices improves the learning speed and performance of a reinforcement learning agent."}}
{"id": "c-ReZb8JLD", "cdate": 1640995200000, "mdate": 1681490375351, "content": {"title": "POMRL: No-Regret Learning-to-Plan with Increasing Horizons", "abstract": ""}}
{"id": "CR_Bbbcj6w", "cdate": 1640995200000, "mdate": 1695960791307, "content": {"title": "Towards Continual Reinforcement Learning: A Review and Perspectives", "abstract": "In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, we review and present a taxonomy of continual RL approaches. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics."}}
{"id": "xWRX16GCugt", "cdate": 1632875549219, "mdate": null, "content": {"title": "Sequoia: A Software Framework to Unify Continual Learning Research", "abstract": "The field of Continual Learning (CL) seeks to develop algorithms that accumulate knowledge and skills over time through interaction with non-stationary environments. In practice, a plethora of evaluation procedures (settings) and algorithmic solutions (methods) exist, each with their own potentially disjoint set of assumptions. This variety makes measuring progress in CL difficult. We propose a taxonomy of settings, where each setting is described as a set of assumptions. A tree-shaped hierarchy emerges from this view, where more general settings become the parents of those with more restrictive assumptions. This makes it possible to use inheritance to share and reuse research, as developing a method for a given setting also makes it directly applicable onto any of its children. We instantiate this idea as a publicly available software framework called Sequoia, which features a wide variety of settings from both the Continual Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) domains. Sequoia also includes a growing suite of methods which are easy to extend and customize, in addition to more specialized methods from external libraries. We hope that this new paradigm and its first implementation can help unify and accelerate research in CL. You can help us grow the tree by visiting (this GitHub URL)."}}
{"id": "LGvlCcMgWqb", "cdate": 1621629748761, "mdate": null, "content": {"title": "Temporally Abstract Partial Models", "abstract": "Humans and animals have the ability to reason and make predictions about different courses of action at many time scales. In reinforcement learning, option models (Sutton, Precup \\& Singh, 1999; Precup, 2000) provide the framework for this kind of temporally abstract prediction and reasoning. Natural intelligent agents are also able to focus their attention on courses of action that are relevant or feasible in a given situation, sometimes termed affordable actions. In this paper, we define a notion of affordances for options, and develop temporally abstract partial option models, that take into account the fact that an option might be affordable only in certain situations. We analyze the trade-offs between estimation and approximation error in planning and learning when using such models, and identify some interesting special cases. Additionally, we empirically demonstrate the ability to learn both affordances and partial option models online resulting in improved sample efficiency and planning time in the Taxi domain."}}
{"id": "rhVb_Fo_izc", "cdate": 1609459200000, "mdate": 1648229248122, "content": {"title": "Learning Robust State Abstractions for Hidden-Parameter Block MDPs", "abstract": "Many control tasks exhibit similar dynamics that can be modeled as having common latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs) explicitly model this structure to improve sample efficiency in multi-task settings. However, this setting makes strong assumptions on the observability of the state that limit its application in real-world scenarios with rich observation spaces. In this work, we leverage ideas of common structure from the HiP-MDP setting, and extend it to enable robust state abstractions inspired by Block MDPs. We derive instantiations of this new framework for both multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings. Further, we provide transfer and generalization bounds based on task and state similarity, along with sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work. To further demonstrate efficacy of the proposed method, we empirically compare and show improvement over multi-task and meta-reinforcement learning baselines."}}
{"id": "r6Tu5ZPsq6m", "cdate": 1609459200000, "mdate": 1681695013020, "content": {"title": "Temporally Abstract Partial Models", "abstract": "title> <link rel=\"stylesheet\" href=\"/static/papers/css/papers.css\" /> <meta name=\"citation_title\" content=\"Temporally Abstract Partial Models\" /> <meta name=\"citation_author\" content=\"Khetarpal, Khimya\" /> <meta name=\"citation_author\" content=\"Ahmed, Zafarali\" /> <meta name=\"citation_author\" content=\"Comanici, Gheorghe\" /> <meta name=\"citation_author\" content=\"Precup, Doina\" /> <meta name=\"citation_journal_title\" content=\"Advances in Neural Information Processing Systems\" /> <meta name=\"citation_volume\" content=\"34\" /> <meta name=\"citation_firstpage\" content=\"1979\" /> <meta name=\"citation_lastpage\" content=\"1991\" /> <meta name=\"citation_pdf_url\" content=\"https://proceedings.neurips.cc/paper_files/paper/2021/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf\" /> <meta name=\"citation_publication_date\" content=\"2021-12-06\" /><!-- Bootstrap CSS --> <!-- https://codepen.io/surjithctly/pen/PJqKzQ --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\" /> <link href=\"/static/menus/css/menus.css\" rel=\"stylesheet\" id=\"bootstrap-css\" /> <link rel=\"stylesheet\" href=\"https://use.fontawesome.com/releases/v5.8.1/css/all.css\" integrity=\"sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf\" crossorigin=\"anonymous\" /> <script type=\"text/javascript\" async=\"async\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML\"></script> <script type=\"text/x-mathjax-config\"> <![CDATA[ MathJax.Hub.Config({ \"tex2jax\": { \"inlineMath\": [[\"$\",\"$\"], [\"\\\\(\",\"\\\\)\"]], \"displayMath\": [[\"\\\\[\",\"\\\\]\"]], \"processEscapes\": true } } ); ]]> </script> <style> <![CDATA[ @media (prefers-color-scheme: dark) { body { background-color: #333; color: #eee; } } .btn-spacer { margin: 2px; } .footer { position: fixed; left: 0; bottom: 0; width: 100%; background-color: #eee; color: black; } ]]> </style> <nav class=\"navbar navbar-expand-md navbar-light bg-light\"> <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarToggler6\" aria-controls=\"navbarToggler6\" aria-expanded=\"false\" aria-label=\"Toggle navigation\"><span class=\"navbar-toggler-icon\"></span></button> <div class=\"collapse navbar-collapse\" id=\"navbarToggler6\"> <a class=\"navbar-brand\" href=\"/\">NeurIPS Proceedings</a> <ul class=\"navbar-nav mr-auto mt-2 mt-md-0\"> <li class=\"nav-item\"> <a class=\"nav-link\" href=\"/admin/login/?next=/admin/\"><i class=\"fas fa-sign-in-alt\" title=\"Login\"></i></a> <li class=\"nav-item\"> <a class=\"nav-link\" href=\"/admin/logout/?nextp=/admin\"><i class=\"fas fa-sign-out-alt\" title=\"Logout\"></i></a> <form class=\"form-inline my-2 my-lg-0\" method=\"get\" role=\"search\" action=\"/papers/search\"> <input class=\"form-control mr-sm-2\" type=\"text\" name=\"q\" placeholder=\"Search\" aria-label=\"Search\" id=\"navsearch\" /> <button class=\"btn btn-outline-success my-2 my-sm-0\" type=\"submit\">Search</button>"}}
