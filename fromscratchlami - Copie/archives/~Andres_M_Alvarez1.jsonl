{"id": "Jg4O6wOOfZ", "cdate": 1620676325392, "mdate": null, "content": {"title": "Enhanced automatic twin support vector machine for imbalanced data classification", "abstract": "Most of the classification approaches assume that the sample distribution among classes is balanced. Still, such an assumption leads to biased performance over the majority class. This paper proposes an enhanced automatic twin support vector machine \u2013 (EATWSVM) to deal with imbalanced data, which incorporates a kernel representation within a TWSVM-based optimization. To learn the kernel function, we impose a Gaussian similarity, ruled by a Mahalanobis distance, and couple a centered kernel alignment-based approach to improving the data separability. Besides, we suggest a suitable range to fix the regularization parameters concerning both the dataset\u2019 imbalance ratio and overlap. Lastly, we adopt One-vs-One and One-vs-Rest frameworks to extend our EATWSVM formulation for multi-class tasks. Obtained results on synthetic and real-world datasets show that our approach outperforms state-of-the-art methods concerning classification performance and training time."}}
{"id": "utQhYZgzhH", "cdate": 1620676269834, "mdate": null, "content": {"title": "Learning from multiple inconsistent and dependent annotators to support classification tasks", "abstract": "Training a supervised learning model requires a labeled dataset, where the labeling process is usually carried out by an expert to provide the ground truth/gold standard for each sample. However, in many applications, such a gold standard is not available. Instead, several real-world scenarios give us access to annotations provided by crowds, holding different and unknown levels of expertise. Thus, Learning from crowds is a subject undergoing intense study, and its main aim is to manage various machine learning paradigms in the presence of multiple annotators. Most of the state-of-the-art approaches reside on two key assumptions: i) the labeler\u2019s performance does not depend on the input feature space, and ii) independence among the annotators is imposed. Here, we introduce a localized kernel alignment-based annotator relevance analysis (LKAAR) to code each labeler\u2019s expertise as the matching between the feature and label spaces. Namely, LKAAR takes into account inter-annotators dependencies and models labelers\u2019 knowledge as a function of the input feature space. Experimental results devoted to classification, show that LKAAR achieves suitable performances from inconsistent labelers, even if the gold standard is not available."}}
{"id": "lR566EUHUtQ", "cdate": 1620676215069, "mdate": null, "content": {"title": "Single-Trial Kernel-Based Functional Connectivity for Enhanced Feature Extraction in Motor-Related Tasks", "abstract": "Motor learning is associated with functional brain plasticity, involving specific functional connectivity changes in the neural networks. However, the degree of learning new motor skills varies among individuals, which is mainly due to the between-subject variability in brain structure and function captured by electroencephalographic (EEG) recordings. Here, we propose a kernel-based functional connectivity measure to deal with inter/intra-subject variability in motor-related tasks. To this end, from spatio-temporal-frequency patterns, we extract the functional connectivity between EEG channels through their Gaussian kernel cross-spectral distribution. Further, we optimize the spectral combination weights within a sparse-based \u21132-norm feature selection framework matching the motor-related labels that perform the dimensionality reduction of the extracted connectivity features. From the validation results in three databases with motor imagery and motor execution tasks, we conclude that the single-trial Gaussian functional connectivity measure provides very competitive classifier performance values, being less affected by feature extraction parameters, like the sliding time window, and avoiding the use of prior linear spatial filtering. We also provide interpretability for the clustered functional connectivity patterns and hypothesize that the proposed kernel-based metric is promising for evaluating motor skills. "}}
{"id": "MboxjOP_1AN", "cdate": 1620676145601, "mdate": null, "content": {"title": "Relevant information undersampling to support imbalanced data classification", "abstract": "Traditional classification algorithms suppose that the sample distribution among classes is balanced. Yet, such an assumption leads to biased performance over the majority class. This paper proposes a Relevant Information-based UnderSampling (RIUS) approach to select the most relevant examples from the majority class to improve the classification performance for imbalanced data scenarios. RIUS builds on the information-preservation principle that extracts the majority class\u2019s underlying structure with fewer samples. Additionally, we couple our RIUS approach to the well-known Clustering-based Undersampling algorithm (CBUS) to enhance the data representation, and named this RIUS enhancement as CRIUS. Experimental results show that RIUS and CRIUS reveal the data\u2019s relevant structure and reduce the loss of information by selecting the most informative instances."}}
{"id": "sGPCSXckmAS", "cdate": 1620676081286, "mdate": null, "content": {"title": "Spatial interpretability of time-frequency relevance optimized in motor imagery discrimination using Deep&Wide networks", "abstract": "Medical diagnosis and monitoring benefit from exploiting the advantages of motor imagery (MI) training, which highly depends on the proper interpretation of elicited brain activity responses. Convolutional neural networks (CNN) are increasingly used to improve MI classification performance by employing multi-view extracted features (time, frequency, and spatial). However, deep learning gains knowledge from abundant, complex neural models, resulting in poor interpretability assessments. Here, to enhance the understanding of imagined actions, we develop the relevance analysis of topographic time-frequency representation, preserving an adequate classification performance. Namely, to deal better with the subject variability, a 2D feature combination of continuous wavelet transform and common spatial patterns is extracted to feed a Deep&Wide learning model, assessing the relevance of input multi-view representation that contribute the best to the classifier accuracy. We estimate the feature contribution through the information back-propagated across the hidden layers\u2019 weights before predicting the output label. Evaluation, presented in two databases with bi- and four-label MI tasks, proves that the developed Deep&Wide-based relevance analysis reveals insights about the electrodes, time segments, and frequency bands with relevant MI neural responses, favoring explanation of inter and intra-subject variability because of coordination skills."}}
{"id": "Bt_8IAGig7b", "cdate": 1599165770996, "mdate": null, "content": {"title": "A Data-Driven Measure of Effective Connectivity Based on Renyi's \u03b1-Entropy", "abstract": "Transfer entropy (TE) is a model-free effective connectivity measure based on information theory. It has been increasingly used in neuroscience because of its ability to detect unknown non-linear interactions, which makes it well suited for exploratory brain effective connectivity analyses. Like all information theoretic quantities, TE is defined regarding the probability distributions of the system under study, which in practice are unknown and must be estimated from data. Commonly used methods for TE estimation rely on a local approximation of the probability distributions from nearest neighbor distances, or on symbolization schemes that then allow the probabilities to be estimated from the symbols' relative frequencies. However, probability estimation is a challenging problem, and avoiding this intermediate step in TE computation is desirable. In this work, we propose a novel TE estimator using functionals defined on positive definite and infinitely divisible kernels matrices that approximate Renyi's entropy measures of order \u03b1. Our data-driven approach estimates TE directly from data, sidestepping the need for probability distribution estimation. Also, the proposed estimator encompasses the well-known definition of TE as a sum of Shannon entropies in the limiting case when \u03b1 \u2192 1. We tested our proposal on a simulation framework consisting of two linear models, based on autoregressive approaches and a linear coupling function, respectively, and on the public electroencephalogram (EEG) database BCI Competition IV, obtained under a motor imagery paradigm. For the synthetic data, the proposed kernel-based TE estimation method satisfactorily identifies the causal interactions present in the data. Also, it displays robustness to varying noise levels and data sizes, and to the presence of multiple interaction delays in the same connected network. Obtained results for the motor imagery task show that our approach codes discriminant spatiotemporal patterns for the left and right-hand motor imagination tasks, with classification performances that compare favorably to the state-of-the-art."}}
{"id": "9bBnXAmo8i_", "cdate": 1599165677300, "mdate": null, "content": {"title": "Tensor decomposition processes for interpolation of diffusion magnetic resonance imaging", "abstract": "\nDiffusion magnetic resonance imaging (dMRI) is an established medical technique used for describing water diffusion in an organic tissue. Typically, rank-2 or 2nd-order tensors quantify this diffusion. From this quantification, it is possible to calculate relevant scalar measures (i.e. fractional anisotropy) employed in the clinical diagnosis of neurological diseases. Nonetheless, 2nd-order tensors fail to represent complex tissue structures like crossing fibers. To overcome this limitation, several researchers proposed a diffusion representation with higher order tensors (HOT), specifically 4th and 6th orders. However, the current acquisition protocols of dMRI data allow images with a spatial resolution between 1 mm3 and 2 mm3, and this voxel size is much bigger than tissue structures. Therefore, several clinical procedures derived from dMRI may be inaccurate. Concerning this, interpolation has been used to enhance the resolution of dMRI in a tensorial space. Most interpolation methods are valid only for rank-2 tensors and a generalization for HOT data is missing. In this work, we propose a probabilistic framework for performing HOT data interpolation. In particular, we introduce two novel probabilistic models based on the Tucker and the canonical decompositions. We call our approaches: Tucker decomposition process (TDP) and canonical decomposition process (CDP). We test the TDP and CDP in rank-2, 4 and 6 HOT fields. For rank-2 tensors, we compare against direct interpolation, log-Euclidean approach, and Generalized Wishart processes. For rank-4 and 6 tensors, we compare against direct interpolation and raw dMRI interpolation. Results obtained show that TDP and CDP interpolate accurately the HOT fields in terms of Frobenius distance, anisotropy measurements, and fiber tracts. Besides, CDP and TDP can be generalized to any rank. Also, the proposed framework keeps the mandatory constraint of positive definite tensors, and preserves morphological properties such as fractional anisotropy (FA), generalized anisotropy (GA) and tractography."}}
{"id": "qbStW9LGHpp", "cdate": 1599165545867, "mdate": null, "content": {"title": "AKL-ABC: An Automatic Approximate Bayesian Computation Approach Based on Kernel Learning", "abstract": "Bayesian statistical inference under unknown or hard to asses likelihood functions is a very challenging task. Currently, approximate Bayesian computation (ABC) techniques have emerged as a widely used set of likelihood-free methods. A vast number of ABC-based approaches have appeared in the literature; however, they all share a hard dependence on free parameters selection, demanding expensive tuning procedures. In this paper, we introduce an automatic kernel learning-based ABC approach, termed AKL-ABC, to automatically compute posterior estimations from a weighting-based inference. To reach this goal, we propose a kernel learning stage to code similarities between simulation and parameter spaces using a centered kernel alignment (CKA) that is automated via an Information theoretic learning approach. Besides, a local neighborhood selection (LNS) algorithm is used to highlight local dependencies over simulations relying on graph theory. Attained results on synthetic and real-world datasets show our approach is a quite competitive method compared to other non-automatic state-of-the-art ABC techniques."}}
{"id": "wyDrIs3p3na", "cdate": 1599164971836, "mdate": null, "content": {"title": "Bearing Health Monitoring Using Relief-F-Based Feature Relevance Analysis and HMM", "abstract": "Nowadays, bearings installed in industrial electric motors are constituted as the primary mode of a failure affecting the global energy consumption. Since industries\u2019 energy demand has a growing tendency, interest for efficient maintenance in electric motors is decisive. Vibration signals from bearings are employed commonly as a non-invasive approach to support fault diagnosis and severity evaluation of rotating machinery. However, vibration-based diagnosis poses a challenge concerning the signal properties, e.g., highly dynamic and non-stationary. Here, we introduce a knowledge-based tool to analyze multiple health conditions in bearings. Our approach includes a stochastic feature selection method, termed Stochastic Feature Selection (SFS), highlighting and interpreting relevant multi-domain attributes (time, frequency, and time\u2013frequency) related to the bearing faults discriminability. In particular, a relief-F-based ranking and a Hidden Markov Model are trained under a windowing scheme to achieve our SFS. Obtained results in a public database demonstrate that our proposal is competitive compared to state-of-the-art algorithms concerning both the number of features selected and the classification accuracy. "}}
{"id": "1gQciubNeYV", "cdate": 1599164376241, "mdate": null, "content": {"title": "Enhanced automatic twin support vector machine for imbalanced data classification", "abstract": "Most of the classification approaches assume that the sample distribution among classes is balanced. Still, such an assumption leads to biased performance over the majority class. This paper proposes an enhanced automatic twin support vector machine \u2013 (EATWSVM) to deal with imbalanced data, which incorporates a kernel representation within a TWSVM-based optimization. To learn the kernel function, we impose a Gaussian similarity, ruled by a Mahalanobis distance, and couple a centered kernel alignment-based approach to improving the data separability. Besides, we suggest a suitable range to fix the regularization parameters concerning both the dataset\u2019 imbalance ratio and overlap. Lastly, we adopt One-vs-One and One-vs-Rest frameworks to extend our EATWSVM formulation for multi-class tasks. Obtained results on synthetic and real-world datasets show that our approach outperforms state-of-the-art methods concerning classification performance and training time."}}
