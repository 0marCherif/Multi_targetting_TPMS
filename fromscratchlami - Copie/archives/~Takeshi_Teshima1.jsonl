{"id": "WGfppY9Yx7", "cdate": 1640995200000, "mdate": 1681654366931, "content": {"title": "Universal approximation property of invertible neural networks", "abstract": ""}}
{"id": "qh6TmIbfDeC", "cdate": 1609459200000, "mdate": 1683165403284, "content": {"title": "Incorporating causal graphical prior knowledge into predictive modeling via simple data augmentation", "abstract": "Causal graphs (CGs) are compact representations of the knowledge of the data generating processes behind the data distributions. When a CG is available, e.g., from the domain knowledge, we can infe..."}}
{"id": "eBoqn0pm8H", "cdate": 1609459200000, "mdate": 1665560298288, "content": {"title": "Rethinking Importance Weighting for Transfer Learning", "abstract": "A key assumption in supervised learning is that training and test data follow the same probability distribution. However, this fundamental assumption is not always satisfied in practice, e.g., due to changing environments, sample selection bias, privacy concerns, or high labeling costs. Transfer learning (TL) relaxes this assumption and allows us to learn under distribution shift. Classical TL methods typically rely on importance-weighting -- a predictor is trained based on the training losses weighted according to the importance (i.e., the test-over-training density ratio). However, as real-world machine learning tasks are becoming increasingly complex, high-dimensional, and dynamical, novel approaches are explored to cope with such challenges recently. In this article, after introducing the foundation of TL based on importance-weighting, we review recent advances based on joint and dynamic importance-predictor estimation. Furthermore, we introduce a method of causal mechanism transfer that incorporates causal structure in TL. Finally, we discuss future perspectives of TL research."}}
{"id": "N4H2rFGSOj", "cdate": 1609459200000, "mdate": 1683165403269, "content": {"title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation", "abstract": "Density ratio estimation (DRE) is at the core of various machine learning tasks such as anomaly detection and domain adaptation. In the DRE literature, existing studies have extensively studied met..."}}
{"id": "-CLulc5dkSt", "cdate": 1609459200000, "mdate": null, "content": {"title": "\u03b3-ABC: Outlier-Robust Approximate Bayesian Computation Based on a Robust Divergence Estimator", "abstract": "Approximate Bayesian computation (ABC) is a likelihood-free inference method that has been employed in various applications. However, ABC can be sensitive to outliers if a data discrepancy measure is chosen inappropriately. In this paper, we propose to use a nearest-neighbor-based \u03b3-divergence estimator as a data discrepancy measure. We show that our estimator possesses a suitable robustness property called the redescending property. In addition, our estimator enjoys various desirable properties such as high flexibility, asymptotic unbiasedness, almost sure convergence, and linear time complexity. Through experiments, we demonstrate that our method achieves significantly higher robustness than existing discrepancy measures."}}
{"id": "PGmqOzKEPZN", "cdate": 1601308119107, "mdate": null, "content": {"title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation", "abstract": "The estimation of the ratio of two probability densities has garnered attention as the density ratio is useful in various machine learning tasks, such as anomaly detection and domain adaptation. To estimate the density ratio, methods collectively known as direct density ratio estimation (DRE) have been explored. These methods are based on the minimization of the Bregman (BR) divergence between a density ratio model and the true density ratio. However, existing direct DRE suffers from serious overfitting when using flexible models such as neural networks. In this paper, we introduce a non-negative correction for empirical risk using only the prior knowledge of the upper bound of the density ratio. This correction makes a DRE method more robust against overfitting and enables the use of flexible models. In the theoretical analysis, we discuss the consistency of the empirical risk. In our experiments, the proposed estimators show favorable performance in inlier-based outlier detection and covariate shift adaptation."}}
{"id": "QU416G1lUmU", "cdate": 1577836800000, "mdate": 1631157560677, "content": {"title": "Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators", "abstract": "Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself."}}
{"id": "4bSNH_KO3qz", "cdate": 1577836800000, "mdate": 1631233035816, "content": {"title": "Few-shot Domain Adaptation by Causal Mechanism Transfer", "abstract": "We study few-shot supervised domain adaptation (DA) for regression problems, where only a few labeled target domain data and many labeled source domain data are available. Many of the current DA me..."}}
{"id": "-lLsv9uCBB6", "cdate": 1577836800000, "mdate": null, "content": {"title": "Universal Approximation Property of Neural Ordinary Differential Equations", "abstract": "Neural ordinary differential equations (NODEs) is an invertible neural network architecture promising for its free-form Jacobian and the availability of a tractable Jacobian determinant estimator. Recently, the representation power of NODEs has been partly uncovered: they form an $L^p$-universal approximator for continuous maps under certain conditions. However, the $L^p$-universality may fail to guarantee an approximation for the entire input domain as it may still hold even if the approximator largely differs from the target function on a small region of the input space. To further uncover the potential of NODEs, we show their stronger approximation property, namely the $\\sup$-universality for approximating a large class of diffeomorphisms. It is shown by leveraging a structure theorem of the diffeomorphism group, and the result complements the existing literature by establishing a fairly large set of mappings that NODEs can approximate with a stronger guarantee."}}
{"id": "nNurIaAbfI", "cdate": 1546300800000, "mdate": 1683165403290, "content": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets."}}
