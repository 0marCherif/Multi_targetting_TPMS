{"id": "9Y0DFkzbMct", "cdate": 1684041534772, "mdate": 1684041534772, "content": {"title": "A geometric structure of acceleration and its role in making gradients small fast", "abstract": "Since Nesterov's seminal 1983 work, many accelerated first-order optimization methods have been proposed, but their analyses lacks a common unifying structure. In this work, we identify a geometric structure satisfied by a wide range of first-order accelerated methods. Using this geometric insight, we present several novel generalizations of accelerated methods. Most interesting among them is a method that reduces the squared gradient norm with  rate in the prox-grad setup, faster than the  rates of Nesterov's FGM or Kim and Fessler's FPGM-m."}}
{"id": "qBdhCLvDFg", "cdate": 1684041465932, "mdate": 1684041465932, "content": {"title": "Neural Tangent Kernel Analysis of Deep Narrow Neural Networks", "abstract": "The tremendous recent progress in analyzing the training dynamics of overparameterized neural networks has primarily focused on wide networks and therefore does not sufficiently address the role of depth in deep learning. In this work, we present the first trainability guarantee of infinitely deep but narrow neural networks. We study the infinite-depth limit of a multilayer perceptron (MLP) with a specific initialization and establish a trainability guarantee using the NTK theory. We then extend the analysis to an infinitely deep convolutional neural network (CNN) and perform brief experiments."}}
{"id": "OWBsT8PRaHh", "cdate": 1683881922050, "mdate": 1683881922050, "content": {"title": "Convergence Analyses of Davis-Yin Splitting via Scaled Relative Graphs II: Convex Optimization Problems", "abstract": "The prior work of [arXiv:2207.04015, 2022] used scaled relative graphs (SRG) to analyze the convergence of Davis-Yin splitting (DYS) iterations on monotone inclusion problems. In this work, we use this machinery to analyze DYS iterations on convex optimization problems and obtain state-of-the-art linear convergence rates."}}
{"id": "E3K_jT8zl5", "cdate": 1683881816287, "mdate": 1683881816287, "content": {"title": "Convergence Analyses of Davis-Yin Splitting via Scaled Relative Graphs", "abstract": "Davis-Yin splitting (DYS) has found a wide range of applications in optimization, but its linear rates of convergence have not been studied extensively. The scaled relative graph (SRG) simplifies the convergence analysis of operator splitting methods by mapping the action of the operator onto the complex plane, but the prior SRG theory did not fully apply to the DYS operator. In this work, we formalize an SRG theory for the DYS operator and use it to obtain tighter contraction factors."}}
{"id": "hD3yWT-ly_-", "cdate": 1680214695131, "mdate": 1680214695131, "content": {"title": "Decentralized Proximal Gradient Algorithms with Linear Convergence Rates", "abstract": "This work studies a class of non-smooth decentralized multi-agent optimization problems where the\nagents aim at minimizing a sum of local strongly-convex smooth components plus a common non-smooth\nterm. We propose a general primal-dual algorithmic framework that unifies many existing state-of-the-art\nalgorithms. We establish linear convergence of the proposed method to the exact solution in the presence\nof the non-smooth term. Moreover, for the more general class of problems with agent specific non-smooth\nterms, we show that linear convergence cannot be achieved (in the worst case) for the class of algorithms\nthat uses the gradients and the proximal mappings of the smooth and non-smooth parts, respectively.\nWe further provide a numerical counterexample that shows how some state-of-the-art algorithms fail to\nconverge linearly for strongly-convex objectives and different local non-smooth terms.\n"}}
{"id": "Z-DwfjRU6s", "cdate": 1672531200000, "mdate": 1683312933076, "content": {"title": "Rotation and Translation Invariant Representation Learning with Implicit Neural Representations", "abstract": "In many computer vision applications, images are acquired with arbitrary or random rotations and translations, and in such setups, it is desirable to obtain semantic representations disentangled from the image orientation. Examples of such applications include semiconductor wafer defect inspection, plankton microscope images, and inference on single-particle cryo-electron microscopy (cryo-EM) micro-graphs. In this work, we propose Invariant Representation Learning with Implicit Neural Representation (IRL-INR), which uses an implicit neural representation (INR) with a hypernetwork to obtain semantic representations disentangled from the orientation of the image. We show that IRL-INR can effectively learn disentangled semantic representations on more complex images compared to those considered in prior works and show that these semantic representations synergize well with SCAN to produce state-of-the-art unsupervised clustering results."}}
{"id": "qQeNmPq4M4", "cdate": 1640995200000, "mdate": 1683312933144, "content": {"title": "Neural Tangent Kernel Analysis of Deep Narrow Neural Networks", "abstract": "The tremendous recent progress in analyzing the training dynamics of overparameterized neural networks has primarily focused on wide networks and therefore does not sufficiently address the role of depth in deep learning. In this work, we present the first trainability guarantee of infinitely deep but narrow neural networks. We study the infinite-depth limit of a multilayer perceptron (MLP) with a specific initialization and establish a trainability guarantee using the NTK theory. We then extend the analysis to an infinitely deep convolutional neural network (CNN) and perform brief experiments."}}
{"id": "izNpPEedNEl", "cdate": 1640995200000, "mdate": 1683312933154, "content": {"title": "Robust Probabilistic Time Series Forecasting", "abstract": "Probabilistic time series forecasting has played critical role in decision-making processes due to its capability to quantify uncertainties. Deep forecasting models, however, could be prone to input perturbations, and the notion of such perturbations, together with that of robustness, has not even been completely established in the regime of probabilistic forecasting. In this work, we propose a framework for robust probabilistic time series forecasting. First, we generalize the concept of adversarial input perturbations, based on which we formulate the concept of robustness in terms of bounded Wasserstein deviation. Then we extend the randomized smoothing technique to attain robust probabilistic forecasters with theoretical robustness certificates against certain classes of adversarial perturbations. Lastly, extensive experiments demonstrate that our methods are empirically effective in enhancing the forecast quality under additive adversarial attacks and forecast consistency under supplement of noisy observations."}}
{"id": "iILiDUWSKar", "cdate": 1640995200000, "mdate": 1683312933076, "content": {"title": "Robust Probabilistic Time Series Forecasting", "abstract": "Probabilistic time series forecasting has played critical role in decision-making processes due to its capability to quantify uncertainties. Deep forecasting models, however, could be prone to input perturbations, and the notion of such perturbations, together with that of robustness, has not even been completely established in the regime of probabilistic forecasting. In this work, we propose a framework for robust probabilistic time series forecasting. First, we generalize the concept of adversarial input perturbations, based on which we formulate the concept of robustness in terms of bounded Wasserstein deviation. Then we extend the randomized smoothing technique to attain robust probabilistic forecasters with theoretical robustness certificates against certain classes of adversarial perturbations. Lastly, extensive experiments demonstrate that our methods are empirically effective in enhancing the forecast quality under additive adversarial attacks and forecast consistency under supplement of noisy observations. The code for our experiments is available at https://github.com/tetrzim/robust-probabilistic-forecasting."}}
{"id": "bdr1XyBxHs", "cdate": 1640995200000, "mdate": 1683312933075, "content": {"title": "Scaled relative graphs: nonexpansive operators via 2D Euclidean geometry", "abstract": "Many iterative methods in applied mathematics can be thought of as fixed-point iterations, and such algorithms are usually analyzed analytically, with inequalities. In this paper, we present a geometric approach to analyzing contractive and nonexpansive fixed point iterations with a new tool called the scaled relative graph. The SRG provides a correspondence between nonlinear operators and subsets of the 2D plane. Under this framework, a geometric argument in the 2D plane becomes a rigorous proof of convergence."}}
