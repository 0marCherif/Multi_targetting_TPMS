{"id": "1jDN-RfQfrb", "cdate": 1663850445362, "mdate": null, "content": {"title": "Unveiling Transformers with LEGO: A Synthetic Reasoning Task", "abstract": "We propose a synthetic reasoning task, LEGO (Learning Equality and Group Operations), that encapsulates the problem of following a chain of reasoning, and we study how the Transformer architectures learn this task. We pay special attention to data effects such as pretraining (on seemingly unrelated NLP tasks) and dataset composition (e.g., differing chain length at training and test time), as well as architectural variants such as weight-tied layers or adding convolutional components. We study how the trained models eventually succeed at the task, and in particular, we are able to understand (to some extent) some of the attention heads as well as how the information flows in the network. Based on these observations we propose a hypothesis that here pretraining helps for LEGO tasks due to certain structured attention patterns, and we experimentally verify this hypothesis. We also observe that in some data regimes the trained transformer finds ``shortcut\" solutions to follow the chain of reasoning, which impedes the model's robustness, and moreover we propose ways to prevent it. Motivated by our findings on structured attention patterns, we propose to replace certain attention heads with hardcoded patterns. This architectural change significantly reduces Flops and maintains or even improves the model's performance at large-scale pretraining."}}
{"id": "AyGJDpN2eR6", "cdate": 1652737741823, "mdate": null, "content": {"title": "Exponentially Improving the Complexity of Simulating the Weisfeiler-Lehman Test with Graph Neural Networks", "abstract": "Recent work shows that the expressive power of Graph Neural Networks (GNNs) in distinguishing non-isomorphic graphs is exactly the same as that of the Weisfeiler-Lehman (WL) graph test. In particular, they show that the WL test can be simulated by GNNs. However, those simulations involve neural networks for the \u201ccombine\u201d function of size polynomial or even exponential in the number of graph nodes $n$, as well as feature vectors of length linear in $n$. \n\nWe present an improved simulation of the WL test on GNNs with {\\em exponentially} lower complexity. In particular,  the neural network implementing the  combine function  in each node has only $\\mathrm{polylog}(n)$ parameters, and the feature vectors exchanged by the nodes of GNN consists of only $O(\\log n)$ bits. We also give logarithmic lower bounds for the feature vector length and the size of the neural networks, showing the (near)-optimality of our construction. "}}
{"id": "hjPAurYQzjF", "cdate": 1652714383581, "mdate": 1652714383581, "content": {"title": "Learning-based Support Estimation in Sublinear Time", "abstract": "We consider the problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to $\\pm \\epsilon n$ from a sample of size $O(\\log^2(1/\\epsilon) \\cdot n/ \\log n)$, where n is the data set size. Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of its frequency. We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly, to $ \\log(1/\\epsilon) \\cdot n^{1-\\Theta(1/\\log(1/\\epsilon))} $.\nWe evaluate the proposed algorithms on a collection of data sets, using the neural-network based estimators from {Hsu et al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm. "}}
{"id": "m78lSssqAP", "cdate": 1652713323010, "mdate": 1652713323010, "content": {"title": "Triangle and Four Cycle Counting with Predictions in Graph Stream", "abstract": "We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, (Hsu 2018) and (Jiang 2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior \"classical\" algorithms that did not use oracles. In this paper, we explore the power of a \"heavy edge\" oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on \"classical\" streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms."}}
{"id": "8in_5gN9I0", "cdate": 1632875720230, "mdate": null, "content": {"title": "Triangle and Four Cycle Counting with Predictions in Graph Streams", "abstract": "We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, Hsu et al. (2019) and Jiang et al. (2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior \u201cclassical\u201d algorithms that did not use oracles. In this paper, we explore the power of a \u201cheavy edge\u201d oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on \u201cclassical\u201d streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms."}}
{"id": "6dUJPrPPUau", "cdate": 1621630093842, "mdate": null, "content": {"title": "Few-Shot Data-Driven Algorithms for Low Rank Approximation", "abstract": "Recently, data-driven and learning-based algorithms for low rank matrix approximation were shown to outperform classical data-oblivious algorithms by wide margins in terms of accuracy.  Those algorithms are based on the optimization of sparse sketching matrices, which lead to large savings in time and memory during testing. However, they require long training times on a large amount of existing data, and rely on access to specialized hardware and software. \n\nIn this work, we develop new data-driven low rank approximation algorithms with better computational efficiency in the training phase, alleviating these drawbacks. Furthermore, our methods are interpretable: while previous algorithms choose the sketching matrix either at random or by black-box learning, we show that it can be set (or initialized) to clearly interpretable values extracted from the dataset. \n\nOur experiments show that our algorithms, either by themselves or in combination with previous methods, achieve significant empirical advantage over previous work, improving training times by up to an order of magnitude toward achieving the same target accuracy."}}
{"id": "qvMuYCUroq0", "cdate": 1609459200000, "mdate": null, "content": {"title": "Faster Kernel Matrix Algebra via Density Estimation", "abstract": "We study fast algorithms for computing fundamental properties of a positive semidefinite kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ corresponding to $n$ points $x_1,\\ldots,x_n \\in \\mathbb{R}^d$. In particular, we consider estimating the sum of kernel matrix entries, along with its top eigenvalue and eigenvector. We show that the sum of matrix entries can be estimated to $1+\\epsilon$ relative error in time $sublinear$ in $n$ and linear in $d$ for many popular kernels, including the Gaussian, exponential, and rational quadratic kernels. For these kernels, we also show that the top eigenvalue (and an approximate eigenvector) can be approximated to $1+\\epsilon$ relative error in time $subquadratic$ in $n$ and linear in $d$. Our algorithms represent significant advances in the best known runtimes for these problems. They leverage the positive definiteness of the kernel matrix, along with a recent line of work on efficient kernel density estimation."}}
{"id": "tilovEHA3YS", "cdate": 1601308133334, "mdate": null, "content": {"title": "Learning-based Support Estimation in Sublinear Time", "abstract": "We consider the  problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to $ \\pm \\varepsilon n$ from a sample of size $O(\\log^2(1/\\varepsilon) \\cdot n/\\log n)$, where $n$ is the data set size.  Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of  its frequency.  We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly,  to\n$$ \\ \\log (1/\\varepsilon) \\cdot n^{1-\\Theta(1/\\log(1/\\varepsilon))}. $$\nWe evaluate the proposed algorithms on a collection of data sets, using the neural-network based estimators from {Hsu et al, ICLR'19} as predictors. Our experiments  demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm."}}
{"id": "yMeRYAAPRgR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Scalable Nearest Neighbor Search for Optimal Transport", "abstract": "The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly popular similarity measure for rich data domains, such as images or text documents. This raises the necessity for fast nearest..."}}
{"id": "x_sGqeM84ws", "cdate": 1577836800000, "mdate": null, "content": {"title": "Eccentricity Heuristics through Sublinear Analysis Lenses", "abstract": "The eccentricity of a node in a graph is its maximal shortest-path distance to any other node. Computing all eccentricities is a basic task in large-scale graph mining. Shun (KDD 2015) empirically studied two simple heuristics for this task: k-BFS1, based on parallel BFS from a small sample of nodes, was shown to work well on a variety of graphs; k-BFS2, a two-phase version, was shown to outperform state-of-the-art algorithms by up to orders of magnitude. This empirical success stands in apparent contrast to recent theoretical hardness results on approximating all eccentricities (Backurs et al., STOC 2018). This paper aims to formally explain the performance of these heuristics, by studying them through computational models designed for sublinear time or sublinear space algorithms. We use the proposed framework to derive improved variants, which retain their practicality while having better performance and formal guarantees. For k-BFS1, we draw a connection to diameter property testing (Parnas and Ron, Random Struct Alg. 2002). It is not hard to observe that k-BFS1 essentially tests the values of all eccentricities simultaneously, in the classical property testing sense. We show that the same guarantee is achieved by a more efficient algorithm, whose work is nearly linear in the number of nodes and independent of the number of edges. By utilizing the connection in the opposite direction, we also obtain some results on classical testing of the graph radius and diameter. For k-BFS2, we draw a connection to the streaming Set Cover algorithm of Demaine et al. (DISC 2014). We use it to suggest a variant of k-BFS2 with similar work and depth bounds, which is guaranteed to compute almost all eccentricities exactly, if the graph satisfies a condition we call small eccentric cover. The condition can be ascertained for all real-world graph used in Shun (KDD 2015) and in our experiments. Our experimental results on real-world graphs demonstrate the validity of our analysis and the empirical advantage of the proposed algorithms."}}
