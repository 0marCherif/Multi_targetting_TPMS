{"id": "tIvwOFz0By", "cdate": 1672531200000, "mdate": 1699190406642, "content": {"title": "Language-Driven Representation Learning for Robotics", "abstract": ""}}
{"id": "mRyUgZptz3s", "cdate": 1672531200000, "mdate": 1679925756557, "content": {"title": "No, to the Right: Online Language Corrections for Robotic Manipulation via Shared Autonomy", "abstract": ""}}
{"id": "hxlsk4Ph12J", "cdate": 1672531200000, "mdate": 1699190406642, "content": {"title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents", "abstract": "Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELICS, we train vision and language models of 9 and 80 billion parameters named IDEFICS, and obtain competitive performance on different multimodal benchmarks. We release our dataset, models and code."}}
{"id": "Pkg_eOAz90Z", "cdate": 1672531200000, "mdate": 1699190406663, "content": {"title": "Toward Grounded Social Reasoning", "abstract": "Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not socially appropriate to disassemble the sports car and put it away as part of the \"tidying\". How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable social reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and *actively gather information from the environment* that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded social reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/groundedsocialreasoning."}}
{"id": "EXp63SV9Hz", "cdate": 1672531200000, "mdate": 1679925756670, "content": {"title": "Language-Driven Representation Learning for Robotics", "abstract": ""}}
{"id": "iRabxvK3j0", "cdate": 1655376346623, "mdate": null, "content": {"title": "Eliciting Compatible Demonstrations for Multi-Human Imitation Learning", "abstract": "Imitation learning from human-provided demonstrations is a strong approach for learning policies for robot manipulation. While the ideal dataset for imitation learning is homogenous and low-variance - reflecting a single, optimal method for performing a task - natural human behavior has a great deal of heterogeneity, with several optimal ways to demonstrate a task. This multimodality is inconsequential to human users, with task variations manifesting as subconscious choices; for example, reaching down, then across to grasp an object, versus reaching across, then down. Yet, this mismatch presents a problem for interactive imitation learning, where sequences of users improve on a policy by iteratively collecting new, possibly conflicting demonstrations. To combat this problem of demonstrator incompatibility, this work designs an approach for 1) measuring the compatibility of a new demonstration given a base policy, and 2) actively eliciting more compatible demonstrations from new users. Across two simulation tasks requiring long-horizon, dexterous manipulation and a real-world ``food plating'' task with a Franka Emika Panda arm, we show that we can both identify incompatible demonstrations via post-hoc filtering, and apply our compatibility measure to actively elicit compatible demonstrations from new users, leading to improved task success rates across simulated and real environments."}}
{"id": "BHBz6bP2jb9", "cdate": 1647195908786, "mdate": null, "content": {"title": "Shared Autonomy for Robotic Manipulation with Language Corrections", "abstract": "Traditional end-to-end instruction following approaches for robotic manipulation are notoriously sample inefficient and lack adaptivity; for most single-turn methods, there is no way to provide additional language supervision to adapt robot behavior online \u2013 a property critical to deploying robots in collaborative, safety-critical environments. In this work, we present a method for incorporating language corrections, built on the insight that an initial instruction and subsequent corrections differ mainly in the amount of grounded context needed. To focus on manipulation domains where the sample efficiency of existing work is prohibitive, we incorporate our method into a shared autonomy system. Shared autonomy splits agency between the human and robot; rather than specifying a goal the robot needs to achieve alone, language informs the control space provided to the human. Splitting agency this way allows the robot to learn the coarse, high-level parts of a task, offloading more involved decisions \u2013 such as when to execute a grasp, or if a grasp is solid \u2013 to humans. Our user study on a Franka Emika Panda arm shows that our correction-aware system is sample-efficient and obtains significant gains over non-adaptive baselines."}}
{"id": "qzVfz_Q8Ri", "cdate": 1640995200000, "mdate": 1679925756617, "content": {"title": "Eliciting Compatible Demonstrations for Multi-Human Imitation Learning", "abstract": ""}}
{"id": "_lkBGOctkip", "cdate": 1624097108313, "mdate": null, "content": {"title": "LILA: Language-Informed Latent Actions", "abstract": "We introduce Language-Informed Latent Actions (LILA), a framework for learning natural language interfaces in the context of human-robot collaboration. LILA falls under the shared autonomy paradigm: in addition to providing discrete language inputs, humans are given a low-dimensional controller \u2013 e.g., a 2 degree-of-freedom (DoF) joystick that can move left/right and up/down \u2013 for operating the robot. LILA learns to use language to modulate this controller, providing users with a language-informed control space: given an instruction like \"place the cereal bowl on the tray,\" LILA may learn a 2-DoF space where one dimension controls the distance from the robot's end-effector to the bowl, and the other dimension controls the robot's end-effector pose relative to the grasp point on the bowl. We evaluate LILA with real-world user studies, where users can provide a language instruction while operating a 7-DoF Franka Emika Panda Arm to complete a series of complex manipulation tasks. We show that LILA models are not only more sample efficient and performant than imitation learning and end-effector control baselines, but that they are also qualitatively preferred by users."}}
{"id": "VvUldGZ3izR", "cdate": 1621630106771, "mdate": null, "content": {"title": "ELLA: Exploration through Learned Language Abstraction", "abstract": "Building agents capable of understanding language instructions is critical to effective and robust human-AI collaboration. Recent work focuses on training these agents via reinforcement learning in environments with synthetic language; however, instructions often define long-horizon, sparse-reward tasks, and learning policies requires many episodes of experience. We introduce ELLA: Exploration through Learned Language Abstraction, a reward shaping approach geared towards boosting sample efficiency in sparse reward environments by correlating high-level instructions with simpler low-level constituents. ELLA has two key elements: 1) A termination classifier that identifies when agents complete low-level instructions, and 2) A relevance classifier that correlates low-level instructions with success on high-level tasks. We learn the termination classifier offline from pairs of instructions and terminal states. Notably, in departure from prior work in language and abstraction, we learn the relevance classifier online, without relying on an explicit decomposition of high-level instructions to low-level instructions. On a suite of complex BabyAI environments with varying instruction complexities and reward sparsity, ELLA shows gains in sample efficiency relative to language-based shaping and traditional RL methods."}}
