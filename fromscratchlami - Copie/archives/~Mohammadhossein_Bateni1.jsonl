{"id": "2mKhSxNy9h", "cdate": 1676827091462, "mdate": null, "content": {"title": "SubMix: Learning to Mix Graph Sampling Heuristics", "abstract": "Sampling subgraphs for training Graph Neural Networks (GNNs) is receiving much attention from the GNN community. While a variety of methods have been proposed, each method samples the graph according to its own heuristic. However, there has been little work in mixing these heuristics in an end-to-end trainable manner. In this work, we design a generative framework for graph sampling. Our method, SubMix, parameterizes graph sampling as a convex combination of heuristics. We show that a continuous relaxation of the discrete sampling process allows us to efficiently obtain analytical gradients for training the sampling parameters. Our experimental results illustrate the usefulness of learning graph sampling in three scenarios: (1) robust training of GNNs by automatically learning to discard noisy edge sources; (2) improving model performance by trainable and online edge subset selection; and (3) by integrating our framework into state-of-the-art (SOTA) decoupled GNN models, for homogeneous OGBN datasets. Our method raises the SOTA on challenging ogbn-arxiv and ogbn-products, respectively, by over 4 and 0.5 percentage points."}}
{"id": "gZ5GdBD08s", "cdate": 1672531200000, "mdate": 1682334338285, "content": {"title": "Optimal Fully Dynamic k-Center Clustering for Adaptive and Oblivious Adversaries", "abstract": "In fully dynamic clustering problems, a clustering of a given data set in a metric space must be maintained while it is modified through insertions and deletions of individual points. In this paper, we resolve the complexity of fully dynamic k-center clustering against both adaptive and oblivious adversaries. Against oblivious adversaries, we present the first algorithm for fully dynamic k-center in an arbitrary metric space that maintains an optimal (2 + \u03b5)-approximation in O(k \u00b7 polylog(n, \u0394)) amortized update time. Here, n is an upper bound on the number of active points at any time, and \u0394 is the aspect ratio of the metric space. Previously, the best known amortized update time was O(k2 \u00b7 polylog(n, \u0394)), and is due to Chan, Gourqin, and Sozio (2018). Moreover, we demonstrate that our runtime is optimal up to polylog(n, \u0394) factors. In fact, we prove that even offline algorithms for k-clustering tasks in arbitrary metric spaces, including k-medians, k-means, and k-center, must make at least \u03a9(nk) distance queries to achieve any non-trivial approximation factor. This implies a lower bound of \u03a9(k) which holds even for the insertions-only setting. For adaptive adversaries, we give the first deterministic algorithm for fully dynamic k-center which achieves a approximation in O(k-polylog(n, \u0394)) amortized update time. Further, we demonstrate that any algorithm which achieves a -approximation against adaptive adversaries requires f (k,n) update time, for any arbitrary function f. Thus, in the regime where , we close the complexity of the problem up to polylog(n, \u0394) factors in the update time. Our lower bound extends to other k-clustering tasks in arbitrary metric spaces, including k-medians and k-means. Finally, despite the aforementioned lower bounds, we demonstrate that an update time sublinear in k is possible against oblivious adversaries for metric spaces which admit locally sensitive hash functions (LSH), resulting in improved algorithms for a large class of metrics including Euclidean space, \u2113p-spaces, the Hamming Metric, and the Jaccard Metric. We also give the first fully dynamic O(1)-approximation algorithms for the closely related k-sum-of-radii and k-sum-of-diameter problems, with O(poly(k, log \u0394)) update time."}}
{"id": "PhnANhWCeX", "cdate": 1672531200000, "mdate": 1682334340114, "content": {"title": "Optimal Fully Dynamic k-Center Clustering for Adaptive and Oblivious Adversaries", "abstract": "In fully dynamic clustering problems, a clustering of a given data set in a metric space must be maintained while it is modified through insertions and deletions of individual points. In this paper, we resolve the complexity of fully dynamic $k$-center clustering against both adaptive and oblivious adversaries. Against oblivious adversaries, we present the first algorithm for fully dynamic $k$-center in an arbitrary metric space that maintains an optimal $(2+\\epsilon)$-approximation in $O(k \\cdot \\mathrm{polylog}(n,\\Delta))$ amortized update time. Here, $n$ is an upper bound on the number of active points at any time, and $\\Delta$ is the aspect ratio of the metric space. Previously, the best known amortized update time was $O(k^2\\cdot \\mathrm{polylog}(n,\\Delta))$, and is due to Chan, Gourqin, and Sozio (2018). Moreover, we demonstrate that our runtime is optimal up to $\\mathrm{polylog}(n,\\Delta)$ factors. In fact, we prove that even offline algorithms for $k$-clustering tasks in arbitrary metric spaces, including $k$-medians, $k$-means, and $k$-center, must make at least $\\Omega(n k)$ distance queries to achieve any non-trivial approximation factor. This implies a lower bound of $\\Omega(k)$ which holds even for the insertions-only setting. We also show deterministic lower and upper bounds for adaptive adversaries, demonstrate that an update time sublinear in $k$ is possible against oblivious adversaries for metric spaces which admit locally sensitive hash functions (LSH) and give the first fully dynamic $O(1)$-approximation algorithms for the closely related $k$-sum-of-radii and $k$-sum-of-diameter problems."}}
{"id": "FN9r7AQcHL", "cdate": 1672531200000, "mdate": 1682334338352, "content": {"title": "Agile Modeling: Image Classification with Domain Experts in the Loop", "abstract": "The application of computer vision to nuanced subjective use cases is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a \"zebra\"), it now falters on tasks where there is substantial subjectivity in the concept (such as identifying \"gourmet tuna\"). However, empowering any user to develop a classifier for their concept is technically difficult: users are neither machine learning experts, nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vision model through a real-time user-in-the-loop interactions. We instantiate an Agile Modeling prototype for image classification and show through a user study (N=14) that users can create classifiers with minimal effort under 30 minutes. We compare this user driven process with the traditional crowdsourcing paradigm and find that the crowd's notion often differs from that of the user's, especially as the concepts become more subjective. Finally, we scale our experiments with simulations of users training classifiers for ImageNet21k categories to further demonstrate the efficacy."}}
{"id": "gdPiOBu99Y", "cdate": 1665069631602, "mdate": null, "content": {"title": "Scalable and Improved Algorithms for Individually Fair Clustering", "abstract": "We present scalable and improved algorithms for the individually fair ($p$, $k$)-clustering problem \nintroduced by Jung et al and Mahabadi et al.\nGiven $n$ points $P$ in a metric space, \nlet $\\delta(x)$ for $x\\in P$ be the radius of the smallest ball around $x$ \ncontaining at least $\\nicefrac nk$ points.  \n\nIn this work, we present two main contributions.\nWe first present local-search algorithms improving prior work along cost and maximum fairness violation.\nThen we design a fast local-search algorithm\nthat runs in $\\tO(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. \nFinally we show empirically that not only is our algorithm much faster than prior work, but it also produces lower-cost solutions.\n"}}
{"id": "TTLLGx3eet", "cdate": 1663850198166, "mdate": null, "content": {"title": "Sequential Attention for Feature Selection", "abstract": "Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, and thus inherits all of its provable guarantees. Our theoretical and empirical analyses offer new explanations towards the effectiveness of attention and its connections to overparameterization, which may be of independent interest."}}
{"id": "yvIj19gyve", "cdate": 1640995200000, "mdate": 1682334340632, "content": {"title": "Fair Resource Allocation in a Volatile Marketplace", "abstract": "In settings where a platform must allocate finite supplies of goods to buyers, balancing overall platform revenues with the fairness of the individual allocations to platform participants is paramo..."}}
{"id": "Yo9KxvXRTvJ", "cdate": 1640995200000, "mdate": 1682334338260, "content": {"title": "Tackling Provably Hard Representative Selection via Graph Neural Networks", "abstract": "Representative Selection (RS) is the problem of finding a small subset of exemplars from a dataset that is representative of the dataset. In this paper, we study RS for attributed graphs, and focus on finding representative nodes that optimize the accuracy of a model trained on the selected representatives. Theoretically, we establish a new hardness result forRS (in the absence of a graph structure) by proving that a particular, highly practical variant of it (RS for Learning) is hard to approximate in polynomial time within any reasonable factor, which implies a significant potential gap between the optimum solution of widely-used surrogate functions and the actual accuracy of the model. We then study the setting where a (homophilous) graph structure is available, or can be constructed, between the data points.We show that with an appropriate modeling approach, the presence of such a structure can turn a hard RS (for learning) problem into one that can be effectively solved. To this end, we develop RS-GNN, a representation learning-based RS model based on Graph Neural Networks. Empirically, we demonstrate the effectiveness of RS-GNN on problems with predefined graph structures as well as problems with graphs induced from node feature similarities, by showing that RS-GNN achieves significant improvements over established baselines on a suite of eight benchmarks."}}
{"id": "Ep16rUlOuU", "cdate": 1640995200000, "mdate": 1682334338280, "content": {"title": "Efficient Distributed Algorithms for Minimum Spanning Tree in Dense Graphs", "abstract": ""}}
{"id": "C1vXTFsduiU", "cdate": 1640995200000, "mdate": 1682334340560, "content": {"title": "Sequential Attention for Feature Selection", "abstract": "Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, and thus inherits all of its provable guarantees. Our theoretical and empirical analyses offer new explanations towards the effectiveness of attention and its connections to overparameterization, which may be of independent interest."}}
