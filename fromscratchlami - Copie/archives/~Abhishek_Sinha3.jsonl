{"id": "Z8D-8rp9Ua-", "cdate": 1672531200000, "mdate": 1683878928903, "content": {"title": "BanditQ - No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments", "abstract": "Classic no-regret online prediction algorithms, including variants of the Upper Confidence Bound ($\\texttt{UCB}$) algorithm, $\\texttt{Hedge}$, and $\\texttt{EXP3}$, are inherently unfair by design. The unfairness stems from their very objective of playing the most rewarding arm as many times as possible while ignoring the less rewarding ones among $N$ arms. In this paper, we consider a fair prediction problem in the stochastic setting with hard lower bounds on the rate of accrual of rewards for a set of arms. We study the problem in both full and bandit feedback settings. Using queueing-theoretic techniques in conjunction with adversarial learning, we propose a new online prediction policy called $\\texttt{BanditQ}$ that achieves the target reward rates while achieving a regret and target rate violation penalty of $O(T^{\\frac{3}{4}}).$ In the full-information setting, the regret bound can be further improved to $O(\\sqrt{T})$ when considering the average regret over the entire horizon of length $T$. The proposed policy is efficient and admits a black-box reduction from the fair prediction problem to the standard MAB problem with a carefully defined sequence of rewards. The design and analysis of the $\\texttt{BanditQ}$ policy involve a novel use of the potential function method in conjunction with scale-free second-order regret bounds and a new self-bounding inequality for the reward gradients, which are of independent interest."}}
{"id": "20cVgsKD8UX", "cdate": 1672531200000, "mdate": 1682545241429, "content": {"title": "No-regret Algorithms for Fair Resource Allocation", "abstract": "We consider a fair resource allocation problem in the no-regret setting against an unrestricted adversary. The objective is to allocate resources equitably among several agents in an online fashion so that the difference of the aggregate $\\alpha$-fair utilities of the agents between an optimal static clairvoyant allocation and that of the online policy grows sub-linearly with time. The problem is challenging due to the non-additive nature of the $\\alpha$-fairness function. Previously, it was shown that no online policy can exist for this problem with a sublinear standard regret. In this paper, we propose an efficient online resource allocation policy, called Online Proportional Fair (OPF), that achieves $c_\\alpha$-approximate sublinear regret with the approximation factor $c_\\alpha=(1-\\alpha)^{-(1-\\alpha)}\\leq 1.445,$ for $0\\leq \\alpha < 1$. The upper bound to the $c_\\alpha$-regret for this problem exhibits a surprising phase transition phenomenon. The regret bound changes from a power-law to a constant at the critical exponent $\\alpha=\\frac{1}{2}.$ As a corollary, our result also resolves an open problem raised by Even-Dar et al. [2009] on designing an efficient no-regret policy for the online job scheduling problem in certain parameter regimes. The proof of our results introduces new algorithmic and analytical techniques, including greedy estimation of the future gradients for non-additive global reward functions and bootstrapping adaptive regret bounds, which may be of independent interest."}}
{"id": "pFsJL7dyak0", "cdate": 1640995200000, "mdate": 1683477366950, "content": {"title": "Universal Caching", "abstract": "In learning theory, the performance of an online policy is commonly measured in terms of the static regret metric, which compares the cumulative loss of an online policy to that of an optimal benchmark in hindsight. In the definition of static regret, the action of the benchmark policy remains fixed throughout the time horizon. Naturally, the resulting regret bounds become loose in non-stationary settings where fixed actions often suffer from poor performance. In this paper, we investigate a stronger notion of regret minimization in the context of online caching. In particular, we allow the action of the benchmark at any round to be decided by a finite state machine containing any number of states. Popular caching policies, such as LRU and FIFO, belong to this class. Using ideas from the universal prediction literature in information theory, we propose an efficient online caching policy with a sub-linear regret bound. To the best of our knowledge, this is the first data-dependent regret bound known for the caching problem in the universal setting. We establish this result by combining a recently-proposed online caching policy with an incremental parsing algorithm, namely Lempel-Ziv '78. Our methods also yield a simpler learning-theoretic proof of the improved regret bound as opposed to the involved problem-specific combinatorial arguments used in the earlier works."}}
{"id": "pA3zAhb2dci", "cdate": 1640995200000, "mdate": 1683878928861, "content": {"title": "k-experts - Online Policies and Fundamental Limits", "abstract": "We introduce the k-experts problem - a generalization of the classic Prediction with Expert\u2019s Advice framework. Unlike the classic version, where the learner selects exactly one expert from a pool of N experts at each round, in this problem, the learner selects a subset of k experts at each round (1<= k <= N). The reward obtained by the learner at each round is assumed to be a function of the k selected experts. The primary objective is to design an online learning policy with a small regret. In this pursuit, we propose SAGE (Sampled Hedge) - a framework for designing efficient online learning policies by leveraging statistical sampling techniques. For a wide class of reward functions, we show that SAGE either achieves the first sublinear regret guarantee or improves upon the existing ones. Furthermore, going beyond the notion of regret, we fully characterize the mistake bounds achievable by online learning policies for stable loss functions. We conclude the paper by establishing a tight regret lower bound for a variant of the k-experts problem and carrying out experiments with standard datasets."}}
{"id": "ngLVW_U1Y5", "cdate": 1640995200000, "mdate": 1683878928862, "content": {"title": "Joint Power and Subcarrier Allocation in Multi-Cell Multi-Carrier NOMA", "abstract": "Non-orthogonal multiple access (NOMA) is a technology proposed for next generation cellular networks because of its high spectral efficiency and enhanced user connectivity. However, in the literature the optimal joint power and sub-carrier allocation for NOMA has been proposed for single cell only. Consequently, a global optimal algorithm for the joint power and sub-carrier allocation for NOMA system in multi-cell scenario is still an open problem. In this work, we propose a polyblock optimization based algorithm for obtaining a global optimal solution. It has reduced complexity due to a necessary and sufficient condition for feasible successive interference cancellation (SIC). Besides, we can adjust its optimization approximation parameter to serve as benchmark solution or to provide suitable solution for multi-cell multi-carrier NOMA systems. Numerical studies have shown its effectiveness."}}
{"id": "fjpTwLyaXw7", "cdate": 1640995200000, "mdate": 1675102551427, "content": {"title": "Optimizing Age-of-Information in Adversarial and Stochastic Environments", "abstract": "We design efficient online scheduling policies to maximize the freshness of information delivered to the users in a cellular network under both adversarial and stochastic channel and mobility assumptions. The information freshness achieved by a policy is investigated through the lens of a recently proposed metric - <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Age-of-Information</i> (AoI). We show that a natural greedy scheduling policy is competitive against any optimal offline policy in minimizing the AoI in the adversarial setting. We also derive universal lower bounds to the competitive ratio achievable by any online policy in the adversarial framework. In the stochastic setting, we show that a simple index policy is near-optimal for minimizing the average AoI in two different mobility scenarios. Further, we prove that the greedy scheduling policy minimizes the peak AoI for static users in the stochastic setting. Simulation results show that the proposed policies perform well under realistic conditions."}}
{"id": "VL5xkvtW_0", "cdate": 1640995200000, "mdate": 1675102551401, "content": {"title": "Optimizing Age-of-Information in Adversarial Environments with Channel State Information", "abstract": "This paper considers a multi-user downlink scheduling problem with access to the channel state information at the transmitter (CSIT) to minimize the Age-of-Information (AoI) in a non-stationary environment. The non-stationary environment is modelled using a novel adversarial framework. In this setting, we propose a greedy scheduling policy, called MA-CSIT, that takes into account the current channel state information. We establish a finite upper bound on the competitive ratio achieved by the MA-CSIT policy for a small number of users and show that the proposed policy has a better performance guarantee than a recently proposed greedy scheduler that operates without CSIT. In particular, we show that access to the additional channel state information improves the competitive ratio from 8 to 2 in the two-user case and from 18 to 8/3 in the three-user case. Finally, we carry out extensive numerical simulations to quantify the advantage of knowing CSIT in order to minimize the Age-of-Information for an arbitrary number of users."}}
{"id": "S2drBoPwHjw", "cdate": 1640995200000, "mdate": 1683878928919, "content": {"title": "Joint Power and Subcarrier Allocation in Multi-Cell Multi-Carrier NOMA", "abstract": "Non-orthogonal multiple access (NOMA) is a technology proposed for next generation cellular networks because of its high spectral efficiency and enhanced user connectivity. However, in the literature the optimal joint power and sub-carrier allocation for NOMA has been proposed for single cell only. Consequently, a global optimal algorithm for the joint power and sub-carrier allocation for NOMA system in multi-cell scenario is still an open problem. In this work, we propose a polyblock optimization based algorithm for obtaining a global optimal solution. It has reduced complexity due to a necessary and sufficient condition for feasible successive interference cancellation (SIC). Besides, we can adjust its optimization approximation parameter to serve as benchmark solution or to offer suitable practical solution for multi-cell multi-carrier NOMA systems. Numerical studies have shown its effectiveness."}}
{"id": "Kiv39Vy8Mh8", "cdate": 1640995200000, "mdate": 1683878929040, "content": {"title": "Online Subset Selection using $\u03b1$-Core with no Augmented Regret", "abstract": "We revisit the classic problem of optimal subset selection in the online learning set-up. Assume that the set $[N]$ consists of $N$ distinct elements. On the $t$th round, an adversary chooses a monotone reward function $f_t: 2^{[N]} \\to \\mathbb{R}_+$ that assigns a non-negative reward to each subset of $[N].$ An online policy selects (perhaps randomly) a subset $S_t \\subseteq [N]$ consisting of $k$ elements before the reward function $f_t$ for the $t$th round is revealed to the learner. As a consequence of its choice, the policy receives a reward of $f_t(S_t)$ on the $t$th round. Our goal is to design an online sequential subset selection policy to maximize the expected cumulative reward accumulated over a time horizon. In this connection, we propose an online learning policy called SCore (Subset Selection with Core) that solves the problem for a large class of reward functions. The proposed SCore policy is based on a new polyhedral characterization of the reward functions called $\\alpha$-Core - a generalization of Core from the cooperative game theory literature. We establish a learning guarantee for the SCore policy in terms of a new performance metric called $\\alpha$-augmented regret. In this new metric, the performance of the online policy is compared with an unrestricted offline benchmark that can select all $N$ elements at every round. We show that a large class of reward functions, including submodular, can be efficiently optimized with the SCore policy. We also extend the proposed policy to the optimistic learning set-up where the learner has access to additional untrusted hints regarding the reward functions. Finally, we conclude the paper with a list of open problems."}}
{"id": "Gl8-f5Xq8h", "cdate": 1640995200000, "mdate": 1683878929041, "content": {"title": "Optimistic No-regret Algorithms for Discrete Caching", "abstract": "We take a systematic look at the problem of storing whole files in a cache with limited capacity in the context of optimistic learning, where the caching policy has access to a prediction oracle (provided by, e.g., a Neural Network). The successive file requests are assumed to be generated by an adversary, and no assumption is made on the accuracy of the oracle. In this setting, we provide a universal lower bound for prediction-assisted online caching and proceed to design a suite of policies with a range of performance-complexity trade-offs. All proposed policies offer sublinear regret bounds commensurate with the accuracy of the oracle. Our results substantially improve upon all recently-proposed online caching policies, which, being unable to exploit the oracle predictions, offer only $O(\\sqrt{T})$ regret. In this pursuit, we design, to the best of our knowledge, the first comprehensive optimistic Follow-the-Perturbed leader policy, which generalizes beyond the caching problem. We also study the problem of caching files with different sizes and the bipartite network caching problem. Finally, we evaluate the efficacy of the proposed policies through extensive numerical experiments using real-world traces."}}
