{"id": "VM7u8ecLrZV", "cdate": 1652737791542, "mdate": null, "content": {"title": "Average Sensitivity of Euclidean k-Clustering", "abstract": "Given a set of $n$ points in $\\mathbb{R}^d$, the goal of Euclidean $(k,\\ell)$-clustering is to find $k$ centers that minimize the sum of the $\\ell$-th powers of the Euclidean distance of each point to the closest center. In practical situations, the clustering result must be stable against points missing in the input data so that we can make trustworthy and consistent decisions. To address this issue, we consider the average sensitivity of Euclidean $(k,\\ell)$-clustering, which measures the stability of the output in total variation distance against deleting a random point from the input data. We first show that a popular algorithm \\textsc{$k$-means++} and its variant called \\textsc{$D^\\ell$-sampling} have low average sensitivity. Next, we show that any approximation algorithm for Euclidean $(k,\\ell)$-clustering can be transformed to an algorithm with low average sensitivity while almost preserving the approximation guarantee. As byproducts of our results, we provide several algorithms for consistent $(k,\\ell)$-clustering and dynamic $(k,\\ell)$-clustering in the random-order model, where the input points are randomly permuted and given in an online manner. The goal of the consistent setting is to maintain a good solution while minimizing the number of changes to the solution during the process, and that of the dynamic setting is to maintain a good solution while minimizing the  (amortized) update time."}}
{"id": "Xm0976LQTn_", "cdate": 1652737672617, "mdate": null, "content": {"title": "Single Loop Gaussian Homotopy Method for Non-convex Optimization", "abstract": "The Gaussian homotopy (GH) method is a popular approach to finding better stationary points for non-convex optimization problems by gradually reducing a parameter value $t$, which changes the problem to be solved from an almost convex one to the original target one. Existing GH-based methods repeatedly call an iterative optimization solver to find a stationary point every time $t$ is updated, which incurs high computational costs. We propose a novel single loop framework for GH methods (SLGH) that updates the parameter $t$ and the optimization decision variables at the same. Computational complexity analysis is performed on the SLGH algorithm under various situations: either a gradient or gradient-free oracle of a GH function can be obtained for both deterministic and stochastic settings. The convergence rate of SLGH with a tuned hyperparameter becomes consistent with the convergence rate of gradient descent, even though the problem to be solved is gradually changed due to $t$. In numerical experiments, our SLGH algorithms show faster convergence than an existing double loop GH method while outperforming gradient descent-based methods in terms of finding a better solution."}}
{"id": "rF6zwkyMABn", "cdate": 1652737663399, "mdate": null, "content": {"title": "Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with Feedback Graphs", "abstract": "This study considers online learning with general directed feedback graphs. For this problem, we present best-of-both-worlds algorithms that achieve nearly tight regret bounds for adversarial environments as well as poly-logarithmic regret bounds for stochastic environments. As Alon et al. [2015] have shown, tight regret bounds depend on the structure of the feedback graph: strongly observable graphs yield minimax regret of $\\tilde{\\Theta}( \\alpha^{1/2} T^{1/2} )$, while weakly observable graphs induce minimax regret of $\\tilde{\\Theta}( \\delta^{1/3} T^{2/3} )$, where $\\alpha$ and $\\delta$, respectively, represent the independence number of the graph and the domination number of a certain portion of the graph. Our proposed algorithm for strongly observable graphs has a regret bound of $\\tilde{O}( \\alpha^{1/2} T^{1/2} )$ for adversarial environments, as well as of  $ {O} ( \\frac{\\alpha (\\ln T)^3 }{\\Delta_{\\min}} ) $ for stochastic environments, where $\\Delta_{\\min}$ expresses the minimum suboptimality gap. This result resolves an open question raised by Erez and Koren [2021]. We also provide an algorithm for weakly observable graphs that achieves a regret bound of $\\tilde{O}( \\delta^{1/3}T^{2/3} )$ for adversarial environments and poly-logarithmic regret for stochastic environments. The proposed algorithms are based on the follow-the-regularized-leader approach combined with newly designed update rules for learning rates."}}
{"id": "h3M00I96Ed", "cdate": 1621630011852, "mdate": null, "content": {"title": "Hybrid Regret Bounds for Combinatorial Semi-Bandits and Adversarial Linear Bandits", "abstract": "This study aims to develop bandit algorithms that automatically exploit tendencies of certain environments to improve performance, without any prior knowledge regarding the environments. We first propose an algorithm for combinatorial semi-bandits with a hybrid regret bound that includes two main features: a best-of-three-worlds guarantee and multiple data-dependent regret bounds. The former means that the algorithm will work nearly optimally in all environments in an adversarial setting, a stochastic setting, or a stochastic setting with adversarial corruptions. The latter implies that, even if the environment is far from exhibiting stochastic behavior, the algorithm will perform better as long as the environment is \"easy\" in terms of certain metrics. The metrics w.r.t. the easiness referred to in this paper include cumulative loss for optimal actions, total quadratic variation of losses, and path-length of a loss sequence. We also show hybrid data-dependent regret bounds for adversarial linear bandits, which include a first path-length regret bound that is tight up to logarithmic factors."}}
{"id": "iCoK73Q9TW2", "cdate": 1621630011525, "mdate": null, "content": {"title": "On Optimal Robustness to Adversarial Corruption in Online Decision Problems", "abstract": "This paper considers two fundamental sequential decision-making problems: the problem of prediction with expert advice and the multi-armed bandit problem.  We focus on stochastic regimes in which an adversary may corrupt losses, and we investigate what level of robustness can be achieved against adversarial corruption.  The main contribution of this paper is to show that optimal robustness can be expressed by a square-root dependency on the amount of corruption.  More precisely, we show that two classes of algorithms, anytime Hedge with decreasing learning rate and algorithms with second-order regret bounds, achieve $O( \\frac{\\log N}{\\Delta} + \\sqrt{ \\frac{C \\log N }{\\Delta} } )$-regret, where $N, \\Delta$, and $C$ represent the number of experts, the gap parameter, and the corruption level, respectively.  We further provide a matching lower bound, which means that this regret bound is tight up to a constant factor. For the multi-armed bandit problem, we also provide a nearly-tight lower bound up to a logarithmic factor."}}
{"id": "lWVoJsGyocV", "cdate": 1609459200000, "mdate": null, "content": {"title": "Tracking Regret Bounds for Online Submodular Optimization", "abstract": "In this paper, we propose algorithms for online submodular optimization with tracking regret bounds. Online submodular optimization is a generic framework for sequential decision making used to select subsets. Existing algorithms for online submodular optimization have been shown to achieve small (static) regret, which means that the algorithm\u2019s performance is comparable to the performance of a fixed optimal action. Such algorithms, however, may perform poorly in an environment that changes over time. To overcome this problem, we apply a tracking-regret-analysis framework to online submodular optimization, one by which output is assessed through comparison with time-varying optimal subsets. We propose algorithms for submodular minimization, monotone submodular maximization under a size constraint, and unconstrained submodular maximization, and we show tracking regret bounds. In addition, we show that our tracking regret bound for submodular minimization is nearly tight."}}
{"id": "bc-jwSgUVf", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Parameter-Free Algorithm for Misspecified Linear Contextual Bandits", "abstract": "We investigate the misspecified linear contextual bandit (MLCB) problem, which is a generalization of the linear contextual bandit (LCB) problem. The MLCB problem is a decision-making problem in which a learner observes $d$-dimensional feature vectors, called arms, chooses an arm from $K$ arms, and then obtains a reward from the chosen arm in each round. The learner aims to maximize the sum of the rewards over $T$ rounds. In contrast to the LCB problem, the rewards in the MLCB problem may not be represented by a linear function in feature vectors; instead, it is approximated by a linear function with additive approximation parameter $\\varepsilon \\geq 0$. In this paper, we propose an algorithm that achieves $\\tilde{O}(\\sqrt{dT\\log(K)} + \\varepsilon\\sqrt{d}T)$ regret, where $\\tilde{O}(\\cdot)$ ignores polylogarithmic factors in $d$ and $T$. This is the first algorithm that guarantees a high-probability regret bound for the MLCB problem without knowledge of the approximation parameter $\\varepsilon$."}}
{"id": "6LEwwly_mqk", "cdate": 1609459200000, "mdate": null, "content": {"title": "Near-Optimal Regret Bounds for Contextual Combinatorial Semi-Bandits with Linear Payoff Functions", "abstract": "The contextual combinatorial semi-bandit problem with linear payoff functions is a decision-making problem in which a learner chooses a set of arms with the feature vectors in each round under given constraints so as to maximize the sum of rewards of arms. Several existing algorithms have regret bounds that are optimal with respect to the number of rounds $T$. However, there is a gap of $\\tilde{O}(\\max(\\sqrt{d}, \\sqrt{k}))$ between the current best upper and lower bounds, where $d$ is the dimension of the feature vectors, $k$ is the number of the chosen arms in a round, and $\\tilde{O}(\\cdot)$ ignores the logarithmic factors. The dependence of $k$ and $d$ is of practical importance because $k$ may be larger than $T$ in real-world applications such as recommender systems. In this paper, we fill the gap by improving the upper and lower bounds. More precisely, we show that the C${}^2$UCB algorithm proposed by Qin, Chen, and Zhu (2014) has the optimal regret bound $\\tilde{O}(d\\sqrt{kT} + dk)$ for the partition matroid constraints. For general constraints, we propose an algorithm that modifies the reward estimates of arms in the C${}^2$UCB algorithm and demonstrate that it enjoys the optimal regret bound for a more general problem that can take into account other objectives simultaneously. We also show that our technique would be applicable to related problems. Numerical experiments support our theoretical results and considerations."}}
{"id": "dT8PwyPaKxO", "cdate": 1577836800000, "mdate": null, "content": {"title": "An Optimal Algorithm for Bandit Convex Optimization with Strongly-Convex and Smooth Loss", "abstract": "We consider non-stochastic bandit convex optimization with strongly-convex and smooth loss functions. For this problem, Hazan and Levy have proposed an algorithm with a regret bound of $\\tilde{O}(d..."}}
{"id": "W2ZZp9P6yo", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Tight Lower Bound and Efficient Reduction for Swap Regret", "abstract": "Swap regret, a generic performance measure of online decision-making algorithms, plays an important role in the theory of repeated games, along with a close connection to correlated equilibria in strategic games. This paper shows an $\\Omega( \\sqrt{T N\\log{N}} )$-lower bound for swap regret, where $T$ and $N$ denote the numbers of time steps and available actions, respectively. Our lower bound is tight up to a constant, and resolves an open problem mentioned, e.g., in the book by Nisan et al. (2007). Besides, we present a computationally efficient reduction method that converts no-external-regret algorithms to no-swap-regret algorithms. This method can be applied not only to the full-information setting but also to the bandit setting and provides a better regret bound than previous results."}}
