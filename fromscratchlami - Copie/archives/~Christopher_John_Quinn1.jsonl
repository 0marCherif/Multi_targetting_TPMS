{"id": "BMAX1DBoKCQ", "cdate": 1686250301729, "mdate": null, "content": {"title": "Combinatorial Stochastic-Greedy Bandit", "abstract": "We propose a novel combinatorial stochastic-greedy bandit (SGB) algorithm for combinatorial multi-armed bandit problems when no extra information other than the joint reward of the selected set of $n$ arms at each time step $t\\in [T]$ is observed. SGB adopts an optimized stochastic-explore-then-commit approach and is specifically designed for scenarios with a large set of base arms. Unlike existing methods that explore the entire set of unselected base arms during each selection step, our SGB algorithm samples only an optimized proportion of unselected arms and selects actions from this subset. We prove that our algorithm achieves a $(1-1/e)$-regret bound of $\\mathcal{O}(n^{\\frac{1}{3}} k^{\\frac{2}{3}} T^{\\frac{2}{3}} \\log(T)^{\\frac{2}{3}})$ for monotone stochastic submodular rewards, which outperforms the state-of-the-art in terms of the cardinality constraint $k$. Furthermore, we empirically evaluate the performance of our algorithm in the context of online constrained social influence maximization. Our results demonstrate that our proposed approach consistently outperforms the other algorithms, increasing the performance gap as $k$ grows."}}
{"id": "CL93Le0zhDt", "cdate": 1676827104159, "mdate": null, "content": {"title": "Size-Constrained k-Submodular Maximization in Near-Linear Time", "abstract": "We investigate the problems of maximizing k-submodular functions over total size constraints and over individual size constraints. k-submodularity is a generalization of submodularity beyond just picking items of a ground set, instead associating one of k types to chosen items.  For sensor selection problems, for instance, this enables modeling of which type of sensor to put at a location, not simply whether to put a sensor or not.   We propose and analyze threshold-greedy algorithms for both types of constraints.  We prove that our proposed algorithms achieve the best known approximation ratios for both constraint types, up to a user-chosen parameter that balances computational complexity and the approximation ratio, while only using a number of function evaluations that depends linearly (up to poly-logarithmic terms) on the number of elements n, the number of types k, and the inverse of the user chosen parameter. Other algorithms that achieve the best-known deterministic approximation ratios require a number of function evaluations that depend linearly on the budget B, while our methods do not.  We empirically demonstrate our algorithms' performance in applications of sensor placement with k types and influence maximization with k topics."}}
{"id": "wDM0wGXFVU", "cdate": 1672531200000, "mdate": 1682634641591, "content": {"title": "Randomized Greedy Learning for Non-monotone Stochastic Submodular Maximization Under Full-bandit Feedback", "abstract": "We investigate the problem of unconstrained combinatorial multi-armed bandits with full-bandit feedback and stochastic rewards for submodular maximization. Previous works investigate the same problem assuming a submodular and monotone reward function. In this work, we study a more general problem, i.e., when the reward function is not necessarily monotone, and the submodularity is assumed only in expectation. We propose Randomized Greedy Learning (RGL) algorithm and theoretically prove that it achieves a $\\frac{1}{2}$-regret upper bound of $\\tilde{\\mathcal{O}}(n T^{\\frac{2}{3}})$ for horizon $T$ and number of arms $n$. We also show in experiments that RGL empirically outperforms other full-bandit variants in submodular and non-submodular settings."}}
{"id": "O8ehzt2Mfem", "cdate": 1672531200000, "mdate": 1682634641590, "content": {"title": "A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback", "abstract": "We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\\alpha$-regret methods that only require bandit feedback, achieving $\\mathcal{O}\\left(T^\\frac{2}{3}\\log(T)^\\frac{1}{3}\\right)$ expected cumulative $\\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to multiple problems in submodular maximization, adapting approximation algorithms for cardinality and for knapsack constraints. The new CMAB algorithms for knapsack constraints outperform a full-bandit method developed for the adversarial setting in experiments with real-world data."}}
{"id": "Hg-IBdIo5e9", "cdate": 1646077549155, "mdate": null, "content": {"title": "An Explore-then-Commit Algorithm for Submodular Maximization Under Full-bandit Feedback", "abstract": "We investigate the problem of combinatorial multi-armed bandits with stochastic submodular (in expectation) rewards and full-bandit feedback, where no extra information other than the reward of selected action at each time step $t$ is observed. We propose a simple algorithm, Explore-Then-Commit Greedy (ETCG) and prove that it  achieves a $(1-1/e)$-regret upper bound of $\\mathcal{O}(n^\\frac{1}{3}k^\\frac{4}{3}T^\\frac{2}{3}\\log(T)^\\frac{1}{2})$ for a horizon $T$, number of base elements $n$, and cardinality constraint $k$. We also show in experiments with synthetic and real-world data that the ETCG empirically outperforms other full-bandit methods."}}
{"id": "cWXBU8iwyC", "cdate": 1640995200000, "mdate": 1682634641592, "content": {"title": "An explore-then-commit algorithm for submodular maximization under full-bandit feedback", "abstract": "We investigate the problem of combinatorial multi-armed bandits with stochastic submodular (in expectation) rewards and full-bandit feedback, where no extra information other than the reward of sel..."}}
{"id": "jwIi4nZhkzG", "cdate": 1609459200000, "mdate": 1632860808668, "content": {"title": "DART: Adaptive Accept Reject Algorithm for Non-Linear Combinatorial Bandits", "abstract": "We consider the bandit problem of selecting K out of N arms at each time step. The joint reward can be a non-linear function of the rewards of the selected individual arms. The direct use of a multi-armed bandit algorithm requires choosing among all possible combinations, making the action space large. To simplify the problem, existing works on combinatorial bandits typically assume feedback as a linear function of individual rewards. In this paper, we prove the lower bound for top-K subset selection with bandit feedback with possibly correlated rewards. We present a novel algorithm for the combinatorial setting without using individual arm feedback or requiring linearity of the reward function. Additionally, our algorithm works on correlated rewards of individual arms. Our algorithm, aDaptive Accept RejecT (DART), sequentially finds good arms and eliminates bad arms based on confidence bounds. DART is computationally efficient and uses storage linear in N. Further, DART achieves a regret bound of \u00d5(K\u221aKNT) for a time horizon T, which matches the lower bound in bandit feedback up to a factor of \u221alog 2NT. When applied to the problem of cross-selling optimization and maximizing the mean of individual rewards, the performance of the proposed algorithm surpasses that of state-of-the-art algorithms. We also show that DART significantly outperforms existing methods for both linear and non-linear joint reward environments."}}
{"id": "jhSerBTHDu", "cdate": 1609459200000, "mdate": 1682634641591, "content": {"title": "Stochastic Top K-Subset Bandits with Linear Space and Non-Linear Feedback with Applications to Social Influence Maximization", "abstract": ""}}
{"id": "d-eRrZ1QmvRg", "cdate": 1609459200000, "mdate": 1632860808496, "content": {"title": "Stochastic Top-K Subset Bandits with Linear Space and Non-Linear Feedback", "abstract": "Many real-world problems like Social Influence Maximization face the dilemma of choosing the best $K$ out of $N$ options at a given time instant. This setup can be modeled as a combinatorial bandit..."}}
{"id": "STP-vugEz79", "cdate": 1609459200000, "mdate": 1648668782561, "content": {"title": "Information Flow in Markov Chains", "abstract": "We consider the problem of characterizing the flow of information in stochastic systems. Recently, several measures of partial information decomposition (PID) have been proposed which, for a fixed target variable, can distinguish unique, redundant, and synergistic contributions from the predictor variables. We study how each of those partial informations travel in a Markov chain, entering at one variable, passing through several variables, and eventually exiting downstream. Our work is agnostic to specific partial information decomposition (PID) measures. We investigate partial information flow among variables relating to overflow events in a river system."}}
