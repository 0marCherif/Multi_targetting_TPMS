{"id": "ENsP1O_X65", "cdate": 1677628800000, "mdate": 1700056800446, "content": {"title": "A variational hardcut EM algorithm for the mixtures of Gaussian processes", "abstract": ""}}
{"id": "oaXEWV-bfy", "cdate": 1672531200000, "mdate": 1699626784527, "content": {"title": "GradSalMix: Gradient Saliency-Based Mix for Image Data Augmentation", "abstract": "The success of CutMix in image classification has sparked interest in saliency-based mix augmentation methods, which refer to detecting saliency regions to generate more valid images. However, existing mix works either require external tools to locate saliency regions, or rely on additional complex optimization policy for generating new images, which limits their application ranges. To address these deficiencies, we propose Gradient Saliency-based Mix (GradSalMix), a simple yet more general mix augmentation, whose operations are all based on the gradients of the training neural network itself. Specifically, we first locate the saliency regions of two images via their gradients of manifolds, and then directly migrate the region, sampled around the center with a large gradient response value, from one image to another. Afterwards, the labels of images are weighted by their accumulated gradient values for new soft labels, which are shown more accurate than the ones weighted by area ratio. The experimental results show that our proposed method outperforms previous works, in terms of accuracy and robustness against adversarial attacks, on four image classification benchmarks. Moreover, extensive experiments on object detection and point cloud classification also verify the superiority and generality of our method."}}
{"id": "jpja0E29ZFM", "cdate": 1672531200000, "mdate": 1700056801088, "content": {"title": "FaultSeg Swin-UNETR: Transformer-Based Self-Supervised Pretraining Model for Fault Recognition", "abstract": "This paper introduces an approach to enhance seismic fault recognition through self-supervised pretraining. Seismic fault interpretation holds great significance in the fields of geophysics and geology. However, conventional methods for seismic fault recognition encounter various issues, including dependence on data quality and quantity, as well as susceptibility to interpreter subjectivity. Currently, automated fault recognition methods proposed based on small synthetic datasets experience performance degradation when applied to actual seismic data. To address these challenges, we have introduced the concept of self-supervised learning, utilizing a substantial amount of relatively easily obtainable unlabeled seismic data for pretraining. Specifically, we have employed the Swin Transformer model as the core network and employed the SimMIM pretraining task to capture unique features related to discontinuities in seismic data. During the fine-tuning phase, inspired by edge detection techniques, we have also refined the structure of the Swin-UNETR model, enabling multiscale decoding and fusion for more effective fault detection. Experimental results demonstrate that our proposed method attains state-of-the-art performance on the Thebe dataset, as measured by the OIS and ODS metrics."}}
{"id": "e70dzXj0nR", "cdate": 1672531200000, "mdate": 1700056801007, "content": {"title": "UCLD-Net: Decoupling Network via Unsupervised Contrastive Learning for Image Dehazing", "abstract": "From traditional algorithms based on handcrafted prior to learning algorithms based on neural networks, the image dehazing technique has gone through great development. The handcrafted prior-based methods need to first estimate the transmission map and atmosphere light in the atmospheric scattering model separately, and then calculate the final haze-free image, which often leads to a gradual accumulation of errors. In contrast, in the end-to-end neural network-based methods, supervised learning with labels is a major element for the improvement of the dehazing effect. But in the physical situation, paired (hazy, haze-free) images are difficult to collect, which limits the application scope of supervised dehazing. To address this deficiency, we propose a Decoupling Network for image dehazing via Unsupervised Contrastive Learning mechanism which is widely used in self-supervised representation learning, named UCLD-Net. Specifically, we use the estimated transmission map and atmosphere light to design the structure of UCLD-Net and introduce prior knowledge to construct its loss function. It is demonstrated by the experiments that UCLD-Net achieves comparable results in the dehazing experiments on the benchmark RESIDE dataset, which sufficiently verifies its effectiveness."}}
{"id": "c-tt5baQal", "cdate": 1672531200000, "mdate": 1682395162113, "content": {"title": "Is Bigger Always Better? An Empirical Study on Efficient Architectures for Style Transfer and Beyond", "abstract": "Network architecture plays a pivotal role in style transfer. Most existing algorithms use VGG19 as the feature extractor, which incurs a high computational cost. In this work, we conduct an empirical study on the popular network architectures and find that some more efficient networks can replace VGG19 while having comparable style transfer performance. Beyond that, we show that an efficient network can be further accelerated by removing its empty channels via a simple channel pruning method tweaked for style transfer. To prevent the potential performance drop due to using a more lightweight network and obtain better style transfer results, we introduce a more accurate deep feature alignment strategy to improve existing style transfer modules. Taking GoogLeNet as an exemplary efficient network, the pruned GoogLeNet with the improved style transfer module is 2.3 ~ 107.4\u00d7 faster than the state-of-the-art approaches and can achieve 68.03 FPS on 512\u00d7512 images. Extensive experiments demonstrate that VGG19 can be replaced by a more lightweight network with significantly improved efficiency and comparable style transfer quality."}}
{"id": "b5Bnhvzy4AQ", "cdate": 1672531200000, "mdate": 1699155806163, "content": {"title": "Overcoming Catastrophic Forgetting for Fine-Tuning Pre-trained GANs", "abstract": "The great transferability of DNNs has induced a popular paradigm of \u201cpre-training & fine-tuning\u201d, by which a data-scarce task can be performed much more easily. However, compared to the existing efforts made in the context of supervised transfer learning, fewer explorations have been made on effectively fine-tuning pre-trained Generative Adversarial Networks (GANs). As reported in recent empirical studies, fine-tuning GANs faces the similar challenge of catastrophic forgetting as in supervised transfer learning. This causes a severe capacity loss of the pre-trained model when adapting it to downstream datasets. While most existing approaches suggest to directly interfere parameter updating, this paper introduces novel schemes from another perspective, i.e. inputs and features, thus essentially focuses on data aspect. Firstly, we adopt a trust-region method to smooth the adaptation dynamics by progressively adjusting input distributions, aiming to avoid dramatic parameter changes, especially when the pre-trained GAN has no information of target data. Secondly, we aim to avoid the loss of the diversity of the generated results of the fine-tuned GAN. This is achieved by explicitly encouraging generated images to encompass diversified spectral components in their deep features. We theoretically study the rationale of the proposed schemes, and conduct extensive experiments on popular transfer learning benchmarks to demonstrate the superiority of the schemes. The code and corresponding supplemental materials are available at https://github.com/zezeze97/Transfer-Pretrained-Gan ."}}
{"id": "XoGUpk3qxk", "cdate": 1672531200000, "mdate": 1700056800931, "content": {"title": "Automatic Text Extractive Summarization Based on Text Graph Representation and Attention Matrix", "abstract": "Automatic text summarization via representing a text as a graph has been investigated for over ten years. With the developments of attention mechanism and Transformer on natural language processing (NLP), it is possible to make a connection between the graph and attention structure for a text. In this paper, we propose an attention matrix text graph model for extractive text summarization. Specifically, an attention matrix between all the sentences of the whole text is adopted as a weighted adjacent matrix of a fully connected graph of the text where each node represents a sentence, which can be computed using the pre-training language model. The GCN is further applied to the text graph model for classifying all the nodes and finding out the salient sentences from the text to generate a summary. It is demonstrated by the experimental results on two typical datasets that our proposed model can achieve a competitive result in comparison with state-of-the-art models."}}
{"id": "RpcIt94AWh", "cdate": 1672531200000, "mdate": 1700056800880, "content": {"title": "One-Dimensional Feature Supervision Network for Object Detection", "abstract": "Self-attention mechanisms have been widely used in object detection tasks to distinguish the importance of different channels and reinforce important information in features, and also leads to the exciting results at all scales. However, most of the self-attentive mechanisms, as well as their variants, focus only on the channel dimension and thus easily ignore the wide and high dimensions of the feature map that play an important role in capturing local contextual information. To alleviate this problem, in this paper we propose an one-dimensional feature supervision network for object detection (1DSNet). Specifically, we first propose an one-dimensional feature supervision module (1DSM). It uses a lightweight one-dimensional feature vector to weight the features from the width and height perspectives, respectively, for jointly reinforcing the important information in the features. Moreover, in order to improve the representation of multi-scale feature context information, we construct a receptive field dilated pyramid pooling (RFD-SPP) that can obtain a larger field of view based on the spatial pyramid pooling. Finally, experimental results demonstrate that our proposed 1DSNet is effective and competitive when compared with some representative methods."}}
{"id": "PtxjyEnkcY", "cdate": 1672531200000, "mdate": 1700056800954, "content": {"title": "PCSalmix: Gradient Saliency-Based Mix Augmentation for Point Cloud Classification", "abstract": "Point cloud classification has sparked many researchers\u2019 interest for its cornerstone role in 3D applications. Inheriting the CutMix series augmentation that performs well in 2D images, PointCutMix and RSMix are proposed to generate new samples for 3D point clouds, by replacing partial points of one cloud with those of another. However, the selection of mixed regions is all built on randomness, ignoring the significance of point clouds\u2019 saliency. To address this deficiency, we propose PCSalMix: a novel Saliency-based Mix augmentation for Point Cloud classification. The gradient of classification network on inputs is a natural tool to locate the saliency. Based on this discovery, we extract points with larger gradient values to make more representative samples. Afterward, the soft labels are weighted more accurately by accumulated gradients rather than count ratios of points. The experimental results verify the outperformance of our method on ModelNet40 and ModelNet10 benchmarks in terms of accuracy and robustness against adversarial attacks."}}
{"id": "HtYiLI6oiAc", "cdate": 1672531200000, "mdate": 1700056800988, "content": {"title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism", "abstract": "Recently, a variety of methods under the name of non-contrastive learning (like BYOL, SimSiam, SwAV, DINO) show that when equipped with some asymmetric architectural designs, aligning positive pairs alone is sufficient to attain good performance in self-supervised visual learning. Despite some understandings of some specific modules (like the predictor in BYOL), there is yet no unified theoretical understanding of how these seemingly different asymmetric designs can all avoid feature collapse, particularly considering methods that also work without the predictor (like DINO). In this work, we propose a unified theoretical understanding for existing variants of non-contrastive learning. Our theory named Rank Differential Mechanism (RDM) shows that all these asymmetric designs create a consistent rank difference in their dual-branch output features. This rank difference will provably lead to an improvement of effective dimensionality and alleviate either complete or dimensional feature collapse. Different from previous theories, our RDM theory is applicable to different asymmetric designs (with and without the predictor), and thus can serve as a unified understanding of existing non-contrastive learning methods. Besides, our RDM theory also provides practical guidelines for designing many new non-contrastive variants. We show that these variants indeed achieve comparable performance to existing methods on benchmark datasets, and some of them even outperform the baselines. Our code is available at \\url{https://github.com/PKU-ML/Rank-Differential-Mechanism}."}}
