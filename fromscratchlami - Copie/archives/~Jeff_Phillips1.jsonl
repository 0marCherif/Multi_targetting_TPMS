{"id": "TkQ1sxd9P4", "cdate": 1663850161683, "mdate": null, "content": {"title": "Interpretable Debiasing of Vectorized Language Representations with Iterative Orthogonalization", "abstract": "We propose a new mechanism to augment a word vector embedding representation that offers improved bias removal while retaining the key information\u2014resulting in improved interpretability of the representation. Rather than removing the information associated with a concept that may induce bias, our proposed method identifies two concept subspaces and makes them orthogonal. The resulting representation has these two concepts uncorrelated. Moreover, because they are orthogonal, one can simply apply a rotation on the basis of the representation so that the resulting subspace corresponds with coordinates. This explicit encoding of concepts to coordinates works because they have been made fully orthogonal, which previous approaches do not achieve. Furthermore, we show that this can be extended to multiple subspaces. As a result, one can choose a subset of concepts to be represented transparently and explicitly, while the others are retained in the mixed but extremely expressive format of the representation."}}
{"id": "MNQMy2MpbcO", "cdate": 1652737648233, "mdate": null, "content": {"title": "Batch Multi-Fidelity Active Learning with Budget Constraints", "abstract": "Learning functions with high-dimensional outputs is critical in many applications, such as physical simulation and engineering design. However, collecting training examples for these applications is often costly, e.g., by running numerical solvers. The recent work (Li et al., 2022) proposes the first multi-fidelity active learning approach for high-dimensional outputs, which can acquire examples at different fidelities to reduce the cost while improving the learning performance. However,  this method only queries at one pair of fidelity and input at a time, and hence has a risk of bringing in strongly correlated examples to reduce the learning efficiency. In this paper, we propose Batch Multi-Fidelity Active Learning with Budget Constraints (BMFAL-BC), which can promote the diversity of training examples to improve the benefit-cost ratio, while respecting a given budget constraint for batch queries. Hence, our method can be more practically useful. Specifically, we propose a novel batch acquisition function that measures the mutual information between a batch of multi-fidelity queries and the target function, so as to penalize highly correlated queries and encourages diversity. The optimization of the batch acquisition function is challenging in that it involves a combinatorial search over many fidelities while subject to the budget constraint. To address this challenge, we develop a weighted greedy algorithm that can sequentially identify each (fidelity, input) pair, while achieving a near $(1 - 1/e)$-approximation of the optimum. We show the advantage of our method in several computational physics and engineering applications."}}
{"id": "Nh7CtbyoqV5", "cdate": 1632875675524, "mdate": null, "content": {"title": "Normalization of Language Embeddings for Cross-Lingual Alignment", "abstract": "Learning a good transfer function to map the word vectors from two languages into a shared cross-lingual word vector space plays a crucial role in cross-lingual NLP. It is useful in translation tasks and important in allowing complex models built on a high-resource language like English to be directly applied on an aligned low resource language.  While Procrustes and other techniques can align language models with some success, it has recently been identified that structural differences (for instance, due to differing word frequency) create different profiles for various monolingual embedding. When these profiles differ across languages, it correlates with how well languages can align and their performance on cross-lingual downstream tasks.  In this work, we develop a very general language embedding normalization procedure, building and subsuming various previous approaches, which removes these structural profiles across languages without destroying their intrinsic meaning.  We demonstrate that meaning is retained and alignment is improved on similarity, translation, and cross-language classification tasks.  Our proposed normalization clearly outperforms all prior approaches like centering and vector normalization on each task and with each alignment approach. "}}
{"id": "8PPmLbV6WiQ", "cdate": 1621629919935, "mdate": null, "content": {"title": "Normalization of Language Embeddings for Cross-Lingual Alignment", "abstract": "Learning a good transfer function to map the word vectors from two languages into a shared cross-lingual word vector space plays a crucial role in cross-lingual NLP. It is useful in translation tasks and important in allowing complex models built on a high-resource language like English to be directly applied on an aligned low resource language. \u00a0While previous approaches have identified Procrustes and other techniques to align language models with some success, it has recently been identified that structural differences (for instance, due to differing word frequency) create different profiles for various monolingual embedding. When these profiles differ across languages, it correlates with how well languages can align and their performance on cross-lingual downstream tasks. \u00a0In this work, we develop a very general language embedding normalization procedure, building and subsuming various previous approaches, which removes these structural profiles across languages without destroying their intrinsic meaning. \u00a0We demonstrate that meaning is retained, and alignment is improved on similarity, translation, and cross-language classification tasks. \u00a0Our proposed normalization clearly outperforms all prior approaches like centering and vector normalization on each task and with each alignment approach."}}
{"id": "swur4c3YSyF", "cdate": 1621629789478, "mdate": null, "content": {"title": "Self-Adaptable Point Processes with Nonparametric Time Decays", "abstract": "Many applications involve multi-type event data. Understanding the complex influences of the events on each other is critical to discover useful knowledge and to predict future events and their types. Existing methods either ignore or partially account for these influences. Recent works use recurrent neural networks to model the event rate. While being highly expressive, they couple all the temporal dependencies in a black-box and can hardly extract meaningful knowledge. More important, most methods assume an exponential time decay of the influence strength, which is over-simplified and can miss many important strength varying patterns.  To overcome these limitations, we propose SPRITE, a $\\underline{S}$elf-adaptable  $\\underline{P}$oint p$\\underline{R}$ocess w$\\underline{I}$th nonparametric $\\underline{T}$ime d$\\underline{E}$cays, which can decouple the influences between every pair of the events and capture various time decays of the influence strengths. Specifically, we use an embedding to represent each event type and model the event influence as an unknown function of the embeddings and time span. We derive a general construction that can cover all possible time decaying functions. By placing Gaussian process (GP) priors over the latent functions and using Gauss-Legendre quadrature to obtain the integral in the construction, we can flexibly estimate all kinds of time-decaying influences, without restricting to any specific form or imposing derivative constraints that bring learning difficulties.  We then use weight space augmentation of GPs to develop an efficient stochastic variational learning algorithm. We show the advantages of our approach in both the ablation study and real-world applications. "}}
{"id": "umfVgsTdxXJ", "cdate": 1609459200000, "mdate": null, "content": {"title": "VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word Representations", "abstract": "Word vector embeddings have been shown to contain and amplify biases in data they are extracted from. Consequently, many techniques have been proposed to identify, mitigate, and attenuate these biases in word representations. In this paper, we utilize interactive visualization to increase the interpretability and accessibility of a collection of state-of-the-art debiasing techniques. To aid this, we present Visualization of Embedding Representations for deBiasing system (\"VERB\"), an open-source web-based visualization tool that helps the users gain a technical understanding and visual intuition of the inner workings of debiasing techniques, with a focus on their geometric properties. In particular, VERB offers easy-to-follow use cases in exploring the effects of these debiasing techniques on the geometry of high-dimensional word vectors. To help understand how various debiasing techniques change the underlying geometry, VERB decomposes each technique into interpretable sequences of primitive transformations and highlights their effect on the word vectors using dimensionality reduction and interactive visual exploration. VERB is designed to target natural language processing (NLP) practitioners who are designing decision-making systems on top of word embeddings, and also researchers working with fairness and ethics of machine learning systems in NLP. It can also serve as a visual medium for education, which helps an NLP novice to understand and mitigate biases in word embeddings."}}
{"id": "tQAmRvw2cKL", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Deterministic Streaming Sketch for Ridge Regression", "abstract": "We provide a deterministic space-efficient algorithm for estimating ridge regression. For n data points with d features and a large enough regularization parameter, we provide a solution within eps L_2 error using only O(d/eps) space. This is the first o(d^2) space deterministic streaming algorithm with guaranteed solution error and risk bound for this classic problem. The algorithm sketches the covariance matrix by variants of Frequent Directions, which implies it can operate in insertion-only streams and a variety of distributed data settings. In comparisons to randomized sketching algorithms on synthetic and real-world datasets, our algorithm has less empirical error using less space and similar time."}}
{"id": "KY-RNNI6RIa", "cdate": 1609459200000, "mdate": null, "content": {"title": "Inferencing hourly traffic volume using data-driven machine learning and graph theory", "abstract": "Highlights \u2022 We developed a tree ensemble model to spatially predict hourly traffic volume. \u2022 We explored the spatial dependency among road segments. \u2022 We propose an innovative approach by leveraging large-scale trajectory data. \u2022 Results indicate our model will generalize to accurately predict traffic volumes. Abstract Traffic volume is a critical piece of information in many applications, such as transportation long-range planning and traffic operation analysis. Effectively capturing traffic volumes on a network scale is beneficial to Transportation Systems Management & Operations (TSM&O). Yet it is impractical to install sensors to cover a large road network. To address this issue, spatial prediction techniques are widely performed to estimate traffic volumes at sites without sensors. In retrospect, most relevant studies resort to machine learning methods and treat each prediction location independently during the training process, ignoring the potential spatial dependency among them. This paper presents an innovative spatial prediction method of hourly traffic volume on a network scale. To achieve this, we applied a state-of-the-art tree ensemble model - extreme gradient boosting tree (XGBoost) - to handle the large-scale features and hourly traffic volume samples, due to the model's powerful scalability. Moreover, spatial dependency among road segments is taken into account in the proposed model using graph theory. Specifically, we created a traffic network graph leveraging probe trajectory data, and implemented a graph-based approach - breadth first search (BFS) - to search neighboring sites in this graph for computing spatial dependency. The proposed spatial dependency feature is subsequently incorporated as a new feature fed into XGBoost. The proposed model is tested on the road network in the state of Utah. Numerical results not only indicate high computational efficiency of the proposed model, but also demonstrate significant improvement in prediction accuracy of hourly traffic volume comparing with the benchmarked models."}}
{"id": "G--1nRsSO8t", "cdate": 1609459200000, "mdate": null, "content": {"title": "Closed form word embedding alignment", "abstract": "We develop a family of techniques to align word embeddings which are derived from different source datasets or created using different mechanisms (e.g., GloVe or word2vec). Our methods are simple and have a closed form to optimally rotate, translate, and scale to minimize root mean squared errors or maximize the average cosine similarity between two embeddings of the same vocabulary into the same dimensional space. Our methods extend approaches known as absolute orientation, which are popular for aligning objects in three dimensions, and generalize an approach by Smith et al. (ICLR 2017). We prove new results for optimal scaling and for maximizing cosine similarity. Then, we demonstrate how to evaluate the similarity of embeddings from different sources or mechanisms, and that certain properties like synonyms and analogies are preserved across the embeddings and can be enhanced by simply aligning and averaging ensembles of embeddings."}}
{"id": "mCEkXmZtHil", "cdate": 1599281780411, "mdate": null, "content": {"title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings", "abstract": "Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated."}}
