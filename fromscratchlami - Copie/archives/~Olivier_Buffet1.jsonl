{"id": "bQNG7HAs8sX", "cdate": 1676827077131, "mdate": null, "content": {"title": "Monte-Carlo Search for an Equilibrium in Dec-POMDPs", "abstract": "Decentralized partially observable Markov decision processes (Dec-POMDPs) formalize the problem of designing individual controllers for a group of collaborative agents under stochastic dynamics and partial observability. Seeking a global optimum is difficult (NEXP complete), but seeking a Nash equilibrium \u2014 each agent policy being a best response to the other agents \u2014 is more accessible, and allowed addressing infinite-horizon problems with solutions in the form of finite state controllers. In this paper, we show that this approach can be adapted to cases where only a generative model (a simulator) of the Dec-POMDP is available. This requires relying on a simulation-based POMDP solver to construct an agent\u2019s FSC node by node. A related process is used to heuristically derive initial FSCs. Experiment with benchmarks shows that MC-JESP is competitive with existing Dec-POMDP solvers, even better than many offline methods using explicit models."}}
{"id": "vZNdp7H8nav", "cdate": 1609459200000, "mdate": null, "content": {"title": "Monte Carlo Information-Oriented Planning", "abstract": "In this article, we discuss how to solve information-gathering problems expressed as rho-POMDPs, an extension of Partially Observable Markov Decision Processes (POMDPs) whose reward rho depends on the belief state. Point-based approaches used for solving POMDPs have been extended to solving rho-POMDPs as belief MDPs when its reward rho is convex in B or when it is Lipschitz-continuous. In the present paper, we build on the POMCP algorithm to propose a Monte Carlo Tree Search for rho-POMDPs, aiming for an efficient on-line planner which can be used for any rho function. Adaptations are required due to the belief-dependent rewards to (i) propagate more than one state at a time, and (ii) prevent biases in value estimates. An asymptotic convergence proof to epsilon-optimal values is given when rho is continuous. Experiments are conducted to analyze the algorithms at hand and show that they outperform myopic approaches."}}
{"id": "tD8eA0ChCe", "cdate": 1577836800000, "mdate": null, "content": {"title": "Monte Carlo Information-Oriented Planning", "abstract": "In this article, we discuss how to solve information-gathering problems expressed as \u03c1-POMDPs, an extension of Partially Observable Markov Decision Processes (POMDPs) whose reward \u03c1 depends on the belief state. Point-based approaches used for solving POMDPs have been extended to solving \u03c1-POMDPs as belief MDPs when its reward \u03c1 is convex in B or when it is Lipschitz-continuous. In the present paper, we build on the POMCP algorithm to propose a Monte Carlo Tree Search for \u03c1-POMDPs, aiming for an efficient on-line planner which can be used for any \u03c1 function. Adaptations are required due to the belief-dependent rewards to (i) propagate more than one state at a time, and (ii) prevent biases in value estimates. An asymptotic convergence proof to \u00cf\u0163-optimal values is given when \u03c1 is continuous. Experiments are conducted to analyze the algorithms at hand and show that they outperform myopic approaches."}}
{"id": "pkt2T-c8YKr", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimally Solving Two-Agent Decentralized POMDPs Under One-Sided Information Sharing", "abstract": "Optimally solving decentralized partially observable Markov decision processes under either full or no information sharing received significant attention in recent years. However, little is known a..."}}
{"id": "Eh-WKcDiIzJ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Solving K-MDPs", "abstract": "Markov Decision Processes (MDPs) are employed to model sequential decision-making problems under uncertainty. Traditionally, algorithms to solve MDPs have focused on solving large state or action spaces. With increasing applications of MDPs to human-operated domains such as conservation of biodiversity and health, developing easy-to-interpret solutions is of paramount importance to increase uptake of MDP policies. Here, we define the problem of solving K-MDPs, i.e., given an original MDP and a constraint on the number of states (K), generate a reduced state space MDP that minimizes the difference between the original optimal MDP value function and the reduced optimal K-MDP value function. Building on existing non-transitive and transitive approximate state abstraction functions, we propose a family of three algorithms based on binary search with sub-optimality bounded polynomially in a precision parameter: \u03d5Q*\u03b5K-MDP-ILP, \u03d5Q*dK-MDP and \u03d5a*dK-MDP. We compare these algorithms to a greedy algorithm (\u03d5Q*\u03b5 Greedy K-MDP) and clustering approach (k-means++ K-MDP). On randomly generated MDPs and two computational sustainability MDPs, \u03d5a*dK-MDP outperformed all algorithms when it could find a feasible solution. While numerous state abstraction problems have been proposed in the literature, this is the first time that the general problem of solving K-MDPs is suggested. We hope that our work will generate future research aiming at increasing the interpretability of MDP policies in human-operated domains."}}
{"id": "9pC1xUV6PzL", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Bellman's Optimality Principle for zs-POSGs", "abstract": "Many non-trivial sequential decision-making problems are efficiently solved by relying on Bellman's optimality principle, i.e., exploiting the fact that sub-problems are nested recursively within the original problem. Here we show how it can apply to (infinite horizon) 2-player zero-sum partially observable stochastic games (zs-POSGs) by (i) taking a central planner's viewpoint, which can only reason on a sufficient statistic called occupancy state, and (ii) turning such problems into zero-sum occupancy Markov games (zs-OMGs). Then, exploiting the Lipschitz-continuity of the value function in occupancy space, one can derive a version of the HSVI algorithm (Heuristic Search Value Iteration) that provably finds an $\\epsilon$-Nash equilibrium in finite time."}}
{"id": "xwTr0kmCNDR", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning to Act in Continuous Dec-POMDPs", "abstract": "We address a long-standing open problem of reinforcement learning in continuous decentralized partially observable Markov decision processes. Previous attempts focused on different forms of generalized policy iteration, which at best led to local optima. In this paper, we restrict attention to plans, which are simpler to store and update than policies. We derive, under mild conditions, the first optimal cooperative multi-agent reinforcement learning algorithm. To achieve significant scalability gains, we replace the greedy maximization by mixed-integer linear programming. Experiments show our approach can learn to act optimally in many finite domains from the literature."}}
{"id": "SkEA3cb_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning to Act in Decentralized Partially Observable MDPs", "abstract": "We address a long-standing open problem of reinforcement learning in decentralized partially observable Markov decision processes. Previous attempts focussed on different forms of generalized polic..."}}
{"id": "HyZYXu-OZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions", "abstract": "Many state-of-the-art algorithms for solving Partially Observable Markov Decision Processes (POMDPs) rely on turning the problem into a \u201cfully observable\u201d problem\u2014a belief MDP\u2014and exploiting the piece-wise linearity and convexity (PWLC) of the optimal value function in this new state space (the belief simplex \u2206). This approach has been extended to solving \u03c1-POMDPs\u2014i.e., for information-oriented criteria\u2014when the reward \u03c1 is convex in \u2206. General \u03c1-POMDPs can also be turned into \u201cfully observable\u201d problems, but with no means to exploit the PWLC property. In this paper, we focus on POMDPs and \u03c1-POMDPs with \u03bb \u03c1 -Lipschitz reward function, and demonstrate that, for finite horizons, the optimal value function is Lipschitz-continuous. Then, value function approximators are proposed for both upper- and lower-bounding the optimal value function, which are shown to provide uniformly improvable bounds. This allows proposing two algorithms derived from HSVI which are empirically evaluated on various benchmark problems."}}
{"id": "PzfTytuBoNV", "cdate": 1451606400000, "mdate": null, "content": {"title": "Intersections intelligentes pour le contr\u00f4le de v\u00e9hicules sans pilote. Coordination locale et optimisation globale", "abstract": ""}}
