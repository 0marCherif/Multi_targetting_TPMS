{"id": "5Qt6ZqXSDEZ", "cdate": 1663850394212, "mdate": null, "content": {"title": "From Adaptive Query Release to Machine Unlearning", "abstract": "We formalize the problem of machine unlearning as design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We give efficient unlearning algorithms for linear and prefix-sum query classes. As applications, we show that unlearning in many problems, in particular, stochastic convex optimization (SCO),  can be reduced to the above, yielding improved guarantees for the problem. In particular, for smooth Lipschitz losses and any $\\rho>0$, our results yield an unlearning algorithm with excess population risk of $\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\rho}\\big)$ with unlearning query (gradient) complexity $\\tilde O(\\rho \\cdot \\text{Retraining Complexity})$, where $d$ is the model dimensionality and $n$ is the initial number of samples. For non-smooth Lipschitz losses, we give an unlearning algorithm with excess population risk $\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\big(\\frac{\\sqrt{d}}{n\\rho}\\big)^{1/2}\\big)$ with the same unlearning query (gradient) complexity. Furthermore, in the special case of Generalized Linear Models (GLMs), such as those in linear and logistic regression, we get dimension-independent rates of $\\tilde O\\big(\\frac{1}{\\sqrt{n}} +\\frac{1}{(n\\rho)^{2/3}}\\big)$ and $\\tilde O\\big(\\frac{1}{\\sqrt{n}} +\\frac{1}{(n\\rho)^{1/3}}\\big)$ for smooth Lipschitz and non-smooth Lipschitz losses respectively. Finally, we give generalizations of the above from one unlearning request to dynamic streams consisting of insertions and deletions."}}
{"id": "agihaAKJ89X", "cdate": 1652737834114, "mdate": null, "content": {"title": "Differentially Private Generalized Linear Models Revisited", "abstract": "We study the problem of $(\\epsilon,\\delta)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an  upper bound on the excess population risk of $\\tilde{O}\\left(\\frac{\\Vert w^*\\Vert}{\\sqrt{n}} + \\min\\left\\{\\frac{\\Vert w^* \\Vert^2}{(n\\epsilon)^{2/3}},\\frac{\\sqrt{d}\\Vert w^*\\Vert^2}{n\\epsilon}\\right\\}\\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\\Vert w^\\ast\\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\\tilde{\\Omega}\\left(\\frac{1}{\\sqrt{n}} + {\\min\\left\\{\\frac{\\Vert w^*\\Vert^{4/3}}{(n\\epsilon)^{2/3}}, \\frac{\\sqrt{d}\\Vert w^*\\Vert}{n\\epsilon}\\right\\}}\\right)$. We also revisit the previously studied case of Lipschitz losses \\cite{SSTT21}.  For this case, we close the gap in the existing work and show that the optimal rate is (up to log factors) $\\Theta\\left(\\frac{\\Vert w^*\\Vert}{\\sqrt{n}} + \\min\\left\\{\\frac{\\Vert w^*\\Vert}{\\sqrt{n\\epsilon}},\\frac{\\sqrt{\\text{rank}}\\Vert w^*\\Vert}{n\\epsilon}\\right\\}\\right)$, where $\\text{rank}$ is the rank of the design matrix. This improves over existing work in the high privacy regime. Finally, our algorithms involve a private model selection approach that we develop to enable attaining the stated rates without a-priori knowledge of $\\Vert w^*\\Vert$. "}}
{"id": "bt25vx3aW_", "cdate": 1652737785991, "mdate": null, "content": {"title": "Adversarial Robustness is at Odds with Lazy Training", "abstract": "Recent works show that adversarial examples exist for random neural networks [Daniely and Schacham, 2020] and that these examples can be found using a single step of gradient ascent [Bubeck et al., 2021]. In this work, we extend this line of work to ``lazy training'' of neural networks -- a dominant model in deep learning theory in which neural networks are provably efficiently learnable. We show that over-parametrized neural networks that are guaranteed to generalize well and enjoy strong computational guarantees remain vulnerable to attacks generated using a single step of gradient ascent. "}}
{"id": "rygzsSrxIB", "cdate": 1567802777845, "mdate": null, "content": {"title": "Communication-efficient Distributed SGD with Sketching", "abstract": "Large-scale distributed training of neural networks is often limited by network bandwidth, wherein the communication time overwhelms the local computation time. Motivated by the success of sketching methods in sub-linear/streaming algorithms, we introduce \\ssgd, an algorithm for carrying out distributed SGD by communicating sketches instead of full gradients. We show that \\ssgd has favorable convergence rates on several classes of functions. When considering all communication -- both of gradients and of updated model weights -- \\ssgd reduces the amount of communication required compared to other gradient compression methods from $\\mathcal{O}(d)$ or $\\mathcal{O}(W)$ to $\\mathcal{O}(\\log d)$, where $d$ is the number of model parameters and $W$ is the number of workers participating in training. We run experiments on a transformer model, an LSTM, and a residual network, demonstrating up to a 40x reduction in total communication cost with no loss in final model performance. We also show experimentally that \\ssgd scales to at least 256 workers without increasing communication cost or degrading model performance."}}
{"id": "rkgd0iA9FQ", "cdate": 1538087887543, "mdate": null, "content": {"title": "Convergence Guarantees for RMSProp and ADAM in Non-Convex Optimization and an Empirical Comparison to Nesterov Acceleration", "abstract": "RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear. Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants. In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways. First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.\n\nNext we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10. Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \\beta_1. We show that at very high values of the momentum parameter (\\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses. On the other hand, NAG can sometimes do better when ADAM's \\beta_1 is set to the most commonly used value: \\beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.\n\nWe also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates. "}}
{"id": "r1-V0vZ_Wr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Streaming Kernel PCA with \\tilde{O}(\\sqrt{n}) Random Features", "abstract": "We study the statistical and computational aspects of kernel principal component analysis using random Fourier features and show that under mild assumptions, $O(\\sqrt{n} \\log n)$ features suffices to achieve $O(1/\\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient streaming algorithm based on classical Oja's algorithm that achieves this rate"}}
