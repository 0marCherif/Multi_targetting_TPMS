{"id": "_TdAKZGuvH8", "cdate": 1672531200000, "mdate": 1698642079105, "content": {"title": "Leveraging Future Relationship Reasoning for Vehicle Trajectory Prediction", "abstract": ""}}
{"id": "CGBCTp2M6lA", "cdate": 1663850066686, "mdate": null, "content": {"title": "Leveraging Future Relationship Reasoning for Vehicle Trajectory Prediction", "abstract": "Understanding the interaction between multiple agents is crucial for realistic vehicle trajectory prediction. \nExisting methods have attempted to infer the interaction from the observed past trajectories of agents using pooling, attention, or graph-based methods, which rely on a deterministic approach. \nHowever, these methods can fail under complex road structures, as they cannot predict various interactions that may occur in the future. \nIn this paper, we propose a novel approach that uses lane information to predict a stochastic future relationship among agents. \nTo obtain a coarse future motion of agents, our method first predicts the probability of lane-level waypoint occupancy of vehicles. \nWe then utilize the temporal probability of passing adjacent lanes for each agent pair, assuming that agents passing adjacent lanes will highly interact. \nWe also model the interaction using a probabilistic distribution, which allows for multiple possible future interactions. \nThe distribution is learned from the posterior distribution of interaction obtained from ground truth future trajectories. \nWe validate our method on popular trajectory prediction datasets: nuScenes and Argoverse. \nThe results show that the proposed method brings remarkable performance gain in prediction accuracy, and achieves state-of-the-art performance in long-term prediction benchmark dataset."}}
{"id": "TLkQBIJP9-q", "cdate": 1640995200000, "mdate": 1667442780921, "content": {"title": "BIPS: Bi-modal Indoor Panorama Synthesis via Residual Depth-Aided Adversarial Learning", "abstract": "Providing omnidirectional depth along with RGB information is important for numerous applications. However, as omnidirectional RGB-D data is not always available, synthesizing RGB-D panorama data from limited information of a scene can be useful. Therefore, some prior works tried to synthesize RGB panorama images from perspective RGB images; however, they suffer from limited image quality and can not be directly extended for RGB-D panorama synthesis. In this paper, we study a new problem: RGB-D panorama synthesis under the various configurations of cameras and depth sensors. Accordingly, we propose a novel bi-modal (RGB-D) panorama synthesis (BIPS) framework. Especially, we focus on indoor environments where the RGB-D panorama can provide a complete 3D model for many applications. We design a generator that fuses the bi-modal information and train it via residual depth-aided adversarial learning (RDAL). RDAL allows to synthesize realistic indoor layout structures and interiors by jointly inferring RGB panorama, layout depth, and residual depth. In addition, as there is no tailored evaluation metric for RGB-D panorama synthesis, we propose a novel metric (FAED) to effectively evaluate its perceptual quality. Extensive experiments show that our method synthesizes high-quality indoor RGB-D panoramas and provides more realistic 3D indoor models than prior methods. Code is available at https://github.com/chang9711/BIPS ."}}
{"id": "OneWDKM_6Qw", "cdate": 1609459200000, "mdate": 1667442780920, "content": {"title": "Unlocking the Potential of Ordinary Classifier: Class-specific Adversarial Erasing Framework for Weakly Supervised Semantic Segmentation", "abstract": "Weakly supervised semantic segmentation (WSSS) using image-level classification labels usually utilizes the Class Activation Maps (CAMs) to localize objects of interest in images. While pointing out that CAMs only highlight the most discriminative regions of the classes of interest, adversarial erasing (AE) methods have been proposed to further explore the less discriminative regions. In this paper, we review the potential of the pre-trained classifier which is trained on the raw images. We experimentally verify that the ordinary classifier <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>  already has the capability to activate the less discriminative regions if the most discriminative regions are erased to some extent. Based on that, we propose a class-specific AE-based framework that fully exploits the potential of an ordinary classifier. Our framework (1) adopts the ordinary classifier to notify the regions to be erased and (2) generates a class-specific mask for erasing by randomly sampling a single specific class to be erased (target class) among the existing classes on the image for obtaining more precise CAMs. Specifically, with the guidance of the ordinary classifier, the proposed CAMs Generation Network (CGNet) is enforced to generate a CAM of the target class while constraining the CAM not to intrude the object regions of the other classes. Along with the pseudo-labels refined from our CAMs, we achieve the state-of-the-art WSSS performance on both PASCAL VOC 2012 and MS-COCO dataset only with image-level supervision. The code is available at https://github.com/KAIST-vilab/OC-CSE."}}
{"id": "CVOXGz9u428", "cdate": 1609459200000, "mdate": 1667442780939, "content": {"title": "Identifying Reflected Images From Object Detector in Indoor Environment Utilizing Depth Information", "abstract": "We observed that mirror reflection severely degrades person detection performance in an indoor environment, which is an essential task for service robots. To address this problem, we propose a new real-time method to identify reflected virtual images in an indoor environment utilizing 3D depth information. Images reflected by the mirror are similar to real objects, so it is a non-trivial task to differentiate them. Conventional object detectors, which do not deal with this problem, obviously recognize reflected images as real objects. The proposed method compares the geometric relationship between the 3D spatial information of the detected object and its surrounding environment where the object locates. It analyzes the layout of surrounding indoor space utilizing semantic segmentation and plane detection method. With the estimated layout of indoor space, detected object candidates are examined whether they are real or reflected images utilizing 3D depth information. To verify the proposed method, a large indoor dataset was newly acquired and examined in a dedicated <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Living-lab</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> environment. The performance of the algorithm is verified by comparing conventional detectors with the proposed method in the acquired <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Living-lab dataset</i> <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/> ."}}
