{"id": "MA264WcjXj", "cdate": 1681659318476, "mdate": 1681659318476, "content": {"title": "Effect of Post-processing on Contextualized Word Representations", "abstract": "Post-processing of static embedding has been shown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principal components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy) and sequence classification tasks. Our findings raise interesting points in relation to the research studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application."}}
{"id": "2jDgsazgLjK", "cdate": 1681659245732, "mdate": 1681659245732, "content": {"title": " Analyzing Encoded Concepts in Transformer Language Models", "abstract": "We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts."}}
{"id": "bCG0IrNsSR", "cdate": 1681659191705, "mdate": 1681659191705, "content": {"title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "abstract": "The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions."}}
{"id": "kq3gK9Dm6h9", "cdate": 1681659152505, "mdate": 1681659152505, "content": {"title": "On the Transformation of Latent Space in Fine-Tuned NLP Models", "abstract": "We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use an alignment function to gauge the similarity between the latent space of a pre-trained model and its fine-tuned version. We use traditional linguistic concepts to facilitate our understanding and also study how the model space transforms towards task-specific information. We perform a thorough analysis, comparing pre-trained and fine-tuned models across three models and three downstream tasks. The notable findings of our work are: i) the latent space of the higher layers evolve towards task-specific concepts, ii) whereas the lower layers retain generic concepts acquired in the pre-trained model, iii) we discovered that some concepts in the higher layers acquire polarity towards the output class, and iv) that these concepts can be used for generating adversarial triggers"}}
{"id": "HK8w9GeHAh", "cdate": 1681659105778, "mdate": 1681659105778, "content": {"title": "On the Effect of Dropping Layers of Pre-trained Transformer Models", "abstract": "Transformer-based NLP models are trained using hundreds of millions or even billions of parameters, limiting their applicability in computationally constrained environments. While the number of parameters generally correlates with performance, it is not clear whether the entire network is required for a downstream task. Motivated by the recent work on pruning and distilling pre-trained models, we explore strategies to drop layers in pre-trained models, and observe the effect of pruning on downstream GLUE tasks. We were able to prune BERT, RoBERTa and XLNet models up to 40%, while maintaining up to 98% of their original performance. Additionally we show that our pruned models are on par with those built using knowledge distillation, both in terms of size and performance. Our experiments yield interesting observations such as: (i) the lower layers are most critical to maintain downstream task performance, (ii) some tasks such as paraphrase detection and sentence similarity are more robust to the dropping of layers, and (iii) models trained using different objective function exhibit different learning patterns and w.r.t the layer dropping."}}
{"id": "HsijwzggY6Q", "cdate": 1637004973779, "mdate": 1637004973779, "content": {"title": "Eyes Don't Lie: Predicting Machine Translation Quality Using Eye Movement", "abstract": "Poorly translated text is often disfluent and difficult to read. In contrast, well-formed translations require less time to process. In this paper, we model the differences in reading patterns of Machine Translation (MT) evaluators using novel features extracted from their gaze data, and we learn to predict the quality scores given by those evaluators. We test our predictions in a pairwise ranking scenario, measuring Kendall\u2019s tau correlation with the judgments. We show that our features provide information beyond fluency, and can be combined with BLEU for better predictions. Furthermore, our results show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators."}}
{"id": "POTMtpYI1xH", "cdate": 1632875704863, "mdate": null, "content": {"title": "Discovering Latent Concepts Learned in BERT", "abstract": "A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model's perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. \nOur findings show: i) a model learns novel concepts (e.g. animal categories and demographic groups), which do not strictly adhere to any pre-defined categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and  morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered  latent concepts highlight potential biases learned in the model. We also release a novel BERT ConceptNet dataset consisting of 174 concept labels and 1M annotated instances."}}
{"id": "-4aHqeKdXe", "cdate": 1620419819908, "mdate": null, "content": {"title": "AraBench: Benchmarking Dialectal Arabic-English Machine Translation", "abstract": "Low-resource machine translation suffers from the scarcity of training data and the unavailability\nof standard evaluation sets. While a number of research efforts target the former, the unavailability\nof evaluation benchmarks remain a major hindrance in tracking the progress in low-resource\nmachine translation. In this paper, we introduce AraBench, an evaluation suite for dialectal\nArabic to English machine translation. Compared to Modern Standard Arabic, Arabic dialects\nare challenging due to their spoken nature, non-standard orthography, and a large variation in\ndialectness. To this end, we pool together already available Dialectal Arabic-English resources\nand additionally build novel test sets. AraBench offers 4 coarse, 15 fine-grained and 25 city-level\ndialect categories, belonging to diverse genres, such as media, chat, religion and travel with\nvarying level of dialectness. We report strong baselines using several training settings: fine-tuning,\nback-translation and data augmentation. The evaluation suite opens a wide range of research\nfrontiers to push efforts in low-resource machine translation, particularly Arabic dialect translation.\nThe evaluation suite1\nand the dialectal system2\nare publicly available for research purposes."}}
{"id": "4wI1zji9CB-", "cdate": 1620419644605, "mdate": null, "content": {"title": "Similarity Analysis of Contextual Word Representation Models", "abstract": "This paper investigates contextual word representation models from the lens of similarity\nanalysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically,\nthese models come from vastly different architectures. We use existing and novel similarity\nmeasures that aim to gauge the level of localization of information in the deep models, and\nfacilitate the investigation of which design factors affect model similarity, without requiring\nany external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures\nhave rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks."}}
{"id": "V9fXtOZhF0N", "cdate": 1620419465808, "mdate": null, "content": {"title": "On the Linguistic Representational Power of Neural Machine Translation Models", "abstract": "Despite the recent success of deep neural networks in natural language processing and other\nspheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity\nand evaluate their quality through relevant extrinsic properties. In particular, we seek answers to\nthe following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii)\nDo the representations capture long-range dependencies, and effectively handle syntactically\ndivergent languages? (iii) Do the representations capture lexical semantics? We conduct a\nthorough investigation along several parameters: (i) Which layers in the architecture capture\neach of these linguistic phenomena; (ii) How does the choice of translation unit (word, character,\nor subword unit) impact the linguistic properties captured by the underlying representations?\n(iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations\nlearned by multilingual NMT models capture the same amount of linguistic information as\ntheir bilingual counterparts? Our data-driven, quantitative evaluation illuminates important\naspects in NMT models and their ability to capture various linguistic phenomena. We show\nthat deep NMT models trained in an end-to-end fashion, without being provided any direct\nsupervision during the training process, learn a non-trivial amount of linguistic information.\nNotable findings include the following observations: (i) Word morphology and part-of-speech\ninformation are captured at the lower layers of the model; (ii) In contrast, lexical semantics or\nnon-local syntactic and semantic dependencies are better represented at the higher layers of the\nmodel; (iii) Representations learned using characters are more informed about word-morphology\ncompared to those learned using subword units; and (iv) Representations learned by multilingual\nmodels are richer compared to bilingual models."}}
