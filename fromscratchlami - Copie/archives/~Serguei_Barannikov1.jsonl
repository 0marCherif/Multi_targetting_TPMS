{"id": "AMJmPlwPoc", "cdate": 1706788962504, "mdate": 1706788962504, "content": {"title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts", "abstract": "Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample. We show that the average intrinsic dimensionality of fluent texts in a natural language is hovering around the value 9 for several alphabet-based languages and around 7 for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is \u22481.5 lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin."}}
{"id": "HklsD_J30u0", "cdate": 1680004973850, "mdate": null, "content": {"title": "The Framed Morse Complex and its Invariants", "abstract": "The invariants of filtered complexes, termed later \"persistence barcodes\" or \"persistence diagrams\", were first introduced in this paper, together with the algorithm for their calculation. The Persistence Homology Main (also termed Structure, or Classification) theorem is proven in section 2. These invariants are applied then to study the behavior of filtered Morse complex under deformations and its stability properties. "}}
{"id": "a0yoVn0zv3D", "cdate": 1672531200000, "mdate": 1693811440938, "content": {"title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts", "abstract": "Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin."}}
{"id": "XERqHeUbF2e", "cdate": 1672531200000, "mdate": 1677169472114, "content": {"title": "Learning Topology-Preserving Data Representations", "abstract": ""}}
{"id": "UOyBzwtZwW", "cdate": 1672531200000, "mdate": 1693811440887, "content": {"title": "Learning topology-preserving data representations", "abstract": ""}}
{"id": "lIu-ixf-Tzf", "cdate": 1663850304828, "mdate": null, "content": {"title": "Learning topology-preserving data representations", "abstract": "We propose a method for learning topology-preserving data representations (dimensionality reduction). The method aims to provide topological similarity between the data manifold and its latent representation via enforcing the similarity in topological features (clusters,  loops, 2D voids, etc.) and their localization. \nThe core of the method is the minimization of the Representation Topology Divergence (RTD) between original high-dimensional data and low-dimensional representation in latent space. RTD minimization provides closeness in topological features with strong theoretical guarantees. \nWe develop a scheme for RTD differentiation and apply it as a loss term for the autoencoder. The proposed method \"RTD-AE\" better preserves the global structure and topology of the data manifold than state-of-the-art competitors as measured by linear correlation, triplet distance ranking accuracy, and Wasserstein distance between persistence barcodes."}}
{"id": "dGvLBckPwR", "cdate": 1640995200000, "mdate": 1675803788819, "content": {"title": "Acceptability Judgements via Examining the Topology of Attention Maps", "abstract": "Daniil Cherniavskii, Eduard Tulchinskii, Vladislav Mikhailov, Irina Proskurina, Laida Kushnareva, Ekaterina Artemova, Serguei Barannikov, Irina Piontkovskaya, Dmitri Piontkovski, Evgeny Burnaev. Findings of the Association for Computational Linguistics: EMNLP 2022. 2022."}}
{"id": "_XoR3zXwPn", "cdate": 1640995200000, "mdate": 1674670812974, "content": {"title": "Topological Data Analysis for Speech Processing", "abstract": "We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. In particular, we achieve an improvement of about $9\\%$ accuracy and $5\\%$ ERR on four common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy $80.155$. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction. Appendices, an introduction to TDA, and other additional materials are available here - https://topohubert.github.io/speech-topology-webpages/"}}
{"id": "NvX7-IYyCVq", "cdate": 1640995200000, "mdate": 1652782632234, "content": {"title": "Representation Topology Divergence: A Method for Comparing Neural Network Representations", "abstract": "Comparison of data representations is a complex multi-aspect problem that has not enjoyed a complete solution yet. We propose a method for comparing two data representations. We introduce the Representation Topology Divergence (RTD), measuring the dissimilarity in multi-scale topology between two point clouds of equal size with a one-to-one correspondence between points. The data point clouds are allowed to lie in different ambient spaces. The RTD is one of the few TDA-based practical methods applicable to real machine learning datasets. Experiments show that the proposed RTD agrees with the intuitive assessment of data representation similarity and is sensitive to its topological structure. We apply RTD to gain insights on neural networks representations in computer vision and NLP domains for various problems: training dynamics analysis, data distribution shift, transfer learning, ensemble learning, disentanglement assessment."}}
{"id": "0Sr90mw9mJB", "cdate": 1640995200000, "mdate": 1664713575770, "content": {"title": "Representation Topology Divergence: A Method for Comparing Neural Network Representations", "abstract": "Comparison of data representations is a complex multi-aspect problem. We propose a method for comparing two data representations. We introduce the Representation Topology Divergence (RTD) score mea..."}}
