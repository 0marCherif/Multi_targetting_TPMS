{"id": "T2C8vpXzrU8", "cdate": 1683879232279, "mdate": 1683879232279, "content": {"title": "Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation", "abstract": "Text-to-3D generation has shown rapid progress in recent days with the advent of score distillation, a methodology of using pretrained text-to-2D diffusion models to optimize neural radiance field (NeRF) in the zero-shot setting. However, the lack of 3D awareness in the 2D diffusion models destabilizes score distillation-based methods from reconstructing a plausible 3D scene. To address this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness into pretrained 2D diffusion models, enhancing the robustness and 3D consistency of score distillation-based methods. We realize this by first constructing a coarse 3D structure of a given text prompt and then utilizing projected, view-specific depth map as a condition for the diffusion model. Additionally, we introduce a training strategy that enables the 2D diffusion model learns to handle the errors and sparsity within the coarse 3D structure for robust generation, as well as a method for ensuring semantic consistency throughout all viewpoints of the scene. Our framework surpasses the limitations of prior arts, and has significant implications for 3D consistent generation of 2D diffusion models."}}
{"id": "esUTHvgn2m", "cdate": 1683879100556, "mdate": 1683879100556, "content": {"title": "Online Exemplar Fine-Tuning for Image-to-Image Translation", "abstract": "Existing techniques to solve exemplar-based image-to-image translation within deep convolutional neural networks (CNNs) generally require a training phase to optimize the network parameters on domain-specific and task-specific benchmarks, thus having limited applicability and generalization ability. In this paper, we propose a novel framework, for the first time, to solve exemplar-based translation through an online optimization given an input image pair, called online exemplar fine-tuning (OEFT), in which we fine-tune the off-the-shelf and general-purpose networks to the input image pair themselves. We design two sub-networks, namely correspondence fine-tuning and multiple GAN inversion, and optimize these network parameters and latent codes, starting from the pre-trained ones, with well-defined loss functions. Our framework does not require the off-line training phase, which has been the main challenge of existing methods, but the pre-trained networks to enable optimization in online. Experimental results prove that our framework is effective in having a generalization power to unseen image pairs and clearly even outperforms the state-of-the-arts needing the intensive training phase.\n"}}
{"id": "xB8FWHveptD", "cdate": 1672531200000, "mdate": 1679902045928, "content": {"title": "Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation", "abstract": ""}}
{"id": "tCcfs24bKtD", "cdate": 1672531200000, "mdate": 1681699964302, "content": {"title": "ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained Illumination", "abstract": "In this paper, we propose a new challenge that synthesizes a novel view in a more practical environment, where the number of input multi-view images is limited and illumination variations are significant. Despite recent success, neural radiance fields (NeRF) require a massive amount of input multi-view images taken under constrained illuminations. To address the problem, we suggest ExtremeNeRF, which utilizes occlusion-aware multiview albedo consistency, supported by geometric alignment and depth consistency. We extract intrinsic image components that should be illumination-invariant across different views, enabling direct appearance comparison between the input and novel view under unconstrained illumination. We provide extensive experimental results for an evaluation of the task, using the newly built NeRF Extreme benchmark, which is the first in-the-wild novel view synthesis benchmark taken under multiple viewing directions and varying illuminations. The project page is at https://seokyeong94.github.io/ExtremeNeRF/"}}
{"id": "s7K_OZpJj-", "cdate": 1672531200000, "mdate": 1681699964827, "content": {"title": "CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation", "abstract": "Existing works on open-vocabulary semantic segmentation have utilized large-scale vision-language models, such as CLIP, to leverage their exceptional open-vocabulary recognition capabilities. However, the problem of transferring these capabilities learned from image-level supervision to the pixel-level task of segmentation and addressing arbitrary unseen categories at inference makes this task challenging. To address these issues, we aim to attentively relate objects within an image to given categories by leveraging relational information among class categories and visual semantics through aggregation, while also adapting the CLIP representations to the pixel-level task. However, we observe that direct optimization of the CLIP embeddings can harm its open-vocabulary capabilities. In this regard, we propose an alternative approach to optimize the image-text similarity map, i.e. the cost map, using a novel cost aggregation-based method. Our framework, namely CAT-Seg, achieves state-of-the-art performance across all benchmarks. We provide extensive ablation studies to validate our choices. Project page: https://ku-cvlab.github.io/CAT-Seg/."}}
{"id": "hu5ejtpFlm", "cdate": 1672531200000, "mdate": 1681699965070, "content": {"title": "Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation", "abstract": "The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between faithfulness to the 2D diffusion models and 3D consistency with little overhead."}}
{"id": "YOkZJ_HMrep", "cdate": 1672531200000, "mdate": 1681699965285, "content": {"title": "GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency", "abstract": "We present a novel framework to regularize Neural Radiance Field (NeRF) in a few-shot setting with a geometry-aware consistency regularization. The proposed approach leverages a rendered depth map at unobserved viewpoint to warp sparse input images to the unobserved viewpoint and impose them as pseudo ground truths to facilitate learning of NeRF. By encouraging such geometry-aware consistency at a feature-level instead of using pixel-level reconstruction loss, we regularize the NeRF at semantic and structural levels while allowing for modeling view dependent radiance to account for color variations across viewpoints. We also propose an effective method to filter out erroneous warped solutions, along with training strategies to stabilize training during optimization. We show that our model achieves competitive results compared to state-of-the-art few-shot NeRF models. Project page is available at https://ku-cvlab.github.io/GeCoNeRF/."}}
{"id": "9sp0M5GfgOa", "cdate": 1672531200000, "mdate": 1681699964440, "content": {"title": "6D Object Pose Estimation Using a Particle Filter With Better Initialization", "abstract": "Estimation of 6D object poses is a key issue in robotic grasping tasks. Recently, many high-performance learning-based methods have been introduced using robust deep learning techniques; however, applying these methods to real robot environments requires many ground truth 6D pose annotations for training. To address this problem, we propose a template matching-based particle filter approach for 6D pose estimation; the proposed method does not require ground truth 6D poses. Although particle filter approaches can stochastically avoid local optima, they require adequate initial pose hypotheses for estimating an accurate 6D object pose. Therefore, we estimated an initial translation of the target object for accurately initializing a particle filter by developing a new deep network. Once the proposed centroid prediction network (CPN) is trained with a specific dataset, no additional training is required for new objects not in the dataset. We evaluated the performance of the CPN and the proposed 6D pose estimation method on benchmark datasets, which demonstrated that the CPN can predict the centroid for any object, including those not in the training data, and that our 6D pose estimation method outperforms existing methods for partially occluded objects. Finally, we tested a grasping task based on our proposed method using a real robot platform to demonstrate an application of our method to a downstream task. This experiment shows that our method can be applied to part assembly, bin picking, and object manipulation without large training datasets with 6D pose annotations. The code and models are available at: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/oorrppp2/Particle_filter_approach_6D_pose_estimation</uri> ."}}
{"id": "3lQV2ngJ9y", "cdate": 1672531200000, "mdate": 1681699964343, "content": {"title": "3D GAN Inversion with Pose Optimization", "abstract": "With the recent advances in NeRF-based 3D aware GANs quality, projecting an image into the latent space of these 3D-aware GANs has a natural advantage over 2D GAN inversion: not only does it allow multi-view consistent editing of the projected image, but it also enables 3D reconstruction and novel view synthesis when given only a single image. However, the explicit viewpoint control acts as a main hindrance in the 3D GAN inversion process, as both camera pose and latent code have to be optimized simultaneously to reconstruct the given image. Most works that explore the latent space of the 3D-aware GANs rely on ground-truth camera viewpoint or deformable 3D model, thus limiting their applicability. In this work, we introduce a generalizable 3D GAN inversion method that infers camera viewpoint and latent code simultaneously to enable multi-view consistent semantic image editing. The key to our approach is to leverage pre-trained estimators for better initialization and utilize the pixel-wise depth calculated from NeRF parameters to better reconstruct the given image. We conduct extensive experiments on image reconstruction and editing both quantitatively and qualitatively, and further compare our results with 2D GAN-based editing to demonstrate the advantages of utilizing the latent space of 3D GANs. Additional results and visualizations are available at https://3dgan-inversion.github.io/."}}
{"id": "0mm7g6exeW", "cdate": 1672531200000, "mdate": 1681699964422, "content": {"title": "You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona", "abstract": "To build a conversational agent that interacts fluently with humans, previous studies blend knowledge or personal profile into the pre-trained language model. However, the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of using personas. We propose an effective dialogue agent that grounds external knowledge and persona simultaneously. The agent selects the proper knowledge and persona to use for generating the answers with our candidate scoring implemented with a poly-encoder. Then, our model generates the utterance with lesser hallucination and more engagingness utilizing retrieval augmented generation with knowledge-persona enhanced query. We conduct experiments on the persona-knowledge chat and achieve state-of-the-art performance in grounding and generation tasks on the automatic metrics. Moreover, we validate the answers from the models regarding hallucination and engagingness through human evaluation and qualitative results. We show our retriever's effectiveness in extracting relevant documents compared to the other previous retrievers, along with the comparison of multiple candidate scoring methods. Code is available at https://github.com/dlawjddn803/INFO"}}
