{"id": "7pNV4PCjbQy", "cdate": 1652737722505, "mdate": null, "content": {"title": "Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems", "abstract": "We consider the problem of controlling an unknown stochastic linear system with quadratic costs -- called the adaptive LQ control problem. We re-examine an approach called ``Reward-Biased Maximum Likelihood Estimate'' (RBMLE) that was proposed more than forty years ago, and which predates the ``Upper Confidence Bound'' (UCB) method, as well as the definition of ``regret'' for bandit problems. It simply added a term favoring parameters with larger rewards to the criterion for parameter estimation.  We show how the RBMLE and UCB methods can be reconciled, and thereby propose an Augmented RBMLE-UCB algorithm that combines the penalty of the RBMLE method with the constraints of the UCB method, uniting the two approaches to optimism in the face of uncertainty. We establish that theoretically, this method retains ${\\mathcal{O}}(\\sqrt{T})$ regret, the best known so far. We further compare the empirical performance of the proposed Augmented RBMLE-UCB and the standard RBMLE (without the augmentation) with UCB, Thompson Sampling, Input Perturbation, Randomized Certainty Equivalence and StabL on many real-world examples including flight control of Boeing 747 and Unmanned Aerial Vehicle. We perform extensive simulation studies showing that the Augmented RBMLE consistently outperforms UCB, Thompson Sampling and StabL by a huge margin, while it is marginally better than Input Perturbation and moderately better than Randomized Certainty Equivalence."}}
{"id": "JkV9LFh1oAY", "cdate": 1640995200000, "mdate": 1682357124404, "content": {"title": "The RBMLE method for Reinforcement Learning", "abstract": "The Reward Biased Maximum Likelihood Estimate (RBMLE) method was proposed about four decades ago for the adaptive control of unknown Markov Decision Processes, and later studied for more general Controlled Markovian Systems and Linear Quadratic Gaussian systems. It showed that if one could bias the Maximum Likelihood Estimate in favor of parameters with larger rewards then one could obtain long-term average optimality. It provided a reason for preferring parameters with larger rewards based on the fact that generally one can only identify the behavior of a system under closed-loop, and therefore any limiting parameter estimate has to necessarily have lower reward than the true parameter. It thereby provided a reason for what his now called \u201coptimism in the face of uncertainty\u201d. It similarly preceded the definition of \u201cregret\u201d, and it is only in the last three years that it has been analyzed for its regret performance, both analytically, and in comparative simulation testing. This paper provides an account of the RBMLE method for reinforcement learning."}}
{"id": "7_SbE_y2fV", "cdate": 1640995200000, "mdate": 1682357124405, "content": {"title": "Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems", "abstract": "We consider the problem of controlling an unknown stochastic linear system with quadratic costs - called the adaptive LQ control problem. We re-examine an approach called ''Reward Biased Maximum Likelihood Estimate'' (RBMLE) that was proposed more than forty years ago, and which predates the ''Upper Confidence Bound'' (UCB) method as well as the definition of ''regret'' for bandit problems. It simply added a term favoring parameters with larger rewards to the criterion for parameter estimation. We show how the RBMLE and UCB methods can be reconciled, and thereby propose an Augmented RBMLE-UCB algorithm that combines the penalty of the RBMLE method with the constraints of the UCB method, uniting the two approaches to optimism in the face of uncertainty. We establish that theoretically, this method retains $\\Tilde{\\mathcal{O}}(\\sqrt{T})$ regret, the best-known so far. We further compare the empirical performance of the proposed Augmented RBMLE-UCB and the standard RBMLE (without the augmentation) with UCB, Thompson Sampling, Input Perturbation, Randomized Certainty Equivalence and StabL on many real-world examples including flight control of Boeing 747 and Unmanned Aerial Vehicle. We perform extensive simulation studies showing that the Augmented RBMLE consistently outperforms UCB, Thompson Sampling and StabL by a huge margin, while it is marginally better than Input Perturbation and moderately better than Randomized Certainty Equivalence."}}
{"id": "1PJ1BiWBGn", "cdate": 1609459200000, "mdate": 1682357124403, "content": {"title": "Reward Biased Maximum Likelihood Estimation for Reinforcement Learning", "abstract": "The Reward-Biased Maximum Likelihood Estimate (RBMLE) for adaptive control of Markov chains was proposed in (Kumar and Becker, 1982) to overcome the central obstacle of what is variously called the fundamental \u201cclosed-identifiability problem\u201d of adaptive control (Borkar and Varaiya, 1979), the \u201cdual control problem\u201d by Feldbaum (Feldbaum, 1960a,b), or, contemporaneously, the \u201cexploration vs. exploitation problem\u201d. It exploited the key observation that since the maximum likelihood parameter estimator can asymptotically identify the closed-transition probabilities under a certainty equivalent approach (Borkar and Varaiya, 1979), the limiting parameter estimates must necessarily have an optimal reward that is less than the optimal reward attainable for the true but unknown system. Hence it proposed a counteracting reverse bias in favor of parameters with larger optimal rewards, providing a carefully structured solution to the fundamental problem alluded to above. It thereby proposed an optimistic approach of favoring parameters with larger optimal rewards, now known as \u201coptimism in the face of uncertainty.\u201d The RBMLE approach has been proved to be long-term average reward optimal in a variety of contexts including controlled Markov chains, linear quadratic Gaussian (LQG) systems, some nonlinear systems, and diffusions. However, modern attention is focused on the much finer notion of \u201cregret,\u201d or finite-time performance for all time, espoused by (Lai and Robbins, 1985). Recent analysis of RBMLE for multi-armed stochastic bandits (Liu et al., 2020) and linear contextual bandits (Hung et al., 2020) has shown that it not only has state-of-the-art regret, but it also exhibits empirical performance comparable to or better than the best current contenders, and leads to several new and strikingly simple index policies for these classical problems. Motivated by this, we examine the finite-time performance of RBMLE for reinforcement learning tasks that involve the general problem of optimal control of unknown Markov Decision Processes. We show that it has a regret of O(log T ) over a time horizon of T, similar to state-of-art algorithms."}}
{"id": "g4WjDBgxPE", "cdate": 1577836800000, "mdate": 1682357124406, "content": {"title": "Reward Biased Maximum Likelihood Estimation for Reinforcement Learning", "abstract": "The Reward-Biased Maximum Likelihood Estimate (RBMLE) for adaptive control of Markov chains was proposed to overcome the central obstacle of what is variously called the fundamental \"closed-identifiability problem\" of adaptive control, the \"dual control problem\", or, contemporaneously, the \"exploration vs. exploitation problem\". It exploited the key observation that since the maximum likelihood parameter estimator can asymptotically identify the closed-transition probabilities under a certainty equivalent approach, the limiting parameter estimates must necessarily have an optimal reward that is less than the optimal reward attainable for the true but unknown system. Hence it proposed a counteracting reverse bias in favor of parameters with larger optimal rewards, providing a solution to the fundamental problem alluded to above. It thereby proposed an optimistic approach of favoring parameters with larger optimal rewards, now known as \"optimism in the face of uncertainty\". The RBMLE approach has been proved to be long-term average reward optimal in a variety of contexts. However, modern attention is focused on the much finer notion of \"regret\", or finite-time performance. Recent analysis of RBMLE for multi-armed stochastic bandits and linear contextual bandits has shown that it not only has state-of-the-art regret, but it also exhibits empirical performance comparable to or better than the best current contenders, and leads to strikingly simple index policies. Motivated by this, we examine the finite-time performance of RBMLE for reinforcement learning tasks that involve the general problem of optimal control of unknown Markov Decision Processes. We show that it has a regret of $\\mathcal{O}( \\log T)$ over a time horizon of $T$ steps, similar to state-of-the-art algorithms. Simulation studies show that RBMLE outperforms other algorithms such as UCRL2 and Thompson Sampling."}}
{"id": "r9ym3S9Ueu5", "cdate": 1546300800000, "mdate": null, "content": {"title": "Partial Server Pooling in Redundancy Systems", "abstract": "Partial sharing allows providers to possibly pool a fraction of their resources when full pooling is not beneficial to them. Recent work in systems without sharing has shown that redundancy can improve performance considerably. In this paper, we combine partial sharing and redundancy by developing partial sharing models for providers operating multi-server systems with redundancy. Two M/M/N queues with redundant service models are considered. Copies of an arriving job are placed in the queues of servers that can serve the job. Partial sharing models for cancel-on-complete and cancel-on-start redundancy models are developed. For cancel-on-complete, it is shown that the Pareto efficient region is the full pooling configuration. For a cancel-on-start policy, we conjecture that the Pareto frontier is always non-empty and is such that at least one of the two providers is sharing all of its resources. For this system, using bargaining theory the sharing configuration that the providers may use is determined. Mean response time and probability of waiting are the performance metrics considered."}}
{"id": "THGYcmCAglR", "cdate": 1546300800000, "mdate": 1682357124406, "content": {"title": "Low Latency Scheduling for D2D Communication", "abstract": "We consider a single cell system with a basestation and multiple cellular devices. Each cellular device wants to communicate with the basestation and some of its neighbors. Device-to-Device communication is used for the communication between any two neighboring cellular devices. We consider uplink underlay D2D communication i.e., the resources designated for uplink communication are reused to carry out D2D communication.Our goal is to develop a centralized low latency scheduling policy for D2D communication. We propose an adaptation of the Server Side Greedy algorithm for D2D communication. We show that this algorithm has low latency and the performance of the algorithm improves exponentially with the number of users making the algorithm highly suitable for large systems."}}
{"id": "-nsifcYt7s3", "cdate": 1546300800000, "mdate": null, "content": {"title": "Partial Server Pooling in Redundancy Systems", "abstract": "Partial sharing allows providers to possibly pool a fraction of their resources when full pooling is not beneficial to them. Recent work in systems without sharing has shown that redundancy can improve performance considerably. In this paper, we combine partial sharing and redundancy by developing partial sharing models for providers operating multi-server systems with redundancy. Two M/M/N queues with redundant service models are considered. Copies of an arriving job are placed in the queues of servers that can serve the job. Partial sharing models for cancel-on-complete and cancel-on-start redundancy models are developed. For cancel-on-complete, it is shown that the Pareto efficient region is the full pooling configuration. For a cancel-on-start policy, we conjecture that the Pareto frontier is always non-empty and is such that at least one of the two providers is sharing all of its resources. For this system, using bargaining theory the sharing configuration that the providers may use is determined. Mean response time and probability of waiting are the performance metrics considered."}}
{"id": "rCikyX0wlnZ", "cdate": 1514764800000, "mdate": 1682357124410, "content": {"title": "Caching Policies for D2D-Assisted Content Delivery Systems", "abstract": "We consider a content delivery system consisting of a central server and multiple end-users. The central server stores the entire catalog of contents on offer and can deliver the requested content to the end-users. In addition, the end-users are equipped with limited caching capabilities and have the ability to deliver content to each other via D2D communication. The system also allows a third mode of content delivery where the central server delivers content to some of the end-users who then relay it to the other users. Our goal is to determine which contents to cache at the end-users in order to minimize the cost of service. We characterize the optimal caching policy and evaluate the benefits of allowing the central server to use other end-users as relays to deliver content. The key takeaway from this work is that if end-users have caching capabilities, the benefits of the central server using end-users as relays is negligible. This is in contrast to the case where the end-users cannot cache content where using end-users as relays leads to significant improvement in system performance."}}
