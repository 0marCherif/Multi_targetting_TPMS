{"id": "L3EIzUOGDCC", "cdate": 1698857020284, "mdate": 1698857020284, "content": {"title": "STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos", "abstract": "We address the problem of extracting key steps from unlabeled procedural videos, motivated by the potential of Augmented Reality (AR) headsets to revolutionize job training and performance. We decompose the problem into two steps: representation learning and key steps extraction. We propose a training objective, Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn discriminative representations for various steps without any labels. Different from prior works, we develop techniques to train a light-weight temporal module which uses off-the-shelf features for self supervision. Our approach can seamlessly leverage information from multiple cues like optical flow, depth or gaze to learn discriminative features for key-steps, making it amenable for AR applications. We finally extract key steps via a tunable algorithm that clusters the representations and samples. We show significant improvements over prior works for the task of key step localization and phase classification. Qualitative results demonstrate that the extracted key steps are meaningful and succinctly represent various steps of the procedural tasks."}}
{"id": "NDTtDRWVdX7", "cdate": 1674724960034, "mdate": 1674724960034, "content": {"title": "Multi-View Action Recognition using Contrastive Learning", "abstract": "In this work, we present a method for RGB-based action recognition using multi-view videos. We present a supervised contrastive learning framework to learn a feature embedding robust to changes in viewpoint, by effectively leveraging multi-view data. We use an improved supervised contrastive loss and augment the positives with those coming from synchronized viewpoints. We also propose a new approach to use classifier probabilities to guide the selection of hard negatives in the contrastive loss, to learn a more discriminative representation. Negative samples from confusing classes based on posterior are weighted higher. We also show that our method leads to better domain generalization compared to the standard supervised training based on synthetic multi-view data. Extensive experiments on real (NTU-60, NTU-120, NUMA) and synthetic (RoCoG) data demonstrate the effectiveness of our approach."}}
{"id": "xpdaDM_B4D", "cdate": 1652737699755, "mdate": null, "content": {"title": "FeLMi : Few shot Learning with hard Mixup", "abstract": "Learning from a few examples is a challenging computer vision task. Traditionally,\nmeta-learning-based methods have shown promise towards solving this problem.\nRecent approaches show benefits by learning a feature extractor on the abundant\nbase examples and transferring these to the fewer novel examples. However, the\nfinetuning stage is often prone to overfitting due to the small size of the novel\ndataset. To this end, we propose Few shot Learning with hard Mixup (FeLMi)\nusing manifold mixup to synthetically generate samples that helps in mitigating\nthe data scarcity issue. Different from a na\u00efve mixup, our approach selects the hard\nmixup samples using an uncertainty-based criteria. To the best of our knowledge,\nwe are the first to use hard-mixup for the few-shot learning problem. Our approach\nallows better use of the pseudo-labeled base examples through base-novel mixup\nand entropy-based filtering. We evaluate our approach on several common few-shot\nbenchmarks - FC-100, CIFAR-FS, miniImageNet and tieredImageNet and obtain\nimprovements in both 1-shot and 5-shot settings. Additionally, we experimented on\nthe cross-domain few-shot setting (miniImageNet \u2192 CUB) and obtain significant\nimprovements."}}
{"id": "kbTeUQabKDy", "cdate": 1652713564203, "mdate": 1652713564203, "content": {"title": "OBJECT-AWARE CROPPING FOR SELF-SUPERVISED LEARNING", "abstract": "A core component of the recent success of self-supervised learning is cropping\ndata augmentation, which selects sub-regions of an image to be used as positive\nviews in the self-supervised loss. The underlying assumption is that randomly\ncropped and resized regions of a given image share information about the objects of\ninterest, which the learned representation will capture. This assumption is mostly\nsatisfied in datasets such as ImageNet where there is a large, centered object, which\nis highly likely to be present in random crops of the full image. However, in other\ndatasets such as OpenImages or COCO, which are more representative of real world\nuncurated data, there are typically multiple small objects in an image. In this work,\nwe show that self-supervised learning based on the usual random cropping performs\npoorly on such datasets. We propose replacing one or both of the random crops\nwith crops obtained from an object proposal algorithm. This encourages the model\nto learn both object and scene level semantic representations. Using this approach,\nwhich we call object-aware cropping, results in significant improvements over\nscene cropping on classification and object detection benchmarks. For example, on\nOpenImages, our approach achieves an improvement of 8.8% mAP over random\nscene-level cropping using MoCo-v2 based pre-training. We also show significant\nimprovements on COCO and PASCAL-VOC object detection and segmentation\ntasks over the state-of-the-art self-supervised learning approaches. Our approach\nis efficient, simple and general, and can be used in most existing contrastive and\nnon-contrastive self-supervised learning frameworks."}}
{"id": "reZLt-eVWc", "cdate": 1640995200000, "mdate": 1646686589926, "content": {"title": "Pose and Joint-Aware Action Recognition", "abstract": "Recent progress on action recognition has mainly focused on RGB and optical flow features. In this paper, we approach the problem of joint-based action recognition. Unlike other modalities, constellation of joints and their motion generate models with succinct human motion information for activity recognition. We present a new model for joint-based action recognition, which first extracts motion features from each joint separately through a shared motion encoder before performing collective reasoning. Our joint selector module re-weights the joint information to select the most discriminative joints for the task. We also propose a novel joint-contrastive loss that pulls together groups of joint features which convey the same action. We strengthen the joint-based representations by using a geometry-aware data augmentation technique which jitters pose heatmaps while retaining the dynamics of the action. We show large improvements over the current state-of-the-art joint-based approaches on JHMDB, HMDB, Charades, AVA action recognition datasets. A late fusion with RGB and Flow-based approaches yields additional improvements. Our model also outperforms the existing baseline on Mimetics, a dataset with out-of-context actions."}}
{"id": "BKL-IKblN-c", "cdate": 1640995200000, "mdate": 1646686589923, "content": {"title": "Unfolding a blurred image", "abstract": "We present a solution for the goal of extracting a video from a single motion blurred image to sequentially reconstruct the clear views of a scene as beheld by the camera during the time of exposure. We first learn motion representation from sharp videos in an unsupervised manner through training of a convolutional recurrent video autoencoder network that performs a surrogate task of video reconstruction. Once trained, it is employed for guided training of a motion encoder for blurred images. This network extracts embedded motion information from the blurred image to generate a sharp video in conjunction with the trained recurrent video decoder. As an intermediate step, we also design an efficient architecture that enables real-time single image deblurring and outperforms competing methods across all factors: accuracy, speed, and compactness. Experiments on real scenes and standard datasets demonstrate the superiority of our framework over the state-of-the-art and its ability to generate a plausible sequence of temporally consistent sharp frames."}}
{"id": "3XcEQTRyxhp", "cdate": 1632875448548, "mdate": null, "content": {"title": "Object-Aware Cropping for Self-Supervised Learning", "abstract": "A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised loss. The underlying assumption is that randomly cropped and resized regions of a given image share information about the objects of interest, which the learned representation will capture. This assumption is mostly satisfied in datasets such as ImageNet where there is a large, centered object, which is highly likely to be present in random crops of the full image. However, in other datasets such as OpenImages or COCO, which are more representative of real world uncurated data, there are typically multiple small objects in an image. In this work, we show that self-supervised learning based on the usual random cropping performs poorly on such datasets. We propose replacing one or both of the random crops with crops obtained from an object proposal algorithm. This encourages the model to learn both object and scene level semantic representations. Using this approach, which we call object-aware cropping, results in significant improvements over scene cropping on classification and object detection benchmarks. For example, on OpenImages, our approach achieves an improvement of 8.8% mAP over random scene-level cropping using MoCo-v2 based pre-training. We also show significant improvements on COCO and PASCAL-VOC object detection and segmentation tasks over the state-of-the-art self-supervised learning approaches.\nOur approach is efficient, simple and general, and can be used in most existing contrastive and non-contrastive self-supervised learning frameworks. "}}
{"id": "rAMmIKWlVb5", "cdate": 1609459200000, "mdate": 1646686589925, "content": {"title": "Max-Margin Contrastive Learning", "abstract": "Standard contrastive learning approaches usually require a large number of negatives for effective unsupervised learning and often exhibit slow convergence. We suspect this behavior is due to the suboptimal selection of negatives used for offering contrast to the positives. We counter this difficulty by taking inspiration from support vector machines (SVMs) to present max-margin contrastive learning (MMCL). Our approach selects negatives as the sparse support vectors obtained via a quadratic optimization problem, and contrastiveness is enforced by maximizing the decision margin. As SVM optimization can be computationally demanding, especially in an end-to-end setting, we present simplifications that alleviate the computational burden. We validate our approach on standard vision benchmark datasets, demonstrating better performance in unsupervised representation learning over state-of-the-art, while having better empirical convergence properties."}}
{"id": "HCzNIKZgEbc", "cdate": 1609459200000, "mdate": 1646686589926, "content": {"title": "Object-Aware Cropping for Self-Supervised Learning", "abstract": "A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised loss. The underlying assumption is that randomly cropped and resized regions of a given image share information about the objects of interest, which the learned representation will capture. This assumption is mostly satisfied in datasets such as ImageNet where there is a large, centered object, which is highly likely to be present in random crops of the full image. However, in other datasets such as OpenImages or COCO, which are more representative of real world uncurated data, there are typically multiple small objects in an image. In this work, we show that self-supervised learning based on the usual random cropping performs poorly on such datasets. We propose replacing one or both of the random crops with crops obtained from an object proposal algorithm. This encourages the model to learn both object and scene level semantic representations. Using this approach, which we call object-aware cropping, results in significant improvements over scene cropping on classification and object detection benchmarks. For example, on OpenImages, our approach achieves an improvement of 8.8% mAP over random scene-level cropping using MoCo-v2 based pre-training. We also show significant improvements on COCO and PASCAL-VOC object detection and segmentation tasks over the state-of-the-art self-supervised learning approaches. Our approach is efficient, simple and general, and can be used in most existing contrastive and non-contrastive self-supervised learning frameworks."}}
{"id": "xrUySgB5ZOK", "cdate": 1601308203391, "mdate": null, "content": {"title": "Learning Visual Representations for Transfer Learning by Suppressing Texture", "abstract": "Recent works have shown that features obtained from supervised training of CNNs may over-emphasize texture rather than encoding high-level information.  In self-supervised learning, in particular, texture as a low-level cue may provide shortcuts that prevent the network from learning higher-level representations.  To address these problems we propose to use classic methods based on anisotropic diffusion to augment training using images with suppressed texture. This simple method helps retain important edge information and suppress texture at the same time. \nWe report our observations for fully supervised and self-supervised learning tasks like MoCoV2 and Jigsaw and achieve state-of-the-art results on object detection and image classification with eight diverse datasets. \nOur method is particularly effective for transfer learning tasks and we observed improved performance on five standard transfer learning datasets. \nThe large improvements on the Sketch-ImageNet dataset, DTD dataset and\nadditional visual analyses of saliency maps suggest that our approach helps in learning better representations that transfer well."}}
