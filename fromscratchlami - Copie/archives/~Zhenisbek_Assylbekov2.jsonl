{"id": "SPFPN3dlXbt", "cdate": 1672531200000, "mdate": 1696341956551, "content": {"title": "Hardness of Learning AES Key (Student Abstract)", "abstract": "We show hardness of learning AES key from pairs of ciphertexts under the assumption of computational closeness of AES to pairwise independence. The latter is motivated by a recent result on statistical closeness of AES to pairwise independence."}}
{"id": "yVvl4KqCl0", "cdate": 1640995200000, "mdate": 1696341956553, "content": {"title": "Hardness of Learning AES Key (Short Paper)", "abstract": ""}}
{"id": "XGzyJVxLudK", "cdate": 1640995200000, "mdate": 1696341956593, "content": {"title": "From Hyperbolic Geometry Back to Word Embeddings", "abstract": ""}}
{"id": "Fe-TIG0n0FC", "cdate": 1640995200000, "mdate": 1696341956554, "content": {"title": "Speeding Up Entmax", "abstract": ""}}
{"id": "tLxHjcW7P7", "cdate": 1609459200000, "mdate": 1696341956614, "content": {"title": "Geometric Probing of Word Vectors", "abstract": ""}}
{"id": "pdj3LYjwtiJ", "cdate": 1609459200000, "mdate": 1650786022061, "content": {"title": "Approximation error of Fourier neural networks", "abstract": "The paper investigates approximation error of two-layer feedforward Fourier Neural Networks (FNNs). Such networks are motivated by the approximation properties of Fourier series. Several implementati..."}}
{"id": "TBi43trTghC", "cdate": 1609459200000, "mdate": 1650786022063, "content": {"title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics", "abstract": "There is an ongoing debate in the NLP community whether modern language models contain linguistic knowledge, recovered through so-called probes. In this paper, we study whether linguistic knowledge is a necessary condition for the good performance of modern language models, which we call the rediscovery hypothesis. In the first place, we show that language models that are significantly compressed but perform well on their pretraining objectives retain good scores when probed for linguistic structures. This result supports the rediscovery hypothesis and leads to the second contribution of our paper: an information-theoretic framework that relates language modeling objectives with linguistic information. This framework also provides a metric to measure the impact of linguistic information on the word prediction task. We reinforce our analytical results with various experiments, both on synthetic and on real NLP tasks in English."}}
{"id": "GVOeQGIQt7", "cdate": 1609459200000, "mdate": 1696341956626, "content": {"title": "Hyperbolic Embedding for Finding Syntax in BERT", "abstract": ""}}
{"id": "isRTwZWLnq", "cdate": 1577836800000, "mdate": 1650786022066, "content": {"title": "Context Vectors Are Reflections of Word Vectors in Half the Dimensions (Extended Abstract)", "abstract": "This paper takes a step towards the theoretical analysis of the relationship between word embeddings and context embeddings in models such as word2vec. We start from basic probabilistic assumptions on the nature of word vectors, context vectors, and text generation. These assumptions are supported either empirically or theoretically by the existing literature. Next, we show that under these assumptions the widely-used word-word PMI matrix is approximately a random symmetric Gaussian ensemble. This, in turn, implies that context vectors are reflections of word vectors in approximately half the dimensions. As a direct application of our result, we suggest a theoretically grounded way of tying weights in the SGNS model."}}
{"id": "Lgd5bv0KdoQ", "cdate": 1577836800000, "mdate": 1650786022063, "content": {"title": "Squashed Shifted PMI Matrix: Bridging Word Embeddings and Hyperbolic Spaces", "abstract": "We show that removing sigmoid transformation in the skip-gram with negative sampling (SGNS) objective does not harm the quality of word vectors significantly and at the same time is related to factorizing a squashed shifted PMI matrix which, in turn, can be treated as a connection probabilities matrix of a random graph. Empirically, such graph is a complex network, i.e. it has strong clustering and scale-free degree distribution, and is tightly connected with hyperbolic spaces. In short, we show the connection between static word embeddings and hyperbolic spaces through the squashed shifted PMI matrix using analytical and empirical methods."}}
