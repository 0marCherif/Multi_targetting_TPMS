{"id": "S8M2edMoZaU", "cdate": 1681233080742, "mdate": 1681233080742, "content": {"title": "Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model", "abstract": "Piecing together the meaning of a narrative requires understanding both individual words and the intricate relationships between them. How does the brain construct this kind of rich, contextual meaning? Recently, a new class of artificial neural networks\u2014based on the Transformer architecture\u2014has revolutionized the field of language modeling. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. In this paper, we deconstruct these circuit computations and analyze the associated \u201ctransformations\u201d (alongside the more commonly studied \u201cembeddings\u201d) to provide a fine-grained window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we find that these transformations capture a hierarchy of linguistic computations across cortex, with transformations at later layers in the model mapping onto higher-level language areas in the brain. We then decompose these transformations into individual, functionally-specialized \u201cattention heads\u201d and demonstrate that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings indicate that large language models and the cortical language network converge on similar trends of computational specialization for processing natural language."}}
{"id": "Wfcbb0d7UEs", "cdate": 1664358385126, "mdate": null, "content": {"title": "How to talk so AI will learn: instructions, descriptions, and pragmatics", "abstract": "Humans intuitively use language to express our beliefs and desires, but today we lack computational models explaining such abstract language use.To address this challenge, we consider social learning in a linear bandit setting and ask how a human might communicate preferences over behaviors (i.e. the reward function). We study two distinct types of language: instructions, which specify partial policies, and descriptions, which provide information about the reward function. To explain how humans use such language, we suggest they reason about both known present and unknown future states: instructions optimize for the present, while descriptions optimize for the future. We formalize this choice by extending reward design to consider a distribution over states.We then define a pragmatic listener agent that infers the speaker's reward function by reasoning about how the speaker expresses themselves. Simulations suggest that (1) descriptions afford stronger learning than instructions; and (2) maintaining uncertainty over the speaker's pedagogical intent allows for robust reward inference. We hope these insights facilitate a shift from developing agents that obey language to agents that learn from it."}}
{"id": "ZLsZmNe1RDb", "cdate": 1652737572957, "mdate": null, "content": {"title": "How to talk so AI will learn: Instructions, descriptions, and autonomy", "abstract": "From the earliest years of our lives, humans use language to express our beliefs and desires. Being able to talk to artificial agents about our preferences would thus fulfill a central goal of value alignment. Yet today, we lack computational models explaining such language use. To address this challenge, we formalize learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviors. We study two distinct types of language: instructions, which provide information about the desired policy, and descriptions, which provide information about the reward function. We show that the agent's degree of autonomy determines which form of language is optimal: instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker's reward function by reasoning about how the speaker expresses themselves. We validate our models with a behavioral experiment, demonstrating that (1) our speaker model predicts human behavior, and (2) our pragmatic listener successfully recovers humans' reward functions. Finally, we show that this form of social learning can integrate with and reduce regret in traditional reinforcement learning. We hope these insights facilitate a shift from developing agents that obey language to agents that learn from it."}}
{"id": "buXZ7nIqiwE", "cdate": 1652737410082, "mdate": null, "content": {"title": "Using natural language and program abstractions to instill human inductive biases in machines", "abstract": "Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key. "}}
{"id": "Bh4u3ZDhsWq", "cdate": 1647195908324, "mdate": null, "content": {"title": "Linguistic communication as (inverse) reward design", "abstract": "Natural language is an intuitive and expressive way to communicate reward information to autonomous agents. It encompasses everything from concrete instructions to abstract descriptions of the world. Despite this, natural language is often challenging to learn from: it is difficult for machine learning methods to make appropriate inferences from such a wide range of input.\nThis paper proposes a generalization of reward design as a unifying principle to ground linguistic communication: speakers choose utterances to maximize expected rewards from the listener's future behaviors. We first extend reward design to incorporate reasoning about unknown future states in a linear bandit setting. We then define a speaker model which chooses utterances according to this objective. Simulations show that short-horizon speakers (reasoning primarily about a single, known state) tend to use instructions, while long-horizon speakers (reasoning primarily about unknown, future states) tend to describe the reward function. We then define a pragmatic listener which performs inverse reward design by jointly inferring the speaker's latent horizon and rewards. Our findings suggest that this extension of reward design to linguistic communication, including the notion of a latent speaker horizon, is a promising direction for achieving more robust alignment outcomes from natural language supervision."}}
{"id": "STDLnZwnsbc", "cdate": 1647195908261, "mdate": null, "content": {"title": "Using Natural Language to Guide Meta-Learning Agents towards Human-like Inductive Biases", "abstract": "Inductive biases are a key component of human intelligence, allowing people to acquire, represent, and use abstract knowledge. Although meta-learning has emerged as an approach to endowing neural networks with  inductive biases, agents trained via meta-learning can use very different strategies compared to humans. We show that co-training these agents on predicting human-generated natural language task descriptions guides them toward human-like inductive biases that more appropriately capture the structure of the task distribution as humans see it. We further show that the level of abstraction at which humans write these descriptions influences the size of the effect. This work provides a foundation for investigating how to collect task descriptions at the appropriate level of abstraction to leverage for approximating human-like learning of structured representations in neural networks.  "}}
{"id": "S-ExJ3H3px9", "cdate": 1646278054720, "mdate": 1646278054720, "content": {"title": "Open-domain clarification question generation without question examples", "abstract": "An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in dialogue. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model\u2019s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers."}}
{"id": "o9TraNEMMhF", "cdate": 1609459200000, "mdate": 1637139298551, "content": {"title": "Open-domain clarification question generation without question examples", "abstract": "Julia White, Gabriel Poesia, Robert Hawkins, Dorsa Sadigh, Noah Goodman. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "c__QYtl_Mv", "cdate": 1609459200000, "mdate": null, "content": {"title": "From partners to populations: A hierarchical Bayesian account of coordination and convention", "abstract": "Languages are powerful solutions to coordination problems: they provide stable, shared expectations about how the words we say correspond to the beliefs and intentions in our heads. Yet language use in a variable and non-stationary social environment requires linguistic representations to be flexible: old words acquire new ad hoc or partner-specific meanings on the fly. In this paper, we introduce CHAI (Continual Hierarchical Adaptation through Inference), a hierarchical Bayesian theory of coordination and convention formation that aims to reconcile the long-standing tension between these two basic observations. We argue that the central computational problem of communication is not simply transmission, as in classical formulations, but continual learning and adaptation over multiple timescales. Partner-specific common ground quickly emerges from social inferences within dyadic interactions, while community-wide social conventions are stable priors that have been abstracted away from interactions with multiple partners. We present new empirical data alongside simulations showing how our model provides a computational foundation for several phenomena that have posed a challenge for previous accounts: (1) the convergence to more efficient referring expressions across repeated interaction with the same partner, (2) the gradual transfer of partner-specific common ground to strangers, and (3) the influence of communicative context on which conventions eventually form."}}
{"id": "LVgsnRgc8Vz", "cdate": 1609459200000, "mdate": 1637139299667, "content": {"title": "The Division of Labor in Communication: Speakers Help Listeners Account for Asymmetries in Visual Perspective", "abstract": "Recent debates over adults' theory of mind use have been fueled by surprising failures of perspective-taking in communication, suggesting that perspective-taking may be relatively effortful. Yet adul..."}}
