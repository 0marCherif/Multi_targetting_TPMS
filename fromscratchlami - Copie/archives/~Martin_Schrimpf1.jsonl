{"id": "iPF7mhoWkOl", "cdate": 1664294260190, "mdate": null, "content": {"title": "Primate Inferotemporal Cortex Neurons Generalize Better to Novel Image Distributions Than Analogous Deep Neural Networks Units", "abstract": "Humans are successfully able to recognize objects in a variety of image distributions. Today's artificial neural networks (ANNs), on the other hand, struggle to recognize objects in many image domains, especially those different from the training distribution. It is currently unclear which parts of the ANNs could be improved in order to close this generalization gap. In this work, we used recordings from primate high-level visual cortex (IT) to isolate whether ANNs lag behind primate generalization capabilities because of their encoder (transformations up to the penultimate layer), or their decoder (linear transformation into class labels). Specifically, we fit a linear decoder on images from one domain and evaluate transfer performance on twelve held-out domains, comparing fitting on primate IT representations vs. representations in ANN penultimate layers. To fairly compare, we scale the number of each ANN's units so that its in-domain performance matches that of the sampled IT population (i.e. 71 IT neural sites, 73% binary-choice accuracy). We find that the sampled primate population achieves, on average, 68% performance on the held-out-domains. Comparably sampled populations from ANN model units generalize less well, maintaining on average 60%. This is independent of the number of sampled units: models' out-of-domain accuracies consistently lag behind primate IT. These results suggest that making ANN model units more like primate IT will improve the generalization performance of ANNs."}}
{"id": "SMYdcXjJh1q", "cdate": 1663850353919, "mdate": null, "content": {"title": "Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness", "abstract": "While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings  across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model \"IT\" representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models' IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains."}}
{"id": "g1SzIRLQXMM", "cdate": 1632965333637, "mdate": null, "content": {"title": "Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream", "abstract": "After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are considered poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image.  While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to standard model training on labeled images in ImageNet, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve 80% of the match to adult ventral stream. Specifically, training benefits predictions of higher visual cortex the most whereas early visual cortex predictions only improve marginally over the course of training. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved \u201cat birth\" (i.e. no training at all). Third, we find that, by training only 5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. This approach further improves on ImageNet performance over previous attempts in computer vision of minimizing trained components without substantially increasing the relative number of trained parameters. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be \"wired up\" by evolution (a model's \"birth\" state) and by developmental learning (a model's updates based on visual experience)."}}
{"id": "I913Sp8V9MX", "cdate": 1624900542683, "mdate": 1624900542683, "content": {"title": "What constitutes understanding of ventral pathway function?", "abstract": "Scientific question: How can we use computational models to decipher and/or represent neuronal\ntuning properties, and standardize descriptions for comparisons across stimuli/tasks/species?\n\nIntroduction. At the onset of visual neuroscience, first there was light, and then came explanations.\nWorking to stimulate neurons in primary visual cortex (V1) in 1958, Hubel and Wiesel projected white light onto\ncat retinas using a modified ophthalmoscope and a slide projector. They had glass- and brass slides with\ndrawings and cutouts, using them to shape light into simple geometric patterns. Among their many findings,\nthey established V1 neurons showed higher activity to specifically placed line segments \u2013 lines optimized in\ntheir location, length/width, color, and their rotation. The simplicity of these stimuli allowed for straightforward\ninterpretations, specifically that V1 neurons signal contour orientation1.\n\nObservations vs. interpretations. There were five components that made these experiments\ncanonical, and have been included in most subsequent studies of visual neuroscience:\n1) A physical stimulus (e.g., light patterns on a projection screen/computer monitor).\n2) A generative method for producing the physical stimuli (e.g., lines drawn manually on slides,\nvariables for a computer graphics library, vectors in a generative adversarial network).\n3) An experimenter-labeled stimulus space (e.g., orientation, categories) with a metric to order/cluster\nthe physical stimuli (e.g., angular distance, perceptual similarity).\n4) Neuronal activity associated with each stimulus (e.g., spike rates).\n5) A potential mechanism suggesting how those tuning functions could arise from earlier inputs (e.g.,\nspatially aligned projections from neurons in the midbrain [lateral geniculate nucleus]).\nThe first and fourth components are observables (we refer to these as pixels and spikes). The second\nand third components are fundamentally entangled with the experimenter\u2019s theories and interpretations (we\nrefer to these as methods and spaces). The linchpin observation is that in this experimental design, the\nrelationship between pixels and spikes is causal, but the relationship between spaces and spikes is\ncorrelational. Theoretically, there can be alternative explanations implicit in any given stimulus space which\nalso affect neuronal activity \u2014 in an experiment, the subject\u2019s brain only has access to the physical stimuli, not\nto the meaning attached to it."}}
{"id": "db1InWAwW2T", "cdate": 1623031898576, "mdate": null, "content": {"title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation", "abstract": "We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables the simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable ``avatars\u201d that embody AI agents; and support for human interactions with VR devices. TDW\u2019s API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that \u2018learn like a child\u2019, and attention studies in humans and neural networks. "}}
{"id": "-Oa2C_-8ag", "cdate": 1609459200000, "mdate": null, "content": {"title": "Unsupervised neural network models of the ventral visual stream", "abstract": "Primates show remarkable ability to recognize objects. This ability is achieved by their ventral visual stream, multiple hierarchically interconnected brain areas. The best quantitative models of these areas are deep neural networks trained with human annotations. However, they receive more annotations than infants, making them implausible models of the ventral stream development. Here, we report that recent progress in unsupervised learning has largely closed this gap. We find the networks learned with recent unsupervised methods achieve prediction accuracy in the ventral stream that equals or exceeds that of today\u2019s best models. These results illustrate a use of unsupervised learning to model a brain system and present a strong candidate for a biologically plausible computational theory of sensory learning. Codes and data have been deposited in GitHub (<https://github.com/neuroailab/unsup_vvs>)."}}
{"id": "5i4vRgoZauw", "cdate": 1601308385230, "mdate": null, "content": {"title": "Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream", "abstract": "After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image. While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to the current leading model of the adult ventral stream, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve ~80% of the match to adult ventral stream. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved \u201cat birth\" (i.e. no training at all). Third, we find that, by training only ~5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. When these three strategies are applied in combination, we find that these new models achieve ~80% of a fully trained model's match to the brain, while using two orders of magnitude fewer supervised synaptic updates. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be \"wired up\" by evolution (a model's \"birth\" state) and by developmental learning (a model's updates based on visual experience)."}}
{"id": "-o0dOwashib", "cdate": 1596479634812, "mdate": null, "content": {"title": "Is it that simple? The use of linear models in cognitive neuroscience", "abstract": "Title: Is it that simple? The use of linear models in cognitive neuroscience\nScientific question: Do linear models provide an accurate, interpretable, and biologically plausible description of brain activity?"}}
{"id": "svhKZjE54Xa", "cdate": 1577836800000, "mdate": null, "content": {"title": "Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations", "abstract": "Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18% and 3%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications."}}
{"id": "6Gc8HxGlhwY", "cdate": 1577836800000, "mdate": null, "content": {"title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation", "abstract": "We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include: real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable agents that embody AI agents; and support for human interactions with VR devices. TDW's API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that learn like a child, and attention studies in humans and neural networks."}}
