{"id": "XYTwCOoKkLY", "cdate": 1663850484858, "mdate": null, "content": {"title": "Towards Out-of-Distribution Adversarial Robustness", "abstract": "Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach.\nConcretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4\\% the best existing baseline to 25.9\\% on MNIST, and from 16.9\\% to 23.5\\% on CIFAR10."}}
{"id": "ul7HSEpkEHX", "cdate": 1663850312944, "mdate": null, "content": {"title": "Learning Robust Kernel Ensembles with Kernel Average Pooling", "abstract": "Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce \\emph{Kernel Average Pool (KAP)}, a new neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when combined with activation noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness against strong adversarial attacks such as AutoAttack that are on par with adversarially trained networks but are importantly obtained without training on any adversarial examples."}}
{"id": "s1yaWFDLxVG", "cdate": 1652737873971, "mdate": null, "content": {"title": "Gradient Descent Is Optimal Under Lower Restricted Secant Inequality And Upper Error Bound", "abstract": "The study of first-order optimization is sensitive to the assumptions made on the objective functions.\nThese assumptions induce complexity classes which play a key role in worst-case analysis, including\nthe fundamental concept of algorithm optimality. Recent work argues that strong convexity and\nsmoothness\u2014popular assumptions in literature\u2014lead to a pathological definition of the condition\nnumber. Motivated by this result, we focus on the class of functions\nsatisfying a lower restricted secant inequality and an upper error bound. On top of being robust to\nthe aforementioned pathological behavior and including some non-convex functions, this pair of\nconditions displays interesting geometrical properties. In particular, the necessary and sufficient\nconditions to interpolate a set of points and their gradients within the class can be separated into\nsimple conditions on each sampled gradient. This allows the performance estimation problem (PEP)\n to be solved analytically, leading to a lower bound\non the convergence rate that proves gradient descent to be exactly optimal on this class of functions\namong all first-order algorithms."}}
{"id": "gSRnszxHPX", "cdate": 1640995200000, "mdate": 1681914922875, "content": {"title": "Towards Out-of-Distribution Adversarial Robustness", "abstract": "Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4% with the best existing baseline to 25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10."}}
{"id": "bnQF0uJ4tp0", "cdate": 1640995200000, "mdate": 1683901994083, "content": {"title": "Gradient Descent Is Optimal Under Lower Restricted Secant Inequality And Upper Error Bound", "abstract": "The study of first-order optimization is sensitive to the assumptions made on the objective functions.These assumptions induce complexity classes which play a key role in worst-case analysis, includingthe fundamental concept of algorithm optimality. Recent work argues that strong convexity andsmoothness\u2014popular assumptions in literature\u2014lead to a pathological definition of the conditionnumber. Motivated by this result, we focus on the class of functionssatisfying a lower restricted secant inequality and an upper error bound. On top of being robust tothe aforementioned pathological behavior and including some non-convex functions, this pair ofconditions displays interesting geometrical properties. In particular, the necessary and sufficientconditions to interpolate a set of points and their gradients within the class can be separated intosimple conditions on each sampled gradient. This allows the performance estimation problem (PEP) to be solved analytically, leading to a lower boundon the convergence rate that proves gradient descent to be exactly optimal on this class of functionsamong all first-order algorithms."}}
{"id": "H15_S25vbzw", "cdate": 1640995200000, "mdate": 1681967035883, "content": {"title": "Learning Robust Kernel Ensembles with Kernel Average Pooling", "abstract": "Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness against strong adversarial attacks such as AutoAttack without training on any adversarial examples."}}
{"id": "IIo3Ew4v5tk", "cdate": 1621630268833, "mdate": null, "content": {"title": "Adversarial Feature Desensitization", "abstract": "Neural networks are known to be vulnerable to adversarial attacks -- slight but carefully constructed perturbations of the inputs which can drastically impair the network's performance. Many defense methods have been proposed for improving robustness of  deep networks by training them on adversarially perturbed inputs. However, these models often remain vulnerable to new types of attacks not seen during training, and even to slightly stronger versions of previously seen  attacks. In this work, we propose a novel approach to  adversarial robustness, which builds upon the insights from the domain adaptation field. Our method, called Adversarial Feature Desensitization (AFD), aims at learning  features that are invariant towards adversarial perturbations of the inputs. This is achieved through a game where we learn features that are both predictive and robust (insensitive to adversarial attacks), i.e. cannot be used to discriminate between natural and adversarial data. Empirical results on several benchmarks  demonstrate the effectiveness of the proposed approach against a wide range of attack types and attack strengths. Our code is available at https://github.com/BashivanLab/afd."}}
{"id": "4e_Yvt47kh", "cdate": 1621630268833, "mdate": null, "content": {"title": "Adversarial Feature Desensitization", "abstract": "Neural networks are known to be vulnerable to adversarial attacks -- slight but carefully constructed perturbations of the inputs which can drastically impair the network's performance. Many defense methods have been proposed for improving robustness of  deep networks by training them on adversarially perturbed inputs. However, these models often remain vulnerable to new types of attacks not seen during training, and even to slightly stronger versions of previously seen  attacks. In this work, we propose a novel approach to  adversarial robustness, which builds upon the insights from the domain adaptation field. Our method, called Adversarial Feature Desensitization (AFD), aims at learning  features that are invariant towards adversarial perturbations of the inputs. This is achieved through a game where we learn features that are both predictive and robust (insensitive to adversarial attacks), i.e. cannot be used to discriminate between natural and adversarial data. Empirical results on several benchmarks  demonstrate the effectiveness of the proposed approach against a wide range of attack types and attack strengths. Our code is available at https://github.com/BashivanLab/afd."}}
{"id": "MAabCPf9ZF", "cdate": 1609459200000, "mdate": 1681494076319, "content": {"title": "Adversarial Feature Desensitization", "abstract": ""}}
{"id": "whvX7DIspmz5", "cdate": 1577836800000, "mdate": null, "content": {"title": "Linear Lower Bounds and Conditioning of Differentiable Games", "abstract": "Recent successes of game-theoretic formulations in ML have caused a resurgence of research interest in differentiable games. Overwhelmingly, that research focuses on methods and upper bounds on the..."}}
