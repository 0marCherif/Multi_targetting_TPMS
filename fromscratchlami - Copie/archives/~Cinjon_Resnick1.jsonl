{"id": "mOXHnLqR7AC", "cdate": 1632875467799, "mdate": null, "content": {"title": "Causal Scene BERT: Improving object detection by searching for challenging groups", "abstract": "Autonomous vehicles (AV) rely on learning-based perception modules parametrized with neural networks for tasks like object detection. These modules frequently have low expected error overall but high error on atypical groups of data due to biases inherent in the training process. Multiple heuristics are employed to identify \"failures\" in AVs, a typical example being driver interventions. After identification, a human team combs through the associated data to group perception failures that share common causes. More data from these groups is then collected and annotated before retraining the model to fix the issue. In other words, error groups are found and addressed in hindsight as they appear. Our main contribution is a pseudo-automatic method to discover such groups in foresight by performing causal interventions on simulated driving scenes. To keep our interventions on the data manifold, we use masked language models. We verify that the prioritized groups found via intervention are challenging for the object detector and show that retraining with data collected from these groups helps inordinately compared to adding more IID data. We also release software to run interventions in simulated scenes, which we hope will benefit the causality community. "}}
{"id": "7OkETMJ_mAa", "cdate": 1589015034427, "mdate": null, "content": {"title": "Compositionality and Capacity in Emergent Languages", "abstract": "Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community."}}
{"id": "BklVvNdl9H", "cdate": 1572009052201, "mdate": null, "content": {"title": "Capacity, Bandwidth, and Compositionality in Emergent Language Learning", "abstract": "Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality of emergent languages. Our foremost contribution is to explore how capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community."}}
{"id": "HsZ-x1GguaB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Model AI Assignments 2019.", "abstract": "The Model AI Assignments session seeks to gather and disseminate the best assignment designs of the Artificial Intelligence (AI) Education community. Recognizing that assignments form the core of student learning experience, we here present abstracts of ten AI assignments from the 2019 session that are easily adoptable, playfully engaging, and flexible for a variety of instructor needs. Assignment specifications and supporting resources may be found at http: //modelai.gettysburg.edu."}}
{"id": "H1xk8jAqKQ", "cdate": 1538087750724, "mdate": null, "content": {"title": "Backplay: 'Man muss immer umkehren'", "abstract": "Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation."}}
{"id": "HJgBrJBig7", "cdate": 1528676156679, "mdate": null, "content": {"title": "Depth First Learning: Learning to Understand Machine Learning", "abstract": "An experimental science is only as strong as the veracity of its experiments -- when experiments are not reproducible, researchers struggle to place their work on solid footing and progress is hampered. Many fields have had reproducibility crises. Some have recently recognized this as a problem in machine learning as well.\n\nOne way to make research more reproducible is to make it easier to develop a deep understanding of the fundamentals underlying machine learning research papers. In this work, we present a new pedagogy, Depth First Learning, that addresses the challenges in understanding these fundamentals. In Depth First Learning, papers are studied down to their core conceptual dependencies, with additional material such as exercises and reproductions in code. We have used this approach in classes and codified our curricula as an open-source website available at \\url{http://www.depthfirstlearning.com}."}}
{"id": "SJEaYoWObr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders", "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas t..."}}
