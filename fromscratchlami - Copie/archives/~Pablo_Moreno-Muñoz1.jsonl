{"id": "dEqFMpCY8gj", "cdate": 1694278869250, "mdate": null, "content": {"title": "One-Week Suicide Risk Prediction Using Real-Time Smartphone Monitoring: Prospective Cohort Study", "abstract": "Background: Suicide is a major global public health issue that is becoming increasingly common despite preventive efforts.\nThough current methods for predicting suicide risk are not sufficiently accurate, technological advances provide invaluable tools\nwith which we may evolve toward a personalized, predictive approach.\n\nObjective: We aim to predict the short-term (1-week) risk of suicide by identifying changes in behavioral patterns characterized\nthrough real-time smartphone monitoring in a cohort of patients with suicidal ideation.\n\nMethods: We recruited 225 patients between February 2018 and March 2020 with a history of suicidal thoughts and behavior\nas part of the multicenter SmartCrisis study. Throughout 6 months of follow-up, we collected information on the risk of suicide\nor mental health crises. All participants underwent voluntary passive monitoring using data generated by their own smartphones,\nincluding distance walked and steps taken, time spent at home, and app usage. The algorithm constructs daily activity profiles\nfor each patient according to these data and detects changes in the distribution of these profiles over time. Such changes are\nconsidered critical periods, and their relationship with suicide-risk events was tested.\n\nResults: During follow-up, 18 (8%) participants attempted suicide, and 14 (6.2%) presented to the emergency department for\npsychiatric care. The behavioral changes identified by the algorithm predicted suicide risk in a time frame of 1 week with an area\nunder the curve of 0.78, indicating good accuracy.\n\nConclusions: We describe an innovative method to identify mental health crises based on passively collected information from\npatients\u2019smartphones. This technology could be applied to homogeneous groups of patients to identify different types of crises."}}
{"id": "oSaGNr8mEMc", "cdate": 1672531200000, "mdate": 1692366996620, "content": {"title": "Adaptive Cholesky Gaussian Processes", "abstract": "We present a method to approximate Gaussian process regression models to large datasets by considering only a subset of the data. Our approach is novel in that the size of the subset is selected on..."}}
{"id": "Z8zldl2BvI", "cdate": 1672531200000, "mdate": 1708513149897, "content": {"title": "On Masked Pre-training and the Marginal Likelihood", "abstract": "Masked pre-training removes random input dimensions and learns a model that can predict the missing values. Empirical results indicate that this intuitive form of self-supervised learning yields models that generalize very well to new domains. A theoretical understanding is, however, lacking. This paper shows that masked pre-training with a suitable cumulative scoring function corresponds to maximizing the model's marginal likelihood, which is de facto the Bayesian model selection measure of generalization. Beyond shedding light on the success of masked pre-training, this insight also suggests that Bayesian models can be trained with appropriately designed self-supervision. Empirically, we confirm the developed theory and explore the main learning principles of masked pre-training in large language models."}}
{"id": "S7YFOohudd-", "cdate": 1672531200000, "mdate": 1696139492865, "content": {"title": "Riemannian Laplace approximations for Bayesian neural networks", "abstract": "Bayesian neural networks often approximate the weight-posterior with a Gaussian distribution. However, practical posteriors are often, even locally, highly non-Gaussian, and empirical performance deteriorates. We propose a simple parametric approximate posterior that adapts to the shape of the true posterior through a Riemannian metric that is determined by the log-posterior gradient. We develop a Riemannian Laplace approximation where samples naturally fall into weight-regions with low negative log-posterior. We show that these samples can be drawn by solving a system of ordinary differential equations, which can be done efficiently by leveraging the structure of the Riemannian metric and automatic differentiation. Empirically, we demonstrate that our approach consistently improves over the conventional Laplace approximation across tasks. We further show that, unlike the conventional Laplace approximation, our method is not overly sensitive to the choice of prior, which alleviates a practical pitfall of current approaches."}}
{"id": "gNHWhkAW-rD", "cdate": 1663661070309, "mdate": 1663661070309, "content": {"title": "Laplacian Autoencoders for Learning Stochastic Representations", "abstract": "Established methods for unsupervised representation learning such as variational autoencoders produce none or poorly calibrated uncertainty estimates making it difficult to evaluate if learned representations are stable and reliable. In this work, we present a Bayesian autoencoder for unsupervised representation learning, which is trained using a novel variational lower bound of the autoencoder evidence. This is maximized using Monte Carlo EM with a variational distribution that takes the shape of a Laplace approximation. We develop a new Hessian approximation that scales linearly with data size allowing us to model high-dimensional data. Empirically, we show that our Laplacian autoencoder estimates well-calibrated uncertainties in both latent and output space. We demonstrate that this results in improved performance across a multitude of downstream tasks."}}
{"id": "UOxwwmTgBBW", "cdate": 1663660896256, "mdate": 1663660896256, "content": {"title": "Revisiting Active Sets for Gaussian Process Decoders", "abstract": "Decoders built on Gaussian processes (GPs) are enticing due to the marginalisation over the non-linear function space. Such models (also known as GP-LVMs) are often expensive and notoriously difficult to train in practice, but can be scaled using variational inference and inducing points. In this paper, we revisit active set approximations. We develop a new stochastic estimate of the log-marginal likelihood based on recently discovered links to cross-validation, and propose a computationally efficient approximation thereof. We demonstrate that the resulting stochastic active sets (SAS) approximation significantly improves the robustness of GP decoder training while reducing computational cost. The SAS-GP obtains more structure in the latent space, scales to many datapoints and learns better representations than variational autoencoders, which is rarely the case for GP decoders."}}
{"id": "rAVqc7KSGDa", "cdate": 1652737693376, "mdate": null, "content": {"title": "Revisiting Active Sets for Gaussian Process Decoders", "abstract": "Decoders built on Gaussian processes (GPs) are enticing due to the marginalisation over the non-linear function space. Such models (also known as GP-LVMs) are often expensive and notoriously difficult to train in practice, but can be scaled using variational inference and inducing points. In this paper, we revisit active set approximations. We develop a new stochastic estimate of the log-marginal likelihood based on recently discovered links to cross-validation, and we propose a computationally efficient approximation thereof. We demonstrate that the resulting stochastic active sets (SAS) approximation significantly improves the robustness of GP decoder training, while reducing computational cost. The SAS-GP obtains more structure in the latent space, scales to many datapoints, and learns better representations than variational autoencoders, which is rarely the case for GP decoders."}}
{"id": "aaar9y7qjfw", "cdate": 1652737692250, "mdate": null, "content": {"title": "Laplacian Autoencoders for Learning Stochastic Representations", "abstract": "Established methods for unsupervised representation learning such as variational autoencoders produce none or poorly calibrated uncertainty estimates making it difficult to evaluate if learned representations are stable and reliable. In this work, we present a Bayesian autoencoder for unsupervised representation learning, which is trained using a novel variational lower-bound of the autoencoder evidence. This is maximized using Monte Carlo EM with a variational distribution that takes the shape of a Laplace approximation. We develop a new Hessian approximation that scales linearly with data size allowing us to model high-dimensional data. Empirically, we show that our Laplacian autoencoder estimates well-calibrated uncertainties in both latent and output space. We demonstrate that this results in improved performance across a multitude of downstream tasks."}}
{"id": "2EBn01PJh17", "cdate": 1652737394049, "mdate": null, "content": {"title": "Adaptive Cholesky Gaussian Processes", "abstract": "We present a method to fit exact Gaussian process models to large datasets by considering only a subset of the data. Our approach is novel in that the size of the subset is selected on the fly during exact inference with little computational overhead. From an empirical observation that the log-marginal likelihood often exhibits a linear trend once a sufficient subset of a dataset has been observed, we conclude that many large datasets contain redundant information that only slightly affects the posterior. Based on this, we provide probabilistic bounds on the full model evidence that can identify such subsets. Remarkably, these bounds are largely composed of terms that appear in intermediate steps of the standard Cholesky decomposition, allowing us to modify the algorithm to adaptively stop the decomposition once enough data have been observed. Empirically, we show that our method can be directly plugged into well-known inference schemes to fit exact Gaussian process models to large datasets. "}}
{"id": "y9U-MXGk_3", "cdate": 1640995200000, "mdate": 1708513149824, "content": {"title": "Revisiting Active Sets for Gaussian Process Decoders", "abstract": "Decoders built on Gaussian processes (GPs) are enticing due to the marginalisation over the non-linear function space. Such models (also known as GP-LVMs) are often expensive and notoriously difficult to train in practice, but can be scaled using variational inference and inducing points. In this paper, we revisit active set approximations. We develop a new stochastic estimate of the log-marginal likelihood based on recently discovered links to cross-validation, and we propose a computationally efficient approximation thereof. We demonstrate that the resulting stochastic active sets (SAS) approximation significantly improves the robustness of GP decoder training, while reducing computational cost. The SAS-GP obtains more structure in the latent space, scales to many datapoints, and learns better representations than variational autoencoders, which is rarely the case for GP decoders."}}
