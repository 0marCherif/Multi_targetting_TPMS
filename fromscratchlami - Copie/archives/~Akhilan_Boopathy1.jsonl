{"id": "hM8MuFPmHc", "cdate": 1672531200000, "mdate": 1681655323699, "content": {"title": "Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle", "abstract": ""}}
{"id": "4XMAzZasId", "cdate": 1663849802382, "mdate": null, "content": {"title": "Model-agnostic Measure of Generalization Difficulty", "abstract": "The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimensions. Our measure can be applied to compute and compare supervised learning, reinforcement learning and meta-learning task difficulties against each other. We show that applied empirically, it formally quantifies intuitively expected trends, e.g. that in terms of required inductive bias, MNIST $<$ CIFAR10 $<$ Imagenet and fully observable Markov decision processes (MDPs) $<$ partially observable MDPs. Further, we show that classification of complex images $<$ few-shot meta-learning with simple images. Our measure provides a quantitative metric to guide the construction of more complex tasks requiring greater inductive bias, and thereby encourages the development of more sophisticated architectures and learning algorithms with more powerful generalization capabilities."}}
{"id": "ED2Jjms9A4H", "cdate": 1663849801938, "mdate": null, "content": {"title": "Efficient Exploration via Fragmentation and Recall", "abstract": "Efficient exploration and model-building are critical for learning in large state- spaces. However, agents typically face problems like getting stuck locally during exploration and catastrophic forgetting in their construction of models when the environments are heterogeneous. Here, we propose and apply the concept of Fragmentation-and-Recall to solve spatial (FarMap) and reinforcement learning problems (FarCuriosity). Agents construct local maps or local models, respectively, which are used to predict the current observation. High surprisal points lead to a fragmentation event. At fracture points, we store the current map or model fragment in a long-term memory (LTM) and initialize a new fragment. On the other hand, Fragments are recalled (and thus reused) from LTM if the observations of their fracture points match the agent\u2019s current observation during exploration. The set of fracture points defines a set of intrinsic potential subgoals. Agents choose their next subgoal from the set of near and far potential subgoals in the current fragment or LTM, respectively. Thus, local maps and model fragments guide exploration locally and avoid catastrophic forgetting in learning heterogeneous environments, while LTM promotes exploration more globally. We evaluate FarMap and FarCuriosity on complex procedurally-generated spatial environments and on reinforcement learning benchmarks and demonstrate that the proposed methods are more efficient at exploration and memory use, and in harvesting extrinsic rewards, respectively."}}
{"id": "jhGodQ-LKZ", "cdate": 1640995200000, "mdate": 1681655323676, "content": {"title": "How to Train Your Wide Neural Network Without Backprop: An Input-Weight Alignment Perspective", "abstract": ""}}
{"id": "GEwMHTdJltr", "cdate": 1609459200000, "mdate": 1681490806010, "content": {"title": "Fast Training of Provably Robust Neural Networks by SingleProp", "abstract": ""}}
{"id": "FEqIjVXGSrP", "cdate": 1577836800000, "mdate": 1681490809999, "content": {"title": "Proper Network Interpretability Helps Adversarial Robustness in Classification", "abstract": ""}}
{"id": "Hyes70EYDB", "cdate": 1569439267249, "mdate": null, "content": {"title": "Visual Interpretability Alone Helps Adversarial Robustness", "abstract": "Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks."}}
{"id": "Hke_f0EYPH", "cdate": 1569439248044, "mdate": null, "content": {"title": "Efficient Training of Robust and Verifiable Neural Networks", "abstract": "Recent works have developed several methods of defending neural networks against adversarial attacks with certified guarantees. We propose that many common certified defenses can be viewed under a unified framework of regularization. This unified framework provides a technique for comparing different certified defenses with respect to robust generalization. In addition, we develop a new regularizer that is both more efficient than existing certified defenses and can be used to train networks with higher certified accuracy. Our regularizer also extends to an L0 threat model and ensemble models. Through experiments on MNIST, CIFAR-10 and GTSRB, we demonstrate improvements in training speed and certified accuracy compared to state-of-the-art certified defenses."}}
{"id": "SmC4KZGldaH", "cdate": 1546300800000, "mdate": null, "content": {"title": "CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks.", "abstract": "Verifying robustness of neural network classifiers has attracted great interests and attention due to the success of deep neural networks and their unexpected vulnerability to adversarial perturbations. Although finding minimum adversarial distortion of neural networks (with ReLU activations) has been shown to be an NP-complete problem, obtaining a non-trivial lower bound of minimum distortion as a provable robustness guarantee is possible. However, most previous works only focused on simple fully-connected layers (multilayer perceptrons) and were limited to ReLU activations. This motivates us to propose a general and efficient framework, CNN-Cert, that is capable of certifying robustness on general convolutional neural networks. Our framework is general \u2013 we can handle various architectures including convolutional layers, max-pooling layers, batch normalization layer, residual blocks, as well as general activation functions; our approach is efficient \u2013 by exploiting the special structure of convolutional layers, we achieve up to 17 and 11 times of speed-up compared to the state-of-the-art certification algorithms (e.g. Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach while our algorithm obtains similar or even better verification bounds. In addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and CROWN. We demonstrate by extensive experiments that our method outperforms state-of-the-art lowerbound-based certification algorithms in terms of both bound quality and speed."}}
{"id": "SJ-9goZO-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach", "abstract": "We propose a novel framework PROVEN to \\textbf{PRO}babilistically \\textbf{VE}rify \\textbf{N}eural network\u2019s robustness with statistical guarantees. PROVEN provides probability certificates of neura..."}}
