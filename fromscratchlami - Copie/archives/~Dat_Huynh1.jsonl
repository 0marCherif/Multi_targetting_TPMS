{"id": "vgxmvA685H", "cdate": 1668577799635, "mdate": 1668577799635, "content": {"title": "Fine-Grained Generalized Zero-Shot Learning via Dense Attribute-Based Attention", "abstract": "We address the problem of fine-grained generalized zero-shot recognition of visually similar classes without training images for some classes.\nWe propose a dense attribute-based attention mechanism that for each attribute focuses on the most relevant image regions, obtaining attribute-based features. Instead of aligning a global feature vector of an image with its associated class semantic vector, we propose an attribute embedding technique that aligns each attribute-based feature with its attribute semantic vector. Hence, we compute a vector of attribute scores, for the presence of each attribute in an image, whose similarity with the true class semantic vector is maximized. Moreover, we adjust each attribute score using an attention mechanism over attributes to better capture the discriminative power of different attributes. To tackle the challenge of bias towards seen classes during testing, we propose a new self-calibration loss that adjusts the probability of unseen classes to account for the training bias. We conduct experiments on three popular datasets of CUB, SUN and AWA2 as well as the large-scale DeepFashion dataset, showing that our model significantly improves the state of the art. %By extensive experiments, we show that our model significantly improves the state of the art.\n"}}
{"id": "2GcEyBymwh", "cdate": 1668577448000, "mdate": 1668577448000, "content": {"title": "Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling", "abstract": "Open-vocabulary instance segmentation aims at segmenting novel classes without mask annotations.\nIt is an important step toward reducing laborious human supervision.\nMost existing works first pretrain a model on captioned images covering many novel classes and then finetune it on limited base classes with mask annotations.\nHowever, the high-level textual information learned from caption pretraining alone cannot effectively encode the details required for pixel-wise segmentation.\nTo address this, we propose a cross-modal pseudo-labeling framework, which generates training pseudo masks by aligning word semantics in captions with visual features of object masks in images.\nThus, our framework is capable of labeling novel classes in captions via their word semantics to self-train a student model.\nTo account for noises in pseudo masks, we design a robust student model that selectively distills mask knowledge by estimating the mask noise levels, hence mitigating the adverse impact of noisy pseudo masks.\nBy extensive experiments, we show the effectiveness of our framework, where we significantly improve mAP score by 4.5\\% on MS-COCO and 5.1\\% on the large-scale Open Images \\& Conceptual Captions datasets compared to the state-of-the-art.\n"}}
{"id": "bITJpx_NVA", "cdate": 1664928785850, "mdate": null, "content": {"title": "Few-Shot Learnable Augmentation for Financial Time Series Prediction under Distribution Shifts", "abstract": "We address the problem of distribution shift in financial time series prediction, where the behavior of the time series changes over time.\nSatisfactory performance of forecasting algorithms requires constant model recalibration or fine-tuning to adapt to the new data distribution.\nSpecifically, the ability to quickly fine-tune a model with only a few training samples available from the new distribution is crucial for many business applications.\nIn this paper, we develop a novel method for learnable data augmentation that effectively adjusts to the new time series distribution with only a few samples. \nWe demonstrate the effectiveness of our method compared to the state-of-the-art augmentation methods on both univariate time series (e.g., stock data) and multivariate time series (e.g., yield rate curves) in the presence of distribution shift due to the COVID market shock in 2020."}}
{"id": "IACULnkPwYO", "cdate": 1617820299575, "mdate": null, "content": {"title": "Compositional Zero-Shot Learning via Fine-Grained Dense Feature Composition", "abstract": "We develop a novel generative model for zero-shot learning to recognize fine\u0002grained unseen classes without training samples. Our observation is that generating\nholistic features of unseen classes fails to capture every attribute needed to dis\u0002tinguish small differences among classes. We propose a feature composition\nframework that learns to extract attribute-based features from training samples\nand combines them to construct fine-grained features for unseen classes. Feature\ncomposition allows us to not only selectively compose features of unseen classes\nfrom only relevant training samples, but also obtain diversity among composed\nfeatures via changing samples used for composition. In addition, instead of build\u0002ing a global feature of an unseen class, we use all attribute-based features to form\na dense representation consisting of fine-grained attribute details. To recognize\nunseen classes, we propose a novel training scheme that uses a discriminative\nmodel to construct features that are subsequently used to train itself. Therefore,\nwe directly train the discriminative model on composed features without learning\nseparate generative models. We conduct experiments on four popular datasets\nof DeepFashion, AWA2, CUB, and SUN, showing that our method significantly\nimproves the state of the art."}}
