{"id": "9uX8Aeg2kt", "cdate": 1640995200000, "mdate": 1649691784837, "content": {"title": "latent-GLAT: Glancing at Latent Variables for Parallel Text Generation", "abstract": "Recently, parallel text generation has received widespread attention due to its success in generation efficiency. Although many advanced techniques are proposed to improve its generation quality, they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset, limiting their applications. In this paper, we propose $\\textit{latent}$-GLAT, which employs the discrete latent variables to capture word categorical information and invoke an advanced curriculum learning technique, alleviating the multi-modality problem. Experiment results show that our method outperforms strong baselines without the help of an autoregressive model, which further broadens the application scenarios of the parallel decoding paradigm."}}
{"id": "wkPMbIgsEt2", "cdate": 1609459200000, "mdate": 1649691787986, "content": {"title": "Non-Autoregressive Translation by Learning Target Categorical Codes", "abstract": "Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "i4DK9NybG3k", "cdate": 1609459200000, "mdate": 1649691785399, "content": {"title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation", "abstract": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "JHFam-227GC", "cdate": 1609459200000, "mdate": 1634226519112, "content": {"title": "Non-Autoregressive Translation by Learning Target Categorical Codes", "abstract": "Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "BnsvjePe19V", "cdate": 1609459200000, "mdate": 1634226519353, "content": {"title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation", "abstract": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "ZaYZfu8pT_N", "cdate": 1601308330901, "mdate": null, "content": {"title": "Non-iterative Parallel Text Generation via Glancing Transformer", "abstract": "Although non-autoregressive models with one-iteration generation achieve remarkable inference speed-up, they still fall behind their autoregressive counterparts in prediction accuracy. The non-autoregressive models with the best accuracy currently rely on multiple decoding iterations, which largely sacrifice the inference speed of non-autoregressive models.  Inspired by the way of learning word dependencies in autoregressive and iterative-decoding models, we propose Glancing Transformer (GLAT) with a glancing language model (GLM), which learns to capture the word dependency gradually. Experiments on three benchmarks demonstrate that our approach can significantly improve the accuracy of non-autoregressive models without multiple decoding iterations. In particular, GLAT achieves state-of-the-art results among non-iterative models and even outperforms top iterative counterparts in some specific benchmarks."}}
{"id": "e5M2iCCAiKR", "cdate": 1577836800000, "mdate": 1634226517786, "content": {"title": "Explicit Semantic Decomposition for Definition Generation", "abstract": "Jiahuan Li, Yu Bao, Shujian Huang, Xinyu Dai, Jiajun Chen. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "YCwkMc0nLF6", "cdate": 1577836800000, "mdate": 1649691787991, "content": {"title": "Explicit Semantic Decomposition for Definition Generation", "abstract": "Jiahuan Li, Yu Bao, Shujian Huang, Xinyu Dai, Jiajun Chen. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "BJe932EYwS", "cdate": 1569438898205, "mdate": null, "content": {"title": "PNAT: Non-autoregressive Transformer by Position Learning", "abstract": "Non-autoregressive generation is a new paradigm for text generation. Previous work hardly considers to explicitly model the positions of generated words. However, position modeling of output words is an essential problem in non-autoregressive text generation. In this paper, we propose PNAT, which explicitly models positions of output words as latent variables in text generation. The proposed PNATis simple yet effective. Experimental results show that PNATgives very promising results in machine translation and paraphrase generation tasks, outperforming many strong baselines."}}
{"id": "KXjsNBXpsex", "cdate": 1546300800000, "mdate": null, "content": {"title": "Non-autoregressive Transformer by Position Learning", "abstract": "Non-autoregressive models are promising on various text generation tasks. Previous work hardly considers to explicitly model the positions of generated words. However, position modeling is an essential problem in non-autoregressive text generation. In this study, we propose PNAT, which incorporates positions as a latent variable into the text generative process. Experimental results show that PNAT achieves top results on machine translation and paraphrase generation tasks, outperforming several strong baselines."}}
