{"id": "G6ljCiI-rvX", "cdate": 1684344672354, "mdate": 1684344672354, "content": {"title": "Personalized One-Shot Lipreading for an ALS Patient", "abstract": "Lipreading or visually recognizing speech from the mouth movements of a speaker is a challenging and mentally taxing task. Unfortunately, multiple medical conditions force people to depend on this skill in their day-to-day lives for essential communication. Patients suffering from \u2018Amyotrophic Lateral Sclerosis\u2019 (ALS) often lose muscle control, consequently their ability to generate speech and communicate via lip movements. Existing large datasets do not focus on medical patients or curate personalized vocabulary relevant to an individual. Collecting large-scale dataset of a patient, needed to train modern data-hungry deep learning models is however, extremely challenging. In this work, we propose a personalized network to lipread an ALS patient using only one-shot examples. We depend on synthetically generated lip movements to augment the one-shot scenario. A Variational Encoder based domain adaptation technique is used to bridge the real-synthetic domain gap. Our approach significantly improves and achieves high top-5 accuracy with 83.2% accuracy compared to 62.6% achieved by comparable methods for the patient. Apart from evaluating our approach on the ALS patient, we also extend it to people with hearing impairment relying extensively on lip movements to communicate."}}
{"id": "hdjfGyAi3pn", "cdate": 1684344555627, "mdate": 1684344555627, "content": {"title": "Towards MOOCs for Lipreading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale", "abstract": "Many people with some form of hearing loss consider lipreading as their primary mode of day-to-day communication. However, finding resources to learn or improve one\u2019s lipreading skills can be challenging. This is further exacerbated in the COVID-19 pandemic due to restrictions on direct interactions with peers and speech therapists. Today, online MOOCs platforms like Coursera and Udemy have become the most effective form of training for many types of skill development. However, online lipreading resources are scarce as creating such resources is an extensive process needing months of manual effort to record hired actors. Because of the manual pipeline, such platforms are also limited in vocabulary, supported languages, accents, and speakers and have a high usage cost. In this work,\nwe investigate the possibility of replacing real human talking videos with synthetically generated videos. Synthetic data can easily incorporate larger vocabularies, variations in accent, and even local languages and many speakers. We propose an end-to-end automated pipeline to develop such a platform using state-of-the-art talking head video generator networks, text-to-speech models, and computer vision techniques. We then perform an extensive human evaluation using carefully thought-out lipreading exercises to validate the quality of our designed platform against the existing lipreading platforms. Our studies concretely point toward the potential of our approach in developing a large-scale lipreading MOOC platform that can impact millions of people with hearing loss.\n"}}
{"id": "EyX7v9JTG2t", "cdate": 1684344386633, "mdate": 1684344386633, "content": {"title": "FaceOff: A Video-to-Video Face Swapping System", "abstract": "Doubles play an indispensable role in the movie industry. They take the place of the actors in dangerous stunt scenes or scenes where the same actor plays multiple characters. The double\u2019s face is later replaced with the actor\u2019s face and expressions manually using expensive CGI technology, costing millions of dollars and taking months to complete. An automated, inexpensive, and fast way can be to use face-swapping techniques that aim to swap an identity from a source face video (or an image) to a target face video. However, such methods cannot preserve the source expressions of the actor important for the scene\u2019s context. To tackle this challenge, we introduce video-to-video (V2V) face-swapping, a novel task of face-swapping that can preserve (1) the identity and expressions of the source (actor) face video and (2) the background and pose of the target (double) video. We propose FaceOff, a V2V face-swapping system that operates by learning a robust blending operation to merge two face videos following the constraints above. It reduces the videos to a quantized latent space and then blends them in the reduced space. FaceOff is trained in a self-supervised manner and robustly tackles the non-trivial challenges of V2V face-swapping. As shown in the experimental section, FaceOff significantly outperforms alternate approaches qualitatively and quantitatively\n"}}
{"id": "jQC-aFTY_Q", "cdate": 1672531200000, "mdate": 1680444515817, "content": {"title": "FaceOff: A Video-to-Video Face Swapping System", "abstract": ""}}
{"id": "bPFh7DCC04", "cdate": 1672531200000, "mdate": 1680444515808, "content": {"title": "Towards MOOCs for Lipreading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale", "abstract": ""}}
{"id": "b2TCH1ib4w", "cdate": 1672531200000, "mdate": 1680444515827, "content": {"title": "Towards Generating Ultra-High Resolution Talking-Face Videos with Lip synchronization", "abstract": ""}}
{"id": "SDgT6OhwiW", "cdate": 1672531200000, "mdate": 1680444515819, "content": {"title": "Audio-Visual Face Reenactment", "abstract": ""}}
{"id": "p3C0wpCES2", "cdate": 1640995200000, "mdate": 1680444515820, "content": {"title": "Compressing Video Calls using Synthetic Talking Heads", "abstract": ""}}
{"id": "g7sl7LNeMr", "cdate": 1640995200000, "mdate": 1668740420688, "content": {"title": "Audio-Visual Face Reenactment", "abstract": "This work proposes a novel method to generate realistic talking head videos using audio and visual streams. We animate a source image by transferring head motion from a driving video using a dense motion field generated using learnable keypoints. We improve the quality of lip sync using audio as an additional input, helping the network to attend to the mouth region. We use additional priors using face segmentation and face mesh to improve the structure of the reconstructed faces. Finally, we improve the visual quality of the generations by incorporating a carefully designed identity-aware generator module. The identity-aware generator takes the source image and the warped motion features as input to generate a high-quality output with fine-grained details. Our method produces state-of-the-art results and generalizes well to unseen faces, languages, and voices. We comprehensively evaluate our approach using multiple metrics and outperforming the current techniques both qualitative and quantitatively. Our work opens up several applications, including enabling low bandwidth video calls. We release a demo video and additional information at http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr."}}
{"id": "fNM8hhF-DdB", "cdate": 1640995200000, "mdate": 1668740420686, "content": {"title": "FaceOff: A Video-to-Video Face Swapping System", "abstract": "Doubles play an indispensable role in the movie industry. They take the place of the actors in dangerous stunt scenes or scenes where the same actor plays multiple characters. The double's face is later replaced with the actor's face and expressions manually using expensive CGI technology, costing millions of dollars and taking months to complete. An automated, inexpensive, and fast way can be to use face-swapping techniques that aim to swap an identity from a source face video (or an image) to a target face video. However, such methods cannot preserve the source expressions of the actor important for the scene's context. To tackle this challenge, we introduce video-to-video (V2V) face-swapping, a novel task of face-swapping that can preserve (1) the identity and expressions of the source (actor) face video and (2) the background and pose of the target (double) video. We propose FaceOff, a V2V face-swapping system that operates by learning a robust blending operation to merge two face videos following the constraints above. It reduces the videos to a quantized latent space and then blends them in the reduced space. FaceOff is trained in a self-supervised manner and robustly tackles the non-trivial challenges of V2V face-swapping. As shown in the experimental section, FaceOff significantly outperforms alternate approaches qualitatively and quantitatively."}}
