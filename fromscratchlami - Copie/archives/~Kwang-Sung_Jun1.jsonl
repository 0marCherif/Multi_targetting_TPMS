{"id": "l5dSSMBUS8t", "cdate": 1672531200000, "mdate": 1681702932404, "content": {"title": "Tighter PAC-Bayes Bounds Through Coin-Betting", "abstract": "We consider the problem of estimating the mean of a sequence of random elements $f(X_1, \\theta)$ $, \\ldots, $ $f(X_n, \\theta)$ where $f$ is a fixed scalar function, $S=(X_1, \\ldots, X_n)$ are independent random variables, and $\\theta$ is a possibly $S$-dependent parameter. An example of such a problem would be to estimate the generalization error of a neural network trained on $n$ examples where $f$ is a loss function. Classically, this problem is approached through concentration inequalities holding uniformly over compact parameter sets of functions $f$, for example as in Rademacher or VC type analysis. However, in many problems, such inequalities often yield numerically vacuous estimates. Recently, the \\emph{PAC-Bayes} framework has been proposed as a better alternative for this class of problems for its ability to often give numerically non-vacuous bounds. In this paper, we show that we can do even better: we show how to refine the proof strategy of the PAC-Bayes bounds and achieve \\emph{even tighter} guarantees. Our approach is based on the \\emph{coin-betting} framework that derives the numerically tightest known time-uniform concentration inequalities from the regret guarantees of online gambling algorithms. In particular, we derive the first PAC-Bayes concentration inequality based on the coin-betting approach that holds simultaneously for all sample sizes. We demonstrate its tightness showing that by \\emph{relaxing} it we obtain a number of previous results in a closed form including Bernoulli-KL and empirical Bernstein inequalities. Finally, we propose an efficient algorithm to numerically calculate confidence sequences from our bound, which often generates nonvacuous confidence bounds even with one sample, unlike the state-of-the-art PAC-Bayes bounds."}}
{"id": "U_YPSEyN2ls", "cdate": 1652737534770, "mdate": null, "content": {"title": "Improved Regret Analysis for Variance-Adaptive Linear Bandits and Horizon-Free Linear Mixture MDPs", "abstract": "  In online learning problems, exploiting low variance plays an important role in obtaining tight performance guarantees yet is challenging because variances are often not known a priori.\n  Recently, considerable progress has been made by Zhang et al. (2021) where they obtain a variance-adaptive regret bound for linear bandits without knowledge of the variances and a horizon-free regret bound for linear mixture Markov decision processes (MDPs).\n  In this paper, we present novel analyses that improve their regret bounds significantly.\n  For linear bandits, we achieve $\\tilde O(\\min\\{d\\sqrt{K}, d^{1.5}\\sqrt{\\sum_{k=1}^K \\sigma_k^2}\\} + d^2)$ where $d$ is the dimension of the features, $K$ is the time horizon, and $\\sigma_k^2$ is the noise variance at time step $k$, and $\\tilde O$ ignores polylogarithmic dependence, which is a factor of $d^3$ improvement.\n  For linear mixture MDPs with the assumption of maximum cumulative reward in an episode being in $[0,1]$, we achieve a horizon-free regret bound of $\\tilde O(d \\sqrt{K} + d^2)$ where $d$ is the number of base models and $K$ is the number of episodes.\n  This is a factor of $d^{3.5}$ improvement in the leading term and $d^7$ in the lower order term.\n  Our analysis critically relies on a novel peeling-based regret analysis that leverages the elliptical potential `count' lemma. "}}
{"id": "GWcdXz0M6a", "cdate": 1652737357903, "mdate": null, "content": {"title": "PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits", "abstract": "In sparse linear bandits, a learning agent sequentially selects an action from a fixed action set and receives reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we devise a simple, novel sparse linear estimation method called $\\textrm{PopArt}$ that enjoys a tighter $\\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996). Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially in terms of the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in prior work.\n"}}
{"id": "mJi7PtxIZ4I", "cdate": 1640995200000, "mdate": 1681702932690, "content": {"title": "Revisiting Simple Regret Minimization in Multi-Armed Bandits", "abstract": "Simple regret is a natural and parameter-free performance criterion for pure exploration in multi-armed bandits yet is less popular than the probability of missing the best arm or an $\\epsilon$-good arm, perhaps due to lack of easy ways to characterize it. In this paper, we make significant progress on minimizing simple regret in both data-rich ($T\\ge n$) and data-poor regime ($T \\le n$) where $n$ is the number of arms, and $T$ is the number of samples. At its heart is our improved instance-dependent analysis of the well-known Sequential Halving (SH) algorithm, where we bound the probability of returning an arm whose mean reward is not within $\\epsilon$ from the best (i.e., not $\\epsilon$-good) for \\textit{any} choice of $\\epsilon>0$, although $\\epsilon$ is not an input to SH. Our bound not only leads to an optimal worst-case simple regret bound of $\\sqrt{n/T}$ up to logarithmic factors but also essentially matches the instance-dependent lower bound for returning an $\\epsilon$-good arm reported by Katz-Samuels and Jamieson (2020). For the more challenging data-poor regime, we propose Bracketing SH (BSH) that enjoys the same improvement even without sampling each arm at least once. Our empirical study shows that BSH outperforms existing methods on real-world tasks."}}
{"id": "a_nxpmDwS6K", "cdate": 1640995200000, "mdate": 1681702932689, "content": {"title": "PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits", "abstract": "In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in prior work."}}
{"id": "SGbW1wIlSj", "cdate": 1640995200000, "mdate": 1681702932301, "content": {"title": "Maillard Sampling: Boltzmann Exploration Done Optimally", "abstract": "The PhD thesis of Maillard (2013) presents a rather obscure algorithm for the $K$-armed bandit problem. This less-known algorithm, which we call Maillard sampling (MS), computes the probability of choosing each arm in a closed form, which is not true for Thompson sampling, a widely-adopted bandit algorithm in the industry. This means that the bandit-logged data from running MS can be readily used for counterfactual evaluation, unlike Thompson sampling. Motivated by such merit, we revisit MS and perform an improved analysis to show that it achieves both the asymptotical optimality and $\\sqrt{KT\\log{T}}$ minimax regret bound where $T$ is the time horizon, which matches the known bounds for asymptotically optimal UCB. We then propose a variant of MS called MS$^+$ that improves its minimax bound to $\\sqrt{KT\\log{K}}$. MS$^+$ can also be tuned to be aggressive (i.e., less exploration) without losing the asymptotic optimality, a unique feature unavailable from existing bandit algorithms. Our numerical evaluation shows the effectiveness of MS$^+$."}}
{"id": "NbaqBoa7tF", "cdate": 1640995200000, "mdate": 1681702932413, "content": {"title": "Norm-Agnostic Linear Bandits", "abstract": "Linear bandits have a wide variety of applications including recommendation systems yet they make one strong assumption: the algorithms must know an upper bound $S$ on the norm of the unknown parameter $\\theta^*$ that governs the reward generation. Such an assumption forces the practitioner to guess $S$ involved in the confidence bound, leaving no choice but to wish that $\\|\\theta^*\\|\\le S$ is true to guarantee that the regret will be low. In this paper, we propose novel algorithms that do not require such knowledge for the first time. Specifically, we propose two algorithms and analyze their regret bounds: one for the changing arm set setting and the other for the fixed arm set setting. Our regret bound for the former shows that the price of not knowing $S$ does not affect the leading term in the regret bound and inflates only the lower order term. For the latter, we do not pay any price in the regret for now knowing $S$. Our numerical experiments show standard algorithms assuming knowledge of $S$ can fail catastrophically when $\\|\\theta^*\\|\\le S$ is not true whereas our algorithms enjoy low regret."}}
{"id": "FoNDJyx8H9", "cdate": 1640995200000, "mdate": 1681702932023, "content": {"title": "An Experimental Design Approach for Regret Minimization in Logistic Bandits", "abstract": "In this work we consider the problem of regret minimization for logistic bandits. The main challenge of logistic bandits is reducing the dependence on a potentially large problem dependent constant that can at worst scale exponentially with the norm of the unknown parameter vector. Previous works have applied self-concordance of the logistic function to remove this worst-case dependence providing regret guarantees that move the reduce the dependence on this worst case parameter to lower order terms with only polylogarithmic dependence on the main term and as well as linear dependence on the dimension of the unknown parameter. This work improves upon the prior art by 1) removing all scaling of the worst case term on the main term and 2) reducing the dependence on the dependence to scale with the square root of dimension in the fixed arm setting by employing an experimental design procedure. Our regret bound in fact takes a tighter instance (i.e., gap) dependent regret bound for the first time in logistic bandits. We also propose a new warmup sampling algorithm that can dramatically reduce the lower order term in the regret in general and prove that it can exponentially reduce the lower order term's dependency on the worst case parameter in some instances. Finally, we discuss the impact of the bias of the MLE on the logistic bandit problem in d dimensions, providing an example where d^2 lower order regret (cf., it is d for linear bandits) may not be improved as long as the MLE is used and how bias-corrected estimators may be used to make it closer to d."}}
{"id": "7ur7GD-Mr2p", "cdate": 1640995200000, "mdate": 1681702932484, "content": {"title": "Jointly Efficient and Optimal Algorithms for Logistic Bandits", "abstract": "Logistic Bandits have recently undergone careful scrutiny by virtue of their combined theoretical and practical relevance. This research effort delivered statistically efficient algorithms, improving the regret of previous strategies by exponentially large factors. Such algorithms are however strikingly costly as they require $\\Omega(t)$ operations at each round. On the other hand, a different line of research focused on computational efficiency ($\\mathcal{O}(1)$ per-round cost), but at the cost of letting go of the aforementioned exponential improvements. Obtaining the best of both world is unfortunately not a matter of marrying both approaches. Instead we introduce a new learning procedure for Logistic Bandits. It yields confidence sets which sufficient statistics can be easily maintained online without sacrificing statistical tightness. Combined with efficient planning mechanisms we design fast algorithms which regret performance still match the problem-dependent lower-bound of Abeille et al (2021). To the best of our knowledge, those are the first Logistic Bandit algorithms that simultaneously enjoy statistical and computational efficiency."}}
{"id": "6aPwpRpzLf", "cdate": 1640995200000, "mdate": 1649229135775, "content": {"title": "Jointly Efficient and Optimal Algorithms for Logistic Bandits", "abstract": "Logistic Bandits have recently undergone careful scrutiny by virtue of their combined theoretical and practical relevance. This research effort delivered statistically efficient algorithms, improving the regret of previous strategies by exponentially large factors. Such algorithms are however strikingly costly as they require $\\Omega(t)$ operations at each round. On the other hand, a different line of research focused on computational efficiency ($\\mathcal{O}(1)$ per-round cost), but at the cost of letting go of the aforementioned exponential improvements. Obtaining the best of both world is unfortunately not a matter of marrying both approaches. Instead we introduce a new learning procedure for Logistic Bandits. It yields confidence sets which sufficient statistics can be easily maintained online without sacrificing statistical tightness. Combined with efficient planning mechanisms we design fast algorithms which regret performance still match the problem-dependent lower-bound of Abeille et al. (2021). To the best of our knowledge, those are the first Logistic Bandit algorithms that simultaneously enjoy statistical and computational efficiency."}}
