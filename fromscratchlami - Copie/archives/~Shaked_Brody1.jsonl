{"id": "kG7UavvPoH", "cdate": 1672531200000, "mdate": 1684559878390, "content": {"title": "On the Expressivity Role of LayerNorm in Transformers' Attention", "abstract": "Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\\left[1,1,...,1\\right]$ vector, and (b) scaling of all vectors to the same norm of $\\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest attention, and prevents keys from being \"un-select-able\". We show empirically that Transformers do indeed benefit from these properties of LayeNorm in general language modeling and even in computing simple functions such as \"majority\". Our code is available at https://github.com/tech-srl/layer_norm_expressivity_role ."}}
{"id": "Ml0MyBWxAtg", "cdate": 1640995200000, "mdate": 1681649743774, "content": {"title": "How Attentive are Graph Attention Networks?", "abstract": ""}}
{"id": "F72ximsx7C1", "cdate": 1632875712151, "mdate": null, "content": {"title": "How Attentive are Graph Attention Networks? ", "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query.\nHowever, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention.\nBecause GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. \nTo remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 12 OGB and other benchmarks while we match their parametric costs. \nOur code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library."}}
{"id": "mSA-yx6Mt_v", "cdate": 1609459200000, "mdate": 1681649743762, "content": {"title": "How Attentive are Graph Attention Networks?", "abstract": ""}}
{"id": "EbIOVWkMFEE", "cdate": 1577836800000, "mdate": 1680327597282, "content": {"title": "A structural model for contextual code changes", "abstract": ""}}
{"id": "sAiw6Tl7iY", "cdate": 1546300800000, "mdate": 1635479309025, "content": {"title": "code2seq: Generating Sequences from Structured Representations of Code", "abstract": "The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq."}}
{"id": "H1gKYo09tX", "cdate": 1538087809063, "mdate": null, "content": {"title": "code2seq: Generating Sequences from Structured Representations of Code", "abstract": "The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.\nWe demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq."}}
