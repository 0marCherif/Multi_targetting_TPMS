{"id": "7lM8KGqdsEF", "cdate": 1698852216986, "mdate": 1698852216986, "content": {"title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting", "abstract": "Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL's modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest computational resources and public datasets."}}
{"id": "zc0aOQTwcs7", "cdate": 1661442003403, "mdate": 1661442003403, "content": {"title": "Seasonal Contrast: Unsupervised Pre-Training From Uncurated Remote Sensing Data", "abstract": "Remote sensing and automatic earth monitoring are key\nto solve global-scale challenges such as disaster prevention,\nland use monitoring, or tackling climate change. Although\nthere exist vast amounts of remote sensing data, most of\nit remains unlabeled and thus inaccessible for supervised\nlearning algorithms. Transfer learning approaches can reduce the data requirements of deep learning algorithms.\nHowever, most of these methods are pre-trained on ImageNet and their generalization to remote sensing imagery\nis not guaranteed due to the domain gap. In this work, we\npropose Seasonal Contrast (SeCo), an effective pipeline to\nleverage unlabeled data for in-domain pre-training of remote sensing representations. The SeCo pipeline is composed of two parts. First, a principled procedure to\ngather large-scale, unlabeled and uncurated remote sensing\ndatasets containing images from multiple Earth locations at\ndifferent timestamps. Second, a self-supervised algorithm\nthat takes advantage of time and position invariance to\nlearn transferable representations for remote sensing applications. We empirically show that models trained with SeCo\nachieve better performance than their ImageNet pre-trained\ncounterparts and state-of-the-art self-supervised learning\nmethods on multiple downstream tasks. The datasets and\nmodels in SeCo will be made public to facilitate transfer\nlearning and enable rapid progress in remote sensing applications."}}
