{"id": "O1VTD8FPWbc", "cdate": 1667746469184, "mdate": 1667746469184, "content": {"title": "Self-Supervised Learning for Large-Scale Unsupervised Image Clustering", "abstract": "Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at this https URL"}}
{"id": "Tb2an3tVJ2", "cdate": 1667746384014, "mdate": 1667746384014, "content": {"title": "End-to-End Referring Video Object Segmentation With Multimodal Transformers ", "abstract": "The referring video object segmentation task (RVOS) involves segmentation of a text-referred object instance in the frames of a given video. Due to the complex nature of this multimodal task, which combines text reasoning, video understanding, instance segmentation and tracking, existing approaches typically rely on sophisticated pipelines in order to tackle it. In this paper, we propose a simple Transformer-based approach to RVOS. Our framework, termed Multimodal Tracking Transformer (MTTR), models the RVOS task as a sequence prediction problem. Following recent advancements in computer vision and natural language processing, MTTR is based on the realization that video and text can be processed together effectively and elegantly by a single multimodal Transformer model. MTTR is end-to-end trainable, free of text-related inductive bias components and requires no additional mask-refinement post-processing steps. As such, it simplifies the RVOS pipeline considerably compared to existing methods. Evaluation on standard benchmarks reveals that MTTR significantly outperforms previous art across multiple metrics. In particular, MTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and JHMDB-Sentences datasets respectively, while processing 76 frames per second. In addition, we report strong results on the public validation set of Refer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the attention of researchers. The code to reproduce our experiments is available at https://github.com/mttr2021/MTTR "}}
{"id": "TuHkVOjSAR", "cdate": 1663849919606, "mdate": null, "content": {"title": "Strategic Classification with Graph Neural Networks", "abstract": "Strategic classification studies learning in settings where users can modify their features to obtain favorable predictions. Most current works focus on simple classifiers that trigger independent user responses. Here we examine the implications of learning with more elaborate models that break the independence assumption. Motivated by the idea that applications of strategic classification are often social in nature, we focus on graph neural networks, which make use of social relations between users to improve predictions. Using a graph for learning introduces inter-user dependencies in prediction; our key point is that strategic users can exploit these to promote their goals. As we show through analysis and simulation, this can work either against the system---or for it. Based on this, we propose a differentiable framework for strategically-robust learning of graph-based classifiers. Experiments on several real networked datasets demonstrate the utility of our approach."}}
{"id": "BcU49V-1pec", "cdate": 1646223666247, "mdate": null, "content": {"title": "ON RECOVERABILITY OF GRAPH NEURAL NETWORK REPRESENTATIONS", "abstract": "Despite their growing popularity, graph neural networks (GNNs) still have multiple unsolved problems, \nincluding finding more expressive aggregation methods, propagation of information to distant nodes, and training on large-scale graphs.\nUnderstanding and solving such problems require developing analytic tools and techniques.\nIn this work, we propose the notion of \\textit{recoverability}, which is tightly related to information aggregation in GNNs,\nand based on this concept, develop the method for GNN embedding analysis.\nThrough extensive experimental results on various datasets and different GNN architectures, \nwe demonstrate that estimated recoverability correlates with aggregation method expressivity and graph sparsification quality.\nThe code to reproduce our experiments is available at \\url{https://github.com/Anonymous1252022/Recoverability}."}}
{"id": "H9IG5Eby6lc", "cdate": 1646223666112, "mdate": null, "content": {"title": "WEISFEILER AND LEMAN GO INFINITE: SPECTRAL AND COMBINATORIAL PRE-COLORINGS", "abstract": " Two popular alternatives for graph isomorphism testing that offer a good trade-off between expressive power and computational efficiency are combinatorial (i.e., obtained via the Weisfeiler-Leman (WL) test) and spectral invariants. While the exact power of the latter is still an open question, the former is regularly criticized for its limited power, when a standard configuration of uniform pre-coloring is used. This drawback hinders the applicability of Message Passing Graph Neural Networks (MPGNNs), whose expressive power is upper bounded by the WL test. Relaxing the assumption of uniform pre-coloring, we show that one can increase the expressive power of the WL test ad infinitum. Following that, we propose an efficient pre-coloring based on spectral features that provably increase the expressive power of the vanilla WL test.\nThe code to reproduce our experiments is available at \\url{https://github.com/TPFI22/Spectral-and-Combinatorial}"}}
{"id": "GdAzRedTV7J", "cdate": 1635261618301, "mdate": null, "content": {"title": "Weakly Supervised Discovery of Semantic Attributes", "abstract": "We consider the problem of extracting semantic attributes, using only classification labels for supervision. For example, when learning to classify images of birds into species, we would like to observe the emergence of features used by zoologists to classify birds. To tackle this problem, we propose training a neural network with discrete features in the last layer, followed by two heads: a multi-layered perceptron (MLP) and a decision tree. The decision tree utilizes simple binary decision stumps, thus encouraging features to have semantic meaning. We present theoretical analysis, as well as a practical method for learning in the intersection of two hypothesis classes. Compared with various benchmarks, our results show an improved ability to extract a set of features highly correlated with a ground truth set of unseen attributes."}}
{"id": "bZwPTADCCzG", "cdate": 1620964501506, "mdate": null, "content": {"title": "Single-Node Attack for Fooling Graph Neural Networks", "abstract": "Graph neural networks (GNNs) have shown broad applicability in a variety of domains. Some of these domains, such as social networks and product recommendations, are fertile ground for malicious users and behavior. In this paper, we show that GNNs are vulnerable to the extremely limited scenario of a single-node adversarial example, where the node cannot be picked by the attacker. That is, an attacker can force the GNN to classify any target node to a chosen label by only slightly perturbing another single arbitrary node in the graph, even when not being able to pick that specific attacker node. When the adversary is allowed to pick a specific attacker node, the attack is even more effective. We show that this attack is effective across various GNN types, such as GraphSAGE, GCN, GAT, and GIN, across a variety of real-world datasets, and as a targeted and a non-targeted attack. Our code is available at this https URL ."}}
{"id": "uB5x7Y2qsFR", "cdate": 1601308054305, "mdate": null, "content": {"title": "Contrast to Divide: self-supervised pre-training for learning with noisy labels", "abstract": "Advances in semi-supervised methods for image classification  significantly boosted performance in the learning with noisy labels (LNL) task. Specifically, by discarding the erroneous labels (and keeping the samples), the LNL task becomes a semi-supervised one for which powerful tools exist.  Identifying the noisy samples, however, heavily relies on the success of a warm-up stage where standard supervised training is performed using the full (noisy) training set. This stage is sensitive not only to the noise level but also to the choice of hyperparameters. \nIn this paper, we propose to solve this problem by utilizing self-supervised pre-training. Our approach, which we name Contrast to Divide,  offers several important advantages. First, by removing the labels altogether, our pre-trained features become agnostic to the labels' amount of noise, allowing accurate noisy separation even under high noise levels. Second, as recently shown, semi-supervised methods significantly benefit from self-supervised pre-training. Moreover, compared with standard pre-training approaches (e.g., supervised training on ImageNet), self-supervised pre-training does not suffer from a domain gap.\nWe demonstrate the effectiveness of the proposed method in various settings with both synthetic and real noise. Our results indicate that Contrast to Divide brings a new state-of-the-art by a significant margin to both CIFAR-10 and CIFAR-100. For example, in the high-noise regime of 90%, we get a boost of more than 27% for CIFAR-100 and more than 17% for CIFAR-10 over the previous state-of-the-art. Moreover, we achieve comparable performance on Clothing-1M without using ImageNet pre-training. Code for reproducing our experiments is available at https://github.com/ContrastToDivide/C2D"}}
{"id": "u4WfreuXxnk", "cdate": 1601308054189, "mdate": null, "content": {"title": "Single-Node Attack for Fooling Graph Neural Networks", "abstract": "Graph neural networks (GNNs) have shown broad applicability in a variety of domains.\nSome of these domains, such as social networks and product recommendations, are fertile ground for malicious users and behavior.\nIn this paper, we show that GNNs are vulnerable to the extremely limited scenario of a single-node adversarial example, where the node cannot be picked by the attacker. \nThat is, an attacker can force the GNN to classify any target node to a chosen label by only slightly perturbing another single arbitrary node in the graph, even when not being able to pick that specific attacker node. When the adversary is allowed to pick a specific attacker node, the attack is even more effective. \nWe show that this attack is effective across various GNN types (e.g., GraphSAGE, GCN, GAT, and GIN), across a variety of real-world datasets, and as a targeted and non-targeted attack.\nOur code is available anonymously at https://github.com/gnnattack/SINGLE ."}}
{"id": "B1g6V5DF2H", "cdate": 1574693428535, "mdate": null, "content": {"title": "Beholder-Gan: Generation and Beautification of Facial Images with Conditioning on Their Beauty Level", "abstract": "Beauty is in the eye of the beholder.\u201d This maxim, emphasizing the subjectivity of the perception of beauty, has enjoyed a wide consensus since ancient times. In the digital\nera, data-driven methods have been shown to be able to predict human-assigned beauty scores for facial images. In this\nwork, we augment this ability and train a generative model\nthat generates faces conditioned on a requested beauty score.\nIn addition, we show how this trained generator can be used\nto \u201dbeautify\u201d an input face image. By doing so, we achieve\nan unsupervised beautification model, in the sense that it relies on no ground truth target images. Our implementation is\navailable on: https://github.com/beholdergan/Beholder-GAN."}}
