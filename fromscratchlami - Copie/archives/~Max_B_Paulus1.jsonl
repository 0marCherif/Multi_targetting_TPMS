{"id": "yq99nnlz5L", "cdate": 1674000729443, "mdate": 1674000729443, "content": {"title": "Augment with Care: Contrastive Learning for Combinatorial Problems", "abstract": "Supervised learning can improve the design of state-of-the-art solvers for combinatorial problems, but labelling large numbers of combinatorial instances is often impractical due to exponential worst-case complexity. Inspired by the recent success of contrastive pre-training for images, we conduct a scientific study of the effect of augmentation design on contrastive pre-training for the Boolean satisfiability problem. While typical graph contrastive pre-training uses label-agnostic augmentations, our key insight is that many combinatorial problems have well-studied invariances, which allow for the design of label-preserving augmentations. We find that label-preserving augmentations are critical for the success of contrastive pre-training. We show that our representations are able to achieve comparable test accuracy to fully-supervised learning while using only 1% of the labels. We also demonstrate that our representations are more transferable to larger problems from unseen domains."}}
{"id": "aQ9HLHO10j", "cdate": 1672531200000, "mdate": 1680779521475, "content": {"title": "Learning To Dive In Branch And Bound", "abstract": ""}}
{"id": "KL3Yis4Yuee", "cdate": 1672531200000, "mdate": 1680779521560, "content": {"title": "A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning", "abstract": ""}}
{"id": "wmdbwZz65FM", "cdate": 1652737510255, "mdate": null, "content": {"title": "Learning to Drop Out: An Adversarial Approach to Training Sequence VAEs", "abstract": "In principle, applying variational autoencoders (VAEs) to sequential data offers a method for controlled sequence generation, manipulation, and structured representation learning. However, training sequence VAEs is challenging: autoregressive decoders can often explain the data without utilizing the latent space, known as posterior collapse. To mitigate this, state-of-the-art models `weaken' the `powerful decoder' by applying uniformly random dropout to the decoder input.\nWe show theoretically that this removes pointwise mutual information provided by the decoder input, which is compensated for by utilizing the latent space. We then propose an adversarial training strategy to achieve information-based stochastic dropout. Compared to uniform dropout on standard text benchmark datasets, our targeted approach increases both sequence modeling performance and the information captured in the latent space."}}
{"id": "S7uHI51lDrh", "cdate": 1640995200000, "mdate": 1680779521492, "content": {"title": "Instance-wise algorithm configuration with graph neural networks", "abstract": ""}}
{"id": "O1MM_fWQ33O", "cdate": 1640995200000, "mdate": 1680779521484, "content": {"title": "Learning to Cut by Looking Ahead: Cutting Plane Selection via Imitation Learning", "abstract": ""}}
{"id": "yrgQ5RJ5b50", "cdate": 1618018033278, "mdate": null, "content": {"title": "Gradient Estimation with Stochastic Softmax Tricks", "abstract": "The Gumbel-Max trick is the basis of many relaxed gradient estimators. These estimators are easy to implement and low variance, but the goal of scaling them comprehensively to large combinatorial distributions is still outstanding. Working within the perturbation model framework, we introduce stochastic softmax tricks, which generalize the Gumbel-Softmax trick to combinatorial spaces. Our framework is a unified perspective on existing relaxed estimators for perturbation models, and it contains many novel relaxations. We design structured relaxations for subset selection, spanning trees, arborescences, and others. When compared to less structured baselines, we find that stochastic softmax tricks can be used to train latent variable models that perform better and discover more latent structure."}}
{"id": "Mk6PZtgAgfq", "cdate": 1601308130899, "mdate": null, "content": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models."}}
