{"id": "ai_FCyj6b0y", "cdate": 1672531200000, "mdate": 1680787603904, "content": {"title": "Adversarial agent-learning for cybersecurity: a comparison of algorithms", "abstract": ""}}
{"id": "V1ftkjVP2zU", "cdate": 1672531200000, "mdate": 1680787604363, "content": {"title": "Investigating Student's Problem-solving Approaches in MOOCs using Natural Language Processing", "abstract": ""}}
{"id": "AqexjBWRQFx", "cdate": 1652737603136, "mdate": null, "content": {"title": "Convergent Representations of Computer Programs in Human and Artificial Neural Networks", "abstract": "What aspects of computer programs are represented by the human brain during comprehension? We leverage brain recordings derived from functional magnetic resonance imaging (fMRI) studies of programmers comprehending Python code to evaluate the properties and code-related information encoded in the neural signal. We first evaluate a selection of static and dynamic code properties, such as abstract syntax tree (AST)-related and runtime-related metrics. Then, to learn whether brain representations encode fine-grained information about computer programs, we train a probe to align brain recordings with representations learned by a suite of ML models. We find that both the Multiple Demand and Language systems--brain systems which are responsible for very different cognitive tasks, encode specific code properties and uniquely align with machine learned representations of code. These findings suggest at least two distinct neural mechanisms mediating computer program comprehension and evaluation, prompting the design of code model objectives that go beyond static language modeling.\nWe make all the corresponding code, data, and analysis publicly available at https://github.com/ALFA-group/code-representations-ml-brain"}}
{"id": "c2vzOkvycnm", "cdate": 1652714406833, "mdate": 1652714406833, "content": {"title": "On the application of Danskin\u2019s theorem to derivative-free minimax problems", "abstract": "Motivated by Danskin\u2019s theorem, gradient-based methods have been applied with empirical success to solve minimax problems that involve non-convex outer minimization and non-concave inner maximization. On the other hand, recent work has demonstrated that Evolution Strategies (ES) algorithms are stochastic gradient approximators that seek robust solutions. In this paper, we address black-box (gradient-free) minimax problems that have long been tackled in a coevolutionary setup. To this end and guaranteed by Danskin\u2019s theorem, we employ ES as a stochastic estimator for descent directions. The proposed approach is validated on a collection of black-box minimax problems. Based on our experiments, our method\u2019s performance is comparable with its coevolutionary counterparts and favorable for high-dimensional problems. Its efficacy is demonstrated on a real-world application."}}
{"id": "eOfbrqmlYL0", "cdate": 1652714268537, "mdate": 1652714268537, "content": {"title": "Dependency-Based Neural Representations for Classifying Lines of Programs", "abstract": "We investigate the problem of classifying a line of program as containing a vulnerability or not using machine learning. Such a line-level classification task calls for a program representation which goes beyond reasoning from the tokens present in the line. We seek a distributed representation in a latent feature space which can capture the control and data dependencies of tokens appearing on a line of program, while also ensuring lines of similar meaning have similar features. We present a neural architecture, Vulcan, that successfully demonstrates both these requirements. It extracts contextual information about tokens in a line and inputs them as Abstract Syntax Tree (AST) paths to a bi-directional LSTM with an attention mechanism. It concurrently represents the meanings of tokens in a line by recursively embedding the lines where they are most recently defined. In our experiments, Vulcan compares favorably with a state-of-the-art classifier, which requires significant preprocessing of programs, suggesting the utility of using deep learning to model program dependence information."}}
{"id": "uqgXyyUO9Db", "cdate": 1652713619590, "mdate": 1652713619590, "content": {"title": "Generating Adversarial Computer Programs using Optimized Obfuscations", "abstract": "Machine learning (ML) models that learn and predict properties of computer programs are increasingly being adopted and deployed. \nThese models have demonstrated success in applications such as auto-completing code, summarizing large programs, and detecting bugs and malware in programs. \nIn this work, we investigate principled ways to adversarially perturb a computer program to fool such learned models, and thus determine their adversarial robustness. We use program obfuscations, which have conventionally been used to avoid attempts at reverse engineering programs, as adversarial perturbations. These perturbations modify programs in ways that do not alter their functionality but can be crafted to deceive an ML model when making a decision. We provide a general formulation for an adversarial program that allows applying multiple obfuscation transformations to a program in any language. We develop first-order optimization algorithms to  efficiently determine two key aspects -- which parts of the program to transform, and what transformations to use. We show that it is important to optimize both these aspects to generate the best adversarially perturbed program. Due to the discrete nature of this problem, we also propose using randomized smoothing to improve the attack loss landscape to ease optimization. \nWe evaluate our work on Python and Java programs on the problem of program summarization. \nWe show that our best attack proposal achieves a  improvement over a state-of-the-art attack generation approach for programs trained on a \\textsc{seq2seq} model.\nWe further show that our formulation is better at training models that are robust to adversarial attacks."}}
{"id": "tk_TYTrko01", "cdate": 1640995200000, "mdate": 1680787604187, "content": {"title": "Coevolutionary generative adversarial networks for medical image augumentation at scale", "abstract": ""}}
{"id": "oPWl-NeVENn", "cdate": 1640995200000, "mdate": 1680787604362, "content": {"title": "Synthesizing Programs from Program Pieces Using Genetic Programming and Refinement Type Checking", "abstract": ""}}
{"id": "e74qNBL46aE", "cdate": 1640995200000, "mdate": 1680787604199, "content": {"title": "Entropy-Based Characterization of Influence Pathways in Traditional and Social Media", "abstract": ""}}
{"id": "WLgv3luPYry", "cdate": 1640995200000, "mdate": 1680787604345, "content": {"title": "Analyzing multi-agent reinforcement learning and coevolution in cybersecurity", "abstract": ""}}
