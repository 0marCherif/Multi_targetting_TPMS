{"id": "BkL0z2dxn7g", "cdate": 1683893004696, "mdate": 1683893004696, "content": {"title": "Neural Network Information Leakage through Hidden Learning", "abstract": "We investigate the problem of making an artificial neural network perform hidden computations whose result can be easily retrieved from the network's output. In particular, we consider the following scenario. A user is provided a neural network for a classification task by a third party. The user's input to the network contains sensitive information and the third party can only observe the output of the network. I this work, we provide a simple and efficient training procedure, which we call hidden learning, that produces two networks: (i) one that solves the original classification task with performance near to state of the art; (ii) a second one that takes as input the output of the first, retrieving sensitive information to solve a second classification task with good accuracy. Our result might expose important issues from an information security point of view, as for the use of artificial neural networks in sensible applications."}}
{"id": "fcbi50T8A", "cdate": 1683890587061, "mdate": 1683890587061, "content": {"title": "Revisiting the Random Subset Sum problem", "abstract": "The average properties of the well-known Subset Sum Problem can be studied by the means of its randomised version, where we are given a target value $z$, random variables $X_1, \\ldots, X_n$, and an error parameter $\\varepsilon > 0$, and we seek a subset of the $X_i$'s whose sum approximates $z$ up to error $\\varepsilon$.\nIn this setup, it has been shown that, under mild assumptions on the distribution of the random variables, a sample of size $\\mathcal{O}\\left(\\log (1/\\varepsilon)\\right)$ suffices to obtain, with high probability, approximations for all values in $[-1/2, 1/2]$. Recently, this result has been rediscovered outside the algorithms community, enabling meaningful progress in other fields. In this work we present an alternative proof for this theorem, with a more direct approach and resourcing to more elementary tools, in the hope of disseminating it even further."}}
{"id": "kSLnzafzQvm", "cdate": 1640995200000, "mdate": 1681825322469, "content": {"title": "On the Multidimensional Random Subset Sum Problem", "abstract": "In the Random Subset Sum Problem, given $n$ i.i.d. random variables $X_1, ..., X_n$, we wish to approximate any point $z \\in [-1,1]$ as the sum of a suitable subset $X_{i_1(z)}, ..., X_{i_s(z)}$ of them, up to error $\\varepsilon$. Despite its simple statement, this problem is of fundamental interest to both theoretical computer science and statistical mechanics. More recently, it gained renewed attention for its implications in the theory of Artificial Neural Networks. An obvious multidimensional generalisation of the problem is to consider $n$ i.i.d. $d$-dimensional random vectors, with the objective of approximating every point $\\mathbf{z} \\in [-1,1]^d$. In 1998, G. S. Lueker showed that, in the one-dimensional setting, $n=\\mathcal{O}(\\log \\frac 1\\varepsilon)$ samples guarantee the approximation property with high probability.In this work, we prove that, in $d$ dimensions, $n = \\mathcal{O}(d^3\\log \\frac 1\\varepsilon \\cdot (\\log \\frac 1\\varepsilon + \\log d))$ samples suffice for the approximation property to hold with high probability. As an application highlighting the potential interest of this result, we prove that a recently proposed neural network model exhibits universality: with high probability, the model can approximate any neural network within a polynomial overhead in the number of parameters."}}
{"id": "kLEQbAF_Rc", "cdate": 1640995200000, "mdate": 1681825322396, "content": {"title": "Phase transition of a nonlinear opinion dynamics with noisy interactions", "abstract": "In several real Multi-Agent Systems, it has been observed that only weaker forms of metastable consensus are achieved, in which a large majority of agents agree on some opinion while other opinions continue to be supported by a (small) minority of agents. In this work, we take a step towards the investigation of metastable consensus for complex (nonlinear) opinion dynamics by considering the popular Undecided-State dynamics in the binary setting, which is known to reach consensus exponentially faster than the Voter dynamics. We propose a simple form of uniform noise in which each message can change to another one with probability p and we prove that the persistence of a metastable consensus undergoes a phase transition for $$p=\\frac{1}{6}$$ p = 1 6 . In detail, below this threshold, we prove the system reaches with high probability a metastable regime where a large majority of agents keeps supporting the same opinion for polynomial time. Moreover, this opinion turns out to be the initial majority opinion, whenever the initial bias is slightly larger than its standard deviation. On the contrary, above the threshold, we show that the information about the initial majority opinion is \u201clost\u201d within logarithmic time even when the initial bias is maximum. Interestingly, we show our results have explicit connections to two different concrete frameworks. The first one concerns a specific setting of a well-studied value-sensitive decision mechanism inspired by cross-inhibition in house-hunting honeybee swarms. The second framework consists of a consensus process where a subset of agents behave in a stubborn way."}}
{"id": "K1pucj-OLvy", "cdate": 1640995200000, "mdate": 1681825322443, "content": {"title": "Planning with Biological Neurons and Synapses", "abstract": "We revisit the planning problem in the blocks world, and we implement a known heuristic for this task. Importantly, our implementation is biologically plausible, in the sense that it is carried out exclusively through the spiking of neurons. Even though much has been accomplished in the blocks world over the past five decades, we believe that this is the first algorithm of its kind. The input is a sequence of symbols encoding an initial set of block stacks as well as a target set, and the output is a sequence of motion commands such as \"put the top block in stack 1 on the table\". The program is written in the Assembly Calculus, a recently proposed computational framework meant to model computation in the brain by bridging the gap between neural activity and cognitive function. Its elementary objects are assemblies of neurons (stable sets of neurons whose simultaneous firing signifies that the subject is thinking of an object, concept, word, etc.), its commands include project and merge, and its execution model is based on widely accepted tenets of neuroscience. A program in this framework essentially sets up a dynamical system of neurons and synapses that eventually, with high probability, accomplishes the task. The purpose of this work is to establish empirically that reasonably large programs in the Assembly Calculus can execute correctly and reliably; and that rather realistic --- if idealized --- higher cognitive functions, such as planning in the blocks world, can be implemented successfully by such programs."}}
{"id": "57hY-IQtiF", "cdate": 1640995200000, "mdate": 1681825322475, "content": {"title": "Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks", "abstract": "The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs). In this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor."}}
{"id": "Vjki79-619-", "cdate": 1632875662178, "mdate": null, "content": {"title": "Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks", "abstract": "The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs).\nIn this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor."}}
{"id": "oEm9d0jpny1", "cdate": 1609459200000, "mdate": 1681825322524, "content": {"title": "Search via Parallel L\u00e9vy Walks on Z2", "abstract": "Motivated by the L\u00e9vy foraging hypothesis -- the premise that various animal species have adapted to follow L\u00e9vy walks to optimize their search efficiency -- we study the parallel hitting time of L\u00e9vy walks on the infinite two-dimensional grid. We consider k independent discrete-time L\u00e9vy walks, with the same exponent \u03b1 \u2208(1,\u221e), that start from the same node, and analyze the number of steps until the first walk visits a given target at distance \u2113. % We show that for any choice of k and \u2113 from a large range, there is a unique optimal exponent \u03b1_k,\u2208 (2,3), for which the hitting time is \u00d5(\u21132/k) w.h.p., while modifying the exponent by any constant term \u03b5>0 increases the hitting time by a factor polynomial in \u2113, or the walks fail to hit the target almost surely. % Based on that, we propose a surprisingly simple and effective parallel search strategy, for the setting where k and \u2113 are unknown: The exponent of each L\u00e9vy walk is just chosen independently and uniformly at random from the interval (2,3). This strategy achieves optimal search time (modulo polylogarithmic factors) among all possible algorithms (even centralized ones that know k). % Our results should be contrasted with a line of previous work showing that the exponent \u03b1 = 2 is optimal for various search problems. In our setting of k parallel walks, we show that the optimal exponent depends on k and \u2113, and that randomizing the choice of the exponents works simultaneously for all k and \u2113."}}
{"id": "hYT0QYs1lZ7", "cdate": 1609459200000, "mdate": 1681825322514, "content": {"title": "Parallel Load Balancing on constrained client-server topologies", "abstract": ""}}
{"id": "NIVFubA8Dp", "cdate": 1609459200000, "mdate": 1681825322517, "content": {"title": "Planning with Biological Neurons and Synapses", "abstract": "We revisit the planning problem in the blocks world, and we implement a known heuristic for this task. Importantly, our implementation is biologically plausible, in the sense that it is carried out exclusively through the spiking of neurons. Even though much has been accomplished in the blocks world over the past five decades, we believe that this is the first algorithm of its kind. The input is a sequence of symbols encoding an initial set of block stacks as well as a target set, and the output is a sequence of motion commands such as \"put the top block in stack 1 on the table\". The program is written in the Assembly Calculus, a recently proposed computational framework meant to model computation in the brain by bridging the gap between neural activity and cognitive function. Its elementary objects are assemblies of neurons (stable sets of neurons whose simultaneous firing signifies that the subject is thinking of an object, concept, word, etc.), its commands include project and merge, and its execution model is based on widely accepted tenets of neuroscience. A program in this framework essentially sets up a dynamical system of neurons and synapses that eventually, with high probability, accomplishes the task. The purpose of this work is to establish empirically that reasonably large programs in the Assembly Calculus can execute correctly and reliably; and that rather realistic -- if idealized -- higher cognitive functions, such as planning in the blocks world, can be implemented successfully by such programs."}}
