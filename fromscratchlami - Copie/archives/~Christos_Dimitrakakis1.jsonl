{"id": "r0gxrLIoqgc", "cdate": 1646077517238, "mdate": null, "content": {"title": "SENTINEL: Taming Uncertainty with Ensemble based Distributional Reinforcement Learning", "abstract": "    In this paper, we consider risk-sensitive sequential decision-making in Reinforcement Learning (RL). \n    Our contributions are two-fold. First, we introduce a novel and \\emph{coherent} quantification of risk, namely \\emph{composite risk}, which quantifies the joint effect of aleatory and epistemic risk during the learning process.\n    Existing works considered either aleatory or epistemic risk individually, or as an additive combination.\n    We prove that the additive formulation is a particular case of the composite risk when the epistemic risk measure is replaced with expectation.\n    Thus, the composite risk is more sensitive to both aleatory and epistemic uncertainty than the individual and additive formulations.\n    We also propose an algorithm, SENTINEL-K, based on ensemble bootstrapping and distributional RL for representing epistemic and aleatory uncertainty respectively. The ensemble of K learners uses Follow The Regularised Leader (FTRL) to aggregate the return distributions and obtain the composite risk.\n    We experimentally verify that SENTINEL-K estimates the return distribution better, and while used with composite risk estimates, demonstrates higher risk-sensitive performance than state-of-the-art risk-sensitive and distributional RL algorithms."}}
{"id": "LcLb5T6a4IG", "cdate": 1617702477490, "mdate": null, "content": {"title": "Fair Set Selection: Meritocracy and Social Welfare", "abstract": "In  this  paper,  we  formulate  the  problem  of  selecting  a  set  of  individuals  from  a  candidate population as a utility maximisation problem.  From the decision maker\u2019s perspective, it is equivalent to finding a selection policy that maximises expected utility.  Our framework leads to the notion of expected marginal contribution (EMC) of an individual with respect to a selection policy as a measure of deviation from meritocracy.  In order to solve the maximisation problem, we propose to use a policy gradient algorithm.  For certain policy structures, the policy gradients are proportional to EMCs of individuals.  Consequently, the policy gradient algorithm leads to a locally optimal solution that has zero EMC, and satisfies meritocracy.  For uniform policies, EMC reduces to the Shapley  value.   EMC  also  generalises  the  fair  selection  properties  of  Shapley  value  for  general selection policies.  We experimentally analyse the effect of different policy structures in a simulated college admission setting and compare with ranking and greedy algorithms.  Our results verify that separable linear policies achieve high utility while minimising EMCs.  We also show that we can design utility functions that successfully promote notions of group fairness, such as diversity"}}
{"id": "hpzXcsWeSXN", "cdate": 1609459200000, "mdate": 1634362363946, "content": {"title": "High-dimensional near-optimal experiment design for drug discovery via Bayesian sparse sampling", "abstract": "We study the problem of performing automated experiment design for drug screening through Bayesian inference and optimisation. In particular, we compare and contrast the behaviour of linear-Gaussian models and Gaussian processes, when used in conjunction with upper confidence bound algorithms, Thompson sampling, or bounded horizon tree search. We show that non-myopic sophisticated exploration techniques using sparse tree search have a distinct advantage over methods such as Thompson sampling or upper confidence bounds in this setting. We demonstrate the significant superiority of the approach over existing and synthetic datasets of drug toxicity."}}
{"id": "dWv6S9tvDDi", "cdate": 1609459200000, "mdate": null, "content": {"title": "SENTINEL: Taming Uncertainty with Ensemble-based Distributional Reinforcement Learning", "abstract": "In this paper, we consider risk-sensitive sequential decision-making in Reinforcement Learning (RL). Our contributions are two-fold. First, we introduce a novel and coherent quantification of risk, namely composite risk, which quantifies the joint effect of aleatory and epistemic risk during the learning process. Existing works considered either aleatory or epistemic risk individually, or as an additive combination. We prove that the additive formulation is a particular case of the composite risk when the epistemic risk measure is replaced with expectation. Thus, the composite risk is more sensitive to both aleatory and epistemic uncertainty than the individual and additive formulations. We also propose an algorithm, SENTINEL-K, based on ensemble bootstrapping and distributional RL for representing epistemic and aleatory uncertainty respectively. The ensemble of K learners uses Follow The Regularised Leader (FTRL) to aggregate the return distributions and obtain the composite risk. We experimentally verify that SENTINEL-K estimates the return distribution better, and while used with composite risk estimates, demonstrates higher risk-sensitive performance than state-of-the-art risk-sensitive and distributional RL algorithms."}}
{"id": "LjY29Exz8Jh", "cdate": 1609459200000, "mdate": 1634362363946, "content": {"title": "Adaptive Belief Discretization for POMDP Planning", "abstract": "Partially Observable Markov Decision Processes (POMDP) is a widely used model to represent the interaction of an environment and an agent, under state uncertainty. Since the agent does not observe the environment state, its uncertainty is typically represented through a probabilistic belief. While the set of possible beliefs is infinite, making exact planning intractable, the belief space's complexity (and hence planning complexity) is characterized by its covering number. Many POMDP solvers uniformly discretize the belief space and give the planning error in terms of the (typically unknown) covering number. We instead propose an adaptive belief discretization scheme, and give its associated planning error. We furthermore characterize the covering number with respect to the POMDP parameters. This allows us to specify the exact memory requirements on the planner, needed to bound the value function error. We then propose a novel, computationally efficient solver using this scheme. We demonstrate that our algorithm is highly competitive with the state of the art in a variety of scenarios."}}
{"id": "1Jrwl5SiwcR", "cdate": 1609459200000, "mdate": 1634362363946, "content": {"title": "Fair Set Selection: Meritocracy and Social Welfare", "abstract": "Typically, merit is defined with respect to some intrinsic measure of worth. We instead consider a setting where an individual's worth is \\emph{relative}: when a Decision Maker (DM) selects a set of individuals from a population to maximise expected utility, it is natural to consider the \\emph{Expected Marginal Contribution} (EMC) of each person to the utility. We show that this notion satisfies an axiomatic definition of fairness for this setting. We also show that for certain policy structures, this notion of fairness is aligned with maximising expected utility, while for linear utility functions it is identical to the Shapley value. However, for certain natural policies, such as those that select individuals with a specific set of attributes (e.g. high enough test scores for college admissions), there is a trade-off between meritocracy and utility maximisation. We analyse the effect of constraints on the policy on both utility and fairness in extensive experiments based on college admissions and outcomes in Norwegian universities."}}
{"id": "z7UGFIdDFXj", "cdate": 1603119169549, "mdate": null, "content": {"title": "Inferential Induction: A Novel Framework for Bayesian Reinforcement Learning", "abstract": "  Bayesian Reinforcement Learning (BRL) offers a decision-theoretic solution to the reinforcement learning problem. While ''model-based'' BRL algorithms have focused either on maintaining a posterior distribution on models,  BRL ''model-free'' methods try to estimate value function distributions but make strong implicit assumptions or approximations. We describe a novel Bayesian framework, \\emph{inferential induction}, for correctly inferring value function distributions from data, which leads to a new family of BRL algorithms. We design an algorithm, Bayesian Backwards Induction (BBI), with this framework. We experimentally demonstrate that  BBI is competitive with the state of the art. However, its advantage relative to existing BRL model-free methods is not as great as we have expected, particularly when the additional computational burden is taken into account."}}
{"id": "ocY679Wm6am", "cdate": 1577836800000, "mdate": 1634362363944, "content": {"title": "Inferential Induction: Joint Bayesian Estimation of MDPs and Value Functions", "abstract": "Bayesian reinforcement learning (BRL) offers a decision-theoretic solution for reinforcement learning. While \"model-based\" BRL algorithms have focused either on maintaining a posterior distribution on models or value functions and combining this with approximate dynamic programming or tree search, previous Bayesian \"model-free\" value function distribution approaches implicitly make strong assumptions or approximations. We describe a novel Bayesian framework, Inferential Induction, for correctly inferring value function distributions from data, which leads to the development of a new class of BRL algorithms. We design an algorithm, Bayesian Backwards Induction, with this framework. We experimentally demonstrate that the proposed algorithm is competitive with respect to the state of the art."}}
{"id": "hcE0ElD3U4I", "cdate": 1577836800000, "mdate": 1634362363946, "content": {"title": "A Novel Individually Rational Objective In Multi-Agent Multi-Armed Bandits: Algorithms and Regret Bounds", "abstract": "We study a two-player stochastic multi-armed bandit (MAB) problem with different expected rewards for each player, a generalisation of two-player general sum repeated games to stochastic rewards. Our aim is to find the egalitarian bargaining solution (EBS) for the repeated game, which can lead to much higher rewards than the maximin value of both players. Our main contribution is the derivation of an algorithm, UCRG, that achieves simultaneously for both players, a high-probability regret bound of order \u00d5 (T2/3) after any T rounds of play. We demonstrate that our upper bound is nearly optimal by proving a lower bound of (T2/3) for any algorithm. Experiments confirm our theoretical results and the superiority of UCRG compared to the well-known explore-then-commit heuristic."}}
{"id": "NEkOXgUfAin", "cdate": 1577836800000, "mdate": 1634362363945, "content": {"title": "Epistemic Risk-Sensitive Reinforcement Learning", "abstract": ""}}
