{"id": "buaEQHBHAT", "cdate": 1683880256916, "mdate": 1683880256916, "content": {"title": "Dynamic Regret of Online Markov Decision Processes", "abstract": "We investigate online Markov Decision Processes (MDPs) with adversarially changing loss functions and known transitions. We choose dynamic regret as the performance measure, defined as the performance difference between the learner and any sequence of feasible changing policies. The measure is strictly stronger than the standard static regret that benchmarks the learner\u2019s performance with a fixed compared policy. We consider three foundational models of online MDPs, including episodic loop-free Stochastic Shortest Path (SSP), episodic SSP, and infinite-horizon MDPs. For the three models, we propose novel online ensemble algorithms and establish their dynamic regret guarantees respectively, in which the results for episodic (loop-free) SSP are provably minimax optimal in terms of time horizon and certain non-stationarity measure."}}
{"id": "xiNn8xAvoup", "cdate": 1680501138961, "mdate": 1680501138961, "content": {"title": "Towards Understanding Theoretical Advantages of Complex-Reaction Networks", "abstract": "Complex-valued neural networks have attracted increasing attention in recent years, while it remains open on the advantages of complex-valued neural networks in comparison with real-valued networks. This work takes one step on this direction by introducing the complex-reaction network with fully-connected feed-forward architecture. We prove the universal approximation property for complex-reaction networks, and show that a class of radial functions can be approximated by a complex-reaction network using the polynomial number of parameters, whereas real-valued networks need at least exponential parameters to reach the same approximation level. For empirical risk minimization, we study the landscape and convergence of complex gradient descents. Our theoretical result shows that the critical point set of complex-reaction networks is a proper subset of that of real-valued networks, which may show some insights on finding the optimal solutions more easily for complex-reaction networks."}}
{"id": "LDovfXCKKZ", "cdate": 1680501019013, "mdate": 1680501019013, "content": {"title": "On the Consistency Rate of Decision Tree Learning Algorithms", "abstract": "http://aistats.org/aistats2023/accepted.html"}}
{"id": "R7p75Bb1o_D", "cdate": 1680500839617, "mdate": 1680500839617, "content": {"title": "LIFE: Learning Individual Features for Multivariate Time Series Prediction with Missing Values", "abstract": "Multivariate time series (MTS) prediction is ubiquitous in real-world fields, but MTS data often contains missing values. In recent years, there has been an increasing interest in using end-to-end models to handle MTS with missing values. To generate features for prediction, existing methods either merge all input dimensions of MTS or tackle each input dimension independently. However, both approaches are hard to perform well because the former usually produce many unreliable features and the latter lacks correlated information. In this paper, we propose a Learning Individual Features (LIFE) framework, which provides a new paradigm for MTS prediction with missing values. LIFE generates reliable features for prediction by using the correlated dimensions as auxiliary information and suppressing the interference from uncorrelated dimensions with missing values. Experiments on two real-world data sets verify the superiority of LIFE to existing state-of-the-art models. The full version of this work can refer to arXiv (2109.14844)."}}
{"id": "xK7AK6w8pMd", "cdate": 1677628800000, "mdate": 1681746094100, "content": {"title": "Isolation Distributional Kernel: A New Tool for Point and Group Anomaly Detections", "abstract": "We introduce Isolation Distributional Kernel as a new way to measure the similarity between two distributions. Existing approaches based on kernel mean embedding, which convert a point kernel to a distributional kernel, have two key issues: the point kernel employed has a feature map with intractable dimensionality; and it is <i>data independent</i> . This paper shows that Isolation Distributional Kernel (IDK), which is based on a <i>data dependent</i> point kernel, addresses both key issues. We demonstrate IDK\u2019s efficacy and efficiency as a new tool for kernel-based anomaly detection for both point and group anomalies. Without explicit learning, using IDK alone outperforms existing kernel-based point anomaly detector OCSVM and other kernel mean embedding methods that rely on Gaussian kernel. For group anomaly detection, we introduce an IDK based detector called IDK <inline-formula><tex-math notation=\"LaTeX\">$^2$</tex-math></inline-formula> . It reformulates the problem of group anomaly detection in input space into the problem of point anomaly detection in Hilbert Space, without the need for learning. IDK <inline-formula><tex-math notation=\"LaTeX\">$^2$</tex-math></inline-formula> runs orders of magnitude faster than group anomaly detector OCSMM. We reveal for the first time that an effective kernel-based anomaly detector based on kernel mean embedding must employ a characteristic kernel which is data dependent."}}
{"id": "oebt3Zbfi_", "cdate": 1677628800000, "mdate": 1683901478740, "content": {"title": "Isolation Distributional Kernel: A New Tool for Point and Group Anomaly Detections", "abstract": "We introduce Isolation Distributional Kernel as a new way to measure the similarity between two distributions. Existing approaches based on kernel mean embedding, which convert a point kernel to a distributional kernel, have two key issues: the point kernel employed has a feature map with intractable dimensionality; and it is <i>data independent</i> . This paper shows that Isolation Distributional Kernel (IDK), which is based on a <i>data dependent</i> point kernel, addresses both key issues. We demonstrate IDK\u2019s efficacy and efficiency as a new tool for kernel-based anomaly detection for both point and group anomalies. Without explicit learning, using IDK alone outperforms existing kernel-based point anomaly detector OCSVM and other kernel mean embedding methods that rely on Gaussian kernel. For group anomaly detection, we introduce an IDK based detector called IDK <inline-formula><tex-math notation=\"LaTeX\">$^2$</tex-math></inline-formula> . It reformulates the problem of group anomaly detection in input space into the problem of point anomaly detection in Hilbert Space, without the need for learning. IDK <inline-formula><tex-math notation=\"LaTeX\">$^2$</tex-math></inline-formula> runs orders of magnitude faster than group anomaly detector OCSMM. We reveal for the first time that an effective kernel-based anomaly detector based on kernel mean embedding must employ a characteristic kernel which is data dependent."}}
{"id": "MsVp_HI8cZl", "cdate": 1677628800000, "mdate": 1683901373897, "content": {"title": "Isolation Distributional Kernel: A New Tool for Point and Group Anomaly Detections", "abstract": "We introduce Isolation Distributional Kernel as a new way to measure the similarity between two distributions. Existing approaches based on kernel mean embedding, which convert a point kernel to a distributional kernel, have two key issues: the point kernel employed has a feature map with intractable dimensionality; and it is <i>data independent</i> . This paper shows that Isolation Distributional Kernel (IDK), which is based on a <i>data dependent</i> point kernel, addresses both key issues. We demonstrate IDK\u2019s efficacy and efficiency as a new tool for kernel-based anomaly detection for both point and group anomalies. Without explicit learning, using IDK alone outperforms existing kernel-based point anomaly detector OCSVM and other kernel mean embedding methods that rely on Gaussian kernel. For group anomaly detection, we introduce an IDK based detector called IDK <inline-formula><tex-math notation=\"LaTeX\">$^2$</tex-math></inline-formula> . It reformulates the problem of group anomaly detection in input space into the problem of point anomaly detection in Hilbert Space, without the need for learning. IDK <inline-formula><tex-math notation=\"LaTeX\">$^2$</tex-math></inline-formula> runs orders of magnitude faster than group anomaly detector OCSMM. We reveal for the first time that an effective kernel-based anomaly detector based on kernel mean embedding must employ a characteristic kernel which is data dependent."}}
{"id": "3WW4WXieYz", "cdate": 1675048877048, "mdate": 1675048877048, "content": {"title": "Towards Enabling Learnware to Handle Heterogeneous Feature Spaces", "abstract": "The learnware paradigm was recently proposed by Zhou (2016) with the wish of developing the learnware market to help users build models more efciently by reusing existing well-performed models rather than starting from scratch. Specifcally, a learnware in the learnware market is a well-performed pre-trained model with a specifcation describing its specialty and utility, and the market identifes helpful learnware(s) for the user\u2019s task based on the specifcation. Recent studies have attempted to realize a homogeneous  prototype learnware market initially through Reduced Kernel Mean Embedding (RKME) specifcation, which requires all models in the market to share the same feature space. However, this limits the application scope of the learnware paradigm because various pre-trained models are often obtained from diferent feature spaces in real-world scenarios. In this paper, we make the frst attempt to enable the learnware to handle heterogeneous feature spaces. We propose a more powerful specifcation to manage heterogeneous learnwares by integrating subspace learning in the specifcation design, along with a practical approach for identifying and reusing helpful learnwares for the user\u2019s task. Empirical studies on both synthetic data and real-world tasks validate the efcacy of our approach."}}
{"id": "xwyaZP3hGl0", "cdate": 1672531200000, "mdate": 1683901478689, "content": {"title": "Stream Efficient Learning", "abstract": "Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have infinitely supplied computational resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with computational resource concerns. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a theoretical framework which takes into account the influence of computational resource, in learning theory for the first time, in addition to the ability of learning algorithm and complexity of the problem."}}
{"id": "uuVTpe3oof", "cdate": 1672531200000, "mdate": 1681746094355, "content": {"title": "Revisiting Weighted Strategy for Non-stationary Parametric Bandits", "abstract": "Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a refined analysis framework, which simplifies the derivation and importantly produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\\widetilde{O}(k_\\mu^{\\frac{5}{4}} c_\\mu^{-\\frac{3}{4}} d^{\\frac{3}{4}} P_T^{\\frac{1}{4}}T^{\\frac{3}{4}})$ regret, improving the $\\widetilde{O}(k_\\mu^{2} c_\\mu^{-1}d^{\\frac{9}{10}} P_T^{\\frac{1}{5}}T^{\\frac{4}{5}})$ bound in prior work, where $k_\\mu$ and $c_\\mu$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon."}}
