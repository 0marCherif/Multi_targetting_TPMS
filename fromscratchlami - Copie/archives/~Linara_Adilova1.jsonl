{"id": "szT_zD88bm", "cdate": 1672531200000, "mdate": 1695985100589, "content": {"title": "FAM: Relative Flatness Aware Minimization", "abstract": "Flatness of the loss curve around a model at hand has been shown to empirically correlate with its generalization ability. Optimizing for flatness has been proposed as early as 1994 by Hochreiter and Schmidthuber, and was followed by more recent successful sharpness-aware optimization techniques. Their widespread adoption in practice, though, is dubious because of the lack of theoretically grounded connection between flatness and generalization, in particular in light of the reparameterization curse - certain reparameterizations of a neural network change most flatness measures but do not change generalization. Recent theoretical work suggests that a particular relative flatness measure can be connected to generalization and solves the reparameterization curse. In this paper, we derive a regularizer based on this relative flatness that is easy to compute, fast, efficient, and works with arbitrary loss functions. It requires computing the Hessian only of a single layer of the network, which makes it applicable to large neural networks, and with it avoids an expensive mapping of the loss surface in the vicinity of the model. In an extensive empirical evaluation we show that this relative flatness aware minimization (FAM) improves generalization in a multitude of applications and models, both in finetuning and standard training. We make the code available at github."}}
{"id": "Zs9VibpTMOCK", "cdate": 1672531200000, "mdate": 1695375822788, "content": {"title": "Layerwise Linear Mode Connectivity", "abstract": "In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the very beginning of the learning, even when training on the same data. Based on the observation that the learning process evolves differently in different layers, we investigate the barrier between models in a layerwise fashion. Our conjecture is that barriers preventing from successful federated training are caused by a particular layer or group of layers."}}
{"id": "FDBRVwv1F1m", "cdate": 1672531200000, "mdate": 1695985100597, "content": {"title": "Information Plane Analysis for Dropout Neural Networks", "abstract": ""}}
{"id": "EcSPqAmnhr", "cdate": 1672531200000, "mdate": 1681649905918, "content": {"title": "Information Plane Analysis for Dropout Neural Networks", "abstract": ""}}
{"id": "bQB6qozaBw", "cdate": 1663850441991, "mdate": null, "content": {"title": "Information Plane Analysis for Dropout Neural Networks", "abstract": "The information-theoretic framework promises to explain the predictive power of neural networks. In particular, the information plane analysis, which measures mutual information (MI) between input and representation as well as representation and output, should give rich insights into the training process. This approach, however, was shown to strongly depend on the choice of estimator of the MI. The problem is amplified for deterministic networks if the MI between input and representation is infinite. Thus, the estimated values are defined by the different approaches for estimation, but do not adequately represent the training process from an information-theoretic perspective. In this work, we show that dropout with continuously distributed noise ensures that MI is finite. We demonstrate in a range of experiments that this enables a meaningful information plane analysis for a class of dropout neural networks that is widely used in practice."}}
{"id": "BqbX745we5", "cdate": 1640995200000, "mdate": 1645876250664, "content": {"title": "Visual Analytics for Human-Centered Machine Learning", "abstract": "We introduce a new research area in visual analytics (VA) aiming to bridge existing gaps between methods of interactive machine learning (ML) and eXplainable Artificial Intelligence (XAI), on one side, and human minds, on the other side. The gaps are, first, a conceptual mismatch between ML/XAI outputs and human mental models and ways of reasoning, and second, a mismatch between the information quantity and level of detail and human capabilities to perceive and understand. A grand challenge is to adapt ML and XAI to human goals, concepts, values, and ways of thinking. Complementing the current efforts in XAI towards solving this challenge, VA can contribute by exploiting the potential of visualization as an effective way of communicating information to humans and a strong trigger of human abstractive perception and thinking. We propose a cross-disciplinary research framework and formulate research directions for VA."}}
{"id": "sygvo7ctb_", "cdate": 1621629920119, "mdate": null, "content": {"title": "Relative Flatness and Generalization", "abstract": "Flatness of the loss curve is conjectured to be connected to the generalization ability of machine learning models, in particular neural networks. While it has been empirically observed that flatness measures consistently correlate strongly with generalization, it is still an open theoretical problem why and under which circumstances flatness is connected to generalization, in particular in light of reparameterizations that change certain flatness measures but leave generalization unchanged. We investigate the connection between flatness and generalization by relating it to the interpolation from representative data, deriving notions of representativeness, and feature robustness. The notions allow us to rigorously connect flatness and generalization and to identify conditions under which the connection holds. Moreover, they give rise to a novel, but natural relative flatness measure that correlates strongly with generalization, simplifies to ridge regression for ordinary least squares, and solves the reparameterization issue."}}
{"id": "1UhmQhUztmp", "cdate": 1620370279910, "mdate": null, "content": {"title": "EFFICIENT DECENTRALIZED DEEP LEARNING BY DYNAMIC MODEL AVERAGING", "abstract": "We propose an efficient protocol for decentralized training of deep neural networks from distributed data sources. The proposed protocol allows to handle different phases of model training equally well and to quickly adapt to concept drifts. This leads to a reduction of communication by an order of magnitude compared to periodically communicating state-of-the-art approaches. Moreover, we derive a communication bound that scales well with the hardness of the serialized learning problem. The reduction in communication comes at almost no cost, as the predictive performance remains virtually unchanged. Indeed, the proposed protocol retains loss bounds of periodically averaging schemes. An extensive empirical evaluation validates major improvement of the trade-off between model performance and communication which could be beneficial for numerous decentralized learning applications, such as autonomous driving, or voice recognition and image classification on mobile phones."}}
{"id": "uQbDSn9o6v", "cdate": 1609459200000, "mdate": 1681649905638, "content": {"title": "Plants Don't Walk on the Street: Common-Sense Reasoning for Reliable Semantic Segmentation", "abstract": ""}}
{"id": "eITFLsQGNY", "cdate": 1609459200000, "mdate": 1673007492063, "content": {"title": "Novelty Detection in Sequential Data by Informed Clustering and Modeling", "abstract": ""}}
