{"id": "k98U0cb0Ig", "cdate": 1652737762618, "mdate": null, "content": {"title": "Adaptive Stochastic Variance Reduction for Non-convex Finite-Sum Minimization", "abstract": "We propose an adaptive variance-reduction method, called AdaSpider, for minimization of $L$-smooth, non-convex functions with a finite-sum structure. In essence, AdaSpider combines an AdaGrad-inspired (Duchi et al., 2011), but a fairly distinct, adaptive step-size schedule with the recursive \\textit{stochastic path integrated estimator} proposed in (Fang et al., 2018). To our knowledge, AdaSpider is the first parameter-free non-convex variance-reduction method in the sense that it does not require the knowledge of problem-dependent parameters, such as smoothness constant $L$, target accuracy $\\epsilon$ or any bound on gradient norms. In doing so, we are able to compute an $\\epsilon$-stationary point with $\\tilde{O}\\left(n + \\sqrt{n}/\\epsilon^2\\right)$ oracle-calls, which matches the respective lower bound up to logarithmic factors."}}
{"id": "wVc4Qg5Bhah", "cdate": 1652737575692, "mdate": null, "content": {"title": "Extra-Newton: A First Approach to Noise-Adaptive Accelerated Second-Order Methods", "abstract": "In this work, we propose a universal and adaptive second-order method for minimization of second-order smooth, convex functions. Precisely, our algorithm achieves $O(\\sigma / \\sqrt{T})$ when the oracle feedback is stochastic with variance $\\sigma$, and obtains the improved $O( 1 / T^3)$ convergence with deterministic oracles. Our method achieves this rate interpolation without knowing the nature of the oracle a priori, which was enabled by a parameter-free step-size that is oblivious to the knowledge of smoothness modulus, variance bounds and the diameter of the constrained set. To our knowledge, this is the first universal algorithm that achieves the aforementioned global guarantees within second-order convex optimization literature."}}
{"id": "Ev0qWoBXkv", "cdate": 1640995200000, "mdate": 1702583012561, "content": {"title": "Extra-Newton: A First Approach to Noise-Adaptive Accelerated Second-Order Methods", "abstract": "In this work, we propose a universal and adaptive second-order method for minimization of second-order smooth, convex functions. Precisely, our algorithm achieves $O(\\sigma / \\sqrt{T})$ when the oracle feedback is stochastic with variance $\\sigma$, and obtains the improved $O( 1 / T^3)$ convergence with deterministic oracles. Our method achieves this rate interpolation without knowing the nature of the oracle a priori, which was enabled by a parameter-free step-size that is oblivious to the knowledge of smoothness modulus, variance bounds and the diameter of the constrained set. To our knowledge, this is the first universal algorithm that achieves the aforementioned global guarantees within second-order convex optimization literature."}}
{"id": "9nsQcrhzsZG", "cdate": 1640995200000, "mdate": 1653657362213, "content": {"title": "High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize", "abstract": "In this paper, we propose a new, simplified high probability analysis of AdaGrad for smooth, non-convex problems. More specifically, we focus on a particular accelerated gradient (AGD) template (Lan, 2020), through which we recover the original AdaGrad and its variant with averaging, and prove a convergence rate of $\\mathcal O (1/ \\sqrt{T})$ with high probability without the knowledge of smoothness and variance. We use a particular version of Freedman's concentration bound for martingale difference sequences (Kakade & Tewari, 2008) which enables us to achieve the best-known dependence of $\\log (1 / \\delta )$ on the probability margin $\\delta$. We present our analysis in a modular way and obtain a complementary $\\mathcal O (1 / T)$ convergence rate in the deterministic setting. To the best of our knowledge, this is the first high probability result for AdaGrad with a truly adaptive scheme, i.e., completely oblivious to the knowledge of smoothness and uniform variance bound, which simultaneously has best-known dependence of $\\log( 1/ \\delta)$. We further prove noise adaptation property of AdaGrad under additional noise assumptions."}}
{"id": "7VnqOPAVb2", "cdate": 1640995200000, "mdate": 1702583012495, "content": {"title": "Adaptive Stochastic Variance Reduction for Non-convex Finite-Sum Minimization", "abstract": "We propose an adaptive variance-reduction method, called AdaSpider, for minimization of $L$-smooth, non-convex functions with a finite-sum structure. In essence, AdaSpider combines an AdaGrad-inspired (Duchi et al., 2011), but a fairly distinct, adaptive step-size schedule with the recursive \\textit{stochastic path integrated estimator} proposed in (Fang et al., 2018). To our knowledge, AdaSpider is the first parameter-free non-convex variance-reduction method in the sense that it does not require the knowledge of problem-dependent parameters, such as smoothness constant $L$, target accuracy $\\epsilon$ or any bound on gradient norms. In doing so, we are able to compute an $\\epsilon$-stationary point with $\\tilde{O}\\left(n + \\sqrt{n}/\\epsilon^2\\right)$ oracle-calls, which matches the respective lower bound up to logarithmic factors."}}
{"id": "dSw0QtRMJkO", "cdate": 1632875695853, "mdate": null, "content": {"title": "High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize", "abstract": "In this paper, we propose a new, simplified high probability analysis of AdaGrad for smooth, non-convex problems. \nMore specifically, we focus on a particular accelerated gradient (AGD) template (Lan, 2020), through which we recover the original AdaGrad and its variant with averaging, and prove a convergence rate of $\\mathcal O (1/ \\sqrt{T})$ with high probability without the knowledge of smoothness and variance. \nWe use a particular version of Freedman's concentration bound for martingale difference sequences (Kakade & Tewari, 2008) which enables us to achieve the best-known dependence of $\\log (1 / \\delta )$ on the probability margin $\\delta$. \nWe present our analysis in a modular way and obtain a complementary $\\mathcal O (1 / T)$ convergence rate in the deterministic setting. \nTo the best of our knowledge, this is the first high probability result for AdaGrad with a truly adaptive scheme, i.e., completely oblivious to the knowledge of smoothness and uniform variance bound, which simultaneously has best-known dependence of $\\log( 1/ \\delta)$. \nWe further prove noise adaptation property of AdaGrad under additional noise assumptions."}}
{"id": "VsUQQkpEXgr", "cdate": 1621630291034, "mdate": null, "content": {"title": "Sifting through the noise: Universal first-order methods for stochastic variational inequalities", "abstract": "We examine a flexible algorithmic framework for solving monotone variational inequalities in the presence of randomness and uncertainty. The proposed template encompasses a wide range of popular first-order methods, including dual averaging, dual extrapolation and optimistic gradient algorithms \u2013 both adaptive and non-adaptive. Our first result is that the algorithm achieves the optimal rates of convergence for cocoercive problems when the profile of the randomness is known to the optimizer: $\\mathcal{O}(1/\\sqrt{T})$ for absolute noise profiles, and $\\mathcal{O}(1/T)$ for relative ones. Subsequently, we drop all prior knowledge requirements (the absolute/relative variance of the randomness affecting the problem, the operator's cocoercivity constant, etc.), and we analyze an adaptive instance of the method that gracefully interpolates between the above rates \u2013 i.e. it achieves $\\mathcal{O}(1/\\sqrt{T})$ and $\\mathcal{O}(1/T)$ in the absolute and relative cases, respectively. To our knowledge, this is the first universality result of its kind in the literature and, somewhat surprisingly, it shows that an extra-gradient proxy step is not required to achieve optimal rates."}}
{"id": "ytke6qKpxtr", "cdate": 1621630272989, "mdate": null, "content": {"title": "STORM+: Fully Adaptive SGD with Recursive Momentum for Nonconvex Optimization", "abstract": "In this work we investigate stochastic non-convex optimization problems where the objective is an expectation over smooth loss functions, and the goal is to find an approximate stationary point. The most popular approach to handling such problems is variance reduction techniques, which are also known to obtain tight convergence rates, matching the lower bounds in this case. Nevertheless, these techniques require a careful maintenance of anchor points in conjunction with appropriately selected ``mega-batchsizes\". This leads to a challenging hyperparameter tuning problem, that weakens their practicality. Recently, [Cutkosky and Orabona, 2019] have shown that one can employ recursive momentum in order to avoid the use of anchor points and large batchsizes, and still obtain the optimal rate for this setting. Yet, their method called $\\rm{STORM}$ crucially relies on the knowledge of the smoothness, as well a bound on the gradient norms. In this work we propose $\\rm{STORM}^{+}$, a new method that is completely parameter-free, does not require large batch-sizes, and obtains the optimal $O(1/T^{1/3})$ rate for finding an approximate stationary point. Our work builds on the $\\rm{STORM}$ algorithm, in conjunction with a novel approach to adaptively set the learning rate and momentum parameters."}}
{"id": "gwzE1LJo2GA", "cdate": 1609459200000, "mdate": 1653657362228, "content": {"title": "STORM+: Fully Adaptive SGD with Recursive Momentum for Nonconvex Optimization", "abstract": "In this work we investigate stochastic non-convex optimization problems where the objective is an expectation over smooth loss functions, and the goal is to find an approximate stationary point. The most popular approach to handling such problems is variance reduction techniques, which are also known to obtain tight convergence rates, matching the lower bounds in this case. Nevertheless, these techniques require a careful maintenance of anchor points in conjunction with appropriately selected ``mega-batchsizes\". This leads to a challenging hyperparameter tuning problem, that weakens their practicality. Recently, [Cutkosky and Orabona, 2019] have shown that one can employ recursive momentum in order to avoid the use of anchor points and large batchsizes, and still obtain the optimal rate for this setting. Yet, their method called $\\rm{STORM}$ crucially relies on the knowledge of the smoothness, as well a bound on the gradient norms. In this work we propose $\\rm{STORM}^{+}$, a new method that is completely parameter-free, does not require large batch-sizes, and obtains the optimal $O(1/T^{1/3})$ rate for finding an approximate stationary point. Our work builds on the $\\rm{STORM}$ algorithm, in conjunction with a novel approach to adaptively set the learning rate and momentum parameters."}}
{"id": "P1FALhRiLLm", "cdate": 1609459200000, "mdate": 1653657362227, "content": {"title": "Sifting through the noise: Universal first-order methods for stochastic variational inequalities", "abstract": "We examine a flexible algorithmic framework for solving monotone variational inequalities in the presence of randomness and uncertainty. The proposed template encompasses a wide range of popular first-order methods, including dual averaging, dual extrapolation and optimistic gradient algorithms \u2013 both adaptive and non-adaptive. Our first result is that the algorithm achieves the optimal rates of convergence for cocoercive problems when the profile of the randomness is known to the optimizer: $\\mathcal{O}(1/\\sqrt{T})$ for absolute noise profiles, and $\\mathcal{O}(1/T)$ for relative ones. Subsequently, we drop all prior knowledge requirements (the absolute/relative variance of the randomness affecting the problem, the operator's cocoercivity constant, etc.), and we analyze an adaptive instance of the method that gracefully interpolates between the above rates \u2013 i.e. it achieves $\\mathcal{O}(1/\\sqrt{T})$ and $\\mathcal{O}(1/T)$ in the absolute and relative cases, respectively. To our knowledge, this is the first universality result of its kind in the literature and, somewhat surprisingly, it shows that an extra-gradient proxy step is not required to achieve optimal rates."}}
