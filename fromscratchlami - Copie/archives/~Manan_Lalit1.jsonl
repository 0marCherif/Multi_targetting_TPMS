{"id": "JM6GuFGayL5", "cdate": 1612980104589, "mdate": null, "content": {"title": "Embedding-based Instance Segmentation in Microscopy", "abstract": "Automatic detection and segmentation of objects in 2D and 3D microscopy data is important for countless biomedical applications.\nIn the natural image domain, spatial embedding-based instance segmentation methods are known to yield high-quality results, but their utility for segmenting microscopy data is currently little researched. Here we introduce EmbedSeg, an embedding-based instance segmentation method which outperforms existing state-of-the-art baselines on 2D as well as 3D microscopy datasets.\nAdditionally, we show that EmbedSeg has a GPU memory footprint small enough to train even on laptop GPUs, making it accessible to virtually everyone. Finally, we introduce four new 3D microscopy datasets, which we make publicly available alongside ground truth training labels. Our open-source implementation is available at https://github.com/juglab/EmbedSeg.\n"}}
{"id": "NJc4ShCMzV0", "cdate": 1609459200000, "mdate": 1631640797998, "content": {"title": "Embedding-based Instance Segmentation of Microscopy Images", "abstract": "Automatic detection and segmentation of objects in 2D and 3D microscopy data is important for countless biomedical applications. In the natural image domain, spatial embedding-based instance segmentation methods are known to yield high-quality results, but their utility for segmenting microscopy data is currently little researched. Here we introduce EmbedSeg, an embedding-based instance segmentation method which outperforms existing state-of-the-art baselines on 2D as well as 3D microscopy datasets. Additionally, we show that EmbedSeg has a GPU memory footprint small enough to train even on laptop GPUs, making it accessible to virtually everyone. Finally, we introduce four new 3D microscopy datasets, which we make publicly available alongside ground truth training labels. Our open-source implementation is available at https://github.com/juglab/EmbedSeg."}}
{"id": "xHP1W_Y0jF", "cdate": 1595260910878, "mdate": null, "content": {"title": "Registration of multi-modal volumetric images of embryos by establishing correspondences between cells", "abstract": "Early development of an animal from an egg involves a rapid increase in cell number and several cell fate specification events which are accompanied by dynamic morphogenetic changes.\nIn order to correlate the morphological changes with the underlying genetic events, one typically needs to monitor the living system with several imaging modalities offering different spatial and temporal resolution.\nLive imaging allows monitoring the embryo at a high temporal resolution and observing the morphological changes during the early development. Confocal images of specimens fixed and stained for the expression of certain genes provide high spatially-resolved static snapshots and enable observing the transcription states of an embryo at specific time points during development. The two modalities cannot, by definition, be applied to the same specimen and thus, separately obtained images of different specimens need to be registered.\nBiologically, the most meaningful way to register the images is by identifying cellular correspondences between these two imaging modalities. In this way, one can bring the two sources of information into a single domain and combine dynamic information on morphogenesis with static gene expression data. \nThe problem of establishing cellular correspondence is non-trivial due to the stochasticity of developmental processes and the non-linear deformation of the specimen during staining protocols. \nHere we propose a new computational pipeline for identifying cell-to-cell correspondences between images from multiple modalities and for using these correspondences to register 3D images within and across imaging modalities. \nWe demonstrate this pipeline by combining four dimensional time-lapse showing embryogenesis of Spiralian ragworm Platyneries dumerilii with three dimensional scans of fixed Platyneries dumerilii embryos stained for the expression of a variety of important developmental transcription factors.  \nWe compare our approach with methods for aligning point clouds and show that we match the accuracy of these state-of-the-art registration pipelines on synthetic data. We show that our approach outperforms these methods on real biological imaging datasets. In addition, our approach uniquely provides, in addition to the registration, also the non-redundant matching of corresponding, biologically meaningful entities within the registered specimen which is the prerequisite for generating biological insights from the combined datasets.\nThe complete pipeline is available for public use through a Fiji  plugin.\n"}}
{"id": "f3_MpviKwCK", "cdate": 1577836800000, "mdate": 1631640797995, "content": {"title": "Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence", "abstract": "Early development of an animal from an egg involves a rapid increase in cell number and several cell fate specification events accompanied by dynamic morphogenetic changes. In order to correlate the morphological changes with the genetic events, one typically needs to monitor the living system with several imaging modalities offering different spatial and temporal resolution. Live imaging allows monitoring the embryo at a high temporal resolution and observing the morphological changes. On the other hand, confocal images of specimens fixed and stained for the expression of certain genes enable observing the transcription states of an embryo at specific time points during development with high spatial resolution. The two imaging modalities cannot, by definition, be applied to the same specimen and thus, separately obtained images of different specimens need to be registered. Biologically, the most meaningful way to register the images is by identifying cellular correspondences between these two imaging modalities. In this way, one can bring the two sources of information into a single domain and combine dynamic information on morphogenesis with static gene expression data. Here we propose a new computational pipeline for identifying cell-to-cell correspondences between images from multiple modalities and for using these correspondences to register 3D images within and across imaging modalities. We demonstrate this pipeline by combining four-dimensional recording of embryogenesis of Spiralian annelid ragworm Platynereis dumerilii with three-dimensional scans of fixed Platynereis dumerilii embryos stained for the expression of a variety of important developmental genes. We compare our approach with methods for aligning point clouds and show that we match the accuracy of these state-of-the-art registration pipelines on synthetic data. We show that our approach outperforms these methods on real biological imaging datasets. Importantly, our approach uniquely provides, in addition to the registration, also the non-redundant matching of corresponding, biologically meaningful entities within the registered specimen which is the prerequisite for generating biological insights from the combined datasets. The complete pipeline is available for public use through a Fiji plugin."}}
{"id": "ISUZQm8bijR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Probabilistic Noise2Void: Unsupervised Content-Aware Denoising", "abstract": "Today, Convolutional Neural Networks (CNNs) are the leading method for image denoising. They are traditionally trained on pairs of images, which are often hard to obtain for practical applications. This motivates self-supervised training methods, such as Noise2Void (N2V) that operate on single noisy images. Self-supervised methods are, unfortunately, not competitive with models trained on image pairs. Here, we present Probabilistic Noise2Void (PN2V), a method to train CNNs to predict per-pixel intensity distributions. Combining these with a suitable description of the noise, we obtain a complete probabilistic model for the noisy observations and true signal in every pixel. We evaluate PN2V on publicly available microscopy datasets, under a broad range of noise regimes, and achieve competitive results with respect to supervised state-of-the-art methods."}}
{"id": "26FF5lzi4U0", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fully Unsupervised Probabilistic Noise2Void.", "abstract": "Image denoising is the first step in many biomedical image analysis pipelines and Deep Learning (DL) based methods are currently best performing. A new category of DL methods such as Noise2Void or Noise2Self can be used fully unsupervised, requiring nothing but the noisy data. However, this comes at the price of reduced reconstruction quality. The recently proposed Probabilistic Noise2Void (PN2V) improves results, but requires an additional noise model for which calibration data needs to be acquired. Here, we present improvements to PN2V that (i) replace histogram based noise models by parametric noise models, and (ii) show how suitable noise models can be created even in the absence of calibration data. This is a major step since it actually renders PN2V fully unsupervised. We demonstrate that all proposed improvements are not only academic but indeed relevant."}}
{"id": "-MdbZY93HrN", "cdate": 1546300800000, "mdate": null, "content": {"title": "Leveraging Self-supervised Denoising for Image Segmentation.", "abstract": "Deep learning (DL) has arguably emerged as the method of choice for the detection and segmentation of biological structures in microscopy images. However, DL typically needs copious amounts of annotated training data that is for biomedical projects typically not available and excessively expensive to generate. Additionally, tasks become harder in the presence of noise, requiring even more high-quality training data. Hence, we propose to use denoising networks to improve the performance of other DL-based image segmentation methods. More specifically, we present ideas on how state-of-the-art self-supervised CARE networks can improve cell/nuclei segmentation in microscopy data. Using two state-of-the-art baseline methods, U-Net and StarDist, we show that our ideas consistently improve the quality of resulting segmentations, especially when only limited training data for noisy micrographs are available."}}
