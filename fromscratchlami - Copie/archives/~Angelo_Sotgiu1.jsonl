{"id": "-SVKfqbD-U", "cdate": 1672531200000, "mdate": 1681651566524, "content": {"title": "ImageNet-Patch: A dataset for benchmarking machine learning robustness against adversarial patches", "abstract": ""}}
{"id": "6vdDp42oLzC", "cdate": 1654348671601, "mdate": null, "content": {"title": "ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches", "abstract": "Adversarial patches are optimized contiguous pixel blocks in an input image that cause a  machine-learning model to misclassify it.\nHowever, their optimization is computationally demanding and requires careful hyperparameter tuning.\nTo overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches.\nIt consists of a set of patches optimized to generalize across different models and applied to ImageNet data after preprocessing them with affine transformations.\nThis process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations."}}
{"id": "Y1sWzKW0k4L", "cdate": 1652737772792, "mdate": null, "content": {"title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples", "abstract": "Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of robustness by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations.\nAlthough guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner.\nIn this work, we overcome these limitations by: (i) categorizing   attack failures based on how they affect the optimization of gradient-based attacks, while also  unveiling two novel failures affecting many popular attack implementations and past evaluations;\n (ii) proposing six novel \\emph{indicators of failure}, to automatically detect the presence of such failures in the attack optimization process; and (iii) suggesting a systematic protocol to apply the corresponding fixes. \nOur extensive experimental analysis, involving more than 15 models in 3 distinct application domains, shows that our indicators of failure can be used to debug and improve current adversarial robustness evaluations, thereby providing a first concrete step towards automatizing and systematizing them. Our open-source code is available at: https://github.com/pralab/IndicatorsOfAttackFailure."}}
{"id": "mYXn2STxdFU", "cdate": 1640995200000, "mdate": 1681651566606, "content": {"title": "Explainability-based Debugging of Machine Learning for Vulnerability Discovery", "abstract": ""}}
{"id": "_WkahIE7iq", "cdate": 1640995200000, "mdate": 1681651566578, "content": {"title": "secml: Secure and explainable machine learning in Python", "abstract": ""}}
{"id": "_Gsnd19OgjJ", "cdate": 1640995200000, "mdate": 1654005419552, "content": {"title": "ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches", "abstract": "Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations. We showcase the usefulness of this dataset by testing the effectiveness of the computed patches against 127 models. We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch."}}
{"id": "HNgWQAabUlc", "cdate": 1640995200000, "mdate": 1645776331307, "content": {"title": "FADER: Fast adversarial example rejection", "abstract": "Deep neural networks are vulnerable to adversarial examples, i.e., carefully-crafted inputs that mislead classification at test time. Recent defenses have been shown to improve adversarial robustness by detecting anomalous deviations from legitimate training samples at different layer representations - a behavior normally exhibited by adversarial attacks. Despite technical differences, all aforementioned methods share a common backbone structure that we formalize and highlight in this contribution, as it can help in identifying promising research directions and drawbacks of existing methods. The first main contribution of this work is the review of these detection methods in the form of a unifying framework designed to accommodate both existing defenses and newer ones to come. In terms of drawbacks, the overmentioned defenses require comparing input samples against an oversized number of reference prototypes, possibly at different representation layers, dramatically worsening the test-time efficiency. Besides, such defenses are typically based on ensembling classifiers with heuristic methods, rather than optimizing the whole architecture in an end-to-end manner to better perform detection. As a second main contribution of this work, we introduce FADER, a novel technique for speeding up detection-based methods. FADER overcome the issues above by employing RBF networks as detectors: by fixing the number of required prototypes, the runtime complexity of adversarial examples detectors can be controlled. Our experiments outline up to 73 \u00d7 prototypes reduction compared to analyzed detectors for MNIST dataset, up to 50 \u00d7 for CIFAR10 dataset, and up to 82 \u00d7 on ImageNet10 dataset respectively, without sacrificing classification accuracy on both clean and adversarial data."}}
{"id": "8Q0zU04JwPS", "cdate": 1640995200000, "mdate": 1681651566510, "content": {"title": "Domain Knowledge Alleviates Adversarial Attacks in Multi-Label Classifiers", "abstract": ""}}
{"id": "Pvhk4ZFnTR", "cdate": 1624022580034, "mdate": null, "content": {"title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples", "abstract": "Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of security by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations.\nAlthough guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner.\nIn this work, we overcome these limitations by (i) defining a set of quantitative indicators which unveil common failures in the optimization of gradient-based attacks, and (ii) proposing specific mitigation strategies within a systematic evaluation protocol. \nOur extensive experimental analysis shows that the proposed indicators of failure can be used to visualize, debug and improve current adversarial robustness evaluations, providing a first concrete step towards automatizing and systematizing current adversarial robustness evaluations."}}
{"id": "JdQ2-DTaGF", "cdate": 1621630338976, "mdate": null, "content": {"title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples", "abstract": "Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of security by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations. Although guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner. In this work, we overcome these limitations by (i) defining a set of quantitative indicators which unveil common failures in the optimization of gradient-based attacks, and (ii) proposing specific mitigation strategies within a systematic evaluation protocol. Our extensive experimental analysis shows that the proposed indicators of failure can be used to visualize, debug and improve current adversarial robustness evaluations, providing a first concrete step towards automatizing and systematizing current adversarial robustness evaluations."}}
