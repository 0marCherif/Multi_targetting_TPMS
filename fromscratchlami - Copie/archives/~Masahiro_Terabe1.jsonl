{"id": "WHYGaAzgUAoD", "cdate": 1293840000000, "mdate": null, "content": {"title": "A methodology of forest monitoring from hyperspectral images with sparse regularization", "abstract": "This paper presents a methodology to extract information on existing conditions of a forest from hyperspectral images and SAR images for the forest management. To overcome the difficulties in hyperspectral image analysis such as optimal band selection and model overfitting, a machine learning technique called sparse regularization was adopted. Experimental results show the effectiveness of this approach."}}
{"id": "OfTujzixTv4", "cdate": 1230768000000, "mdate": null, "content": {"title": "Anomaly Detection for DNS Servers Using Frequent Host Selection", "abstract": "DNS is one of the internet's fundamental building blocks, used by various applications such as web and mail transfer. Therefore, monitoring DNS traffic has potential to detect host anomalies such as spammers and infected hosts in a network. However, previous works assume a small number of hosts or target on domain name anomalies, so that they cannot be applied to a large-scale networks due to performance issues. A large number of hosts and long-term tracing consume computational resources and make real-time analysis difficult. In this paper, we propose anomaly detection for DNS servers using frequent host selection, which selects only potential hosts and does not depend on the number of hosts. We evaluate the proposed system using DNS traffic for 6 months of tracing, and show that the system can feasibly handle hosts in the dataset and detect anomalies, such as mail servers suffering from spam and DNS servers are configured incorrectly."}}
{"id": "e8R6lbT1jN", "cdate": 1199145600000, "mdate": null, "content": {"title": "Design of Physical Activity Recommendation System", "abstract": ""}}
{"id": "Vl71sjhsz8", "cdate": 1199145600000, "mdate": null, "content": {"title": "Decision Tree Induction from Numeric Data Stream", "abstract": "Hoeffding Tree Algorithm is known as a method to induce decision trees from a data stream. Treatment of numeric attribute on Hoeffding Tree Algorithm has been discussed for stationary input. It has not yet investigated, however, for non-stationary input where the effect of concept drift is apparent. This paper identifies three major approaches to handle numeric values, Exhaustive Method, Gaussian Approximation, and Discretizaion Method, and through experiment shows the best suited modeling of numeric attributes for Hoeffding Tree Algorithm. This paper also experimentaly compares the performance of two known methods for concept drift detection, Hoeffding Bound Based Method and Accuracy Based Method."}}
{"id": "Tt81DRBgzwJ", "cdate": 1199145600000, "mdate": null, "content": {"title": "Multi-class Support Vector Machine Simplification", "abstract": "In support vector learning, computational complexity of testing phase scales linearly with number of support vectors (SVs) included in the solution \u2013 support vector machine (SVM). Among different approaches, reduced set methods speed-up the testing phase by replacing original SVM with a simplified one that consists of smaller number of SVs, called reduced vectors (RV). In this paper we introduce an extension of the bottom-up method for binary-class SVMs to multi-class SVMs. The extension includes: calculations for optimally combining two multi-weighted SVs, selection heuristic for choosing a good pair of SVs for replacing them with a newly created vector, and algorithm for reducing the number of SVs included in a SVM classifier. We show that our method possesses key advantages over others in terms of applicability, efficiency and stability. In constructing RVs, it requires finding a single maximum point of a one-variable function. Experimental results on public datasets show that simplified SVMs can run faster original SVMs up to 100 times with almost no change in predictive accuracy."}}
{"id": "ONSH3DXLWBB", "cdate": 1199145600000, "mdate": null, "content": {"title": "A Novel Web Usage Mining Method - Mining and Clustering of DAG Access Patterns Considering Page Browsing Time", "abstract": ""}}
{"id": "FzK1UKQ8iXw", "cdate": 1199145600000, "mdate": null, "content": {"title": "Two-stage incremental working set selection for fast support vector training on large datasets", "abstract": "We propose iSVM - an incremental algorithm that achieves high speed in training support vector machines (SVMs) on large datasets. In the common decomposition framework, iSVM starts with a minimum working set (WS), and then iteratively selects one training example to update the WS in each optimization loop. iSVM employs a two-stage strategy in processing the training data. In the first stage, the most prominent vector among randomly sampled data is added to the WS. This stage results in an approximate SVM solution. The second stage uses temporal solutions to scan through the whole training data once again to find the remaining support vectors (SVs). We show that iSVM is especially efficient for training SVMs on applications where data size is much larger than number of SVs. On the KDD-CUP 1999 network intrusion detection dataset with nearly five millions training examples, iSVM takes less than one hour to train an SVM with 94% testing accuracy, compared to seven hours with LibSVM - one of the state-of-the-art SVM implementations. We also provide analysis and experimental comparisons between iSVM and the related algorithms."}}
{"id": "86MTOONB5Iv", "cdate": 1199145600000, "mdate": null, "content": {"title": "Learning Higher Accuracy Decision Trees from Concept Drifting Data Streams", "abstract": "In this paper, we propose to combine the naive-Bayes approach with CVFDT, which is known as one of the major algorithms to induce a high-accuracy decision tree from time-changing data streams. The proposed improvement, called CVFDTNBC, induces a decision tree as CVFDT does, but contains naive-Bayes classifiers in the leaf nodes of the induced decision tree. The experiment using the artificially generated time-changing data streams shows that CVFDTNBC can induce a decision tree with more accuracy than CVFDT does."}}
{"id": "KhatCik9-SA", "cdate": 1009843200000, "mdate": null, "content": {"title": "Attribute Generation Based on Association Rules", "abstract": "A decision tree is considered to be appropriate (1) if the tree can classify the unseen data accurately, and (2) if the size of the tree is small. One of the approaches to induce such a good decision tree is to add new attributes and their values to enhance the expressiveness of the training data at the data pre-processing stage. There are many existing methods for attribute extraction and construction, but constructing new attributes is still an art. These methods are very time consuming, and some of them need a priori knowledge of the data domain. They are not suitable for data mining dealing with large volumes of data. We propose a novel approach that the knowledge on attributes relevant to the class is extracted as association rules from the training data. The new attributes and the values are generated from the association rules among the originally given attributes. We elaborate on the method and investigate its feature. The effectiveness of our approach is demonstrated through some experiments."}}
{"id": "J7A665mlOu2", "cdate": 978307200000, "mdate": null, "content": {"title": "S3Bagging: Fast Classifier Induction Method with Subsampling and Bagging", "abstract": "In the data mining process, it is often necessary to induce classifiers iteratively by the human analysts complete to extract valuable knowledge from data. Therefore, the data mining tools need to extract valid knowledge from a large amount of data quickly enough in response to the human demand. One of the approaches to answer this request is to reduce the training data size by subsampling. In many cases, the accuracy of the induced classifier becomes worse when the training data is subsampled. We propose S3Bagging (Small SubSampled Bagging) that adopts both subsampling and a method of committee learning, i.e., Bagging. S3Bagging can induce classifier efficiently by reducing the training data size by subsampling and parallel processing. Additionally, the accuracy of the classifier is maintained by aggregating the result of each classifier through the Bagging process. The performance of S3Bagging is investigated by carefully designed experiments."}}
