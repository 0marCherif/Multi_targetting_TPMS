{"id": "zoGm9f4PfD", "cdate": 1696118400000, "mdate": 1707576093907, "content": {"title": "Static and adaptive subspace information fusion for indefinite heterogeneous proximity data", "abstract": ""}}
{"id": "xUWMiXwAaSl", "cdate": 1672531200000, "mdate": 1707576093904, "content": {"title": "Efficient Cross-Domain Federated Learning by MixStyle Approximation", "abstract": "With the advent of interconnected and sensor-equipped edge devices, Federated Learning (FL) has gained significant attention, enabling decentralized learning while maintaining data privacy. However, FL faces two challenges in real-world tasks: expensive data labeling and domain shift between source and target samples. In this paper, we introduce a privacy-preserving, resource-efficient FL concept for client adaptation in hardware-constrained environments. Our approach includes server model pre-training on source data and subsequent fine-tuning on target data via low-end clients. The local client adaptation process is streamlined by probabilistic mixing of instance-level feature statistics approximated from source and target domain data. The adapted parameters are transferred back to the central server and globally aggregated. Preliminary results indicate that our method reduces computational and transmission costs while maintaining competitive performance on downstream tasks."}}
{"id": "MD2kyQ0zLM", "cdate": 1672531200000, "mdate": 1707576093903, "content": {"title": "Unlocking the Potential of Non-PSD Kernel Matrices: A Polar Decomposition-based Transformation for Improved Prediction Models", "abstract": "Kernel functions are a key element in many machine learning methods to capture the similarity between data points. However, a considerable number of these functions do not meet all mathematical requirements to be a valid positive semi-definite kernel, a crucial precondition for kernel-based classifiers such as Support Vector Machines or Kernel Fisher Discriminant classifiers. In this paper, we propose a novel strategy employing a polar decomposition to effectively transform invalid kernel matrices to positive semi-definite matrices, while preserving the topological structure inherent to the data points. Utilizing polar decomposition allows the effective transformation of indefinite kernel matrices from Krein space to positive semi-definite matrices in Hilbert space, thereby providing an efficient out-of-sample extension for new unseen data and enhancing kernel method applicability across diverse classification tasks. We evaluate our approach on a variety of benchmark datasets and demonstrate its superiority over competitive methods."}}
{"id": "vzPgga8dCW", "cdate": 1648711281822, "mdate": 1648711281822, "content": {"title": "Sparsification of core set models in non-metric supervised learning", "abstract": "Supervised learning employing positive semi definite kernels has gained wide attraction and lead to a\nvariety of successful machine learning approaches. The restriction to positive semi definite kernels and\na hilbert space is common to simplify the mathematical derivations of the respective learning methods,\nbut is also limiting because more recent research indicates that non-metric, and therefore non positive\nsemi definite, data representations are often more effective. This challenge is addressed by multiple approaches and recently dedicated algorithms for so called indefinite learning have been proposed. Along\nthis line, the Krein\u02d8 space Support Vector Machine (KSVM) and variants are very efficient classifiers for\nindefinite learning problems, but with a non-sparse decision function. This very dense decision function\nprevents practical applications due to a costly out of sample extension. We focus on this problem and\nprovide two post processing techniques to sparsify models as obtained by a Krein\u02d8 space SVM approach.\nIn particular we consider the indefinite Core Vector Machine and indefinite Core Vector Regression Machine which are both efficient for psd kernels, but suffer from the same dense decision function, if the\nKrein\u02d8 space approach is used. We evaluate the influence of different levels of sparsity and employ a Nystr\u00f6m approach to address large scale problems. Experiments show that our algorithm is similar efficient\nas the non-sparse Krein\u02d8 space Support Vector Machine but with substantially lower costs, such that also\nproblems of larger scale can be processed."}}
{"id": "Fs6c10iDS6R", "cdate": 1648711237218, "mdate": 1648711237218, "content": {"title": "Indefinite proximity learning - A review", "abstract": "Efficient learning of a data analysis task strongly depends on the data representation.\nMost methods rely on (symmetric) similarity or dissimilarity representations by means\nof metric inner products or distances, providing easy access to powerful mathematical\nformalisms like kernel or branch-and-bound approaches. Similarities and dissimilarities\nare however often naturally obtained by non-metric proximity measures which can not\neasily be handled by classical learning algorithms. In the last years major efforts have\nbeen undertaken to provide approaches which can either directly be used for such data\nor to make standard methods available for these type of data. We provide a comprehensive survey for the field of learning with non-metric proximities. First we introduce the\nformalism used in non-metric spaces and motivate specific treatments for non-metric\nproximity data. Secondly we provide a systematization of the various approaches. For\neach category of approaches we provide a comparative discussion of the individual algorithms and address complexity issues and generalization properties. In a summarizing chapter we provide a larger experimental study for the majority of the algorithms on\nstandard datasets. We also address the problem of large scale proximity learning which\nis often overlooked in this context and of major importance to make the method relevant in practice. The algorithms discussed in this paper are in general applicable\nfor proximity based clustering, one-class classification, classification, regression or\nembedding approaches. In the experimental part we focus on classification tasks."}}
{"id": "YJ_fqyUX47_", "cdate": 1648711168251, "mdate": 1648711168251, "content": {"title": "Data-driven supervised learning for life science data", "abstract": "Life science data are often encoded in a non-standard way by means of alpha-numeric sequences, graph\nrepresentations, numerical vectors of variable length or other formats. Domain specific or data-driven\nsimilarity measures like alignment functions have been employed with great success. The vast majority\nof more complex data analysis algorithms require fixed length vectorial input data, asking for substantial\npreprocessing of life science data. Data-driven measures are widely ignored in favour of simple encodings.\nThese preprocessing steps are not always easy to perform nor particular effective, with a potential loss of\ninformation and interpretability. We present some strategies and concepts of how to employ data-driven\nsimilarity measures in the life science context and other complex biological systems. In particular, we show\nhow to use data-driven similarity measures effectively in standard learning algorithms."}}
{"id": "teaXhczdkmG", "cdate": 1648710649640, "mdate": 1648710649640, "content": {"title": "Learning with interpretable models", "abstract": "The current decades in the field of computer science and machine learning are dominated by\nan increasing amount of electronically available data also known as the big data challenge\n[75, 38].\nWith projects like the genome sequencing initiative, huge amounts of sequence data are\navailable, social media generate continuously large amounts of data in different formats,\nand many other sources provide data and challenging data analysis and interpretation tasks\n[36, 82, 43, 68, 6, 79].\nThe interpretation of these data and its structuring in form of compact models is widely\nconsidered as important to access these data in an efficient manner.\nTo get access to these sources, efficient data analysis algorithms and models are necessary.\nOne can consider an algorithm to be efficient, if it successfully tackles the problem and\nprovides a model at acceptable costs with accurate results and good generalization ability\non new data. The costs can be defined by means of the necessary training time, by the\namount of memory necessary to obtain and store the model, but also by evaluating the time\na model needs to give a prediction on a new item.\nThe problem of large scale data analysis has been addressed already by different approximation or sparsity strategies, heuristics or by novel mathematical concepts exploring e.g.\ngeometric properties of the data space, or by incorporating additional domain knowledge\nsimplifying the original problem [59, 47, 111, 121, 12]. Also the extraction of simplifying\nrules from more complex models has been discussed [42, 56]. Here multiple disciplines and\nresearch fields often have to work together in an interdisciplinary way to define appropriate\nsolutions. While the domain expert, may be e.g. a biologist or clinician, the measurement\nsystem is best understood by a physicist and all have to work together with mathematicians\nor computer scientists to define an efficient algorithm for the analysis of the data."}}
{"id": "sY9OC-JlEh", "cdate": 1640995200000, "mdate": 1707576094088, "content": {"title": "Memory Efficient Kernel Approximation for Non-Stationary and Indefinite Kernels", "abstract": "Matrix approximations are a key element in large-scale algebraic machine learning approaches. The recently pro-posed method MEKA [1] effectively employs two common assumptions in Hilbert spaces: the low-rank property of an inner product matrix obtained from a shift-invariant kernel function and a data compactness hypothesis by means of an inherent block-cluster structure. In this work, we extend MEKA to be applicable not only for shift-invariant kernels but also for non-stationary kernels like polynomial kernels and an extreme learning kernel. We also address in detail how to handle non-positive semi-definite kernel functions within MEKA, either caused by the approximation itself or by the intentional use of general kernel functions. We present a Lanczos-based estimation of a spectrum shift to develop a stable positive semi-definite MEKA approximation, also usable in classical convex optimization frameworks. Furthermore, we support our findings with theoretical considerations and a variety of experiments on synthetic and real-world data."}}
{"id": "p4yHgBdWvSs", "cdate": 1640995200000, "mdate": 1707576094080, "content": {"title": "Federated Learning - Methods, Applications and beyond", "abstract": "In recent years the applications of machine learning models have increased rapidly, due to the large amount of available data and technological progress.While some domains like web analysis can benefit from this with only minor restrictions, other fields like in medicine with patient data are strongerregulated. In particular \\emph{data privacy} plays an important role as recently highlighted by the trustworthy AI initiative of the EU or general privacy regulations in legislation. Another major challenge is, that the required training \\emph{data is} often \\emph{distributed} in terms of features or samples and unavailable for classicalbatch learning approaches. In 2016 Google came up with a framework, called \\emph{Federated Learning} to solve both of these problems. We provide a brief overview on existing Methods and Applications in the field of vertical and horizontal \\emph{Federated Learning}, as well as \\emph{Fderated Transfer Learning}."}}
{"id": "kj3lmn-gta", "cdate": 1640995200000, "mdate": 1707576093909, "content": {"title": "Adaptive multi-modal positive semi-definite and indefinite kernel fusion for binary classification", "abstract": ""}}
