{"id": "_qvv2FNTAf", "cdate": 1672531200000, "mdate": 1696037249391, "content": {"title": "AdKDD 2023", "abstract": "The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user's path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60% agreed that AdKDD is the reason they attended KDD, and over 90% indicated they would attend next year. The 2023 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising."}}
{"id": "D6Kp-2Ezlj", "cdate": 1672531200000, "mdate": 1696037249392, "content": {"title": "Bush Detection for Vision-based UGV Guidance in Blueberry Orchards: Data Set and Methods", "abstract": "Object detection has reached strong performance in the last decade, having seen its usage spreading to various application areas, such as medicine, transportation, sports, and others. However, one of the more underutilized areas where advanced detection methods have yet to fully fulfill their promise is in the area of agriculture, where a strong potential exists for applying learned models to achieve practical, real-world impact affecting a large number of people. In this work, we focus on this application area and consider the problem of orchard guidance for ground robots, focusing on obstacle and plant detection from RGB camera images. First, we present an overview of public data sets used to train models to detect relevant objects from camera images and other sensor inputs. Then we introduce a novel data set collected in blueberry orchards that contains camera images in various conditions and provides blueberry bushes as targets for detection. The introduced data set provides the research community with a novel task of blueberry bush detection, which was not commonly considered thus far due to the lack of relevant data sets. We describe a detailed analysis of the data set, and finally provide an experimental study with several state-of-the-art deep object detection models, that set a baseline for the performance on this novel data set. The data set is made available online, enriching the variability of the existing tasks in the field and supporting further development of smart agriculture applications."}}
{"id": "jdYPIxMq2J", "cdate": 1640995200000, "mdate": 1674672490406, "content": {"title": "Detection of Active Emergency Vehicles using Per-Frame CNNs and Output Smoothing", "abstract": "While inferring common actor states (such as position or velocity) is an important and well-explored task of the perception system aboard a self-driving vehicle (SDV), it may not always provide sufficient information to the SDV. This is especially true in the case of active emergency vehicles (EVs), where light-based signals also need to be captured to provide a full context. We consider this problem and propose a sequential methodology for the detection of active EVs, using an off-the-shelf CNN model operating at a frame level and a downstream smoother that accounts for the temporal aspect of flashing EV lights. We also explore model improvements through data augmentation and training with additional hard samples."}}
{"id": "ZO1GmaE8cg", "cdate": 1640995200000, "mdate": 1674672489432, "content": {"title": "Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving", "abstract": "We present an end-to-end method for object detection and trajectory prediction utilizing multi-view representations of LiDAR returns and camera images. In this work, we recognize the strengths and weaknesses of different view representations, and we propose an efficient and generic fusing method that aggregates benefits from all views. Our model builds on a state-of-the-art Bird\u2019s-Eye View (BEV) network that fuses voxelized features from a sequence of historical LiDAR data as well as rasterized high-definition map to perform detection and prediction tasks. We extend this model with additional LiDAR Range-View (RV) features that use the raw LiDAR information in its native, non-quantized representation. The RV feature map is projected into BEV and fused with the BEV features computed from LiDAR and high-definition map. The fused features are then further processed to output the final detections and trajectories, within a single end-to-end trainable network. In addition, the RV fusion of LiDAR and camera is performed in a straightforward and computationally efficient manner using this framework. The proposed multi-view fusion approach improves the state-of-the-art on proprietary large-scale real-world data collected by a fleet of self-driving vehicles, as well as on the public nuScenes data set with minimal increases on the computational cost."}}
{"id": "TpzgNakwnEE", "cdate": 1640995200000, "mdate": 1681708471627, "content": {"title": "AdKDD 2022", "abstract": "An average consumer spends 8+ hours a day across all devices interacting with online content almost entirely sponsored by advertisements. At over $450B global market size in 2022 and expected to pass $1T by 2027, online advertising has already surpassed traditional ads in global spend. Moreover, computational advertising in particular is perhaps the most visible and ubiquitous application of machine learning and one that interacts directly with consumers. When done right, ads help us enrich our lives and creep us out when done badly. Looking at the published literature over the last few years, many researchers might consider computational advertising as a mature field. Yet, the opposite is true. The field is evolving, however, from ads controlled by monolithic publishers and randomly rotating banner ads to highly personalized content experiences in news feeds on mobile devices and even on TV-all utilizing data amassed from petabytes of stored user data. Ads are far from done."}}
{"id": "BVNW82wIDdk", "cdate": 1640995200000, "mdate": 1668777495687, "content": {"title": "Convolutions for Spatial Interaction Modeling", "abstract": "In many different fields interactions between objects play a critical role in determining their behavior. Graph neural networks (GNNs) have emerged as a powerful tool for modeling interactions, although often at the cost of adding considerable complexity and latency. In this paper, we consider the problem of spatial interaction modeling in the context of predicting the motion of actors around autonomous vehicles, and investigate alternatives to GNNs. We revisit 2D convolutions and show that they can demonstrate comparable performance to graph networks in modeling spatial interactions with lower latency, thus providing an effective and efficient alternative in time-critical systems. Moreover, we propose a novel interaction loss to further improve the interaction modeling of the considered methods."}}
{"id": "y31iYYJ8yJO", "cdate": 1609459200000, "mdate": 1654723611403, "content": {"title": "Autonomous Vehicle Vision 2021: ICCV Workshop Summary", "abstract": "This paper summarizes the 2nd Autonomous Vehicle Vision (AVVision) workshop (avvision.xyz/iccv21), organized virtually in conjunction with ICCV 2021. The organizers invited seven experts from both industry and academia to deliver keynote talks, discussing the state-of-the-art and challenges in the field of autonomous driving. A total of 27 papers were accepted for publication in the ICCV 2021 proceedings (IEEE Xplore and CVF open access), resulting in an acceptance rate of 50.9%. In addition to serving as a workshop summary and a brief overview of the existing challenges, this paper also presents how these challenges were addressed by the authors through their pro-posed solutions."}}
{"id": "rUqrkLXj2hF", "cdate": 1609459200000, "mdate": 1674672489752, "content": {"title": "Ellipse Loss for Scene-Compliant Motion Prediction", "abstract": "Motion prediction is a critical part of self-driving technology, responsible for inferring future behavior of traffic actors in autonomous vehicle\u2019s surroundings. In order to ensure safe and efficient operations, prediction models need to output accurate trajectories that obey the map constraints. In this paper, we address this task and propose a novel ellipse loss that allows the models to better reason about scene compliance and predict more realistic trajectories. Ellipse loss penalizes off-road predictions directly in a supervised manner, by projecting the output trajectories into the top-down map frame using a differentiable trajectory rasterizer module. Moreover, it takes into account actor dimensions and orientation, providing more direct training signals to the model. We applied ellipse loss to a recently proposed state-of-the-art joint detection-prediction model to showcase its benefits. Evaluation on large-scale autonomous driving data strongly indicates that the method allows for more accurate and more realistic trajectory predictions."}}
{"id": "l-HWsIwJZCY", "cdate": 1609459200000, "mdate": null, "content": {"title": "Investigating the Effect of Sensor Modalities in Multi-Sensor Detection-Prediction Models", "abstract": "Detection of surrounding objects and their motion prediction are critical components of a self-driving system. Recently proposed models that jointly address these tasks rely on a number of sensors to achieve state-of-the-art performance. However, this increases system complexity and may result in a brittle model that overfits to any single sensor modality while ignoring others, leading to reduced generalization. We focus on this important problem and analyze the contribution of sensor modalities towards the model performance. In addition, we investigate the use of sensor dropout to mitigate the above-mentioned issues, leading to a more robust, better-performing model on real-world driving data."}}
{"id": "i8tqPlmeyu", "cdate": 1609459200000, "mdate": 1674672489718, "content": {"title": "Uncertainty-Aware Estimation of Vehicle Orientation for Self-Driving Applications", "abstract": "Object detection is a critical component of a self-driving system, tasked with inferring the current states of the surrounding traffic actors. While there exist a number of studies on the problem of inferring the position and shape of vehicle actors, understanding actors' orientation remains a challenge for existing state-of-the-art detectors. Orientation is an important property for downstream modules of an autonomous system, particularly relevant for motion prediction of stationary or reversing actors where current approaches struggle. We focus on this task and present a general method that allows us to more accurately infer vehicle orientations, which we apply to the state-of-the-art models that perform joint object detection and motion prediction. In addition, the approach is able to quantify prediction uncertainty, outputting the probability that the inferred orientation is flipped, which allows for improved motion prediction and safer autonomous operations. Empirical results show the benefits of the approach, obtaining state-of-the-art performance on the open-sourced nuScenes data set."}}
