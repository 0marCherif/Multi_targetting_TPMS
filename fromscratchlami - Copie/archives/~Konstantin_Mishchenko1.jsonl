{"id": "vtv83s2Ps9", "cdate": 1664731448583, "mdate": null, "content": {"title": "Parameter Free Dual Averaging: Optimizing Lipschitz Functions in a Single Pass", "abstract": "Both gradient descent and dual averaging for convex Lipschitz functions\nhave convergence rates that are highly dependent on the choice of learning\nrate. Even when the Lipschitz constant is known, setting the learning rate to achieve the optimal convergence rate requires knowing a bound on the distance from the initial point to the solution set $D$. A number\nof approaches are known that relax this requirement, but they either\nrequire line searches, restarting (hyper-parameter grid search), or do not derive\nfrom the gradient descent or dual averaging frameworks (coin-betting).\nIn this work we describe a single pass method, with no back-tracking or line searches, \nderived from dual averaging,\nwhich does not require knowledge of $D$ yet asymptotically achieves\nthe optimal rate of convergence for the complexity class of Convex\nLipschitz functions.\n"}}
{"id": "4XP0ZuQKXmV", "cdate": 1652737683455, "mdate": null, "content": {"title": "Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays", "abstract": "The existing analysis of asynchronous stochastic gradient descent (SGD) degrades dramatically when any delay is large, giving the impression that performance depends primarily on the delay. On the contrary, we prove much better guarantees for the same asynchronous SGD algorithm regardless of the delays in the gradients, depending instead just on the number of parallel devices used to implement the algorithm. Our guarantees are strictly better than the existing analyses, and we also argue that asynchronous SGD outperforms synchronous minibatch SGD in the settings we consider. For our analysis, we introduce a novel recursion based on ``virtual iterates'' and delay-adaptive stepsizes, which allow us to derive state-of-the-art guarantees for both convex and non-convex objectives. "}}
{"id": "Savbs0BHHlq", "cdate": 1640995200000, "mdate": 1645725138857, "content": {"title": "Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization", "abstract": "We present a theoretical study of server-side optimization in federated learning. Our results are the first to show that the widely popular heuristic of scaling the client updates with an extra parameter is very useful in the context of Federated Averaging (FedAvg) with local passes over the client data. Each local pass is performed without replacement using Random Reshuffling, which is a key reason we can show improved complexities. In particular, we prove that whenever the local stepsizes are small, and the update direction is given by FedAvg in conjunction with Random Reshuffling over all clients, one can take a big leap in the obtained direction and improve rates for convex, strongly convex, and non-convex objectives. In particular, in non-convex regime we get an enhancement of the rate of convergence from $\\mathcal{O}\\left(\\varepsilon^{-3}\\right)$ to $\\mathcal{O}\\left(\\varepsilon^{-2}\\right)$. This result is new even for Random Reshuffling performed on a single node. In contrast, if the local stepsizes are large, we prove that the noise of client sampling can be controlled by using a small server-side stepsize. To the best of our knowledge, this is the first time that local steps provably help to overcome the communication bottleneck. Together, our results on the advantage of large and small server-side stepsizes give a formal justification for the practice of adaptive server-side optimization in federated learning. Moreover, we consider a variant of our algorithm that supports partial client participation, which makes the method more practical."}}
{"id": "pFyXqxChZc", "cdate": 1632875443909, "mdate": null, "content": {"title": "IntSGD: Adaptive Floatless Compression of Stochastic Gradients", "abstract": "We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by (Sapio et al., 2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and non-smooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance."}}
{"id": "5la5tka8a4-", "cdate": 1621630315684, "mdate": null, "content": {"title": "Proximal and Federated Random Reshuffling", "abstract": "Random Reshuffling (RR), also known as Stochastic Gradient Descent (SGD) without replacement, is a popular and theoretically grounded method for finite-sum minimization. We propose two new algorithms: Proximal and Federated Random Reshuffling (ProxRR and FedRR). The first algorithm, ProxRR, solves composite convex finite-sum minimization problems in which the objective is the sum of a (potentially non-smooth) convex regularizer and an average of $n$ smooth objectives. ProxRR evaluates the proximal operator once per epoch only. When the proximal operator is expensive to compute, this small difference makes ProxRR up to $n$ times faster than algorithms that evaluate the proximal operator in every iteration, such as proximal (stochastic) gradient descent. We give examples of practical optimization tasks where the proximal operator is difficult to compute and ProxRR has a clear advantage. One such task is federated or distributed optimization, where the evaluation of the proximal operator corresponds to communication across the network. We obtain our second algorithm, FedRR, as a special case of ProxRR applied to federated optimization, and prove it has a smaller communication footprint than either distributed gradient descent or Local SGD. Our theory covers both constant and decreasing stepsizes, and allows for importance resampling schemes that can improve conditioning, which may be of independent interest. Our theory covers both convex and nonconvex regimes. Finally, we corroborate our results with experiments on real data sets."}}
{"id": "hEj6-8ol77W", "cdate": 1609459200000, "mdate": null, "content": {"title": "IntSGD: Floatless Compression of Stochastic Gradients", "abstract": "We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by Sapio et al. (2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and non-smooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance."}}
{"id": "SU9-iABSSeq", "cdate": 1609459200000, "mdate": 1645725138861, "content": {"title": "Regularized Newton Method with Global O(1/k2) Convergence", "abstract": "We present a Newton-type method that converges fast from any initialization and for arbitrary convex objectives with Lipschitz Hessians. We achieve this by merging the ideas of cubic regularization with a certain adaptive Levenberg--Marquardt penalty. In particular, we show that the iterates given by $x^{k+1}=x^k - \\bigl(\\nabla^2 f(x^k) + \\sqrt{H\\|\\nabla f(x^k)\\|} \\mathbf{I}\\bigr)^{-1}\\nabla f(x^k)$, where $H>0$ is a constant, converge globally with a $\\mathcal{O}(\\frac{1}{k^2})$ rate. Our method is the first variant of Newton's method that has both cheap iterations and provably fast global convergence. Moreover, we prove that locally our method converges superlinearly when the objective is strongly convex. To boost the method's performance, we present a line search procedure that does not need hyperparameters and is provably efficient."}}
{"id": "Dp208ALO8Hs", "cdate": 1609459200000, "mdate": null, "content": {"title": "Proximal and Federated Random Reshuffling", "abstract": "Random Reshuffling (RR), also known as Stochastic Gradient Descent (SGD) without replacement, is a popular and theoretically grounded method for finite-sum minimization. We propose two new algorithms: Proximal and Federated Random Reshuffing (ProxRR and FedRR). The first algorithm, ProxRR, solves composite convex finite-sum minimization problems in which the objective is the sum of a (potentially non-smooth) convex regularizer and an average of $n$ smooth objectives. We obtain the second algorithm, FedRR, as a special case of ProxRR applied to a reformulation of distributed problems with either homogeneous or heterogeneous data. We study the algorithms' convergence properties with constant and decreasing stepsizes, and show that they have considerable advantages over Proximal and Local SGD. In particular, our methods have superior complexities and ProxRR evaluates the proximal operator once per epoch only. When the proximal operator is expensive to compute, this small difference makes ProxRR up to $n$ times faster than algorithms that evaluate the proximal operator in every iteration. We give examples of practical optimization tasks where the proximal operator is difficult to compute and ProxRR has a clear advantage. Finally, we corroborate our results with experiments on real data sets."}}
{"id": "J5rIjpfyi7", "cdate": 1599206150575, "mdate": null, "content": {"title": "Distributed Learning with Compressed Gradient Differences", "abstract": "Training large machine learning models requires a distributed computing approach, with communication of the model updates being the bottleneck. For this reason, several methods based on the compression (e.g., sparsification and/or quantization) of updates were recently proposed, including QSGD (Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et al., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods are able to learn the gradients, which renders them incapable of converging to the true optimum in the batch mode, incompatible with non-smooth regularizers, and slows down their convergence. In this work we propose a new distributed learning method --- DIANA --- which resolves these issues via compression of gradient differences. We perform a theoretical analysis in the strongly convex and nonconvex settings and show that our rates are superior to existing rates. Our analysis of block-quantization and differences between \u21132 and \u2113\u221e quantization closes the gaps in theory and practice. Finally, by applying our analysis technique to TernGrad, we establish the first convergence rate for this method."}}
{"id": "qbXe3k5xk5R", "cdate": 1577836800000, "mdate": null, "content": {"title": "DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate", "abstract": "In this paper, we consider distributed algorithms for solving the empirical risk minimization problem under the master/worker communication model. We develop a distributed asynchronous quasi-Newton..."}}
