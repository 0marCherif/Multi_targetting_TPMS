{"id": "8T7uAP6yz0", "cdate": 1683915569348, "mdate": null, "content": {"title": "Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners", "abstract": "Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and effective manner when the learner uses a small value for its discounting rate and a large value for its learning rate. However, they still are suboptimal. We also find that providing people with real-time updates of how possible feedback would affect the Q-learner's internal states weakly helps them teach. Our results reveal how people teach using evaluative feedback and provide guidance for how engineers should design machine agents in a manner that is intuitive for people."}}
{"id": "uENysQ2mL5", "cdate": 1674765086448, "mdate": null, "content": {"title": "The Sample Complexity of Teaching by Reinforcement on Q-Learning", "abstract": "We study the sample complexity of teaching, termed as \u201cteaching dimension\u201d (TDim) in the literature, for the teachingby-reinforcement paradigm, where the teacher guides the student through rewards. This is distinct from the teachingby-demonstration paradigm motivated by robotics applications, where the teacher teaches by providing demonstrations of state/action trajectories. The teaching-by-reinforcement paradigm applies to a wider range of real-world settings where a demonstration is inconvenient, but has not been studied systematically. In this paper, we focus on a specific family of reinforcement learning algorithms, Q-learning, and characterize the TDim under different teachers with varying control power over the environment, and present matching optimal teaching algorithms. Our TDim results provide the minimum number of samples needed for reinforcement learning, and we discuss their connections to standard PAC-style RL sample complexity and teaching-by-demonstration sample complexity results. Our teaching algorithms have the potential to speed up RL agent learning in applications where a helpful teacher is available."}}
{"id": "wNS3hgT1Vzw", "cdate": 1672531200000, "mdate": 1695397693434, "content": {"title": "Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP", "abstract": "In this paper, we study representation learning in partially observable Markov Decision Processes (POMDPs), where the agent learns a decoder function that maps a series of high-dimensional raw obse..."}}
{"id": "sHjWr_58tn3", "cdate": 1672531200000, "mdate": 1695397693373, "content": {"title": "Provable Benefits of Representational Transfer in Reinforcement Learning", "abstract": "We study the problem of representational transfer in RL, where an agent first pretrains in a number of \\emph{source tasks} to discover a shared representation, which is subsequently used to learn a..."}}
{"id": "l3sM9AxIlNM", "cdate": 1672531200000, "mdate": 1695397693364, "content": {"title": "Representation Learning for Low-rank General-sum Markov Games", "abstract": ""}}
{"id": "esgKxYoose", "cdate": 1672531200000, "mdate": 1695397693367, "content": {"title": "Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP", "abstract": "In this paper, we study representation learning in partially observable Markov Decision Processes (POMDPs), where the agent learns a decoder function that maps a series of high-dimensional raw observations to a compact representation and uses it for more efficient exploration and planning. We focus our attention on the sub-classes of \\textit{$\\gamma$-observable} and \\textit{decodable POMDPs}, for which it has been shown that statistically tractable learning is possible, but there has not been any computationally efficient algorithm. We first present an algorithm for decodable POMDPs that combines maximum likelihood estimation (MLE) and optimism in the face of uncertainty (OFU) to perform representation learning and achieve efficient sample complexity, while only calling supervised learning computational oracles. We then show how to adapt this algorithm to also work in the broader class of $\\gamma$-observable POMDPs."}}
{"id": "UWYO6fVpyp", "cdate": 1672531200000, "mdate": 1695397693370, "content": {"title": "Byzantine-Robust Online and Offline Distributed Reinforcement Learning", "abstract": "We consider a distributed reinforcement learning setting where multiple agents separately explore the environment and communicate their experiences through a central server. However, $\\alpha$-fract..."}}
{"id": "NQrU6DLMuS", "cdate": 1664994273936, "mdate": null, "content": {"title": "Provable Benefits of Representational Transfer in Reinforcement Learning", "abstract": "We study the problem of representational transfer in RL, where an agent first pretrains offline in a number of source tasks to discover a shared representation, which is subsequently used to learn a good policy online in a target task. We propose a new notion of task relatedness between source and target tasks and develop a novel approach for representational transfer under this assumption. Concretely, we show that given generative access to a set of source tasks, we can discover a representation, using which subsequent linear RL techniques quickly converge to a near-optimal policy, with only online access to the target task. \nThe sample complexity is close to knowing the ground truth features in the target task and comparable to prior representation learning results in the source tasks. We complement our positive results with lower bounds without generative access and validate our findings with empirical evaluation on rich observation MDPs that requires deep exploration."}}
{"id": "_J1uezt3PYI", "cdate": 1664642336870, "mdate": 1664642336870, "content": {"title": "Training Set Camouflage", "abstract": "We introduce a form of steganography in the domain of machine learning which we call training set camouflage. Imagine Alice has a\ntraining set on an illicit machine learning classification task. Alice wants\nBob (a machine learning system) to learn the task. However, sending\neither the training set or the trained model to Bob can raise suspicion if\nthe communication is monitored. Training set camouflage allows Alice to\ncompute a second training set on a completely different \u2013 and seemingly\nbenign \u2013 classification task. By construction, sending the second training set will not raise suspicion. When Bob applies his standard (public)\nlearning algorithm to the second training set, he approximately recovers\nthe classifier on the original task. Training set camouflage is a novel form\nof steganography in machine learning. We formulate training set camouflage as a combinatorial bilevel optimization problem and propose solvers\nbased on nonlinear programming and local search. Experiments on real\nclassification tasks demonstrate the feasibility of such camouflage."}}
{"id": "49N06mWPFUm", "cdate": 1663850426318, "mdate": null, "content": {"title": "Provably Efficient Reinforcement Learning for Online Adaptive Influence Maximization", "abstract": "Online influence maximization aims to maximize the influence spread of a content in a social network with an unknown network model by selecting a few seed nodes. Recent studies followed a non-adaptive setting, where the seed nodes are selected before the start of the diffusion process and network parameters are updated when the diffusion stops. We consider an adaptive version of content-dependent online influence maximization problem where the seed nodes are sequentially activated based on real-time feedback. In this paper, we formulate the problem as an infinite-horizon discounted MDP under a linear diffusion process and present a model-based reinforcement learning solution. Our algorithm maintains a network model estimate and selects seed users adaptively, exploring the social network while improving the optimal policy optimistically. We establish $\\widetilde O(\\sqrt{T})$ regret bound for our algorithm. Empirical evaluations on synthetic and real-world networks demonstrate the efficiency of our algorithm. "}}
