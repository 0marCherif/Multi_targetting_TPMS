{"id": "2SF_HsfhjdB", "cdate": 1676472365100, "mdate": null, "content": {"title": "Fairness-Aware Data Valuation for Supervised Learning", "abstract": "Data valuation is a ML field that studies the value of training instances towards a given predictive task. Although data bias is one of the main sources of downstream model unfairness, previous work in data valuation does not consider how training instances may influence both performance and fairness of ML models. Thus, we propose Fairness-Aware Data vauatiOn (FADO), a data valuation framework that can be used to incorporate fairness concerns into a series of ML-related tasks (e.g., data pre-processing, exploratory data analysis, active learning). We propose an entropy-based data valuation metric suited to address our two-pronged goal of maximizing both performance and fairness, which is more computationally efficient than existing metrics. We then show how FADO can be applied as the basis for unfairness mitigation pre-processing techniques. Our methods achieve promising results \u2014 up to a 40 p.p. improvement in fairness at a less than 1 p.p. loss in performance compared to a baseline \u2014 and promote fairness in a data-centric way, where a deeper understanding of data quality takes center stage."}}
{"id": "7IyjmYBxokW", "cdate": 1667393652157, "mdate": null, "content": {"title": "Distinguishing Cause from Effect on Categorical Data: The Uniform Channel Model", "abstract": "Distinguishing cause from effect using observations of a pair of random variables is a core problem in causal discovery. Most approaches proposed for this task, namely additive noise models (ANM), are only adequate for quantitative data. We propose a criterion to address the cause-effect problem with categorical variables (living in sets with no meaningful order), inspired by seeing a conditional probability mass function (pmf) as a discrete memoryless channel. We select as the most likely causal direction the one in which the conditional pmf is closer to a uniform channel (UC). The rationale is that, in a UC, as in an ANM, the conditional entropy (of the effect given the cause) is independent of the cause distribution, in agreement with the principle of independence of cause and mechanism. Our approach, which we call the uniform channel model (UCM), thus extends the ANM rationale to categorical variables. To assess how close a conditional pmf (estimated from data) is to a UC, we use statistical testing, supported by a closed-form estimate of a UC channel. On the theoretical front, we prove identifiability of the UCM and show its equivalence with a structural causal model with a low-cardinality exogenous variable. Finally, the proposed method compares favorably with recent state-of-the-art alternatives in experiments on synthetic, benchmark, and real data."}}
{"id": "hDrn2Dmb7_I", "cdate": 1635261619891, "mdate": null, "content": {"title": "Differentiable Causal Discovery Under Latent Interventions", "abstract": "Recent work has shown promising results in causal discovery by leveraging interventional data with gradient-based methods, even when the intervened variables are unknown. However, previous work assumes that the correspondence between samples and interventions is known, which is often unrealistic. We envision a scenario with an extensive dataset sampled from multiple intervention distributions and one observation distribution, but where we do not know which distribution originated each sample and how the intervention affected the system, \\textit{i.e.}, interventions are entirely latent. We propose a method based on neural networks and variational inference that addresses this scenario by framing it as learning a shared causal graph among a infinite mixture (under a Dirichlet process prior) of intervention structural causal models . Experiments with synthetic and real data show that our approach and its semi-supervised variant are able to discover causal relations in this challenging scenario. "}}
{"id": "wfY7MFok5o6", "cdate": 1609459200000, "mdate": null, "content": {"title": "Block-Gaussian-Mixture Priors for Hyperspectral Denoising and Inpainting", "abstract": "This article proposes a denoiser for hyperspectral (HS) images that consider, not only spatial features, but also spectral features. The method starts by projecting the noisy (observed) HS data onto a lower dimensional subspace and then learns a Gaussian mixture model (GMM) from 3-D patches or blocks extracted from the projected data cube. Afterward, the minimum mean squared error (MMSE) estimates of the blocks are obtained in closed form and returned to their original positions. Experiments show that the proposed algorithm is able to outperform other state-of-the-art methods under Gaussian and Poissonian noise and to reconstruct high-quality images in the presence of stripes."}}
{"id": "_KvVpsEqT3", "cdate": 1609459200000, "mdate": null, "content": {"title": "Distributed Picard Iteration", "abstract": "The Banach-Picard iteration is widely used to find fixed points of locally contractive (LC) maps. This paper extends the Banach-Picard iteration to distributed settings; specifically, we assume the map of which the fixed point is sought to be the average of individual (not necessarily LC) maps held by a set of agents linked by a communication network. An additional difficulty is that the LC map is not assumed to come from an underlying optimization problem, which prevents exploiting strong global properties such as convexity or Lipschitzianity. Yet, we propose a distributed algorithm and prove its convergence, in fact showing that it maintains the linear rate of the standard Banach-Picard iteration for the average LC map. As another contribution, our proof imports tools from perturbation theory of linear operators, which, to the best of our knowledge, had not been used before in the theory of distributed computation."}}
{"id": "JrRj2w5VI-", "cdate": 1609459200000, "mdate": null, "content": {"title": "On the Improvement of Feature Selection Techniques: The Fitness Filter", "abstract": ""}}
{"id": "C2nr-4elBV", "cdate": 1582750154365, "mdate": null, "content": {"title": "Can auto-encoders help with filling missing data?", "abstract": "This paper introduces an approach to filling in missing data based on deep auto-encoder models, adequate to high-dimensional data exhibiting complex dependencies, such as images. The method exploits the properties of auto-encoders' vector fields, which allows to approximate the gradient of the log-density from its reconstruction error, based on which we propose a projected gradient ascent algorithm to obtain the  conditionally most probable estimate of the missing values. Experiments performed on benchmark datasets show that imputations produced by our model are sharp and realistic."}}
{"id": "p7J2BlvOql", "cdate": 1579955664457, "mdate": null, "content": {"title": "Multi-Task Deep Learning: Simultaneous Segmentation and Survival Analysis via Cox Proportional Hazards Regression", "abstract": "Multi-task learning has taken an important place as a tool for medical image analysis, namely for the development of predictive models of disease. This study aims at developing a new deep learning model for simultaneous segmentation and survival regression, using a version of the Cox model to support the learning process. We use a combination of a 2D U-net and a residual network to minimize a combined loss function for segmentation and survival regression. To validate our method, we created a simple synthetic data set - the model segments circles of different sizes and regresses the area of circles. The main motivation of this work is to create a workflow for segmentation and regression for medical images application: in specific, we use this model to segment lesions or organs and regress clinical outcomes as overall or disease-free survival."}}
{"id": "xMqrf18sCNE", "cdate": 1577836800000, "mdate": null, "content": {"title": "Variational Mixture of Normalizing Flows", "abstract": "In the past few years, deep generative models, such as generative adversarial networks \\autocite{GAN}, variational autoencoders \\autocite{vaepaper}, and their variants, have seen wide adoption for the task of modelling complex data distributions. In spite of the outstanding sample quality achieved by those early methods, they model the target distributions \\emph{implicitly}, in the sense that the probability density functions induced by them are not explicitly accessible. This fact renders those methods unfit for tasks that require, for example, scoring new instances of data with the learned distributions. Normalizing flows have overcome this limitation by leveraging the change-of-variables formula for probability density functions, and by using transformations designed to have tractable and cheaply computable Jacobians. Although flexible, this framework lacked (until recently \\autocites{semisuplearning_nflows, RAD}) a way to introduce discrete structure (such as the one found in mixtures) in the models it allows to construct, in an unsupervised scenario. The present work overcomes this by using normalizing flows as components in a mixture model and devising an end-to-end training procedure for such a model. This procedure is based on variational inference, and uses a variational posterior parameterized by a neural network. As will become clear, this model naturally lends itself to (multimodal) density estimation, semi-supervised learning, and clustering. The proposed model is illustrated on two synthetic datasets, as well as on a real-world dataset. Keywords: Deep generative models, normalizing flows, variational inference, probabilistic modelling, mixture models."}}
{"id": "xLBS9TOAZlE", "cdate": 1577836800000, "mdate": null, "content": {"title": "Iterative Imputation of Missing Data Using Auto-Encoder Dynamics", "abstract": "This paper introduces an approach to missing data imputation based on deep auto-encoder models, adequate to high-dimensional data exhibiting complex dependencies, such as images. The method exploits the properties of the vector field associated to an auto-encoder, which allows to approximate the gradient of the log-density from its reconstruction error, based on which we propose a projected gradient ascent algorithm to obtain the conditionally most probable estimate of the missing values. Our approach does not require any specialized training procedure and can be used together with any auto-encoder model trained on complete data in a classical way. Experiments performed on benchmark datasets show that imputations produced by our model are sharp and realistic."}}
