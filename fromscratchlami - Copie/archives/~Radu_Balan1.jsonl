{"id": "m7VWzbQlWT", "cdate": 1696355654661, "mdate": 1696355654661, "content": {"title": "Optimal \u21131 Rank One Matrix Decomposition", "abstract": "In this paper, we consider the decomposition of positive semidefinite matrices as a sum of rank one matrices. We introduce and investigate the properties of various measures of optimality of such decompositions. For some classes of positive semidefinite matrices, we give explicitly these optimal decompositions. These classes include diagonally dominant matrices and certain of their generalizations, 2\u2009\u00d7\u20092, and a class of 3\u2009\u00d7\u20093 matrices."}}
{"id": "H0pLCwgQIP3", "cdate": 1680698085837, "mdate": null, "content": {"title": "Relationships between the Phase Retrieval Problem and Permutation Invariant Embeddings", "abstract": "This paper discusses the connection between the phase retrieval problem and permutation invariant embeddings. We show that the real phase retrieval problem for $\\R^d/O(1)$ is equivalent to Euclidean embeddings of the quotient space $\\R^{2\\times d}/S_2$ performed by the sorting encoder introduced in an earlier work. In addition, this relationship provides us with inversion algorithms of the orbits induced by the group of permutation matrices."}}
{"id": "kIo_C6QmMOM", "cdate": 1663850530163, "mdate": null, "content": {"title": "Coupled Multiwavelet Operator Learning for Coupled Differential Equations", "abstract": "Coupled partial differential equations (PDEs) are key tasks in modeling the complex dynamics of many physical processes. Recently, neural operators have shown the ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet space, so the difficulty of solving the coupled PDEs depends on dealing with the coupled mappings between the functions. Towards this end, we propose a \\textit{coupled multiwavelets neural operator} (CMWNO) learning scheme by decoupling the coupled integral kernels during the multiwavelet decomposition and reconstruction procedures in the Wavelet space. The proposed model achieves significantly higher accuracy compared to previous learning-based solvers in solving the coupled PDEs including Gray-Scott (GS) equations and the non-local mean field game (MFG) problem. According to our experimental results, the proposed model exhibits a $2X-4X$ improvement relative $L$2 error compared to the best results from the state-of-the-art models."}}
{"id": "dZ2psVxQk_", "cdate": 1661990400000, "mdate": 1683565268910, "content": {"title": "Lipschitz Analysis of Generalized Phase Retrievable Matrix Frames", "abstract": "The classical phase retrieval problem arises in contexts ranging from speech recognition to x-ray crystallography and quantum state tomography. The generalization to matrix frames is natural in the sense that it corresponds to quantum tomography of impure states. We provide computable global stability bounds for the quasi-linear analysis map $\\beta$ and a path forward for understanding related problems in terms of the differential geometry of key spaces. In particular, we manifest a Whitney stratification of the positive semidefinite matrices of low rank which allows us to \u201cstratify\u201d the computation of the global stability bound. We show that for the impure state case no such global stability bounds can be obtained for the nonlinear analysis map $\\alpha$ with respect to certain natural distance metrics. Finally, our computation of the global lower Lipschitz constant for the $\\beta$ analysis map provides novel conditions for a frame to be generalized phase retrievable."}}
{"id": "rn4xnDUjcl9", "cdate": 1646077539797, "mdate": null, "content": {"title": "VQ-Flows: Vector Quantized Local Normalizing Flows", "abstract": "Normalizing flows provide an elegant approach to generative modeling that allows for efficient sampling and exact density evaluation of unknown data distributions. However, current techniques have significant limitations in their expressivity when the data distribution is supported on a low-dimensional manifold or has a non-trivial topology. We introduce a novel statistical framework for learning a mixture of local normalizing flows as ``chart maps'' over the data manifold. Our framework augments the expressivity of recent approaches while preserving the signature property of normalizing flows, that they admit exact density evaluation. We learn a suitable atlas of charts for the data manifold via a vector quantized auto-encoder (VQ-AE) and the distributions over them using a conditional flow. We validate experimentally that our probabilistic framework enables existing approaches to better model data distributions over complex manifolds."}}
{"id": "TgWEvrciZMh", "cdate": 1640995200000, "mdate": 1674242400393, "content": {"title": "VQ-Flows: Vector quantized local normalizing flows", "abstract": "Normalizing flows provide an elegant approach to generative modeling that allows for efficient sampling and exact density evaluation of unknown data distributions. However, current techniques have significant limitations in their expressivity when the data distribution is supported on a low-dimensional manifold or has a non-trivial topology. We introduce a novel statistical framework for learning a mixture of local normalizing flows as \u201cchart maps\u201d over the data manifold. Our framework augments the expressivity of recent approaches while preserving the signature property of normalizing flows, that they admit exact density evaluation. We learn a suitable atlas of charts for the data manifold via a vector quantized auto-encoder (VQ-AE) and the distributions over them using a conditional flow. We validate experimentally that our probabilistic framework enables existing approaches to better model data distributions over complex manifolds."}}
{"id": "NRVmH1FQhkR", "cdate": 1640995200000, "mdate": 1681737642080, "content": {"title": "VQ-Flows: Vector Quantized Local Normalizing Flows", "abstract": "Normalizing flows provide an elegant approach to generative modeling that allows for efficient sampling and exact density evaluation of unknown data distributions. However, current techniques have significant limitations in their expressivity when the data distribution is supported on a low-dimensional manifold or has a non-trivial topology. We introduce a novel statistical framework for learning a mixture of local normalizing flows as \"chart maps\" over the data manifold. Our framework augments the expressivity of recent approaches while preserving the signature property of normalizing flows, that they admit exact density evaluation. We learn a suitable atlas of charts for the data manifold via a vector quantized auto-encoder (VQ-AE) and the distributions over them using a conditional flow. We validate experimentally that our probabilistic framework enables existing approaches to better model data distributions over complex manifolds."}}
{"id": "N0n9rOF9dO", "cdate": 1640995200000, "mdate": 1681737643901, "content": {"title": "Permutation Invariant Representations with Applications to Graph Deep Learning", "abstract": "This paper presents primarily two Euclidean embeddings of the quotient space generated by matrices that are identified modulo arbitrary row permutations. The original application is in deep learning on graphs where the learning task is invariant to node relabeling. Two embedding schemes are introduced, one based on sorting and the other based on algebras of multivariate polynomials. While both embeddings exhibit a computational complexity exponential in problem size, the sorting based embedding is globally bi-Lipschitz and admits a low dimensional target space. Additionally, an almost everywhere injective scheme can be implemented with minimal redundancy and low computational cost. In turn, this proves that almost any classifier can be implemented with an arbitrary small loss of performance. Numerical experiments are carried out on two data sets, a chemical compound data set (QM9) and a proteins data set (PROTEINS)."}}
{"id": "d2TT6gK9qZn", "cdate": 1632875697638, "mdate": null, "content": {"title": "Non-Linear Operator Approximations for Initial Value Problems", "abstract": "Time-evolution of partial differential equations is the key to model several dynamical processes, events forecasting but the operators associated with such problems are non-linear. We propose a Pad\u00e9 approximation based exponential neural operator scheme for efficiently learning the map between a given initial condition and activities at a later time. The multiwavelets bases are used for space discretization. By explicitly embedding the exponential operators in the model, we reduce the training parameters and make it more data-efficient which is essential in dealing with scarce real-world datasets. The Pad\u00e9 exponential operator uses a $\\textit{recurrent structure with shared parameters}$ to model the non-linearity compared to recent neural operators that rely on using multiple linear operator layers in succession. We show theoretically that the gradients associated with the recurrent Pad\u00e9 network are bounded across the recurrent horizon. We perform experiments on non-linear systems such as Korteweg-de Vries (KdV) and Kuramoto\u2013Sivashinsky (KS) equations to show that the proposed approach achieves the best performance and at the same time is data-efficient. We also show that urgent real-world problems like Epidemic forecasting (for example, COVID-19) can be formulated as a 2D time-varying operator problem. The proposed Pad\u00e9 exponential operators yield better prediction results ($\\textbf{53\\%} (\\textbf{52\\%})$ better MAE than best neural operator (non-neural operator deep learning model)) compared to state-of-the-art forecasting models."}}
{"id": "vuswlxIQlX", "cdate": 1577836800000, "mdate": 1683565268953, "content": {"title": "The Autoregressive Linear Mixture Model: A Time-Series Model for an Instantaneous Mixture of Network Processes", "abstract": "Vector autoregressive models provide a simple generative model for multivariate, time-series data. The autoregressive coefficients of the vector autoregressive model describe a network process. However, in real-world applications such as macroeconomics or neuroimaging, time-series data arise not from isolated network processes but instead from the simultaneous occurrence of multiple network processes. Standard vector autoregressive models cannot provide insights about the underlying structure of such time-series data. In this work, we present the autoregressive linear mixture (ALM) model. The ALM proposes a decomposition of time-series data into co-occurring network processes that we call autoregressive components. We also present a non-convex likelihood-based estimator for fitting the ALM model and show that it can be solved using the proximal alternating linearized minimization (PALM) algorithm. We validate the ALM on both synthetic and real-world electroencephalography data, showing that we can disambiguate task-relevant autoregressive components that correspond with distinct network processes."}}
