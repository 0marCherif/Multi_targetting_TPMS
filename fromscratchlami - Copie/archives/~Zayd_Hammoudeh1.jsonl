{"id": "LT6vJq1zaY", "cdate": 1682325756200, "mdate": 1682325756200, "content": {"title": "Feature Partition Aggregation: A Fast Certified Defense Against a Union of Sparse Adversarial Attacks", "abstract": "Deep networks are susceptible to numerous types of adversarial attacks. Certified defenses provide guarantees on a model's robustness, but most of these defenses are restricted to a single attack type. In contrast, this paper proposes feature partition aggregation (FPA) - a certified defense against a union of attack types, namely evasion, backdoor, and poisoning attacks. We specifically consider an $\\ell_0$ or sparse attacker that arbitrarily controls an unknown subset of the training and test features - even across all instances. FPA generates robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Following existing certified sparse defenses, we generalize FPA's guarantees to top-$k$ predictions. FPA significantly outperforms state-of-the-art sparse defenses providing larger and stronger robustness guarantees while simultaneously being up to 5,000$\\times$ faster."}}
{"id": "PWf0OsvK43F", "cdate": 1661329128506, "mdate": null, "content": {"title": "Reducing Certified Regression to Certified Classification for General Poisoning Attacks", "abstract": "Adversarial training instances can severely distort a model\u2019s behavior. This work investigates certified regression defenses, which provide guaranteed limits on how much a regressor\u2019s prediction may change under a training-set attack. Our key insight is that certified regression reduces to certified classification when using median as a model\u2019s primary decision function. Coupling our reduction with existing certified classifiers, we propose six new provably-robust regressors. To the extent of our knowledge, this is the first work that certifies the robustness of individual regression predictions without any assumptions about the data distribution and model architecture. We also show that the assumptions made by existing state-of-the-art certified classifiers are often overly pessimistic. We introduce a tighter analysis of model robustness, which in many cases results in significantly improved certified guarantees. Lastly, we empirically demonstrate our approaches\u2019 effectiveness on both regression and classification data, where the accuracy of up to 50% of test predictions can be guaranteed under 1% training-set corruption and up to 30% of predictions under 4% corruption. Our source code is available at https://github.com/ZaydH/certified-regression."}}
{"id": "H3fgqlaVzm5", "cdate": 1648671985540, "mdate": 1648671985540, "content": {"title": "What Models Know About Their Attackers: Deriving Attacker Information From Latent Representations", "abstract": "Adversarial attacks curated against NLP models are increasingly becoming practical threats. Although various methods have been developed to detect adversarial attacks, securing learning-based NLP systems in practice would require more than identifying and evading perturbed instances. To address these issues, we propose a new set of adversary identification tasks, Attacker Attribute Classification via Textual Analysis (AACTA), that attempts to obtain more detailed information about the attackers from adversarial texts. Specifically, given a piece of adversarial text, we hope to accomplish tasks such as localizing perturbed tokens, identifying the attacker\u2019s access level to the target model, determining the evasion mechanism imposed, and specifying the perturbation type employed by the attacking algorithm. Our contributions are as follows: we formalize the task of classifying attacker attributes, and create a benchmark on various target models from sentiment classification and abuse detection domains. We show that signals from BERT models and target models can be used to train classifiers that reveal the properties of the attacking algorithms. We demonstrate that adversarial attacks leave interpretable traces in the feature space of both of pre-trained language models and target models, making AACTA a promising direction towards more trustworthy NLP systems."}}
{"id": "ezk5w7Nika1", "cdate": 1640995200000, "mdate": 1673406722841, "content": {"title": "Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees", "abstract": ""}}
{"id": "Run_T4mQaUX", "cdate": 1640995200000, "mdate": 1673406722888, "content": {"title": "Training Data Influence Analysis and Estimation: A Survey", "abstract": ""}}
{"id": "NNo2pfGkfO9", "cdate": 1640995200000, "mdate": 1673406709827, "content": {"title": "Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation", "abstract": ""}}
{"id": "KAwNX3Rns2", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning from Positive and Unlabeled Data with Arbitrary Positive Shift", "abstract": "Positive-unlabeled (PU) learning trains a binary classifier using only positive and unlabeled data. A common simplifying assumption is that the positive data is representative of the target positive class. This assumption rarely holds in practice due to temporal drift, domain shift, and/or adversarial manipulation. This paper shows that PU learning is possible even with arbitrarily non-representative positive data given unlabeled data from the source and target distributions. Our key insight is that only the negative class's distribution need be fixed. We integrate this into two statistically consistent methods to address arbitrary positive bias - one approach combines negative-unlabeled learning with unlabeled-unlabeled learning while the other uses a novel, recursive risk estimator. Experimental results demonstrate our methods' effectiveness across numerous real-world datasets and forms of positive bias, including disjoint positive class-conditional supports. Additionally, we propose a general, simplified approach to address PU risk estimation overfitting."}}
{"id": "mEwm5y0j-8q", "cdate": 1514764800000, "mdate": null, "content": {"title": "Fast and Flexible Probabilistic Model Counting", "abstract": "We present a probabilistic model counter that can trade off running time with approximation accuracy. As in several previous works, the number of models of a formula is estimated by adding random parity constraints (equations). One key difference with prior works is that the systems of parity equations used correspond to the parity check matrices of Low Density Parity Check (LDPC) error-correcting codes. As a result, the equations tend to be much shorter, often containing fewer than 10 variables each, making the search for models that also satisfy the parity constraints far more tractable. The price paid for computational tractability is that the statistical properties of the basic estimator are not as good as when longer constraints are used. We show how one can deal with this issue and derive rigorous approximation guarantees by performing more solver invocations."}}
{"id": "VWhF5Cfewp", "cdate": 1514764800000, "mdate": null, "content": {"title": "Fast Sampling of Perfectly Uniform Satisfying Assignments", "abstract": "We present an algorithm for perfectly uniform sampling of satisfying assignments, based on the exact model counter sharpSAT and reservoir sampling. In experiments across several hundred formulas, our sampler is faster than the state of the art by 10 to over 100,000 times."}}
{"id": "oarAf7OWZTl", "cdate": 1483228800000, "mdate": null, "content": {"title": "Clustering-Based, Fully Automated Mixed-Bag Jigsaw Puzzle Solving", "abstract": "The jig swap puzzle is a variant of the traditional jigsaw puzzle, wherein all pieces are equal-sized squares that must be placed adjacent to one another to reconstruct an original, unknown image. This paper proposes an agglomerative hierarchical clustering-based solver that can simultaneously reconstruct multiple, mixed jig swap puzzles. Our solver requires no additional information beyond an unordered input bag of puzzle pieces, and it significantly outperforms the current state of the art in terms of both the reconstructed output quality as well the number of input puzzles it supports. In addition, we define the first quality metrics specifically tailored for multi-puzzle solvers, the Enhanced Direct Accuracy Score (EDAS), the Shiftable Enhanced Direct Accuracy Score (SEDAS), and the Enhanced Neighbor Accuracy Score (ENAS)."}}
