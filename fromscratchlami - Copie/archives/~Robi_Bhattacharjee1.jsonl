{"id": "ibB8dC4uEi", "cdate": 1672531200000, "mdate": 1682614553895, "content": {"title": "Data-Copying in Generative Models: A Formal Framework", "abstract": "There has been some recent interest in detecting and addressing memorization of training data by deep neural networks. A formal framework for memorization in generative models, called \"data-copying,\" was proposed by Meehan et. al. (2020). We build upon their work to show that their framework may fail to detect certain kinds of blatant memorization. Motivated by this and the theory of non-parametric methods, we provide an alternative definition of data-copying that applies more locally. We provide a method to detect data-copying, and provably show that it works with high probability when enough data is available. We also provide lower bounds that characterize the sample requirement for reliable detection."}}
{"id": "aXgiMkN-8yj", "cdate": 1672531200000, "mdate": 1682614553811, "content": {"title": "Robust Empirical Risk Minimization with Tolerance", "abstract": "Developing simple, sample-efficient learning algorithms for robust classification is a pressing issue in today\u2019s tech-dominated world, and current theoretical techniques requiring exponential sampl..."}}
{"id": "NqYc1GibScf", "cdate": 1672531200000, "mdate": 1682614553894, "content": {"title": "Online k-means Clustering on Arbitrary Data Streams", "abstract": "We consider $k$-means clustering in an online setting where each new data point is assigned to its closest cluster center and incurs a loss equal to the squared distance to that center, after which..."}}
{"id": "OSunXnn9T6", "cdate": 1640995200000, "mdate": 1664502935719, "content": {"title": "Learning what to remember", "abstract": "We consider a lifelong learning scenario in which a learner faces a neverending and arbitrary stream of facts and has to decide which ones to retain in its limited memory. We introduce a mathematic..."}}
{"id": "vAMh-dcNMcR", "cdate": 1621630085071, "mdate": null, "content": {"title": "Consistent Non-Parametric Methods for Maximizing Robustness", "abstract": "Learning classifiers that are robust to adversarial examples has received a great deal of recent attention. A major drawback of the standard robust learning framework is the imposition of an artificial robustness radius $r$ that applies to all inputs, and ignores the fact that data may be highly heterogeneous. In particular, it is plausible that robustness regions should be larger in some regions of data, and smaller in other. In this paper, we address this limitation by proposing a new limit classifier, called the neighborhood optimal classifier, that extends the Bayes optimal classifier outside its support by using the label of the closest in-support point. We then argue that this classifier maximizes the size of its robustness regions subject to the constraint of having accuracy equal to the Bayes optimal. We then present sufficient conditions under which general non-parametric methods that can be represented as weight functions converge towards this limit object, and show that both nearest neighbors and kernel classifiers (under certain assumptions) suffice."}}
{"id": "qX8uDF2YRd_", "cdate": 1609459200000, "mdate": 1682614553965, "content": {"title": "Consistent Non-Parametric Methods for Maximizing Robustness", "abstract": "Learning classifiers that are robust to adversarial examples has received a great deal of recent attention. A major drawback of the standard robust learning framework is the imposition of an artificial robustness radius $r$ that applies to all inputs, and ignores the fact that data may be highly heterogeneous. In particular, it is plausible that robustness regions should be larger in some regions of data, and smaller in other. In this paper, we address this limitation by proposing a new limit classifier, called the neighborhood optimal classifier, that extends the Bayes optimal classifier outside its support by using the label of the closest in-support point. We then argue that this classifier maximizes the size of its robustness regions subject to the constraint of having accuracy equal to the Bayes optimal. We then present sufficient conditions under which general non-parametric methods that can be represented as weight functions converge towards this limit object, and show that both nearest neighbors and kernel classifiers (under certain assumptions) suffice."}}
{"id": "feODU7K3y8", "cdate": 1609459200000, "mdate": 1682614553896, "content": {"title": "No-substitution k-means Clustering with Adversarial Order", "abstract": "We investigate $k$-means clustering in the online no-substitution setting when the input arrives in \\emph{arbitrary} order. In this setting, points arrive one after another, and the algorithm is required to instantly decide whether to take the current point as a center before observing the next point. Decisions are irrevocable. The goal is to minimize both the number of centers and the $k$-means cost. Previous works in this setting assume that the input\u2019s order is random, or that the input\u2019s aspect ratio is bounded. It is known that if the order is arbitrary and there is no assumption on the input, then any algorithm must take all points as centers. Moreover, assuming a bounded aspect ratio is too restrictive \u2014 it does not include natural input generated from mixture models. We introduce a new complexity measure that quantifies the difficulty of clustering a dataset arriving in arbitrary order. We design a new random algorithm and prove that if applied on data with complexity $d$, the algorithm takes $O(d\\log(n) k\\log(k))$ centers and is an $O(k^3)$-approximation. We also prove that if the data is sampled from a \u201cnatural\" distribution, such as a mixture of $k$ Gaussians, then the new complexity measure is equal to $O(k^2\\log(n))$. This implies that for data generated from those distributions, our new algorithm takes only $poly(k\\log(n))$ centers and is a $poly(k)$-approximation. In terms of negative results, we prove that the number of centers needed to achieve an $\\alpha$-approximation is at least $\\Omega\\left(\\frac{d}{k\\log(n\\alpha)}\\right)$."}}
{"id": "B5-m4KOni89", "cdate": 1609459200000, "mdate": 1682614553897, "content": {"title": "Sample Complexity of Robust Linear Classification on Separated Data", "abstract": "We consider the sample complexity of learning with adversarial robustness. Most prior theoretical results for this problem have considered a setting where different classes in the data are close to..."}}
{"id": "kcAk7qhsiz", "cdate": 1577836800000, "mdate": 1682614553895, "content": {"title": "What relations are reliably embeddable in Euclidean space?", "abstract": "We consider the problem of embedding a relation, represented as a directed graph, into Euclidean space. For three types of embeddings motivated by the recent literature on knowledge graphs, we obta..."}}
{"id": "byiNzvs7e8", "cdate": 1577836800000, "mdate": null, "content": {"title": "When are Non-Parametric Methods Robust?", "abstract": "A growing body of research has shown that many classifiers are susceptible to adversarial examples \u2013 small strategic modifications to test inputs that lead to misclassification. In this work, we st..."}}
