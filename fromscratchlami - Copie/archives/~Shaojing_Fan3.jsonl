{"id": "iDkCCnOoyI", "cdate": 1677628800000, "mdate": 1681872944784, "content": {"title": "When and Why Static Images Are More Effective Than Videos", "abstract": "People often prefer videos over images in research and applications, believing that videos are more effective for eliciting human emotions and building machine intelligence. However, our research shows that this assumption is not always correct when it comes to evoking emotions in human observers. In this article, we compare thirteen emotions and two perceptions elicited by short videos (2-6 second, silent video clips) versus static frames extracted from the videos. We show that static frames and videos elicit most emotions similarly, but static frames elicit negative emotions more strongly than videos. We test two complementary explanations: differential activation of suspense and the peak-end rule. These findings help us to computationally model human reactions more faithfully with fewer video frames. Our interdisciplinary results have important implications for methods, theory, and applications in diverse fields, including social psychology, computer vision, mass media, and marketing."}}
{"id": "hqyqMpxDuA", "cdate": 1672531200000, "mdate": 1681872944784, "content": {"title": "Emotional Attention: From Eye Tracking to Computational Modeling", "abstract": "Attending selectively to emotion-eliciting stimuli is intrinsic to human vision. In this research, we investigate how emotion-elicitation features of images relate to human selective attention. We create the EMOtional attention dataset (EMOd). It is a set of diverse emotion-eliciting images, each with (1) eye-tracking data from 16 subjects, (2) image context labels at both object- and scene-level. Based on analyses of human perceptions of EMOd, we report an emotion prioritization effect: emotion-eliciting content draws stronger and earlier human attention than neutral content, but this advantage diminishes dramatically after initial fixation. We find that human attention is more focused on awe eliciting and aesthetic vehicle and animal scenes in EMOd. Aiming to model the above human attention behavior computationally, we design a deep neural network (CASNet II), which includes a channel weighting subnetwork that prioritizes emotion-eliciting objects, and an Atrous Spatial Pyramid Pooling (ASPP) structure that learns the relative importance of image regions at multiple scales. Visualizations and quantitative analyses demonstrate the model\u2019s ability to simulate human attention behavior, especially on emotion-eliciting content."}}
{"id": "X20vV-18P5", "cdate": 1672531200000, "mdate": 1681872944770, "content": {"title": "Polarity-aware attention network for image sentiment analysis", "abstract": "Image sentiment analysis aims to employ a computational model to automatically discover the implied emotions from the underlying image, which are crucial in many practical applications. The psychological finding demonstrates that the emotional content is ordinarily involved in some informative regions. In fact, these informative regions of visual images also convey different emotional polarities and intensities. The emotion conveyed by the whole image can be regard as the combined effect of the positive-polarity emotional regions and the negative-polarity emotional regions. Motivated by this psychological prior knowledge, we propose a new polarity-aware attention network for image sentiment analysis in an end-to-end manner. Specifically, the proposed network is composed of a sentimental feature extraction backbone, a polarity-aware attention module and a fused classification module. The backbone is used to extract the global contextual features. The polarity-aware attention module not only attends to positive and negative emotion regions by predicting polarity-aware attention maps, but also estimates their polarity intensities. The fused classification module integrates the output of the first two modules for the final image sentiment prediction. A compound loss function is designed to guide network learning using the weakly-supervised manner and the distributed label smooth learning method. We validate our method on multiple benchmarks and the experimental results demonstrate that our method can obtain superior performance over several state-of-the-art methods."}}
{"id": "fKqU5ydPNJx", "cdate": 1648695635344, "mdate": 1648695635344, "content": {"title": "When and Why Static Images Are More Effective Than Videos", "abstract": "People often prefer videos over images in research and applied applications, believing that videos are more effective for eliciting human emotions and building machine intelligence. However, our research shows that this assumption is not always correct when it comes to evoking emotions in human observers. In this study, we compare thirteen emotions and two perceptions elicited by short videos (2-6 second, silent video clips) versus static frames extracted from the videos. We show that static frames and videos elicit most emotions similarly, but static frames elicit negative emotions more strongly than videos. We test two complementary explanations: differential activation of suspense and the peak-end rule. These findings help us to computationally model human reactions more faithfully with fewer video frames. Our interdisciplinary results have important implications for methods, theory, and applications in diverse fields, including social psychology, computer vision, mass media, and marketing."}}
{"id": "Z6v2aohnSo3", "cdate": 1648695504474, "mdate": 1648695504474, "content": {"title": "Human Attributes Prediction under Privacy-preserving Conditions", "abstract": "Human attributes prediction in visual media is a well-researched topic with a major focus on human faces. However, face images are often of high privacy concern as they can reveal an individual's identity. How to balance this trade-off between privacy and utility is a key problem among researchers and practitioners. In this study, we make one of the first attempts to investigate the human attributes (emotion, age, and gender) prediction under the different de-identification (eyes, lower-face, face, and head obfuscation) privacy scenarios. We first constructed the Diversity in People and Context Dataset (DPaC). We then performed a human study with eye-tracking on how humans recognize facial attributes without the presence of face and context. Results show that in an image, situational context is informative of a target's attributes. Motivated by our human study, we proposed a multi-tasking deep learning model\u2014Context-Guided Human Attributes Prediction (CHAPNet),\nfor human attributes prediction under privacy-preserving conditions. Extensive experiments on DPaC and three commonly used benchmark datasets demonstrate the superiority of CHAPNet in leveraging the situational context for a better interpretation of a target\u2019s attributes without the full presence of the target\u2019s face. Our research demonstrates the feasibility of visual analytics under de-identification for privacy."}}
{"id": "xvASVUFEKJ", "cdate": 1640995200000, "mdate": 1681872944782, "content": {"title": "Visual saliency prediction using multi-scale attention gated network", "abstract": "Predicting human visual attention cannot only increase our understanding of the underlying biological mechanisms, but also bring new insights for other computer vision-related tasks such as autonomous driving and human\u2013computer interaction. Current deep learning-based methods often place emphasis on high-level semantic feature for prediction. However, high-level semantic feature lacks fine-scale spatial information. Ideally, a saliency prediction model should include both spatial and semantic features. In this paper, we propose a multi-scale attention gated network (we refer to as MSAGNet) to fuse semantic features with different spatial resolutions for visual saliency prediction. Specifically, we adopt the high-resolution net (HRNet) as the backbone to extract the multi-scales semantic features. A multi-scale attention gating module is designed to adaptively fuse these multi-scale features in a hierarchical way. Different from the conventional way of feature concatenation from multiple layers or multi-scale inputs, this module calculates a spatial attention map from high-level semantic feature and then fuses it with the low-level spatial feature through gating operation. Through the hierarchical gating fusion, final saliency prediction is achieved at the finest scale. Extensive experimental analyses on three benchmark datasets demonstrate the superior performance of the proposed method."}}
{"id": "VMeRIU7qSKV", "cdate": 1640995200000, "mdate": 1668084866038, "content": {"title": "Compute to Tell the Tale: Goal-Driven Narrative Generation", "abstract": "Man is by nature a social animal. One important facet of human evolution is through narrative imagination, be it fictional or factual, and to tell the tale to other individuals. The factual narrative, such as news, journalism, field report, etc., is based on real-world events and often requires extensive human efforts to create. In the era of big data where video capture devices are commonly available everywhere, a massive amount of raw videos (including life-logging, dashcam or surveillance footage) are generated daily. As a result, it is rather impossible for humans to digest and analyze these video data. This paper reviews the problem of computational narrative generation where a goal-driven narrative (in the form of text with or without video) is generated from a single or multiple long videos. Importantly, the narrative generation problem makes itself distinguished from the existing literature by its focus on a comprehensive understanding of user goal, narrative structure and open-domain input. We tentatively outline a general narrative generation framework and discuss the potential research problems and challenges in this direction. Informed by the real-world impact of narrative generation, we then illustrate several practical use cases in Video Logging as a Service platform which enables users to get more out of the data through a goal-driven intelligent storytelling AI agent."}}
{"id": "QEQfbuDsEn", "cdate": 1609459200000, "mdate": 1681872944789, "content": {"title": "Human Attributes Prediction under Privacy-preserving Conditions", "abstract": "Human attributes prediction in visual media is a well-researched topic with a major focus on human faces. However, face images are often of high privacy concern as they can reveal an individual's identity. How to balance this trade-off between privacy and utility is a key problem among researchers and practitioners. In this study, we make one of the first attempts to investigate the human attributes (emotion, age, and gender) prediction under the different de-identification (eyes, lower-face, face, and head obfuscation) privacy scenarios. We first constructed the Diversity in People and Context Dataset (DPaC). We then performed a human study with eye-tracking on how humans recognize facial attributes without the presence of face and context. Results show that in an image, situational context is informative of a target's attributes. Motivated by our human study, we proposed a multi-tasking deep learning model - Context-Guided Human Attributes Prediction (CHAPNet), for human attributes prediction under privacy-preserving conditions. Extensive experiments on DPaC and three commonly used benchmark datasets demonstrate the superiority of CHAPNet in leveraging the situational context for a better interpretation of a target's attributes without the full presence of the target's face. Our research demonstrates the feasibility of visual analytics under de-identification for privacy."}}
{"id": "jvK5KLi9zr", "cdate": 1577836800000, "mdate": 1681872944788, "content": {"title": "MediaEval 2020: Maintaining Human-Imperceptibility of Image Adversarial Attack by Using Human-Aware Sensitivity Map", "abstract": ""}}
{"id": "jW7mVED2vFR", "cdate": 1577836800000, "mdate": 1681872945009, "content": {"title": "Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia Research Agenda", "abstract": "Participation on social media platforms has many benefits but also poses substantial threats. Users often face an unintended loss of privacy, are bombarded with mis-/disinformation, or are trapped in filter bubbles due to over-personalized content. These threats are further exacerbated by the rise of hidden AI-driven algorithms working behind the scenes to shape users' thoughts, attitudes, and behavior. We investigate how multimedia researchers can help tackle these problems to level the playing field for social media users. We perform a comprehensive survey of algorithmic threats on social media and use it as a lens to set a challenging but important research agenda for effective and real-time user nudging. We further implement a conceptual prototype and evaluate it with experts to supplement our research agenda. This paper calls for solutions that combat the algorithmic threats on social media by utilizing machine learning and multimedia content analysis techniques but in a transparent manner and for the benefit of the users."}}
