{"id": "U_PvlhPbsGS", "cdate": 1672531200000, "mdate": 1682329923739, "content": {"title": "Adaptive Sampling for Probabilistic Forecasting under Distribution Shift", "abstract": "The world is not static: This causes real-world time series to change over time through external, and potentially disruptive, events such as macroeconomic cycles or the COVID-19 pandemic. We present an adaptive sampling strategy that selects the part of the time series history that is relevant for forecasting. We achieve this by learning a discrete distribution over relevant time steps by Bayesian optimization. We instantiate this idea with a two-step method that is pre-trained with uniform sampling and then training a lightweight adaptive architecture with adaptive sampling. We show with synthetic and real-world experiments that this method adapts to distribution shift and significantly reduces the forecasting error of the base model for three out of five datasets."}}
{"id": "oPHuNpJl3c", "cdate": 1664928778720, "mdate": null, "content": {"title": "Adaptive Sampling for Probabilistic Forecasting under Distribution Shift", "abstract": "The world is not static: This causes real-world time series to change over time\nthrough external, and potentially disruptive, events such as macroeconomic cycles\nor the COVID-19 pandemic. We present an adaptive sampling strategy that selects\nthe part of the time series history that is relevant for forecasting. We achieve this by\nlearning a discrete distribution over relevant time steps by Bayesian optimization.\nWe instantiate this idea with a two-step method that is pre-trained with uniform\nsampling and then training a lightweight adaptive architecture with adaptive sam-\npling. We show with synthetic and real-world experiments that this method adapts\nto distribution shift and significantly reduces the forecasting error of the base model\nfor three out of five datasets."}}
{"id": "iLaHpvlHDsH", "cdate": 1640995200000, "mdate": 1682329923753, "content": {"title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series", "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in several downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score for time series, Context-FID, assessing the quality of synthetic time series samples. We find that Context-FID is indicative for downstream performance. Therefore, Context-FID could be a useful tool to develop time series GAN models."}}
{"id": "4kQeOURQri", "cdate": 1640995200000, "mdate": 1682329923742, "content": {"title": "Resilient Neural Forecasting Systems", "abstract": "Industrial machine learning systems face data challenges that are often under-explored in the academic literature. Common data challenges are data distribution shifts, missing values and anomalies. In this paper, we discuss data challenges and solutions in the context of a Neural Forecasting application on labor planning.We discuss how to make this forecasting system resilient to these data challenges. We address changes in data distribution with a periodic retraining scheme and discuss the critical importance of model stability in this setting. Furthermore, we show how our deep learning model deals with missing values natively without requiring imputation. Finally, we describe how we detect anomalies in the input data and mitigate their effect before they impact the forecasts. This results in a fully autonomous forecasting system that compares favorably to a hybrid system consisting of the algorithm and human overrides."}}
{"id": "Ix_mh42xq5w", "cdate": 1632875639189, "mdate": null, "content": {"title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series", "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in several downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score for time series, Context-FID, assessing the quality of synthetic time series samples. We find that Context-FID is indicative for downstream performance. Therefore, Context-FID could be a useful tool to develop time series GAN models. "}}
{"id": "t8zBkMGHMEu", "cdate": 1609459200000, "mdate": 1682329923739, "content": {"title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series", "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in two downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score, Context-FID, assessing the quality of synthetic time series samples. In our downstream tasks, we find that the lowest scoring models correspond to the best-performing ones. Therefore, Context-FID could be a useful tool to develop time series GAN models."}}
{"id": "YoiYiLVlv1y", "cdate": 1577836800000, "mdate": 1682329923809, "content": {"title": "Can Pre-training help VQA with Lexical Variations?", "abstract": ""}}
{"id": "8718qzAAtsD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Resilient Neural Forecasting Systems", "abstract": "Industrial machine learning systems face data challenges that are often under-explored in the academic literature. Common data challenges are data distribution shifts, missing values and anomalies. In this paper, we discuss data challenges and solutions in the context of a Neural Forecasting application on labor planning. We discuss how to make this forecasting system resilient to these data challenges. We address changes in data distribution with a periodic retraining scheme and discuss the critical importance of model stability in this setting. Furthermore, we show how our deep learning model deals with missing values natively without requiring imputation. Finally, we describe how we detect anomalies in the input data and mitigate their effect before they impact the forecasts. This results in a fully autonomous forecasting system that compares favourably to a hybrid system consisting of the algorithm and human overrides."}}
{"id": "eZzY4QEMAZ", "cdate": 1546300800000, "mdate": 1682329923723, "content": {"title": "Bootstrapping NLU Models with Multi-task Learning", "abstract": "Bootstrapping natural language understanding (NLU) systems with minimal training data is a fundamental challenge of extending digital assistants like Alexa and Siri to a new language. A common approach that is adapted in digital assistants when responding to a user query is to process the input in a pipeline manner where the first task is to predict the domain, followed by the inference of intent and slots. However, this cascaded approach instigates error propagation and prevents information sharing among these tasks. Further, the use of words as the atomic units of meaning as done in many studies might lead to coverage problems for morphologically rich languages such as German and French when data is limited. We address these issues by introducing a character-level unified neural architecture for joint modeling of the domain, intent, and slot classification. We compose word-embeddings from characters and jointly optimize all classification tasks via multi-task learning. In our results, we show that the proposed architecture is an optimal choice for bootstrapping NLU systems in low-resource settings thus saving time, cost and human effort."}}
