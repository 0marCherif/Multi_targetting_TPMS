{"id": "WATlWd-RtW", "cdate": 1687222689936, "mdate": 1687222689936, "content": {"title": "Poincar\u00e9 ResNet", "abstract": " This paper introduces an end-to-end residual network that operates entirely on the Poincar\u00e9 ball model of hyperbolic space. Hyperbolic learning has recently shown great potential for visual understanding, but is currently only performed in the penultimate layer(s) of deep networks. All visual representations are still learned through standard Euclidean networks. In this paper we investigate how to learn hyperbolic representations of visual data directly from the pixel-level. We propose Poincar\u00e9 ResNet, a hyperbolic counterpart of the celebrated residual network, starting from Poincar\u00e9 2D convolutions up to Poincar\u00e9 residual connections. We identify three roadblocks for training convolutional networks entirely in hyperbolic space and propose a solution for each: (i) Current hyperbolic network initializations collapse to the origin, limiting their applicability in deeper networks. We provide an identity-based initialization that preserves norms over many layers. (ii) Residual networks rely heavily on batch normalization, which comes with expensive Fr\u00e9chet mean calculations in hyperbolic space. We introduce Poincar\u00e9 midpoint batch normalization as a faster and equally effective alternative. (iii) Due to the many intermediate operations in Poincar\u00e9 layers, we lastly find that the computation graphs of deep learning libraries blow up, limiting our ability to train on deep hyperbolic networks. We provide manual backward derivations of core hyperbolic operations to maintain manageable computation graphs."}}
{"id": "MbVS6BuJ3ql", "cdate": 1652737333348, "mdate": null, "content": {"title": "Maximum Class Separation as Inductive Bias in One Matrix", "abstract": "Maximizing the separation between classes constitutes a well-known inductive bias in machine learning and a pillar of many traditional algorithms. By default, deep networks are not equipped with this inductive bias and therefore many alternative solutions have been proposed through differential optimization. Current approaches tend to optimize classification and separation jointly: aligning inputs with class vectors and separating class vectors angularly. This paper proposes a simple alternative: encoding maximum separation as an inductive bias in the network by adding one fixed matrix multiplication before computing the softmax activations. The main observation behind our approach is that separation does not require optimization but can be solved in closed-form prior to training and plugged into a network. We outline a recursive approach to obtain the matrix consisting of maximally separable vectors for any number of classes, which can be added with negligible engineering effort and computational overhead. Despite its simple nature, this one matrix multiplication provides real impact. We show that our proposal directly boosts classification, long-tailed recognition, out-of-distribution detection, and open-set recognition, from CIFAR to ImageNet. We find empirically that maximum separation works best as a fixed bias; making the matrix learnable adds nothing to the performance. The closed-form implementation and code to reproduce the experiments are available on github."}}
