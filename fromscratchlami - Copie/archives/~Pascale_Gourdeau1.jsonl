{"id": "RQ385yD9dqR", "cdate": 1652737413957, "mdate": null, "content": {"title": "When are Local Queries Useful for Robust Learning?", "abstract": "Distributional assumptions have been shown to be necessary for the robust learnability of concept classes when considering the exact-in-the-ball robust risk and access to random examples by Gourdeau et al. (2019). In this paper, we study learning models where the learner is given more power through the use of local queries, and give the first distribution-free algorithms that perform robust empirical risk minimization (ERM) for this notion of robustness. The first learning model we consider uses local membership queries (LMQ), where the learner can query the label of points near the training sample. We show that, under the uniform distribution, LMQs do not increase the robustness threshold of conjunctions and any superclass, e.g., decision lists and halfspaces. Faced with this negative result, we introduce the local equivalence query (LEQ) oracle, which returns whether the hypothesis and target concept agree in the perturbation region around a point in the training sample, as well as a counterexample if it exists. We show a separation result: on one hand, if the query radius $\\lambda$ is strictly smaller than the adversary's perturbation budget $\\rho$, then distribution-free robust learning is impossible for a wide variety of concept classes; on the other hand, the setting $\\lambda=\\rho$ allows us to develop robust ERM algorithms. We then bound the query complexity of these algorithms based on online learning guarantees and further improve these bounds for the special case of conjunctions. We finish by giving robust learning algorithms for halfspaces with margins on both $\\{0,1\\}^n$ and $\\mathbb{R}^n$."}}
{"id": "uCO24YoSLm", "cdate": 1640995200000, "mdate": 1680259318419, "content": {"title": "Sample Complexity Bounds for Robustly Learning Decision Lists against Evasion Attacks", "abstract": ""}}
{"id": "6u16zHzp-A-", "cdate": 1640995200000, "mdate": 1680259318266, "content": {"title": "When are Local Queries Useful for Robust Learning?", "abstract": ""}}
{"id": "5LyzQGyPbQ", "cdate": 1640995200000, "mdate": 1680259318262, "content": {"title": "Bisimulation metrics and norms for real-weighted automata", "abstract": ""}}
{"id": "TyReG39VUX", "cdate": 1609459200000, "mdate": 1680259318421, "content": {"title": "On the Hardness of Robust Classification", "abstract": ""}}
{"id": "rkevhVBeUB", "cdate": 1567802542613, "mdate": null, "content": {"title": "On the Hardness of Robust Classification", "abstract": "It is becoming increasingly important to understand the vulnerability of machine-learning models to adversarial attacks.  In this paper we study the feasibility of robust learning from the perspective of computational learning theory, considering both sample and computational complexity.  In particular, our definition of robust learnability requires polynomial sample complexity.  We start with two negative results.  We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit.  We show moreover that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb $\\omega(\\log n)$ input bits. However if the adversary is restricted to perturbing $O(\\log n)$ bits, then the class of monotone conjunctions can be robustly learned with respect to a general class of distributions (that includes the uniform distribution). Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on another computational model (e.g. the statistical query model) nor on any hardness assumption other than the existence of a hard learning problem in the PAC framework."}}
{"id": "vwq_bF_Wyh", "cdate": 1483228800000, "mdate": null, "content": {"title": "Bisimulation Metrics for Weighted Automata", "abstract": "We develop a new bisimulation (pseudo)metric for weighted finite automata (WFA) that generalizes Boreale's linear bisimulation relation. Our metrics are induced by seminorms on the state space of WFA. Our development is based on spectral properties of sets of linear operators. In particular, the joint spectral radius of the transition matrices of WFA plays a central role. We also study continuity properties of the bisimulation pseudometric, establish an undecidability result for computing the metric, and give a preliminary account of applications to spectral learning of weighted automata."}}
