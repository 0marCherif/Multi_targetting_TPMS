{"id": "_RvQKPW8ZCp", "cdate": 1667319434697, "mdate": null, "content": {"title": "Sequential Learning in GPs with Memory and Bayesian Leverage Score", "abstract": "Limited access to previous data is challenging when using Gaussian process (GP) models for sequential learning. This results in inaccuracies in posterior, hyperparameter learning, and inducing variables. The recently proposed \u2018dual\u2019 sparse GP model enables inference of variational parameters in such a setup. In this paper, using the dual GP, we tackle the problem arising due to a lack of access to previous data for estimating hyperparameters of a sparse Gaussian process. We propose utilizing the concept of \u2018memory\u2019. To pick representative memory, we develop the \u2018Bayesian leverage score\u2019 built on the ridge leverage score. We experiment and perform an ablation study with a sequential learning data set, split MNIST, to showcase the usefulness of the proposed method."}}
{"id": "5PPuxl9fkn", "cdate": 1640995200000, "mdate": 1682317863590, "content": {"title": "Fantasizing with Dual GPs in Bayesian Optimization and Active Learning", "abstract": "Gaussian processes (GPs) are the main surrogate functions used for sequential modelling such as Bayesian Optimization and Active Learning. Their drawbacks are poor scaling with data and the need to run an optimization loop when using a non-Gaussian likelihood. In this paper, we focus on `fantasizing' batch acquisition functions that need the ability to condition on new fantasized data computationally efficiently. By using a sparse Dual GP parameterization, we gain linear scaling with batch size as well as one-step updates for non-Gaussian likelihoods, thus extending sparse models to greedy batch fantasizing acquisition functions."}}
{"id": "yk3ghzrAVnp", "cdate": 1632781959110, "mdate": null, "content": {"title": "Sparse Gaussian Processes for Stochastic Differential Equations", "abstract": "We frame the problem of learning stochastic differential equations (SDEs) from noisy observations as an inference problem and aim to maximize the marginal likelihood of the observations in a joint model of the latent paths and the noisy observations. As this problem is intractable, we derive an approximate (variational) inference algorithm and propose a novel parameterization of the approximate distribution over paths using a sparse Markovian Gaussian process. The approximation is efficient in storage and computation, allowing the usage of well-established optimizing algorithms such as natural gradient descent. We demonstrate the capability of the proposed method on the Ornstein-Uhlenbeck process."}}
{"id": "admg0sZZm1e", "cdate": 1621630169816, "mdate": null, "content": {"title": "Scalable Inference in SDEs by Direct Matching of the Fokker\u2013Planck\u2013Kolmogorov Equation", "abstract": "Simulation-based techniques such as variants of stochastic Runge\u2013Kutta are the de facto approach for inference with stochastic differential equations (SDEs) in machine learning. These methods are general-purpose and used with parametric and non-parametric models, and neural SDEs. Stochastic Runge\u2013Kutta relies on the use of sampling schemes that can be inefficient in high dimensions. We address this issue by revisiting the classical SDE literature and derive direct approximations to the (typically intractable) Fokker\u2013Planck\u2013Kolmogorov equation by matching moments. We show how this workflow is fast, scales to high-dimensional latent spaces, and is applicable to scarce-data applications, where a non-parametric SDE with a driving Gaussian process velocity field specifies the model. "}}
{"id": "lGZ4Rjqln-H", "cdate": 1609459200000, "mdate": 1682317863227, "content": {"title": "Scalable Inference in SDEs by Direct Matching of the Fokker-Planck-Kolmogorov Equation", "abstract": "Simulation-based techniques such as variants of stochastic Runge\u2013Kutta are the de facto approach for inference with stochastic differential equations (SDEs) in machine learning. These methods are general-purpose and used with parametric and non-parametric models, and neural SDEs. Stochastic Runge\u2013Kutta relies on the use of sampling schemes that can be inefficient in high dimensions. We address this issue by revisiting the classical SDE literature and derive direct approximations to the (typically intractable) Fokker\u2013Planck\u2013Kolmogorov equation by matching moments. We show how this workflow is fast, scales to high-dimensional latent spaces, and is applicable to scarce-data applications, where a non-parametric SDE with a driving Gaussian process velocity field specifies the model."}}
{"id": "DlwL9g_Oyho", "cdate": 1609459200000, "mdate": 1682317863170, "content": {"title": "Scalable Inference in SDEs by Direct Matching of the Fokker-Planck-Kolmogorov Equation", "abstract": "Simulation-based techniques such as variants of stochastic Runge-Kutta are the de facto approach for inference with stochastic differential equations (SDEs) in machine learning. These methods are general-purpose and used with parametric and non-parametric models, and neural SDEs. Stochastic Runge-Kutta relies on the use of sampling schemes that can be inefficient in high dimensions. We address this issue by revisiting the classical SDE literature and derive direct approximations to the (typically intractable) Fokker-Planck-Kolmogorov equation by matching moments. We show how this workflow is fast, scales to high-dimensional latent spaces, and is applicable to scarce-data applications, where a non-parametric SDE with a driving Gaussian process velocity field specifies the model."}}
