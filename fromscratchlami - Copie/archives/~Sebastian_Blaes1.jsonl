{"id": "il-rK4V1ds", "cdate": 1686250303740, "mdate": null, "content": {"title": "Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning", "abstract": "We introduce a simple but effective method for managing risk in model-based reinforcement learning with trajectory sampling that involves probabilistic safety constraints and balancing of optimism in the face of epistemic uncertainty and pessimism in the face of aleatoric uncertainty of an ensemble of stochastic neural networks. Various experiments indicate that the separation of uncertainties is essential  to performing well with data-driven MPC approaches in uncertain and safety-critical control environments."}}
{"id": "3k5CUGDLNdd", "cdate": 1663850028857, "mdate": null, "content": {"title": "Benchmarking Offline Reinforcement Learning on Real-Robot Hardware", "abstract": "Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems."}}
{"id": "x6INXlnUGro", "cdate": 1655376344475, "mdate": null, "content": {"title": "Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations", "abstract": "Learning agile skills is one of the main challenges in robotics. To this end, reinforcement learning approaches have achieved impressive results. These methods require explicit task information in terms of a reward function or an expert that can be queried in simulation to provide a target control output, which limits their applicability. In this work, we propose a generative adversarial method for inferring reward functions from partial and potentially physically incompatible demonstrations for successful skill acquirement where reference or expert demonstrations are not easily accessible. Moreover, we show that by using a Wasserstein GAN formulation and transitions from demonstrations with rough and partial information as input, we are able to extract policies that are robust and capable of imitating demonstrated behaviors. Finally, the obtained skills such as a backflip are tested on an agile quadruped robot called Solo 8 and present faithful replication of hand-held human demonstrations."}}
{"id": "NnuYZ1el24C", "cdate": 1652737764804, "mdate": null, "content": {"title": "Curious Exploration via Structured World Models Yields Zero-Shot Object Manipulation", "abstract": "It has been a long-standing dream to design artificial agents that explore their environment efficiently via intrinsic motivation, similar to how children perform curious free play. Despite recent advances in intrinsically motivated reinforcement learning (RL), sample-efficient exploration in object manipulation scenarios remains a significant challenge as most of the relevant information lies in the sparse agent-object and object-object interactions. In this paper, we propose to use structured world models to incorporate relational inductive biases in the control loop to achieve sample-efficient and interaction-rich exploration in compositional multi-object environments. By planning for future novelty inside structured world models, our method generates free-play behavior that starts to interact with objects early on and develops more complex behavior over time. Instead of using models only to compute intrinsic rewards, as commonly done, our method showcases that the self-reinforcing cycle between good models and good exploration also opens up another avenue: zero-shot generalization to downstream tasks via model-based planning. After the entirely intrinsic task-agnostic exploration phase, our method solves challenging downstream tasks such as stacking, flipping, pick & place, and throwing that generalizes to unseen numbers and arrangements of objects without any additional training."}}
{"id": "zs5C24n95t", "cdate": 1640995200000, "mdate": 1682318576667, "content": {"title": "Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations", "abstract": "Learning agile skills is one of the main challenges in robotics. To this end, reinforcement learning approaches have achieved impressive results. These methods require explicit task information in terms of a reward function or an expert that can be queried in simulation to provide a target control output, which limits their applicability. In this work, we propose a generative adversarial method for inferring reward functions from partial and potentially physically incompatible demonstrations for successful skill acquirement where reference or expert demonstrations are not easily accessible. Moreover, we show that by using a Wasserstein GAN formulation and transitions from demonstrations with rough and partial information as input, we are able to extract policies that are robust and capable of imitating demonstrated behaviors. Finally, the obtained skills such as a backflip are tested on an agile quadruped robot called Solo 8 and present faithful replication of hand-held human demonstrations."}}
{"id": "ViiO9FC85", "cdate": 1640995200000, "mdate": 1684171136663, "content": {"title": "Curious Exploration via Structured World Models Yields Zero-Shot Object Manipulation", "abstract": "It has been a long-standing dream to design artificial agents that explore their environment efficiently via intrinsic motivation, similar to how children perform curious free play. Despite recent advances in intrinsically motivated reinforcement learning (RL), sample-efficient exploration in object manipulation scenarios remains a significant challenge as most of the relevant information lies in the sparse agent-object and object-object interactions. In this paper, we propose to use structured world models to incorporate relational inductive biases in the control loop to achieve sample-efficient and interaction-rich exploration in compositional multi-object environments. By planning for future novelty inside structured world models, our method generates free-play behavior that starts to interact with objects early on and develops more complex behavior over time. Instead of using models only to compute intrinsic rewards, as commonly done, our method showcases that the self-reinforcing cycle between good models and good exploration also opens up another avenue: zero-shot generalization to downstream tasks via model-based planning. After the entirely intrinsic task-agnostic exploration phase, our method solves challenging downstream tasks such as stacking, flipping, pick & place, and throwing that generalizes to unseen numbers and arrangements of objects without any additional training."}}
{"id": "To83VToiog", "cdate": 1640995200000, "mdate": 1684308531194, "content": {"title": "Nature-Inspired Inductive Biases in Learning Robots", "abstract": "Die in dieser Dissertation vorgestellten Arbeiten studieren verschiedene von der Natur inspirierte induktive Verzerrungen im Kontext von modellfreiem und modellbasiertem selbstverst\u00e4rkenden Lernen, mit dem Ziel, KI Agenten zu entwerfen, die effizient und autonom in der realen Welt handeln. Dabei sind von Robotern zu bew\u00e4ltigende Objektmanipulationsaufgaben von besonderem Interesse, da die zeitliche Entwicklung dieser dynamischen Systeme nicht trivial ist und Manipulationsaufgaben schwierige Planungsprobleme darstellen. Die betrachteten induktiven Verzerrungen sind haupts\u00e4chlich von in der Natur zu findenden intelligenten Agenten, wie Tiere und Menschen, inspiriert. Die prim\u00e4ren Inspirationsquellen sind wie folgt. (1) Hierarchisch organisierte und spezialisierte kortikale Strukturen, die die effektive Erlernung von F\u00e4higkeiten unterst\u00fctzen. (2) Das selbstorganisierte Spielen von Kindern zum Zwecke der Formung intuitiver Modelle und Theorien \u00fcber die Welt. (3) Strukturierte Explorationsstrategien basierend auf unterschiedliche Formen von intrinsischer Motivation und lang anhaltender zeitlicher Korrelationen in motorischen Befehlen. (4) Imitationslernen. (5) Die Planung von Aktionssequenzen unter der Ber\u00fccksichtigung von Unsicherheiten in mentalen Modellen der nichtdeterministischen Welt. Diese Arbeit ist die Fortsetzung einer langen Historie von Ideen und Forschungsbem\u00fchungen, die Inspiration aus der Natur ziehen, um kompetentere KI Agenten zu entwickeln. Die Bem\u00fchungen in diesen Forschungsfeldern m\u00fcndeten in der Ausbildung verschiedener Forschungsfelder wie hierarchisches selbstverst\u00e4rkendes Lernen, Entwicklungsrobotik, intrinsisch motiviertes selbstverst\u00e4rkendes Lernen und Repr\u00e4sentationslernen. Diese Arbeit baut auf den in diesen Feldern entwickelten Ideen und Konzepten auf und kombiniert diese mit Methoden von modellfreiem und modellbasiertem selbstverst\u00e4rkenden Lernen, um es Robotern zu erm\u00f6glichen, herausfordernde Objektmanipulationsaufgaben von Grund auf zu l\u00f6sen. Die Hypothese, dass von der Natur inspirierte induktive Verzerrungen einen essenziellen Beitrag zur Erschaffung kompetenterer KI Agenten liefern k\u00f6nnten, wird dabei durch zahlreiche empirische Studien unterst\u00fctzt."}}
{"id": "DD3Z9iqjYj", "cdate": 1640995200000, "mdate": 1682318576801, "content": {"title": "Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations", "abstract": "Learning agile skills is one of the main challenges in robotics. To this end, reinforcement learning approaches have achieved impressive results. These methods require explicit task information in ..."}}
{"id": "9trWx_ItJvQ", "cdate": 1640995200000, "mdate": 1683970251397, "content": {"title": "Curious Exploration via Structured World Models Yields Zero-Shot Object Manipulation", "abstract": "It has been a long-standing dream to design artificial agents that explore their environment efficiently via intrinsic motivation, similar to how children perform curious free play. Despite recent advances in intrinsically motivated reinforcement learning (RL), sample-efficient exploration in object manipulation scenarios remains a significant challenge as most of the relevant information lies in the sparse agent-object and object-object interactions. In this paper, we propose to use structured world models to incorporate relational inductive biases in the control loop to achieve sample-efficient and interaction-rich exploration in compositional multi-object environments. By planning for future novelty inside structured world models, our method generates free-play behavior that starts to interact with objects early on and develops more complex behavior over time. Instead of using models only to compute intrinsic rewards, as commonly done, our method showcases that the self-reinforcing cycle between good models and good exploration also opens up another avenue: zero-shot generalization to downstream tasks via model-based planning. After the entirely intrinsic task-agnostic exploration phase, our method solves challenging downstream tasks such as stacking, flipping, pick &amp; place, and throwing that generalizes to unseen numbers and arrangements of objects without any additional training."}}
{"id": "3TSFfF0rkt2", "cdate": 1640995200000, "mdate": 1682318576847, "content": {"title": "Versatile Skill Control via Self-supervised Adversarial Imitation of Unlabeled Mixed Motions", "abstract": "Learning diverse skills is one of the main challenges in robotics. To this end, imitation learning approaches have achieved impressive results. These methods require explicitly labeled datasets or assume consistent skill execution to enable learning and active control of individual behaviors, which limits their applicability. In this work, we propose a cooperative adversarial method for obtaining single versatile policies with controllable skill sets from unlabeled datasets containing diverse state transition patterns by maximizing their discriminability. Moreover, we show that by utilizing unsupervised skill discovery in the generative adversarial imitation learning framework, novel and useful skills emerge with successful task fulfillment. Finally, the obtained versatile policies are tested on an agile quadruped robot called Solo 8 and present faithful replications of diverse skills encoded in the demonstrations."}}
