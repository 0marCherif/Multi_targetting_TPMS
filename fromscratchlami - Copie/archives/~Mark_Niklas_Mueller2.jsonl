{"id": "K0_LSwNvEkT", "cdate": 1672531200000, "mdate": 1681192293256, "content": {"title": "First Three Years of the International Verification of Neural Networks Competition (VNN-COMP)", "abstract": ""}}
{"id": "_-gZhHVnI3e", "cdate": 1665069636813, "mdate": null, "content": {"title": "Certified Training: Small Boxes are All You Need", "abstract": "We propose the novel certified training method, SABR, which outperforms existing methods across perturbation magnitudes on MNIST, CIFAR-10, and TinyImageNet, in terms of both standard and certifiable accuracies. The key insight behind SABR is that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. SABR does not only establish a new state-of-the-art in all commonly used benchmarks but more importantly, points to a new class of certified training methods promising to overcome the robustness-accuracy trade-off."}}
{"id": "hC2_w2d2DY", "cdate": 1665013554940, "mdate": null, "content": {"title": "Efficient Robustness Verification of Neural Ordinary Differential Equations", "abstract": "Neural Ordinary Differential Equations (NODEs) are a novel neural architecture, built around initial value problems with learned dynamics. Thought to be inherently more robust against adversarial perturbations, they were recently shown to be vulnerable to strong adversarial attacks, highlighting the need for formal guarantees. In this work, we tackle this challenge and propose GAINS, an analysis framework for NODEs based on three key ideas: (i) a novel class of ODE solvers, based on variable but discrete time steps, (ii) an efficient graph representation of solver trajectories, and (iii) a bound propagation algorithm operating on this graph representation. Together, these advances enable the efficient analysis and certified training of high-dimensional NODEs, which we demonstrate in an extensive evaluation on computer vision and time-series forecasting problems."}}
{"id": "7oFuxtJtUMH", "cdate": 1663850538830, "mdate": null, "content": {"title": "Certified Training: Small Boxes are All You Need", "abstract": "To obtain, deterministic guarantees of adversarial robustness, specialized training methods are used. We propose, SABR, a novel such certified training method, based on the key insight that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. We show in an extensive empirical evaluation that SABR outperforms existing certified defenses in terms of both standard and certifiable accuracies across perturbation magnitudes and datasets, pointing to a new class of certified training methods promising to alleviate the robustness-accuracy trade-off."}}
{"id": "KyoVpYvWWnK", "cdate": 1663850533395, "mdate": null, "content": {"title": "Efficient Certified Training and Robustness Verification of Neural ODEs", "abstract": "Neural Ordinary Differential Equations (NODEs) are a novel neural architecture, built around initial value problems with learned dynamics which are solved during inference. Thought to be inherently more robust against adversarial perturbations, they were recently shown to be vulnerable to strong adversarial attacks, highlighting the need for formal guarantees.  However, despite significant progress in robustness verification for standard feed-forward architectures, the verification of high dimensional NODEs remains an open problem. In this work we address this challenge and propose GAINS, an analysis framework for NODEs combining three key ideas: (i) a novel class of ODE solvers, based on variable but discrete time steps, (ii) an efficient graph representation of solver trajectories, and (iii) a novel abstraction algorithm operating on this graph representation. Together, these advances enable the efficient analysis and certified training of high-dimensional NODEs, by reducing the runtime from an intractable $\\mathcal{O}(\\exp(d)+\\exp(T))$ to $\\mathcal{O}(d+T^2\\log^2T)$ in the dimensionality $d$ and integration time $T$.  In an extensive evaluation on computer vision (MNIST and Fashion-MNIST) and time-series forecasting (Physio-Net) problems, we demonstrate the effectiveness of both our certified training and verification methods."}}
{"id": "IbBHnPyjkco", "cdate": 1652737808859, "mdate": null, "content": {"title": "(De-)Randomized Smoothing for Decision Stump Ensembles", "abstract": "Tree-based models are used in many high-stakes application domains such as \ufb01nance and medicine, where robustness and interpretability are of utmost importance. Yet, methods for improving and certifying their robustness are severely under-explored, in contrast to those focusing on neural networks. Targeting this important challenge, we propose deterministic smoothing for decision stump ensembles. Whereas most prior work on randomized smoothing focuses on evaluating arbitrary base models approximately under input randomization, the key insight of our work is that decision stump ensembles enable exact yet ef\ufb01cient evaluation via dynamic programming. Importantly, we obtain deterministic robustness certi\ufb01cates, even jointly over numerical and categorical features, a setting ubiquitous in the real world. Further, we derive an MLE-optimal training method for smoothed decision stumps under randomization and propose two boosting approaches to improve their provable robustness. An extensive experimental evaluation on computer vision and tabular data tasks shows that our approach yields signi\ufb01cantly higher certi\ufb01ed accuracies than the state-of-the-art for tree-based models. We release all code and trained models at https://github.com/eth-sri/drs."}}
{"id": "dR4nhPLOgD7", "cdate": 1640995200000, "mdate": 1674983355724, "content": {"title": "Robust and Accurate - Compositional Architectures for Randomized Smoothing", "abstract": "Randomized Smoothing (RS) is considered the state-of-the-art approach to obtain certifiably robust models for challenging tasks. However, current RS approaches drastically decrease standard accuracy on unperturbed data, severely limiting their real-world utility. To address this limitation, we propose a compositional architecture, ACES, which certifiably decides on a per-sample basis whether to use a smoothed model yielding predictions with guarantees or a more accurate standard model without guarantees. This, in contrast to prior approaches, enables both high standard accuracies and significant provable robustness. On challenging tasks such as ImageNet, we obtain, e.g., $80.0\\%$ natural accuracy and $28.2\\%$ certifiable accuracy against $\\ell_2$ perturbations with $r=1.0$. We release our code and models at https://github.com/eth-sri/aces."}}
{"id": "WvTkLDF9JhY", "cdate": 1640995200000, "mdate": 1681192291812, "content": {"title": "The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results", "abstract": ""}}
{"id": "HWVWvbMcwgc", "cdate": 1640995200000, "mdate": 1645875710525, "content": {"title": "PRIMA: general and precise neural network certification via scalable convex hull approximations", "abstract": "Formal verification of neural networks is critical for their safe adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge. In this paper, we take a major step forward in addressing this challenge and present a new verification framework, called PRIMA. PRIMA is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex abstractions involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss. We evaluate the effectiveness of PRIMA on a variety of challenging tasks from prior work. Our results show that PRIMA is significantly more precise than the state-of-the-art, verifying robustness to input perturbations for up to 20%, 30%, and 34% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, PRIMA enables, for the first time, the precise verification of a realistic neural network for autonomous driving within a few minutes."}}
{"id": "32_Sdbdahb", "cdate": 1640995200000, "mdate": 1674983355749, "content": {"title": "Boosting Randomized Smoothing with Variance Reduced Classifiers", "abstract": "Randomized Smoothing (RS) is a promising method for obtaining robustness certi\ufb01cates by evaluating a base model under noise. In this work, we: (i) theoretically motivate why ensembles are a particularly suitable choice as base models for RS, and (ii) empirically con\ufb01rm this choice, obtaining state-of-the-art results in multiple settings. The key insight of our work is that the reduced variance of ensembles over the perturbations introduced in RS leads to signi\ufb01cantly more consistent classi\ufb01cations for a given input. This, in turn, leads to substantially increased certi\ufb01able radii for samples close to the decision boundary. Additionally, we introduce key optimizations which enable an up to 55-fold decrease in sample complexity of RS for predetermined radii, thus drastically reducing its computational overhead. Experimentally, we show that ensembles of only 3 to 10 classi\ufb01ers consistently improve on their strongest constituting model with respect to their average certi\ufb01ed radius (ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new state-of-the-art ACR of 0.86 and 1.11, respectively. We release all code and models required to reproduce our results at https://github.com/eth-sri/smoothing-ensembles."}}
