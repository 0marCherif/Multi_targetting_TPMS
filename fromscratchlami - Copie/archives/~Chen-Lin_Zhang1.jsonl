{"id": "NZ8FA9wDTBv", "cdate": 1690848000000, "mdate": 1695952057598, "content": {"title": "Salvage of Supervision in Weakly Supervised Object Detection and Segmentation", "abstract": "Weakly supervised vision tasks, including detection and segmentation, have attracted much attention in the vision community recently. However, the lack of detailed and precise annotations in the weakly supervised case leads to a large accuracy gap between weakly- and fully-supervised methods. In this article, we propose a new framework, Salvage of Supervision (SoS), with the key idea being to effectively harness every potentially useful supervisory signal in weakly supervised vision tasks. Starting with weakly supervised object detection (WSOD), we propose SoS-WSOD to shrink the technology gap between WSOD and FSOD, which utilizes the weak image-level labels, the pseudo-labels, and the power of semi-supervised object detection for WSOD. Moreover, SoS-WSOD removes restrictions in traditional WSOD methods, including the reliance on ImageNet pretraining and inability to use modern backbones. The SoS framework also extends to weakly supervised semantic segmentation and instance segmentation. On several weakly supervised vision benchmarks, SoS achieves significant performance boost and generalization ability."}}
{"id": "_GMFHJ_y19H", "cdate": 1682899200000, "mdate": 1695949349170, "content": {"title": "Weakly supervised foreground learning for weakly supervised localization and detection", "abstract": ""}}
{"id": "E7d8LtwP3P", "cdate": 1672531200000, "mdate": 1681649660073, "content": {"title": "A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector", "abstract": ""}}
{"id": "pFcAGcltC1", "cdate": 1640995200000, "mdate": 1681649659892, "content": {"title": "ActionFormer: Localizing Moments of Actions with Transformers", "abstract": ""}}
{"id": "dU-jHmNy54z", "cdate": 1640995200000, "mdate": 1652633765134, "content": {"title": "ActionFormer: Localizing Moments of Actions with Transformers", "abstract": "Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer -- a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at http://github.com/happyharrycn/actionformer_release."}}
{"id": "_wxUVThacG", "cdate": 1640995200000, "mdate": 1667403487188, "content": {"title": "Minimum Efforts to Build an End-to-End Spatial-Temporal Action Detector", "abstract": "Spatial-temporal action detection is a vital part of video understanding. Current spatial-temporal action detection methods mostly use an object detector to obtain person candidates and classify these person candidates into different action categories. So-called two-stage methods are heavy and hard to apply in real-world applications. Some existing methods build one-stage pipelines, But a large performance drop exists with the vanilla one-stage pipeline and extra classification modules are needed to achieve comparable performance. In this paper, we explore a simple and effective pipeline to build a strong one-stage spatial-temporal action detector. The pipeline is composed by two parts: one is a simple end-to-end spatial-temporal action detector. The proposed end-to-end detector has minor architecture changes to current proposal-based detectors and does not add extra action classification modules. The other part is a novel labeling strategy to utilize unlabeled frames in sparse annotated data. We named our model as SE-STAD. The proposed SE-STAD achieves around 2% mAP boost and around 80% FLOPs reduction. Our code will be released at https://github.com/4paradigm-CV/SE-STAD."}}
{"id": "82Gm7AaLAw", "cdate": 1640995200000, "mdate": 1667403487181, "content": {"title": "Salvage of Supervision in Weakly Supervised Object Detection", "abstract": ""}}
{"id": "YxFa0uGwXyD", "cdate": 1623630419183, "mdate": 1623630419183, "content": {"title": "Rethinking the Route Towards Weakly Supervised Object Localization", "abstract": "Weakly supervised object localization (WSOL) aims to localize objects with only image-level labels. Previous methods often try to utilize feature maps and classification weights to localize objects using image level annotations indirectly. In this paper, we demonstrate that weakly supervised object localization should be divided into two parts: the class-agnostic object localization and the object classification. For class-agnostic object localization, we should use class-agnostic methods to generate noisy pseudo annotations and then perform bounding box regression on them without class labels. We propose the pseudo supervised\nobject localization (PSOL) method as a new way to solve WSOL. Our PSOL models have good transferability across\ndifferent datasets without fine-tuning. With the generated pseudo bounding boxes, we achieve 58.00% localization accuracy on ImageNet and 74.97% localization accuracy on CUB-200, which have a large edge over previous models."}}
{"id": "rkYqTJ_TNIK", "cdate": 1609459200000, "mdate": 1631242758659, "content": {"title": "Weakly Supervised Foreground Learning for Weakly Supervised Localization and Detection", "abstract": "Modern deep learning models require large amounts of accurately annotated data, which is often difficult to satisfy. Hence, weakly supervised tasks, including weakly supervised object localization~(WSOL) and detection~(WSOD), have recently received attention in the computer vision community. In this paper, we motivate and propose the weakly supervised foreground learning (WSFL) task by showing that both WSOL and WSOD can be greatly improved if groundtruth foreground masks are available. More importantly, we propose a complete WSFL pipeline with low computational cost, which generates pseudo boxes, learns foreground masks, and does not need any localization annotations. With the help of foreground masks predicted by our WSFL model, we achieve 72.97% correct localization accuracy on CUB for WSOL, and 55.7% mean average precision on VOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL model also shows excellent transfer ability."}}
{"id": "CEN56HZbGgv", "cdate": 1609459200000, "mdate": 1652633765133, "content": {"title": "Salvage of Supervision in Weakly Supervised Detection", "abstract": "Weakly supervised object detection~(WSOD) has recently attracted much attention. However, the lack of bounding-box supervision makes its accuracy much lower than fully supervised object detection (FSOD), and currently modern FSOD techniques cannot be applied to WSOD. To bridge the performance and technical gaps between WSOD and FSOD, this paper proposes a new framework, Salvage of Supervision (SoS), with the key idea being to harness every potentially useful supervisory signal in WSOD: the weak image-level labels, the pseudo-labels, and the power of semi-supervised object detection. This paper proposes new approaches to utilize these weak and noisy signals effectively, and shows that each type of supervisory signal brings in notable improvements, outperforms existing WSOD methods (which mainly use only the weak labels) by large margins. The proposed SoS-WSOD method also has the ability to freely use modern FSOD techniques. SoS-WSOD achieves 64.4 $m\\text{AP}_{50}$ on VOC2007, 61.9 $m\\text{AP}_{50}$ on VOC2012 and 16.6 $m\\text{AP}_{50:95}$ on MS-COCO, and also has fast inference speed. Ablations and visualization further verify the effectiveness of SoS."}}
