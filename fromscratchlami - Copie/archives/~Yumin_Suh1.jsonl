{"id": "21FNoED07B", "cdate": 1696382905153, "mdate": 1696382905153, "content": {"title": "Efficient Controllable Multi-Task Architectures", "abstract": "We aim to train a multi-task model such that users can\nadjust the desired compute budget and relative importance\nof task performances after deployment, without retraining.\nThis enables optimizing performance for dynamically varying user needs, without heavy computational overhead to\ntrain and save models for various scenarios. To this end,\nwe propose a multi-task model consisting of a shared encoder and task-specific decoders where both encoder and\ndecoder channel widths are slimmable. Our key idea is to\ncontrol the task importance by varying the capacities of\ntask-specific decoders, while controlling the total computational cost by jointly adjusting the encoder capacity. This\nimproves overall accuracy by allowing a stronger encoder\nfor a given budget, increases control over computational\ncost, and delivers high-quality slimmed sub-architectures\nbased on user\u2019s constraints. Our training strategy involves a\nnovel \u2018Configuration-Invariant Knowledge Distillation\u2019 loss\nthat enforces backbone representations to be invariant under\ndifferent runtime width configurations to enhance accuracy.\nFurther, we present a simple but effective search algorithm\nthat translates user constraints to runtime width configurations of both the shared encoder and task decoders, for\nsampling the sub-architectures. The key rule for the search\nalgorithm is to provide a larger computational budget to\nthe higher preferred task decoder, while searching a shared\nencoder configuration that enhances the overall MTL performance. Various experiments on three multi-task benchmarks\n(PASCALContext, NYUDv2, and CIFAR100-MTL) with diverse backbone architectures demonstrate the advantage of\nour approach. For example, our method shows a higher\ncontrollability by \u223c 33.5% in the NYUD-v2 dataset over\nprior methods, while incurring much less compute cost."}}
{"id": "O7gAffL9a0", "cdate": 1663849814751, "mdate": null, "content": {"title": "Confidence and Dispersity Speak: Characterising Prediction Matrix for Unsupervised Accuracy Estimation", "abstract": "This work focuses on estimating how well a model performs on out-of-distribution (OOD) datasets without using labels. Our intuition is that a well-performing model should give predictions with high confidence and high dispersity. While recent methods study the prediction confidence, this work newly finds dispersity is another informative cue. Confidence reflects whether the individual prediction is certain; dispersity indicates how the overall predictions are distributed across all categories. To achieve a more accurate estimation, we propose to jointly consider these two properties by using the nuclear norm of the prediction matrix. In our experiments, we extensively validate the effectiveness of nuclear norm for various models (e.g., ViT and ConvNeXt), different datasets (e.g., ImageNet and CUB-200), and diverse types of distribution shifts (e.g., style shift and reproduction shift). We show that the nuclear norm is more accurate and robust in predicting OOD accuracy than existing methods. Lastly, we study the limitation of the nuclear norm and discuss potential directions."}}
{"id": "iMqwIyhGwA", "cdate": 1640995200000, "mdate": 1668022327676, "content": {"title": "On Generalizing Beyond Domains in Cross-Domain Continual Learning", "abstract": "Humans have the ability to accumulate knowledge of new tasks in varying conditions, but deep neural networks of-ten suffer from catastrophic forgetting of previously learned knowledge after learning a new task. Many recent methods focus on preventing catastrophic forgetting under the assumption of train and test data following similar distributions. In this work, we consider a more realistic scenario of continual learning under domain shifts where the model must generalize its inference to an unseen domain. To this end, we encourage learning semantically meaningful features by equipping the classifier with class similarity metrics as learning parameters which are obtained through Mahalanobis similarity computations. Learning of the backbone representation along with these extra parameters is done seamlessly in an end-to-end manner. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation. We demonstrate that, to a great extent, existing continual learning algorithms fail to handle the forgetting issue under multiple distributions, while our proposed approach learns new tasks under domain shift with accuracy boosts up to 10% on challenging datasets such as DomainNet and OfficeHome."}}
{"id": "KIS5pN0nSl", "cdate": 1640995200000, "mdate": 1668022327693, "content": {"title": "Controllable Dynamic Multi-Task Architectures", "abstract": "Multi-task learning commonly encounters competition for resources among tasks, specifically when model capac-ity is limited. This challenge motivates models which al-low control over the relative importance of tasks and total compute cost during inference time. In this work, we pro-pose such a controllable multi-task network that dynami-cally adjusts its architecture and weights to match the de-sired task preference as well as the resource constraints. In contrast to the existing dynamic multi-task approaches that adjust only the weights within a fixed architecture, our approach affords the flexibility to dynamically control the total computational cost and match the user-preferred task importance better. We propose a disentangled training of two hype rnetwo rks, by exploiting task affinity and a novel branching regularized loss, to take input prefer-ences and accordingly predict tree-structured models with adapted weights. Experiments on three multi-task bench-marks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of our approach. Project page is available at https://www.nec-labs.com/-mas/DYMU."}}
{"id": "1kWerh98Y4Z", "cdate": 1640995200000, "mdate": 1668022327688, "content": {"title": "Learning Semantic Segmentation from Multiple Datasets with Label Shifts", "abstract": "While it is desirable to train segmentation models on an aggregation of multiple datasets, a major challenge is that the label space of each dataset may be in conflict with one another. To tackle this challenge, we propose UniSeg, an effective and model-agnostic approach to automatically train segmentation models across multiple datasets with heterogeneous label spaces, without requiring any manual relabeling efforts. Specifically, we introduce two new ideas that account for conflicting and co-occurring labels to achieve better generalization performance in unseen domains. First, we identify a gradient conflict in training incurred by mismatched label spaces and propose a class-independent binary cross-entropy loss to alleviate such label conflicts. Second, we propose a loss function that considers class-relationships across datasets for a better multi-dataset training scheme. Extensive quantitative and qualitative analyses on road-scene datasets show that UniSeg improves over multi-dataset baselines, especially on unseen datasets, e.g., achieving more than 8%p gain in IoU on KITTI. Furthermore, UniSeg achieves 39.4% IoU on the WildDash2 public benchmark, making it one of the strongest submissions in the zero-shot setting. Our project page is available at https://www.nec-labs.com/~mas/UniSeg ."}}
{"id": "8uX44XqkYKa", "cdate": 1609459200000, "mdate": null, "content": {"title": "Cross-Domain Similarity Learning for Face Recognition in Unseen Domains", "abstract": "Face recognition models trained under the assumption of identical training and test distributions often suffer from poor generalization when faced with unknown variations, such as a novel ethnicity or unpredictable individual make-ups during test time. In this paper, we introduce a novel cross-domain metric learning loss, which we dub Cross-Domain Triplet (CDT) loss, to improve face recognition in unseen domains. The CDT loss encourages learning semantically meaningful features by enforcing compact feature clusters of identities from one domain, where the compactness is measured by underlying similarity metrics that belong to another training domain with different statistics. Intuitively, it discriminatively correlates explicit metrics derived from one domain, with triplet samples from another domain in a unified loss function to be minimized within a network, which leads to better alignment of the training domains. The network parameters are further enforced to learn generalized features under domain shift, in a model-agnostic learning pipeline. Unlike the recent work of Meta Face Recognition, our method does not require careful hard-pair sample mining and filtering strategy during training. Extensive experiments on various face recognition benchmarks show the superiority of our method in handling variations, compared to baseline and the state-of-the-art methods."}}
{"id": "u5H2tCIfP4", "cdate": 1580416730045, "mdate": null, "content": {"title": "Graph Matching via Sequential Monte Carlo", "abstract": "Graph matching is a powerful tool for computer vision andmachine learning. In this paper, a novel approach to graph matchingis developed based on the sequential Monte Carlo framework. By con-structing a sequence of intermediate target distributions, the proposedalgorithm sequentially performs a sampling and importance resamplingto maximize the graph matching objective. Through the sequential sam-pling procedure, the algorithm effectively collects potential matches un-der one-to-one matching constraints to avoid the adverse effect of outliersand deformation. Experimental evaluations on synthetic graphs and realimages demonstrate its higher robustness to deformation and outliers."}}
{"id": "1X9kzHQJMR", "cdate": 1580416622478, "mdate": null, "content": {"title": "Discrete Tabu Search for Graph Matching", "abstract": "Graph matching is a fundamental problem in computervision.  In this paper, we propose a novel graph match-ing algorithm based on tabu search [13].  The proposedmethod solves graph matching problem by casting it intoan equivalent weighted maximum clique problem of thecorresponding association graph, which we further penal-ize through introducing negative weights. Subsequent tabusearch optimization allows for overcoming the conventionof using positive weights. The method\u2019s distinct feature isthat it utilizes the history of search to make more strate-gic decisions while looking for the optimal solution, thuseffectively escaping local optima and in practice achiev-ing superior results. The proposed method, unlike the ex-isting algorithms, enables direct optimization in the orig-inal discrete space while encouraging rather than artifi-cially enforcing hard one-to-one constraint, thus resultingin better solution.  The experiments demonstrate the ro-bustness of the algorithm in a variety of settings, present-ing the state-of-the-art results.  The code is available athttp://cv.snu.ac.kr/research/ \u0303DTSGM/"}}
{"id": "Y9h-5M6o7k", "cdate": 1580416545417, "mdate": null, "content": {"title": "Subgraph Matching using Compactness Priorfor Robust Feature Correspondence", "abstract": "Feature correspondence plays a central role in variouscomputer  vision  applications.   It  is  widely  formulated  asa  graph  matching  problem  due  to  its  robust  performanceunder challenging conditions, such as background clutter,object  deformation  and  repetitive  patterns.   A  variety  offast and accurate algorithms have been proposed for graphmatching.  However, most of them focus on improving therecall  of  the  solution  while  rarely  considering  its  preci-sion, thus inducing a solution with numerous outliers.  Toaddress both precision and recall feature correspondenceshould rather be formulated as a subgraph matching prob-lem. This paper proposes a new subgraph matching formu-lation which uses a compactness prior, an additional con-straint that prefers sparser solutions and effectively elimi-nates outliers.  To solve the new optimization problem, wepropose  a  meta-algorithm  based  on  Markov  chain  MonteCarlo.    By  constructing  Markov  chain  on  the  restrictedsearch  space  instead  of  the  original  solution  space,  ourmethod  approximates  the  solution  effectively.   The  exper-iments  indicate  that  our  proposed  formulation  and  algo-rithm significantly improve the baseline performance underchallenging conditions when both outliers and deformationnoise are present"}}
{"id": "3IqbCoNft", "cdate": 1580416426051, "mdate": null, "content": {"title": "Part-Aligned Bilinear Representationsfor Person Re-identification", "abstract": "Comparing the appearance of corresponding body parts is essentialfor person re-identification. As body parts are frequently misaligned between thedetected human boxes, an image representation that can handle this misalign-ment is required. In this paper, we propose a network that learns a part-alignedrepresentation for person re-identification. Our model consists of a two-streamnetwork, which generates appearance and body part feature maps respectively,and a bilinear-pooling layer that fuses two feature maps to an image descriptor.We show that it results in a compact descriptor, where the image matching sim-ilarity is equivalent to an aggregation of the local appearance similarities of thecorresponding body parts. Since the image similarity does not depend on the rel-ative positions of parts, our approach significantly reduces the part misalignmentproblem. Training the network does not require any part annotation on the personre-identification dataset. Instead, we simply initialize the part sub-stream usinga pre-trained sub-network of an existing pose estimation network and train thewhole network to minimize the re-identification loss. We validate the effective-ness of our approach by demonstrating its superiority over the state-of-the-artmethods on the standard benchmark datasets including Market-1501, CUHK03,CUHK01and DukeMTMC, and standard video dataset MARS."}}
