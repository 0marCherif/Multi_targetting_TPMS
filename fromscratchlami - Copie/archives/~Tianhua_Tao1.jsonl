{"id": "q6TlctYY3m", "cdate": 1705960346685, "mdate": null, "content": {"title": "SlimPajama-DC: Understanding Data Combinations for LLM Training", "abstract": "This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama [33] is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset [7] contributed by Together. We\u2019ve termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highlydeduplicated multi-source datasets in the combination. To study this, we construct six configurations of SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT [11] model with Alibi [28] and SwiGLU [32]. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16 \u00d7 CS-2 cluster with a total of 80\nPFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our models and the separate SlimPajamaDC datasets are available at: link1 and original SlimPajama is at: link2."}}
{"id": "H7kukcflIKs", "cdate": 1640995200000, "mdate": 1664616736162, "content": {"title": "On the Learning of Non-Autoregressive Transformers", "abstract": "Non-autoregressive Transformer (NAT) is a family of text generation models, which aims to reduce the decoding latency by predicting the whole sentences in parallel. However, such latency reduction ..."}}
{"id": "rmMOupN1Sqp", "cdate": 1632875464319, "mdate": null, "content": {"title": "Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation", "abstract": "Neural text generation models are typically trained by maximizing log-likelihood with the sequence cross entropy loss, which encourages an \\emph{exact} token-by-token match between a target sequence with a generated sequence. Such training objective is sub-optimal when the target sequence is not perfect, e.g., when the target sequence is corrupted with noises, or when only weak sequence supervision is available. To address this challenge, we propose a novel Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a target $n$-gram with all $n$-grams in the generated sequence. Drawing inspirations from the classical convolutional networks (ConvNets) which capture shift-invariance in image modeling, EISL is designed to be robust to the shift of $n$-grams to tolerate various noises and edits in the target sequences. Moreover, the EISL computation is essentially a convolution operation with target $n$-grams as kernels, which is easy to implement and efficient to compute with existing libraries. To demonstrate the effectiveness of EISL, we conduct experiments on a wide range of tasks, including machine translation with noisy target sequences, unsupervised text style transfer with only weak training signals, and non-autoregressive generation with non-predefined generation order. Experimental results show our method significantly outperforms the common cross-entropy loss and other strong baselines on all the tasks. "}}
