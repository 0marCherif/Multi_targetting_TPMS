{"id": "zWwrB9wenY1U", "cdate": 1663849899032, "mdate": null, "content": {"title": "Soundness and Completeness: An Algorithmic Perspective on Evaluation of Feature Attribution", "abstract": "Feature attribution is a fundamental approach to explaining neural networks by quantifying the importance of input features for a model's prediction. Although a variety of feature attribution methods have been proposed, there is little consensus on the assessment of attribution methods. In this study, we empirically show the limitations of \\emph{order-based} and \\emph{model-retraining} metrics. To overcome the limitations and enable evaluation with higher granularity, we propose a novel method to evaluate the \\emph{completeness} and \\emph{soundness} of feature attribution methods. Our proposed evaluation metrics are mathematically grounded on algorithm theory and require no knowledge of \"ground truth\" informative features. We validate our proposed metrics by conducting experiments on synthetic and real-world datasets. Lastly, we use the proposed metrics to benchmark a wide range of feature attribution methods. Our evaluation results provide an innovative perspective on comparing feature attribution methods. Code is in the supplementary material. "}}
{"id": "1nU3UdUHtm", "cdate": 1640995200000, "mdate": 1683879758961, "content": {"title": "Analyzing the Effects of Handling Data Imbalance on Learned Features from Medical Images by Looking Into the Models", "abstract": "One challenging property lurking in medical datasets is the imbalanced data distribution, where the frequency of the samples between the different classes is not balanced. Training a model on an imbalanced dataset can introduce unique challenges to the learning problem where a model is biased towards the highly frequent class. Many methods are proposed to tackle the distributional differences and the imbalanced problem. However, the impact of these approaches on the learned features is not well studied. In this paper, we look deeper into the internal units of neural networks to observe how handling data imbalance affects the learned features. We study several popular cost-sensitive approaches for handling data imbalance and analyze the feature maps of the convolutional neural networks from multiple perspectives: analyzing the alignment of salient features with pathologies and analyzing the pathology-related concepts encoded by the networks. Our study reveals differences and insights regarding the trained models that are not reflected by quantitative metrics such as AUROC and AP and show up only by looking at the models through a lens."}}
{"id": "HglgPZAYhcG", "cdate": 1621629805017, "mdate": null, "content": {"title": "Fine-Grained Neural Network Explanation by Identifying Input Features with Predictive Information", "abstract": "One principal approach for illuminating a black-box neural network is feature attribution, i.e. identifying the importance of input features for the network\u2019s prediction. The predictive information of features is recently proposed as a proxy for the measure of their importance. So far, the predictive information is only identified for latent features by placing an information bottleneck within the network. We propose a method to identify features with predictive information in the input domain. The method results in fine-grained identification of input features' information and is agnostic to network architecture. The core idea of our method is leveraging a bottleneck on the input that only lets input features associated with predictive latent features pass through. We compare our method with several feature attribution methods using mainstream feature attribution evaluation experiments. The code is publicly available."}}
{"id": "vIUTwImhfz", "cdate": 1609459200000, "mdate": 1683879759693, "content": {"title": "Fine-Grained Neural Network Explanation by Identifying Input Features with Predictive Information", "abstract": "One principal approach for illuminating a black-box neural network is feature attribution, i.e. identifying the importance of input features for the network\u2019s prediction. The predictive information of features is recently proposed as a proxy for the measure of their importance. So far, the predictive information is only identified for latent features by placing an information bottleneck within the network. We propose a method to identify features with predictive information in the input domain. The method results in fine-grained identification of input features' information and is agnostic to network architecture. The core idea of our method is leveraging a bottleneck on the input that only lets input features associated with predictive latent features pass through. We compare our method with several feature attribution methods using mainstream feature attribution evaluation experiments. The code is publicly available."}}
{"id": "32H-SUSvzB", "cdate": 1609459200000, "mdate": 1683879759700, "content": {"title": "Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features", "abstract": "Neural networks have demonstrated remarkable performance in classification and regression tasks on chest X-rays. In order to establish trust in the clinical routine, the networks\u2019 prediction mechanism needs to be interpretable. One principal approach to interpretation is feature attribution. Feature attribution methods identify the importance of input features for the output prediction. Building on Information Bottleneck Attribution (IBA) method, for each prediction we identify the chest X-ray regions that have high mutual information with the network\u2019s output. Original IBA identifies input regions that have sufficient predictive information. We propose Inverse IBA to identify all informative regions. Thus all predictive cues for pathologies are highlighted on the X-rays, a desirable property for chest X-ray diagnosis. Moreover, we propose Regression IBA for explaining regression models. Using Regression IBA we observe that a model trained on cumulative severity score labels implicitly learns the severity of different X-ray regions. Finally, we propose Multi-layer IBA to generate higher resolution and more detailed attribution/saliency maps. We evaluate our methods using both human-centric (ground-truth-based) interpretability metrics, and human-agnostic feature importance metrics on NIH Chest X-ray8 and BrixIA datasets. The code ( https://github.com/CAMP-eXplain-AI/CheXplain-IBA ) is publicly available."}}
{"id": "a3RXI47NgK", "cdate": 1546300800000, "mdate": 1683879759693, "content": {"title": "Logistic Regression with Robust Bootstrapping", "abstract": "The bootstrap method is a widely used technique for statistical learning and inference. However, its performance can be dramatically affected by outliers in the training data. In this paper, we propose two robust bootstrapping algorithms for logistic regression. These methods have several compelling advantages: they alleviate the dependency on fine-tuning the hyperparameters, and can still remain robust against highly contaminated data. The performance of the proposed algorithms is evaluated on synthetic and real-world data sets."}}
