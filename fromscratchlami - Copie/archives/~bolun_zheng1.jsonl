{"id": "4maAiUt0A4", "cdate": 1652737362073, "mdate": null, "content": {"title": "Boosting Out-of-distribution Detection with Typical Features", "abstract": "Out-of-distribution (OOD) detection is a critical task for ensuring the reliability and safety of deep neural networks in real-world scenarios. Different from most previous OOD detection methods that focus on designing OOD scores or introducing diverse outlier examples to retrain the model, we delve into the obstacle factors in OOD detection from the perspective of typicality and regard the feature's high-probability region of the deep model as the feature's typical set. We propose to rectify the feature into its typical set and calculate the OOD score with the typical features to achieve reliable uncertainty estimation. The feature rectification can be conducted as a plug-and-play module with various OOD scores. We evaluate the superiority of our method on both the commonly used benchmark (CIFAR) and the more challenging high-resolution benchmark with large label space (ImageNet). Notably, our approach outperforms state-of-the-art methods by up to 5.11% in the average FPR95 on the ImageNet benchmark.  "}}
{"id": "v-RtR8hpSZ", "cdate": 1640995200000, "mdate": 1669110257109, "content": {"title": "Dense Attention-Guided Cascaded Network for Salient Object Detection of Strip Steel Surface Defects", "abstract": "Recently, more and more researchers have paid attention to the surface defect detection of strip steel. However, the performance of existing methods usually fails to detect the defect regions from some complex scenes, especially with the noise disturbance and diverse defect types. Therefore, this article proposes an end-to-end dense attention-guided cascaded network (DACNet) to detect salient objects (i.e., defects) on the strip steel surface, where the proposed DACNet is a U-shape network including an encoder and a decoder. The encoder first deploys multiresolution convolutional branches (i.e., high/medium/low) in a cascaded way. Concretely, the cascaded feature integration (CFI) unit fuses the deep features from the last convolutional blocks of multiresolution branches, yielding the enhanced high-level deep semantic feature. Subsequently, coupled with the multilevel deep features from high-resolution branch, the new multiscale deep features are capable of characterizing various defects. Then, driven by the dense attention mechanism which enables the deeper attention cues flow into decoding stages, the decoder progressively integrates the multiscale deep features into the final saliency map, where the dense attention is designed to steer deep features pay more concerns to the defect regions. Comprehensive experiments are conducted on the public strip steel datasets, and the experimental results demonstrate that our model consistently outperforms the state-of-the-art models in all evaluation metrics."}}
{"id": "qd-kOGDGvgV", "cdate": 1640995200000, "mdate": 1669110257165, "content": {"title": "CBREN: Convolutional Neural Networks for Constant Bit Rate Video Quality Enhancement", "abstract": "Constant bit rate (CBR) videos are widely used in streaming playback applications. However, the image quality of the CBR video is often unstable, especially for scenes with large motion. To this end, we design a new model to represent the distortion of High Efficiency Video Coding (HEVC) constant bit rate video, and propose a neural network for a constant bit rate video quality enhancement (CBREN). We propose a dual-domain restoration module (DRM) to jointly learn the prior knowledge in the pixel domain and the frequency domain. To address the degradation resulting from compression, we propose a two-step quantization degradation estimation strategy. The Inverse DCT (IDCT) Translation Unit (ITU) is used to constrain the quantization table of the constant bit rate video to a suitable range, and the Dynamic Alpha Unit (DAU) is used to fine-tune the quantization table according to the content of each frame. In order to effectively reduce the block distortion of different sizes produced in the compression process, we adopt a multi-scale network. Extensive experiments show that our approach can greatly enhance the quality of CBR compressed video. Moreover, our method can also be applied to constant quantization parameter (CQP) video enhancement tasks, and is certainly superior to existing methods."}}
{"id": "ox8cpbijlA", "cdate": 1640995200000, "mdate": 1669110257082, "content": {"title": "Each Part Matters: Local Patterns Facilitate Cross-View Geo-Localization", "abstract": "Cross-view geo-localization is to spot images of the same geographic target from different platforms, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i> , drone-view cameras and satellites. It is challenging in the large visual appearance changes caused by extreme viewpoint variations. Existing methods usually concentrate on mining the fine-grained feature of the geographic target in the image center, but underestimate the contextual information in neighbor areas. In this work, we argue that neighbor areas can be leveraged as auxiliary information, enriching discriminative clues for geo-localization. Specifically, we introduce a simple and effective deep neural network, called Local Pattern Network ( <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">LPN</b> ), to take advantage of contextual information in an end-to-end manner. Without using extra part estimators, LPN adopts a square-ring feature partition strategy, which provides the attention according to the distance to the image center. It eases the part matching and enables the part-wise representation learning. Owing to the square-ring partition design, the proposed LPN has good scalability to rotation variations and achieves competitive results on three prevailing benchmarks, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i.e.</i> , University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN can be easily embedded into other frameworks to further boost performance."}}
{"id": "kaXF5BY2kjB", "cdate": 1640995200000, "mdate": 1669110257166, "content": {"title": "Constrained Predictive Filters for Single Image Bokeh Rendering", "abstract": "Bokeh rendering is a technique used to take pictures with out-of-focus areas to highlight regions of interest. Due to limitations in hardware and shooting condition, rendering a bokeh image from a full-focus image has attracted a lot of interest. In this paper, we model bokeh rendering as the combination of salient region retention and bokeh blurring, and propose a neural network to generate a realistic bokeh image from a single full-focus image through end-to-end training. Specifically, we propose a gate fusion block to estimate the salient area, and introduce a constrained predictive filter for salient region retention and bokeh blurring within a unified architecture. Further, we utilize a pixel coordinate-based map to enhance the training. Experimental results illustrate the effectiveness of our model. The comparison with state-of-the-art methods (PyNET [1], DMSHN [2], BGGAN [3], etc.) shows that our model produces better bokeh effects and retains salient objects."}}
{"id": "dM7nnyqwNf", "cdate": 1640995200000, "mdate": 1669110257258, "content": {"title": "Age-Invariant Face Recognition by Multi-Feature Fusionand Decomposition with Self-attention", "abstract": "Different from general face recognition, age-invariant face recognition (AIFR) aims at matching faces with a big age gap. Previous discriminative methods usually focus on decomposing facial feature into age-related and age-invariant components, which suffer from the loss of facial identity information. In this article, we propose a novel Multi-feature Fusion and Decomposition (MFD) framework for age-invariant face recognition, which learns more discriminative and robust features and reduces the intra-class variants. Specifically, we first sample multiple face images of different ages with the same identity as a face time sequence. Then, the multi-head attention is employed to capture contextual information from facial feature series, extracted by the backbone network. Next, we combine feature decomposition with fusion based on the face time sequence to ensure that the final age-independent features effectively represent the identity information of the face and have stronger robustness against the aging process. Besides, we also mitigate imbalanced age distribution in the training data by a re-weighted age loss. We experimented with the proposed MFD over the popular CACD and CACD-VS datasets, where we show that our approach improves the AIFR performance than previous state-of-the-art methods. We simultaneously show the performance of MFD on LFW dataset."}}
{"id": "cGdJZt_vGg", "cdate": 1640995200000, "mdate": 1669110257081, "content": {"title": "Learning Frequency Domain Priors for Image Demoireing", "abstract": "Image demoireing is a multi-faceted image restoration task involving both moire pattern removal and color restoration. In this paper, we raise a general degradation model to describe an image contaminated by moire patterns, and propose a novel multi-scale bandpass convolutional neural network (MBCNN) for single image demoireing. For moire pattern removal, we propose a multi-block-size learnable bandpass filters (M-LBFs), based on a block-wise frequency domain transform, to learn the frequency domain priors of moire patterns. We also introduce a new loss function named Dilated Advanced Sobel loss (D-ASL) to better sense the frequency information. For color restoration, we propose a two-step tone mapping strategy, which first applies a global tone mapping to correct for a global color shift, and then performs local fine tuning of the color per pixel. To determine the most appropriate frequency domain transform, we investigate several transforms including DCT, DFT, DWT, learnable non-linear transform and learnable orthogonal transform. We finally adopt the DCT. Our basic model won the AIM2019 demoireing challenge. Experimental results on three public datasets show that our method outperforms state-of-the-art methods by a large margin."}}
{"id": "XyWULXwlEc", "cdate": 1640995200000, "mdate": 1669110257257, "content": {"title": "Toward Understanding and Boosting Adversarial Transferability From a Distribution Perspective", "abstract": "Transferable adversarial attacks against Deep neural networks (DNNs) have received broad attention in recent years. An adversarial example can be crafted by a surrogate model and then attack the unknown target model successfully, which brings a severe threat to DNNs. The exact underlying reasons for the transferability are still not completely understood. Previous work mostly explores the causes from the model perspective, e.g., decision boundary, model architecture, and model capacity. Here, we investigate the transferability from the data distribution perspective and hypothesize that pushing the image away from its original distribution can enhance the adversarial transferability. To be specific, moving the image out of its original distribution makes different models hardly classify the image correctly, which benefits the untargeted attack, and dragging the image into the target distribution misleads the models to classify the image as the target class, which benefits the targeted attack. Towards this end, we propose a novel method that crafts adversarial examples by manipulating the distribution of the image. We conduct comprehensive transferable attacks against multiple DNNs to demonstrate the effectiveness of the proposed method. Our method can significantly improve the transferability of the crafted attacks and achieves state-of-the-art performance in both untargeted and targeted scenarios, surpassing the previous best method by up to 40% in some cases. In summary, our work provides new insight into studying adversarial transferability and provides a strong counterpart for future research on adversarial defense."}}
{"id": "It3dC5SlKXm", "cdate": 1640995200000, "mdate": 1669110257159, "content": {"title": "Towards Understanding and Boosting Adversarial Transferability from a Distribution Perspective", "abstract": "Transferable adversarial attacks against Deep neural networks (DNNs) have received broad attention in recent years. An adversarial example can be crafted by a surrogate model and then attack the unknown target model successfully, which brings a severe threat to DNNs. The exact underlying reasons for the transferability are still not completely understood. Previous work mostly explores the causes from the model perspective, e.g., decision boundary, model architecture, and model capacity. adversarial attacks against Deep neural networks (DNNs) have received broad attention in recent years. An adversarial example can be crafted by a surrogate model and then attack the unknown target model successfully, which brings a severe threat to DNNs. The exact underlying reasons for the transferability are still not completely understood. Previous work mostly explores the causes from the model perspective. Here, we investigate the transferability from the data distribution perspective and hypothesize that pushing the image away from its original distribution can enhance the adversarial transferability. To be specific, moving the image out of its original distribution makes different models hardly classify the image correctly, which benefits the untargeted attack, and dragging the image into the target distribution misleads the models to classify the image as the target class, which benefits the targeted attack. Towards this end, we propose a novel method that crafts adversarial examples by manipulating the distribution of the image. We conduct comprehensive transferable attacks against multiple DNNs to demonstrate the effectiveness of the proposed method. Our method can significantly improve the transferability of the crafted attacks and achieves state-of-the-art performance in both untargeted and targeted scenarios, surpassing the previous best method by up to 40$\\%$ in some cases."}}
{"id": "EoiZl85tzK", "cdate": 1640995200000, "mdate": 1669110257104, "content": {"title": "Rich Structural Index for Stereoscopic Image Quality Assessment", "abstract": "The human visual system (HVS), affected by viewing distance when perceiving the stereo image information, is of great significance to study of stereoscopic image quality assessment. Many methods of stereoscopic image quality assessment do not have comprehensive consideration for human visual perception characteristics. In accordance with this, we propose a Rich Structural Index (RSI) for Stereoscopic Image objective Quality Assessment (SIQA) method based on multi-scale perception characteristics. To begin with, we put the stereo pair into the image pyramid based on Contrast Sensitivity Function (CSF) to obtain sensitive images of different resolution. Then, we obtain local Luminance and Structural Index (LSI) in a locally adaptive manner on gradient maps which consider the luminance masking and contrast masking. At the same time we use Singular Value Decomposition (SVD) to obtain the Sharpness and Intrinsic Structural Index (SISI) to effectively capture the changes introduced in the image (due to distortion). Meanwhile, considering the disparity edge structures, we use gradient cross-mapping algorithm to obtain Depth Texture Structural Index (DTSI). After that, we apply the standard deviation method for the above results to obtain contrast index of reference and distortion components. Finally, for the loss caused by the randomness of the parameters, we use Support Vector Machine Regression based on Genetic Algorithm (GA-SVR) training to obtain the final quality score. We conducted a comprehensive evaluation with state-of-the-art methods on four open databases. The experimental results show that the proposed method has stable performance and strong competitive advantage."}}
