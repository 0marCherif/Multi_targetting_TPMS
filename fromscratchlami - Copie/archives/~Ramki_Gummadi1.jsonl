{"id": "6zsXr9oUhKS", "cdate": 1675034860946, "mdate": 1675034860946, "content": {"title": "A Parametric Class of Approximate Gradient Updates for Policy Optimization", "abstract": "Approaches to policy optimization have been motivated from diverse principles, based on how the\nparametric model is interpreted (e.g. value versus policy representation) or how the learning\nobjective is formulated, yet they share a common\ngoal of maximizing expected return. To better\ncapture the commonalities and identify key differences between policy optimization methods, we\ndevelop a unified perspective that re-expresses the\nunderlying updates in terms of a limited choice of\ngradient form and scaling function. In particular,\nwe identify a parameterized space of approximate\ngradient updates for policy optimization that is\nhighly structured, yet covers both classical and\nrecent examples, including PPO. As a result, we\nobtain novel yet well motivated updates that generalize existing algorithms in a way that can deliver benefits both in terms of convergence speed\nand final result quality. An experimental investigation demonstrates that the additional degrees\nof freedom provided in the parameterized family\nof updates can be leveraged to obtain non-trivial\nimprovements both in synthetic domains and on\npopular deep RL benchmarks"}}
{"id": "shbAgEsk3qM", "cdate": 1632875748869, "mdate": null, "content": {"title": "Understanding and Leveraging Overparameterization in Recursive Value Estimation", "abstract": "The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization.  Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck.  To better understand the utility of deep models in RL we present an analysis of recursive value estimation using \\emph{overparameterized} linear representations that provides useful, transferable findings.  First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to \\emph{different} fixed points than residual minimization (RM) in the overparameterized linear case.  We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints.  A practical consequence is that RM can be modified by a simple alteration of the backup targets to obtain the same fixed points as FVI and TD (when they converge), while universally ensuring stability.  Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM.  \nGiven this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models. \nIn particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively.  Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability. "}}
{"id": "p-X5QVUQf2", "cdate": 1546300800000, "mdate": null, "content": {"title": "Surrogate Objectives for Batch Policy Optimization in One-step Decision Making", "abstract": "We investigate batch policy optimization for cost-sensitive classification and contextual bandits---two related tasks that obviate exploration but require generalizing from observed rewards to action selections in unseen contexts. When rewards are fully observed, we show that the expected reward objective exhibits suboptimal plateaus and exponentially many local optima in the worst case. To overcome the poor landscape, we develop a convex surrogate that is calibrated with respect to entropy regularized expected reward. We then consider the partially observed case, where rewards are recorded for only a subset of actions. Here we generalize the surrogate to partially observed data, and uncover novel objectives for batch contextual bandit training. We find that surrogate objectives remain provably sound in this setting and empirically demonstrate state-of-the-art performance."}}
{"id": "fOCec5kEzWj", "cdate": 1514764800000, "mdate": null, "content": {"title": "Variational Rejection Sampling", "abstract": "Learning latent variable models with stochastic variational inference is challenging when the approximate posterior is far from the true posterior, due to high variance in the gradient estimates. W..."}}
{"id": "DZ8pMLF6qXH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Variational Rejection Sampling", "abstract": "Learning latent variable models with stochastic variational inference is challenging when the approximate posterior is far from the true posterior, due to high variance in the gradient estimates. We propose a novel rejection sampling step that discards samples from the variational posterior which are assigned low likelihoods by the model. Our approach provides an arbitrarily accurate approximation of the true posterior at the expense of extra computation. Using a new gradient estimator for the resulting unnormalized proposal distribution, we achieve average improvements of 3.71 nats and 0.21 nats over state-of-the-art single-sample and multi-sample alternatives respectively for estimating marginal log-likelihoods using sigmoid belief networks on the MNIST dataset."}}
