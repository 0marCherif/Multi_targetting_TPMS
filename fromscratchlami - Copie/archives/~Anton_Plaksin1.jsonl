{"id": "ttC9p-CtYT", "cdate": 1652737631345, "mdate": null, "content": {"title": "Continuous Deep Q-Learning in Optimal Control Problems: Normalized Advantage Functions Analysis", "abstract": "One of the most effective continuous deep reinforcement learning algorithms is normalized advantage functions (NAF). The main idea of NAF consists in the approximation of the Q-function by functions quadratic with respect to the action variable. This idea allows to apply the algorithm to continuous reinforcement learning problems, but on the other hand, it brings up the question of classes of problems in which this approximation is acceptable. The presented paper describes one such class. We consider reinforcement learning problems obtained by the discretization of certain optimal control problems. Based on the idea of NAF, we present a new family of quadratic functions and prove its suitable approximation properties. Taking these properties into account, we provide several ways to improve NAF. The experimental results confirm the efficiency of our improvements."}}
{"id": "53kBt5AXLM", "cdate": 1640995200000, "mdate": 1683880892861, "content": {"title": "Continuous Deep Q-Learning in Optimal Control Problems: Normalized Advantage Functions Analysis", "abstract": "One of the most effective continuous deep reinforcement learning algorithms is normalized advantage functions (NAF). The main idea of NAF consists in the approximation of the Q-function by functions quadratic with respect to the action variable. This idea allows to apply the algorithm to continuous reinforcement learning problems, but on the other hand, it brings up the question of classes of problems in which this approximation is acceptable. The presented paper describes one such class. We consider reinforcement learning problems obtained by the discretization of certain optimal control problems. Based on the idea of NAF, we present a new family of quadratic functions and prove its suitable approximation properties. Taking these properties into account, we provide several ways to improve NAF. The experimental results confirm the efficiency of our improvements."}}
{"id": "TVs3zZOOZ8t", "cdate": 1632875657916, "mdate": null, "content": {"title": "Continuous Deep Q-Learning in Optimal Control Problems: Normalized Advantage Functions Analysis", "abstract": "One of the most effective continuous deep reinforcement learning algorithms is normalized advantage functions (NAF). The main idea of NAF consists in the approximation of the Q-function by functions quadratic with respect to the action variable. This idea allows to apply the algorithm to continuous reinforcement learning problems, but on the other hand, it brings up the question of classes of problems in which this approximation is acceptable. The presented paper describes one such class. We consider reinforcement learning problems obtained by the time-discretization of certain optimal control problems. Based on the idea of NAF, we present a new family of quadratic functions and prove its suitable approximation properties. Taking these properties into account, we provide several ways to improve NAF. The experimental results confirm the efficiency of our improvements."}}
{"id": "D83t250634S", "cdate": 1609459200000, "mdate": 1682340661666, "content": {"title": "Viscosity Solutions of Hamilton-Jacobi-Bellman-Isaacs Equations for Time-Delay Systems", "abstract": ""}}
{"id": "oGeN2G6_5Uv", "cdate": 1577836800000, "mdate": 1682340661848, "content": {"title": "Minimax and Viscosity Solutions of Hamilton-Jacobi-Bellman Equations for Time-Delay Systems", "abstract": ""}}
