{"id": "vD5P0B8gjBU", "cdate": 1546300800000, "mdate": null, "content": {"title": "Garbage Collection Algorithms for Meta Data Updates in NAND Flash", "abstract": "Garbage collection (GC) is ubiquitously used to reclaim useful space during the data update process in NAND flash memories. Conventional GC algorithms keep track of a table storing the number of valid pages in each block and select ones with the smallest number of valid pages to erase. We notice that NAND flash not only stores user data but also keeps updating meta data frequently and one of the most important requirements of GC for meta data updates is latency predictability, which means the latency of GC should not only be low on average and on tail distributions, but also independent of workload, i.e., the sequence of meta data updates. It is also desirable to have all NAND pages/blocks written the same number of times to avoid any latency incurred by wear leveling. In this paper, we propose GC algorithms that do not require any look-up tables, which avoids the latency for reading the table and sorting the number of valid pages, and intrinsically enables all pages to be worn equally. Several GC algorithms are proposed to make trade-offs between over-provisioning and write amplification. We also provide sufficient and necessary conditions that the proposed GC algorithms will not encounter a deadlock, i.e., the algorithms can run continuously on all meta data update sequences without data loss."}}
{"id": "oEePlODL7AK", "cdate": 1514764800000, "mdate": null, "content": {"title": "Towards Robust File System Checkers", "abstract": "File systems may become corrupted for many reasons despite various protection techniques. Therefore, most file systems come with a checker to recover the file system to a consistent state. However, existing checkers are commonly assumed to be able to complete the repair without interruption, which may not be true in practice. In this work, we demonstrate via fault injection experiments that checkers of widely used file systems (EXT4, XFS, BtrFS, and F2FS) may leave the file system in an uncorrectable state if the repair procedure is interrupted unexpectedly. To address the problem, we first fix the ordering issue in the undo logging of\u00a0e2fsck and then build a general logging library (i.e., rfsck-lib) for strengthening checkers. To demonstrate the practicality, we integrate rfsck-lib with existing checkers and create two new checkers: rfsck-ext, a robust checker for Ext-family file systems, and rfsck-xfs, a robust checker for XFS file systems, both of which require only tens of lines of modification to the original versions. Both rfsck-ext and rfsck-xfs are resilient to faults in our experiments. Also, both checkers incur reasonable performance overhead (i.e., up to 12%) compared to the original unreliable versions. Moreover, rfsck-ext outperforms the patched\u00a0e2fsck by up to nine times while achieving the same level of robustness."}}
{"id": "XHxoUC2ITv", "cdate": 1514764800000, "mdate": null, "content": {"title": "Storage-Efficient Shared Memory Emulation", "abstract": "We study the design of storage-efficient algorithms for emulating atomic shared memory over an asynchronous, distributed message-passing system. Our first algorithm is an atomic single-writer multi-reader algorithm based on a novel erasure-coding technique, termed \\emph{multi-version code}. Next, we propose an extension of our single-writer algorithm to a multi-writer multi-reader environment. Our second algorithm combines replication and multi-version code, and is suitable in situations where we expect a large number of concurrent writes. Moreover, when the number of concurrent writes is bounded, we propose a simplified variant of the second algorithm that has a simple structure similar to the single-writer algorithm. Let $N$ be the number of servers, and the shared memory variable be of size 1 unit. Our algorithms have the following properties: (i) The write operation terminates if the number of server failures is bounded by a parameter $f$. The algorithms also guarantee the termination of the read as long as the number of writes concurrent with the read is smaller than a design parameter $\\nu$, and the number of server failures is bounded by $f$. (ii) The overall storage size for the first algorithm, and the steady-state storage size for the second algorithm, are all $N/\\lceil \\frac{N-2f}{\\nu} \\rceil$ units. Moreover, our simplified variant of the second algorithm achieves the worst-case storage cost of $N/\\lceil \\frac{N-2f}{\\nu} \\rceil$, asymptotically matching a lower bound by Cadambe et al. for $N \\gg f, \\nu \\le f+1$. (iii) The write and read operations only consist of a small number (2 to 3) of communication rounds. (iv) For all algorithms, the server maintains a simple data structure. A server only needs to store the information associated with the latest value it observes, similar to replication-based algorithms."}}
{"id": "0_lSZoJF64h", "cdate": 1514764800000, "mdate": null, "content": {"title": "Towards Robust File System Checkers", "abstract": ""}}
{"id": "U8uMwH_vj1", "cdate": 1483228800000, "mdate": null, "content": {"title": "Low Read Latency Rewriting Codes for Multi-Level 3-D NAND Flash", "abstract": "Rewriting codes can improve the lifetime capacity of NAND flash. All existing q-ary rewriting codes for NAND flash assume that the exact cell levels are known to decoders, and thus the number of reads needed is q - 1, while the current NAND technology enables low latency page reads by dividing a physical page into log q logical pages, so that the average number of reads per page is q-1 /log q . We consider 2-write rewriting codes that enable low latency page reads for multi-level NAND flash memories. We design a low read latency rewriting code that enables 17% capacity improvement and maintains 1.5 reads per page for four level multiple-level cells. We also design codes for eight-level triple level cells that enable 11% and 24% capacity improvement with 2 and 2.5 reads per page, respectively. The same Gray codes in current NAND technology that map binary data to physical cell levels are applied to avoid redesigning hardware and circuits inside the chip."}}
{"id": "xJYQ-nb9YlO", "cdate": 1451606400000, "mdate": null, "content": {"title": "Locally rewritable codes for resistive memories", "abstract": "We propose locally rewritable codes (LWC) for resistive memories inspired by locally repairable codes (LRC) for distributed storage systems. Small values of repair locality of LRC enable fast repair of a single failed node since the lost data in the failed node can be recovered by accessing only a small fraction of other nodes. By using rewriting locality, LWC can improve endurance limit and power consumption which are major challenges for resistive memories. We point out the duality between LRC and LWC, which indicates that existing construction methods of LRC can be applied to construct LWC."}}
{"id": "si8qxYYicNE", "cdate": 1451606400000, "mdate": null, "content": {"title": "Opening the Chrysalis: On the Real Repair Performance of MSR Codes", "abstract": ""}}
{"id": "rHUxuDFU22v", "cdate": 1451606400000, "mdate": null, "content": {"title": "Spider Codes: Practical erasure codes for distributed storage systems", "abstract": "Distributed storage systems use erasure codes to reliably store data with a small storage overhead. To further improve system performance, some novel erasure codes introduce new features such as the regenerating property or symbol locality, enabling these codes to have optimal repair times and optimal degraded read performance. Unfortunately, the introduction of these new features often exacerbates the performance of other system metrics such as encoding throughput, data reliability, and storage overhead, among others. In this paper we describe the intricate relationships between erasure code properties and system-level performance metrics, showing the different tradeoffs distributed storage designers need to face. We also present Spider Codes, a new erasure code achieving a practical trade-off between the different system-level performance metrics."}}
{"id": "Y_27GhJ9oIx", "cdate": 1451606400000, "mdate": null, "content": {"title": "Locally Rewritable Codes for Resistive Memories", "abstract": "Resistive memories, such as phase change memories and resistive random access memories, have attracted significant research interest because of their scalability, non-volatility, fast speed, and rewritability. However, their write endurance needs to be improved substantially for large-scale deployment of resistive memories. In addition, their write power consumption is much higher than the power consumption of read operation. Inspired by locally repairable codes (LRCs) recently introduced for distributed storage systems, we propose locally rewritable codes (LWCs) for resistive memories. We define a novel parameter of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">rewriting locality</i> , which can be connected to <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">repair locality</i> of LRC. As small values of repair locality of LRC enable fast repair in distributed storage systems, small values of rewriting locality of LWC are able to reduce the problems of write endurance and write power consumption. We show how a small value of rewriting locality can improve write endurance and power consumption by deriving the upper bounds on writing cost. Also, we point out the dual relation of LRC and LWC, which indicates that the existing construction methods of LRC can be applied to construct LWC. Finally, we investigate the construction of LWC with error correcting capability for random errors."}}
{"id": "868qJLHSBQl", "cdate": 1451606400000, "mdate": null, "content": {"title": "Locally rewritable codes for resistive memories", "abstract": "We propose locally rewritable codes (LWC) for resistive memories inspired by locally repairable codes (LRC) for distributed storage systems. Small values of repair locality of LRC enable fast repair of a single failed node since the lost data in the failed node can be recovered by accessing only a small fraction of other nodes. By using rewriting locality, LWC can improve endurance and power consumption which are major challenges for resistive memories. We point out the duality between LRC and LWC, which indicates that existing construction methods of LRC can be applied to construct LWC."}}
