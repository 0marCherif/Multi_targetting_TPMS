{"id": "sHtaFHJ8tPc", "cdate": 1640995200000, "mdate": 1666060354274, "content": {"title": "Style Fader Generative Adversarial Networks for Style Degree Controllable Artistic Style Transfer", "abstract": "Artistic style transfer is the task of synthesizing content images with learned artistic styles. Recent studies have shown the potential of Generative Adversarial Networks (GANs) for producing artistically rich stylizations. Despite the promising results, they usually fail to control the generated images' style degree, which is inflexible and limits their applicability for practical use. To address the issue, in this paper, we propose a novel method that for the first time allows adjusting the style degree for existing GAN-based artistic style transfer frameworks in real time after training. Our method introduces two novel modules into existing GAN-based artistic style transfer frameworks: a Style Scaling Injection (SSI) module and a Style Degree Interpretation (SDI) module. The SSI module accepts the value of Style Degree Factor (SDF) as the input and outputs parameters that scale the feature activations in existing models, offering control signals to alter the style degrees of the stylizations. And the SDI module interprets the output probabilities of a multi-scale content-style binary classifier as the style degrees, providing a mechanism to parameterize the style degree of the stylizations. Moreover, we show that after training our method can enable existing GAN-based frameworks to produce over-stylizations. The proposed method can facilitate many existing GAN-based artistic style transfer frameworks with marginal extra training overheads and modifications. Extensive qualitative evaluations on two typical GAN-based style transfer models demonstrate the effectiveness of the proposed method for gaining style degree control for them."}}
{"id": "fuVgs4P-Fhb", "cdate": 1640995200000, "mdate": 1666060354313, "content": {"title": "DivSwapper: Towards Diversified Patch-based Arbitrary Style Transfer", "abstract": "Gram-based and patch-based approaches are two important research lines of style transfer. Recent diversified Gram-based methods have been able to produce multiple and diverse stylized outputs for the same content and style images. However, as another widespread research interest, the diversity of patch-based methods remains challenging due to the stereotyped style swapping process based on nearest patch matching. To resolve this dilemma, in this paper, we dive into the crux of existing patch-based methods and propose a universal and efficient module, termed DivSwapper, for diversified patch-based arbitrary style transfer. The key insight is to use an essential intuition that neural patches with higher activation values could contribute more to diversity. Our DivSwapper is plug-and-play and can be easily integrated into existing patch-based and Gram-based methods to generate diverse results for arbitrary styles. We conduct theoretical analyses and extensive experiments to demonstrate the effectiveness of our method, and compared with state-of-the-art algorithms, it shows superiority in diversity, quality, and efficiency."}}
{"id": "RvoGnhId0Kr", "cdate": 1640995200000, "mdate": 1666060354261, "content": {"title": "Texture Reformer: Towards Fast and Universal Interactive Texture Transfer", "abstract": "In this paper, we present the texture reformer, a fast and universal neural-based framework for interactive texture transfer with user-specified guidance. The challenges lie in three aspects: 1) the diversity of tasks, 2) the simplicity of guidance maps, and 3) the execution efficiency. To address these challenges, our key idea is to use a novel feed-forward multi-view and multi-stage synthesis procedure consisting of I) a global view structure alignment stage, II) a local view texture refinement stage, and III) a holistic effect enhancement stage to synthesize high-quality results with coherent structures and fine texture details in a coarse-to-fine fashion. In addition, we also introduce a novel learning-free view-specific texture reformation (VSTR) operation with a new semantic map guidance strategy to achieve more accurate semantic-guided and structure-preserved texture transfer. The experimental results on a variety of application scenarios demonstrate the effectiveness and superiority of our framework. And compared with the state-of-the-art interactive texture transfer algorithms, it not only achieves higher quality results but, more remarkably, also is 2-5 orders of magnitude faster."}}
{"id": "A5NhU9GLSp5", "cdate": 1640995200000, "mdate": 1666060354235, "content": {"title": "Dual distribution matching GAN", "abstract": ""}}
{"id": "hm0i-cunzGW", "cdate": 1621629894545, "mdate": null, "content": {"title": "Artistic Style Transfer with Internal-external Learning and Contrastive Learning", "abstract": "Although existing artistic style transfer methods have achieved significant improvement with deep neural networks, they still suffer from artifacts such as disharmonious colors and repetitive patterns. Motivated by this, we propose an internal-external style transfer method with two contrastive losses. Specifically, we utilize internal statistics of a single style image to determine the colors and texture patterns of the stylized image, and in the meantime, we leverage the external information of the large-scale style dataset to learn the human-aware style information, which makes the color distributions and texture patterns in the stylized image more reasonable and harmonious. In addition, we argue that existing style transfer methods only consider the content-to-stylization and style-to-stylization relations, neglecting the stylization-to-stylization relations. To address this issue, we introduce two contrastive losses, which pull the multiple stylization embeddings closer to each other when they share the same content or style, but push far away otherwise. We conduct extensive experiments, showing that our proposed method can not only produce visually more harmonious and satisfying artistic images, but also promote the stability and consistency of rendered video clips."}}
{"id": "zDMACNl-Gf", "cdate": 1609459200000, "mdate": 1666060354581, "content": {"title": "Diverse Image Style Transfer via Invertible Cross-Space Mapping", "abstract": "Image style transfer aims to transfer the styles of artworks onto arbitrary photographs to create novel artistic images. Although style transfer is inherently an underdetermined problem, existing approaches usually assume a deterministic solution, thus failing to capture the full distribution of possible outputs. To address this limitation, we propose a Diverse Image Style Transfer (DIST) framework which achieves significant diversity by enforcing an invertible cross-space mapping. Specifically, the framework consists of three branches: disentanglement branch, inverse branch, and stylization branch. Among them, the disentanglement branch factorizes artworks into content space and style space; the inverse branch encourages the invertible mapping between the latent space of input noise vectors and the style space of generated artistic images; the stylization branch renders the input content image with the style of an artist. Armed with these three branches, our approach is able to synthesize significantly diverse stylized images without loss of quality. We conduct extensive experiments and comparisons to evaluate our approach qualitatively and quantitatively. The experimental results demonstrate the effectiveness of our method."}}
{"id": "vJclpnw1Hn8", "cdate": 1609459200000, "mdate": 1666060354341, "content": {"title": "Real-Time High Realistic Web Display Method of Complex 3D Model", "abstract": "With the in-depth application of 3D models in many fields, the display of models, especially the model display based on B / S architecture, has become an indispensable part of the application of 3D models. The development of 3D modeling technology has led to the emergence of a large number of complex three-dimensional models with more than one hundred thousand facets and hundred megabytes of file size. Coupled with the limit of B / S architecture by the server's concurrency, network bandwidth, browser's processing capabilities, and many other objective factors, how to achieve its real-time and high-realistic display on the web pages has become one of the hot spots research in the field of graphics, which has great research significance and challenge. In order to resolve the above problems, we propose a new model display method. The method pre-renders the model into a two-dimensional image by sampling the subdivide surface of the model's circumscribed sphere and uses the view-dependent level-of-detail method to divide the pre-rendered image into different layers and tiles. Combined with the optimization, including adaptive and progressive network transmission, index-weighted user interaction prediction, and Hilbert-based image indexing, the method achieves a frame rate of more than 20 frames per second in the actual tests."}}
{"id": "guiLSVUCv2", "cdate": 1609459200000, "mdate": 1666060354301, "content": {"title": "Corrections to \"Real-Time High Realistic Web Display Method of Complex 3D Model\"", "abstract": "In the above article <xref ref-type=\"bibr\" rid=\"ref1\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">[1]</xref> , Table 5 is updated to <xref ref-type=\"table\" rid=\"table1\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Table 1</xref> . Table 6 in the article <xref ref-type=\"bibr\" rid=\"ref1\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">[1]</xref> is updated to <xref ref-type=\"table\" rid=\"table2\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Table 2</xref> ."}}
{"id": "eNBJwZl7ka", "cdate": 1609459200000, "mdate": 1666060354377, "content": {"title": "Diversified text-to-image generation via deep mutual information estimation", "abstract": ""}}
{"id": "QwJz2m2qpL", "cdate": 1609459200000, "mdate": 1666060354335, "content": {"title": "Evaluate and improve the quality of neural style transfer", "abstract": ""}}
