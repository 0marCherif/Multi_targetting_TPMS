{"id": "CcgFEQvdfm_", "cdate": 1663939409457, "mdate": null, "content": {"title": "Towards Provably Personalized Federated Learning via Threshold-Clustering of Similar Clients", "abstract": "Clustering clients with similar objectives together and learning a model per cluster is an intuitive and interpretable approach to personalization in federated learning (PFL). However, doing so with provable and optimal guarantees has remained an open challenge. In this work, we formalize personalized federated learning as a stochastic optimization problem where the stochastic gradients on a client may correspond to one of $K$ distributions. In such a setting, we show that using i) a simple thresholding based clustering algorithm, and ii) local client momentum obtains optimal convergence guarantees. In fact, our rates asymptotically match those obtained if we knew the true underlying clustering of the clients. Further, we extend our algorithm to the decentralized setting where each node performs clustering using itself as the center."}}
{"id": "qxcQqFUTIpQ", "cdate": 1663850001160, "mdate": null, "content": {"title": "Byzantine-robust Decentralized Learning via ClippedGossip", "abstract": "In this paper, we study the challenging task of Byzantine-robust decentralized training on arbitrary communication graphs. Unlike federated learning where workers communicate through a server, workers in the decentralized environment can only talk to their neighbors, making it harder to reach consensus and benefit from collaborative training. To address these issues, we propose an algorithm, termed ClippedGossip, for Byzantine-robust consensus and optimization, which is the first to provably converge to a $O(\\delta_{\\max}\\zeta^2/\\gamma^2)$ neighborhood of the stationary point for non-convex objectives under standard assumptions. Finally, we demonstrate the encouraging empirical performance of ClippedGossip under a large number of attacks."}}
{"id": "jXKKDEi5vJt", "cdate": 1632875439250, "mdate": null, "content": {"title": "Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing", "abstract": "In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.\n"}}
{"id": "Qo6kYy4SBI-", "cdate": 1621630041142, "mdate": null, "content": {"title": "RelaySum for Decentralized Deep Learning on Heterogeneous Data", "abstract": "In decentralized machine learning, workers compute model updates on their local data.\nBecause the workers only communicate with few neighbors without central coordination, these updates propagate progressively over the network.\nThis paradigm enables distributed training on networks without all-to-all connectivity, helping to protect data privacy as well as to reduce the communication cost of distributed training in data centers.\nA key challenge, primarily in decentralized deep learning, remains the handling of differences between the workers' local data distributions.\nTo tackle this challenge, we introduce the RelaySum mechanism for information propagation in decentralized learning.\nRelaySum uses spanning trees to distribute information exactly uniformly across all workers with finite delays depending on the distance between nodes.\nIn contrast, the typical gossip averaging mechanism only distributes data uniformly asymptotically while using the same communication volume per step as RelaySum.\nWe prove that RelaySGD, based on this mechanism, is independent of data heterogeneity and scales to many workers, enabling highly accurate decentralized deep learning on heterogeneous data."}}
{"id": "69EFStdgTD2", "cdate": 1601308334567, "mdate": null, "content": {"title": "Secure Byzantine-Robust Machine Learning", "abstract": "Increasingly machine learning systems are being deployed to edge servers and devices (e.g. mobile phones) and trained in a collaborative manner. Such distributed/federated/decentralized training raises a number of concerns about the robustness, privacy, and security of the procedure. While extensive work has been done in tackling with robustness, privacy, or security individually, their combination has rarely been studied. In this paper, we propose a secure multi-server protocol that offers both input privacy and Byzantine-robustness. In addition, this protocol is communication-efficient, fault-tolerant, and enjoys local differential privacy."}}
{"id": "7JSTDTZtn7-", "cdate": 1601308057518, "mdate": null, "content": {"title": "Byzantine-Robust Learning on Heterogeneous Datasets via Resampling", "abstract": "In Byzantine-robust distributed optimization, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages to the server. While this problem has received significant attention recently, most current defenses assume that the workers have identical data distribution. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks that circumvent these defenses leading to significant loss of performance. We then propose a universal resampling scheme that addresses data heterogeneity at a negligible computational cost. We theoretically and experimentally validate our approach, showing that combining resampling with existing robust algorithms is effective against challenging attacks.\n"}}
{"id": "S1Vw_wWObH", "cdate": 1514764800000, "mdate": null, "content": {"title": "COLA: Decentralized Linear Learning", "abstract": "Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and allows for unreliable and heterogeneous participating devices."}}
