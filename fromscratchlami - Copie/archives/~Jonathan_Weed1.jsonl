{"id": "BEYOkgAAr9", "cdate": 1675736093929, "mdate": 1675736093929, "content": {"title": "Plugin Estimation of Smooth Optimal Transport Maps", "abstract": "We analyze a number of natural estimators for the optimal transport map between two distributions and show that they are minimax optimal. We adopt the plugin approach: our estimators are simply optimal couplings between measures derived from our observations, appropriately extended so that they define functions on R^d. When the underlying map is assumed to be Lipschitz, we show that computing the optimal coupling between the empirical measures, and extending it using linear smoothers, already gives a minimax optimal estimator. When the underlying map enjoys higher regularity, we show that the optimal coupling between appropriate nonparametric density estimates yields faster rates. Our work also provides new bounds on the risk of corresponding plugin estimators for the quadratic Wasserstein distance, and we show how this problem relates to that of estimating optimal transport maps using stability arguments for smooth and strongly convex Brenier potentials. As an application of our results, we derive central limit theorems for plugin estimators of the squared Wasserstein distance, which are centered at their population counterpart when the underlying distributions have sufficiently smooth densities. In contrast to known central limit theorems for empirical estimators, this result easily lends itself to statistical inference for the quadratic Wasserstein distance."}}
{"id": "lmwqImMa-mj", "cdate": 1675736035592, "mdate": 1675736035592, "content": {"title": "Sharp Convergence Rates for Empirical Optimal Transport with Smooth Costs", "abstract": "We revisit the question of characterizing the convergence rate of plug-in estimators of optimal transport costs. It is well known that an empirical measure comprising independent samples from an absolutely continuous distribution on R^d converges to that distribution at the rate n^{-1/d} in Wasserstein distance, which can be used to prove that plug-in estimators of many optimal transport costs converge at this same rate. However, we show that when the cost is smooth, this analysis is loose: plug-in estimators based on empirical measures converge quadratically faster, at the rate n^{-2/d}. As a corollary, we show that the Wasserstein distance between two distributions is significantly easier to estimate when the measures are far apart. We also prove lower bounds, showing not only that our analysis of the plug-in estimator is tight, but also that no other estimator can enjoy significantly faster rates of convergence uniformly over all pairs of measures. Our proofs rely on empirical process theory arguments based on tight control of L^2 covering numbers for locally Lipschitz and semi-concave functions. As a byproduct of our proofs, we derive L^\\infty estimates on the displacement induced by the optimal coupling between any two measures satisfying suitable moment conditions, for a wide range of cost functions."}}
{"id": "UqNCVq9bdCM", "cdate": 1675735547452, "mdate": 1675735547452, "content": {"title": "Sharp Convergence Rates for Empirical Optimal Transport with Smooth Costs", "abstract": "We revisit the question of characterizing the convergence rate of plug-in estimators of optimal transport costs. It is well known that an empirical measure comprising independent samples from an absolutely continuous distribution on R^d converges to that distribution at the rate n^{-1/d} in Wasserstein distance, which can be used to prove that plug-in estimators of many optimal transport costs converge at this same rate. However, we show that when the cost is smooth, this analysis is loose: plug-in estimators based on empirical measures converge quadratically faster, at the rate n^{-2/d}. As a corollary, we show that the Wasserstein distance between two distributions is significantly easier to estimate when the measures are far apart. We also prove lower bounds, showing not only that our analysis of the plug-in estimator is tight, but also that no other estimator can enjoy significantly faster rates of convergence uniformly over all pairs of measures. Our proofs rely on empirical process theory arguments based on tight control of L^2 covering numbers for locally Lipschitz and semi-concave functions. As a byproduct of our proofs, we derive L^\\infty estimates on the displacement induced by the optimal coupling between any two measures satisfying suitable moment conditions, for a wide range of cost functions."}}
{"id": "nBOZCyPB9Q", "cdate": 1675735266485, "mdate": 1675735266485, "content": {"title": "Plugin Estimation of Smooth Optimal Transport Maps", "abstract": "We analyze a number of natural estimators for the optimal transport map between two distributions and show that they are minimax optimal. We adopt the plugin approach: our estimators are simply optimal couplings between measures derived from our observations, appropriately extended so that they define functions on . When the underlying map is assumed to be Lipschitz, we show that computing the optimal coupling between the empirical measures, and extending it using linear smoothers, already gives a minimax optimal estimator. When the underlying map enjoys higher regularity, we show that the optimal coupling between appropriate nonparametric density estimates yields faster rates. Our work also provides new bounds on the risk of corresponding plugin estimators for the quadratic Wasserstein distance, and we show how this problem relates to that of estimating optimal transport maps using stability arguments for smooth and strongly convex Brenier potentials. As an application of our results, we derive central limit theorems for plugin estimators of the squared Wasserstein distance, which are centered at their population counterpart when the underlying distributions have sufficiently smooth densities. In contrast to known central limit theorems for empirical estimators, this result easily lends itself to statistical inference for the quadratic Wasserstein distance."}}
{"id": "uAmv2zRAWn", "cdate": 1663850148307, "mdate": null, "content": {"title": "Perturbation Analysis of Neural Collapse", "abstract": "Training deep neural networks for classification often includes minimizing the training loss beyond the zero training error point. In this phase of training, a \u201cneural collapse\u201d behavior has been observed: the variability of features (outputs of the penultimate layer) of within-class samples decreases and the mean features of different classes approach a certain tight frame structure. Recent works analyze this behavior via idealized unconstrained features models where all the minimizers exhibit exact collapse. However, with practical networks and datasets, the features typically do not reach exact collapse, e.g., because deep layers cannot arbitrarily modify intermediate features that are far from being collapsed. In this paper, we propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish results that cannot be obtained by the previously studied models. For example, we prove reduction in the within-class variability of the optimized features compared to the predefined input features (via analyzing gradient flow on the \u201ccentral-path\u201d with minimal assumptions), analyze the minimizers in the near-collapse regime, and provide insights on the effect of regularization hyperparameters on the closeness to collapse. We support our theory with experiments in practical deep learning settings."}}
{"id": "z2cG3k8xa3C", "cdate": 1652737557797, "mdate": null, "content": {"title": "Asymptotics of smoothed Wasserstein distances in the small noise regime", "abstract": "We study the behavior of the Wasserstein-$2$ distance between discrete measures $\\mu$ and $\\nu$ in $\\mathbb{R}^d$ when both measures are smoothed by small amounts of Gaussian noise. This procedure, known as Gaussian-smoothed optimal transport, has recently attracted attention as a statistically attractive alternative to the unregularized Wasserstein distance. We give precise bounds on the approximation properties of this proposal in the small noise regime, and establish the existence of a phase transition: we show that, if the optimal transport plan from $\\mu$ to $\\nu$ is unique and a perfect matching, there exists a critical threshold such that the difference between $W_2(\\mu, \\nu)$ and the Gaussian-smoothed OT distance $W_2(\\mu \\ast \\mathcal{N}_\\sigma, \\nu\\ast \\mathcal{N}_\\sigma)$ scales like $\\exp(-c /\\sigma^2)$ for $\\sigma$ below the threshold, and scales like $\\sigma$ above it. These results establish that for $\\sigma$ sufficiently small, the smoothed Wasserstein distance approximates the unregularized distance exponentially well."}}
{"id": "m8YYs8nJF3T", "cdate": 1652737435184, "mdate": null, "content": {"title": "Distributional Convergence of the Sliced Wasserstein Process", "abstract": "Motivated by the statistical and computational challenges of computing Wasserstein distances in high-dimensional contexts, machine learning researchers have defined modified Wasserstein distances based on computing distances between one-dimensional projections of the measures. Different choices of how to aggregate these projected distances (averaging, random sampling, maximizing) give rise to different distances, requiring different statistical analyses. We define the \\emph{Sliced Wasserstein Process}, a stochastic process defined by the empirical Wasserstein distance between projections of empirical probability measures to all one-dimensional subspaces, and prove a uniform distributional limit theorem for this process. As a result, we obtain a unified framework in which to prove sample complexity and distributional limit results for all Wasserstein distances based on one-dimensional projections. We illustrate these results on a number of examples where no distributional limits were previously known."}}
{"id": "H1gT_NBg8B", "cdate": 1567802484569, "mdate": null, "content": {"title": "Massively scalable Sinkhorn distances via the Nystr\u00f6m method", "abstract": "The Sinkhorn ``distance,'' a variant of the Wasserstein distance with entropic regularization, is an increasingly popular tool in machine learning and statistical inference. However, the time and memory requirements of standard algorithms for computing this distance grow quadratically with the size of the data, rendering them prohibitively expensive on massive data sets. In this work, we show that this challenge is surprisingly easy to circumvent: combining two simple techniques\u2014the Nystr\u00f6m method and Sinkhorn scaling\u2014provably yields an accurate approximation of the Sinkhorn distance with significantly lower time and memory requirements than other approaches. We prove our results via new, explicit analyses of the Nystr\u00f6m method and of the stability properties of Sinkhorn scaling. We validate our claims experimentally by showing that our approach easily computes Sinkhorn distances on data sets hundreds of times larger than can be handled by other techniques."}}
{"id": "Hk-xMvW_WB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration", "abstract": "Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iterations, which also directly suggests a new greedy coordinate descent algorithm Greenkhorn with the same theoretical guarantees. Numerical simulations illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice."}}
