{"id": "62GLWUoOLb5", "cdate": 1652737653566, "mdate": null, "content": {"title": "Scalable Distributional Robustness in a Class of Non-Convex Optimization with Guarantees", "abstract": "Distributionally robust optimization (DRO) has shown a lot of promise in providing robustness in learning as well as sample-based optimization problems. We endeavor to provide DRO solutions for a class of sum of fractionals, non-convex optimization which is used for decision making in prominent areas such as facility location and security games. In contrast to previous work, we find it more tractable to optimize the equivalent variance regularized form of DRO rather than the minimax form. We transform the variance regularized form to a mixed-integer second-order cone program (MISOCP), which, while guaranteeing global optimality, does not scale enough to solve problems with real-world datasets. We further propose two abstraction approaches based on clustering and stratified sampling to increase scalability, which we then use for real-world datasets. Importantly, we provide global optimality guarantees for our approach and show experimentally that our solution quality is better than the locally optimal ones achieved by state-of-the-art gradient-based methods. We experimentally compare our different approaches and baselines and reveal nuanced properties of a DRO solution."}}
{"id": "Sy1cQ8cKzXE", "cdate": 1577836800000, "mdate": null, "content": {"title": "Solving Online Threat Screening Games using Constrained Action Space Reinforcement Learning", "abstract": "Large-scale screening for potential threats with limited resources and capacity for screening is a problem of interest at airports, seaports, and other ports of entry. Adversaries can observe screening procedures and arrive at a time when there will be gaps in screening due to limited resource capacities. To capture this game between ports and adversaries, this problem has been previously represented as a Stackelberg game, referred to as a Threat Screening Game (TSG). Given the significant complexity associated with solving TSGs and uncertainty in arrivals of customers, existing work has assumed that screenees arrive and are allocated security resources at the beginning of the time-window. In practice, screenees such as airport passengers arrive in bursts correlated with flight time and are not bound by fixed time-windows. To address this, we propose an online threat screening model in which the screening strategy is determined adaptively as a passenger arrives while satisfying a hard bound on acceptable risk of not screening a threat. To solve the online problem, we first reformulate it as a Markov Decision Process (MDP) in which the hard bound on risk translates to a constraint on the action space and then solve the resultant MDP using Deep Reinforcement Learning (DRL). To this end, we provide a novel way to efficiently enforce linear inequality constraints on the action output in DRL. We show that our solution allows us to significantly reduce screenee wait time without compromising on the risk."}}
{"id": "JirfOAUe85", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bounding Regret in Empirical Games", "abstract": "Empirical game-theoretic analysis refers to a set of models and techniques for solving large-scale games. However, there is a lack of a quantitative guarantee about the quality of output approximate Nash equilibria (NE). A natural quantitative guarantee for such an approximate NE is the regret in the game (i.e. the best deviation gain). We formulate this deviation gain computation as a multi-armed bandit problem, with a new optimization goal unlike those studied in prior work. We propose an efficient algorithm Super-Arm UCB (SAUCB) for the problem and a number of variants. We present sample complexity results as well as extensive experiments that show the better performance of SAUCB compared to several baselines."}}
{"id": "EFz0UH1YXyb", "cdate": 1577836800000, "mdate": null, "content": {"title": "Two Can Play That Game: An Adversarial Evaluation of a Cyber-Alert Inspection System", "abstract": "Cyber-security is an important societal concern. Cyber-attacks have increased in numbers as well as in the extent of damage caused in every attack. Large organizations operate a Cyber Security Operation Center (CSOC), which forms the first line of cyber-defense. The inspection of cyber-alerts is a critical part of CSOC operations (defender or blue team). Recent work proposed a reinforcement learning (RL) based approach for the defender\u2019s decision-making to prevent the cyber-alert queue length from growing large and overwhelming the defender. In this article, we perform a red team (adversarial) evaluation of this approach. With the recent attacks on learning-based decision-making systems, it is even more important to test the limits of the defender\u2019s RL approach. Toward that end, we learn several adversarial alert generation policies and the best response against them for various defender\u2019s inspection policy. Surprisingly, we find the defender\u2019s policies to be quite robust to the best response of the attacker. In order to explain this observation, we extend the earlier defender\u2019s RL model to a game model with adversarial RL, and show that there exist defender policies that can be robust against any adversarial policy. We also derive a competitive baseline from the game theory model and compare it to the defender\u2019s RL approach. However, when we go further to exploit the assumptions made in the Markov Decision Process (MDP) in the defender\u2019s RL model, we discover an attacker policy that overwhelms the defender. We use a double oracle like approach to retrain the defender with episodes from this discovered attacker policy. This made the defender robust to the discovered attacker policy and no further harmful attacker policies were discovered. Overall, the adversarial RL and double oracle approach in RL are general techniques that are applicable to other RL usage in adversarial environments."}}
{"id": "21CgneeDNQS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Partial Adversarial Behavior Deception in Security Games", "abstract": "Learning attacker behavior is an important research topic in security games as security agencies are often uncertain about attackers' decision making. Previous work has focused on developing various behavioral models of attackers based on historical attack data. However, a clever attacker can manipulate its attacks to fail such attack-driven learning, leading to ineffective defense strategies. We study attacker behavior deception with three main contributions. First, we propose a new model, named partial behavior deception model, in which there is a deceptive attacker (among multiple attackers) who controls a portion of attacks. Our model captures real-world security scenarios such as wildlife protection in which multiple poachers are present. Second, we introduce a new scalable algorithm, GAMBO, to compute an optimal deception strategy of the deceptive attacker. Our algorithm employs the projected gradient descent and uses the implicit function theorem for the computation of gradient. Third, we conduct a comprehensive set of experiments, showing a significant benefit for the attacker and loss for the defender due to attacker deception."}}
{"id": "XYoVIMaVYsS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Incentivizing Collaboration in a Competition", "abstract": "Research and design competitions aim to promote innovation or creative production, which are often best achieved through collaboration. The nature of a competition, however, typically necessitates sorting by individual performance. This presents tradeoffs for the competition designer, between incentivizing global performance and distinguishing individual capability. We model this situation in terms of an abstract collaboration game, where individual effort also benefits neighboring agents. We propose a scoring mechanism called LSWM that rewards agents based on localized social welfare. We show that LSWM promotes global performance, in that social optima are equilibria of the mechanism. Moreover, we establish conditions under which the mechanism leads to increased collaboration, and under which it ensures a formally defined distinguishability property. Through experiments, we evaluate the degree of distinguishability achieved whether or not the theoretical conditions identified hold."}}
{"id": "NyOl4n12xst", "cdate": 1546300800000, "mdate": null, "content": {"title": "Adversarial Contract Design for Private Data Commercialization", "abstract": "The proliferation of data collection and machine learning techniques has created an opportunity for commercialization of private data by data aggregators. In this paper, we study this data monetization problem as a mechanism design problem, specifically using a contract-theoretic approach. Our proposed adversarial contract design framework provides a fundamental extension to the classic contract theory set-up in order to account for the heterogeneity in honest buyers' demands for data, as well as the presence of adversarial buyers who may purchase data to compromise its privacy. We propose the notion of Price of Adversary $(PoAdv)$ to quantify the effects of adversarial users on the data seller's revenue, and provide bounds on the $PoAdv$ for various classes of adversary utility. We also provide a fast approximate technique to compute contracts in the presence of adversaries."}}
{"id": "Hj8WQJMldTH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Deception in Finitely Repeated Security Games.", "abstract": "Allocating resources to defend targets from attack is often complicated by uncertainty about the attacker\u2019s capabilities, objectives, or other underlying characteristics. In a repeated interaction setting, the defender can collect attack data over time to reduce this uncertainty and learn an effective defense. However, a clever attacker can manipulate the attack data to mislead the defender, influencing the learning process toward its own benefit. We investigate strategic deception on the part of an attacker with private type information, who interacts repeatedly with a defender. We present a detailed computation and analysis of both players\u2019 optimal strategies given the attacker may play deceptively. Computational experiments illuminate conditions conducive to strategic deception, and quantify benefits to the attacker. By taking into account the attacker\u2019s deception capacity, the defender can significantly mitigate loss from misleading attack actions."}}
{"id": "rke41hC5Km", "cdate": 1538087899570, "mdate": null, "content": {"title": "Generating Realistic Stock Market Order Streams", "abstract": "We propose an approach to generate realistic and high-fidelity stock market data based on generative adversarial networks.\nWe model the order stream as a stochastic process with finite history dependence, and employ a conditional Wasserstein GAN to capture history dependence of orders in a stock market. \nWe test our approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data. "}}
{"id": "SkWcCJ-u-B", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deceitful Attacks in Security Games", "abstract": "Given recent applications of defender-attacker Stackelberg Security Games in real-world domains such as wildlife protection, a majority of research has focused on addressing uncertainties regarding the attacker in these games based on the exploitation of attack data. However, there is an important challenge of deceitful attacks; the attacker can manipulate his attacks to mislead the defender, leading her to conduct ineffective patrolling strategies. In this work, we focus on addressing this challenge while providing the following main contributions. First, we introduce a new game model with uncertainty about the attacker type and repeated interactions between the players. In our game model, the defender attempts to collect attack data over time to learn about the attacker type while the attacker aims at playing deceitfully. Second, based on the new game model, we propose new game-theoretic algorithms to compute optimal strategies for both players. Third, we present preliminary experiment results to evaluate our proposed algorithms, showing that our defense solutions can effectively address deceitful attacks."}}
