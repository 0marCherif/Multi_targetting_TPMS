{"id": "u0j97AwT7Dw", "cdate": 1672531200000, "mdate": 1681785037308, "content": {"title": "Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning", "abstract": "We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one."}}
{"id": "EFPpmyWljQX", "cdate": 1664731443207, "mdate": null, "content": {"title": "Solving a Special Type of Optimal Transport Problem by a Modified Hungarian Algorithm", "abstract": "We observe that computing empirical Wasserstein distance in the independence test is an optimal transport (OT) problem with a special structure. This observation inspires us to study a special type of OT problem and propose a modified Hungarian algorithm to solve it exactly. For an OT problem involving two marginals with $m$ and $n$ atoms ($m\\geq n$), respectively, the computational complexity of the proposed algorithm is $\\mathcal{O}(m^2n)$. Computing the empirical Wasserstein distance in the independence test requires solving this special type of OT problem, where we have $m=n^2$. The associated computational complexity of our algorithm is $\\mathcal{O}(n^5)$, while the order of applying the classic Hungarian algorithm is $\\mathcal{O}(n^6)$. Numerical experiments validate our theoretical results. Broader applications of the proposed algorithm are discussed at the end."}}
{"id": "r90KYcuB7JS", "cdate": 1663850123898, "mdate": null, "content": {"title": "Approximation and non-parametric estimation of functions over high-dimensional spheres via deep ReLU networks", "abstract": "We develop a new approximation and estimation analysis of deep feed-forward neural networks (FNNs) with the Rectified Linear Unit (ReLU) activation. The functions of interests for the approximation and estimation are assumed to be from Sobolev spaces defined over the $d$-dimensional unit sphere with smoothness index $r>0$. In the regime where $r$ is in the constant order (i.e., $r=\\mathcal{O}(1)$), it is shown that at most $d^d$ active parameters are required for getting $d^{-C}$ approximation rate for some constant $C>0$. In contrast, in the regime where the index $r$ grows in the order of $d$ (i.e., $r=\\mathcal{O}(d)$) asymptotically, we prove the approximation error decays in the rate $d^{-d^{\\beta}}$ with $0<\\beta<1$ up to some constant factor independent of $d$. The required number of active parameters in the networks for the approximation increases polynomially in $d$ as $d\\rightarrow{\\infty}$. In addition to this, it is shown that bound on the excess risk has a $d^d$ factor, when $r=\\mathcal{O}(1)$, whereas it has $d^{\\mathcal{O}(1)}$ factor, when $r=\\mathcal{O}(d)$. We emphasize our findings by making comparisons to the results on approximation and estimation errors of deep ReLU FNN when functions are from Sobolev spaces defined over $d$-dimensional cube. Here, we show that with the current state-of-the-art result, $d^{d}$ factor remain both in the approximation and estimation error, regardless of the order of $r$. "}}
{"id": "nKQLqSgmP30", "cdate": 1640995200000, "mdate": 1681785037581, "content": {"title": "Implicit Regularization Properties of Variance Reduced Stochastic Mirror Descent", "abstract": "In machine learning and statistical data analysis, we often run into objective function that is a summation: the number of terms in the summation possibly is equal to the sample size, which can be enormous. In such a setting, the stochastic mirror descent (SMD) algorithm is a numerically efficient method\u2014each iteration involving a very small subset of the data. The variance reduction version of SMD (VRSMD) can further improve SMD by inducing faster convergence. On the other hand, algorithms such as gradient descent and stochastic gradient descent have the implicit regularization property that leads to better performance in terms of the generalization errors. Little is known on whether such a property holds for VRSMD. We prove here that the discrete VRSMD estimator sequence converges to the minimum mirror interpolant in the linear regression. This establishes the implicit regularization property for VRSMD. As an application of the above result, we derive a model estimation accuracy result in the setting when the true model is sparse. We use numerical examples to illustrate the empirical power of VRSMD."}}
{"id": "fbBCYhKd5YJ", "cdate": 1640995200000, "mdate": 1681785037096, "content": {"title": "An Accelerated Stochastic Algorithm for Solving the Optimal Transport Problem", "abstract": "A primal-dual accelerated stochastic gradient descent with variance reduction algorithm (PDASGD) is proposed to solve linear-constrained optimization problems. PDASGD could be applied to solve the discrete optimal transport (OT) problem and enjoys the best-known computational complexity -- $\\widetilde{\\mathcal{O}}(n^2/\\epsilon)$, where $n$ is the number of atoms, and $\\epsilon>0$ is the accuracy. In the literature, some primal-dual accelerated first-order algorithms, e.g., APDAGD, have been proposed and have the order of $\\widetilde{\\mathcal{O}}(n^{2.5}/\\epsilon)$ for solving the OT problem. To understand why our proposed algorithm could improve the rate by a factor of $\\widetilde{\\mathcal{O}}(\\sqrt{n})$, the conditions under which our stochastic algorithm has a lower order of computational complexity for solving linear-constrained optimization problems are discussed. It is demonstrated that the OT problem could satisfy the aforementioned conditions. Numerical experiments demonstrate superior practical performances of the proposed PDASGD algorithm for solving the OT problem."}}
{"id": "dfoQHgp7-b", "cdate": 1640995200000, "mdate": 1681785037442, "content": {"title": "Implicit Regularization Properties of Variance Reduced Stochastic Mirror Descent", "abstract": "In machine learning and statistical data analysis, we often run into objective function that is a summation: the number of terms in the summation possibly is equal to the sample size, which can be enormous. In such a setting, the stochastic mirror descent (SMD) algorithm is a numerically efficient method -- each iteration involving a very small subset of the data. The variance reduction version of SMD (VRSMD) can further improve SMD by inducing faster convergence. On the other hand, algorithms such as gradient descent and stochastic gradient descent have the implicit regularization property that leads to better performance in terms of the generalization errors. Little is known on whether such a property holds for VRSMD. We prove here that the discrete VRSMD estimator sequence converges to the minimum mirror interpolant in the linear regression. This establishes the implicit regularization property for VRSMD. As an application of the above result, we derive a model estimation accuracy result in the setting when the true model is sparse. We use numerical examples to illustrate the empirical power of VRSMD."}}
{"id": "cepKjp_Zg39", "cdate": 1640995200000, "mdate": 1681785037579, "content": {"title": "A New Sparse-Learning Model for Maximum Gap Reduction of Composite Fuselage Assembly", "abstract": "Natural dimensional variabilities of incoming fuselages affect the assembly speed and quality of fuselage joins in composite fuselage assembly processes. Shape control is critical to ensure the qua..."}}
{"id": "aFkUN7yICRt", "cdate": 1640995200000, "mdate": 1681785036968, "content": {"title": "The Directional Bias Helps Stochastic Gradient Descent to Generalize in Kernel Regression Models", "abstract": "We study the Stochastic Gradient Descent (SGD) algorithm in nonparametric statistics: kernel regression in particular. The directional bias property of SGD, which is known in the linear regression setting, is generalized to the kernel regression. More specifically, we prove that SGD with moderate and annealing step-size converges along the direction of the eigenvector that corresponds to the largest eigenvalue of the Gram matrix. In addition, the Gradient Descent (GD) with a moderate or small step-size converges along the direction that corresponds to the smallest eigenvalue. These facts are referred to as the directional bias properties; they may interpret how an SGD-computed estimator has a potentially smaller generalization error than a GD-computed estimator. The application of our theory is demonstrated by simulation studies and a case study that is based on the FashionMNIST dataset."}}
{"id": "W_24LHS89d", "cdate": 1640995200000, "mdate": 1681785037315, "content": {"title": "The Directional Bias Helps Stochastic Gradient Descent to Generalize in Kernel Regression Models", "abstract": "We study the Stochastic Gradient Descent (SGD) algorithm in nonparametric statistics: kernel regression in particular. The directional bias property of SGD, which is known in the linear regression setting, is generalized to the kernel regression. More specifically, we prove that SGD with moderate and annealing step-size converges along the direction of the eigenvector that corresponds to the largest eigenvalue of the Gram matrix. In addition, the Gradient Descent (GD) with a moderate or small step-size converges along the direction that corresponds to the smallest eigenvalue. These facts are referred to as the directional bias properties; they may interpret how an SGD-computed estimator has a potentially smaller generalization error than a GD-computed estimator. The application of our theory is demonstrated by simulation studies and a case study that is based on the FashionMNIST dataset."}}
{"id": "PlCRG-vSwRx", "cdate": 1640995200000, "mdate": 1681785036596, "content": {"title": "Covariance Estimators for the ROOT-SGD Algorithm in Online Learning", "abstract": "Online learning naturally arises in many statistical and machine learning problems. The most widely used methods in online learning are stochastic first-order algorithms. Among this family of algorithms, there is a recently developed algorithm, Recursive One-Over-T SGD (ROOT-SGD). ROOT-SGD is advantageous in that it converges at a non-asymptotically fast rate, and its estimator further converges to a normal distribution. However, this normal distribution has unknown asymptotic covariance; thus cannot be directly applied to measure the uncertainty. To fill this gap, we develop two estimators for the asymptotic covariance of ROOT-SGD. Our covariance estimators are useful for statistical inference in ROOT-SGD. Our first estimator adopts the idea of plug-in. For each unknown component in the formula of the asymptotic covariance, we substitute it with its empirical counterpart. The plug-in estimator converges at the rate $\\mathcal{O}(1/\\sqrt{t})$, where $t$ is the sample size. Despite its quick convergence, the plug-in estimator has the limitation that it relies on the Hessian of the loss function, which might be unavailable in some cases. Our second estimator is a Hessian-free estimator that overcomes the aforementioned limitation. The Hessian-free estimator uses the random-scaling technique, and we show that it is an asymptotically consistent estimator of the true covariance."}}
