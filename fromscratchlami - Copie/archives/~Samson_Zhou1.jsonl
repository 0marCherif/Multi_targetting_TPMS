{"id": "pP3p0rmeyRb", "cdate": 1673287857554, "mdate": null, "content": {"title": "Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging", "abstract": "Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from\nprevious tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay. \n\nWe evaluated the coreset lifelong deep reinforcement learning technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifelong learning models trained on a sequence of 10 different brain MR imaging environments demonstrated excellent performance localizing the ventricle with a mean pixel error distance of 12.93, 13.46, 17.75, and 18.55 for the compression ratios of 10x, 20x, 30x, and 40x, respectively. In comparison, the conventional lifelong learning model localized the ventricle with a mean pixel distance of 10.87. Similarly, the coreset lifelong learning models trained on whole-body MRI demonstrated no significant difference (p=0.28) between the 10x compressed coreset lifelong learning models and conventional lifelong learning models for all the landmarks. The mean pixel distance for the 10x compressed models across all the landmarks was 25.30, compared to 19.24 for the conventional lifelong learning models. Our results demonstrate that the potential of the coreset-based ERB compression method for compressing experiences without a significant drop in performance."}}
{"id": "zXCvVc35B7", "cdate": 1672531200000, "mdate": 1695953662832, "content": {"title": "Near-Linear Sample Complexity for L Polynomial Regression.p", "abstract": "We study Lp polynomial regression. Given query access to a function f : [\u22121,1]\u2192\u211d, the goal is to find a degree d polynomial q\u0302 such that, for a given parameter \u03b5 > 0 Here || \u00b7 ||p is the Lp norm, \u2016g\u2016p = (\u222b1\u22121|g(t)|p dt)1/p. We show that querying f at points randomly drawn from the Chebyshev measure on [-1,1] is a near-optimal strategy for polynomial regression in all Lp norms. In particular, to find q\u0302, it suffices to sample points from [-1,1] with probabilities proportional to this measure. While the optimal sample complexity for polynomial regression was well understood for L2 and L\u221e, our result is the first that achieves sample complexity linear in d and error (1 + \u03b5) for other values of p without any assumptions. Our result requires two main technical contributions. The first concerns p \u2264 2, for which we provide explicit bounds on the Lp Lewis weight function of the infinite linear operator underlying polynomial regression. Using tools from the orthogonal polynomial literature, we show that this function is bounded by the Chebyshev density. Our second key contribution is to take advantage of the structure of polynomials to reduce the p > 2 case to the p \u2264 2 case. By doing so, we obtain a better sample complexity than what is possible for general p-norm linear regression problems, for which \u03a9(dp/2) samples are required."}}
{"id": "xycywZn38nt", "cdate": 1672531200000, "mdate": 1682318571384, "content": {"title": "Provable Data Subset Selection For Efficient Neural Network Training", "abstract": "Radial basis function neural networks (\\emph{RBFNN}) are {well-known} for their capability to approximate any continuous function on a closed bounded set with arbitrary precision given enough hidden neurons. In this paper, we introduce the first algorithm to construct coresets for \\emph{RBFNNs}, i.e., small weighted subsets that approximate the loss of the input data on any radial basis function network and thus approximate any function defined by an \\emph{RBFNN} on the larger input data. In particular, we construct coresets for radial basis and Laplacian loss functions. We then use our coresets to obtain a provable data subset selection algorithm for training deep neural networks. Since our coresets approximate every function, they also approximate the gradient of each weight in a neural network, which is a particular function on the input. We then perform empirical evaluations on function approximation and dataset subset selection on popular network architectures and data sets, demonstrating the efficacy and accuracy of our coreset construction."}}
{"id": "r5HMIvijPAz", "cdate": 1672531200000, "mdate": 1695953662925, "content": {"title": "Private Data Stream Analysis for Universal Symmetric Norm Estimation", "abstract": "We study how to release summary statistics on a data stream subject to the constraint of differential privacy. In particular, we focus on releasing the family of symmetric norms, which are invariant under sign-flips and coordinate-wise permutations on an input data stream and include $L_p$ norms, $k$-support norms, top-$k$ norms, and the box norm as special cases. Although it may be possible to design and analyze a separate mechanism for each symmetric norm, we propose a general parametrizable framework that differentially privately releases a number of sufficient statistics from which the approximation of all symmetric norms can be simultaneously computed. Our framework partitions the coordinates of the underlying frequency vector into different levels based on their magnitude and releases approximate frequencies for the \"heavy\" coordinates in important levels and releases approximate level sizes for the \"light\" coordinates in important levels. Surprisingly, our mechanism allows for the release of an arbitrary number of symmetric norm approximations without any overhead or additional loss in privacy. Moreover, our mechanism permits $(1+\\alpha)$-approximation to each of the symmetric norms and can be implemented using sublinear space in the streaming model for many regimes of the accuracy and privacy parameters."}}
{"id": "pHZFjIUS12o", "cdate": 1672531200000, "mdate": 1681689083854, "content": {"title": "Near-Linear Sample Complexity for L Polynomial Regression.p", "abstract": "We study Lp polynomial regression. Given query access to a function f : [\u22121,1]\u2192\u211d, the goal is to find a degree d polynomial q\u0302 such that, for a given parameter \u03b5 > 0 Here || \u00b7 ||p is the Lp norm, \u2016g\u2016p = (\u222b1\u22121|g(t)|p dt)1/p. We show that querying f at points randomly drawn from the Chebyshev measure on [-1,1] is a near-optimal strategy for polynomial regression in all Lp norms. In particular, to find q\u0302, it suffices to sample points from [-1,1] with probabilities proportional to this measure. While the optimal sample complexity for polynomial regression was well understood for L2 and L\u221e, our result is the first that achieves sample complexity linear in d and error (1 + \u03b5) for other values of p without any assumptions. Our result requires two main technical contributions. The first concerns p \u2264 2, for which we provide explicit bounds on the Lp Lewis weight function of the infinite linear operator underlying polynomial regression. Using tools from the orthogonal polynomial literature, we show that this function is bounded by the Chebyshev density. Our second key contribution is to take advantage of the structure of polynomials to reduce the p > 2 case to the p \u2264 2 case. By doing so, we obtain a better sample complexity than what is possible for general p-norm linear regression problems, for which \u03a9(dp/2) samples are required."}}
{"id": "mshSpxjQrST", "cdate": 1672531200000, "mdate": 1682318568656, "content": {"title": "On Differential Privacy and Adaptive Data Analysis with Bounded Space", "abstract": ""}}
{"id": "mFBj2hN05R", "cdate": 1672531200000, "mdate": 1695953662812, "content": {"title": "Fast (1+\u03b5)-Approximation Algorithms for Binary Matrix Factorization", "abstract": "We introduce efficient $(1+\\varepsilon)$-approximation algorithms for the binary matrix factorization (BMF) problem, where the inputs are a matrix $\\mathbf{A}\\in\\{0,1\\}^{n\\times d}$, a rank paramet..."}}
{"id": "k12djaV6lJ0", "cdate": 1672531200000, "mdate": 1682318571020, "content": {"title": "Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging", "abstract": "Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay. We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifelong learning models trained on a sequence of 10 different brain MR imaging environments demonstrated excellent performance localizing the ventricle with a mean pixel error distance of 12.93 for the compression ratio of 10x. In comparison, the conventional lifelong learning model localized the ventricle with a mean pixel distance of 10.87. Similarly, the coreset lifelong learning models trained on whole-body MRI demonstrated no significant difference (p=0.28) between the 10x compressed coreset lifelong learning models and conventional lifelong learning models for all the landmarks. The mean pixel distance for the 10x compressed models across all the landmarks was 25.30, compared to 19.24 for the conventional lifelong learning models. Our results demonstrate that the potential of the coreset-based ERB compression method for compressing experiences without a significant drop in performance."}}
{"id": "gfee_B11JHp", "cdate": 1672531200000, "mdate": 1681689083857, "content": {"title": "Provable Data Subset Selection For Efficient Neural Network Training", "abstract": "Radial basis function neural networks (\\emph{RBFNN}) are {well-known} for their capability to approximate any continuous function on a closed bounded set with arbitrary precision given enough hidden neurons. In this paper, we introduce the first algorithm to construct coresets for \\emph{RBFNNs}, i.e., small weighted subsets that approximate the loss of the input data on any radial basis function network and thus approximate any function defined by an \\emph{RBFNN} on the larger input data. In particular, we construct coresets for radial basis and Laplacian loss functions. We then use our coresets to obtain a provable data subset selection algorithm for training deep neural networks. Since our coresets approximate every function, they also approximate the gradient of each weight in a neural network, which is a particular function on the input. We then perform empirical evaluations on function approximation and dataset subset selection on popular network architectures and data sets, demonstrating the efficacy and accuracy of our coreset construction."}}
{"id": "ZDxR6OFfUh", "cdate": 1672531200000, "mdate": 1695953668134, "content": {"title": "Near-Linear Sample Complexity for L Polynomial Regression.p", "abstract": "We study Lp polynomial regression. Given query access to a function f : [\u22121,1]\u2192\u211d, the goal is to find a degree d polynomial q\u0302 such that, for a given parameter \u03b5 > 0 Here || \u00b7 ||p is the Lp norm, \u2016g\u2016p = (\u222b1\u22121|g(t)|p dt)1/p. We show that querying f at points randomly drawn from the Chebyshev measure on [-1,1] is a near-optimal strategy for polynomial regression in all Lp norms. In particular, to find q\u0302, it suffices to sample points from [-1,1] with probabilities proportional to this measure. While the optimal sample complexity for polynomial regression was well understood for L2 and L\u221e, our result is the first that achieves sample complexity linear in d and error (1 + \u03b5) for other values of p without any assumptions. Our result requires two main technical contributions. The first concerns p \u2264 2, for which we provide explicit bounds on the Lp Lewis weight function of the infinite linear operator underlying polynomial regression. Using tools from the orthogonal polynomial literature, we show that this function is bounded by the Chebyshev density. Our second key contribution is to take advantage of the structure of polynomials to reduce the p > 2 case to the p \u2264 2 case. By doing so, we obtain a better sample complexity than what is possible for general p-norm linear regression problems, for which \u03a9(dp/2) samples are required."}}
