{"id": "GWa2efGorZH", "cdate": 1684220922876, "mdate": 1684220922876, "content": {"title": "Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization", "abstract": "We consider the optimization problem of the form \n$\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}[F(x;\\xi)]$ \n, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth.\nThe recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2}  \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2}  \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$."}}
{"id": "PzNjdn9huN", "cdate": 1684220674643, "mdate": 1684220674643, "content": {"title": "Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization", "abstract": "We consider the optimization problem of the form \n$\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}[F(x;\\xi)]$ \n, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth.\nThe recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2}  \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2}  \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$."}}
{"id": "dBRT5-p7Pej", "cdate": 1672531200000, "mdate": 1684279004532, "content": {"title": "On Bilevel Optimization without Lower-level Strong Convexity", "abstract": "Bilevel optimization reveals the inner structure of otherwise oblique optimization problems, such as hyperparameter tuning and meta-learning. A common goal in bilevel optimization is to find stationary points of the hyper-objective function. Although this hyper-objective approach is widely used, its theoretical properties have not been thoroughly investigated in cases where the lower-level functions lack strong convexity. In this work, we take a step forward and study the hyper-objective approach without the typical lower-level strong convexity assumption. Our hardness results show that the hyper-objective of general convex lower-level functions can be intractable either to evaluate or to optimize. To tackle this challenge, we introduce the gradient dominant condition, which strictly relaxes the strong convexity assumption by allowing the lower-level solution set to be non-singleton. Under the gradient dominant condition, we propose the Inexact Gradient-Free Method (IGFM), which uses the Switching Gradient Method (SGM) as the zeroth order oracle, to find an approximate stationary point of the hyper-objective. We also extend our results to nonsmooth lower-level functions under the weak sharp minimum condition."}}
{"id": "9JahY13NUDd", "cdate": 1672531200000, "mdate": 1674734802097, "content": {"title": "Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization", "abstract": "We consider the optimization problem of the form $\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}_{\\xi} [F(x; \\xi)]$, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2} \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2} \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$."}}
{"id": "JSha3zfdmSo", "cdate": 1652737319922, "mdate": null, "content": {"title": "Faster Stochastic Algorithms for Minimax Optimization under Polyak-{\\L}ojasiewicz Condition", "abstract": "This paper considers stochastic first-order algorithms for minimax optimization under Polyak-{\\L}ojasiewicz (PL) conditions. \nWe propose SPIDER-GDA for solving the finite-sum problem of the form $\\min_x \\max_y f(x,y)\\triangleq \\frac{1}{n} \\sum_{i=1}^n f_i(x,y)$, where the objective function $f(x,y)$ is $\\mu_x$-PL in $x$ and $\\mu_y$-PL in $y$; and each $f_i(x,y)$ is $L$-smooth. We prove SPIDER-GDA could find an $\\epsilon$-approximate solution within ${\\mathcal O}\\left((n + \\sqrt{n}\\,\\kappa_x\\kappa_y^2)\\log (1/\\epsilon)\\right)$ stochastic first-order oracle (SFO) complexity, which is better than the state-of-the-art method whose SFO upper bound is ${\\mathcal O}\\big((n + n^{2/3}\\kappa_x\\kappa_y^2)\\log (1/\\epsilon)\\big)$, where $\\kappa_x\\triangleq L/\\mu_x$ and $\\kappa_y\\triangleq L/\\mu_y$.\nFor the ill-conditioned case, we provide an accelerated algorithm to reduce the computational cost further. It achieves $\\tilde{{\\mathcal O}}\\big((n+\\sqrt{n}\\,\\kappa_x\\kappa_y)\\log^2 (1/\\epsilon)\\big)$ SFO upper bound when $\\kappa_x\\geq\\sqrt{n}$. Our ideas also can be applied to the more general setting that the objective function only satisfies PL condition for one variable. Numerical experiments validate the superiority of proposed methods."}}
{"id": "Qp2d1bcQmR", "cdate": 1640995200000, "mdate": 1684279004539, "content": {"title": "Faster Stochastic Algorithms for Minimax Optimization under Polyak-{\\L}ojasiewicz Condition", "abstract": "This paper considers stochastic first-order algorithms for minimax optimization under Polyak-{\\L}ojasiewicz (PL) conditions. We propose SPIDER-GDA for solving the finite-sum problem of the form $\\min_x \\max_y f(x,y)\\triangleq \\frac{1}{n} \\sum_{i=1}^n f_i(x,y)$, where the objective function $f(x,y)$ is $\\mu_x$-PL in $x$ and $\\mu_y$-PL in $y$; and each $f_i(x,y)$ is $L$-smooth. We prove SPIDER-GDA could find an $\\epsilon$-approximate solution within ${\\mathcal O}\\left((n + \\sqrt{n}\\,\\kappa_x\\kappa_y^2)\\log (1/\\epsilon)\\right)$ stochastic first-order oracle (SFO) complexity, which is better than the state-of-the-art method whose SFO upper bound is ${\\mathcal O}\\big((n + n^{2/3}\\kappa_x\\kappa_y^2)\\log (1/\\epsilon)\\big)$, where $\\kappa_x\\triangleq L/\\mu_x$ and $\\kappa_y\\triangleq L/\\mu_y$.For the ill-conditioned case, we provide an accelerated algorithm to reduce the computational cost further. It achieves $\\tilde{{\\mathcal O}}\\big((n+\\sqrt{n}\\,\\kappa_x\\kappa_y)\\log^2 (1/\\epsilon)\\big)$ SFO upper bound when $\\kappa_x\\geq\\sqrt{n}$. Our ideas also can be applied to the more general setting that the objective function only satisfies PL condition for one variable. Numerical experiments validate the superiority of proposed methods."}}
{"id": "G8kkc_ginP", "cdate": 1640995200000, "mdate": 1674734802106, "content": {"title": "A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization", "abstract": "This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\\mathcal{O}(\\kappa^3\\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\\mathcal{O}\\big(\\kappa^2\\epsilon^{-2}/\\sqrt{1-\\lambda_2(W)}\\,\\big)$ communication rounds to find an $\\epsilon$-stationary point, where $\\kappa$ is the condition number and $\\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\\mathcal{O}\\big(\\kappa^2 \\sqrt{N} \\epsilon^{-2}\\big)$ SFO calls and the same communication complexity as the online setting."}}
{"id": "7t6hClT4er", "cdate": 1640995200000, "mdate": 1674734802257, "content": {"title": "Near-Optimal Algorithms for Making the Gradient Small in Stochastic Minimax Optimization", "abstract": "We study the problem of finding a near-stationary point for smooth minimax optimization. The recent proposed extra anchored gradient (EAG) methods achieve the optimal convergence rate for the convex-concave minimax problem in deterministic setting. However, the direct extension of EAG to stochastic optimization is not efficient.In this paper, we design a novel stochastic algorithm called Recursive Anchored IteratioN (RAIN). We show that the RAIN achieves near-optimal stochastic first-order oracle (SFO) complexity for stochastic minimax optimization in both convex-concave and strongly-convex-strongly-concave cases. In addition, we extend the idea of RAIN to solve structured nonconvex-nonconcave minimax problem and it also achieves near-optimal SFO complexity."}}
