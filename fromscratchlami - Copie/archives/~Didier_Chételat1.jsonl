{"id": "0VhrZPJXcTU", "cdate": 1652737851897, "mdate": null, "content": {"title": "Learning to Compare Nodes in Branch and Bound with Graph Neural Networks", "abstract": "Branch-and-bound approaches in integer programming require ordering portions of the space to explore next, a problem known as node comparison. We propose a new siamese graph neural network model to tackle this problem, where the nodes are represented as bipartite graphs with attributes. Similar to prior work, we train our model to imitate a diving oracle that plunges towards the optimal solution. We evaluate our method by solving the instances in a plain framework where the nodes are explored according to their rank. On three NP-hard benchmarks chosen to be particularly primal-difficult, our approach leads to faster solving and smaller branch- and-bound trees than the default ranking function of the open-source solver SCIP, as well as competing machine learning methods. Moreover, these results generalize to instances larger than used for training. Code for reproducing the experiments can be found at https://github.com/ds4dm/learn2comparenodes."}}
{"id": "M4OllVd70mJ", "cdate": 1652737729455, "mdate": null, "content": {"title": "Learning to Branch with Tree MDPs", "abstract": "State-of-the-art Mixed Integer Linear Programming (MILP) solvers combine systematic tree search with a plethora of hard-coded heuristics, such as branching rules.\u00a0While approaches to learn branching strategies have received\u00a0increasing\u00a0attention and have shown\u00a0very\u00a0promising results,\u00a0most of the literature focuses\u00a0on\u00a0learning fast\u00a0approximations of the \\emph{strong branching} rule. Instead, we propose to learn branching rules from scratch with Reinforcement Learning (RL). We revisit the work of Etheve et al. (2020) and\u00a0propose a generalization of Markov Decisions Processes (MDP), which\u00a0we call \\emph{tree MDP},\u00a0that provides a more suitable formulation of the\u00a0branching\u00a0problem. We derive a policy gradient theorem for tree MDPs that exhibits a better credit assignment compared to its temporal counterpart. We demonstrate through computational experiments that this\u00a0new\u00a0framework is\u00a0suitable\u00a0to tackle the learning-to-branch problem\u00a0in MILP, and improves the learning convergence."}}
{"id": "lgn-fv22Ha", "cdate": 1609459200000, "mdate": null, "content": {"title": "Combinatorial optimization and reasoning with graph neural networks", "abstract": "Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks (GNNs), as a key building block for combinatorial tasks, either directly as solvers or by enhancing exact solvers. The inductive bias of GNNs effectively encodes combinatorial and relational input due to their invariance to permutations and awareness of input sparsity. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at optimization and machine learning researchers."}}
{"id": "2YuskFXCj3J", "cdate": 1609459200000, "mdate": null, "content": {"title": "Ecole: A Library for Learning Inside MILP Solvers", "abstract": "In this paper we describe Ecole (Extensible Combinatorial Optimization Learning Environments), a library to facilitate integration of machine learning in combinatorial optimization solvers. It exposes sequential decision making that must be performed in the process of solving as Markov decision processes. This means that, rather than trying to predict solutions to combinatorial optimization problems directly, Ecole allows machine learning to work in cooperation with a state-of-the-art a mixed-integer linear programming solver that acts as a controllable algorithm. Ecole provides a collection of computationally efficient, ready to use learning environments, which are also easy to extend to define novel training tasks. Documentation and code can be found at https://www.ecole.ai."}}
{"id": "IVc9hqgibyB", "cdate": 1602926478778, "mdate": null, "content": {"title": "Ecole: A Gym-like Library for Machine Learning in Combinatorial Optimization Solvers", "abstract": "We present Ecole, a new library to simplify machine learning research for combinatorial optimization. Ecole exposes several key decision tasks arising in general-purpose combinatorial optimization solvers as control problems over Markov decision processes. Its interface mimics the popular OpenAI Gym library and is both extensible and intuitive to use. We aim at making this library a standardized platform that will lower the bar of entry and accelerate innovation in this growing field. Documentation and code can be found at https://www.ecole.ai."}}
{"id": "htDTT2_VLb_", "cdate": 1598645116382, "mdate": null, "content": {"title": "The middle-scale asymptotics of Wishart matrices", "abstract": "We study the behavior of a real p-dimensional Wishart random matrix with n degrees of freedom when n,p\u2192\u221e but p/n\u21920. We establish the existence of phase transitions when p grows at the order n(K+1)/(K+3) for every K\u2208N, and derive expressions for approximating densities between every two phase transitions. To do this, we make use of a novel tool we call the F-conjugate of an absolutely continuous distribution, which is obtained from the Fourier transform of the square root of its density. In the case of the normalized Wishart distribution, this represents an extension of the t-distribution to the space of real symmetric matrices."}}
{"id": "AIzRNYfY8wm", "cdate": 1577836800000, "mdate": null, "content": {"title": "Change Point Detection by Cross-Entropy Maximization", "abstract": "Many offline unsupervised change point detection algorithms rely on minimizing a penalized sum of segment-wise costs. We extend this framework by proposing to minimize a sum of discrepancies between segments. In particular, we propose to select the change points so as to maximize the cross-entropy between successive segments, balanced by a penalty for introducing new change points. We propose a dynamic programming algorithm to solve this problem and analyze its complexity. Experiments on two challenging datasets demonstrate the advantages of our method compared to three state-of-the-art approaches."}}
{"id": "w-vjQ9MNEdV", "cdate": 1546300800000, "mdate": null, "content": {"title": "Exact Combinatorial Optimization with Graph Convolutional Neural Networks", "abstract": "Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems. Code for reproducing all the experiments can be found at https://github.com/ds4dm/learn2branch."}}
{"id": "mCAo6-JRILc", "cdate": 1546300800000, "mdate": null, "content": {"title": "Exact Combinatorial Optimization with Graph Convolutional Neural Networks", "abstract": "Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems. Code for reproducing all the experiments can be found at https://github.com/ds4dm/learn2branch."}}
{"id": "8xhs-JPjZWc", "cdate": 1514764800000, "mdate": null, "content": {"title": "On the domain of attraction of a Tracy-Widom law with applications to testing multiple largest roots", "abstract": "The greatest root statistic arises as a test statistic in several multivariate analysis settings. Suppose there is a global null hypothesis H 0 that consists of m different independent sub null hypotheses, i.e., H 0 "}}
