{"id": "mzze3bubjk", "cdate": 1652737835867, "mdate": null, "content": {"title": "The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics", "abstract": "Many high-dimensional statistical inference problems are believed to possess inherent computational hardness. Various frameworks have been proposed to give rigorous evidence for such hardness, including lower bounds against restricted models of computation (such as low-degree functions), as well as methods rooted in statistical physics that are based on free energy landscapes. This paper aims to make a rigorous connection between the seemingly different low-degree and free-energy based approaches. We define a free-energy based criterion for hardness and formally connect it to the well-established notion of low-degree hardness for a broad class of statistical problems, namely all Gaussian additive models and certain models with a sparse planted signal. By leveraging these rigorous connections we are able to: establish that for Gaussian additive models the \"algebraic\" notion of low-degree hardness implies failure of \"geometric\" local MCMC algorithms, and provide new low-degree lower bounds for sparse linear regression which seem difficult to prove directly. These results provide both conceptual insights into the connections between different notions of hardness, as well as concrete technical tools such as new methods for proving low-degree lower bounds."}}
{"id": "ArZWGF0Ifl7", "cdate": 1652737758882, "mdate": null, "content": {"title": "Archimedes Meets Privacy: On Privately Estimating Quantiles in High Dimensions Under Minimal Assumptions", "abstract": "The last few years have seen a surge of work on high dimensional statistics under privacy constraints, mostly following two main lines of work: the \"worst case\" line, which does not make any distributional assumptions on the input data; and the \"strong assumptions\" line, which assumes that the data is generated from specific families, e.g., subgaussian distributions.\nIn this work we take a middle ground, obtaining new differentially private algorithms with polynomial sample complexity for estimating quantiles in high-dimensions, as well as estimating and sampling points of high Tukey depth, all working under very mild distributional assumptions. \n\nFrom the technical perspective, our work relies upon fundamental robustness results in the convex geometry literature, demonstrating how such results can be used in a private context. Our main object of interest is the (convex) floating body (FB), a notion going back to Archimedes, which is a robust and well studied high-dimensional analogue of the interquantile range of a distribution.  We show how one can privately, and with polynomially many samples, (a) output an approximate interior point of the FB -- e.g., \"a typical user\" in a high-dimensional database -- by leveraging the robustness of the Steiner point of the FB; and at the expense of polynomially many more samples, (b) produce an approximate uniform sample from the FB, by constructing a private noisy projection oracle.\n"}}
{"id": "yCdJnwfEoj", "cdate": 1627483425569, "mdate": 1627483425569, "content": {"title": "Optimal Private Median Estimation under Minimal Distributional Assumptions", "abstract": "We study the fundamental task of estimating the median of an underlying distribution from a finite number of samples, under pure differential privacy constraints. We focus on distributions satisfying the minimal assumption that they have a positive density at a small neighborhood around the median. In particular, the distribution is allowed to output unbounded values and is not required to have finite moments. We compute the exact, up-to-constant terms, statistical rate of estimation for the median by providing nearly-tight upper and lower bounds. Furthermore, we design a polynomial-time differentially private algorithm which provably achieves the optimal performance. At a technical level, our results leverage a Lipschitz Extension Lemma which allows us to design and analyze differentially private algorithms solely on appropriately defined \"typical\" instances of the samples.\n"}}
{"id": "gXdOOeqRN8g", "cdate": 1621630059248, "mdate": null, "content": {"title": "On the Cryptographic Hardness of Learning Single Periodic Neurons", "abstract": "We show a simple reduction which demonstrates the cryptographic hardness of learning a single periodic neuron over isotropic Gaussian distributions in the presence of noise. More precisely, our reduction shows that any polynomial-time algorithm (not necessarily gradient-based) for learning such functions under small noise implies a polynomial-time quantum algorithm for solving worst-case lattice problems, whose hardness form the foundation of lattice-based cryptography. Our core hard family of functions, which are well-approximated by one-layer neural networks, take the general form of a univariate periodic function applied to an affine projection of the data. These functions have appeared in previous seminal works which demonstrate their hardness against gradient-based (Shamir'18), and Statistical Query (SQ) algorithms (Song et al.'17). We show that if (polynomially) small noise is added to the labels, the intractability of learning these functions applies to all polynomial-time algorithms, beyond gradient-based and SQ algorithms, under the aforementioned cryptographic assumptions. Moreover, we demonstrate the necessity of noise in the hardness result by designing a polynomial-time algorithm for learning certain families of such functions under exponentially small adversarial noise. Our proposed algorithm is not a gradient-based or an SQ algorithm, but is rather based on the celebrated Lenstra-Lenstra-Lov\\'asz (LLL) lattice basis reduction algorithm. Furthermore, in the absence of noise, this algorithm can be directly applied to solve CLWE detection (Bruna et al.'21) and phase retrieval with an optimal sample complexity of $d+1$ samples. In the former case, this improves upon the quadratic-in-$d$ sample complexity required in (Bruna et al.'21)."}}
{"id": "Iaop1cqDNRj", "cdate": 1577836800000, "mdate": null, "content": {"title": "Neural Networks and Polynomial Regression. Demystifying the Overparametrization Phenomena.", "abstract": "In the context of neural network models, overparametrization refers to the phenomena whereby these models appear to generalize well on the unseen data, even though the number of parameters significantly exceeds the sample sizes, and the model perfectly fits the in-training data. A conventional explanation of this phenomena is based on self-regularization properties of algorithms used to train the data. In this paper we prove a series of results which provide a somewhat diverging explanation. Adopting a teacher/student model where the teacher network is used to generate the predictions and student network is trained on the observed labeled data, and then tested on out-of-sample data, we show that any student network interpolating the data generated by a teacher network generalizes well, provided that the sample size is at least an explicit quantity controlled by data dimension and approximation guarantee alone, regardless of the number of internal nodes of either teacher or student network.   Our claim is based on approximating both teacher and student networks by polynomial (tensor) regression models with degree depending on the desired accuracy and network depth only. Such a parametrization notably does not depend on the number of internal nodes. Thus a message implied by our results is that parametrizing wide neural networks by the number of hidden nodes is misleading, and a more fitting measure of parametrization complexity is the number of regression coefficients associated with tensorized data. In particular, this somewhat reconciles the generalization ability of neural networks with more classical statistical notions of data complexity and generalization bounds. Our empirical results on MNIST and Fashion-MNIST datasets indeed confirm that tensorized regression achieves a good out-of-sample performance, even when the degree of the tensor is at most two."}}
{"id": "wW2rKyaYvAP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Stationary Points of Shallow Neural Networks with Quadratic Activation Function.", "abstract": "We consider the teacher-student setting of learning shallow neural networks with quadratic activations and planted weight matrix $W^*\\in\\mathbb{R}^{m\\times d}$, where $m$ is the width of the hidden layer and $d\\le m$ is the data dimension. We study the optimization landscape associated with the empirical and the population squared risk of the problem. Under the assumption the planted weights are full-rank we obtain the following results. First, we establish that the landscape of the empirical risk admits an \"energy barrier\" separating rank-deficient $W$ from $W^*$: if $W$ is rank deficient, then its risk is bounded away from zero by an amount we quantify. We then couple this result by showing that, assuming number $N$ of samples grows at least like a polynomial function of $d$, all full-rank approximate stationary points of the empirical risk are nearly global optimum. These two results allow us to prove that gradient descent, when initialized below the energy barrier, approximately minimizes the empirical risk and recovers the planted weights in polynomial-time. Next, we show that initializing below this barrier is in fact easily achieved when the weights are randomly generated under relatively weak assumptions. We show that provided the network is sufficiently overparametrized, initializing with an appropriate multiple of the identity suffices to obtain a risk below the energy barrier. At a technical level, the last result is a consequence of the semicircle law for the Wishart ensemble and could be of independent interest. Finally, we study the minimizers of the empirical risk and identify a simple necessary and sufficient geometric condition on the training data under which any minimizer has necessarily zero generalization error. We show that as soon as $N\\ge N^*=d(d+1)/2$, randomly generated data enjoys this geometric condition almost surely, while that ceases to be true if $N<N^*$."}}
{"id": "u8JuZ_pSo_", "cdate": 1546300800000, "mdate": null, "content": {"title": "All-or-Nothing Phenomena: From Single-Letter to High Dimensions.", "abstract": "We consider the linear regression problem of estimating a $p$-dimensional vector $\\beta$ from $n$ observations $Y = X \\beta + W$, where $\\beta_j \\stackrel{\\text{i.i.d.}}{\\sim} \\pi$ for a real-valued distribution $\\pi$ with zero mean and unit variance, $X_{ij} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1)$, and $W_i\\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)$. In the asymptotic regime where $n/p \\to \\delta$ and $ p/ \\sigma^2 \\to \\mathsf{snr}$ for two fixed constants $\\delta, \\mathsf{snr}\\in (0, \\infty)$ as $p \\to \\infty$, the limiting (normalized) minimum mean-squared error (MMSE) has been characterized by the MMSE of an associated single-letter (additive Gaussian scalar) channel. In this paper, we show that if the MMSE function of the single-letter channel converges to a step function, then the limiting MMSE of estimating $\\beta$ in the linear regression problem converges to a step function which jumps from $1$ to $0$ at a critical threshold. Moreover, we establish that the limiting mean-squared error of the (MSE-optimal) approximate message passing algorithm also converges to a step function with a larger threshold, providing evidence for the presence of a computational-statistical gap between the two thresholds."}}
{"id": "o5lCQIzQmnG", "cdate": 1546300800000, "mdate": null, "content": {"title": "A simple bound on the BER of the MAP decoder for massive MIMO systems.", "abstract": "The deployment of massive MIMO systems has revived much of the interest in the study of the large-system performance of multiuser detection systems. In this paper, we prove a non-trivial upper bound on the bit-error rate (BER) of the MAP detector for BPSK signal transmission and equal-power condition. In particular, our bound is approximately tight at high-SNR. The proof is simple and relies on Gordon's comparison inequality. Interestingly, we show that under the assumption that Gordon's inequality is tight, the resulting BER prediction matches that of the replica method under the replica symmetry (RS) ansatz. Also, we prove that, when the ratio of receive to transmit antennas exceeds $0.9251$, the replica prediction matches the matched filter lower bound (MFB) at high-SNR. We corroborate our results by numerical evidence."}}
{"id": "kHGYdHgNAZC", "cdate": 1546300800000, "mdate": null, "content": {"title": "The All-or-Nothing Phenomenon in Sparse Linear Regression.", "abstract": "We study the problem of recovering a hidden binary $k$-sparse $p$-dimensional vector $\\beta$ from $n$ noisy linear observations $Y=X\\beta+W$ where $X_{ij}$ are i.i.d. $\\mathcal{N}(0,1)$ and $W_i$ are i.i.d. $\\mathcal{N}(0,\\sigma^2)$. A closely related hypothesis testing problem is to distinguish the pair $(X,Y)$ generated from this structured model from a corresponding null model where $(X,Y)$ consist of purely independent Gaussian entries. In the low sparsity $k=o(p)$ and high signal to noise ratio $k/\\sigma^2=\\Omega\\left(1\\right)$ regime, we establish an \u201cAll-or-Nothing\u201d information-theoretic phase transition at a critical sample size $n^*=2 k\\log \\left(p/k\\right) /\\log \\left(1+k/\\sigma^2\\right)$, resolving a conjecture of [GamarnikZadik17]. Specifically, we show that if $\\liminf_{p\\rightarrow \\infty} n/n^*>1$, then the maximum likelihood estimator almost perfectly recovers the hidden vector with high probability and moreover the true hypothesis can be detected with a vanishing error probability. Conversely, if $\\limsup_{p\\rightarrow \\infty} n/n^*<1$, then it becomes information-theoretically impossible even to recover an arbitrarily small but fixed fraction of the hidden vector support, or to test hypotheses strictly better than random guess. Our proof of the impossibility result builds upon two key techniques, which could be of independent interest. First, we use a conditional second moment method to upper bound the Kullback-Leibler (KL) divergence between the structured and the null model. Second, inspired by the celebrated area theorem, we establish a lower bound to the minimum mean squared estimation error of the hidden vector in terms of the KL divergence between the two models."}}
{"id": "bz9TK9-hvk", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Simple Bound on the BER of the Map Decoder for Massive MIMO Systems.", "abstract": "The deployment of massive MIMO systems has revived much of the interest in the study of the large-system performance of multiuser detection systems. In this paper, we prove a non-trivial upper bound on the bit-error rate (BER) of the MAP detector for BPSK signal transmission and equal-power condition. In particular, our bound is approximately tight at high-SNR. The proof is simple and relies on Gordon's comparison inequality. Interestingly, we show that under the assumption that Gordon's inequality is tight, the resulting BER prediction matches that of the replica method under the replica symmetry (RS) ansatz. Also, we prove that, when the ratio of receive to transmit antennas exceeds 0.9251, the replica prediction matches the matched filter lower bound (MFB) at high-SNR. We corroborate our results by numerical evidence."}}
