{"id": "Yl0uBgH709W", "cdate": 1577836800000, "mdate": null, "content": {"title": "Semi-supervised URL Segmentation with Recurrent Neural Networks Pre-trained on Knowledge Graph Entities", "abstract": "Breaking domain names such as openresearch into component words open and research is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks (RNNs) using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33% and brings the sequence accuracy to 85%."}}
{"id": "KsjLPp_y-nb", "cdate": 1577836800000, "mdate": null, "content": {"title": "Semi-supervised URL Segmentation with Recurrent Neural NetworksPre-trained on Knowledge Graph Entities", "abstract": "Breaking domain names such as openresearch into component words open and research is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks (RNNs) using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33% and brings the sequence accuracy to 85%."}}
{"id": "wBO8yGKVYEi", "cdate": 1546300800000, "mdate": null, "content": {"title": "Improving Performance of End-to-End ASR on Numeric Sequences", "abstract": "Recognizing written domain numeric utterances (e.g., I need 1.25.) can be challenging for ASR systems, particularly when numeric sequences are not seen during training. This out-of-vocabulary (OOV) issue is addressed in conventional ASR systems by training part of the model on spoken domain utterances (e.g., I need one dollar and twenty five cents.), for which numeric sequences are composed of in-vocabulary numbers, and then using an FST verbalizer to denormalize the result. Unfortunately, conventional ASR models are not suitable for the low memory setting of on-device speech recognition. E2E models such as RNN-T are attractive for on-device ASR, as they fold the AM, PM and LM of a conventional model into one neural network. However, in the on-device setting the large memory footprint of an FST denormer makes spoken domain training more difficult. In this paper, we investigate techniques to improve E2E model performance on numeric data. We find that using a text-to-speech system to generate additional numeric training data, as well as using a small-footprint neural network to perform spoken-to-written domain denorming, yields improvement in several numeric classes. In the case of the longest numeric sequences, we see reduction of WER by up to a factor of 8."}}
{"id": "dp1KyEcLdrQ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Improving Performance of End-to-End ASR on Numeric Sequences", "abstract": "Recognizing written domain numeric utterances (e.g. I need $1.25.) can be challenging for ASR systems, particularly when numeric sequences are not seen during training. This out-of-vocabulary (OOV) issue is addressed in conventional ASR systems by training part of the model on spoken domain utterances (e.g. I need one dollar and twenty five cents.), for which numeric sequences are composed of in-vocabulary numbers, and then using an FST verbalizer to denormalize the result. Unfortunately, conventional ASR models are not suitable for the low memory setting of on-device speech recognition. E2E models such as RNN-T are attractive for on-device ASR, as they fold the AM, PM and LM of a conventional model into one neural network. However, in the on-device setting the large memory footprint of an FST denormer makes spoken domain training more difficult. In this paper, we investigate techniques to improve E2E model performance on numeric data. We find that using a text-to-speech system to generate additional numeric training data, as well as using a small-footprint neural network to perform spoken-to-written domain denorming, yields improvement in several numeric classes. In the case of the longest numeric sequences, we see reduction of WER by up to a factor of 8."}}
{"id": "TabuvSZkj88", "cdate": 1546300800000, "mdate": null, "content": {"title": "Dual Encoder Classifier Models as Constraints in Neural Text Normalization", "abstract": "Neural text normalization systems can achieve low error rates; however, the errors they make include not only ones from which the hearer can recover (such as reading 3 as three dollar) but also unrecoverable errors, such as reading 3 as three euros. FST decoding constraints have proven effective at reducing unrecoverable errors. In this paper we explore an alternative approach to error mitigation: using dual encoder classifiers trained with both positive and negative examples to implement soft constraints on acceptability. Since the error rates are very low, it is difficult to determine when improvement is significant, but qualitative analysis suggests that soft dual encoder constraints can help reduce the number of unrecoverable errors."}}
{"id": "2y-vtDg6uET", "cdate": 1546300800000, "mdate": null, "content": {"title": "Neural Models of Text Normalization for Speech Applications", "abstract": "Machine learning, including neural network techniques, have been applied to virtually every domain in natural language processing. One problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text-to-speech synthesis (TTS). In this application, one must decide, for example, that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 King Ave. For this task, state-of-the-art industrial systems depend heavily on hand-written language-specific grammars.We propose neural network models that treat text normalization for TTS as a sequence-to-sequence problem, in which the input is a text token in context, and the output is the verbalization of that token. We find that the most effective model, in accuracy and efficiency, is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization. This model allows for a great deal of flexibility in terms of representing the context, and also allows us to integrate tagging and segmentation into the process.These models perform very well overall, but occasionally they will predict wildly inappropriate verbalizations, such as reading 3 cm as three kilometers. Although rare, such verbalizations are a major issue for TTS applications. We thus use finite-state covering grammars to guide the neural models, either during training and decoding, or just during decoding, away from such \u201cunrecoverable\u201d errors. Such grammars can largely be learned from data."}}
{"id": "NGlxP3_YGE", "cdate": 1514764800000, "mdate": null, "content": {"title": "Fast and Accurate Reordering with ITG Transition RNN", "abstract": "Attention-based sequence-to-sequence neural network models learn to jointly align and translate. The quadratic-time attention mechanism is powerful as it is capable of handling arbitrary long-distance reordering, but computationally expensive. In this paper, towards making neural translation both accurate and efficient, we follow the traditional pre-reordering approach to decouple reordering from translation. We add a reordering RNN that shares the input encoder with the decoder. The RNNs are trained jointly with a multi-task loss function and applied sequentially at inference time. The task of the reordering model is to predict the permutation of the input words following the target language word order. After reordering, the attention in the decoder becomes more peaked and monotonic. For reordering, we adopt the Inversion Transduction Grammars (ITG) and propose a transition system to parse input to trees for reordering. We harness the ITG transition system with RNN. With the modeling power of RNN, we achieve superior reordering accuracy without any feature engineering. In experiments, we apply the model to the task of text normalization. Compared to a strong baseline of attention-based RNN, our ITG RNN re-ordering model can reach the same reordering accuracy with only 1/10 of the training data and is 2.5x faster in decoding."}}
{"id": "-kqmkoPSqj3", "cdate": 1451606400000, "mdate": null, "content": {"title": "Knowledge Exploration using Tables on the Web", "abstract": ""}}
{"id": "UOoY2S44V3Z", "cdate": 1230768000000, "mdate": null, "content": {"title": "Binarization of Synchronous Context-Free Grammars", "abstract": "Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages. We develop a theory of binarization for synchronous context-free grammars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem."}}
{"id": "zKJ5FiaPkPP", "cdate": 1199145600000, "mdate": null, "content": {"title": "Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time", "abstract": "Hao Zhang, Daniel Gildea, David Chiang. Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). 2008."}}
