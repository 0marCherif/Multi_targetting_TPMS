{"id": "uOzDfh1q-b", "cdate": 1577836800000, "mdate": 1631737064502, "content": {"title": "Identifying Critical Neurons in ANN Architectures using Mixed Integer Programming", "abstract": "We introduce a mixed integer program (MIP) for assigning importance scores to each neuron in deep neural network architectures which is guided by the impact of their simultaneous pruning on the main learning task of the network. By carefully devising the objective function of the MIP, we drive the solver to minimize the number of critical neurons (i.e., with high importance score) that need to be kept for maintaining the overall accuracy of the trained neural network. Further, the proposed formulation generalizes the recently considered lottery ticket optimization by identifying multiple \"lucky\" sub-networks resulting in optimized architecture that not only performs well on a single dataset, but also generalizes across multiple ones upon retraining of network weights. Finally, we present a scalable implementation of our method by decoupling the importance scores across layers using auxiliary networks. We demonstrate the ability of our formulation to prune neural networks with marginal loss in accuracy and generalizability on popular datasets and architectures."}}
{"id": "IaSuQRTPnWo", "cdate": 1577836800000, "mdate": 1631737064501, "content": {"title": "Towards Lifelong Self-Supervision For Unpaired Image-to-Image Translation", "abstract": "Unpaired Image-to-Image Translation (I2IT) tasks often suffer from lack of data, a problem which self-supervised learning (SSL) has recently been very popular and successful at tackling. Leveraging auxiliary tasks such as rotation prediction or generative colorization, SSL can produce better and more robust representations in a low data regime. Training such tasks along an I2IT task is however computationally intractable as model size and the number of task grow. On the other hand, learning sequentially could incur catastrophic forgetting of previously learned tasks. To alleviate this, we introduce Lifelong Self-Supervision (LiSS) as a way to pre-train an I2IT model (e.g., CycleGAN) on a set of self-supervised auxiliary tasks. By keeping an exponential moving average of past encoders and distilling the accumulated knowledge, we are able to maintain the network's validation performance on a number of tasks without any form of replay, parameter isolation or retraining techniques typically used in continual learning. We show that models trained with LiSS perform better on past tasks, while also being more robust than the CycleGAN baseline to color bias and entity entanglement (when two entities are very close)."}}
{"id": "agbmsY9-snj", "cdate": 1514764800000, "mdate": 1631737101852, "content": {"title": "Gender aware spoken language translation applied to English-Arabic", "abstract": "Spoken Language Translation (SLT) is becoming more widely used and becoming a communication tool that helps in crossing language barriers. One of the challenges of SLT is the translation from a language without gender agreement to a language with gender agreement such as English to Arabic. In this paper, we introduce an approach to tackle such limitation by enabling a Neural Machine Translation system to produce gender-aware unbiased translation. We show that NMT system can model the speaker/listener gender information to produce gender-aware translation that reduces the bias effect resulting from having training data dominated by particular gender forms. We propose a method to generate data used in adapting a NMT system to produce gender-aware and unbiased translation. The proposed approach can achieve significant improvement of the translation quality by 2 BLEU points."}}
{"id": "4h1dDWM9wF3", "cdate": 1483228800000, "mdate": 1631737101851, "content": {"title": "Synthetic Spoken Data for Neural Machine Translation", "abstract": "In this paper, we introduce a novel approach to generate synthetic data for training Neural Machine Translation systems. The proposed approach transforms a given parallel corpus between a written language and a target language to a parallel corpus between a spoken dialect variant and the target language. Our approach is language independent and can be used to generate data for any variant of the source language such as slang or spoken dialect or even for a different language that is closely related to the source language. The proposed approach is based on local embedding projection of distributed representations which utilizes monolingual embeddings to transform parallel data across language variants. We report experimental results on Levantine to English translation using Neural Machine Translation. We show that the generated data can improve a very large scale system by more than 2.8 Bleu points using synthetic spoken data which shows that it can be used to provide a reliable translation system for a spoken dialect that does not have sufficient parallel data."}}
