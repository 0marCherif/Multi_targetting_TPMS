{"id": "KAv7y4Ey0PE", "cdate": 1686250301976, "mdate": null, "content": {"title": "Bad Habits: Policy Confounding and Out-of-Trajectory Generalization in RL", "abstract": "Reinforcement learning agents may sometimes develop habits that are effective only when specific policies are followed. After an initial exploration phase in which agents try out different actions, they eventually converge toward a particular policy. When this occurs, the distribution of state-action trajectories becomes narrower, and agents start experiencing the same transitions again and again. At this point, spurious correlations may arise. Agents may then pick up on these correlations and learn state representations that do not generalize beyond the agent's trajectory distribution. In this paper, we provide a mathematical characterization of this phenomenon, which we refer to as policy confounding, and show, through a series of examples, when and how it occurs in practice."}}
{"id": "qceQIa70YC", "cdate": 1653042913484, "mdate": 1653042913484, "content": {"title": "Influence-Augmented Local Simulators: A Scalable Solution for Fast Deep RL in Large Networked Systems", "abstract": "Learning effective policies for real-world problems is still an open challenge for the field of reinforcement learning (RL). The main limitation being the amount of data needed and the pace at which that data can be obtained. In this paper, we study how to build lightweight simulators of complicated systems that can run sufficiently fast for deep RL to be applicable. We focus on domains where agents interact with a reduced portion of a larger environment while still being affected by the global dynamics. Our method combines the use of local simulators with learned models that mimic the influence of the global system. The experiments reveal that incorporating this idea into the deep RL workflow can considerably accelerate the training process and presents several opportunities for the future."}}
{"id": "lKFOwaYNQlb", "cdate": 1652737754038, "mdate": null, "content": {"title": "Distributed Influence-Augmented Local Simulators for Parallel MARL in Large Networked Systems", "abstract": "Due to its high sample complexity, simulation is, as of today, critical for the successful application of reinforcement learning. Many real-world problems, however, exhibit overly complex dynamics, making their full-scale simulation computationally slow. In this paper, we show how to factorize large networked systems of many agents into multiple local regions such that we can build separate simulators that run independently and in parallel. To monitor the influence that the different local regions exert on one another, each of these simulators is equipped with a learned model that is periodically trained on real trajectories. Our empirical results reveal that distributing the simulation among different processes not only makes it possible to train large multi-agent systems in just a few hours but also helps mitigate the negative effects of simultaneous learning."}}
{"id": "_Aqh3HW6Bi", "cdate": 1640995200000, "mdate": 1682327287413, "content": {"title": "Distributed Influence-Augmented Local Simulators for Parallel MARL in Large Networked Systems", "abstract": "Due to its high sample complexity, simulation is, as of today, critical for the successful application of reinforcement learning. Many real-world problems, however, exhibit overly complex dynamics, which makes their full-scale simulation computationally slow. In this paper, we show how to decompose large networked systems of many agents into multiple local components such that we can build separate simulators that run independently and in parallel. To monitor the influence that the different local components exert on one another, each of these simulators is equipped with a learned model that is periodically trained on real trajectories. Our empirical results reveal that distributing the simulation among different processes not only makes it possible to train large multi-agent systems in just a few hours but also helps mitigate the negative effects of simultaneous learning."}}
{"id": "Mf1-Bebi2f", "cdate": 1640995200000, "mdate": 1682327287301, "content": {"title": "Influence-Augmented Local Simulators: a Scalable Solution for Fast Deep RL in Large Networked Systems", "abstract": "Learning effective policies for real-world problems is still an open challenge for the field of reinforcement learning (RL). The main limitation being the amount of data needed and the pace at whic..."}}
{"id": "KH-hTzDfGLj", "cdate": 1640995200000, "mdate": 1682327287392, "content": {"title": "Online Planning in POMDPs with Self-Improving Simulators", "abstract": "How can we plan efficiently in a large and complex environment when the time budget is limited? Given the original simulator of the environment, which may be computationally very demanding, we propose to learn online an approximate but much faster simulator that improves over time. To plan reliably and efficiently while the approximate simulator is learning, we develop a method that adaptively decides which simulator to use for every simulation, based on a statistic that measures the accuracy of the approximate simulator. This allows us to use the approximate simulator to replace the original simulator for faster simulations when it is accurate enough under the current context, thus trading off simulation speed and accuracy. Experimental results in two large domains show that when integrated with POMCP, our approach allows to plan with improving efficiency over time."}}
{"id": "0LLqLjMFkB", "cdate": 1640995200000, "mdate": 1682327287370, "content": {"title": "Speeding up Deep Reinforcement Learning through Influence-Augmented Local Simulators", "abstract": ""}}
{"id": "_aIw7yYsSlZ", "cdate": 1609459200000, "mdate": 1682350731011, "content": {"title": "Offline Contextual Bandits for Wireless Network Optimization", "abstract": "The explosion in mobile data traffic together with the ever-increasing expectations for higher quality of service call for the development of AI algorithms for wireless network optimization. In this paper, we investigate how to learn policies that can automatically adjust the configuration parameters of every cell in the network in response to the changes in the user demand. Our solution combines existent methods for offline learning and adapts them in a principled way to overcome crucial challenges arising in this context. Empirical results suggest that our proposed method will achieve important performance gains when deployed in the real network while satisfying practical constrains on computational efficiency."}}
{"id": "Arx7Gw3BhX", "cdate": 1577836800000, "mdate": 1682327287378, "content": {"title": "Influence-Augmented Online Planning for Complex Environments", "abstract": "How can we plan efficiently in real time to control an agent in a complex environment that may involve many other agents? While existing sample-based planners have enjoyed empirical success in large POMDPs, their performance heavily relies on a fast simulator. However, real-world scenarios are complex in nature and their simulators are often computationally demanding, which severely limits the performance of online planners. In this work, we propose influence-augmented online planning, a principled method to transform a factored simulator of the entire environment into a local simulator that samples only the state variables that are most relevant to the observation and reward of the planning agent and captures the incoming influence from the rest of the environment using machine learning methods. Our main experimental results show that planning on this less accurate but much faster local simulator with POMCP leads to higher real-time planning performance than planning on the simulator that models the entire environment."}}
{"id": "rJlS-ertwr", "cdate": 1569439740851, "mdate": null, "content": {"title": "Influence-aware Memory for Deep Reinforcement Learning", "abstract": "Making the right decisions when some of the state variables are hidden, involves reasoning about all the possible states of the environment. An agent receiving only partial observations needs to infer the true values of these hidden variables based on the history of experiences. Recent deep reinforcement learning methods use recurrent models to keep track of past information. However, these models are sometimes expensive to train and have convergence difficulties, especially when dealing with high dimensional input spaces. Taking inspiration from influence-based abstraction, we show that effective policies can be learned in the presence of uncertainty by only memorizing a small subset of input variables. We also incorporate a mechanism in our network that learns to automatically choose the important pieces of information that need to be remembered. The results indicate that, by forcing the agent's internal memory to focus on the selected regions while treating the rest of the observable variables as Markovian, we can outperform ordinary recurrent architectures in situations where the amount of information that the agent needs to retain represents a small fraction of the entire observation input. The method also reduces training time and obtains better scores than methods that use a fixed window of experiences as input to remove partial observability in domains where long-term memory is required."}}
