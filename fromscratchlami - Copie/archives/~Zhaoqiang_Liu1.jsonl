{"id": "eKGZzvhmuv", "cdate": 1672531200000, "mdate": 1681969424210, "content": {"title": "DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning", "abstract": "Diffusion models have proven to be highly effective in generating high-quality images. However, adapting large pre-trained diffusion models to new domains remains an open challenge, which is critical for real-world applications. This paper proposes DiffFit, a parameter-efficient strategy to fine-tune large pre-trained diffusion models that enable fast adaptation to new domains. DiffFit is embarrassingly simple that only fine-tunes the bias term and newly-added scaling factors in specific layers, yet resulting in significant training speed-up and reduced model storage costs. Compared with full fine-tuning, DiffFit achieves 2$\\times$ training speed-up and only needs to store approximately 0.12\\% of the total model parameters. Intuitive theoretical analysis has been provided to justify the efficacy of scaling factors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior or competitive performances compared to the full fine-tuning while being more efficient. Remarkably, we show that DiffFit can adapt a pre-trained low-resolution generative model to a high-resolution one by adding minimal cost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of 3.02 on ImageNet 512$\\times$512 benchmark by fine-tuning only 25 epochs from a public pre-trained ImageNet 256$\\times$256 checkpoint while being 30$\\times$ more training efficient than the closest competitor."}}
{"id": "RUXo2_HerXd", "cdate": 1672531200000, "mdate": 1681831148434, "content": {"title": "DDP: Diffusion Model for Dense Visual Prediction", "abstract": "We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline. Our approach follows a \"noise-to-map\" generative paradigm for prediction by progressively removing noise from a random Gaussian distribution, guided by the image. The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline. Without task-specific design and architecture customization, DDP is easy to generalize to most dense prediction tasks, e.g., semantic segmentation and depth estimation. In addition, DDP shows attractive properties such as dynamic inference and uncertainty awareness, in contrast to previous single-step discriminative methods. We show top results on three representative tasks with six diverse benchmarks, without tricks, DDP achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts. For example, semantic segmentation (83.9 mIoU on Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation (0.05 REL on KITTI). We hope that our approach will serve as a solid baseline and facilitate future research"}}
{"id": "--aQNMdJc9x", "cdate": 1652737331173, "mdate": null, "content": {"title": "Misspecified Phase Retrieval with Generative Priors", "abstract": "In this paper, we study phase retrieval under model misspecification and generative priors. In particular, we aim to estimate an $n$-dimensional signal $\\mathbf{x}$ from $m$ i.i.d.~realizations of the single index model $y = f(\\mathbf{a}^T\\mathbf{x})$, where $f$ is an unknown and possibly random nonlinear link function and $\\mathbf{a} \\in \\mathbb{R}^n$ is a standard Gaussian vector. We make the assumption $\\mathrm{Cov}[y,(\\mathbf{a}^T\\mathbf{x})^2] \\ne 0$, which corresponds to the misspecified phase retrieval problem. In addition, the underlying signal $\\mathbf{x}$ is assumed to lie in the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a two-step approach, for which the first step plays the role of spectral initialization and the second step refines the estimated vector produced by the first step iteratively. We show that both steps enjoy a statistical rate of order $\\sqrt{(k\\log L)\\cdot (\\log m)/m}$ under suitable conditions. Experiments on image datasets are performed to demonstrate that our approach performs on par with or even significantly outperforms several competing methods. "}}
{"id": "pgir5f7ekAL", "cdate": 1632875466115, "mdate": null, "content": {"title": "Generative Principal Component Analysis", "abstract": "In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the first principal eigenvector lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\\sqrt{\\frac{k\\log L}{m}}$, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges exponentially fast to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis."}}
{"id": "k7Q71M4BPNK", "cdate": 1621629863191, "mdate": null, "content": {"title": "Towards Sample-Optimal Compressive Phase Retrieval with Sparse and Generative Priors", "abstract": "Compressive phase retrieval is a popular variant of the standard compressive sensing problem in which the measurements only contain magnitude information. In this paper, motivated by recent advances in deep generative models, we provide recovery guarantees with near-optimal sample complexity for phase retrieval with generative priors. We first show that when using i.i.d. Gaussian measurements and an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs, roughly $O(k \\log L)$ samples suffice to guarantee that any signal minimizing an amplitude-based empirical loss function is close to the true signal. Attaining this sample complexity with a practical algorithm remains a difficult challenge, and finding a good initialization for gradient-based methods has been observed to pose a major bottleneck. To partially address this, we further show that roughly $O(k \\log L)$ samples ensure sufficient closeness between the underlying signal and any {\\em globally optimal} solution to an optimization problem designed for spectral initialization (though finding such a solution may still be challenging). We also adapt this result to sparse phase retrieval, and show that $O(s \\log n)$ samples are sufficient for a similar guarantee when the underlying signal is $s$-sparse and $n$-dimensional, matching an information-theoretic lower bound. While these guarantees do not directly correspond to a practical algorithm, we propose a practical spectral initialization method motivated by our findings, and experimentally observe performance gains over various existing spectral initialization methods for sparse phase retrieval."}}
{"id": "vR5QLn0qw8k", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Generalized Lasso with Nonlinear Observations and Generative Priors", "abstract": "In this paper, we study the problem of signal estimation from noisy non-linear measurements when the unknown $n$-dimensional signal is in the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We make the assumption of sub-Gaussian measurements, which is satisfied by a wide range of measurement models, such as linear, logistic, 1-bit, and other quantized models. In addition, we consider the impact of adversarial corruptions on these measurements. Our analysis is based on a generalized Lasso approach (Plan and Vershynin, 2016). We first provide a non-uniform recovery guarantee, which states that under i.i.d.~Gaussian measurements, roughly $O\\left(\\frac{k}{\\epsilon^2}\\log L\\right)$ samples suffice for recovery with an $\\ell_2$-error of $\\epsilon$, and that this scheme is robust to adversarial noise. Then, we apply this result to neural network generative models, and discuss various extensions to other models and non-i.i.d.~measurements. Moreover, we show that our result can be extended to the uniform recovery guarantee under the assumption of a so-called local embedding property, which is satisfied by the 1-bit and censored Tobit models."}}
{"id": "LPfQHr8nkZQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sample Complexity Bounds for 1-bit Compressive Sensing and Binary Stable Embeddings with Generative Priors", "abstract": "The goal of standard 1-bit compressive sensing is to accurately recover an unknown sparse vector from binary-valued measurements, each indicating the sign of a linear function of the vector. Motiva..."}}
{"id": "Bda_5tZrTCl", "cdate": 1577836800000, "mdate": null, "content": {"title": "Information-Theoretic Lower Bounds for Compressive Sensing With Generative Models", "abstract": "It has recently been shown that for compressive sensing, significantly fewer measurements may be required if the sparsity assumption is replaced by the assumption the unknown vector lies near the range of a suitably-chosen generative model. In particular, in (Bora et al., 2017) it was shown roughly O(k log L) random Gaussian measurements suffice for accurate recovery when the generative model is an L-Lipschitz function with bounded k-dimensional inputs, and O(kdlogw) measurements suffice when the generative model is a k-input ReLU network with depth d and width w. In this paper, we establish corresponding algorithm-independent lower bounds on the sample complexity using tools from minimax statistical analysis. In accordance with the above upper bounds, our results are summarized as follows: (i) We construct an L-Lipschitz generative model capable of generating group-sparse signals, and show that the resulting necessary number of measurements is \u03a9(k logL); (ii) Using similar ideas, we construct ReLU networks with high depth and/or high depth for which the necessary number of measurements scales as \u03a9(kd logw/logn) (with output dimension n), and in some cases \u03a9(kd logw). As a result, we establish that the scaling laws derived in (Bora et al., 2017) are optimal or near-optimal in the absence of further assumptions."}}
{"id": "SJgYiQnq8H", "cdate": 1568486305299, "mdate": null, "content": {"title": "Sample Complexity Lower Bounds for Compressive Sensing with Generative Models", "abstract": "The goal of standard compressive sensing is to estimate an unknown vector from linear measurements under the assumption of sparsity in some basis. Recently, it has been shown that significantly fewer measurements may be required if the sparsity assumption is replaced by the assumption that the unknown vector lies near the range of a suitably-chosen generative model.  In particular, in (Bora {\\em et al.}, 2017) it was shown that roughly $O(k\\log L)$ random Gaussian measurements suffice for accurate recovery when the $k$-input generative model is bounded and $L$-Lipschitz, and that $O(kd \\log w)$ measurements suffice for $k$-input ReLU networks with depth $d$ and width $w$.  In this paper, we establish corresponding algorithm-independent lower bounds on the sample complexity using tools from minimax statistical analysis.  In accordance with the above upper bounds, our results are summarized as follows: (i) We construct an $L$-Lipschitz generative model capable of generating group-sparse signals, and show that the resulting necessary number of measurements is $\\Omega(k \\log L)$; (ii) Using similar ideas, we construct two-layer ReLU networks of high width requiring $\\Omega(k \\log w)$ measurements, as well as lower-width deep ReLU networks requiring $\\Omega(k d)$ measurements.  As a result, we establish that the scaling laws derived in (Bora {\\em et al.}, 2017) are optimal or near-optimal in the absence of further assumptions."}}
{"id": "cY9FQ4pktZc", "cdate": 1546300800000, "mdate": null, "content": {"title": "Error Bounds for Spectral Clustering over Samples from Spherical Gaussian Mixture Models", "abstract": "Spectral clustering has been one of the most popular methods for clustering multivariate data and has been widely used in image processing and data mining. Despite its considerable empirical success, the theoretical properties of spectral clustering are not yet fully developed. In this paper, we derive upper bounds for the clustering error of spectral clustering for data samples generated from spherical Gaussian mixture models. In our analysis, first, the graph Laplacian calculated from samples is approximated by a reference graph Laplacian which has good spectral properties. Second, we use the Davis-Kahan perturbation theorem to provide an upper bound for the sum of squared distances between each projected data point and its cluster center. Finally, we leverage theoretical results of MeilIa's to prove an upper bound for the clustering error from the upper bound for the sum of squared distances."}}
