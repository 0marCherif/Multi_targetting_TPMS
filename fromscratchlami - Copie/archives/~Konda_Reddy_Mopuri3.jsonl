{"id": "feVz2v5ty8", "cdate": 1672531200000, "mdate": 1681672363532, "content": {"title": "Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation", "abstract": "Data-free Knowledge Distillation (DFKD) has gained popularity recently, with the fundamental idea of carrying out knowledge transfer from a Teacher neural network to a Student neural network in the absence of training data. However, in the Adversarial DFKD framework, the student network's accuracy, suffers due to the non-stationary distribution of the pseudo-samples under multiple generator updates. To this end, at every generator update, we aim to maintain the student's performance on previously encountered examples while acquiring knowledge from samples of the current distribution. Thus, we propose a meta-learning inspired framework by treating the task of Knowledge-Acquisition (learning from newly generated samples) and Knowledge-Retention (retaining knowledge on previously met samples) as meta-train and meta-test, respectively. Hence, we dub our method as Learning to Retain while Acquiring. Moreover, we identify an implicit aligning factor between the Knowledge-Retention and Knowledge-Acquisition tasks indicating that the proposed student update strategy enforces a common gradient direction for both tasks, alleviating interference between the two objectives. Finally, we support our hypothesis by exhibiting extensive evaluation and comparison of our method with prior arts on multiple datasets."}}
{"id": "fjIF9Zt75m", "cdate": 1640995200000, "mdate": 1668239958327, "content": {"title": "Mining Data Impressions From Deep Models as Substitute for the Unavailable Training Data", "abstract": "Pretrained deep models hold their learnt knowledge in the form of model parameters. These parameters act as \u201cmemory\u201d for the trained models and help them generalize well on unseen data. However, in absence of training data, the utility of a trained model is merely limited to either inference or better initialization towards a target task. In this paper, we go further and extract synthetic data by leveraging the learnt model parameters. We dub them <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Data Impressions</i> , which act as proxy to the training data and can be used to realize a variety of tasks. These are useful in scenarios where only the pretrained models are available and the training data is not shared (e.g., due to privacy or sensitivity concerns). We show the applicability of data impressions in solving several computer vision tasks such as unsupervised domain adaptation, continual learning as well as knowledge distillation. We also study the adversarial robustness of lightweight models trained via knowledge distillation using these data impressions. Further, we demonstrate the efficacy of data impressions in generating data-free Universal Adversarial Perturbations (UAPs) with better fooling rates. Extensive experiments performed on benchmark datasets demonstrate competitive performance achieved using data impressions in absence of original training data."}}
{"id": "54UQL-lEI8", "cdate": 1640995200000, "mdate": 1682322891891, "content": {"title": "Adv-Cut Paste: Semantic adversarial class specific data augmentation technique for object detection", "abstract": "Data augmentation has been a prevalent approach in improving the performance of deep learning models against slight variations in data. Adversarial learning is one such form of data augmentation. In this work, we aim to introduce a framework to generate harder examples for a specific object class and an adversarial attack for the object detection task. We have also presented our study on the effect of training against such generated harder examples and adversarial samples in object detection. We have applied this adversarial learning technique to a YOLOv3 model and due to the nature of the attack, we demonstrated a substantial improvement in average precision (AP) for a single class of the COCO dataset. As per the literature, we are the first to introduce this kind of class-specific data augmentation strategy in object detection. With our approach, we have shown an improvement of 23.34% in AP for Cat class and 3.1% on overall mAP of YOLOv3 model on clean validation data, while 43.5% improvement in AP for the Cat class on the composite images with class-specific adversarial samples."}}
{"id": "mpUj7XO0GwX", "cdate": 1609459200000, "mdate": 1682322891783, "content": {"title": "Dataset Condensation with Gradient Matching", "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available."}}
{"id": "XTuGlTEg2eY", "cdate": 1609459200000, "mdate": 1652707443999, "content": {"title": "Class balancing GAN with a classifier in the loop", "abstract": "Generative Adversarial Networks (GANs) have swiftly evolved to imitate increasingly complex image distributions. However, majority of the developments focus on performance of GANs on balanced datas..."}}
{"id": "Ccrlgyk5I9w", "cdate": 1609459200000, "mdate": null, "content": {"title": "Data Impressions: Mining Deep Models to Extract Samples for Data-free Applications", "abstract": "Pretrained deep models hold their learnt knowledge in the form of model parameters. These parameters act as \"memory\" for the trained models and help them generalize well on unseen data. However, in absence of training data, the utility of a trained model is merely limited to either inference or better initialization towards a target task. In this paper, we go further and extract synthetic data by leveraging the learnt model parameters. We dub them \"Data Impressions\", which act as proxy to the training data and can be used to realize a variety of tasks. These are useful in scenarios where only the pretrained models are available and the training data is not shared (e.g., due to privacy or sensitivity concerns). We show the applicability of data impressions in solving several computer vision tasks such as unsupervised domain adaptation, continual learning as well as knowledge distillation. We also study the adversarial robustness of lightweight models trained via knowledge distillation using these data impressions. Further, we demonstrate the efficacy of data impressions in generating data-free Universal Adversarial Perturbations (UAPs) with better fooling rates. Extensive experiments performed on benchmark datasets demonstrate competitive performance achieved using data impressions in absence of original training data."}}
{"id": "08xayD3MK3m", "cdate": 1609459200000, "mdate": 1668239958346, "content": {"title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation", "abstract": ""}}
{"id": "yEnaS6yOkxy", "cdate": 1601308379001, "mdate": null, "content": {"title": "Class Balancing GAN with a Classifier in the Loop", "abstract": "Generative Adversarial Networks (GANs) have swiftly evolved to imitate increasingly complex image distributions. However, majority of the developments focus on performance of GANs on balanced datasets. We find that the existing GANs and their training regimes which work well on balanced datasets fail to be effective in case of imbalanced (i.e. long-tailed) datasets. In this work we introduce a novel and theoretically motivated Class Balancing regularizer for training GANs. Our regularizer makes use of the knowledge from a pre-trained classifier to ensure balanced learning of all the classes in the dataset. This is achieved via modelling the effective class frequency based on the exponential forgetting observed in neural networks and encouraging the GAN to focus on underrepresented classes. We demonstrate the utility of our contribution in two diverse scenarios: (i) Learning representations for long-tailed distributions, where we achieve better performance than existing approaches, and (ii) Generation of Universal Adversarial Perturbations (UAPs) in the data-free scenario for the large scale datasets, where we bridge the gap between data-driven and data-free approaches for crafting UAPs."}}
{"id": "mSAKhLYLSsl", "cdate": 1601308054074, "mdate": null, "content": {"title": "Dataset Condensation with Gradient Matching", "abstract": "As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available."}}
{"id": "s5UdvvKEull", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adversarial Fooling Beyond \"Flipping the Label\"", "abstract": "Recent advancements in CNNs have shown remarkable achievements in various CV/AI applications. Though CNNs show near human or better than human performance in many critical tasks, they are quite vulnerable to adversarial attacks. These attacks are potentially dangerous in real-life deployments. Though there have been many adversarial attacks proposed in recent years, there is no proper way of quantifying the effectiveness of these attacks. As of today, mere fooling rate is used for measuring the susceptibility of the models, or the effectiveness of adversarial attacks. Fooling rate just considers label flipping and does not consider the cost of such flipping, for instance, in some deployments, flipping between two species of dogs may not be as severe as confusing a dog category with that of a vehicle. Therefore, the metric to quantify the vulnerability of the models should capture the severity of the flipping as well. In this work we first bring out the drawbacks of the existing evaluation and propose novel metrics to capture various aspects of the fooling. Further, for the first time, we present a comprehensive analysis of several important adversarial attacks over a set of distinct CNN architectures. We believe that the presented analysis brings valuable insights about the current adversarial attacks and the CNN models."}}
