{"id": "BJVIVobdbB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Composing Value Functions in Reinforcement Learning", "abstract": "An important property for lifelong-learning agents is the ability to combine existing skills to solve new unseen tasks. In general, however, it is unclear how to compose existing skills in a princi..."}}
{"id": "Hye6uoC9tm", "cdate": 1538087796650, "mdate": null, "content": {"title": "Incremental Hierarchical Reinforcement Learning with Multitask LMDPs", "abstract": "Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these higher levels, rather than in the space of primitives. However, discovering a deep abstraction autonomously remains a largely unsolved problem, with practitioners typically hand-crafting these hierarchical control architectures. Recent work with multitask linear Markov decision processes, allows for the autonomous discovery of deep hierarchical abstractions, but operates exclusively in the offline setting. By extending this work, we develop an agent that is capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration."}}
{"id": "ry80wMW0W", "cdate": 1518730162544, "mdate": null, "content": {"title": "Hierarchical Subtask Discovery with Non-Negative Matrix Factorization", "abstract": "Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks. In this setting, the subtask discovery problem can naturally be posed as finding an optimal low-rank approximation of the set of tasks the agent will face in a domain. We use non-negative matrix factorization to discover this minimal basis set of tasks, and show that the technique learns intuitive decompositions in a variety of domains. Our method has several qualitatively desirable features: it is not limited to learning subtasks with single goal states, instead learning distributed patterns of preferred states; it learns qualitatively different hierarchical decompositions in the same domain depending on the ensemble of tasks the agent will face; and it may be straightforwardly iterated to obtain deeper hierarchical decompositions."}}
{"id": "ikkb5I9Efjz", "cdate": 1514764800000, "mdate": null, "content": {"title": "Will it Blend? Composing Value Functions in Reinforcement Learning.", "abstract": "An important property for lifelong-learning agents is the ability to combine existing skills to solve unseen tasks. In general, however, it is unclear how to compose skills in a principled way. We provide a \"recipe\" for optimal value function composition in entropy-regularised reinforcement learning (RL) and then extend this to the standard RL setting. Composition is demonstrated in a video game environment, where an agent with an existing library of policies is able to solve new tasks without the need for further learning."}}
{"id": "hTy-ODYjuBO", "cdate": 1483228800000, "mdate": null, "content": {"title": "An Algorithm for Minimum L-Infinity Solution of Under-determined Linear Systems.", "abstract": "This paper presents a primal method for finding the minimum L-infinity solution to under-determined linear systems of equations. The method is a two-phase method. Line search is performed at both phases. We establish a condition for a direction to be descent. The convergence proof of the method is shown. Expedient numerical schemes can be used whenever appropriate. Results are presented, which show the superiority of the method over some well-known methods."}}
{"id": "BJ-zyjW_ZS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Hierarchy Through Composition with Multitask LMDPs", "abstract": "Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute actions serially, with macro-actions comprising sequences ..."}}
{"id": "If1vIkUw46", "cdate": 1451606400000, "mdate": null, "content": {"title": "Hierarchy through Composition with Linearly Solvable Markov Decision Processes.", "abstract": "Hierarchical architectures are critical to the scalability of reinforcement learning methods. Current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme uses the concurrent compositionality provided by the linearly solvable Markov decision process (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time."}}
