{"id": "z7PABKNy0-z", "cdate": 1682958567749, "mdate": null, "content": {"title": "FedGP: Buffer-based Gradient Projection for Continual Federated Learning", "abstract": "Continual Federated Learning (CFL) has garnered significant attention in recent years due to its potential in real-world scenarios where multiple clients (e.g., mobile phones, autonomous vehicles) continuously observe data in a dynamic environment. However, CFL suffers from catastrophic forgetting, where the model forgets previously learned knowledge in favor of new data. To address this issue, we propose a novel method called buffer-based Gradient Projection (FedGP) that mitigates catastrophic forgetting by replaying local buffer samples and using aggregated buffer gradients to preserve the previously learned knowledge across clients. Our method can be combined with a variety of existing continual learning methods, and boost their performance in the CFL setup. Our approach is evaluated on both standard benchmark datasets and a realistic streaming decentralized automotive dataset generated using CARLA and OpenFL. "}}
{"id": "7n2dNATSHeQ", "cdate": 1675827744568, "mdate": null, "content": {"title": "Mini-Batch Optimization of Contrastive Loss", "abstract": "In this paper, we study the effect of mini-batch selection on contrastive loss and propose new mini-batch selection methods to improve efficiency.  Theoretically, we show that both the full-batch and mini-batch settings share the same solution, the simplex Equiangular Tight Frame (ETF), if all $\\binom{N}{B}$ mini-batches are seen during training.  However, when not all possible batches are seen, mini-batch training can lead to suboptimal solutions.  To address this issue, we propose efficient mini-batch selection methods that compare favorably with existing methods.  Our experimental results demonstrate the effectiveness of our proposed methods in finding a near-optimal solution with a reduced number of gradient steps and outperforming existing mini-batch selection methods."}}
{"id": "f2lX4uFQCLl", "cdate": 1675827743531, "mdate": null, "content": {"title": "LOOPED TRANSFORMERS AS PROGRAMMABLE COMPUTERS", "abstract": "We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches.\nUsing this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms  to programs that   can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can  emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention."}}
{"id": "3V9tRSJLw8", "cdate": 1675191751925, "mdate": 1675191751925, "content": {"title": "Looped Transformers as Programmable Computers", "abstract": "We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches.\nUsing these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms  to programs that   can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can  emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs."}}
{"id": "3nDOzT-Dfz", "cdate": 1672531200000, "mdate": 1681682630164, "content": {"title": "Looped Transformers as Programmable Computers", "abstract": "We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs."}}
{"id": "dhYUMMy0_Eg", "cdate": 1663850458882, "mdate": null, "content": {"title": "Equal Improvability: A New Fairness Notion Considering the Long-term Impact", "abstract": "Devising a fair classifier that does not discriminate against different groups is an important problem in machine learning. Although researchers have proposed various ways of defining group fairness, most of them only focused on the immediate fairness, ignoring the long-term impact of a fair classifier under the dynamic scenario where each individual can improve its feature over time. Such dynamic scenarios happen in real world, e.g., college admission and credit loaning, where each rejected sample makes effort to change its features to get accepted afterwards. In this dynamic setting, the long-term fairness should equalize the samples\u2019 feature distribution across different groups after the rejected samples make some effort to improve. In order to promote long-term fairness, we propose a new fairness notion called Equal Improvability (EI), which equalizes the potential acceptance rate of the rejected samples across different groups assuming a bounded level of effort will be spent by each rejected sample. We analyze the properties of EI and its connections with existing fairness notions. To find a classifier that satisfies the EI requirement, we propose and study three different approaches that solve EI regularized optimization problems. Through experiments on both synthetic and real datasets, we demonstrate that the proposed EI-regularized algorithms encourage us to find a fair classifier in terms of EI. Finally, we provide experimental results on dynamic scenarios which highlight the advantages of our EI metric in achieving the long-term fairness. Codes are available in anonymous GitHub repository."}}
{"id": "Jpxd93u2vK-", "cdate": 1652737568015, "mdate": null, "content": {"title": "Rare Gems: Finding Lottery Tickets at Initialization", "abstract": "Large neural networks can be pruned to a small fraction of their original size, with little loss in accuracy, by following a time-consuming \"train, prune, re-train\" approach. Frankle & Carbin conjecture that we can avoid this by training lottery tickets, i.e., special sparse subnetworks found at initialization, that can be trained to high accuracy. However, a subsequent line of work presents concrete evidence that current algorithms for finding trainable networks at initialization, fail simple baseline comparisons, e.g., against training random sparse subnetworks. Finding lottery tickets that train to better accuracy compared to simple baselines remains an open problem. In this work, we resolve this open problem by proposing Gem-Miner which finds lottery tickets at initialization that beat current baselines. Gem-Miner finds lottery tickets trainable to accuracy competitive or better than Iterative Magnitude Pruning (IMP), and does so up to $19\\times$ faster. "}}
{"id": "s_PJMEGIUfa", "cdate": 1652737465651, "mdate": null, "content": {"title": "LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks", "abstract": "Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling \"no-code machine learning with LMs.\"  We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning."}}
{"id": "pqbdJuUYaJ", "cdate": 1640995200000, "mdate": 1681686103018, "content": {"title": "Can We Find Strong Lottery Tickets in Generative Models?", "abstract": "Yes. In this paper, we investigate strong lottery tickets in generative models, the subnetworks that achieve good generative performance without any weight update. Neural network pruning is considered the main cornerstone of model compression for reducing the costs of computation and memory. Unfortunately, pruning a generative model has not been extensively explored, and all existing pruning algorithms suffer from excessive weight-training costs, performance degradation, limited generalizability, or complicated training. To address these problems, we propose to find a strong lottery ticket via moment-matching scores. Our experimental results show that the discovered subnetwork can perform similarly or better than the trained dense model even when only 10% of the weights remain. To the best of our knowledge, we are the first to show the existence of strong lottery tickets in generative models and provide an algorithm to find it stably. Our code and supplementary materials are publicly available."}}
{"id": "lr_sjdP49gp", "cdate": 1640995200000, "mdate": 1681682630204, "content": {"title": "Equal Improvability: A New Fairness Notion Considering the Long-term Impact", "abstract": "Devising a fair classifier that does not discriminate against different groups is an important problem in machine learning. Although researchers have proposed various ways of defining group fairness, most of them only focused on the immediate fairness, ignoring the long-term impact of a fair classifier under the dynamic scenario where each individual can improve its feature over time. Such dynamic scenarios happen in real world, e.g., college admission and credit loaning, where each rejected sample makes effort to change its features to get accepted afterwards. In this dynamic setting, the long-term fairness should equalize the samples' feature distribution across different groups after the rejected samples make some effort to improve. In order to promote long-term fairness, we propose a new fairness notion called Equal Improvability (EI), which equalizes the potential acceptance rate of the rejected samples across different groups assuming a bounded level of effort will be spent by each rejected sample. We analyze the properties of EI and its connections with existing fairness notions. To find a classifier that satisfies the EI requirement, we propose and study three different approaches that solve EI-regularized optimization problems. Through experiments on both synthetic and real datasets, we demonstrate that the proposed EI-regularized algorithms encourage us to find a fair classifier in terms of EI. Finally, we provide experimental results on dynamic scenarios which highlight the advantages of our EI metric in achieving the long-term fairness. Codes are available in a GitHub repository, see https://github.com/guldoganozgur/ei_fairness."}}
