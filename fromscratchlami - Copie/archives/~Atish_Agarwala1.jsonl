{"id": "A7IpSdqNWHy", "cdate": 1664731447384, "mdate": null, "content": {"title": "A Second-order Regression Model Shows Edge of Stability Behavior", "abstract": "Recent studies of learning algorithms have shown that there is a regime with an initial increase in the largest eigenvalue of the loss Hessian (progressive sharpening), followed by a stabilization of the eigenvalue near the maximum value which allows convergence (edge of stability). We consider a class of predictive models that are quadratic in the parameters, which we call second-order regression models. This is in contrast with the neural tangent kernel regime, where the predictive function is linear in the parameters. For quadratic objectives in two dimensions, we prove that this second order regression model exhibits both progressive sharpening and edge of stability behavior. We then show that in higher dimensions, the model shows this behavior generically without the structure of a neural network, due to a non-linearity induced in the learning dynamics. Finally, we show that edge of stability behavior in neural networks is correlated with the behavior in quadratic regression models."}}
{"id": "R2M14I9LEwW", "cdate": 1663850478851, "mdate": null, "content": {"title": "A second order regression model shows edge of stability behavior", "abstract": "Recent studies of learning algorithms have shown that there is a regime with an initial increase in the largest eigenvalue of the loss Hessian (progressive sharpening), followed by a stabilization of the eigenvalue near the maximum value which allows convergence (edge of stability). We consider a class of predictive models that are quadratic in the parameters, which we call second-order regression models. This is in contrast with the neural tangent kernel regime, where the predictive function is linear in the parameters. For quadratic objectives in two dimensions, we prove that this second order regression model exhibits both progressive sharpening and edge of stability behavior. We then show that in higher dimensions, the model shows this behavior generically without the structure of a neural network, due to a non-linearity induced in the learning dynamics. Finally, we show that edge of stability behavior in neural networks is correlated with the behavior in quadratic regression models."}}
{"id": "avHr-H-1kEa", "cdate": 1601308111852, "mdate": null, "content": {"title": "Temperature check: theory and practice for training models with softmax-cross-entropy losses", "abstract": "The softmax function combined with a cross-entropy loss is a principled approach to modeling probability distributions that has become ubiquitous in deep learning. The softmax function is defined by a lone hyperparameter, the temperature, that is commonly set to one or regarded as a way to tune model confidence after training; however, less is known about how the temperature impacts training dynamics or generalization performance. In this work we develop a theory of early learning for models trained with softmax-cross-entropy loss and show that the learning dynamics depend crucially on the inverse-temperature $\\beta$ as well as the magnitude of the logits at initialization, $||\\beta\\bf{z}||_{2}$. We follow up these analytic results with a large-scale empirical study of a variety of model architectures trained on CIFAR10, ImageNet, and IMDB sentiment analysis. We find that generalization performance depends strongly on the temperature, but only weakly on the initial logit magnitude. We provide evidence that the dependence of generalization on $\\beta$ is not due to changes in model confidence, but is a dynamical phenomenon. It follows that the addition of $\\beta$ as a tunable hyperparameter is key to maximizing model performance. Although we find the optimal $\\beta$ to be sensitive to the architecture, our results suggest that tuning $\\beta$ over the range $10^{-2}$ to $10^1$  improves performance over all architectures studied. We find that smaller $\\beta$ may lead to better peak performance at the cost of learning stability."}}
{"id": "uz5uw6gM0m", "cdate": 1601308048675, "mdate": null, "content": {"title": "One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks", "abstract": "Can deep learning solve multiple, very different tasks simultaneously? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks---for example, when the distinct tasks are encoded by well-separated clusters or decision trees over some task-code attributes. Indeed, more strongly, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that learning many tasks can be provably hard, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation."}}
