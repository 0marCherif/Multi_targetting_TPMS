{"id": "llza_S8mHT-", "cdate": 1664725484318, "mdate": null, "content": {"title": "On the Maximum Hessian Eigenvalue and Generalization", "abstract": "The mechanisms by which certain training interventions, such as increasing learning rates and applying batch normalization, improve the generalization of deep networks remains a mystery. Prior works have speculated that \"flatter\" solutions generalize better than \"sharper\" solutions to unseen data, motivating several metrics for measuring flatness (particularly $\\lambda_{max}$, the largest eigenvalue of the Hessian of the loss); and algorithms, such as Sharpness-Aware Minimization (SAM), that directly optimize for flatness. Other works question the link between $\\lambda_{max}$ and generalization. In this paper, we present findings that call $\\lambda_{max}$'s influence on generalization further into question. We show that: (1) while larger learning rates reduce $\\lambda_{max}$ for all batch sizes, generalization benefits sometimes vanish at larger batch sizes; (2) by scaling batch size and learning rate simultaneously, we can change $\\lambda_{max}$ without affecting generalization; (3) while SAM produces smaller $\\lambda_{max}$ for all batch sizes, generalization benefits (also) vanish with larger batch sizes; (4) for dropout, excessively high dropout probabilities can degrade generalization, even as they promote smaller $\\lambda_{max}$; and (5) while batch-normalization does not consistently produce smaller $\\lambda_{max}$, it nevertheless confers generalization benefits. While our experiments affirm the generalization benefits of large learning rates and SAM for minibatch SGD, the GD-SGD discrepancy demonstrates limits to $\\lambda_{max}$'s ability to explain generalization in neural networks."}}
{"id": "LE5LxBgjB4V", "cdate": 1663850198878, "mdate": null, "content": {"title": "Disentangling the Mechanisms Behind Implicit Regularization in SGD", "abstract": "A number of competing hypotheses have been proposed to explain why small-batch Stochastic Gradient Descent (SGD) leads to improved generalization over the full-batch regime, with recent work crediting the implicit regularization of various quantities throughout training. However, to date, empirical evidence assessing the explanatory power of these hypotheses is lacking. In this paper, we conduct an extensive empirical evaluation, focusing on the ability of various theorized mechanisms to close the small-to-large batch generalization gap. Additionally, we characterize how the quantities that SGD has been claimed to (implicitly) regularize change over the course of training. By using micro-batches, i.e. disjoint smaller subsets of each mini-batch, we empirically show that explicitly penalizing the gradient norm or the Fisher Information Matrix trace, averaged over micro-batches, in the large-batch regime recovers small-batch SGD generalization, whereas Jacobian-based regularizations fail to do so. This generalization performance is shown to often be correlated with how well the regularized model\u2019s gradient norms resemble those of small-batch SGD. We additionally show that this behavior breaks down as the micro-batch size approaches the batch size. Finally, we note that in this line of inquiry, positive experimental findings on CIFAR10 are often reversed on other datasets like CIFAR100, highlighting the need to test hypotheses on a wider collection of datasets."}}
{"id": "jh-rTtvkGeM", "cdate": 1601308209722, "mdate": null, "content": {"title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability", "abstract": "We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value $2 / \\text{(step size)}$, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability."}}
{"id": "47tamWdEUo", "cdate": 1546300800000, "mdate": 1683485956110, "content": {"title": "Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?", "abstract": "For a standard convolutional neural network, optimizing over the input pixels to maximize the score of some target class will generally produce a grainy-looking version of the original image. However, Santurkar et al. (2019) demonstrated that for adversarially-trained neural networks, this optimization produces images that uncannily resemble the target class. In this paper, we show that these \"perceptually-aligned gradients\" also occur under randomized smoothing, an alternative means of constructing adversarially-robust classifiers. Our finding supports the hypothesis that perceptually-aligned gradients may be a general property of robust classifiers. We hope that our results will inspire research aimed at explaining this link between perceptually-aligned gradients and adversarial robustness."}}
