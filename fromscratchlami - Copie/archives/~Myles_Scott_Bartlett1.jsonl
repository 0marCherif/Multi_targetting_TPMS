{"id": "3wg-rYuo5AN", "cdate": 1652737710751, "mdate": null, "content": {"title": "Okapi: Generalising Better by Making Statistical Matches Match", "abstract": "We propose Okapi, a simple, efficient, and general method for robust semi-supervised learning based on online statistical matching. Our method uses a nearest-neighbours-based matching procedure to generate cross-domain views for a consistency loss, while eliminating statistical outliers. In order to perform the online matching in a runtime- and memory-efficient way, we draw upon the self-supervised literature and combine a memory bank with a slow-moving momentum encoder. The consistency loss is applied within the feature space, rather than on the predictive distribution, making the method agnostic to both the modality and the task in question. We experiment on the WILDS 2.0 datasets Sagawa et al., which significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real-world unsupervised adaptation. Contrary to Sagawa et al., we show that it is in fact possible to leverage additional unlabelled data to improve upon empirical risk minimisation (ERM) results with the right method. Our method outperforms the baseline methods in terms of out-of-distribution (OOD) generalisation on the iWildCam (a multi-class classification task) and PovertyMap (a regression task) image datasets as well as the CivilComments (a binary classification task) text dataset. Furthermore, from a qualitative perspective, we show the matches obtained from the learned encoder are strongly semantically related. Code for our paper is publicly available at https://github.com/wearepal/okapi/."}}
{"id": "7IElVSrNm54", "cdate": 1601308390393, "mdate": null, "content": {"title": "Zero-shot Fairness with Invisible Demographics", "abstract": "In a statistical notion of algorithmic fairness, we partition individuals into groups based on some key demographic factors such as race and gender, and require that some statistics of a classifier be approximately equalized across those groups. Current approaches require complete annotations for demographic factors, or focus on an abstract worst-off group rather than demographic groups. In this paper, we consider the setting where the demographic factors are only partially available. For example, we have training examples for white-skinned and dark-skinned males, and white-skinned females, but we have zero examples for dark-skinned females. We could also have zero examples for females regardless of their skin colors. Without additional knowledge, it is impossible to directly control the discrepancy of the classifier's statistics for those invisible groups. We develop a disentanglement algorithm that splits a representation of data into a component that captures the demographic factors and another component that is invariant to them based on a context dataset. The context dataset is much like the deployment dataset, it is unlabeled but it contains individuals from all demographics including the invisible. We cluster the context set, equalize the cluster size to form a \"perfect batch\", and use it as a supervision signal for the disentanglement. We propose a new discriminator loss based on a learnable attention mechanism to distinguish a perfect batch from a non-perfect one. We evaluate our approach on standard classification benchmarks and show that it is indeed possible to protect invisible demographics."}}
{"id": "Skenc79PvS", "cdate": 1569330067617, "mdate": null, "content": {"title": "No Shortcuts: Removing Spurious Correlations with Invertible Neural Networks", "abstract": "Training data often contains spurious correlations not reflected in the real world.\nNevertheless, machine learning systems\ncan become over-reliant on these correlations for classifications, instead of basing their classification on those which are semantically meaningful.\nThis is undesirable for many reasons, among them limited generalisability.\nWe consider two datasets that exhibit spurious correlations: coloured MNIST\nand the UCI Adult dataset.\nTo solve this problem, we propose dividing training into a straightforward and general two-step procedure in which the model is first trained to produce\ninvariant representations from an unlabelled pre-training set, in which there exists minimal spurious correlations, and a second step in which a classifier is trained on the encodings generated for the biased training set. We leverage recent developments in flow-based modelling\nto ensure preservation of all class-relevant information. Using our proposed schema, we show success in decorrelating the spurious and semantic features on both datasets."}}
