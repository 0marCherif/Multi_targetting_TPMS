{"id": "t5fvApGjDP", "cdate": 1672531200000, "mdate": 1687184099760, "content": {"title": "Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs", "abstract": "Colexification in comparative linguistics refers to the phenomenon of a lexical form conveying two or more distinct meanings. In this paper, we propose simple and effective methods to build multilingual graphs from colexification patterns: ColexNet and ColexNet+. ColexNet's nodes are concepts and its edges are colexifications. In ColexNet+, concept nodes are in addition linked through intermediate nodes, each representing an ngram in one of 1,334 languages. We use ColexNet+ to train high-quality multilingual embeddings $\\overrightarrow{\\mbox{ColexNet+}}$ that are well-suited for transfer learning scenarios. Existing work on colexification patterns relies on annotated word lists. This limits scalability and usefulness in NLP. In contrast, we identify colexification patterns of more than 2,000 concepts across 1,335 languages directly from an unannotated parallel corpus. In our experiments, we first show that ColexNet has a high recall on CLICS, a dataset of crosslingual colexifications. We then evaluate $\\overrightarrow{\\mbox{ColexNet+}}$ on roundtrip translation, verse retrieval and verse classification and show that our embeddings surpass several baselines in a transfer learning setting. This demonstrates the benefits of colexification for multilingual NLP."}}
{"id": "nGZ3lNHzf6B", "cdate": 1672531200000, "mdate": 1687184099728, "content": {"title": "A study of conceptual language similarity: comparison and evaluation", "abstract": "An interesting line of research in natural language processing (NLP) aims to incorporate linguistic typology to bridge linguistic diversity and assist the research of low-resource languages. While most works construct linguistic similarity measures based on lexical or typological features, such as word order and verbal inflection, recent work has introduced a novel approach to defining language similarity based on how they represent basic concepts, which is complementary to existing similarity measures. In this work, we study the conceptual similarity in detail and evaluate it extensively on a binary classification task."}}
{"id": "dQitDeijTPO", "cdate": 1672531200000, "mdate": 1687184099696, "content": {"title": "On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss", "abstract": "Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find this issue is closely related to an unexpected copying behavior during online back-translation (BT). In this work, we propose a simple but effective training schedule that incorporates a language discriminator loss. The loss imposes constraints on the intermediate translation so that the translation is in the desired language. By conducting extensive experiments on different language pairs, including similar and distant, high and low-resource languages, we find that our method alleviates the copying problem, thus improving the translation performance on low-resource languages."}}
{"id": "DgS4uXPV6x", "cdate": 1672531200000, "mdate": 1687184099685, "content": {"title": "A Crosslingual Investigation of Conceptualization in 1335 Languages", "abstract": "Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for `belly' and `womb'. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all languages for one concept (`bird') and an evaluation on gold standard data for 32 Swadesh concepts, we show that Conceptualizer has good alignment accuracy. We demonstrate the potential of research on conceptualization in NLP with two experiments. (1) We define crosslingual stability of a concept as the degree to which it has 1-1 correspondences across languages, and show that concreteness predicts stability. (2) We represent each language by its conceptualization pattern for 83 concepts, and define a similarity measure on these representations. The resulting measure for the conceptual similarity of two languages is complementary to standard genealogical, typological, and surface similarity measures. For four out of six language families, we can assign languages to their correct family based on conceptual similarity with accuracy between 54% and 87%."}}
{"id": "b_El6FUY8eb", "cdate": 1640995200000, "mdate": 1671659569208, "content": {"title": "Flow-Adapter Architecture for Unsupervised Machine Translation", "abstract": ""}}
{"id": "KcG9jMUvhzw", "cdate": 1609459200000, "mdate": 1671659569206, "content": {"title": "A label-oriented loss function for learning sentence representations", "abstract": ""}}
