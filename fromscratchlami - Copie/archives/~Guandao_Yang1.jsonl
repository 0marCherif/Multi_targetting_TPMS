{"id": "cFGOnjaPdS_", "cdate": 1669158595841, "mdate": 1669158595841, "content": {"title": "Learning Gradient Fields for Shape Generation", "abstract": "In this work, we propose a novel technique to generate shapes from point cloud data. A point cloud can be viewed as samples from a distribution of 3D points whose density is concentrated near the surface of the shape. Point cloud generation thus amounts to moving randomly sampled points to high-density areas. We generate point clouds by performing stochastic gradient ascent on an unnormalized probability density, thereby moving sampled points toward the high-likelihood regions. Our model directly predicts the gradient of the log density field and can be trained with a simple objective adapted from score-based generative models. We show that our method can reach state-of-the-art performance for point cloud auto-encoding and generation, while also allowing for extraction of a high-quality implicit surface. Code is available at https://github.com/RuojinCai/ShapeGF."}}
{"id": "juE5ErmZB61", "cdate": 1652737275126, "mdate": null, "content": {"title": "Polynomial Neural Fields for Subband Decomposition and Manipulation", "abstract": "Neural fields have emerged as a new paradigm for representing signals, thanks to their ability to do it compactly while being easy to optimize. In most applications, however, neural fields are treated like a black box, which precludes many signal manipulation tasks. In this paper, we propose a new class of neural fields called basis-encoded polynomial neural fields (PNFs). The key advantage of a PNF is that it can represent a signal as a composition of a number of manipulable and interpretable components without losing the merits of neural fields representation. We develop a general theoretical framework to analyze and design PNFs. We use this framework to design Fourier PNFs, which match state-of-the-art performance in signal representation tasks that use neural fields. In addition, we empirically demonstrate that Fourier PNFs enable signal manipulation applications such as texture transfer and scale-space interpolation. Code is available at https://github.com/stevenygd/PNF."}}
{"id": "JG-SlCAx5_K", "cdate": 1621629675820, "mdate": null, "content": {"title": "Geometry Processing with Neural Fields", "abstract": "Most existing geometry processing algorithms use meshes as the default shape representation.  Manipulating meshes, however, requires one to maintain high quality in the surface discretization.  For example, changing the topology of a mesh usually requires additional procedures such as remeshing. This paper instead proposes the use of neural fields for geometry processing. Neural fields can compactly store complicated shapes without spatial discretization.   Moreover, neural fields are infinitely differentiable, which allows them to be optimized for objectives that involve higher-order derivatives.  This raises the question: can geometry processing be done entirely using neural fields? We introduce loss functions and architectures to show that some of the most challenging geometry processing tasks, such as deformation and filtering, can be done with neural fields. Experimental results show that our methods are on par with the well-established mesh-based methods without committing to a particular surface discretization. Code is available at https://github.com/stevenygd/NFGP."}}
{"id": "mwtZ_xPbw", "cdate": 1580417019882, "mdate": null, "content": {"title": "PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows", "abstract": "As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds,generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions.  Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape.  Our generative model, named PointFlow,learns each level of the distribution with a continuous normalizing flow.  The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework.  Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner"}}
{"id": "HJRV1ZZAW", "cdate": 1518730170416, "mdate": null, "content": {"title": "FAST READING COMPREHENSION WITH CONVNETS", "abstract": "State-of-the-art deep reading comprehension models are dominated by recurrent\nneural nets. Their sequential nature is a natural fit for language, but it also precludes\nparallelization within an instances and often becomes the bottleneck for\ndeploying such models to latency critical scenarios. This is particularly problematic\nfor longer texts. Here we present a convolutional architecture as an alternative\nto these recurrent architectures. Using simple dilated convolutional units in place\nof recurrent ones, we achieve results comparable to the state of the art on two\nquestion answering tasks, while at the same time achieving up to two orders of\nmagnitude speedups for question answering."}}
{"id": "HyWCec-_Zr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep Fundamental Matrix Estimation Without Correspondences", "abstract": "Estimating fundamental matrices is a classic problem in computer vision. Traditional methods rely heavily on the correctness of estimated key-point correspondences, which can be noisy and unreliable. As a result, it is difficult for these methods to handle image pairs with large occlusion or significantly different camera poses. In this paper, we propose novel neural network architectures to estimate fundamental matrices in an end-to-end manner without relying on point correspondences. New modules and layers are introduced in order to preserve mathematical properties of the fundamental matrix as a homogeneous rank-2 matrix with seven degrees of freedom. We analyze performance of the proposed models using various metrics on the KITTI dataset, and show that they achieve competitive performance with traditional methods without the need for extracting correspondences."}}
{"id": "BkWP5YWuWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Single-View 3D Reconstruction with Limited Pose Supervision", "abstract": "It is expensive to label images with 3D structure or precise camera pose. Yet, this is precisely the kind of annotation required to train single-view 3D reconstruction models. In contrast, unlabeled images or images with just category labels are easy to acquire, but few current models can use this weak supervision. We present a unified framework that can combine both types of supervision: a small amount of camera pose annotations are used to enforce pose-invariance and view-point consistency, and unlabeled images combined with an adversarial loss are used to enforce the realism of rendered, generated models. We use this unified framework to measure the impact of each form of supervision in three paradigms: semi-supervised, multi-task, and transfer learning. We show that with a combination of these ideas, we can train single-view reconstruction models that improve up\u00a0to 7 points in performance (AP) when using only 1% pose annotated training data."}}
