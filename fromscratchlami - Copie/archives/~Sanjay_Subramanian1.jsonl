{"id": "EByiF8Svm1", "cdate": 1684073235867, "mdate": 1684073235867, "content": {"title": "AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models", "abstract": "Neural NLP models are increasingly accu- rate but are imperfect and opaque\u2014they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpreta- tion methods ameliorate this opacity by pro- viding explanations for specific model pre- dictions. Unfortunately, existing interpreta- tion codebases make it difficult to apply these methods to new models and tasks, which hin- ders adoption for practitioners and burdens in- terpretability researchers. We introduce Al- lenNLP Interpret, a flexible framework for in- terpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit\u2019s flexibility and util- ity by implementing live demos for five in- terpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension us- ing BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp. org/interpret."}}
{"id": "Yz2rPlgte5", "cdate": 1672531200000, "mdate": 1688239592694, "content": {"title": "Modular Visual Question Answering via Code Generation", "abstract": "We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by roughly 2% compared to the few-shot baseline that does not employ code generation."}}
{"id": "GFmdGzJpnM", "cdate": 1668558717534, "mdate": 1668558717534, "content": {"title": "ReCLIP: A Strong Zero-shot Baseline for Referring Expression Comprehension", "abstract": "Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong \\emph{zero-shot} baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. We reduce the gap between zero-shot baselines from prior work and supervised  models by as much as 29\\% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8\\%."}}
{"id": "GHKGwPnvFO0", "cdate": 1609459200000, "mdate": 1636025847063, "content": {"title": "Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering", "abstract": "Answering questions that involve multi-step reasoning requires decomposing them and using the answers of intermediate steps to reach the final answer. However, state-of-the-art models in grounded question answering often do not explicitly perform decomposition, leading to difficulties in generalization to out-of-distribution examples. In this work, we propose a model that computes a representation and denotation for all question spans in a bottom-up, compositional manner using a CKY-style parser. Our model induces latent trees, driven by end-to-end (the answer) supervision only. We show that this inductive bias towards tree structures dramatically improves systematic generalization to out-of-distribution examples, compared to strong baselines on an arithmetic expressions benchmark as well as on CLOSURE, a dataset that focuses on systematic generalization for grounded question answering. On this challenging dataset, our model reaches an accuracy of 96.1%, significantly higher than prior models that almost perfectly solve the task on a random, in-distribution split."}}
{"id": "yRF9A-EQ_P", "cdate": 1577836800000, "mdate": 1631292473734, "content": {"title": "MedICaT: A Dataset of Medical Images, Captions, and Textual References", "abstract": "Sanjay Subramanian, Lucy Lu Wang, Ben Bogin, Sachin Mehta, Madeleine van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
{"id": "skVr-e7T90x", "cdate": 1577836800000, "mdate": 1636025846905, "content": {"title": "Obtaining Faithful Interpretations from Compositional Neural Networks", "abstract": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "ItFcIqKqEfS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Evaluating Models' Local Decision Boundaries via Contrast Sets", "abstract": "Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, Ben Zhou. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
{"id": "du0dvDgnHg", "cdate": 1546300800000, "mdate": null, "content": {"title": "Analyzing Compositionality in Visual Question Answering", "abstract": ""}}
{"id": "QfaaXpPgPX", "cdate": 1546300800000, "mdate": null, "content": {"title": "AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models", "abstract": "Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, Sameer Singh. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations. 2019."}}
{"id": "AvGVUN8srN", "cdate": 1546300800000, "mdate": 1636683188096, "content": {"title": "Improving Generalization in Coreference Resolution via Adversarial Training", "abstract": "Sanjay Subramanian, Dan Roth. Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019). 2019."}}
