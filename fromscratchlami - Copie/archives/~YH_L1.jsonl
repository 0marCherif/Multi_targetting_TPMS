{"id": "jWrrsrClPIi", "cdate": 1640995200000, "mdate": 1674971147841, "content": {"title": "Difference Advantage Estimation for Multi-Agent Policy Gradients", "abstract": "Multi-agent policy gradient methods in centralized training with decentralized execution recently witnessed many progresses. During centralized training, multi-agent credit assignment is crucial, w..."}}
{"id": "u0N3anjfj_W", "cdate": 1609459200000, "mdate": 1648723668640, "content": {"title": "Decentralized Circle Formation Control for Fish-like Robots in the Real-world via Reinforcement Learning", "abstract": "In this paper, the circle formation control problem is addressed for a group of cooperative underactuated fish-like robots involving unknown nonlinear dynamics and disturbances. Based on the reinforcement learning and cognitive consistency theory, we propose a decentralized controller without the knowledge of the dynamics of the fish-like robots. The proposed controller can be transferred from simulation to reality. It is only trained in our established simulation environment, and the trained controller can be deployed to real robots without any manual tuning. Simulation results confirm that the proposed model-free robust formation control method is scalable with respect to the group size of the robots and outperforms other representative RL algorithms. Several experiments in the real world verify the effectiveness of our RL-based approach for circle formation control."}}
{"id": "ezmnHnIMUWw", "cdate": 1609459200000, "mdate": 1648723668638, "content": {"title": "FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement Learning", "abstract": "Value decomposition recently injects vigorous vitality into multi-agent actor-critic methods. However, existing decomposed actor-critic methods cannot guarantee the convergence of global optimum. I..."}}
{"id": "ijVgDcvLmZ", "cdate": 1601308421882, "mdate": null, "content": {"title": "FSV: Learning to Factorize Soft Value Function for Cooperative Multi-Agent Reinforcement Learning", "abstract": "We explore energy-based solutions for cooperative multi-agent reinforcement learning (MARL) using the idea of function factorization in centralized training with decentralized execution (CTDE). Existing CTDE based factorization methods are susceptible to the relative overgeneralization, where finding a suboptimal Nash Equilibrium, which is a well-known game-theoretic pathology. To resolve this issue, we propose a novel factorization method for cooperative MARL, named FSV, which learns to factorize the joint soft value function into individual ones for decentralized execution. Theoretical analysis shows that FSV solves a rich class of factorization tasks. Our experiment for the well-known task of the Max of Two Quadratics game shows that FSV fully converges to global optima in the joint action space in the continuous tasks by local searching in the joint action space. We evaluate FSV on a challenging set of StarCraft II micromanagement tasks, and show that FSV significantly outperforms existing factorization multi-agent reinforcement learning methods."}}
