{"id": "bmv9wIpjrr", "cdate": 1640995200000, "mdate": 1666356644740, "content": {"title": "A Deep Q-Network for the Beer Game: Deep Reinforcement Learning for Inventory Optimization", "abstract": "Problem definition: The beer game is widely used in supply chain management classes to demonstrate the bullwhip effect and the importance of supply chain coordination. The game is a decentralized, ..."}}
{"id": "3FN7av140hw", "cdate": 1620393862834, "mdate": null, "content": {"title": "AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control", "abstract": "We propose AttendLight, an end-to-end Reinforcement Learning (RL) algorithm for the problem of traffic signal control. Previous approaches for this problem have the shortcoming that they require training for each new intersection with a different structure or traffic flow distribution. AttendLight solves this issue by training a single, universal model for intersections with any number of roads, lanes, phases (possible signals), and traffic flow. To this end, we propose a deep RL model which incorporates two attention models. The first attention model is introduced to handle different numbers of roads-lanes; and the second attention model is intended for enabling decision-making with any number of phases in an intersection. As a result, our proposed model works for any intersection configuration, as long as a similar configuration is represented in the training set. Experiments were conducted with both synthetic and real-world standard benchmark data-sets. The results we show cover intersections with three or four approaching roads; one-directional/bi-directional roads with one, two, and three lanes; different number of phases; and different traffic flows. We consider two regimes:(i) single-environment training, single-deployment, and (ii) multi-environment training, multi-deployment. AttendLight outperforms both classical and other RL-based approaches on all cases in both regimes."}}
{"id": "tgURTo2UJqJ", "cdate": 1612894602033, "mdate": null, "content": {"title": "Automated Learning of Radiation Therapy Dose Distribution for Head and Neck Cancer", "abstract": "In the past decades, 3D dose optimization, with the goal of providing a prescription dose to the tumor while maximally sparing surrounding healthy organs at risk (OARs), has been playing a pivotal role in external beam radiation therapy.  To reach a practical clinical treatment plan, the optimization solution needs to be adjusted by human planners. However, this procedure necessitates intensive computations combined with iterative inputs from an expert human planner.  This is a time-intensive process with dose distribution solutions that are highly dependent on the human expertise of an institution.\nTo remedy the computational time issue, and also reduce the variability, while capturing the existing knowledge, we use machine learning capabilities. In particular, we propose a suitable feature matrix for head and neck cancer and train Deep Neural Network models to mimic the Planned Dose Distribution procedure. When trained, the model can provide dose values in less than a second. Toward this, a specific Convolutional Neural Network called U-Net model is utilized. The numerical results show that this model reduces the solution time of each patient on the utilized data from 3-6 hours to 1 second while providing highly accurate solutions such that the average error is approximately 3.2\\% and 7.0\\% for cancerous organs and OARs, respectively."}}
{"id": "j6uXFpKG0R", "cdate": 1609459200000, "mdate": 1666356644771, "content": {"title": "Towards optimized actions in critical situations of soccer games with deep reinforcement learning", "abstract": "Soccer is a sparse rewarding game: any smart or careless action in critical situations can change the result of the match. Therefore players, coaches, and scouts are all curious about the best action to be performed in critical situations, such as the times with a high probability of losing ball possession or scoring a goal. This work proposes a new state representation for the soccer game and a batch reinforcement learning to train a smart policy network. This network gets the contextual information of the situation and proposes the optimal action to maximize the expected goal for the team. We performed extensive numerical experiments on the soccer logs made by InStat for 104 European soccer matches. The results show that in all 104 games, the optimized policy obtains higher rewards than its counterpart in the behavior policy. Besides, our framework learns policies that are close to the expected behavior in the real world. For instance, in the optimized policy, we observe that some actions such as foul, or ball out can be sometimes more rewarding than a shot in specific situations."}}
{"id": "84NIMQ-saE", "cdate": 1609459200000, "mdate": 1666356644784, "content": {"title": "Towards optimized actions in critical situations of soccer games with deep reinforcement learning", "abstract": "Soccer is a sparse rewarding game: any smart or careless action in critical situations can change the result of the match. Therefore players, coaches, and scouts are all curious about the best action to be performed in critical situations, such as the times with a high probability of losing ball possession or scoring a goal. This work proposes a new state representation for the soccer game and a batch reinforcement learning to train a smart policy network. This network gets the contextual information of the situation and proposes the optimal action to maximize the expected goal for the team. We performed extensive numerical experiments on the soccer logs made by InStat for 104 European soccer matches. The results show that in all 104 games, the optimized policy obtains higher rewards than its counterpart in the behavior policy. Besides, our framework learns policies that are close to the expected behavior in the real world. For instance, in the optimized policy, we observe that some actions such as foul, or ball out can be sometimes more rewarding than a shot in specific situations."}}
{"id": "fPFHzhfEAmm", "cdate": 1577836800000, "mdate": 1666356644763, "content": {"title": "AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control", "abstract": "We propose AttendLight, an end-to-end Reinforcement Learning (RL) algorithm for the problem of traffic signal control. Previous approaches for this problem have the shortcoming that they require training for each new intersection with a different structure or traffic flow distribution. AttendLight solves this issue by training a single, universal model for intersections with any number of roads, lanes, phases (possible signals), and traffic flow. To this end, we propose a deep RL model which incorporates two attention models. The first attention model is introduced to handle different numbers of roads-lanes; and the second attention model is intended for enabling decision-making with any number of phases in an intersection. As a result, our proposed model works for any intersection configuration, as long as a similar configuration is represented in the training set. Experiments were conducted with both synthetic and real-world standard benchmark data-sets. The results we show cover intersections with three or four approaching roads; one-directional/bi-directional roads with one, two, and three lanes; different number of phases; and different traffic flows. We consider two regimes: (i) single-environment training, single-deployment, and (ii) multi-environment training, multi-deployment. AttendLight outperforms both classical and other RL-based approaches on all cases in both regimes."}}
{"id": "SlsdzrVxGeu", "cdate": 1577836800000, "mdate": 1666356644760, "content": {"title": "AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control", "abstract": "We propose AttendLight, an end-to-end Reinforcement Learning (RL) algorithm for the problem of traffic signal control. Previous approaches for this problem have the shortcoming that they require training for each new intersection with a different structure or traffic flow distribution. AttendLight solves this issue by training a single, universal model for intersections with any number of roads, lanes, phases (possible signals), and traffic flow. To this end, we propose a deep RL model which incorporates two attention models. The first attention model is introduced to handle different numbers of roads-lanes; and the second attention model is intended for enabling decision-making with any number of phases in an intersection. As a result, our proposed model works for any intersection configuration, as long as a similar configuration is represented in the training set. Experiments were conducted with both synthetic and real-world standard benchmark datasets. Our numerical experiment covers intersections with three or four approaching roads; one-directional/bi-directional roads with one, two, and three lanes; different number of phases; and different traffic flows. We consider two regimes: (i) single-environment training, single-deployment, and (ii) multi-environment training, multi-deployment. AttendLight outperforms both classical and other RL-based approaches on all cases in both regimes."}}
{"id": "RTomo3bco6", "cdate": 1577836800000, "mdate": 1666356644763, "content": {"title": "Applying deep learning to the newsvendor problem", "abstract": "The newsvendor problem is one of the most basic and widely applied inventory models. If the probability distribution of the demand is known, the problem can be solved analytically. However, approxi..."}}
{"id": "sLn5XmYkhAr", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Review of Cooperative Multi-Agent Deep Reinforcement Learning", "abstract": "Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. In this review article, we have focused on presenting recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. In particular, we have focused on five common approaches on modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critic, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. First, we elaborate on each of these methods, possible challenges, and how these challenges were mitigated in the relevant papers. If applicable, we further make a connection among different papers in each category. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. Due to the recent success of MARL in real-world applications, we assign a section to provide a review of these applications and corresponding articles. Also, a list of available environments for MARL research is provided in this survey. Finally, the paper is concluded with proposals on the possible research directions."}}
{"id": "IrS6X9cT74", "cdate": 1514764800000, "mdate": 1666356644760, "content": {"title": "Deep Reinforcement Learning for Solving the Vehicle Routing Problem", "abstract": "We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single model that finds near-optimal solutions for problem instances sampled from a given distribution, only by observing the reward signals and following feasibility rules. Our model represents a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems."}}
