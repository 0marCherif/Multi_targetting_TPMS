{"id": "UZS8dFsF4t", "cdate": 1675970198763, "mdate": null, "content": {"title": "Self-Supervised Learning with Lie Symmetries for Partial Differential Equations", "abstract": "Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. Data augmentation is central to SSL:  although simple augmentation strategies such as cropping provide satisfactory results, our inclusion of transformations corresponding to the symmetry group of a given PDE significantly improves the quality of the learned representations. "}}
{"id": "B1stEa78wdL", "cdate": 1672531200000, "mdate": 1681739323730, "content": {"title": "Toeplitz Low-Rank Approximation with Sublinear Query Complexity", "abstract": "We present a sublinear query algorithm for outputting a near-optimal low-rank approximation to any positive semidefinite Toeplitz matrix T \u2208 \u211dd\u00d7d. In particular, for any integer rank k \u2264 d and \u03b5, \u03b4 > 0, our algorithm makes \u00d5 (k2 \u00b7 log(1/\u03b4) \u00b7 poly(1/\u03b5)) queries to the entries of T and outputs a rank \u00d5 (k \u00b7 log(1/\u03b4)/\u03b5) matrix d\u00d7d such that ||T \u2013 ||F \u2264 (1 + \u03b5) \u00b7 ||T - Tk ||F + \u03b4||\u03a4||F. Here, || \u00b7 ||F is the Frobenius norm and Tk is the optimal rank-k approximation to T, given by projection onto its top k eigenvectors. \u00d5(\u00b7) hides polylog(d) factors. Our algorithm is structure-preserving, in that the approximation is also Toeplitz. A key technical contribution is a proof that any positive semidefinite Toeplitz matrix in fact has a near-optimal low-rank approximation which is itself Toeplitz. Surprisingly, this basic existence result was not previously known. Building on this result, along with the well-established off-grid Fourier structure of Toeplitz matrices [Cybenko'82], we show that Toeplitz with near optimal error can be recovered with a small number of random queries via a leverage-score-based off-grid sparse Fourier sampling scheme."}}
{"id": "1G_dwmEKX-w", "cdate": 1664194171837, "mdate": null, "content": {"title": "Barron's Theorem for Equivariant Networks", "abstract": "The incorporation of known symmetries in a learning task provides a powerful inductive bias, reducing the sample complexity of learning equivariant functions in both theory and practice. Group-symmetric architectures for equivariant deep learning are now widespread, as are accompanying universality results that verify their representational power. However, these  symmetric approximation theorems suffer from the same major drawback as their original non-symmetric counterparts: namely, they may require impractically large networks. In this work, we demonstrate that for some commonly used groups, there exist smooth subclasses of functions -- analogous to Barron classes of functions -- which can be efficiently approximated using invariant architectures. In particular, for permutation subgroups, there exist invariant approximating architectures whose sizes, while dependent on the precise orbit structure of the function, are in many cases just as small as the non-invariant architectures given by Barron's Theorem. For the rotation group, we define an invariant approximating architecture with a new invariant nonlinearity, which may be of independent practical interest, that is similarly just as small as its non-invariant counterparts. Overall, we view this work as a first step towards capturing the smoothness of invariant functions in invariant universal approximation, thereby providing approximation results that are not only invariant, but efficient."}}
{"id": "99RpBVpLiX", "cdate": 1663850041203, "mdate": null, "content": {"title": "Distilling Model Failures as Directions in Latent Space", "abstract": "Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes."}}
{"id": "TERVhuQVTe", "cdate": 1652737589839, "mdate": null, "content": {"title": "GULP: a prediction-based metric between representations", "abstract": "Comparing the representations learned by different neural networks has recently emerged as a key tool to understand various architectures and ultimately optimize them. In this work, we introduce GULP, a family of distance measures between representations that is explicitly motivated by  downstream predictive tasks. By construction, GULP provides uniform control over the difference in prediction performance between two representations, with respect to regularized linear prediction tasks. Moreover, it satisfies several desirable structural properties, such as the triangle inequality and invariance under orthogonal transformations, and thus lends itself to data embedding and visualization. We extensively evaluate GULP relative to other methods, and demonstrate that it correctly differentiates between architecture families, converges over the course of training, and captures generalization performance on downstream linear tasks. "}}
{"id": "dA0tguU-28o", "cdate": 1640995200000, "mdate": 1681739323781, "content": {"title": "Toeplitz Low-Rank Approximation with Sublinear Query Complexity", "abstract": "We present a sublinear query algorithm for outputting a near-optimal low-rank approximation to any positive semidefinite Toeplitz matrix $T \\in \\mathbb{R}^{d \\times d}$. In particular, for any integer rank $k \\leq d$ and $\\epsilon,\\delta > 0$, our algorithm makes $\\tilde{O} \\left (k^2 \\cdot \\log(1/\\delta) \\cdot \\text{poly}(1/\\epsilon) \\right )$ queries to the entries of $T$ and outputs a rank $\\tilde{O} \\left (k \\cdot \\log(1/\\delta)/\\epsilon\\right )$ matrix $\\tilde{T} \\in \\mathbb{R}^{d \\times d}$ such that $\\| T - \\tilde{T}\\|_F \\leq (1+\\epsilon) \\cdot \\|T-T_k\\|_F + \\delta \\|T\\|_F$. Here, $\\|\\cdot\\|_F$ is the Frobenius norm and $T_k$ is the optimal rank-$k$ approximation to $T$, given by projection onto its top $k$ eigenvectors. $\\tilde{O}(\\cdot)$ hides $\\text{polylog}(d) $ factors. Our algorithm is \\emph{structure-preserving}, in that the approximation $\\tilde{T}$ is also Toeplitz. A key technical contribution is a proof that any positive semidefinite Toeplitz matrix in fact has a near-optimal low-rank approximation which is itself Toeplitz. Surprisingly, this basic existence result was not previously known. Building on this result, along with the well-established off-grid Fourier structure of Toeplitz matrices [Cybenko'82], we show that Toeplitz $\\tilde{T}$ with near optimal error can be recovered with a small number of random queries via a leverage-score-based off-grid sparse Fourier sampling scheme."}}
{"id": "aHS3qMA9GJ", "cdate": 1640995200000, "mdate": 1668716085242, "content": {"title": "Distilling Model Failures as Directions in Latent Space", "abstract": "Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes. Code available at https://github.com/MadryLab/failure-directions"}}
{"id": "PabeDOzFUU", "cdate": 1640995200000, "mdate": 1681745925791, "content": {"title": "Implicit Bias of Linear Equivariant Networks", "abstract": "Group equivariant convolutional neural networks (G-CNNs) are generalizations of convolutional neural networks (CNNs) which excel in a wide range of technical applications by explicitly encoding sym..."}}
{"id": "MHdfYxxNPa", "cdate": 1640995200000, "mdate": 1682635391717, "content": {"title": "GULP: a prediction-based metric between representations", "abstract": "Comparing the representations learned by different neural networks has recently emerged as a key tool to understand various architectures and ultimately optimize them. In this work, we introduce GULP, a family of distance measures between representations that is explicitly motivated by downstream predictive tasks. By construction, GULP provides uniform control over the difference in prediction performance between two representations, with respect to regularized linear prediction tasks. Moreover, it satisfies several desirable structural properties, such as the triangle inequality and invariance under orthogonal transformations, and thus lends itself to data embedding and visualization. We extensively evaluate GULP relative to other methods, and demonstrate that it correctly differentiates between architecture families, converges over the course of training, and captures generalization performance on downstream linear tasks."}}
{"id": "fvybrRLv4m", "cdate": 1632875670037, "mdate": null, "content": {"title": "Dictionary Learning Under Generative Coefficient Priors with Applications to Compression", "abstract": "There is a rich literature on recovering data from limited measurements under the assumption of sparsity in some basis, whether known (compressed sensing) or unknown (dictionary learning). In particular, classical dictionary learning assumes the given dataset is well-described by sparse combinations of an unknown basis set. However, this assumption is of limited validity on real-world data. Recent work spanning theory and computational science has sought to replace the canonical sparsity assumption with more complex data priors, demonstrating how to incorporate pretrained generative models into frameworks such as compressed sensing and phase retrieval. Typically, the dimensionality of the input space of the generative model is much smaller than that of the output space, paralleling the \u201clow description complexity,\u201d or compressibility, of sparse vectors. In this paper, we study dictionary learning under this kind of known generative prior on the coefficients, which may capture non-trivial low-dimensional structure in the coefficients. This is a distributional learning approach to compression, in which we learn a suitable dictionary given access to a small dataset of training instances and a specified generative model for the coefficients. Equivalently, it may be viewed as transfer learning for generative models, in which we learn a new linear layer (the dictionary) to fine-tune a pretrained generative model (the coefficient prior) on a new dataset. We give, to our knowledge, the first provable algorithm for recovering the unknown dictionary given a suitable initialization. Finally, we compare our approach to traditional dictionary learning algorithms on synthetic compression and denoising tasks, demonstrating empirically the advantages of incorporating finer-grained structure than sparsity."}}
