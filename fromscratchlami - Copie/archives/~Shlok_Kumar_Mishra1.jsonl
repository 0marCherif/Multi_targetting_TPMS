{"id": "P4Xr7J9rxp", "cdate": 1674998119530, "mdate": 1674998119530, "content": {"title": "A simple, efficient and scalable contrastive masked autoencoder for learning visual representations", "abstract": "We introduce CAN, a simple, efficient and scalable method for self-supervised\nlearning of visual representations. Our framework is a minimal and conceptually\nclean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the\nnoise prediction approach used in diffusion models. The learning mechanisms are\ncomplementary to one another: contrastive learning shapes the embedding space\nacross a batch of image samples; masked autoencoders focus on reconstruction of\nthe low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image.\nThe combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with 50% of patches in both views\nbeing masked at random, yielding a considerable efficiency improvement over\nprior contrastive learning methods. Extensive empirical studies demonstrate that\nCAN achieves strong downstream performance under both linear and finetuning\nevaluations on transfer learning and robustness tasks. CAN outperforms MAE and\nSimCLR when pre-training on ImageNet, but is especially useful for pre-training\non larger uncurated datasets such as JFT-300M: for linear probe on ImageNet,\nCAN achieves 75.4% compared to 73.4% for SimCLR and 64.1% for MAE. The\nfinetuned performance on ImageNet of our ViT-L model is 86.1%, compared to\n85.5% for SimCLR, and 85.4% for MAE. The overall FLOPs load of SimCLR is\n70% higher than CAN for ViT-L models1"}}
{"id": "7kpmIkHVpHu", "cdate": 1663850033554, "mdate": null, "content": {"title": "Hyperbolic Contrastive Learning for Visual Representations beyond Objects", "abstract": " Although self-/un-supervised methods have led to rapid progress in visual representation learning, these methods generally treat objects and scenes using the same lens. In this paper, we focus on learning representations for objects and scenes \\that preserve the structure among them.\n Motivated by the observation that visually similar objects are close in the representation space, we argue that the scenes and objects should instead follow a hierarchical structure based on their compositionality. To exploit such a structure, we propose a contrastive learning framework where a Euclidean loss is used to learn object representations and a hyperbolic loss is used to encourage representations of scenes to lie close to representations of their constituent objects in a hyperbolic space. This novel hyperbolic objective encourages the scene-object hypernymy among the representations by optimizing the magnitude of their norms. We show that when pretraining on the COCO and OpenImages datasets, the hyperbolic loss improves downstream performance of several baselines across multiple datasets and tasks, including image classification, object detection, and semantic segmentation. We also show that the properties of the learned representations allow us to solve various vision tasks that involve the interaction between scenes and objects in a zero-shot way."}}
{"id": "qmV_tOHp7B9", "cdate": 1663849838805, "mdate": null, "content": {"title": "CAN: A simple, efficient and scalable contrastive masked autoencoder framework for learning visual representations", "abstract": "We introduce CAN, a simple, efficient and scalable method for self-supervised learning of visual representations. Our framework is a minimal and conceptually clean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the noise prediction approach used in diffusion models. The learning mechanisms are \\emph{complementary} to one another: contrastive learning shapes the embedding space across a batch of image samples; masked autoencoders focus on reconstruction of the low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image. The combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with $50\\%$ of patches in \\emph{both views} being masked at random, yielding a considerable efficiency improvement over prior contrastive learning methods. Extensive empirical studies on linear evaluation, finetuning, transfer learning, and robustness demonstrate that our approach achieves strong downstream performance. For instance, when pre-training ViT-B encoders on the curated ImageNet dataset, CAN achieves $74.8\\%$ top-1 linear probing accuracy, an absolute improvement of $6.8\\%$ over MAE and $1.3\\%$ over SimCLR with the same architecture and data augmentations. CAN is especially useful for pre-training on larger uncurated datasets such as JFT-300M: the finetuned performance on ImageNet of our ViT-L model is $85.9\\%$, compared to $85.0\\%$ for SimCLR, and $85.4\\%$ for MAE. For linear probe on ImageNet, CAN achieves $75.4\\%$ compared to $71.8\\%$ for SimCLR and $64.1\\%$ for MAE. The overall FLOPs load is $41\\%$ \\emph{lower} than SimCLR\\footnote{Our code will be released at \\url{www.xxx.yyy}.}. "}}
{"id": "QUyasQGv1Nl", "cdate": 1652737296427, "mdate": null, "content": {"title": "Hyperbolic Contrastive Learning for Visual Representations beyond Objects", "abstract": "Despite the rapid progress in visual representation learning driven by self-/un-supervised methods, both objects and scenes have been primarily treated using the same lens. In this paper, we focus on learning representations for objects and scenes explicitly in the same space. Motivated by the observation that visually similar objects are close in the representation space, we argue that the scenes and objects should further follow a hierarchical structure based on their compositionality. To exploit such a structure, we propose a contrastive learning framework where a Euclidean loss is used to learn object representations and a hyperbolic loss is used to regularize scene representations according to the hierarchy. This novel hyperbolic objective encourages the scene-object hypernymy among the representations by optimizing the magnitude of their norms. We show that when pretraining on the COCO and OpenImages datasets, the hyperbolic loss improves downstream performance across multiple datasets and tasks, including image classification, object detection, and semantic segmentation. We also show that the properties of the learned representations allow us to solve various vision tasks that involve the interaction between scenes and objects in a zero-shot way."}}
{"id": "kbTeUQabKDy", "cdate": 1652713564203, "mdate": 1652713564203, "content": {"title": "OBJECT-AWARE CROPPING FOR SELF-SUPERVISED LEARNING", "abstract": "A core component of the recent success of self-supervised learning is cropping\ndata augmentation, which selects sub-regions of an image to be used as positive\nviews in the self-supervised loss. The underlying assumption is that randomly\ncropped and resized regions of a given image share information about the objects of\ninterest, which the learned representation will capture. This assumption is mostly\nsatisfied in datasets such as ImageNet where there is a large, centered object, which\nis highly likely to be present in random crops of the full image. However, in other\ndatasets such as OpenImages or COCO, which are more representative of real world\nuncurated data, there are typically multiple small objects in an image. In this work,\nwe show that self-supervised learning based on the usual random cropping performs\npoorly on such datasets. We propose replacing one or both of the random crops\nwith crops obtained from an object proposal algorithm. This encourages the model\nto learn both object and scene level semantic representations. Using this approach,\nwhich we call object-aware cropping, results in significant improvements over\nscene cropping on classification and object detection benchmarks. For example, on\nOpenImages, our approach achieves an improvement of 8.8% mAP over random\nscene-level cropping using MoCo-v2 based pre-training. We also show significant\nimprovements on COCO and PASCAL-VOC object detection and segmentation\ntasks over the state-of-the-art self-supervised learning approaches. Our approach\nis efficient, simple and general, and can be used in most existing contrastive and\nnon-contrastive self-supervised learning frameworks."}}
{"id": "9Bv60BRxg4H", "cdate": 1652713344605, "mdate": 1652713344605, "content": {"title": "Robust Contrastive Learning Using Negative Samples with Diminished Semantics Authors", "abstract": "Unsupervised learning has recently made exceptional progress because of the development of more effective contrastive learning methods. However, CNNs are prone\nto depend on low-level features that humans deem non-semantic. This dependency\nhas been conjectured to induce a lack of robustness to image perturbations or\ndomain shift. In this paper, we show that by generating carefully designed negative\nsamples, contrastive learning can learn more robust representations with less dependence on such features. Contrastive learning utilizes positive pairs that preserve\nsemantic information while perturbing superficial features in the training images.\nSimilarly, we propose to generate negative samples in a reversed way, where only\nthe superfluous instead of the semantic features are preserved. We develop two\nmethods, texture-based and patch-based augmentations, to generate negative samples. These samples achieve better generalization, especially under out-of-domain\nsettings. We also analyze our method and the generated texture-based samples,\nshowing that texture features are indispensable in classifying particular ImageNet\nclasses and especially finer classes. We also show that model bias favors texture\nand shape features differently under different test settings. Our code, trained\nmodels, and ImageNet-Texture dataset can be found at https://github.com/\nSongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives."}}
{"id": "3XcEQTRyxhp", "cdate": 1632875448548, "mdate": null, "content": {"title": "Object-Aware Cropping for Self-Supervised Learning", "abstract": "A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised loss. The underlying assumption is that randomly cropped and resized regions of a given image share information about the objects of interest, which the learned representation will capture. This assumption is mostly satisfied in datasets such as ImageNet where there is a large, centered object, which is highly likely to be present in random crops of the full image. However, in other datasets such as OpenImages or COCO, which are more representative of real world uncurated data, there are typically multiple small objects in an image. In this work, we show that self-supervised learning based on the usual random cropping performs poorly on such datasets. We propose replacing one or both of the random crops with crops obtained from an object proposal algorithm. This encourages the model to learn both object and scene level semantic representations. Using this approach, which we call object-aware cropping, results in significant improvements over scene cropping on classification and object detection benchmarks. For example, on OpenImages, our approach achieves an improvement of 8.8% mAP over random scene-level cropping using MoCo-v2 based pre-training. We also show significant improvements on COCO and PASCAL-VOC object detection and segmentation tasks over the state-of-the-art self-supervised learning approaches.\nOur approach is efficient, simple and general, and can be used in most existing contrastive and non-contrastive self-supervised learning frameworks. "}}
{"id": "xLExSzfIDmo", "cdate": 1621629888275, "mdate": null, "content": {"title": "Robust Contrastive Learning Using Negative Samples with Diminished Semantics", "abstract": "Unsupervised learning has recently made exceptional progress because of the development of more effective contrastive learning methods. However, CNNs are prone to depend on low-level features that humans deem non-semantic. This dependency has been conjectured to induce a lack of robustness to image perturbations or domain shift. In this paper, we show that by generating carefully designed negative samples, contrastive learning can learn more robust representations with less dependence on such features. Contrastive learning utilizes positive pairs which preserve semantic information while perturbing superficial features in the training images. Similarly, we propose to generate negative samples in a reversed way, where only the superfluous instead of the semantic features are preserved. We develop two methods, texture-based and patch-based augmentations, to generate negative samples.  These samples achieve better generalization, especially under out-of-domain settings. We also analyze our method and the generated texture-based samples, showing that texture features are indispensable in classifying particular ImageNet classes and especially finer classes. We also show that the model bias between texture and shape features favors them differently under different test settings. "}}
{"id": "pY5-A_UR0QU", "cdate": 1609459200000, "mdate": 1633551081586, "content": {"title": "Improved Detection of Face Presentation Attacks Using Image Decomposition", "abstract": "Presentation attack detection (PAD) is a critical component in secure face authentication. We present a PAD algorithm to distinguish face spoofs generated by a photograph of a subject from live images. Our method uses an image decomposition network to extract albedo and normal. The domain gap between the real and spoof face images leads to easily identifiable differences, especially between the recovered albedo maps. We enhance this domain gap by retraining existing methods using supervised contrastive loss. We present empirical and theoretical analysis that demonstrates that the contrast and lighting effects can play a significant role in PAD; these show up particularly in the recovered albedo. Finally, we demonstrate that by combining all of these methods we achieve state-of-the-art results on datasets such as CelebA-Spoof, OULU and CASIA-SURF."}}
{"id": "xrUySgB5ZOK", "cdate": 1601308203391, "mdate": null, "content": {"title": "Learning Visual Representations for Transfer Learning by Suppressing Texture", "abstract": "Recent works have shown that features obtained from supervised training of CNNs may over-emphasize texture rather than encoding high-level information.  In self-supervised learning, in particular, texture as a low-level cue may provide shortcuts that prevent the network from learning higher-level representations.  To address these problems we propose to use classic methods based on anisotropic diffusion to augment training using images with suppressed texture. This simple method helps retain important edge information and suppress texture at the same time. \nWe report our observations for fully supervised and self-supervised learning tasks like MoCoV2 and Jigsaw and achieve state-of-the-art results on object detection and image classification with eight diverse datasets. \nOur method is particularly effective for transfer learning tasks and we observed improved performance on five standard transfer learning datasets. \nThe large improvements on the Sketch-ImageNet dataset, DTD dataset and\nadditional visual analyses of saliency maps suggest that our approach helps in learning better representations that transfer well."}}
