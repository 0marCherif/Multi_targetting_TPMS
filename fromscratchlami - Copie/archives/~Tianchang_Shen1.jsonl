{"id": "Ha8PXZ5E7n", "cdate": 1698572146811, "mdate": 1698572146811, "content": {"title": "Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes", "abstract": "Reconstruction and intrinsic decomposition of scenes from captured imagery would enable many applications such as relighting and virtual object insertion. Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but bake the lighting and shadows into the radiance field, while mesh-based methods that facilitate intrinsic decomposition through differentiable rendering have not yet scaled to the complexity and scale of outdoor scenes. We present a novel inverse rendering framework for large urban scenes capable of jointly reconstructing the scene geometry, spatially-varying materials, and HDR lighting from a set of posed RGB images with optional depth. Specifically, we use a neural field to account for the primary rays, and use an explicit mesh (reconstructed from the underlying neural field) for modeling secondary rays that produce higher-order lighting effects such as cast shadows. By faithfully disentangling complex geometry and materials from lighting effects, our method enables photorealistic relighting with specular and shadow effects on several outdoor datasets. Moreover, it supports physics-based scene manipulations such as virtual object insertion with ray-traced shadow casting."}}
{"id": "GAUwreODU5L", "cdate": 1652737326487, "mdate": null, "content": {"title": "GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images", "abstract": "As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods."}}
{"id": "_DeBRYW84MG", "cdate": 1640995200000, "mdate": 1668157934614, "content": {"title": "Extracting Triangular 3D Models, Materials, and Lighting From Images", "abstract": ""}}
{"id": "FYp_6rd6Bm", "cdate": 1640995200000, "mdate": 1668157934588, "content": {"title": "GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images", "abstract": ""}}
{"id": "xN3XX6pKSD5", "cdate": 1621629871315, "mdate": null, "content": {"title": "Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis", "abstract": "We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/."}}
{"id": "GFV8IVyMM4n", "cdate": 1621629871315, "mdate": null, "content": {"title": "Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis", "abstract": "We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/."}}
{"id": "LftymtKflPd", "cdate": 1609459200000, "mdate": 1668157934652, "content": {"title": "Zeus: A system description of the two-time winner of the collegiate SAE autodrive competition", "abstract": ""}}
{"id": "HrSF6hMh1D", "cdate": 1609459200000, "mdate": 1668157934626, "content": {"title": "Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis", "abstract": ""}}
{"id": "qn6JbPai0wF", "cdate": 1577836800000, "mdate": 1668157934591, "content": {"title": "Interactive Annotation of 3D Object Geometry Using 2D Scribbles", "abstract": ""}}
