{"id": "VE8QRTrWAMb", "cdate": 1652737556919, "mdate": null, "content": {"title": "Near-Optimal Regret for Adversarial MDP with Delayed Bandit Feedback", "abstract": "The standard assumption in reinforcement learning (RL) is that agents observe feedback for their actions immediately. However, in practice feedback is often observed in delay. This paper studies online learning in episodic Markov decision process (MDP) with unknown transitions, adversarially changing costs, and unrestricted delayed bandit feedback. More precisely, the feedback for the agent in episode $k$ is revealed only in the end of episode $k + d^k$, where the delay $d^k$ can be changing over episodes and chosen by an oblivious adversary. We present the first algorithms that achieve near-optimal $\\sqrt{K + D}$ regret, where $K$ is the number of episodes and $D = \\sum_{k=1}^K d^k$ is the total delay, significantly improving upon the best known regret bound of $(K + D)^{2/3}$."}}
{"id": "gbgZVL_q8Lj", "cdate": 1640995200000, "mdate": 1652993365833, "content": {"title": "Near-Optimal Regret for Adversarial MDP with Delayed Bandit Feedback", "abstract": "The standard assumption in reinforcement learning (RL) is that agents observe feedback for their actions immediately. However, in practice feedback is often observed in delay. This paper studies online learning in episodic Markov decision process (MDP) with unknown transitions, adversarially changing costs, and unrestricted delayed bandit feedback. More precisely, the feedback for the agent in episode $k$ is revealed only in the end of episode $k + d^k$, where the delay $d^k$ can be changing over episodes and chosen by an oblivious adversary. We present the first algorithms that achieve near-optimal $\\sqrt{K + D}$ regret, where $K$ is the number of episodes and $D = \\sum_{k=1}^K d^k$ is the total delay, significantly improving upon the best known regret bound of $(K + D)^{2/3}$."}}
{"id": "ScqOg_wbVp", "cdate": 1640995200000, "mdate": 1652993365830, "content": {"title": "Cooperative Online Learning in Stochastic and Adversarial MDPs", "abstract": "We study cooperative online learning in stochastic and adversarial Markov decision process (MDP). That is, in each episode, $m$ agents interact with an MDP simultaneously and share information in order to minimize their individual regret. We consider environments with two types of randomness: \\emph{fresh} -- where each agent's trajectory is sampled i.i.d, and \\emph{non-fresh} -- where the realization is shared by all agents (but each agent's trajectory is also affected by its own actions). More precisely, with non-fresh randomness the realization of every cost and transition is fixed at the start of each episode, and agents that take the same action in the same state at the same time observe the same cost and next state. We thoroughly analyze all relevant settings, highlight the challenges and differences between the models, and prove nearly-matching regret lower and upper bounds. To our knowledge, we are the first to consider cooperative reinforcement learning (RL) with either non-fresh randomness or in adversarial MDPs."}}
{"id": "V6r1W6QbpVa", "cdate": 1609459200000, "mdate": 1652993365842, "content": {"title": "Stochastic Multi-Armed Bandits with Unrestricted Delay Distributions", "abstract": "We study the stochastic Multi-Armed Bandit (MAB) problem with random delays in the feedback received by the algorithm. We consider two settings: the reward-dependent delay setting, where realized delays may depend on the stochastic rewards, and the reward-independent delay setting. Our main contribution is algorithms that achieve near-optimal regret in each of the settings, with an additional additive dependence on the quantiles of the delay distribution. Our results do not make any assumptions on the delay distributions: in particular, we do not assume they come from any parametric family of distributions and allow for unbounded support and expectation; we further allow for infinite delays where the algorithm might occasionally not observe any feedback."}}
{"id": "0jrOjXaG05", "cdate": 1609459200000, "mdate": 1652993365841, "content": {"title": "Stochastic Multi-Armed Bandits with Unrestricted Delay Distributions", "abstract": "We study the stochastic Multi-Armed Bandit\u00a0(MAB) problem with random delays in the feedback received by the algorithm. We consider two settings: the {\\it reward dependent} delay setting, where real..."}}
{"id": "6-VmPMwA5Qv", "cdate": 1577836800000, "mdate": 1652993365835, "content": {"title": "Learning Adversarial Markov Decision Processes with Delayed Feedback", "abstract": "Reinforcement learning typically assumes that agents observe feedback for their actions immediately, but in many real-world applications (like recommendation systems) feedback is observed in delay. This paper studies online learning in episodic Markov decision processes (MDPs) with unknown transitions, adversarially changing costs and unrestricted delayed feedback. That is, the costs and trajectory of episode $k$ are revealed to the learner only in the end of episode $k + d^k$, where the delays $d^k$ are neither identical nor bounded, and are chosen by an oblivious adversary. We present novel algorithms based on policy optimization that achieve near-optimal high-probability regret of $\\sqrt{K + D}$ under full-information feedback, where $K$ is the number of episodes and $D = \\sum_{k} d^k$ is the total delay. Under bandit feedback, we prove similar $\\sqrt{K + D}$ regret assuming the costs are stochastic, and $(K + D)^{2/3}$ regret in the general case. We are the first to consider regret minimization in the important setting of MDPs with delayed feedback."}}
