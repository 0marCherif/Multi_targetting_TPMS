{"id": "U4BUMoVTrB2", "cdate": 1652737798970, "mdate": null, "content": {"title": "DOPE: Doubly Optimistic and Pessimistic Exploration for Safe Reinforcement Learning", "abstract": "Safe reinforcement learning is extremely challenging--not only must the agent explore an unknown environment, it must do so while ensuring no safety constraint violations. We formulate this safe  reinforcement learning (RL) problem using the framework of a finite-horizon Constrained Markov Decision Process (CMDP) with an unknown transition probability function, where we model the safety requirements as constraints on the expected cumulative costs that must be satisfied during all episodes of learning.  We propose a model-based safe RL algorithm that we call Doubly Optimistic and Pessimistic Exploration (DOPE), and show that it achieves an objective regret $\\tilde{O}(|\\mathcal{S}|\\sqrt{|\\mathcal{A}| K})$ without violating the safety constraints during learning, where  $|\\mathcal{S}|$ is the number of states, $|\\mathcal{A}|$ is the number of actions, and $K$ is the number of learning episodes.  Our key idea is to combine a reward bonus for exploration (optimism) with a conservative constraint (pessimism), in addition to the standard optimistic model-based exploration.  DOPE is not only able to improve the objective regret bound, but also shows a significant empirical performance improvement as compared to earlier optimism-pessimism approaches. "}}
{"id": "pEfpvOWlT1e", "cdate": 1640995200000, "mdate": 1659140070950, "content": {"title": "QFlow: A Learning Approach to High QoE Video Streaming at the Wireless Edge", "abstract": "The predominant use of wireless access networks is for media streaming applications. However, current access networks treat all packets identically, and lack the agility to determine which clients are most in need of service at a given time. Software reconfigurability of networking devices has seen wide adoption, and this in turn implies that agile control policies can be now instantiated on access networks. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to develop QFlow, a platform that instantiates this feedback loop, and instantiate a variety of control policies over it. We use the popular application of video streaming over YouTube as our use case. Our context is priority queueing, with the action space being that of determining which clients should be assigned to each queue at each decision period. We first develop policies based on model-based and model-free reinforcement learning. We then design an auction-based system under which clients place bids for priority service, as well as a more structured index-based policy. Through experiments, we show how these learning-based policies on QFlow are able to select the right clients for prioritization in a high-load scenario to outperform the best known solutions with over 25&#x0025; improvement in QoE, and a perfect QoE score of 5 over 85&#x0025; of the time."}}
{"id": "UtA_L4Ooaug", "cdate": 1640995200000, "mdate": 1659140070949, "content": {"title": "Learning to Cache and Caching to Learn: Regret Analysis of Caching Algorithms", "abstract": "Crucial performance metrics of a caching algorithm include its ability to quickly and accurately learn a popularity distribution of requests. However, a majority of work on analytical performance analysis focuses on hit probability after an asymptotically large time has elapsed. We consider an online learning viewpoint, and characterize the &#x201C;regret&#x201D; in terms of the finite time difference between the hits achieved by a candidate caching algorithm with respect to a genie-aided scheme that places the most popular items in the cache. We first consider the Full Observation regime wherein all requests are seen by the cache. We show that the Least Frequently Used (LFU) algorithm is able to achieve order optimal regret, which is matched by an efficient counting algorithm design that we call LFU-Lite. We then consider the Partial Observation regime wherein only requests for items currently cached are seen by the cache, making it similar to an online learning problem related to the multi-armed bandit problem. We show how approaching this &#x201C;caching bandit&#x201D; using traditional approaches yields either high complexity or regret, but a simple algorithm design that exploits the structure of the distribution can ensure order optimal regret. We conclude by illustrating our insights using numerical simulations."}}
{"id": "k1zH06cgr3", "cdate": 1609459200000, "mdate": 1670520249278, "content": {"title": "Safe Exploration for Constrained Reinforcement Learning with Provable Guarantees", "abstract": "Safe reinforcement learning is extremely challenging--not only must the agent explore an unknown environment, it must do so while ensuring no safety constraint violations. We formulate this safe reinforcement learning (RL) problem using the framework of a finite-horizon Constrained Markov Decision Process (CMDP) with an unknown transition probability function, where we model the safety requirements as constraints on the expected cumulative costs that must be satisfied during all episodes of learning. We propose a model-based safe RL algorithm that we call Doubly Optimistic and Pessimistic Exploration (DOPE), and show that it achieves an objective regret $\\tilde{O}(|\\mathcal{S}|\\sqrt{|\\mathcal{A}| K})$ without violating the safety constraints during learning, where $|\\mathcal{S}|$ is the number of states, $|\\mathcal{A}|$ is the number of actions, and $K$ is the number of learning episodes. Our key idea is to combine a reward bonus for exploration (optimism) with a conservative constraint (pessimism), in addition to the standard optimistic model-based exploration. DOPE is not only able to improve the objective regret bound, but also shows a significant empirical performance improvement as compared to earlier optimism-pessimism approaches."}}
{"id": "M9sqT8HE5J", "cdate": 1609459200000, "mdate": 1670520249271, "content": {"title": "Learning with Safety Constraints: Sample Complexity of Reinforcement Learning for Constrained MDPs", "abstract": "Many physical systems have underlying safety considerations that require that the policy employed ensures the satisfaction of a set of constraints. The analytical formulation usually takes the form of a Constrained Markov Decision Process (CMDP). We focus on the case where the CMDP is unknown, and RL algorithms obtain samples to discover the model and compute an optimal constrained policy. Our goal is to characterize the relationship between safety constraints and the number of samples needed to ensure a desired level of accuracy---both objective maximization and constraint satisfaction---in a PAC sense. We explore two classes of RL algorithms, namely, (i) a generative model based approach, wherein samples are taken initially to estimate a model, and (ii) an online approach, wherein the model is updated as samples are obtained. Our main finding is that compared to the best known bounds of the unconstrained regime, the sample complexity of constrained RL algorithms are increased by a factor that is logarithmic in the number of constraints, which suggests that the approach may be easily utilized in real systems."}}
{"id": "zHvO_-SCNAsH", "cdate": 1577836800000, "mdate": 1659140070949, "content": {"title": "Learning to Cache and Caching to Learn: Regret Analysis of Caching Algorithms", "abstract": "Crucial performance metrics of a caching algorithm include its ability to quickly and accurately learn a popularity distribution of requests. However, a majority of work on analytical performance analysis focuses on hit probability after an asymptotically large time has elapsed. We consider an online learning viewpoint, and characterize the \"regret\" in terms of the finite time difference between the hits achieved by a candidate caching algorithm with respect to a genie-aided scheme that places the most popular items in the cache. We first consider the Full Observation regime wherein all requests are seen by the cache. We show that the Least Frequently Used (LFU) algorithm is able to achieve order optimal regret, which is matched by an efficient counting algorithm design that we call LFU-Lite. We then consider the Partial Observation regime wherein only requests for items currently cached are seen by the cache, making it similar to an online learning problem related to the multi-armed bandit problem. We show how approaching this \"caching bandit\" using traditional approaches yields either high complexity or regret, but a simple algorithm design that exploits the structure of the distribution can ensure order optimal regret. We conclude by illustrating our insights using numerical simulations."}}
{"id": "nr4-jr-L5Mf", "cdate": 1546300800000, "mdate": 1659140070956, "content": {"title": "QFlow: A Reinforcement Learning Approach to High QoE Video Streaming over Wireless Networks", "abstract": "The predominant use of wireless access networks is for media streaming applications, which are only gaining popularity as ever more devices become available for this purpose. However, current access networks treat all packets identically, and lack the agility to determine which clients are most in need of service at a given time. Software reconfigurability of networking devices has seen wide adoption, and this in turn implies that agile control policies can be now instantiated on access networks. The goal of this work is to design, develop and demonstrate QFlow, a learning approach to create a value chain from the application on one side, to algorithms operating over reconfigurable infrastructure on the other, so that applications are able to obtain necessary resources for optimal performance. Using YouTube video streaming as an example, we illustrate how QFlow is able to adaptively provide such resources and attain a high QoE for all clients at a wireless access point."}}
{"id": "Wa5Fj01w-1", "cdate": 1546300800000, "mdate": 1659140070950, "content": {"title": "QFlow: A Reinforcement Learning Approach to High QoE Video Streaming over Wireless Networks", "abstract": "Wireless Internet access has brought legions of heterogeneous applications all sharing the same resources. However, current wireless edge networks that cater to worst or average case performance lack the agility to best serve these diverse sessions. Simultaneously, software reconfigurable infrastructure has become increasingly mainstream to the point that dynamic per packet and per flow decisions are possible at multiple layers of the communications stack. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to design, develop and demonstrate QFlow that instantiates this feedback loop as an application of reinforcement learning (RL). Our context is that of reconfigurable (priority) queueing, and we use the popular application of video streaming as our use case. We develop both model-free and model-based RL approaches that are tailored to the problem of determining which clients should be assigned to which queue at each decision period. Through experimental validation, we show how the RL-based control policies on QFlow are able to schedule the right clients for prioritization in a high-load scenario to outperform the status quo, as well as the best known solutions with over 25% improvement in QoE, and a perfect QoE score of 5 over 85% of the time."}}
{"id": "6CKQm05BOo", "cdate": 1483228800000, "mdate": 1670520249327, "content": {"title": "Latency analysis for distributed storage", "abstract": ""}}
{"id": "btvFI-OcnW3", "cdate": 1451606400000, "mdate": 1670520249287, "content": {"title": "Throughput of TCP over Cognitive Radio Channels", "abstract": "In this paper, we study the performance of a TCP connection over cognitive radio networks. In these networks, the network may not always be available for transmission. Also, the packets can be lost due to wireless channel impairments. We evaluate the throughput and packet retransmission timeout probability of a secondary TCP connection over an ON/OFF channel. We first assume that the ON and OFF time durations are exponential and later extend it to more general distributions. We then consider multiple TCP connections over the ON/OFF channel. We validate our theoretical models and the approximations made therein via ns2 simulations."}}
