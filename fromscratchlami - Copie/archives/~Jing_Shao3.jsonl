{"id": "f_yTeb1v-GW", "cdate": 1663850133692, "mdate": null, "content": {"title": "Siamese DETR", "abstract": "Recent self-supervised methods are mainly designed for representation learning with the base model, e.g., ResNets or ViTs. They cannot be easily transferred to DETR, especially the task-specific module Transformer. In this work, we present Siamese DETR, a Siamese self-supervised pretraining approach for the Transformer architecture in DETR. We consider learning view-invariant and detection-oriented representations simultaneously through two complementary tasks, i.e., localization and discrimination, in a novel multi-view learning framework. Two self-supervised pretext tasks are designed: (a) Multi-View Region Detection aims at learning to localize regions-of-interest between augmented views of the input, and (b) Multi-View Semantic Discrimination attempts to improve object-level discrimination for each region. The proposed Siamese DETR achieves state-of-the-art transfer performance on COCO and PASCAL VOC detection using different DETR variants in all setups. Code will be made available."}}
{"id": "wAEizNH9jJ6", "cdate": 1653595781373, "mdate": null, "content": {"title": "Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision", "abstract": "Contrastive Language-Image Pretraining (CLIP) has emerged as a novel paradigm to learn visual models from language supervision.  While researchers continue to push the frontier of CLIP, reproducing these works remains challenging. This is because researchers do not choose consistent training recipes and even use different data, hampering the fair comparison between different methods. In this work, we propose CLIP-benchmark, a first attempt to evaluate, analyze, and benchmark CLIP and its variants.  We conduct a comprehensive analysis of three key factors: data, supervision, and model architecture. We find considerable intuitive or counter-intuitive insights:  (1). Data quality has a significant impact on performance.  (2). Certain supervision has different effects for Convolutional Networks (ConvNets) and Vision Transformers (ViT).  Applying more proper supervision can effectively improve the performance of CLIP. (3). Curtailing the text encoder reduces the training cost but not much affect the final performance. Moreover, we further combine DeCLIP with FILIP, bringing us the strongest variant DeFILIP. The CLIP\u0002benchmark is released at: https://github.com/Sense-GVT/DeCLIP for future CLIP research."}}
{"id": "uRTW_PgXvc7", "cdate": 1652737456752, "mdate": null, "content": {"title": "ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning", "abstract": "Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (e.g., image understanding) of the pre-trained model. This creates a limit because in some specific modalities, (e.g., video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small ~8% per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency."}}
{"id": "zq1iJkNk3uN", "cdate": 1632875485655, "mdate": null, "content": {"title": "Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image  Pre-training Paradigm", "abstract": "Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to downstream tasks. However, CLIP is quite data-hungry and requires 400M image-text pairs for pre-training, thereby restricting its adoption. This work proposes a novel training paradigm, Data efficient CLIP (DeCLIP), to alleviate this limitation. We demonstrate that by carefully utilizing the widespread supervision among the image-text pairs, our De-CLIP can learn generic visual features more efficiently. Instead of using the single image-text contrastive supervision, we fully exploit data potential through the use of (1) self-supervision within each modality; (2) multi-view supervision across modalities; (3) nearest-neighbor supervision from other similar pairs. Benefiting from intrinsic supervision, our DeCLIP-ResNet50 can achieve 60.4% zero-shot top1 accuracy on ImageNet, which is 0.8% above the CLIP-ResNet50 while using 7.1\u00d7fewer data. Our DeCLIP-ResNet50 outperforms its counterpart in 8 out of 11 visual datasets when transferred to downstream tasks. Moreover, Scaling up the model and computing also works well in our framework."}}
{"id": "aaXlpE8jDUP", "cdate": 1617676619222, "mdate": null, "content": {"title": "CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval", "abstract": "Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach."}}
{"id": "HyetFnEFDS", "cdate": 1569438849343, "mdate": null, "content": {"title": "Diving into Optimization of Topology in Neural Networks", "abstract": "Seeking\u00a0effective\u00a0networks\u00a0has\u00a0become\u00a0one\u00a0of\u00a0the\u00a0most\u00a0crucial\u00a0and\u00a0practical\u00a0areas\u00a0in\u00a0deep\u00a0learning. The\u00a0architecture\u00a0of\u00a0a\u00a0neural\u00a0network\u00a0can\u00a0be\u00a0represented\u00a0as\u00a0a\u00a0directed\u00a0acyclic\u00a0graph, whose\u00a0nodes\u00a0denote\u00a0transformation\u00a0of\u00a0layers\u00a0and\u00a0edges\u00a0represent\u00a0information\u00a0flow. Despite\u00a0the\u00a0selection\u00a0of\u00a0\\textit{micro}\u00a0node\u00a0operations,\u00a0\\textit{macro}\u00a0connections\u00a0among\u00a0the\u00a0whole network,\u00a0noted\u00a0as\u00a0\\textit{topology},\u00a0largely\u00a0affects\u00a0the\u00a0optimization\u00a0process. We\u00a0first\u00a0rethink\u00a0the\u00a0residual\u00a0connections\u00a0via\u00a0a\u00a0new\u00a0\\textit{topological\u00a0view}\u00a0and observe\u00a0the\u00a0benefits\u00a0provided\u00a0by\u00a0dense\u00a0connections\u00a0to\u00a0the\u00a0optimization. Motivated\u00a0by\u00a0which,\u00a0we\u00a0propose\u00a0an\u00a0innovation\u00a0method\u00a0to\u00a0optimize\u00a0the\u00a0topology\u00a0of a\u00a0neural\u00a0network. The\u00a0optimization\u00a0space\u00a0is\u00a0defined\u00a0as\u00a0a\u00a0complete\u00a0graph,\u00a0through\u00a0assigning\u00a0learnable\u00a0weights\u00a0which\u00a0reflect\u00a0the\u00a0importance\u00a0of\u00a0connections, the\u00a0optimization\u00a0of\u00a0topology\u00a0is\u00a0transformed\u00a0into\u00a0learning\u00a0a\u00a0set\u00a0of\u00a0 continuous\u00a0variables\u00a0of\u00a0edges. To\u00a0extend\u00a0the\u00a0optimization\u00a0to\u00a0larger\u00a0search\u00a0spaces,\u00a0a\u00a0new\u00a0series\u00a0of\u00a0networks,\nnamed\u00a0as\u00a0TopoNet,\u00a0are\u00a0designed. To\u00a0further\u00a0focus\u00a0on\u00a0critical\u00a0edges\u00a0and\u00a0promote\u00a0generalization\u00a0ablity\u00a0in\u00a0dense\u00a0topologies,\u00a0auxiliary\u00a0sparsity\u00a0constraint\u00a0is\u00a0adopted\u00a0to\u00a0constrain\u00a0the\u00a0distribution\u00a0of\u00a0edges. Experiments\u00a0on\u00a0classical\u00a0networks\u00a0prove\u00a0the\u00a0effectiveness\u00a0of\u00a0the\u00a0optimization\u00a0of\u00a0topology. Experiments\u00a0with\u00a0TopoNets\u00a0further\u00a0verify\u00a0both\u00a0availability\u00a0and\u00a0transferability\u00a0of\u00a0the\u00a0proposed\u00a0method\u00a0in\ndifferent\u00a0tasks\u00a0e.g.\u00a0image\u00a0classification,\u00a0object\u00a0detection\u00a0and\u00a0face\u00a0recognition."}}
{"id": "Ssob9g7lupr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Video Generation From Single Semantic Label Map.", "abstract": "This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between flexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difficult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the first frame. Then we animate the scene based on its semantic meaning to obtain temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical flow as a beneficial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the flow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods."}}
{"id": "HmvypzxuaH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Context and Attribute Grounded Dense Captioning.", "abstract": "Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to state-of-the-art methods."}}
{"id": "HQKNokQeuaH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Semantics Disentangling for Text-To-Image Generation.", "abstract": "Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods."}}
{"id": "BXAvzxXgdpr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing.", "abstract": "Referring expression grounding aims at locating certain objects or persons in an image with a referring expression, where the key challenge is to comprehend and align various types of information from visual and textual domain, such as visual attributes, location and interactions with surrounding regions. Although the attention mechanism has been successfully applied for cross-modal alignments, previous attention models focus on only the most dominant features of both modalities, and neglect the fact that there could be multiple comprehensive textual-visual correspondences between images and referring expressions. To tackle this issue, we design a novel cross-modal attention-guided erasing approach, where we discard the most dominant information from either textual or visual domains to generate difficult training samples online, and to drive the model to discover complementary textual-visual correspondences. Extensive experiments demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance on three referring expression grounding datasets."}}
