{"id": "UxqUgchwXkK", "cdate": 1663850540147, "mdate": null, "content": {"title": "Fast Nonlinear Vector Quantile Regression", "abstract": "$$\n\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n$$\nQuantile regression (QR) is a powerful tool for estimating one or more conditional quantiles of a target variable $\\rvar{Y}$ given explanatory features $\\rvec{X}$.\nA limitation of QR is that it is only defined for scalar target variables, due to the formulation of its objective function, and since the notion of quantiles has no standard definition for multivariate distributions.\nRecently, vector quantile regression (VQR) was proposed as an extension of QR for vector-valued target variables, thanks to a meaningful generalization of the notion of quantiles to multivariate distributions via optimal transport.\nDespite its elegance, VQR is arguably not applicable in practice due to several limitations:\n(i) it assumes a linear model for the quantiles of the target $\\rvec{Y}$ given the features $\\rvec{X}$;\n(ii) its exact formulation is intractable even for modestly-sized problems in terms of target dimensions, number of regressed quantile levels, or number of features, and its relaxed dual formulation may violate the monotonicity of the estimated quantiles;\n(iii) no fast or scalable solvers for VQR currently exist.\n\nIn this work we fully address these limitations, namely:\n(i) We extend VQR to the non-linear case, showing substantial improvement over linear VQR;\n(ii) We propose {vector monotone rearrangement}, a method which ensures the quantile functions estimated by VQR are monotone functions;\n(iii) We provide fast, GPU-accelerated solvers for linear and nonlinear VQR which maintain a fixed memory footprint, and demonstrate that they scale to millions of samples and thousands of quantile levels;\n(iv) We release an optimized python package of our solvers as to widespread the use of VQR in real-world applications."}}
{"id": "yXk83o735o", "cdate": 1663850362316, "mdate": null, "content": {"title": "Conformal Prediction is Robust to Label Noise", "abstract": "We study the robustness of conformal prediction\u2014a powerful tool for uncertainty quantification\u2014to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. Through stylized theoretical examples and practical experiments, we argue that na\u00efve conformal prediction covers the noiseless ground truth label unless the noise distribution is adversarially designed. This leads us to believe that correcting for label noise is unnecessary except for pathological data distributions or noise sources. In such cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure correct coverage of the ground truth labels without score or data regularity."}}
{"id": "uqLDy0HGPR7", "cdate": 1663850155317, "mdate": null, "content": {"title": "Risk Control for Online Learning Models", "abstract": "To provide rigorous uncertainty quantification for online learning models, we develop a framework for constructing uncertainty sets that provably control risk---such as coverage of confidence intervals, false negative rate, or F1 score---in the online setting. This extends conformal prediction to apply to a larger class of online learning problems. Our method guarantees risk control at any user-specified level even when the underlying data distribution shifts drastically, even adversarially, over time in an unknown fashion.\nThe technique we propose is highly flexible as it can be applied with any base online learning algorithm (e.g., a deep neural network trained online), requiring minimal implementation effort and essentially zero additional computational cost.\nWe further extend our approach to control multiple risks simultaneously, so the prediction sets we generate are valid for all given risks.\nTo demonstrate the utility of our method, we conduct experiments on real-world tabular time-series data sets showing that the proposed method rigorously controls various natural risks. \nFurthermore, we show how to construct valid intervals for an online image-depth estimation problem that previous sequential calibration schemes cannot handle."}}
{"id": "4ROZcrsCYP", "cdate": 1661437109176, "mdate": null, "content": {"title": "Semantic uncertainty intervals for disentangled latent spaces", "abstract": "Meaningful uncertainty quantification in computer vision requires reasoning about semantic information -- say, the hair color of the person in a photo or the location of a car on the street. To this end, recent breakthroughs in generative modeling allow us to represent semantic information in disentangled latent spaces, but providing uncertainties on the semantic latent variables has remained challenging. In this work, we provide principled uncertainty intervals that are guaranteed to contain the true semantic factors for any underlying generative model. The method does the following: (1) it uses quantile regression to output a heuristic uncertainty interval for each element in the latent space (2) calibrates these uncertainties such that they contain the true value of the latent for a new, unseen input. The endpoints of these calibrated intervals can then be propagated through the generator to produce interpretable uncertainty visualizations for each semantic factor. This technique reliably communicates semantically meaningful, principled, and instance-adaptive uncertainty in inverse problems like image super-resolution and image completion."}}
{"id": "6wLXvkHstNR", "cdate": 1652737746574, "mdate": null, "content": {"title": "Semantic uncertainty intervals for disentangled latent spaces", "abstract": "Meaningful uncertainty quantification in computer vision requires reasoning about semantic information---say, the hair color of the person in a photo or the location of a car on the street. To this end, recent breakthroughs in generative modeling allow us to represent semantic information in disentangled latent spaces, but providing uncertainties on the semantic latent variables has remained challenging. In this work, we provide principled uncertainty intervals that are guaranteed to contain the true semantic factors for any underlying generative model. The method does the following: (1) it uses quantile regression to output a heuristic uncertainty interval for each element in the latent space (2) calibrates these uncertainties such that they contain the true value of the latent for a new, unseen input. The endpoints of these calibrated intervals can then be propagated through the generator to produce interpretable uncertainty visualizations for each semantic factor. This technique reliably communicates semantically meaningful, principled, and instance-adaptive uncertainty in inverse problems like image super-resolution and image completion. Project page: https://swamiviv.github.io/semantic_uncertainty_intervals/"}}
{"id": "NaZwgxp-mT_", "cdate": 1652737373666, "mdate": null, "content": {"title": "Training Uncertainty-Aware Classifiers with Conformalized Deep Learning", "abstract": "Deep neural networks are powerful tools to detect hidden patterns in data and leverage them to make predictions, but they are not designed to understand uncertainty and estimate reliable probabilities. In particular, they tend to be overconfident. We begin to address this problem in the context of multi-class classification by developing a novel training algorithm producing models with more dependable uncertainty estimates, without sacrificing predictive power. The idea is to mitigate overconfidence by minimizing a loss function, inspired by advances in conformal inference, that quantifies model uncertainty by carefully leveraging hold-out data. Experiments with synthetic and real data demonstrate this method can lead to smaller conformal prediction sets with higher conditional coverage, after exact calibration with hold-out data, compared to state-of-the-art alternatives."}}
{"id": "6OWASA5P6Y", "cdate": 1652321565198, "mdate": 1652321565198, "content": {"title": "Conformal Prediction using Conditional Histograms", "abstract": "This paper develops a conformal method to compute prediction intervals for non-parametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the black-box model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches."}}
{"id": "744IweFl5cO", "cdate": 1652321527396, "mdate": 1652321527396, "content": {"title": "Classification with Valid and Adaptive Coverage", "abstract": "Conformal inference, cross-validation+, and the jackknife+ are hold-out methods that can be combined with virtually any machine learning algorithm to construct prediction sets with guaranteed marginal coverage. In this paper, we develop specialized versions of these techniques for categorical and unordered response labels that, in addition to providing marginal coverage, are also fully adaptive to complex data distributions, in the sense that they perform favorably in terms of approximate conditional coverage compared to alternative methods. The heart of our contribution is a novel conformity score, which we explicitly demonstrate to be powerful and intuitive for classification problems, but whose underlying principle is potentially far more general. Experiments on synthetic and real data demonstrate the practical value of our theoretical guarantees, as well as the statistical advantages of the proposed methods over the existing alternatives."}}
{"id": "9L1BsI4wP1H", "cdate": 1632875438341, "mdate": null, "content": {"title": "Adversarially Robust Conformal Prediction", "abstract": "Conformal prediction is a model-agnostic tool for constructing prediction sets that are valid under the common i.i.d. assumption, which has been applied to quantify the prediction uncertainty of deep net classifiers. In this paper, we generalize this framework to the case where adversaries exist during inference time, under which the i.i.d. assumption is grossly violated. By combining conformal prediction with randomized smoothing, our proposed method forms a prediction set with finite-sample coverage guarantee that holds for any data distribution with $\\ell_2$-norm bounded adversarial noise, generated by any adversarial attack algorithm. The core idea is to bound the Lipschitz constant of the non-conformity score by smoothing it with Gaussian noise and leverage this knowledge to account for the effect of the unknown adversarial perturbation. We demonstrate the necessity of our method in the adversarial setting and the validity of our theoretical guarantee on three widely used benchmark data sets: CIFAR10, CIFAR100, and ImageNet."}}
{"id": "lytmdv6BCOk", "cdate": 1623574415173, "mdate": 1623574415173, "content": {"title": "Searching for consistent associations with a multi-environment knockoff filter", "abstract": "This paper develops a method based on model-X knockoffs to find conditional associations that are consistent across diverse environments, controlling the false discovery rate. The motivation for this problem is that large data sets may contain numerous associations that are statistically significant and yet misleading, as they are induced by confounders or sampling imperfections. However, associations consistently replicated under different conditions may be more interesting. In fact, consistency sometimes provably leads to valid causal inferences even if conditional associations do not. While the proposed method is flexible and can be deployed in a wide range of applications, this paper highlights its relevance to genome-wide association studies, in which consistency across populations with diverse ancestries mitigates confounding due to unmeasured variants. The effectiveness of this approach is demonstrated by simulations and applications to the UK Biobank data."}}
