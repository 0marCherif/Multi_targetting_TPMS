{"id": "sb7hzqEtpe", "cdate": 1685532018266, "mdate": null, "content": {"title": "On Preemption and Learning in Stochastic Scheduling", "abstract": "We study single-machine scheduling of jobs, each belonging to a job type that determines its duration distribution. We start by analyzing the scenario where the type characteristics are known and then move to two learning scenarios where the types are unknown: non-preemptive problems, where each started job must be completed before moving to another job; and preemptive problems, where job execution can be paused in the favor of moving to a different job. In both cases, we design algorithms that achieve sublinear excess cost, compared to the performance with known types, and prove lower bounds for the non-preemptive case. Notably, we demonstrate, both theoretically and through simulations, how preemptive algorithms can greatly outperform non-preemptive ones when the durations of different job types are far from one another, a phenomenon that does not occur when the type durations are known."}}
{"id": "jaMEBCLv4rZ", "cdate": 1683906229553, "mdate": 1683906229553, "content": {"title": "Pure exploration and regret minimization in matching bandits", "abstract": "Finding an optimal matching in a weighted graph is a standard combinatorial problem. We consider its semi-bandit version where either a pair or a full matching is sampled sequentially. We prove that it is possible to leverage a rank-1 assumption on the adjacency matrix to reduce the sample complexity and the regret of off-the-shelf algorithms up to reaching a linear dependency in the number of vertices (up to to poly-log terms)."}}
{"id": "hQBqyrQwij", "cdate": 1683906162484, "mdate": 1683906162484, "content": {"title": "Robust estimation of discrete distributions under local differential privacy", "abstract": "Although robust learning and local differential privacy are both widely studied fields of research, combining the two settings is just starting to be explored. We consider the problem of estimating a discrete distribution in total variation from contaminated data batches under a local differential privacy constraint. A fraction of the batches contain iid samples drawn from a discrete distribution over elements. To protect the users\u2019 privacy, each of the samples is privatized using an -locally differentially private mechanism. The remaining batches are an adversarial contamination. The minimax rate of estimation under contamination alone, with no privacy, is known to be . Under the privacy constraint alone, the minimax rate of estimation is . We show, up to a factor, that combining the two constraints leads to a minimax estimation rate of , larger than the sum of the two separate rates. We provide a polynomial-time algorithm achieving this bound, as well as a matching information theoretic lower bound."}}
{"id": "TZ0eEqEBRA", "cdate": 1621630035437, "mdate": null, "content": {"title": "Online Matching in Sparse Random Graphs: Non-Asymptotic Performances of Greedy Algorithm", "abstract": "Motivated by sequential budgeted allocation problems, we investigate  online matching problems where connections between vertices are not i.i.d., but they have fixed degree distributions -- the so-called configuration model. We estimate the competitive ratio of the simplest algorithm, GREEDY, by approximating some relevant stochastic discrete processes by their continuous counterparts, that are solutions of an explicit system of partial differential equations. This technique gives precise bounds on the estimation  errors,  with arbitrarily high probability as the problem size increases. In particular, it allows the formal comparison between different configuration models. We also prove that, quite surprisingly,  GREEDY can have  better performance guarantees than RANKING, another celebrated algorithm for online matching that usually outperforms the former."}}
{"id": "BuoTowxp-9", "cdate": 1621629841592, "mdate": null, "content": {"title": "Decentralized Learning in Online Queuing Systems", "abstract": "Motivated by packet routing in computer networks, online queuing systems are composed of queues receiving packets at different rates. Repeatedly, they send packets to servers, each of them treating only at most one packet at a time. In the centralized case, the number of accumulated packets remains bounded (i.e., the system is stable) as long as the ratio between service rates and arrival rates is larger than $1$. In the decentralized case, individual no-regret strategies ensures stability when this ratio is larger than $2$. Yet, myopically minimizing  regret disregards the long term effects due to the carryover of packets to further rounds. On the other hand, minimizing long term costs leads to stable Nash equilibria as soon as the ratio exceeds $\\frac{e}{e-1}$. Stability with decentralized learning strategies  with a ratio below $2$ was a major remaining question. We first argue that for ratios up to $2$, cooperation is required for stability of learning strategies, as  selfish minimization of policy regret, a patient notion of regret, might indeed still be unstable in this case. We therefore consider cooperative queues and propose the first learning decentralized algorithm guaranteeing stability of the system as long as the ratio of rates is larger than $1$, thus reaching performances comparable to centralized strategies."}}
