{"id": "04OL67rm6ok", "cdate": 1663850297127, "mdate": null, "content": {"title": "QUIC-FL: : Quick Unbiased Compression for Federated Learning", "abstract": "Distributed Mean Estimation (DME) is a fundamental building block in communication efficient federated learning. In DME, clients communicate their lossily compressed gradients to the parameter server, which estimates the average and updates the model. \nState of the art DME techniques apply either unbiased quantization methods, resulting in large estimation errors, or biased quantization methods, where unbiasing the result requires that the server decodes each gradient individually, which markedly slows the aggregation time.\nIn this paper, we propose QUIC-FL, a DME algorithm that achieves the best of all worlds. QUIC-FL is unbiased, offers fast aggregation time, and is competitive with the most accurate (slow aggregation) DME techniques. To achieve this, we formalize the problem in a novel way that allows us to use standard solvers to design near-optimal unbiased quantization schemes."}}
{"id": "oBa91w2pde", "cdate": 1640995200000, "mdate": 1679906618417, "content": {"title": "SDR: Efficient Neural Re-ranking using Succinct Document Representation", "abstract": ""}}
{"id": "lTmJ1iAtM2i", "cdate": 1640995200000, "mdate": 1674492437056, "content": {"title": "EDEN: Communication-Efficient and Robust Distributed Mean Estimation for Federated Learning", "abstract": "Distributed Mean Estimation (DME) is a central building block in federated learning, where clients send local gradients to a parameter server for averaging and updating the model. Due to communicat..."}}
{"id": "KXRTmcv3dQ8", "cdate": 1621629929205, "mdate": null, "content": {"title": "DRIVE: One-bit Distributed Mean Estimation", "abstract": "We consider the problem where $n$ clients transmit $d$-dimensional real-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the receiver to approximately reconstruct their mean. Such compression problems naturally arise in distributed and federated learning. We provide novel mathematical results and derive computationally efficient algorithms that are more accurate than previous compression techniques.  We evaluate our methods on a collection of distributed and federated learning tasks, using a variety of datasets, and show a consistent improvement over the state of the art."}}
{"id": "gcBNtT_nsF", "cdate": 1609459200000, "mdate": 1674492437159, "content": {"title": "DRIVE: One-bit Distributed Mean Estimation", "abstract": "We consider the problem where $n$ clients transmit $d$-dimensional real-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the receiver to approximately reconstruct their mean. Such compression problems naturally arise in distributed and federated learning. We provide novel mathematical results and derive computationally efficient algorithms that are more accurate than previous compression techniques. We evaluate our methods on a collection of distributed and federated learning tasks, using a variety of datasets, and show a consistent improvement over the state of the art."}}
{"id": "Gm0ki2brezt", "cdate": 1420070400000, "mdate": 1679906618416, "content": {"title": "A generic decentralized trust management framework", "abstract": ""}}
