{"id": "I9JwcR97UAL", "cdate": 1672531200000, "mdate": 1696517618720, "content": {"title": "Image-based Treatment Effect Heterogeneity", "abstract": "Randomized controlled trials (RCTs) are considered the gold standard for estimating the Average Treatment Effect (ATE) of interventions. One important use of RCTs is to study the causes of global p..."}}
{"id": "Dkhvt7ICgs", "cdate": 1667393653979, "mdate": null, "content": {"title": "Image-based Treatment Effect Heterogeneity", "abstract": "Randomized controlled trials (RCTs) are considered the gold standard for estimating the Average Treatment Effect (ATE) of interventions. One important use of RCTs is to study the causes of global poverty---a subject explicitly cited in the 2019 Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel awarded to Duflo, Banerjee, and Kremer \u201cfor their experimental approach to alleviating global poverty.\u201d Because the ATE is a population summary, researchers often want to better understand how the treatment effect varies across different populations by conditioning on tabular variables such as age and ethnicity that were measured during the RCT data collection. Although such variables carry substantive importance, they are often only observed only near the time of the experiment: exclusive use of such variables may fail to capture historical, geographical, or neighborhood-specific contributors to effect variation. In global poverty research, when the geographical location of the experiment units is approximately known, satellite imagery can provide a window into such historical and geographical factors important for understanding heterogeneity. However, there is no causal inference method that specifically enables applied researchers to analyze Conditional Average Treatment Effects (CATEs) from images. In this paper, we develop a deep probabilistic modeling framework that identifies clusters of images with similar treatment effect distributions, enabling researchers to analyze treatment effect variation by image. Our interpretable image CATE model also emphasizes an image sensitivity factor that quantifies the importance of image segments in contributing to the mean effect cluster prediction. We compare the proposed methods against alternatives in simulation; additionally, we show how the model works in an actual RCT, estimating the effects of an anti-poverty intervention in northern Uganda and obtaining a posterior predictive distribution over treatment effects for the rest of the country where no experimental data was collected. We make code for all modeling strategies available in an open-source software package and discuss their applicability in other domains (such as the biomedical sciences) where image data are also prevalent."}}
{"id": "Y-sn95ozNHB", "cdate": 1653073281972, "mdate": 1653073281972, "content": {"title": "Linking Datasets on Organizations Using Half A Billion Open Collaborated Records", "abstract": "Scholars studying organizations would like to work with multiple datasets lacking shared\nunique identifiers or covariates. In such situations, researchers often turn to approximate\nstring matching methods to combine datasets. String matching, although useful, faces funda-\nmental challenges. String distance metrics are static, and do not explicitly maximize match\nprobabilities. Moreover, many entities have multiple names that are dissimilar (e.g., \u201cFannie\nMae\u201d and \u201cFederal National Mortgage Association\u201d). This paper proposes two approaches\nto leveraging the massive amount of human-collaborated data from an employment-related\nsocial networking site (LinkedIn) to address this problem. The first approach builds a\nmachine learning model for predicting matches, treating the trillion user-contributed orga-\nnizational name pairs as a training corpus. The second approach uses community detection\nand treats user records as a network. We document substantial improvements over fuzzy\nmatching in three organization name matching exercises. We make our methods available\nin an open-source R package (LinkOrgs)."}}
{"id": "Zn3_j0nSeGg", "cdate": 1653073172209, "mdate": 1653073172209, "content": {"title": "An Improved Method of Automated Nonparametric Content Analysis for Social Science", "abstract": "Some scholars build models to classify documents into chosen categories. Others,\nespecially social scientists who tend to focus on population characteristics, instead\nusually estimate the proportion of documents in each category \u2014 using either para-\nmetric \u201cclassify-and-count\u201d methods or \u201cdirect\u201d nonparametric estimation of propor-\ntions without individual classification. Unfortunately, classify-and-count methods\ncan be highly model dependent or generate more bias in the proportions even as the\npercent of documents correctly classified increases. Direct estimation avoids these\nproblems, but can suffer when the meaning of language changes between training\nand test sets or is too similar across categories. We develop an improved direct es-\ntimation approach without these issues by including and optimizing continuous text\nfeatures, along with a form of matching adapted from the causal inference literature.\nOur approach substantially improves performance in a diverse collection of 73 data\nsets. We also offer easy-to-use software that implements all ideas discussed herein"}}
{"id": "nhX97snseBK", "cdate": 1653073068742, "mdate": 1653073068742, "content": {"title": "Conceptualizing Treatment Leakage in Text-based Causal Inference", "abstract": "Causal inference methods that control for text- based confounders are becoming increasingly important in the social sciences and other dis- ciplines where text is readily available. How- ever, these methods rely on a critical assump- tion that there is no treatment leakage: that is, the text contains only information about the confounder and no information about treat- ment assignment (leading to post-treatment bias). However, this assumption may be un- realistic in real-world situations involving text, as human language is rich and flexible. We first define the leakage problem, discussing the identification and estimation challenges it raises. We also discuss the conditions under which leakage can be addressed by removing the treatment-related signal from the text in a pre-processing step we define as text distilla- tion. Then, using simulation, we investigate the mechanics of treatment leakage on esti- mates of the average treatment effect (ATE)"}}
