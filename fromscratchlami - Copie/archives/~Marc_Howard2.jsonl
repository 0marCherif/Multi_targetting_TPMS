{"id": "GWdFDRrqwv", "cdate": 1672531200000, "mdate": 1696014595889, "content": {"title": "Representing Latent Dimensions Using Compressed Number Lines", "abstract": "Humans use log-compressed number lines to represent different quantities, including elapsed time, traveled distance, numerosity, sound frequency, etc. Inspired by recent cognitive science and computational neuroscience work, we developed a neural network that learns to construct log-compressed number lines. The network computes a discrete approximation of a real-domain Laplace transform using an RNN with analytically derived weights giving rise to a log-compressed timeline of the past. The network learns to extract latent variables from the input and uses them for global modulation of the recurrent weights turning a timeline into a number line over relevant dimensions. The number line representation greatly simplifies learning on a set of problems that require learning associations in different spaces - problems that humans can typically solve easily. This approach illustrates how combining deep learning with cognitive models can result in systems that learn to represent latent variables in a brain-like manner and exhibit human-like behavior manifested through Weber-Fechner law."}}
{"id": "KJ8iuccbPB", "cdate": 1663850171316, "mdate": null, "content": {"title": "Representing Latent Dimensions Using Compressed Number Lines", "abstract": "Humans use log-compressed number lines to represent different quantities, including elapsed time, traveled distance, numerosity, sound frequency, etc. Inspired by recent cognitive science and computational neuroscience work, we developed a neural network that learns to construct log-compressed number lines. The network computes a discrete approximation of a real-domain Laplace transform using an RNN with analytically derived weights giving rise to a log-compressed timeline of the past. The network learns to extract latent variables from the input and uses them for global modulation of the recurrent weights turning a timeline into a number line over relevant dimensions. The number line representation greatly simplifies learning on a set of problems that require learning associations in different spaces - problems that humans can typically solve easily. This approach illustrates how combining deep learning with cognitive models can result in systems that learn to represent latent variables in a brain-like manner and exhibit human-like behavior manifested through Weber-Fechner law."}}
{"id": "aFIyOcq86q", "cdate": 1663105072588, "mdate": 1663105072588, "content": {"title": "Memory for time", "abstract": "The brain maintains a record of recent events including information about\nthe time at which events were experienced. We review behavioral and neurophysiological evidence as well as computational models to better understand\nmemory for time. Neurophysiologically, populations of neurons that record\nthe time of recent events have been observed in many brain regions. Time\ncells fire in long sequences after a triggering event demonstrating memory for\nthe past. Populations of exponentially-decaying neurons record past events\nat many delays by decaying at different rates. Both kinds of representations record distant times with less temporal resolution. The work reviewed\nhere converges on the idea that the brain maintains a representation of past\nevents along a scale-invariant compressed timeline."}}
{"id": "wukduxR2k1", "cdate": 1640995200000, "mdate": 1682358997186, "content": {"title": "A deep convolutional neural network that is invariant to time rescaling", "abstract": "Human learners can readily understand speech, or a melody, when it is presented slower or faster than usual. This paper presents a deep CNN (SITHCon) that uses a logarithmically compressed temporal..."}}
{"id": "Bo0Rum58hVr", "cdate": 1640995200000, "mdate": 1696014595891, "content": {"title": "Predicting the Future With a Scale-Invariant Temporal Memory for the Past", "abstract": "In recent years, it has become clear that the brain maintains a temporal memory of recent events stretching far into the past. This letter presents a neurally inspired algorithm to use a scale-invariant temporal representation of the past to predict a scale-invariant future. The result is a scale-invariant estimate of future events as a function of the time at which they are expected to occur. The algorithm is time-local, with credit assigned to the present event by observing how it affects the prediction of the future. To illustrate the potential utility of this approach, we test the model on simultaneous renewal processes with different timescales. The algorithm scales well on these problems despite the fact that the number of states needed to describe them as a Markov process grows exponentially."}}
{"id": "tn6vqNUJaEW", "cdate": 1621630021509, "mdate": null, "content": {"title": "DeepSITH: Efficient Learning via Decomposition of What and When Across Time Scales", "abstract": "Extracting temporal relationships over a range of scales is a hallmark of\nhuman perception and cognition---and thus it is a critical feature of machine\nlearning applied to real-world problems.  Neural networks are either plagued\nby the exploding/vanishing gradient problem in recurrent neural networks\n(RNNs) or must adjust their parameters to learn the relevant time scales\n(e.g., in LSTMs). This paper introduces DeepSITH, a deep network comprising\nbiologically-inspired Scale-Invariant Temporal History (SITH) modules in\nseries with dense connections between layers. Each SITH module is simply a\nset of time cells coding what happened when with a geometrically-spaced set of\ntime lags.  The dense connections between layers change the definition of what\nfrom one layer to the next.  The geometric series of time lags implies that\nthe network codes time on a logarithmic scale, enabling DeepSITH network to\nlearn problems requiring memory over a wide range of time scales. We compare\nDeepSITH to LSTMs and other recent RNNs on several time series prediction and\ndecoding tasks. DeepSITH achieves results comparable to state-of-the-art\nperformance on these problems and continues to perform well even as the delays\nare increased.\n"}}
{"id": "wdwfTkSQmw", "cdate": 1609459200000, "mdate": 1696014595944, "content": {"title": "Predicting the future with a scale-invariant temporal memory for the past", "abstract": "In recent years it has become clear that the brain maintains a temporal memory of recent events stretching far into the past. This paper presents a neurally-inspired algorithm to use a scale-invariant temporal representation of the past to predict a scale-invariant future. The result is a scale-invariant estimate of future events as a function of the time at which they are expected to occur. The algorithm is time-local, with credit assigned to the present event by observing how it affects the prediction of the future. To illustrate the potential utility of this approach, we test the model on simultaneous renewal processes with different time scales. The algorithm scales well on these problems despite the fact that the number of states needed to describe them as a Markov process grows exponentially."}}
{"id": "cdrEjT88kh-", "cdate": 1609459200000, "mdate": 1682358997295, "content": {"title": "DeepSITH: Efficient Learning via Decomposition of What and When Across Time Scales", "abstract": "Extracting temporal relationships over a range of scales is a hallmark ofhuman perception and cognition---and thus it is a critical feature of machinelearning applied to real-world problems. Neural networks are either plaguedby the exploding/vanishing gradient problem in recurrent neural networks(RNNs) or must adjust their parameters to learn the relevant time scales(e.g., in LSTMs). This paper introduces DeepSITH, a deep network comprisingbiologically-inspired Scale-Invariant Temporal History (SITH) modules inseries with dense connections between layers. Each SITH module is simply aset of time cells coding what happened when with a geometrically-spaced set oftime lags. The dense connections between layers change the definition of whatfrom one layer to the next. The geometric series of time lags implies thatthe network codes time on a logarithmic scale, enabling DeepSITH network tolearn problems requiring memory over a wide range of time scales. We compareDeepSITH to LSTMs and other recent RNNs on several time series prediction anddecoding tasks. DeepSITH achieves results comparable to state-of-the-artperformance on these problems and continues to perform well even as the delaysare increased."}}
{"id": "F-ChIPifK6", "cdate": 1609459200000, "mdate": 1682358997176, "content": {"title": "SITHCon: A neural network robust to variations in input scaling on the time dimension", "abstract": "Human learners can readily understand speech, or a melody, when it is presented slower or faster than usual. Although deep convolutional neural networks (CNNs) are extremely powerful in extracting information from time series, they require explicit training to generalize to different time scales. This paper presents a deep CNN that incorporates a temporal representation inspired by recent findings from neuroscience. In the mammalian brain, time is represented by populations of neurons with temporal receptive fields. Critically, the peaks of the receptive fields form a geometric series, such that the population codes a set of temporal basis functions over log time. Because memory for the recent past is a function of log time, rescaling the input results in translation of the memory. The Scale-Invariant Temporal History Convolution network (SITHCon) builds a convolutional layer over this logarithmically-distributed temporal memory. A max-pool operation results in a network that is invariant to rescalings of time modulo edge effects. We compare performance of SITHCon to a Temporal Convolution Network (TCN). Although both networks can learn classification and regression problems on both univariate and multivariate time series f(t), only SITHCon generalizes to rescalings f(at). This property, inspired by findings from contemporary neuroscience and consistent with findings from cognitive psychology, may enable networks that learn with fewer training examples, fewer weights and that generalize more robustly to out of sample data."}}
{"id": "D-HdSp8F6Y", "cdate": 1609459200000, "mdate": 1682358997204, "content": {"title": "DeepSITH: Efficient Learning via Decomposition of What and When Across Time Scales", "abstract": "Extracting temporal relationships over a range of scales is a hallmark of human perception and cognition -- and thus it is a critical feature of machine learning applied to real-world problems. Neural networks are either plagued by the exploding/vanishing gradient problem in recurrent neural networks (RNNs) or must adjust their parameters to learn the relevant time scales (e.g., in LSTMs). This paper introduces DeepSITH, a network comprising biologically-inspired Scale-Invariant Temporal History (SITH) modules in series with dense connections between layers. SITH modules respond to their inputs with a geometrically-spaced set of time constants, enabling the DeepSITH network to learn problems along a continuum of time-scales. We compare DeepSITH to LSTMs and other recent RNNs on several time series prediction and decoding tasks. DeepSITH achieves state-of-the-art performance on these problems."}}
