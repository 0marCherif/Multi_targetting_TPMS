{"id": "YM_6gYU0a7", "cdate": 1672531200000, "mdate": 1680275593707, "content": {"title": "Robust Mean Estimation Without a Mean: Dimension-Independent Error in Polynomial Time for Symmetric Distributions", "abstract": ""}}
{"id": "Oqs_JEh0Pm", "cdate": 1672531200000, "mdate": 1680275593759, "content": {"title": "Sparse PCA Beyond Covariance Thresholding", "abstract": ""}}
{"id": "KhY2JCHj6UI", "cdate": 1672531200000, "mdate": 1680275593740, "content": {"title": "Higher degree sum-of-squares relaxations robust against oblivious outliers", "abstract": ""}}
{"id": "DpVF-jIX3I4", "cdate": 1640995200000, "mdate": 1682413153678, "content": {"title": "Higher degree sum-of-squares relaxations robust against oblivious outliers", "abstract": "We consider estimation models of the form $Y=X^*+N$, where $X^*$ is some $m$-dimensional signal we wish to recover, and $N$ is symmetrically distributed noise that may be unbounded in all but a small $\\alpha$ fraction of the entries. We introduce a family of algorithms that under mild assumptions recover the signal $X^*$ in all estimation problems for which there exists a sum-of-squares algorithm that succeeds in recovering the signal $X^*$ when the noise $N$ is Gaussian. This essentially shows that it is enough to design a sum-of-squares algorithm for an estimation problem with Gaussian noise in order to get the algorithm that works with the symmetric noise model. Our framework extends far beyond previous results on symmetric noise models and is even robust to adversarial perturbations. As concrete examples, we investigate two problems for which no efficient algorithms were known to work for heavy-tailed noise: tensor PCA and sparse PCA. For the former, our algorithm recovers the principal component in polynomial time when the signal-to-noise ratio is at least $\\tilde{O}(n^{p/4}/\\alpha)$, that matches (up to logarithmic factors) current best known algorithmic guarantees for Gaussian noise. For the latter, our algorithm runs in quasipolynomial time and matches the state-of-the-art guarantees for quasipolynomial time algorithms in the case of Gaussian noise. Using a reduction from the planted clique problem, we provide evidence that the quasipolynomial time is likely to be necessary for sparse PCA with symmetric noise. In our proofs we use bounds on the covering numbers of sets of pseudo-expectations, which we obtain by certifying in sum-of-squares upper bounds on the Gaussian complexities of sets of solutions. This approach for bounding the covering numbers of sets of pseudo-expectations may be interesting in its own right and may find other application in future works."}}
{"id": "BaHth99Sp45", "cdate": 1621630045081, "mdate": null, "content": {"title": "Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers", "abstract": "We develop machinery to design efficiently computable and \\emph{consistent} estimators, achieving estimation error approaching zero as the number of observations grows, when facing an oblivious adversary that may corrupt responses in all but an $\\alpha$ fraction of the samples.\nAs concrete examples, we investigate two problems: \nsparse regression and principal component analysis (PCA).\nFor sparse regression, we achieve consistency for optimal sample size $n\\gtrsim (k\\log d)/\\alpha^2$ \nand optimal error rate $O(\\sqrt{(k\\log d)/(n\\cdot \\alpha^2)})$\nwhere $n$ is the number of observations, $d$ is the number of dimensions and $k$ is the sparsity of the parameter vector, allowing the fraction of inliers to be inverse-polynomial in the number of samples.\nPrior to this work, no estimator was known to be consistent when the fraction of inliers $\\alpha$ is $o(1/\\log \\log n)$, even for (non-spherical) Gaussian design matrices.\nResults holding under weak design assumptions and in the presence of such general noise have only been shown in dense setting (i.e., general linear regression) very recently by d'Orsi et al.~\\cite{ICML-linear-regression}.\nIn the context of PCA, we attain optimal error guarantees under broad spikiness assumptions on the parameter matrix (usually used in matrix completion). \nPrevious works could obtain non-trivial guarantees only under the assumptions that the measurement noise corresponding to the inliers is polynomially small in $n$ (e.g., Gaussian with variance $1/n^2$).\n\nTo devise our estimators, we equip the Huber loss with non-smooth regularizers such as the $\\ell_1$ norm or the nuclear norm, and extend d'Orsi et al.'s approach~\\cite{ICML-linear-regression} in a novel way to analyze the loss function.\nOur machinery appears to be easily applicable to a wide range of estimation problems.\nWe complement these algorithmic results with statistical lower bounds showing that the fraction of inliers that our PCA estimator can deal with is optimal up to a constant factor."}}
{"id": "mvc3UmEP8d", "cdate": 1609459200000, "mdate": 1680275593756, "content": {"title": "Consistent regression when oblivious outliers overwhelm", "abstract": ""}}
{"id": "I7kTNMdtwk", "cdate": 1609459200000, "mdate": 1680275593704, "content": {"title": "Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers", "abstract": ""}}
{"id": "ElW3QXT_f2", "cdate": 1609459200000, "mdate": 1682413153705, "content": {"title": "Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers", "abstract": "We develop machinery to design efficiently computable and consistent estimators, achieving estimation error approaching zero as the number of observations grows, when facing an oblivious adversary that may corrupt responses in all but an $\\alpha$ fraction of the samples. As concrete examples, we investigate two problems: sparse regression and principal component analysis (PCA). For sparse regression, we achieve consistency for optimal sample size $n\\gtrsim (k\\log d)/\\alpha^2$ and optimal error rate $O(\\sqrt{(k\\log d)/(n\\cdot \\alpha^2)})$ where $n$ is the number of observations, $d$ is the number of dimensions and $k$ is the sparsity of the parameter vector, allowing the fraction of inliers to be inverse-polynomial in the number of samples. Prior to this work, no estimator was known to be consistent when the fraction of inliers $\\alpha$ is $o(1/\\log \\log n)$, even for (non-spherical) Gaussian design matrices. Results holding under weak design assumptions and in the presence of such general noise have only been shown in dense setting (i.e., general linear regression) very recently by d'Orsi et al. [dNS21]. In the context of PCA, we attain optimal error guarantees under broad spikiness assumptions on the parameter matrix (usually used in matrix completion). Previous works could obtain non-trivial guarantees only under the assumptions that the measurement noise corresponding to the inliers is polynomially small in $n$ (e.g., Gaussian with variance $1/n^2$). To devise our estimators, we equip the Huber loss with non-smooth regularizers such as the $\\ell_1$ norm or the nuclear norm, and extend d'Orsi et al.'s approach [dNS21] in a novel way to analyze the loss function. Our machinery appears to be easily applicable to a wide range of estimation problems."}}
{"id": "s5_HmHipSL", "cdate": 1577836800000, "mdate": 1680275593693, "content": {"title": "Sparse PCA: Algorithms, Adversarial Perturbations and Certificates", "abstract": ""}}
{"id": "gTjHuR5jyqC", "cdate": 1577836800000, "mdate": 1682413153693, "content": {"title": "Sparse PCA: Algorithms, Adversarial Perturbations and Certificates", "abstract": "We study efficient algorithms for Sparse PCA in standard statistical models (spiked covariance in its Wishart form). Our goal is to achieve optimal recovery guarantees while being resilient to small perturbations. Despite a long history of prior works, including explicit studies of perturbation resilience, the best known algorithmic guarantees for Sparse PCA are fragile and break down under small adversarial perturbations. We observe a basic connection between perturbation resilience and \\emph{certifying algorithms} that are based on certificates of upper bounds on sparse eigenvalues of random matrices. In contrast to other techniques, such certifying algorithms, including the brute-force maximum likelihood estimator, are automatically robust against small adversarial perturbation. We use this connection to obtain the first polynomial-time algorithms for this problem that are resilient against additive adversarial perturbations by obtaining new efficient certificates for upper bounds on sparse eigenvalues of random matrices. Our algorithms are based either on basic semidefinite programming or on its low-degree sum-of-squares strengthening depending on the parameter regimes. Their guarantees either match or approach the best known guarantees of \\emph{fragile} algorithms in terms of sparsity of the unknown vector, number of samples and the ambient dimension. To complement our algorithmic results, we prove rigorous lower bounds matching the gap between fragile and robust polynomial-time algorithms in a natural computational model based on low-degree polynomials (closely related to the pseudo-calibration technique for sum-of-squares lower bounds) that is known to capture the best known guarantees for related statistical estimation problems. The combination of these results provides formal evidence of an inherent price to pay to achieve robustness."}}
