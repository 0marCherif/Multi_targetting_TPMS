{"id": "NmUWaaFEDdn", "cdate": 1652737496385, "mdate": null, "content": {"title": "On the Double Descent of Random Features Models Trained with SGD", "abstract": "We study generalization properties of random features (RF) regression in high dimensions optimized by stochastic gradient descent (SGD) in under-/over-parameterized regime. In this work, we derive precise non-asymptotic error bounds of RF regression under both constant and polynomial-decay step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well for interpolation learning, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimum-norm interpolator, as a theoretical justification of using SGD in practice."}}
{"id": "gTdmGt48ht1", "cdate": 1632875469536, "mdate": null, "content": {"title": "On the Double Descent of Random Features Models Trained with SGD", "abstract": "We study generalization properties of random features (RF) regression in high dimensions  optimized by stochastic gradient descent (SGD). In this regime, we derive precise non-asymptotic error bounds of RF regression under both constant and adaptive step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well in the interpolation setting, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimal-norm interpolator, as a theoretical justification of using SGD in practice."}}
{"id": "3xUBgZQ04X", "cdate": 1601308210942, "mdate": null, "content": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  "}}
{"id": "AIM2LvTUR5", "cdate": 1599617615684, "mdate": null, "content": {"title": "Sparse Kernel Regression with Coefficient-based lq\u2212regularization", "abstract": "In this paper, we consider the lq\u2212 regularized kernel regression with 0< q\u2264 1. In form, the algorithm minimizes a least-square loss functional adding a coefficient-based lq\u2212 penalty term over a linear span of features generated by a kernel function. We study the asymptotic behavior of the algorithm under the framework of learning theory. The contribution of this paper is two-fold. First, we derive a tight bound on the l2\u2212 empirical covering numbers of the related function space involved in the error analysis. Based on this result, we obtain the convergence rates for the l1\u2212 regularized kernel regression which is the best so far. Second, for the case 0< q< 1, we show that the regularization parameter plays a role as a trade-off between sparsity and convergence rates. Under some mild conditions, the fraction of non-zero coefficients in a local minimizer of the algorithm will tend to 0 at a polynomial decay rate when the sample size m becomes large. As the concerned algorithm is non-convex, we also discuss how to generate a minimizing sequence iteratively, which can help us to search a local minimizer around any initial point."}}
{"id": "QbIqaS2xc-E", "cdate": 1599617277440, "mdate": null, "content": {"title": "Sparse Kernel Regression with Coefficient-based lq\u2212regularization", "abstract": "In this paper, we consider the lq\u2212 regularized kernel regression with 0< q\u2264 1. In form, the algorithm minimizes a least-square loss functional adding a coefficient-based lq\u2212 penalty term over a linear span of features generated by a kernel function. We study the asymptotic behavior of the algorithm under the framework of learning theory. The contribution of this paper is two-fold. First, we derive a tight bound on the l2\u2212 empirical covering numbers of the related function space involved in the error analysis. Based on this result, we obtain the convergence rates for the l1\u2212 regularized kernel regression which is the best so far. Second, for the case 0< q< 1, we show that the regularization parameter plays a role as a trade-off between sparsity and convergence rates. Under some mild conditions, the fraction of non-zero coefficients in a local minimizer of the algorithm will tend to 0 at a polynomial decay rate when the sample size m becomes large. As the concerned algorithm is non-convex, we also discuss how to generate a minimizing sequence iteratively, which can help us to search a local minimizer around any initial point."}}
{"id": "ycsDVZCixSn", "cdate": 1599143032615, "mdate": null, "content": {"title": "A Theoretical Framework for Target Propagation", "abstract": "The success of deep learning, a brain-inspired form of AI, has sparked interest in understanding how the brain could similarly learn across multiple layers of neurons. However, the majority of biologically-plausible learning algorithms have not yet reached the performance of backpropagation (BP), nor are they built on strong theoretical foundations. Here, we analyze target propagation (TP), a popular but not yet fully understood alternative to BP, from the standpoint of mathematical optimization. Our theory shows that TP is closely related to Gauss-Newton optimization and thus substantially differs from BP. Furthermore, our analysis reveals a fundamental limitation of difference target propagation (DTP), a well-known variant of TP, in the realistic scenario of non-invertible neural networks. We provide a first solution to this problem through a novel reconstruction loss that improves feedback weight training, while simultaneously introducing architectural flexibility by allowing for direct feedback connections from the output to each hidden layer. Our theory is corroborated by experimental results that show significant improvements in performance and in the alignment of forward weight updates with loss gradients, compared to DTP."}}
{"id": "v8oH7VeRTzB", "cdate": 1596183791416, "mdate": null, "content": {"title": "Kernel Density Estimation for Dynamical Systems", "abstract": "We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the C-mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under L1-norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under L1-norm. In the analysis, the density function f is only assumed to be H \u0308older continuous or pointwise H \u0308older controllable which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under L\u221e-norm and L1-norm can be achieved when the density function is Holder continuous, compactly supported, and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations."}}
{"id": "ryghPCVYvH", "cdate": 1569439331866, "mdate": null, "content": {"title": "Generative Restricted Kernel Machines", "abstract": "We introduce a novel framework for generative models based on Restricted Kernel Machines (RKMs) with multi-view generation and uncorrelated feature learning capabilities, called Gen-RKM. To incorporate multi-view generation, this mechanism uses a shared representation of data from various views. The mechanism is flexible to incorporate both kernel-based, (deep) neural network and convolutional based models within the same setting. To update the parameters of the network, we propose a novel training procedure which jointly learns the features and shared representation. Experiments demonstrate the potential of the framework through qualitative evaluation of generated samples."}}
{"id": "SJgYmxo4jX", "cdate": 1539776545100, "mdate": null, "content": {"title": "Spatio-temporal Stacked LSTM for Temperature Prediction in Weather Forecasting", "abstract": "Long Short-Term Memory (LSTM) is a well-known method used widely on sequence learning and time series prediction. In this paper we deployed stacked LSTM model in an application of weather forecasting. We propose a 2-layer spatio-temporal stacked LSTM model which consists of independent LSTM models per location in the first LSTM layer. Subsequently, the input of the second LSTM layer is formed based on the combination of the hidden states of the first layer LSTM models. The experiments show that by utilizing the spatial information the prediction performance of the stacked LSTM model improves in most of the cases"}}
{"id": "rjiEr1HgdpH", "cdate": 1388534400000, "mdate": null, "content": {"title": "Support Vector Machine Classifier With Pinball Loss.", "abstract": "Traditionally, the hinge loss is used to construct support vector machine (SVM) classifiers. The hinge loss is related to the shortest distance between sets and the corresponding classifier is hence sensitive to noise and unstable for re-sampling. In contrast, the pinball loss is related to the quantile distance and the result is less sensitive. The pinball loss has been deeply studied and widely applied in regression but it has not been used for classification. In this paper, we propose a SVM classifier with the pinball loss, called pin-SVM, and investigate its properties, including noise insensitivity, robustness, and misclassification error. Besides, insensitive zone is applied to the pin-SVM for a sparse model. Compared to the SVM with the hinge loss, the proposed pin-SVM has the same computational complexity and enjoys noise insensitivity and re-sampling stability."}}
