{"id": "LzYKvG7Us8", "cdate": 1683879343292, "mdate": 1683879343292, "content": {"title": "Spectral Clustering with Robust Self-learning Constraints", "abstract": "Spectral clustering is a leading unsupervised classification algorithm widely used to capture complex clusters in unlabeled data. Additional prior information can further enhance the quality of spectral clustering results to satisfy users' expectations. However, it is challenging for users to find the prior information under unsupervised scenes. To get rid of the deficiency, we propose a spectral clustering model with robust self-learning constraints. In this model, we first extend the optimization problem of spectral clustering by seeing label constraints as variables to learn the constraints and the clustering result simultaneously. Furthermore, we add a robust term to the proposed model so that we can learn multiple groups of label constraints to guide the clustering process and find a robust self-constrained spectral clustering result. The robust term can reduce the impact of uncertainty in the quality of a single set of label constraints on the performance of the proposed model. An iterative strategy with update formulas for variables is proposed to solve the self-constrained spectral clustering problem. We provide the theoretical analysis to explain the importance of the learned constraints in spectral clustering. Furthermore, we analyze the convergence of our optimization scheme. Finally, we have done many experiments on benchmark data sets to illustrate the effectiveness of the proposed algorithm."}}
{"id": "XUqTyU9VlWp", "cdate": 1663850200642, "mdate": null, "content": {"title": "The Impact of Neighborhood Distribution in Graph Convolutional Networks", "abstract": "Graph Convolutional Networks (GCNs) which aggregate information from neighbors to learn node representation, have shown excellent ability in processing graph-structured data. However, it is inaccurate that the notable performance of GCNs tends to depend on strong homophily assumption of networks, since GCNs can also perform well over some heterophilous graphs. Thus the impact of homophily on GCNs needs to be reconsidered. In this paper, we study what influences the aggregation of GCNs from the perspective of neighborhood distribution. Theoretical and empirical analysis is provided to reveal that the distinguishability of neighborhood distribution plays a more important role in the performance of GCN than homophily. Furthermore, we address that neighborhood structure and neighborhood range are two key factors for GCNs to promote neighborhood distinguishability. Based on the conclusion, we propose an improved graph convolution network (GCN-PND) including updating graph topology based on the similarity between local neighborhood distribution of nodes and designing extensible aggregation from multi-hop neighbors. We did extensive experiments on graph benchmark datasets to analyze the superiority of the proposed method. The experimental results demonstrate that GCN-PND is more effective on heterophilous datasets than most of existing state-of-the-art GCN methods.\n"}}
{"id": "OXdI1jgdEZJ", "cdate": 1631879195593, "mdate": 1631879195593, "content": {"title": "The impact of cluster representatives on the convergence of the K-Modes type clustering", "abstract": "As a leading partitional clustering technique, (k)-modes is one of the most computationally efficient clustering methods for categorical data. In the (k)-modes, a cluster is represented by a \u201cmode,\u201d which is composed of the attribute value that occurs most frequently in each attribute domain of the cluster, whereas, in real applications, using only one attribute value in each attribute to represent a cluster may not be adequate as it could in turn affect the accuracy of data analysis. To get rid of this deficiency, several modified clustering algorithms were developed by assigning appropriate weights to several attribute values in each attribute. Although these modified algorithms are quite effective, their convergence proofs are lacking. In this paper, we analyze their convergence property and prove that they cannot guarantee to converge under their optimization frameworks unless they degrade to the original (k)--modes type algorithms. Furthermore, we propose two different modified algorithms with weighted cluster prototypes to overcome the shortcomings of these existing algorithms. We rigorously derive updating formulas for the proposed algorithms and prove the convergence of the proposed algorithms. The experimental studies show that the proposed algorithms are effective and efficient for large categorical datasets."}}
{"id": "6owAuzSNb70", "cdate": 1631879118230, "mdate": 1631879118230, "content": {"title": "A three-level optimization model for nonlinearly separable clustering", "abstract": "Due to the complex structure of the real-world data, nonlinearly separable clustering is one of popular and widely studied clustering problems. Currently, various types of algorithms, such as kernel k-means, spectral clustering and density clustering, have been developed to solve this problem. However, it is difficult for them to balance the efficiency and effectiveness of clustering, which limits their real applications. To get rid of the deficiency, we propose a three-level optimization model for nonlinearly separable clustering which divides the clustering problem into three sub-problems: a linearly separable clustering on the object set, a nonlinearly separable clustering on the cluster set and an ensemble clustering on the partition set. An iterative algorithm is proposed to solve the optimization problem. The proposed algorithm can use low computational cost to effectively recognize nonlinearly separable clusters. The performance of this algorithm has been studied on synthetical and real data sets. Comparisons with other nonlinearly separable clustering algorithms illustrate the efficiency and effectiveness of the proposed algorithm."}}
{"id": "ppZoneM-VwE", "cdate": 1631878974296, "mdate": 1631878974296, "content": {"title": "Sparse subspace clustering with entropy-norm", "abstract": "In this paper, we provide an explicit theoretical connection between Sparse subspace clustering (SSC) and spectral clustering (SC) from the perspective of learning a data similarity matrix. We show that spectral clustering with Gaussian kernel can be viewed as sparse subspace clustering with entropy-norm (SSC+E). Compared to SSC, SSC+E can obtain an analytical, symmetrical, nonnegative and nonlinearly-representational similarity matrix. Besides, SSC+E makes use of Gaussian kernel to compute the sparse similarity matrix of objects, which can avoid the complex computation of the sparse optimization program of SSC. Finally, we provide the experimental analysis to compare the efficiency and effectiveness of sparse subspace clustering and spectral clustering on ten benchmark data sets. The theoretical and experimental analysis can well help users for the selection of high-dimensional data clustering algorithms."}}
{"id": "IL44ARsfqf2", "cdate": 1631878825660, "mdate": 1631878825660, "content": {"title": "Semi-supervised clustering with constraints of different types from multiple information sources", "abstract": "Semi-supervised clustering is one of important research topics in cluster analysis, which uses pre-given knowledge as constraints to improve the clustering performance. While clustering a data set, people often get prior constraints from different information sources, which may have different representations and contents, to guide clustering process. However, most of existing semi-supervised clustering algorithms are based on single-source constraints and rarely consider to integrate multi-source constraints to enhance the clustering quality. To solve the problem, we analyze the relations among different types of constraints and propose an uniform representation for them. Based it, we propose a new semi-supervised clustering algorithm to find out a clustering that has good cluster structure and high consensus of all the sources of constraints. In the algorithm, we construct an optimization objective model and its solution method to achieve the aim. This algorithm can integrate multi-source constraints well to reduce the effect of incorrect constraints from single sources and find out a high-quality clustering. By the experimental studies on several benchmark data sets, we illustrate the effectiveness of the proposed algorithm, compared to other semi-supervised clustering algorithms."}}
