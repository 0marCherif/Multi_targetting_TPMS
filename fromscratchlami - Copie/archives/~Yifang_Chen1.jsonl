{"id": "mfKvV-uZrFH", "cdate": 1664815574093, "mdate": null, "content": {"title": "Causal Bandits: Online Decision-Making in Endogenous Settings", "abstract": "The deployment of Multi-Armed Bandits (MAB) has become commonplace in many economic applications. However, regret guarantees for even state-of-the-art linear bandit algorithms (such as Optimism in the Face of Uncertainty Linear bandit (OFUL)) make strong exogeneity assumptions w.r.t. arm covariates. This assumption is very often violated in many economic contexts and using such algorithms can lead to sub-optimal decisions. In this paper, we consider the problem of online learning in linear stochastic multi-armed bandit problems with endogenous covariates. We propose an algorithm we term BanditIV, that uses instrumental variables to correct for this bias, and prove an $\\tilde{\\mathcal{O}}(k\\sqrt{T})$ upper bound for the expected regret of the algorithm. Further, in economic contexts, it is also important to understand how the model parameters behave asymptotically. To this end, we additionally propose $\\epsilon$-\\textit{BanditIV} algorithm and demonstrate its asymptotic consistency and normality while ensuring the same regret bound. Finally, we carry out extensive Monte Carlo simulations to demonstrate the performance of our algorithms compared to other methods. We show that BanditIV and $\\epsilon$-BanditIV significantly outperform other existing methods. "}}
{"id": "_QJBNTVGyHy", "cdate": 1664300344406, "mdate": null, "content": {"title": "Causal Bandits: Online Decision-Making in Endogenous Settings", "abstract": "The deployment of Multi-Armed Bandits (MAB) has become commonplace in many economic applications. However, regret guarantees for even state-of-the-art linear bandit algorithms (such as Optimism in the Face of Uncertainty Linear bandit (OFUL)) make strong exogeneity assumptions w.r.t. arm covariates. This assumption is very often violated in many economic contexts and using such algorithms can lead to sub-optimal decisions. In this paper, we consider the problem of online learning in linear stochastic multi-armed bandit problems with endogenous covariates. We propose an algorithm we term BanditIV, that uses instrumental variables to correct for this bias, and prove an $\\tilde{\\mathcal{O}}(k\\sqrt{T})$ upper bound for the expected regret of the algorithm. Further, in economic contexts, it is also important to understand how the model parameters behave asymptotically. To this end, we additionally propose $\\epsilon$-BanditIV algorithm and demonstrate its asymptotic consistency and normality while ensuring the same regret bound. Finally, we carry out extensive Monte Carlo simulations to demonstrate the performance of our algorithms compared to other methods. We show that BanditIV and $\\epsilon$-BanditIV significantly outperform other existing methods. "}}
{"id": "Ruw3MHL9jAO", "cdate": 1621630152456, "mdate": null, "content": {"title": "Corruption Robust Active Learning", "abstract": "We conduct theoretical studies on streaming-based active learning for binary classification under unknown adversarial label corruptions. In this setting, every time before the learner observes a sample, the adversary decides whether to corrupt the label ornot. First, we show that, in a benign corruption setting (which includes the misspecification setting as a special case),\nwith a slight enlargement on the hypothesis elimination threshold, the classical RobustCAL framework can (surprisingly) achieve nearly the same label complexity guarantee as in the non-corrupted setting. However, this algorithm can fail in the general corruption setting. To resolve this drawback, we propose a new algorithm which is provably correct without any assumptions on the presence of corruptions. Furthermore, this algorithm enjoys the minimax label complexity in the non-corrupted setting (which is achieved by RobustCAL) and only requires $\\tilde{\\mathcal{O}}(C_{\\mathrm{total}})$ additional labels in the corrupted setting to achieve $\\mathcal{O}(\\varepsilon + \\frac{C_{\\mathrm{total}}}{n})$, where $\\varepsilon$ is the target accuracy, $C_{\\mathrm{total}}$ is the total number of corruptions and $n$ is the total number of unlabeled samples."}}
