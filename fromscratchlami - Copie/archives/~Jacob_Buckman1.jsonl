{"id": "XByg4kotW5", "cdate": 1652737425222, "mdate": null, "content": {"title": "When does return-conditioned supervised learning work for offline reinforcement learning?", "abstract": "Several recent works have proposed a class of algorithms for the offline reinforcement learning (RL) problem that we will refer to as return-conditioned supervised learning (RCSL). RCSL algorithms learn the distribution of actions conditioned on both the state and the return of the trajectory. Then they define a policy by conditioning on achieving high return. In this paper, we provide a rigorous study of the capabilities and limitations of RCSL something which is crucially missing in previous work. We find that RCSL returns the optimal policy under a set of assumptions that are stronger than those needed for the more traditional dynamic programming-based algorithms. We provide specific examples of MDPs and datasets that illustrate the necessity of these assumptions and the limits of RCSL. Finally, we present empirical evidence that these limitations will also cause issues in practice by providing illustrative experiments in simple point-mass environments and on datasets from the D4RL benchmark."}}
{"id": "E3Ys6a1NTGT", "cdate": 1601308165180, "mdate": null, "content": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments."}}
{"id": "rJbn0q-OWS", "cdate": 1546300800000, "mdate": null, "content": {"title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning", "abstract": "Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce th..."}}
{"id": "S18Su--CW", "cdate": 1518730168199, "mdate": null, "content": {"title": "Thermometer Encoding: One Hot Way To Resist Adversarial Examples", "abstract": "It is well known that it is possible to construct \"adversarial examples\"\nfor neural networks: inputs which are misclassified by the network\nyet indistinguishable from true data. We propose a simple\nmodification to standard neural network architectures, thermometer\nencoding, which significantly increases the robustness of the network to\nadversarial examples. We demonstrate this robustness with experiments\non the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that\nmodels with thermometer-encoded inputs consistently have higher accuracy\non adversarial examples, without decreasing generalization.\nState-of-the-art accuracy under the strongest known white-box attack was \nincreased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10.\nWe explore the properties of these networks, providing evidence\nthat thermometer encodings help neural networks to\nfind more-non-linear decision boundaries."}}
{"id": "ryWQxj-uZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Is Generator Conditioning Causally Related to GAN Performance?", "abstract": "Recent work suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of sing..."}}
{"id": "rJNEkDZO-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion", "abstract": "There is growing interest in combining model-free and model-based approaches in reinforcement learning with the goal of achieving the high performance of model-free algorithms with low sample complexity. This is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency."}}
{"id": "HybHfMG_bS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Transition-Based Dependency Parsing with Heuristic Backtracking", "abstract": ""}}
