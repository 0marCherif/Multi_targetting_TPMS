{"id": "3sFiUllkI1", "cdate": 1668154089145, "mdate": null, "content": {"title": "Domain Adaptive Hand Keypoint and Pixel Localization in the Wild", "abstract": "We aim to improve the performance of regressing hand keypoints and segmenting pixel-level hand masks under new imaging conditions (e.g., outdoors) when we only have labeled images taken under very different conditions (e.g., indoors). In the real world, it is important that the model trained for both tasks works under various imaging conditions. However, their variation covered by existing labeled hand datasets is limited. Thus, it is necessary to adapt the model trained on the labeled images (source) to unlabeled images (target) with unseen imaging conditions. While self-training domain adaptation methods (i.e., learning from the unlabeled target images in a self-supervised manner) have been developed for both tasks, their training may degrade performance when the predictions on the target images are noisy. To avoid this, it is crucial to assign a low importance (confidence) weight to the noisy predictions during self-training. In this paper, we propose to utilize the divergence of two predictions to estimate the confidence of the target image for both tasks. These predictions are given from two separate networks, and their divergence helps identify the noisy predictions. To integrate our proposed confidence estimation into self-training, we propose a teacher-student framework where the two networks (teachers) provide supervision to a network (student) for self-training, and the teachers are learned from the student by knowledge distillation. Our experiments show its superiority over state-of-the-art methods in adaptation settings with different lighting, grasping objects, backgrounds, and camera viewpoints. Our method improves by 4% the multi-task score on HO3D compared to the latest adversarial adaptation method. We also validate our method on Ego4D, egocentric videos with rapid changes in imaging conditions outdoors."}}
{"id": "W4ub8fyCpED", "cdate": 1663849817875, "mdate": null, "content": {"title": "Learning a 3D-Aware Encoder for Style-based Generative Radiance Field", "abstract": "We tackle the task of GAN inversion for 3D generative radiance field, (e.g., StyleNeRF). In the inversion task, we aim to learn an inversion function to project an input image to the latent space of a generator and then synthesize novel views of the original image based on the latent code. Compared with GAN inversion for 2D generative models, 3D inversion not only needs to 1) preserve the identity of the input image, but also 2) ensure 3D consistency in generated novel views. This requires the latent code obtained from the single view image to be invariant across multiple views. To address this new challenge, we propose a two-stage encoder for 3D generative NeRF inversion. In the first stage, we introduce a base encoder that converts the input image to a latent code. To ensure the latent code can be used to synthesize identity preserving and 3D consistent novel view images, we utilize identity contrastive learning to train the base encoder. Since collecting real-world multi-view images of the same identity is expensive, we leverage multi-view images synthesized by the generator itself for contrastive learning. Second, to better preserve the identity of the input image, we introduce a residual encoder to refine the latent code and add finer details to the output image. Through extensive experiments, we demonstrate that our proposed two-stage encoder qualitatively and quantitatively exhibits superiority over the existing encoders for GAN inver- sion in both image reconstruction and novel-view rendering."}}
{"id": "eBZsAZB8Rfh", "cdate": 1632875488475, "mdate": null, "content": {"title": "Adaptive Unbiased Teacher for Cross-Domain Object Detection", "abstract": "We tackle the problem of domain adaptation in object detection, where the main challenge lies in significant domain shifts between source (one domain with supervision) and target (a domain of interest without supervision).   Although the teacher-student framework (a student model learns from pseudo labels generated from a teacher model) has been adopted to enable domain adaptation and yielded accuracy  gains  on  the  target  domain,  the  teacher  model  still  generates  a  large number of low-quality pseudo labels (e.g.,false positives) due to its bias toward source domain. This leads to sub-optimal domain adaptation performance. To ad-dress this issue, we propose Adaptive Unbiased Teacher (AUT), a teacher-student framework leveraging adversarial learning (on features derived from backbone)and weak-strong data augmentation to address domain shifts. Specifically, we em-ploy feature-level adversarial training, ensuring features extracted from the source and target domains share similar statistics. This enables the student model to capture domain-invariant features. Furthermore, we apply weak-strong augmentation and mutual learning of the teacher for target domain and student model for both domains.  This enables the updated teacher model to gradually benefit from the student model without suffering domain shift.  We show that AUT demonstrates superiority over all existing approaches and even Oracle (fully-supervised) mod-els by a huge margin.  For example, we achieve 50.9% (49.3%) mAP on FoggyCityscape (Clipart1K), which is 9.2% (5.2%) and 8.2% (11.0%) higher than previous state of the arts and Oracle, respectively."}}
{"id": "rX4okMxdpS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning Resolution-Invariant Deep Representations for Person Re-Identification.", "abstract": "Person re-identification (re-ID) solves the task of matching images across cameras and is among the research topics in vision community. Since query images in real-world scenarios might suffer from resolution loss, how to solve the resolution mismatch problem during person re-ID becomes a practical problem. Instead of applying separate image super-resolution models, we propose a novel network architecture of Resolution Adaptation and re-Identification Network (RAIN) to solve cross-resolution person re-ID. Advancing the strategy of adversarial learning, we aim at extracting resolution-invariant representations for re-ID, while the proposed model is learned in an end-to-end training fashion. Our experiments confirm that the use of our model can recognize low-resolution query images, even if the resolution is not seen during training. Moreover, the extension of our model for semi-supervised re-ID further confirms the scalability of our proposed method for real-world scenarios and applications."}}
{"id": "SjlePJGmlO6H", "cdate": 1546300800000, "mdate": null, "content": {"title": "Spot and Learn: A Maximum-Entropy Patch Sampler for Few-Shot Image Classification.", "abstract": "Few-shot learning (FSL) requires one to learn from object categories with a small amount of training data (as novel classes), while the remaining categories (as base classes) contain a sufficient amount of data for training. It is often desirable to transfer knowledge from the base classes and derive dominant features efficiently for the novel samples. In this work, we propose a sampling method that de-correlates an image based on maximum entropy reinforcement learning, and extracts varying sequences of patches on every forward-pass with discriminative information observed. This can be viewed as a form of \"learned\" data augmentation in the sense that we search for different sequences of patches within an image and performs classification with aggregation of the extracted features, resulting in improved FSL performances. In addition, our positive and negative sampling policies along with a newly defined reward function would favorably improve the effectiveness of our model. Our experiments on two benchmark datasets confirm the effectiveness of our framework and its superiority over recent FSL approaches."}}
{"id": "H1WUMCW_-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification", "abstract": "Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training."}}
