{"id": "Nc1ZkRW8Vde", "cdate": 1663850258756, "mdate": null, "content": {"title": "Near-optimal Coresets for Robust Clustering", "abstract": "We consider robust clustering problems in $\\mathbb{R}^d$, specifically $k$-clustering problems (e.g., $k$-Median and $k$-Means) with $m$ \\emph{outliers}, where the cost for a given center set $C \\subset \\mathbb{R}^d$ aggregates the distances from $C$ to all but the furthest $m$ data points, instead of all points as in classical clustering. We focus on the $\\epsilon$-coreset for robust clustering, a small proxy of the dataset that preserves the clustering cost within $\\epsilon$-relative error for all center sets. Our main result is an $\\epsilon$-coreset of size $O(m + \\mathrm{poly}(k \\epsilon^{-1}))$ that can be constructed in near-linear time. This significantly improves previous results, which either suffers an exponential dependence on $(m + k)$ [Feldman and Schulman, SODA'12], or has a weaker bi-criteria guarantee [Huang et al., FOCS'18]. Furthermore, we show this dependence in $m$ is nearly-optimal, and the fact that it is isolated from other factors may be crucial for dealing with large number of outliers. We construct our coresets by adapting to the outlier setting a recent framework [Braverman et al., FOCS'22] which was designed for capacity-constrained clustering, overcoming a new challenge that the participating terms in the cost, particularly the excluded $m$ outlier points, are dependent on the center set $C$. We validate our coresets on various datasets, and we observe a superior size-accuracy tradeoff compared with popular baselines including uniform sampling and sensitivity sampling. We also achieve a significant speedup of existing approximation algorithms for robust clustering using our coresets."}}
{"id": "1H6zA8wIhKk", "cdate": 1621629958005, "mdate": null, "content": {"title": "Coresets for Clustering with Missing Values", "abstract": "We provide the first coreset for clustering points in $\\mathbb{R}^d$ that have multiple missing values (coordinates). Previous coreset constructions only allow one missing coordinate. The challenge in this setting is that objective functions, like \\kMeans, are evaluated only on the set of available (non-missing) coordinates, which varies across points. Recall that an $\\epsilon$-coreset of a large dataset is a small proxy, usually a reweighted subset of points, that $(1+\\epsilon)$-approximates the clustering objective for every possible center set.\n\nOur coresets for $k$-Means and $k$-Median clustering have size $(jk)^{O(\\min(j,k))} (\\epsilon^{-1} d \\log n)^2$, where $n$ is the number of data points, $d$ is the dimension and $j$ is the maximum number of missing coordinates for each data point. We further design an algorithm to construct these coresets in near-linear time, and consequently improve a recent quadratic-time PTAS for $k$-Means with missing values [Eiben et al., SODA 2021] to near-linear time.\n\nWe validate our coreset construction, which is based on importance sampling and is easy to implement, on various real data sets. Our coreset exhibits a flexible tradeoff between coreset size and accuracy, and generally outperforms the uniform-sampling baseline. Furthermore, it significantly speeds up a Lloyd's-style heuristic for $k$-Means with missing values."}}
{"id": "Z2-DODia6YD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Coresets for Clustering in Excluded-minor Graphs and Beyond", "abstract": "Coresets are modern data-reduction tools that are widely used in data analysis to improve efficiency in terms of running time, space and communication complexity. Our main result is a fast algorithm to construct a small coreset for k-Median in (the shortest-path metric of) an excluded-minor graph. Specifically, we give the first coreset of size that depends only on $k$, $\\epsilon$ and the excluded-minor size, and our running time is quasi-linear (in the size of the input graph). The main innovation in our new algorithm is that is iterative; it first reduces the $n$ input points to roughly $O(\\log n)$ reweighted points, then to $O(\\log\\log n)$, and so forth until the size is independent of $n$. Each step in this iterative size reduction is based on the importance sampling framework of Feldman and Langberg (STOC 2011), with a crucial adaptation that reduces the number of \\emph{distinct points}, by employing a terminal embedding (where low distortion is guaranteed only for the distance from every terminal to all other points). Our terminal embedding is technically involved and relies on shortest-path separators, a standard tool in planar and excluded-minor graphs. Furthermore, our new algorithm is applicable also in Euclidean metrics, by simply using a recent terminal embedding result of Narayanan and Nelson, (STOC 2019), which extends the Johnson-Lindenstrauss Lemma. We thus obtain an efficient coreset construction in high-dimensional Euclidean spaces, thereby matching and simplifying state-of-the-art results (Sohler and Woodruff, FOCS 2018; Huang and Vishnoi, STOC 2020). In addition, we also employ terminal embedding with additive distortion to obtain small coresets in graphs with bounded highway dimension, and use applications of our coresets to obtain improved approximation schemes, e.g., an improved PTAS for planar k-Median via a new centroid set."}}
{"id": "x6338Vt4Kg8", "cdate": 1546300800000, "mdate": null, "content": {"title": "Coresets for Clustering in Graphs of Bounded Treewidth.", "abstract": "We initiate the study of coresets for clustering in graph metrics, i.e., the shortest-path metric of edge-weighted graphs. Such clustering problems are essential to data analysis and used for example in road networks and data visualization. A coreset is a compact summary of the data that approximately preserves the clustering objective for every possible center set, and it offers significant efficiency improvements in terms of running time, storage, and communication, including in streaming and distributed settings. Our main result is a near-linear time construction of a coreset for k-Median in a general graph $G$, with size $O_{\\epsilon, k}(\\mathrm{tw}(G))$ where $\\mathrm{tw}(G)$ is the treewidth of $G$, and we complement the construction with a nearly-tight size lower bound. The construction is based on the framework of Feldman and Langberg [STOC 2011], and our main technical contribution, as required by this framework, is a uniform bound of $O(\\mathrm{tw}(G))$ on the shattering dimension under any point weights. We validate our coreset on real-world road networks, and our scalable algorithm constructs tiny coresets with high accuracy, which translates to a massive speedup of existing approximation algorithms such as local search for graph k-Median."}}
{"id": "sD9Fa-LmIEP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Coresets for Ordered Weighted Clustering", "abstract": "We design coresets for Ordered k-Median, a generalization of classical clustering problems such as k-Median and k-Center, that offers a more flexible data analysis, like easily combining multiple objectives (e.g., to increase fairness or for Pareto optimization). Its objective function is defined via the Ordered Weighted Averaging (OWA) paradigm of Yager (1988), where data points are weighted according to a predefined weight vector, but in order of their contribution to the objective (distance from the centers). A powerful data-reduction technique, called a coreset, is to summarize a point set $X$ in $\\mathbb{R}^d$ into a small (weighted) point set $X'$, such that for every set of $k$ potential centers, the objective value of the coreset $X'$ approximates that of $X$ within factor $1\\pm \\epsilon$. When there are multiple objectives (weights), the above standard coreset might have limited usefulness, whereas in a \\emph{simultaneous} coreset, which was introduced recently by Bachem and Lucic and Lattanzi (2018), the above approximation holds for all weights (in addition to all centers). Our main result is a construction of a simultaneous coreset of size $O_{\\epsilon, d}(k^2 \\log^2 |X|)$ for Ordered k-Median. To validate the efficacy of our coreset construction we ran experiments on a real geographical data set. We find that our algorithm produces a small coreset, which translates to a massive speedup of clustering computations, while maintaining high accuracy for a range of weights."}}
{"id": "mdsJqNbsqlP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Coresets for Ordered Weighted Clustering", "abstract": "We design coresets for Ordered k-Median, a generalization of classical clustering problems such as k-Median and k-Center. Its objective function is defined via the Ordered Weighted Averaging (OWA) ..."}}
{"id": "N-hAt0iEMd-", "cdate": 1546300800000, "mdate": null, "content": {"title": "Coresets for Gaussian Mixture Models of Any Shape", "abstract": "An $\\varepsilon$-coreset for a given set $D$ of $n$ points, is usually a small weighted set, such that querying the coreset \\emph{provably} yields a $(1+\\varepsilon)$-factor approximation to the original (full) dataset, for a given family of queries. Using existing techniques, coresets can be maintained for streaming, dynamic (insertion/deletions), and distributed data in parallel, e.g. on a network, GPU or cloud. We suggest the first coresets that approximate the negative log-likelihood for $k$-Gaussians Mixture Models (GMM) of arbitrary shapes (ratio between eigenvalues of their covariance matrices). For example, for any input set $D$ whose coordinates are integers in $[-n^{100},n^{100}]$ and any fixed $k,d\\geq 1$, the coreset size is $(\\log n)^{O(1)}/\\varepsilon^2$, and can be computed in time near-linear in $n$, with high probability. The optimal GMM may then be approximated quickly by learning the small coreset. Previous results [NIPS'11, JMLR'18] suggested such small coresets for the case of semi-speherical unit Gaussians, i.e., where their corresponding eigenvalues are constants between $\\frac{1}{2\\pi}$ to $2\\pi$. Our main technique is a reduction between coresets for $k$-GMMs and projective clustering problems. We implemented our algorithms, and provide open code, and experimental results. Since our coresets are generic, with no special dependency on GMMs, we hope that they will be useful for many other functions."}}
{"id": "TbDRCsB8Ov", "cdate": 1514764800000, "mdate": null, "content": {"title": "Epsilon-Coresets for Clustering (with Outliers) in Doubling Metrics.", "abstract": "We study the problem of constructing \u03b5-coresets for the (k, z)-clustering problem in a doubling metric M(X, d). An \u03b5-coreset is a weighted subset S \u2286 X with weight function w : S \u2192 \u211d <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u22650</sub> , such that for any k-subset C \u2208 [X] <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">k</sup> , it holds that \u03a3 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">x\u2208S</sub> w(x) \u00b7 d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">z</sup> (x, C) \u2208 (1 \u00b1 \u03b5) \u00b7 \u03a3 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">x\u2208X</sub> d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">z</sup> (x, C). We present an efficient algorithm that constructs an \u03b5-coreset for the (k, z)-clustering problem in M(X, d), where the size of the coreset only depends on the parameters k, z, \u03b5 and the doubling dimension ddim(M). To the best of our knowledge, this is the first efficient c-coreset construction of size independent of |X| for general clustering problems in doubling metrics. To this end, we establish the first relation between the doubling dimension of M(X, d) and the shattering dimension (or VC-dimension) of the range space induced by the distance d. Such a relation is not known before, since one can easily construct instances in which neither one can be bounded by (some function of) the other. Surprisingly, we show that if we allow a small (1 \u00b1 \u03b5)-distortion of the distance function d (the distorted distance is called the smoothed distance function), the shattering dimension can be upper bounded by O(\u03b5 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-O(ddim(M))</sup> ). For the purpose of coreset construction, the above bound does not suffice as it only works for unweighted spaces. Therefore, we introduce the notion of \u03c4-error probabilistic shattering dimension, and prove a (drastically better) upper bound of O(ddim(M)\u00b7log(1/\u03b5)+log log 1/\u03c4) for the probabilistic shattering dimension for weighted doubling metrics. As it turns out, an upper bound for the probabilistic shattering dimension is enough for constructing a small coreset. We believe the new relation between doubling and shattering dimensions is of independent interest and may find other applications. Furthermore, we study robust coresets for (k, z)-clustering with outliers in a doubling metric. We show an improved connection between \u03b1-approximation and robust coresets. This also leads to improvement upon the previous best known bound of the size of robust coreset for Euclidean space [Feldman and Langberg, STOC 11]. The new bound entails a few new results in clustering and property testing. As another application, we show constant-sized (\u03b5, k, z)centroid sets in doubling metrics can be constructed by extending our coreset construction. Prior to our result, constantsized centroid sets for general clustering problems were only known for Euclidean spaces. We can apply our centroid set to accelerate the local search algorithm (studied in [Friggstad et al., FOCS 2016]) for the (k, z)-clustering problem in doubling metrics."}}
{"id": "LWbXJ5UYY24", "cdate": 1514764800000, "mdate": null, "content": {"title": "\u03b5-Coresets for Clustering (with Outliers) in Doubling Metrics.", "abstract": "We study the problem of constructing $\\varepsilon$-coresets for the $(k, z)$-clustering problem in a doubling metric $M(X, d)$. An $\\varepsilon$-coreset is a weighted subset $S\\subseteq X$ with weight function $w : S \\rightarrow \\mathbb{R}_{\\geq 0}$, such that for any $k$-subset $C \\in [X]^k$, it holds that $\\sum_{x \\in S}{w(x) \\cdot d^z(x, C)} \\in (1 \\pm \\varepsilon) \\cdot \\sum_{x \\in X}{d^z(x, C)}$. We present an efficient algorithm that constructs an $\\varepsilon$-coreset for the $(k, z)$-clustering problem in $M(X, d)$, where the size of the coreset only depends on the parameters $k, z, \\varepsilon$ and the doubling dimension $\\mathsf{ddim}(M)$. To the best of our knowledge, this is the first efficient $\\varepsilon$-coreset construction of size independent of $|X|$ for general clustering problems in doubling metrics. To this end, we establish the first relation between the doubling dimension of $M(X, d)$ and the shattering dimension (or VC-dimension) of the range space induced by the distance $d$. Such a relation was not known before, since one can easily construct instances in which neither one can be bounded by (some function of) the other. Surprisingly, we show that if we allow a small $(1\\pm\\epsilon)$-distortion of the distance function $d$, and consider the notion of $\\tau$-error probabilistic shattering dimension, we can prove an upper bound of $O( \\mathsf{ddim}(M)\\cdot \\log(1/\\varepsilon) +\\log\\log{\\frac{1}{\\tau}} )$ for the probabilistic shattering dimension for even weighted doubling metrics. We believe this new relation is of independent interest and may find other applications. We also study the robust coresets and centroid sets in doubling metrics. Our robust coreset construction leads to new results in clustering and property testing, and the centroid sets can be used to accelerate the local search algorithms for clustering problems."}}
{"id": "EzXTdbF7At", "cdate": 1483228800000, "mdate": null, "content": {"title": "Wasserstein Identity Testing", "abstract": "Uniformity testing and the more general identity testing are well studied problems in distributional property testing. Most previous work focuses on testing under $L_1$-distance. However, when the support is very large or even continuous, testing under $L_1$-distance may require a huge (even infinite) number of samples. Motivated by such issues, we consider the identity testing in Wasserstein distance (a.k.a. transportation distance and earthmover distance) on a metric space (discrete or continuous). In this paper, we propose the Wasserstein identity testing problem (Identity Testing in Wasserstein distance). We obtain nearly optimal worst-case sample complexity for the problem. Moreover, for a large class of probability distributions satisfying the so-called \"Doubling Condition\", we provide nearly instance-optimal sample complexity."}}
