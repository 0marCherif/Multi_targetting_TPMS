{"id": "BD2uLkMOquE", "cdate": 1672648391222, "mdate": 1672648391222, "content": {"title": "A new convergent variant of $Q$-learning with linear function approximation", "abstract": "In this work, we identify a novel set of conditions that ensure convergence with probability 1 of Q-learning with linear function approximation, by proposing a two time-scale variation thereof. In the faster time scale, the algorithm features an update similar to that of DQN, where the impact of bootstrapping is attenuated by using a Q-value estimate akin to that of the target network in DQN. The slower time-scale, in turn, can be seen as a modified target network update. We establish the convergence of our algorithm, provide an error bound and discuss our results in light of existing convergence results on reinforcement learning with function approximation. Finally, we illustrate the convergent behavior of our method in domains where standard Q-learning has previously been shown to diverge."}}
{"id": "85mZZbY9j8", "cdate": 1672648310845, "mdate": 1672648310845, "content": {"title": "Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning", "abstract": "We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully perform cooperative tasks with any communication level at execution time by taking advantage of informationsharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully\ndecentralized), to a setting featuring full communication (fully centralized). To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly models a communication process between the agents. We contribute MARO, an approach that combines an autoregressive predictive model to estimate missing agents\u2019 observations, and a dropout-based RL training scheme that simulates different communication levels during the centralized training phase. We evaluate MARO on standard scenarios and extensions of previous benchmarks tailored to emphasize the negative impact of partial observability in MARL. Experimental results show that our method consistently outperforms baselines, allowing agents to act with faulty communication while successfully exploiting shared information."}}
{"id": "Tg9AvNbTUJo", "cdate": 1663850117734, "mdate": null, "content": {"title": "$Q$-learning with regularization converges with non-linear non-stationary features", "abstract": "The deep $Q$-learning architecture is a neural network composed of non-linear hidden layers that learn features of states and actions and a final linear layer that learns the $Q$-values of the features. The parameters of both components can possibly diverge. Regularization of the updates is known to solve the divergence problem of fully linear architectures, where features are stationary and known a priori. We propose a deep $Q$-learning scheme that uses regularization of the final linear layer of architecture, updating it along a faster time-scale, and stochastic full-gradient descent updates for the non-linear features at a slower time-scale. We prove the proposed scheme converges with probability 1. Finally, we provide a bound on the error introduced by regularization of the final linear layer of the architecture."}}
{"id": "u9hnCwX99I1", "cdate": 1663850115478, "mdate": null, "content": {"title": "Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning", "abstract": "We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully perform cooperative tasks with any communication level at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized). To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly models a communication process between the agents. We contribute MARO, an approach that combines an autoregressive predictive model to estimate missing agents' observations, and a dropout-based RL training scheme that simulates different communication levels during the centralized training phase. We evaluate MARO on standard scenarios and extensions of previous benchmarks tailored to emphasize the negative impact of partial observability in MARL. Experimental results show that our method consistently outperforms baselines, allowing agents to act with faulty communication while successfully exploiting shared information."}}
{"id": "Ikr92BKR8vN", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimal action sequence generation for assistive agents in fixed horizon tasks", "abstract": "Agents providing assistance to humans are faced with the challenge of automatically adjusting the level of assistance to ensure optimal performance. In this work, we argue that identifying the right level of assistance consists in balancing positive assistance outcomes and some (domain-dependent) measure of cost associated with assistive actions. Towards this goal, we contribute a general mathematical framework for structured tasks where an agent playing the role of a \u2018provider\u2019\u2014e.g., therapist, teacher\u2014assists a human \u2018receiver\u2019\u2014e.g., patient, student. We specifically consider tasks where the provider agent needs to plan a sequence of actions over a fixed time horizon, where actions are organized along a hierarchy with increasing success probabilities, and some associated costs. The goal of the provider is to achieve a success with the lowest expected cost possible. We present OAssistMe, an algorithm that generates cost-optimal action sequences given the action parameters, and investigate several extensions of it, motivated by different potential application domains. We provide an analysis of the algorithms, including proofs for a number of properties of optimal solutions that, we show, align with typical human provider strategies. Finally, we instantiate our theoretical framework in the context of robot-assisted therapy tasks for children with Autism Spectrum Disorder (ASD). In this context, we present methods for determining action parameters based on a survey of domain experts and real child-robot interaction data. Our contributions unlock increased levels of flexibility for agents introduced in a variety of assistive contexts."}}
{"id": "4cScB7z8Kvq", "cdate": 1577836800000, "mdate": null, "content": {"title": "MHVAE: a Human-Inspired Deep Hierarchical Generative Model for Multimodal Representation Learning", "abstract": "Humans are able to create rich representations of their external reality. Their internal representations allow for cross-modality inference, where available perceptions can induce the perceptual experience of missing input modalities. In this paper, we contribute the Multimodal Hierarchical Variational Auto-encoder (MHVAE), a hierarchical multimodal generative model for representation learning. Inspired by human cognitive models, the MHVAE is able to learn modality-specific distributions, of an arbitrary number of modalities, and a joint-modality distribution, responsible for cross-modality inference. We formally derive the model's evidence lower bound and propose a novel methodology to approximate the joint-modality posterior based on modality-specific representation dropout. We evaluate the MHVAE on standard multimodal datasets. Our model performs on par with other state-of-the-art generative models regarding joint-modality reconstruction from arbitrary input modalities and cross-modality inference."}}
{"id": "-RAV7rh-q8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Playing Games in the Dark: An Approach for Cross-Modality Transfer in Reinforcement Learning", "abstract": "In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality---for example, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms."}}
{"id": "yOMRa8Gz_1J", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning multimodal representations for sample-efficient recognition of human actions", "abstract": "Humans interact in rich and diverse ways with the environment. However, the representation of such behavior by artificial agents is often limited. In this work we present \\textit{motion concepts}, a novel multimodal representation of human actions in a household environment. A motion concept encompasses a probabilistic description of the kinematics of the action along with its contextual background, namely the location and the objects held during the performance. Furthermore, we present Online Motion Concept Learning (OMCL), a new algorithm which learns novel motion concepts from action demonstrations and recognizes previously learned motion concepts. The algorithm is evaluated on a virtual-reality household environment with the presence of a human avatar. OMCL outperforms standard motion recognition algorithms on an one-shot recognition task, attesting to its potential for sample-efficient recognition of human actions."}}
{"id": "vA3vl0xKvp", "cdate": 1546300800000, "mdate": null, "content": {"title": "Interactive robots with model-based 'autism-like' behaviors", "abstract": "Due to their predictability, controllability, and simple social abilities, robots are starting to be used in diverse ways to assist individuals with Autism Spectrum Disorder (ASD). In this work, we investigate an alternative and novel research direction for using robots in relation to ASD, through programming a humanoid robot to exhibit behaviors similar to those observed in children with ASD. We designed 16 \u2018autism-like\u2019 behaviors of different severities on a NAO robot, based on ADOS-2, the gold standard for ASD diagnosis. Our behaviors span four dimensions, verbal and non-verbal, and correspond to a spectrum of typical ASD responses to 3 different stimulus families inspired by standard diagnostic tasks. We integrated these behaviors in an autonomous agent running on the robot, with which humans can continuously interact through predefined stimuli. Through user-controllable features, we allow for 256 unique customizations of the robot\u2019s behavioral profile.We evaluated the validity of our interactive robot both in video-based and \u2018in situ\u2019 studies with 3 therapists. We also present subjective evaluations on the potential benefits of such robots to complement existing therapist training, as well as to enable novel tasks for ASD therapy."}}
{"id": "rtcIQnEdEnk", "cdate": 1546300800000, "mdate": null, "content": {"title": "Playing Games in the Dark: An approach for cross-modality transfer in reinforcement learning", "abstract": "In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality-for example, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms."}}
