{"id": "zoidybfiSc", "cdate": 1672531200000, "mdate": 1682356418902, "content": {"title": "The Target-Charging Technique for Privacy Accounting across Interactive Computations", "abstract": "We propose the \\emph{Target Charging Technique} (TCT), a unified privacy analysis framework for interactive settings where a sensitive dataset is accessed multiple times using differentially private algorithms. Unlike traditional composition, where privacy guarantees deteriorate quickly with the number of accesses, TCT allows computations that don't hit a specified \\emph{target}, often the vast majority, to be essentially free (while incurring instead a small overhead on those that do hit their targets). TCT generalizes tools such as the sparse vector technique and top-$k$ selection from private candidates and extends their remarkable privacy enhancement benefits from noisy Lipschitz functions to general private algorithms."}}
{"id": "mbJ_aTT8NK", "cdate": 1672531200000, "mdate": 1681677773069, "content": {"title": "Generalized Private Selection and Testing with High Confidence", "abstract": "Composition theorems are general and powerful tools that facilitate privacy accounting across multiple data accesses from per-access privacy bounds. However they often result in weaker bounds compared with end-to-end analysis. Two popular tools that mitigate that are the exponential mechanism (or report noisy max) and the sparse vector technique, generalized in a recent private selection framework by Liu and Talwar (STOC 2019). In this work, we propose a flexible framework of private selection and testing that generalizes the one proposed by Liu and Talwar, supporting a wide range of applications. We apply our framework to solve several fundamental tasks, including query releasing, top-k selection, and stable selection, with improved confidence-accuracy tradeoffs. Additionally, for online settings, we apply our private testing to design a mechanism for adaptive query releasing, which improves the sample complexity dependence on the confidence parameter for the celebrated private multiplicative weights algorithm of Hardt and Rothblum (FOCS 2010)."}}
{"id": "xvlaiSHgPrC", "cdate": 1652737590029, "mdate": null, "content": {"title": "Composition Theorems for Interactive Differential Privacy", "abstract": "An interactive mechanism is an algorithm that stores a data set and answers adaptively chosen queries to it. The mechanism is called differentially private, if any adversary cannot distinguish whether a specific individual is in the data set by interacting with the mechanism. We study composition properties of differential privacy in concurrent compositions. In this setting, an adversary interacts with $k$ interactive mechanisms in parallel and can interleave its queries to the mechanisms arbitrarily. Previously, Vadhan and Wang [2021] proved an optimal concurrent composition theorem for pure-differential privacy. We significantly generalize and extend their results. Namely, we prove optimal parallel composition properties for several major notions of differential privacy in the literature, including approximate DP, Renyi DP, and zero-concentrated DP. Our results demonstrate that the adversary gains no advantage by interleaving its queries to independently running mechanisms. Hence, interactivity is a feature that differential privacy grants us for free.\nConcurrently and independently of our work, Vadhan and Zhang [2022] proved an optimal concurrent composition theorem for f-DP [Dong et al., 2022], which implies our result for the approximate DP case."}}
{"id": "x8CNWQeO5o", "cdate": 1640995200000, "mdate": 1681677773071, "content": {"title": "On the Robustness of CountSketch to Adaptive Inputs", "abstract": "The last decade saw impressive progress towards understanding the performance of algorithms in <em>adaptive</em> settings, where subsequent inputs may depend on the output from prior inputs. Adapti..."}}
{"id": "QuEwF6WqrQb", "cdate": 1640995200000, "mdate": 1681677773120, "content": {"title": "\u00d5ptimal Differentially Private Learning of Thresholds and Quasi-Concave Optimization", "abstract": "The problem of learning threshold functions is a fundamental one in machine learning. Classical learning theory implies sample complexity of $O(\\xi^{-1} \\log(1/\\beta))$ (for generalization error $\\xi$ with confidence $1-\\beta$). The private version of the problem, however, is more challenging and in particular, the sample complexity must depend on the size $|X|$ of the domain. Progress on quantifying this dependence, via lower and upper bounds, was made in a line of works over the past decade. In this paper, we finally close the gap for approximate-DP and provide a nearly tight upper bound of $\\tilde{O}(\\log^* |X|)$, which matches a lower bound by Alon et al (that applies even with improper learning) and improves over a prior upper bound of $\\tilde{O}((\\log^* |X|)^{1.5})$ by Kaplan et al. We also provide matching upper and lower bounds of $\\tilde{\\Theta}(2^{\\log^*|X|})$ for the additive error of private quasi-concave optimization (a related and more general problem). Our improvement is achieved via the novel Reorder-Slice-Compute paradigm for private data analysis which we believe will have further applications."}}
{"id": "9QWM8JEjDe", "cdate": 1640995200000, "mdate": 1682356418857, "content": {"title": "Improved Pseudorandom Generators for AC0 Circuits", "abstract": ""}}
{"id": "eIdo6M6EmU", "cdate": 1609459200000, "mdate": 1682356419098, "content": {"title": "Majority vs. Approximate Linear Sum and Average-Case Complexity Below NC\u00b9", "abstract": "We develop a general framework that characterizes strong average-case lower bounds against circuit classes \ud835\udc9e contained in NC\u00b9, such as AC\u2070[\u2295] and ACC\u2070. We apply this framework to show: - Generic seed reduction: Pseudorandom generators (PRGs) against \ud835\udc9e of seed length \u2264 n -1 and error \u03b5(n) = n^{-\u03c9(1)} can be converted into PRGs of sub-polynomial seed length. - Hardness under natural distributions: If \ud835\udda4 (deterministic exponential time) is average-case hard against \ud835\udc9e under some distribution, then \ud835\udda4 is average-case hard against \ud835\udc9e under the uniform distribution. - Equivalence between worst-case and average-case hardness: Worst-case lower bounds against MAJ\u2218\ud835\udc9e for problems in \ud835\udda4 are equivalent to strong average-case lower bounds against \ud835\udc9e. This can be seen as a certain converse to the Discriminator Lemma [Hajnal et al., JCSS'93]. These results were not known to hold for circuit classes that do not compute majority. Additionally, we prove that classical and recent approaches to worst-case lower bounds against ACC\u2070 via communication lower bounds for NOF multi-party protocols [H\u00e5stad and Goldmann, CC'91; Razborov and Wigderson, IPL'93] and Torus polynomials degree lower bounds [Bhrushundi et al., ITCS'19] also imply strong average-case hardness against ACC\u2070 under the uniform distribution. Crucial to these results is the use of non-black-box hardness amplification techniques and the interplay between Majority (MAJ) and Approximate Linear Sum (SUM\u0303) gates. Roughly speaking, while a MAJ gate outputs 1 when the sum of the m input bits is at least m/2, a SUM\u0303 gate computes a real-valued bounded weighted sum of the input bits and outputs 1 (resp. 0) if the sum is close to 1 (resp. close to 0), with the promise that one of the two cases always holds. As part of our framework, we explore ideas introduced in [Chen and Ren, STOC'20] to show that, for the purpose of proving lower bounds, a top layer MAJ gate is equivalent to a (weaker) SUM\u0303 gate. Motivated by this result, we extend the algorithmic method and establish stronger lower bounds against bounded-depth circuits with layers of MAJ and SUM\u0303 gates. Among them, we prove that: - Lower bound: NQP does not admit fixed quasi-polynomial size MAJ\u2218SUM\u0303\u2218ACC\u2070\u2218THR circuits. This is the first explicit lower bound against circuits with distinct layers of MAJ, SUM\u0303, and THR gates. Consequently, if the aforementioned equivalence between MAJ and SUM\u0303 as a top gate can be extended to intermediate layers, long sought-after lower bounds against the class THR\u2218THR of depth-2 polynomial-size threshold circuits would follow."}}
{"id": "UcP5NdKeB2V", "cdate": 1609459200000, "mdate": 1682356419366, "content": {"title": "Inverse-exponential correlation bounds and extremely rigid matrices from a new derandomized XOR lemma", "abstract": "In this work we prove that there is a function f \u2208 E NP such that, for every sufficiently large n and d = \u221an/logn, fn (f restricted to n-bit inputs) cannot be (1/2 + 2\u2212d)-approximated by F2-polynomials of degree d. We also observe that a minor improvement (e.g., improving d to n1/2+\u03b5 for any \u03b5 > 0) over our result would imply E NP cannot be computed by depth-3 AC0-circuits of 2n1/2 + \u03b5 size, which is a notoriously hard open question in complexity theory. Using the same proof techniques, we are also able to construct extremely rigid matrices over F2 in P NP. More specifically, we show that for every constant \u03b5 \u2208 (0,1), there is a P NP algorithm which on input 1n outputs an n\u00d7 n F2-matrix Hn satisfying RHn(2log1 \u2212 \u03b5 n) \u2265 (1/2 \u2212 exp(\u2212log2/3 \u00b7 \u03b5 n) ) \u00b7 n2, for every sufficiently large n. This improves the recent P NP constructions of rigid matrices in [Alman and Chen, FOCS 2019] and [Bhangale et al., FOCS 2020], which only give \u03a9(n2) rigidity. The key ingredient in the proof of our new results is a new derandomized XOR lemma based on approximate linear sums, which roughly says that given an n-input function f which cannot be 0.99-approximated by certain linear sum of s many functions in F within \u21131-distance, one can construct a new function Ampf with O(n) input bits, which cannot be (1/2+s\u03a9(1))-approximated by F-functions. Taking F to be a function collection containing low-degree F2-polynomials or low-rank F2-matrices, our results are then obtained by first using the algorithmic method to construct a function which is weakly hard against linear sums of F in the above sense, and then applying the derandomized XOR lemma to f. We obtain our new derandomized XOR lemma by giving a generalization of the famous hardcore lemma by Impagliazzo. Our generalization in some sense constructs a non-Boolean hardcore of a weakly hard function f with respect to F-functions, from the weak inapproximability of f by any linear sum of F with bounded \u2113p-norm. This generalization recovers the original hardcore lemma by considering the \u2113\u221e-norm. Surprisingly, when we switch to the \u21131-norm, we immediately rediscover Levin\u2019s proof of Yao\u2019s XOR Lemma. That is, these first two proofs of Yao\u2019s XOR Lemma can be unified with our new perspective. For proving the correlation bounds, our new derandomized XOR lemma indeed works with the \u21134/3-norm."}}
{"id": "06hYfw_G5Vd", "cdate": 1577836800000, "mdate": 1682356418891, "content": {"title": "Almost-Everywhere Circuit Lower Bounds from Non-Trivial Derandomization", "abstract": "In certain complexity-theoretic settings, it is notoriously difficult to prove complexity separations which hold almost everywhere, i.e., for all but finitely many input lengths. For example, a classical open question is whether NEXP is contained in i.o.-NP; that is, it is open whether nondeterministic exponential time computation can be simulated on infinitely many input lengths by an NP algorithm. This difficulty also applies to Williams' algorithmic method for circuit lower bounds [Williams, J. ACM 2014]. [Murray and Williams, STOC 2018] proved that nondeterminstic quasi-polynomial time is not contained in ACC^0, while it remained an open problem to show that E^NP (2^O(n) time with an NP oracle) is not contained in i.o.-ACC^0. In this paper, we show how many infinitely-often circuit lower bounds proved by the algorithmic method can be adapted to establish almost-everywhere lower bounds. First, we show there is a function f in E^NP such that, for all sufficiently large input lengths n, f cannot be (1/2+exp(-n <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u2227</sup> e))-approximated by exp(n^e)-size ACC^0 circuits on inputs of length n (for all small e), improving lower bounds in [Chen and Ren, STOC 2020] and [Viola, ECCC 2020]. Second, we construct rigid matrices in P^NP for all but finitely many inputs, rather than infinitely often as in [Alman and Chen, FOCS 2019] and [Bhangale et al. 2020]. Third, we show there is a positive c such that E^NP has constant-error probabilistic degree at least cn/(log^2 n) for all large enough n, improving an infinitely-often separation by [Viola, ECCC 2020]. Our key to proving almost-everywhere worst-case lower bounds is a new \u201cconstructive\u201d proof of an NTIME hierarchy theorem proved by [Fortnow and Santhanam, CCC 2016], where we show for every \u201cweak\u201d nondeterminstic algorithm, a \u201crefuter algorithm\u201d exists that can construct \u201cbad\u201d inputs for the hard language. We use this refuter algorithm to construct an almost-everywhere hard function. To extend our lower bounds to the average case, we prove a new XOR Lemma based on approximate linear sums, and combine it with PCP of proximity ideas developed in [Chen and Williams, CCC 2019] and [Chen and Ren, STOC 2020]. As a byproduct of our new XOR Lemma, we obtain a nondeterministic pseudorandom generator for poly-size ACC^0 circuits with seed length polylog(n), which resolves an open question in [Chen and Ren, STOC 2020]."}}
