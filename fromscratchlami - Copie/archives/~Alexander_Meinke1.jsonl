{"id": "QxIXCVYJ2WP", "cdate": 1654348673906, "mdate": null, "content": {"title": "Classifiers Should Do Well Even on Their Worst Classes", "abstract": "The performance of a vision classifier on a given test set is usually measured by its accuracy. For reliable machine learning systems, however, it is important to avoid the existence of areas of the input space where they fail severely. To reflect this, we argue, that a single number does not provide a complete enough picture even for a fixed test set, as there might be particular classes or subtasks where a model that is generally accurate performs unexpectedly poorly. Without using new data, we motivate and establish a wide selection of interesting worst-case performance metrics which can be evaluated besides accuracy on a given test set. Some of these metrics can be extended when a grouping of the original classes into superclasses is available, indicating if the model is exceptionally bad at handling inputs from one superclass."}}
{"id": "9ZWgrozGP0", "cdate": 1652737782037, "mdate": null, "content": {"title": "Provably Adversarially Robust Detection of Out-of-Distribution Data (Almost) for Free", "abstract": "The application of machine learning in safety-critical systems requires a reliable assessment of uncertainty.\nHowever, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data.\nEven if trained to be non-confident on OOD data, one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples.\nWe show that two previously published defenses can be broken by better adapted attacks, highlighting the importance of robustness guarantees around OOD data.\nSince the existing method for this task is hard to train and significantly limits accuracy, we construct a classifier that can simultaneously achieve provably adversarially robust OOD detection and high clean accuracy.\nMoreover, by slightly modifying the classifier's architecture our method provably avoids the asymptotic overconfidence problem of standard neural networks.\nWe provide code for all our experiments."}}
{"id": "qDx6DXD3Fzt", "cdate": 1632875727634, "mdate": null, "content": {"title": "Provably Robust Detection of Out-of-distribution Data (almost) for free", "abstract": "The application of machine learning in safety-critical systems requires a reliable assessment of uncertainy. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data. Even if trained to be non-confident on OOD data one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples. In this paper we propose a novel method that combines a certifiable OOD detector with a standard classifier from first principles into an OOD aware classifier. This way we achieve the best of two worlds: certifiably adversarially robust OOD detection, even for OOD samples close to the in-distribution, without loss in either prediction accuracy or detection performance for non-manipulated OOD data. Moreover, due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks."}}
{"id": "-BTmxCddppP", "cdate": 1632875676462, "mdate": null, "content": {"title": "Revisiting Out-of-Distribution Detection: A Simple Baseline is Surprisingly Effective", "abstract": "It is an important problem in trustworthy machine learning to recognize out-of-distribution (OOD) inputs which are inputs unrelated to the in-distribution task. Many out-of-distribution detection methods have been suggested in recent years. The goal of this paper is to recognize common objectives as well as to identify the implicit scoring functions of different OOD detection methods. In particular, we show that binary discrimination between in- and (different) out-distributions is equivalent to several different formulations of the OOD detection problem. When trained in a shared fashion with a standard classifier, this binary discriminator reaches an OOD detection performance similar to that of Outlier Exposure. Moreover, we show that the confidence loss which is used by Outlier Exposure has an implicit scoring function which differs in a non-trivial fashion from the theoretically optimal scoring function in the case where training and test out-distribution are the same, but is similar to the one used when training with an extra background class. In practice, when trained in exactly the same way, all these methods perform similarly and reach state-of-the-art OOD detection performance."}}
{"id": "Bup_R0u4xZ0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Certifiably Adversarially Robust Detection of Out-of-Distribution Data", "abstract": "Deep neural networks are known to be overconfident when applied to out-of-distribution (OOD) inputs which clearly do not belong to any class. This is a problem in safety-critical applications since a reliable assessment of the uncertainty of a classifier is a key property, allowing to trigger human intervention or to transfer into a safe state. In this paper, we are aiming for certifiable worst case guarantees for OOD detection by enforcing not only low confidence at the OOD point but also in an $l_\\infty$-ball around it. For this purpose, we use interval bound propagation (IBP) to upper bound the maximal confidence in the $l_\\infty$-ball and minimize this upper bound during training time. We show that non-trivial bounds on the confidence for OOD data generalizing beyond the OOD dataset seen at training time are possible. Moreover, in contrast to certified adversarial robustness which typically comes with significant loss in prediction performance, certified guarantees for worst case OOD detection are possible without much loss in accuracy."}}
{"id": "ByxGkySKwH", "cdate": 1569439450243, "mdate": null, "content": {"title": "Towards neural networks that provably know when they don't know", "abstract": "It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the \ntraining data. Thus, ReLU networks do not know when they don't know. However, this is a highly important property in safety\ncritical applications. In the context of out-of-distribution detection (OOD) there have been a number of proposals to mitigate this problem but none of them are able to make any mathematical guarantees. In this paper we propose a new approach to OOD which overcomes both problems. Our approach can be used with ReLU networks and provides provably low confidence predictions far away from the training data as well as the first certificates for low confidence predictions in a neighborhood of an out-distribution point. In the experiments we show that state-of-the-art methods fail in this worst-case setting whereas our model can guarantee its performance while retaining state-of-the-art OOD performance."}}
