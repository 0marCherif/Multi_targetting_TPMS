{"id": "A7O7Fl5Qo9W", "cdate": 1652737612820, "mdate": null, "content": {"title": "On the Sample Complexity of Stabilizing LTI Systems on a Single Trajectory", "abstract": "Stabilizing an unknown dynamical system is one of the central problems in control theory. In this paper, we study the sample complexity of the learn-to-stabilize problem in Linear Time-Invariant (LTI) systems on a single trajectory. Current state-of-the-art approaches require a sample complexity linear in $n$, the state dimension, which incurs a state norm that blows up exponentially in $n$. We propose a novel algorithm based on spectral decomposition that only needs to learn ``a small part'' of the dynamical matrix acting on its unstable subspace. We show that, under proper assumptions, our algorithm stabilizes an LTI system on a single trajectory with $O(k \\log n)$ samples, where $k$ is the instability index of the system. This represents the first sub-linear sample complexity result for the stabilization of LTI systems under the regime when $k = o(n)$."}}
{"id": "jFVfKsmKa-", "cdate": 1652737593980, "mdate": null, "content": {"title": "Bounded-Regret MPC via Perturbation Analysis: Prediction Error, Constraints, and Nonlinearity", "abstract": "We study Model Predictive Control (MPC) and propose a general analysis pipeline to bound its dynamic regret. The pipeline first requires deriving a perturbation bound for a finite-time optimal control problem. Then, the perturbation bound is used to bound the per-step error of MPC, which leads to a bound on the dynamic regret. Thus, our pipeline reduces the study of MPC to the well-studied problem of perturbation analysis, enabling the derivation of regret bounds of MPC under a variety of settings. To demonstrate the power of our pipeline, we use it to generalize existing regret bounds on MPC in linear time-varying (LTV) systems to incorporate prediction errors on costs, dynamics, and disturbances. Further, our pipeline leads to regret bounds on MPC in systems with nonlinear dynamics and constraints."}}
{"id": "NHX9w7ex3fW", "cdate": 1621629984402, "mdate": null, "content": {"title": "Multi-Agent Reinforcement Learning in Stochastic Networked Systems", "abstract": "We study multi-agent reinforcement learning (MARL) in a stochastic network of agents. The objective is to find localized policies that maximize the (discounted) global reward. In general, scalability is a challenge in this setting because the size of the global state/action space can be exponential in the number of agents. Scalable algorithms are only known in cases where dependencies are static, fixed and local, e.g., between neighbors in a fixed, time-invariant underlying graph. In this work, we propose a Scalable Actor Critic framework that applies in settings where the dependencies can be non-local and stochastic, and provide a finite-time error bound that shows how the convergence rate depends on the speed of information spread in the network.  Additionally, as a byproduct of our analysis, we obtain novel finite-time convergence results for a general stochastic approximation scheme and for temporal difference learning with state aggregation, which apply beyond the setting of MARL in networked systems."}}
{"id": "xwGeq7I4Opv", "cdate": 1621629887905, "mdate": null, "content": {"title": "Perturbation-based Regret Analysis of Predictive Control in Linear Time Varying Systems", "abstract": "We study predictive control in a setting where the dynamics are time-varying and linear, and the costs are time-varying and well-conditioned. At each time step, the controller receives the exact predictions of costs, dynamics, and disturbances for the future $k$ time steps. We show that when the prediction window $k$ is sufficiently large, predictive control is input-to-state stable and achieves a dynamic regret of $O(\\lambda^k T)$, where $\\lambda < 1$ is a positive constant. This is the first dynamic regret bound on the predictive control of linear time-varying systems. We also show a variation of predictive control obtains the first competitive bound for the control of linear time-varying systems:  $1 + O(\\lambda^k)$. Our results are derived using a novel proof framework based on a perturbation bound that characterizes how a small change to the system parameters impacts the optimal trajectory."}}
{"id": "BaWQZQqxOLg", "cdate": 1621295801057, "mdate": null, "content": {"title": "Stable Online Control of Linear Time-Varying Systems", "abstract": "In this work, we study the interaction of strategic agents in continuous action Cournot games with limited information feedback. Cournot game is the essential market model for many socio-economic systems where agents learn and compete without the full knowledge of the system or each other. We consider the dynamics of the policy gradient algorithm, which is a widely adopted continuous control reinforcement learning algorithm, in concave Cournot games. We prove the convergence of policy gradient dynamics to the Nash equilibrium when the price function is linear or the number of agents is two. This is the first result (to the best of our knowledge) on the convergence property of learning algorithms with continuous action spaces that do not fall in the no-regret class."}}
{"id": "osscr3JUASl", "cdate": 1609459200000, "mdate": null, "content": {"title": "Stable Online Control of Linear Time-Varying Systems", "abstract": "Linear time-varying (LTV) systems are widely used for modeling real-world dynamical systems due to their generality and simplicity. Providing stability guarantees for LTV systems is one of the central problems in control theory. However, existing approaches that guarantee stability typically lead to significantly sub-optimal cumulative control cost in online settings where only current or short-term system information is available. In this work, we propose an efficient online control algorithm, COvariance Constrained Online Linear Quadratic (COCO-LQ) control, that guarantees input-to-state stability for a large class of LTV systems while also minimizing the control cost. The proposed method incorporates a state covariance constraint into the semi-definite programming (SDP) formulation of the LQ optimal controller. We empirically demonstrate the performance of COCO-LQ in both synthetic experiments and a power system frequency control example."}}
{"id": "z345jRZeTnx", "cdate": 1591623750808, "mdate": null, "content": {"title": "Scalable Reinforcement Learning of Localized Policies for Multi-Agent Networked Systems", "abstract": "We study reinforcement learning (RL) in a setting with a network of agents whose states and actions interact in a local manner where the objective is to find localized policies such that the (discounted) global reward is maximized. A fundamental challenge in this setting is that the state-action space size scales exponentially in the number of agents, rendering the problem intractable for large networks. In this paper, we propose a Scalable Actor Critic (SAC) framework that exploits the network structure and finds a localized policy that is an $O(\\rho^\\kappa)$-approximation of a stationary point of the objective for some $\\rho\\in(0,1)$, with complexity that scales with the local state-action space size of the largest $\\kappa$-hop neighborhood of the network. "}}
{"id": "QsSIUAp1OmC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Accelerated Distributed Nesterov Gradient Descent", "abstract": "This paper considers the distributed optimization problem over a network, where the objective is to optimize a global function formed by a sum of local functions, using only local computation and communication. We develop an accelerated distributed Nesterov gradient descent method. When the objective function is convex and Lsmooth, we show that it achieves a O( 1/ <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">t</sub> 1 .4 -\u03f5) convergence rate for all \u03f5 \u2208 (0, 1.4). We also show the convergence rate can be improved to O(1/ <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">t</sub> 2) if the objective function is a composition of a linear map and a strongly convex and smooth function. When the objective function is \u03bc-strongly convex and L-smooth, we show that it achieves a linear convergence rate of O([1 - C(\u03bc/L )5/7] <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">t</sup> ), whereL\u03bc is the condition number of the objective, and C > 0 is some constant that does not depend on L/\u03bc ."}}
{"id": "BUDNHkwq5et", "cdate": 1577836800000, "mdate": null, "content": {"title": "Combining Model-Based and Model-Free Methods for Nonlinear Control: A Provably Convergent Policy Gradient Approach", "abstract": "Model-free learning-based control methods have seen great success recently. However, such methods typically suffer from poor sample complexity and limited convergence guarantees. This is in sharp contrast to classical model-based control, which has a rich theory but typically requires strong modeling assumptions. In this paper, we combine the two approaches to achieve the best of both worlds. We consider a dynamical system with both linear and non-linear components and develop a novel approach to use the linear model to define a warm start for a model-free, policy gradient method. We show this hybrid approach outperforms the model-based controller while avoiding the convergence issues associated with model-free approaches via both numerical experiments and theoretical analyses, in which we derive sufficient conditions on the non-linear component such that our approach is guaranteed to converge to the (nearly) global optimal controller."}}
{"id": "AuPeQhC1oSy", "cdate": 1577836800000, "mdate": null, "content": {"title": "Finite-Time Analysis of Asynchronous Stochastic Approximation and $Q$-Learning", "abstract": "We consider a general asynchronous Stochastic Approximation (SA) scheme featuring a weighted infinity-norm contractive operator, and prove a bound on its finite-time convergence rate on a single trajectory. Additionally, we specialize the result to asynchronous $Q$-learning. The resulting bound matches the sharpest available bound for synchronous $Q$-learning, and improves over previous known bounds for asynchronous $Q$-learning."}}
