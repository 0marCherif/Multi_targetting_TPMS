{"id": "mamv07NQWk", "cdate": 1652737839719, "mdate": null, "content": {"title": "Regret Bounds for Multilabel Classification in Sparse Label Regimes", "abstract": "Multi-label classification (MLC) has wide practical importance, but the theoretical understanding of its statistical properties is still limited. As an attempt to fill this gap, we thoroughly study upper and lower regret bounds for two canonical MLC performance measures, Hamming loss and Precision@$\\kappa$. We consider two different statistical and algorithmic settings, a non-parametric setting tackled by plug-in classifiers \\`a la $k$-nearest neighbors, and a parametric one tackled by empirical risk minimization operating on surrogate loss functions. For both, we analyze the interplay between a natural MLC variant of the low noise assumption, widely studied in binary classification, and the label sparsity, the latter being a natural property of large-scale MLC problems. We show that those conditions are crucial in improving the bounds, but the way they are tangled is not obvious, and also different across the two settings."}}
{"id": "M7emZFOLbH", "cdate": 1621629816782, "mdate": null, "content": {"title": "Identity testing for Mallows model", "abstract": "In this paper, we devise identity tests for ranking data that is generated from Mallows model both in the \\emph{asymptotic} and \\emph{non-asymptotic} settings. First we consider the case when the central ranking is known, and devise two algorithms for testing the spread parameter of the Mallows model. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test in the asymptotic setting and then converting it into a sample-optimal non-asymptotic identity test. The resulting test is, however, impractical even for medium sized data, because it requires computing the distribution of the sufficient statistic. The second non-asymptotic test is derived from an optimal learning algorithm for the Mallows model. This test is both easy to compute and is sample-optimal for a wide range of parameters. Next, we consider testing Mallows models for the unknown central ranking case. This case can be tackled in the asymptotic setting by introducing a bias that exponentially decays with the sample size. We support all our findings with extensive numerical experiments and show that the proposed tests scale gracefully with the number of items to be ranked.\n"}}
{"id": "iNN14u6FHV7", "cdate": 1599925863539, "mdate": null, "content": {"title": "Learning to Crawl", "abstract": "Web crawling is the problem of keeping a cache of webpages fresh, i.e., having the most recent copy available when a page is requested. This problem is usually coupled with the natural restriction that the bandwidth available to the web crawler is limited. The corresponding optimization problem was solved optimally by Azar et al. [2018] under the assumption that, for each webpage, both the elapsed time between two changes and the elapsed time between two requests follow a Poisson distribution with known parameters. In this paper, we study the same control problem but under the assumption that the change rates are unknown a priori, and thus we need to estimate them in an online fashion using only partial observations (i.e., single-bit signals indicating whether the page has changed since the last refresh). As a point of departure, we characterise the conditions under which one can solve the problem with such partial observability. Next, we propose a practical estimator and compute confidence intervals for it in terms of the elapsed time between the observations. Finally, we show that the explore-and-commit algorithm achieves an \ue23b(T\u203e\u203e\u221a) regret with a carefully chosen exploration horizon. Our simulation study shows that our online policy scales well and achieves close to optimal performance for a wide range of the parameters."}}
{"id": "ZiNr976v2r5", "cdate": 1599925669018, "mdate": null, "content": {"title": "The information-theoretic value of unlabeled data in semi-supervised learning", "abstract": "We quantify the separation between the numbers of labeled examples required to learn in two settings: Settings with and without the knowledge of the distribution of the unlabeled data. More specifically, we prove a separation by \u0398(logn) multiplicative factor for the class of projections over the Boolean hypercube of dimension n. We prove that there is no separation for the class of all functions on domain of any size.\nLearning with the knowledge of the distribution (a.k.a. fixed-distribution learning) can be viewed as an idealized scenario of semi-supervised learning where the number of unlabeled data points is so great that the unlabeled distribution is known exactly. For this reason, we call the separation the value of unlabeled data."}}
{"id": "73IzOYsBso", "cdate": 1599925328894, "mdate": null, "content": {"title": "Finite Sample Analyses for TD(0) with Function Approximation", "abstract": "TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such results. Existing convergence rates for Temporal Difference (TD) methods apply only to somewhat modified versions, e.g., projected variants or ones where stepsizes depend on unknown problem parameters. Our analyses obviate these artificial alterations by exploiting strong properties of TD(0). We provide convergence rates both in expectation and with high-probability. The two are obtained via different approaches that use relatively unknown, recently developed stochastic approximation techniques."}}
{"id": "bqVKC5x2_at", "cdate": 1599925183133, "mdate": null, "content": {"title": "Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning", "abstract": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated using distinct stepsizes. In this work, we develop a novel recipe for their finite sample analysis. Using this, we provide a concentration bound, which is the first such result for a two-timescale SA. The type of bound we obtain is known as `lock-in probability'. We also introduce a new projection scheme, in which the time between successive projections increases exponentially. This scheme allows one to elegantly transform a lock-in probability into a convergence rate result for projected two-timescale SA. From this latter result, we then extract key insights on stepsize selection. As an application, we finally obtain convergence rates for the projected two-timescale RL algorithms GTD(0), GTD2, and TDC."}}
{"id": "U4PhwYztDx6", "cdate": 1599924983605, "mdate": null, "content": {"title": "Distributed Clustering of Linear Bandits in Peer to Peer Networks", "abstract": "We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art."}}
{"id": "qJ5QLUZWmOm", "cdate": 1599924725352, "mdate": null, "content": {"title": "A Tale of Two-Timescale Reinforcement Learning with the Tightest Finite-Time Bound", "abstract": "Policy evaluation in reinforcement learning is often conducted using two-timescale stochastic approximation, which results in various gradient temporal difference methods such as GTD(0), GTD2, and TDC. Here, we provide convergence rate bounds for this suite of algorithms. Algorithms such as these have two iterates, \u03b8n and wn, which are updated using two distinct stepsize sequences, \u03b1n and \u03b2n, respectively. Assuming \u03b1n=n\u2212\u03b1 and \u03b2n=n\u2212\u03b2 with 1>\u03b1>\u03b2>0, we show that, with high probability, the two iterates converge to their respective solutions \u03b8\u2217 and w\u2217 at rates given by \u2016\u03b8n\u2212\u03b8\u2217\u2016=O\u0303 (n\u2212\u03b1/2) and \u2016wn\u2212w\u2217\u2016=O\u0303 (n\u2212\u03b2/2); here, O\u0303  hides logarithmic terms. Via comparable lower bounds, we show that these bounds are, in fact, tight. To the best of our knowledge, ours is the first finite-time analysis which achieves these rates. While it was known that the two timescale components decouple asymptotically, our results depict this phenomenon more explicitly by showing that it in fact happens from some finite time onwards. Lastly, compared to existing works, our result applies to a broader family of stepsizes, including non-square summable ones."}}
