{"id": "hVAK0cgiWrU", "cdate": 1661329137220, "mdate": null, "content": {"title": "Toward Certified Robustness Against Real-World Distribution Shifts", "abstract": "We consider the problem of certifying the robustness of deep neural networks against real-world distribution shifts.  To do so, we bridge the gap between hand-crafted specifications and realistic deployment settings by considering a neural-symbolic verification framework in which generative models are trained to learn perturbations from data and specifications are defined with respect to the output of these learned models.  A pervasive challenge arising from this setting is that although S-shaped activations (e.g., sigmoid, tanh) are common in the last layer of deep generative models, existing verifiers cannot tightly approximate S-shaped activations.  To address this challenge, we propose a general meta-algorithm for handling S-shaped activations which leverages classical notions of counter-example-guided abstraction refinement. The key idea is to ``lazily'' refine the abstraction of S-shaped functions to exclude spurious counter-examples found in the previous abstraction, thus guaranteeing progress in the verification process while keeping the state-space small.  For networks with sigmoid activations, we show that our technique outperforms state-of-the-art verifiers on certifying robustness against both canonical adversarial perturbations and numerous real-world distribution shifts.  Furthermore, experiments on the MNIST and CIFAR-10 datasets show that distribution-shift-aware algorithms have significantly higher certified robustness against distribution shifts."}}
{"id": "6FkSHynJr1", "cdate": 1652737587461, "mdate": null, "content": {"title": "Probable Domain Generalization via Quantile Risk Minimization", "abstract": "Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging data drawn from multiple related training distributions or domains. To achieve this, DG is commonly formulated as an average- or worst-case problem over the set of possible domains. However, predictors that perform well on average lack robustness while predictors that perform well in the worst case tend to be overly-conservative. To address this, we propose a new probabilistic framework for DG where the goal is to learn predictors that perform well with high probability. Our key idea is that distribution shifts seen during training should inform us of probable shifts at test time, which we realize by explicitly relating training and test domains as draws from the same underlying meta-distribution. To achieve probable DG, we propose a new optimization problem called Quantile Risk Minimization (QRM). By minimizing the $\\alpha$-quantile of predictor's risk distribution over domains, QRM seeks predictors that perform well with probability $\\alpha$. To solve QRM in practice, we propose the Empirical QRM (EQRM) algorithm and provide: (i) a generalization bound for EQRM; and (ii) the conditions under which EQRM recovers the causal predictor as $\\alpha \\to 1$. In our experiments, we introduce a more holistic quantile-focused evaluation protocol for DG, and demonstrate that EQRM outperforms state-of-the-art baselines on datasets from WILDS and DomainBed."}}
{"id": "zC5CHG3uUi1", "cdate": 1640995200000, "mdate": 1681649769949, "content": {"title": "Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks", "abstract": ""}}
{"id": "fWvtMAxWzEI", "cdate": 1640995200000, "mdate": 1681649769868, "content": {"title": "Probabilistically Robust Learning: Balancing Average and Worst-case Performance", "abstract": ""}}
{"id": "VduNcSrbphG", "cdate": 1640995200000, "mdate": 1662093984473, "content": {"title": "Toward Certified Robustness Against Real-World Distribution Shifts", "abstract": "We consider the problem of certifying the robustness of deep neural networks against real-world distribution shifts. To do so, we bridge the gap between hand-crafted specifications and realistic deployment settings by proposing a novel neural-symbolic verification framework, in which we train a generative model to learn perturbations from data and define specifications with respect to the output of the learned model. A unique challenge arising from this setting is that existing verifiers cannot tightly approximate sigmoid activations, which are fundamental to many state-of-the-art generative models. To address this challenge, we propose a general meta-algorithm for handling sigmoid activations which leverages classical notions of counter-example-guided abstraction refinement. The key idea is to \"lazily\" refine the abstraction of sigmoid functions to exclude spurious counter-examples found in the previous abstraction, thus guaranteeing progress in the verification process while keeping the state-space small. Experiments on the MNIST and CIFAR-10 datasets show that our framework significantly outperforms existing methods on a range of challenging distribution shifts."}}
{"id": "TJfCrG5H_ir", "cdate": 1640995200000, "mdate": 1681649769557, "content": {"title": "Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks", "abstract": ""}}
{"id": "SdZZm1HBBlq", "cdate": 1640995200000, "mdate": 1645724890750, "content": {"title": "Probabilistically Robust Learning: Balancing Average- and Worst-case Performance", "abstract": "Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called probabilistic robustness that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations. From a theoretical point of view, this framework overcomes the trade-offs between the performance and the sample-complexity of worst-case and average-case learning. From a practical point of view, we propose a novel algorithm based on risk-aware optimization that effectively balances average- and worst-case performance at a considerably lower computational cost relative to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate the advantages of this framework on the spectrum from average- to worst-case robustness."}}
{"id": "Q3YZ9Na6dz", "cdate": 1640995200000, "mdate": 1681649770228, "content": {"title": "Do Deep Networks Transfer Invariances Across Classes?", "abstract": ""}}
{"id": "CwtW5M3GVd", "cdate": 1640995200000, "mdate": 1668537822106, "content": {"title": "Do deep networks transfer invariances across classes?", "abstract": "In order to generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input's class. Many problems have \"class-agnostic\" nuisance transformations that..."}}
{"id": "3or3Vs67uu", "cdate": 1640995200000, "mdate": 1681649769807, "content": {"title": "Probable Domain Generalization via Quantile Risk Minimization", "abstract": ""}}
