{"id": "IkWNNgRcLYq", "cdate": 1675970198175, "mdate": null, "content": {"title": "Grounding Graph Network Simulators using Physical Sensor Observations", "abstract": "Physical simulations that accurately model reality are crucial for many engineering disciplines such as mechanical engineering and robotic motion planning. In recent years, learned Graph Network Simulators produced accurate mesh-based simulations while requiring only a fraction of the computational cost of traditional simulators. As these predictors have to simulate complex physical systems from only an initial state, they exhibit a high error accumulation for long-term predictions. In this work, we integrate sensory information to $\\textit{ground}$ Graph Network Simulators on real world observations in the form of point clouds. The resulting model allows for accurate predictions over longer time horizons, even under uncertainties in the simulation, such as unknown material properties. "}}
{"id": "N5yW7xZZS6b", "cdate": 1675970197869, "mdate": null, "content": {"title": "Swarm Reinforcement Learning for Adaptive Mesh Refinement", "abstract": "Adaptive Mesh Refinement (AMR) is crucial for mesh-based simulations, as it allows for dynamically adjusting the resolution of a mesh to trade off computational cost with the simulation accuracy. Yet, existing methods for AMR either use task-dependent heuristics, expensive error estimators, or do not scale well to larger meshes or more complex problems. In this paper, we formalize AMR as a Swarm Reinforcement Learning problem, viewing each element of a mesh as part of a collaborative system of simple and homogeneous agents. We combine this problem formulation with a novel agent-wise reward function and Graph Neural Networks, allowing us to learn reliable and scalable refinement strategies on arbitrary systems of equations. We experimentally demonstrate the effectiveness of our approach in improving the accuracy and efficiency of complex simulations. Our results show that we outperform learned baselines and achieve a refinement quality that is on par with a traditional error-based AMR refinement strategy without requiring error indicators during inference."}}
{"id": "bhfp5GlDtGe", "cdate": 1663850406406, "mdate": null, "content": {"title": "Adversarial Imitation Learning with Preferences", "abstract": "Designing an accurate and explainable reward function for many Reinforcement Learning tasks is a cumbersome and tedious process. \nInstead, learning policies directly from the feedback of human teachers naturally integrates human domain knowledge into the policy optimization process. \nHowever, different feedback modalities, such as demonstrations and preferences, provide distinct benefits and disadvantages. For example, demonstrations convey a lot of information about the task but are often hard or costly to obtain from real experts while preferences typically contain less information but are in most cases cheap to generate. \nHowever, existing methods centered around human feedback mostly focus on a single teaching modality, causing them to miss out on important training data while making them less intuitive to use.\nIn this paper we propose a novel method for policy learning that incorporates two different feedback types, namely \\emph{demonstrations} and \\emph{preferences}. \nTo this end, we make use of the connection between discriminator training and density ratio estimation to incorporate preferences into the popular Adversarial Imitation Learning paradigm. \nThis insight allows us to express loss functions over both demonstrations and preferences in a unified framework.\nBesides expert demonstrations, we are also able to learn from imperfect ones and combine them with preferences to achieve improved task performance.\nWe experimentally validate the effectiveness of combining both preferences and demonstrations on common benchmarks and also show that our method can efficiently learn challenging robot manipulation tasks."}}
{"id": "jsZsEd8VEY", "cdate": 1663850392085, "mdate": null, "content": {"title": "Grounding Graph Network Simulators using Physical Sensor Observations", "abstract": "Physical simulations that accurately model reality are crucial for many engineering disciplines such as mechanical engineering and robotic motion planning. In recent years, learned Graph Network Simulators produced accurate mesh-based simulations while requiring only a fraction of the computational cost of traditional simulators. Yet, the resulting predictors are confined to learning from data generated by existing mesh-based simulators and thus cannot include real world sensory information such as point cloud data. As these predictors have to simulate complex physical systems from only an initial state, they exhibit a high error accumulation for long-term predictions. In this work, we integrate sensory information to ground Graph Network Simulators on real world observations. In particular, we predict the mesh state of deformable objects by utilizing point cloud data. The resulting model allows for accurate predictions over longer time horizons, even under uncertainties in the simulation, such as unknown material properties. Since point clouds are usually not available for every time step, especially in online settings, we employ an imputation-based model. The model can make use of such additional information only when provided, and resorts to a standard Graph Network Simulator, otherwise. We experimentally validate our approach on a suite of prediction tasks for mesh-based interactions between soft and rigid bodies. Our method results in utilization of additional point cloud information to accurately predict stable simulations where existing Graph Network Simulators fail."}}
{"id": "Mzqn__AxyA6", "cdate": 1655376336269, "mdate": null, "content": {"title": "Inferring Versatile Behavior from Demonstrations by Matching Geometric Descriptors", "abstract": "Humans intuitively solve tasks in versatile ways, varying their behavior in terms of trajectory-based planning and for individual steps. Thus, they can easily generalize and adapt to new and changing environments. Current Imitation Learning algorithms often only consider unimodal expert demonstrations and act in a state-action-based setting, making it difficult for them to imitate human behavior in case of versatile demonstrations. Instead, we combine a mixture of movement primitives with a distribution matching objective to learn versatile behaviors that match the expert\u2019s behavior and versatility. To facilitate generalization to novel task configurations, we do not directly match the agent\u2019s and expert\u2019s trajectory distributions but rather work with concise geometric descriptors which generalize well to unseen task configurations. We empirically validate our method on various robot tasks using versatile human demonstrations and compare to imitation learning algorithms in a state-action setting as well as a trajectory-based setting. We find that the geometric descriptors greatly help in generalizing to new task configurations and that combining them with our distribution-matching objective is crucial for representing and reproducing versatile behavior."}}
