{"id": "UgBYfuBt9c", "cdate": 1695933757316, "mdate": 1695933757316, "content": {"title": "The Dynamics of Functional Diversity throughout Neural Network Training", "abstract": "Deep ensembles offer reduced generalization error and improved predictive uncertainty estimates.\nThese performance gains are attributed to functional diversity among the component models that\nmake up the ensembles: ensemble performance increases with the diversity of the components. A\nstandard way to generate a diversity of components is to train multiple networks on the same data,\nusing different minibatch orders, augmentations, etc. In this work, we focus on how and when this\ntype of diversity in the learned predictor decreases throughout training.\nIn order to study the diversity of networks still accessible via SGD after t iterations, we first train a\nsingle network for t iterations, then duplicate the state of the optimizer and finish the remainder of\ntraining k times, with independent randomness (minibatches, augmentations, etc) for each duplicated\nnetwork. The result is k distinct networks whose training has been coupled for t iterations. We use\nthis methodology\u2014recently exploited for k = 2 to study linear mode connectivity\u2014to construct a\nnovel probe for studying diversity.\nWe find that coupling k for even a few epochs severely restricts the diversity of functions accessible\nby SGD, as measured by the KL divergence between the predicted label distributions as well as the\ncalibration and test error of k-ensembles. We also find that the number of forgetting events [1] drops\noff rapidly.\nThe amount of independent training time decreases with coupling time t however. To control for this\nconfounder, we study extending the number of iterations of high-learning-rate optimization for an\nadditional t iterations post-coupling. We find that this does not restore functional diversity.\nWe also study how functional diversity is affected by retraining after reinitializing the weights in some\nlayers. We find that we recover significantly more diversity by reinitializing layers closer to the input\nlayer, compared to reinitializing layers closer to the output. In this case, we see that reinitialization\nupsets linear mode connectivity. This observation agrees with the performance improvements seen by\narchitectures that share the core of a network but train multiple instantiations of the input layers [2]."}}
{"id": "2F8ktRFqvnM", "cdate": 1653750178554, "mdate": null, "content": {"title": "Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty", "abstract": "A recent line of work has identified a so-called \u2018lazy regime\u2019 where a deep network can be well approximated by its linearization around initialization throughout training. Here we investigate the comparative effect of the lazy (linear) and feature\nlearning (non-linear) regimes on subgroups of examples based on their difficulty. Specifically, we show that easier examples are given more weight in feature learning mode, resulting in faster training compared to more difficult ones. We illustrate this phenomenon across different ways to quantify example difficulty, including c-score, label noise, and in the presence of spurious correlations."}}
{"id": "R2AN-rz4j_X", "cdate": 1632875623131, "mdate": null, "content": {"title": "Continual Learning in Deep Networks: an Analysis of the Last Layer", "abstract": "We study how different output layers in a  deep neural network learn and forget in continual learning settings. The following three  factors  can affect catastrophic forgetting in the output layer: (1) weights modifications, (2) interference, and (3) projection drift. In this paper, our goal is to provide more insights into how changing the output layers may address (1) and (2).  Some potential solutions to those issues are proposed and evaluated here in  several continual learning scenarios. We show that the best-performing type of the output layer  depends on the data distribution drifts and/or the amount of data available. In particular, in some cases where a standard linear layer would fail, it turns out that changing  parameterization is sufficient in order to  achieve a significantly better performance, whithout introducing a continual-learning algorithm and instead using the standard SGD to train a model.  Our analysis and results  shed light on the dynamics of the output layer in continual learning scenarios,  and suggest a way of  selecting the best type of  output layer for a given scenario."}}
{"id": "ziGdL0BwPL", "cdate": 1603141809808, "mdate": null, "content": {"title": "Implicit Regularization via Neural Feature Alignment", "abstract": "We approach the problem of implicit regularization in deep learning from a geometrical viewpoint. We highlight a regularization effect induced by a dynamical alignment of the neural tangent features introduced by Jacot et al (2018), along a small number of task-relevant directions. This can be interpreted as a combined  feature selection and  compression mechanism. By extrapolating a new analysis of Rademacher complexity bounds for linear models,  we propose and study a new heuristic measure of complexity which captures this phenomenon, in terms of sequences of tangent kernel classes along the learning trajectories."}}
{"id": "wabe-NE8-AX", "cdate": 1601308249929, "mdate": null, "content": {"title": "NNGeometry: Easy and Fast Fisher Information Matrices and Neural Tangent Kernels in PyTorch", "abstract": "Fisher Information Matrices (FIM) and Neural Tangent Kernels (NTK) are useful tools in a number of diverse applications related to neural networks. Yet these theoretical tools are often difficult to implement using current libraries for practical size networks, given that they require per-example gradients, and a large amount of memory since they scale as the number of parameters (for the FIM) or the number of examples x cardinality of the output space (for the NTK). NNGeometry is a PyTorch library that offers a simple interface for computing various linear algebra operations such as matrix-vector products, trace, frobenius norm, and so on, where the matrix is either the FIM or the NTK, leveraging recent advances in approximating these matrices. We here present the library and motivate our design choices, then we demonstrate it on actual deep neural networks."}}
{"id": "jpm1AfJucwt", "cdate": 1601308055346, "mdate": null, "content": {"title": "Revisiting Loss Modelling for Unstructured Pruning", "abstract": "By removing parameters from deep neural networks, unstructured pruning methods aim at cutting down memory footprint and computational cost, while maintaining prediction accuracy. In order to tackle this otherwise intractable problem, many of these methods model the loss landscape using first or second order Taylor expansions to identify which parameters can be discarded. We revisit loss modelling for unstructured pruning: we show the importance of ensuring locality of the pruning steps, and systematically compare first and second order Taylor expansions. Finally, we show that better preserving the original network function does not necessarily transfer to better performing networks after fine-tuning, suggesting that only considering the impact of pruning on the loss might not be a sufficient objective to design good pruning criteria."}}
{"id": "ryVC6tkwG", "cdate": 1518472651847, "mdate": null, "content": {"title": "An Evaluation of Fisher Approximations Beyond Kronecker Factorization", "abstract": "We study two coarser approximations on top of a Kronecker factorization (K-FAC) of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks (CNNs). The first considers the activations (feature maps) as spatially uncorrelated while the second considers only correlations among groups of channels. Both variants yield a further block-diagonal approximation tailored for CNNs, which is much more efficient to compute and invert. Experiments on the VGG11 and ResNet50 architectures show the technique can substantially speed up both K-FAC and a baseline with Batch Normalization in wall-clock time, yielding faster convergence to similar or better generalization error."}}
{"id": "usT6Zc-VH_x", "cdate": 1514764800000, "mdate": null, "content": {"title": "An Evaluation of Fisher Approximations Beyond Kronecker Factorization", "abstract": "We study two coarser approximations on top of a Kronecker factorization (K-FAC) of the Fisher information matrix, to scale up Natural Gradient to deep and wide Convolutional Neural Networks (CNNs). The first considers the activations (feature maps) as spatially uncorrelated while the second considers only correlations among groups of channels. Both variants yield a further block-diagonal approximation tailored for CNNs, which is much more efficient to compute and invert. Experiments on the VGG11 and ResNet50 architectures show the technique can substantially speed up both K-FAC and a baseline with Batch Normalization in wall-clock time, yielding faster convergence to similar or better generalization error."}}
{"id": "RjS_oxEVJqA", "cdate": 1514764800000, "mdate": null, "content": {"title": "Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis", "abstract": "Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covari- ance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approxima- tions and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens &amp; Grosse, 2015; Grosse &amp; Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures."}}
{"id": "Bk-1T8b_Zr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis", "abstract": "Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covari- ance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approxima- tions and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens &amp; Grosse, 2015; Grosse &amp; Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures."}}
