{"id": "sQQpY0Uf3z", "cdate": 1672531200000, "mdate": 1682318364151, "content": {"title": "How To Guide Your Learner: Imitation Learning with Active Adaptive Expert Involvement", "abstract": "Imitation learning aims to mimic the behavior of experts without explicit reward signals. Passive imitation learning methods which use static expert datasets typically suffer from compounding error, low sample efficiency, and high hyper-parameter sensitivity. In contrast, active imitation learning methods solicit expert interventions to address the limitations. However, recent active imitation learning methods are designed based on human intuitions or empirical experience without theoretical guarantee. In this paper, we propose a novel active imitation learning framework based on a teacher-student interaction model, in which the teacher's goal is to identify the best teaching behavior and actively affect the student's learning process. By solving the optimization objective of this framework, we propose a practical implementation, naming it AdapMen. Theoretical analysis shows that AdapMen can improve the error bound and avoid compounding error under mild conditions. Experiments on the MetaDrive benchmark and Atari 2600 games validate our theoretical analysis and show that our method achieves near-expert performance with much less expert involvement and total sampling steps than previous methods. The code is available at https://github.com/liuxhym/AdapMen."}}
{"id": "sy_G_zJU4PU", "cdate": 1640995200000, "mdate": 1682318364326, "content": {"title": "Cascaded Algorithm Selection With Extreme-Region UCB Bandit", "abstract": "AutoML aims at best configuring learning systems automatically. It contains core subtasks of algorithm selection and hyper-parameter tuning. Previous approaches considered searching in the joint hyper-parameter space of all algorithms, which forms a huge but redundant space and causes an inefficient search. We tackle this issue in a <i>cascaded algorithm selection</i> way, which contains an upper-level process of algorithm selection and a lower-level process of hyper-parameter tuning for algorithms. While the lower-level process employs an <i>anytime</i> tuning approach, the upper-level process is naturally formulated as a multi-armed bandit, deciding which algorithm should be allocated one more piece of time for the lower-level tuning. To achieve the goal of finding the best configuration, we propose the <i>Extreme-Region Upper Confidence Bound</i> (ER-UCB) strategy. Unlike UCB bandits that maximize the mean of feedback distribution, ER-UCB maximizes the extreme-region of feedback distribution. We first consider stationary distributions and propose the ER-UCB-S algorithm that has <inline-formula><tex-math notation=\"LaTeX\">$O(K\\ln n)$</tex-math></inline-formula> regret upper bound with <inline-formula><tex-math notation=\"LaTeX\">$K$</tex-math></inline-formula> arms and <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math></inline-formula> trials. We then extend to non-stationary settings and propose the ER-UCB-N algorithm that has <inline-formula><tex-math notation=\"LaTeX\">$O(Kn^\\nu)$</tex-math></inline-formula> regret upper bound, where <inline-formula><tex-math notation=\"LaTeX\">$\\frac{2}{3}&lt;\\nu &lt;1$</tex-math></inline-formula> . Finally, empirical studies on synthetic and AutoML tasks verify the effectiveness of ER-UCB-S/N by their outperformance in corresponding settings."}}
{"id": "Bs8jLrxyUiJ", "cdate": 1640995200000, "mdate": 1682318364132, "content": {"title": "The Teaching Dimension of Regularized Kernel Learners", "abstract": "Teaching dimension (TD) is a fundamental theoretical property for understanding machine teaching algorithms. It measures the sample complexity of teaching a target hypothesis to a learner. The TD o..."}}
{"id": "5NGDfqIVAuJ", "cdate": 1640995200000, "mdate": 1682318364122, "content": {"title": "Hybrid Value Estimation for Off-policy Evaluation and Offline Reinforcement Learning", "abstract": "Value function estimation is an indispensable subroutine in reinforcement learning, which becomes more challenging in the offline setting. In this paper, we propose Hybrid Value Estimation (HVE) to reduce value estimation error, which trades off bias and variance by balancing between the value estimation from offline data and the learned model. Theoretical analysis discloses that HVE enjoys a better error bound than the direct methods. HVE can be leveraged in both off-policy evaluation and offline reinforcement learning settings. We, therefore, provide two concrete algorithms Off-policy HVE (OPHVE) and Model-based Offline HVE (MOHVE), respectively. Empirical evaluations on MuJoCo tasks corroborate the theoretical claim. OPHVE outperforms other off-policy evaluation methods in all three metrics measuring the estimation effectiveness, while MOHVE achieves better or comparable performance with state-of-the-art offline reinforcement learning algorithms. We hope that HVE could shed some light on further research on reinforcement learning from fixed data."}}
{"id": "Ba3odanehCw", "cdate": 1621629781374, "mdate": null, "content": {"title": "Regret Minimization Experience Replay in Off-Policy Reinforcement Learning", "abstract": "In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World."}}
{"id": "5AixAJweEyC", "cdate": 1621629781374, "mdate": null, "content": {"title": "Regret Minimization Experience Replay in Off-Policy Reinforcement Learning", "abstract": "In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World."}}
{"id": "RZfl1UMWcVH", "cdate": 1609459200000, "mdate": 1652988638648, "content": {"title": "Regret Minimization Experience Replay in Off-Policy Reinforcement Learning", "abstract": "In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World."}}
