{"id": "zaBJZmaSOf7", "cdate": 1679417874566, "mdate": null, "content": {"title": "Obeying the Order: Introducing Ordered Transfer Hyperparameter Optimisation", "abstract": "We introduce ordered transfer hyperparameter optimisation (OTHPO), a version of transfer\nlearning for hyperparameter optimisation (HPO) where the tasks follow a sequential\norder. Unlike for state-of-the-art transfer HPO, the assumption is that each task is most\ncorrelated to those immediately before it. This matches many deployed settings, where\nhyperparameters are retuned as more data is collected; for instance tuning a sequence of\nmovie recommendation systems as more movies and ratings are added. We propose a formal\ndefinition, outline the differences to related problems and propose a basic OTHPO method\nthat outperforms state-of-the-art transfer HPO. We empirically show the importance of taking order into\naccount using ten benchmarks. The benchmarks are in the setting of gradually accumulating\ndata, and span XGBoost, random forest, approximate k-nearest neighbor, elastic net,\nsupport vector machines and a separate real-world motivated optimisation problem. We\nopen source the benchmarks to foster future research on ordered transfer HPO."}}
{"id": "BNeNQWaBIgq", "cdate": 1645792506921, "mdate": null, "content": {"title": "Automatic Termination for Hyperparameter Optimization", "abstract": "Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) \nin machine learning. \nAt its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops \nthe procedure if it is sufficiently close to the global optimum. Our key insight is that the discrepancy between the true objective (predictive performance on test data) and the computable target (validation performance) suggests stopping once the suboptimality in optimizing the target is dominated by the statistical estimation error.\nAcross an extensive range of real-world HPO problems and baselines, we show that our termination criterion achieves a better trade-off between the test performance and optimization time.\nAdditionally, we find that overfitting may occur in the context of HPO, which is arguably an overlooked problem in the literature, and show how our termination criterion helps to mitigate this phenomenon on both small and large datasets."}}
{"id": "BVeGJ-THIg9", "cdate": 1645792502770, "mdate": null, "content": {"title": "Syne Tune: A Library for Large Scale Hyperparameter Tuning and Reproducible Research", "abstract": "We present Syne Tune, a library for large-scale distributed hyperparameter optimization (HPO). Syne Tune's modular \narchitecture allows users to easily switch between different execution backends to facilitate experimentation and makes it easy to contribute new optimization algorithms. To foster reproducible benchmarking, Syne Tune provides an efficient simulator backend and a benchmarking suite, which are essential for large-scale evaluations of distributed asynchronous HPO algorithms on tabulated and surrogate benchmarks. We showcase these functionalities with a range of state-of-the-art gradient-free optimizers, including multi-fidelity and transfer learning approaches on popular benchmarks from the literature. Additionally, we demonstrate the benefits of Syne Tunefor constrained and multi-objective HPO applications through two use cases: the former considers hyperparameters that induce fair solutions and the latter automatically selects machine types along with the conventional hyperparameters.\n"}}
{"id": "2NqIV8dzR7N", "cdate": 1632875617467, "mdate": null, "content": {"title": "Automatic Termination for Hyperparameter Optimization", "abstract": "Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) of machine learning algorithms. At its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops the procedure if it is sufficiently close to the global optima. Across an extensive range of real-world HPO problems, we show that our termination criterion achieves better test performance compared to existing baselines from the literature, such as stopping when the probability of improvement drops below a fixed threshold. We also provide evidence that these baselines are, compared to our method, highly sensitive to the choices of their own hyperparameters. Additionally, we find that overfitting might occur in the context of HPO, which is arguably an overlooked problem in the literature, and show that our termination criterion mitigates this phenomenon on both small and large datasets."}}
{"id": "1k4rJYEwda-", "cdate": 1629357219443, "mdate": null, "content": {"title": "HPOBench: A Collection of Reproducible Multi-Fidelity Benchmark Problems for HPO", "abstract": "To achieve peak predictive performance, hyperparameter optimization (HPO) is a crucial component of machine learning and its applications. Over the last years, the number of efficient algorithms and tools for HPO grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-fidelity HPO methods. To close this gap, we propose HPOBench, which includes 7 existing and 5 new benchmark families, with a total of more than 100 multi-fidelity benchmark problems. HPOBench allows to run this extendable set of multi-fidelity HPO benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate HPOBench\u2019s broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide HPOBench here: https://github.com/automl/HPOBench."}}
{"id": "34awaeWZgya", "cdate": 1621522019509, "mdate": null, "content": {"title": "Dynamic Pruning of a Neural Network via Gradient Signal-to-Noise Ratio", "abstract": "While training highly overparameterized neural networks is common practice in deep learning, research into post-hoc weight-pruning suggests that more than 90% of parameters can be removed without loss in predictive performance.  To save resources, zero-shot and one-shot pruning attempt to find such a sparse representation at initialization or at an early stage of training. Though efficient, there is no justification, why the sparsity structure should not change during training. Dynamic sparsity pruning undoes this limitation and allows to adapt the structure of the sparse neural network during training. Recent approaches rely on weight magnitude pruning, which has been shown to be sub-optimal when applied at earlier training stages. In this work we propose to use the gradient noise to make pruning decisions. The procedure enables us to automatically adjust the sparsity during training without imposing a hand-designed sparsity schedule, while at the same time being able to recover from previous pruning decisions by unpruning connections as necessary. We evaluate our new method on image and tabular datasets and demonstrate that we reach similar performance as the dense model from which extract the sparse network, while exposing less hyperparameters than other dynamic sparsity methods."}}
{"id": "a2rFihIU7i", "cdate": 1601308337631, "mdate": null, "content": {"title": "Model-based Asynchronous Hyperparameter and Neural Architecture Search", "abstract": "We introduce a model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search that combines the strengths of asynchronous Successive Halving and Gaussian process-based Bayesian optimization. At the heart of our method is a probabilistic model that can simultaneously reason across hyperparameters and resource levels, and supports decision-making in the presence of pending evaluations. We demonstrate the effectiveness of our method on a wide range of challenging benchmarks, for tabular data, image classification and language modelling, and report substantial speed-ups over current state-of-the-art methods. Our new methods, along with asynchronous baselines, are implemented in a distributed framework which will be open sourced along with this publication."}}
{"id": "9qg5hYzkDXi", "cdate": 1577836800000, "mdate": null, "content": {"title": "Model-based Asynchronous Hyperparameter Optimization.", "abstract": "We introduce a model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search that combines the strengths of asynchronous Hyperband and Gaussian process-based Bayesian optimization. At the heart of our method is a probabilistic model that can simultaneously reason across hyperparameters and resource levels, and supports decision-making in the presence of pending evaluations. We demonstrate the effectiveness of our method on a wide range of challenging benchmarks, for tabular data, image classification and language modelling, and report substantial speed-ups over current state-of-the-art methods. Our new methods, along with asynchronous baselines, are implemented in a distributed framework which will be open sourced along with this publication."}}
{"id": "H1gesESeIr", "cdate": 1567802519949, "mdate": null, "content": {"title": "Meta-Surrogate Benchmarking for Hyperparameter Optimization", "abstract": "Despite the recent progress in hyperparameter optimization (HPO), available benchmarks that resemble real-world scenarios consist of a few and very large problem instances that are expensive to solve. This blocks researchers and practitioners not only from systematically running large-scale comparisons that are needed to draw statistically significant results but also from reproducing experiments that were conducted before. This work proposes a method to alleviate these issues by means of a meta-surrogate model for HPO tasks trained on off-line generated data. The model combines a probabilistic encoder with a multi-task model such that it can generate inexpensive and realistic tasks of the class of problems of interest. We demonstrate that benchmarking HPO methods on samples of the generative model allows us to draw more coherent and statistically significant conclusions that can be reached orders of magnitude faster than using the original tasks. We provide evidence of our findings for various HPO methods on a wide class of problems. "}}
{"id": "pTOPbOTpAg", "cdate": 1546300800000, "mdate": null, "content": {"title": "Auto-sklearn: Efficient and Robust Automated Machine Learning.", "abstract": "The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on the Python machine learning package scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub Auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won six out of ten phases of the first ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of Auto-sklearn."}}
