{"id": "5fGpwUMJxs", "cdate": 1680307200000, "mdate": 1683002952933, "content": {"title": "Graph transfer learning", "abstract": "Graph embeddings have been tremendously successful at producing node representations that are discriminative for downstream tasks. In this paper, we study the problem of graph transfer learning: given two graphs and labels in the nodes of the first graph, we wish to predict the labels on the second graph. We propose a tractable, non-combinatorial method for solving the graph transfer learning problem by combining classification and embedding losses with a continuous, convex penalty motivated by tractable graph distances. We demonstrate that our method successfully predicts labels across graphs with almost perfect accuracy; in the same scenarios, training embeddings through standard methods leads to predictions that are no better than random."}}
{"id": "yqtu_1xVQ8b", "cdate": 1609459200000, "mdate": 1683002953020, "content": {"title": "Robust Regression via Model Based Methods", "abstract": "The mean squared error loss is widely used in many applications, including auto-encoders, multi-target regression, and matrix factorization, to name a few. Despite computational advantages due to its differentiability, it is not robust to outliers. In contrast, l_p norms are known to be robust, but cannot be optimized via, e.g., stochastic gradient descent, as they are non-differentiable. We propose an algorithm inspired by so-called model-based optimization (MBO) [35, 36], which replaces a non-convex objective with a convex model function and alternates between optimizing the model function and updating the solution. We apply this to robust regression, proposing SADM, a stochastic variant of the Online Alternating Direction Method of Multipliers (OADM) [50] to solve the inner optimization in MBO. We show that SADM converges with the rate O(log T/T). Finally, we demonstrate experimentally (a) the robustness of l_p norms to outliers and (b) the efficiency of our proposed model-based algorithms in comparison with gradient methods on autoencoders and multi-target regression."}}
{"id": "q2Y3smPuUKW", "cdate": 1609459200000, "mdate": 1624735515946, "content": {"title": "Rate Allocation and Content Placement in Cache Networks", "abstract": "We introduce the problem of optimal congestion control in cache networks, whereby \\emph{both} rate allocations and content placements are optimized \\emph{jointly}. We formulate this as a maximization problem with non-convex constraints, and propose solving this problem via (a) a Lagrangian barrier algorithm and (b) a convex relaxation. We prove different optimality guarantees for each of these two algorithms; our proofs exploit the fact that the non-convex constraints of our problem involve DR-submodular functions."}}
{"id": "YwRpq_CjCWU", "cdate": 1609459200000, "mdate": 1683002953186, "content": {"title": "Graph Transfer Learning", "abstract": "Graph embeddings have been tremendously successful at producing node representations that are discriminative for downstream tasks. In this paper, we study the problem of graph transfer learning: given two graphs and labels in the nodes of the first graph, we wish to predict the labels on the second graph. We propose a tractable, non-combinatorial method for solving the graph transfer learning problem by combining classification and embedding losses with a continuous, convex penalty motivated by tractable graph distances. We demonstrate that our method successfully predicts labels across graphs with almost perfect accuracy; in the same scenarios, training embeddings through standard methods leads to predictions that are no better than random."}}
{"id": "TB4PSE14FD", "cdate": 1609459200000, "mdate": 1683002953045, "content": {"title": "Submodular Maximization via Taylor Series Approximation", "abstract": ""}}
{"id": "K6OfdofL2-X", "cdate": 1609459200000, "mdate": 1683002953006, "content": {"title": "Rate Allocation and Content Placement in Cache Networks", "abstract": "We introduce the problem of optimal congestion control in cache networks, whereby both rate allocations and content placements are optimized jointly. We formulate this as a maximization problem with non-convex constraints, and propose solving this problem via (a) a Lagrangian barrier algorithm and (b) a convex relaxation. We prove different optimality guarantees for each of these two algorithms; our proofs exploit the fact that the non-convex constraints of our problem involve DR-submodular functions."}}
{"id": "IebhBK-dNH8", "cdate": 1609459200000, "mdate": 1624735515929, "content": {"title": "Submodular Maximization via Taylor Series Approximation", "abstract": "We study submodular maximization problems with matroid constraints, in particular, problems where the objective can be expressed via compositions of analytic and multilinear functions. We show that for functions of this form, the so-called continuous greedy algorithm attains a ratio arbitrarily close to $(1-1/e) \\approx 0.63$ using a deterministic estimation via Taylor series approximation. This drastically reduces execution time over prior art that uses sampling."}}
{"id": "G5YQanhwKO", "cdate": 1609459200000, "mdate": 1683002953107, "content": {"title": "Robust Regression via Model Based Methods", "abstract": "The mean squared error loss is widely used in many applications, including auto-encoders, multi-target regression, and matrix factorization, to name a few. Despite computational advantages due to its differentiability, it is not robust to outliers. In contrast, $$\\ell _p$$ norms are known to be robust, but cannot be optimized via, e.g., stochastic gradient descent, as they are non-differentiable. We propose an algorithm inspired by so-called model-based optimization (MBO) [35, 36], which replaces a non-convex objective with a convex model function and alternates between optimizing the model function and updating the solution. We apply this to robust regression, proposing SADM, a stochastic variant of the Online Alternating Direction Method of Multipliers (OADM) [48] to solve the inner optimization in MBO. We show that SADM converges with the rate $$O(\\log T/T)$$ . Finally, we demonstrate experimentally (a) the robustness of $$\\ell _p$$ norms to outliers and (b) the efficiency of our proposed model-based algorithms in comparison with gradient methods on autoencoders and multi-target regression."}}
{"id": "FZe3AzzukGs", "cdate": 1577836800000, "mdate": 1683002953164, "content": {"title": "Kelly Cache Networks", "abstract": "We study networks of M/M/1 queues in which nodes act as caches that store objects. Exogenous requests for objects are routed towards nodes that store them; as a result, object traffic in the network is determined not only by demand but, crucially, by where objects are cached. We determine how to place objects in caches to attain a certain design objective, such as, e.g., minimizing network congestion or retrieval delays. We show that for a broad class of objectives, including minimizing both the expected network delay and the sum of network queue lengths, this optimization problem can be cast as an NP-hard submodular maximization problem. We show that so-called continuous greedy algorithm attains a ratio arbitrarily close to 1 - 1/e \u2248 0.63 using a deterministic estimation via a power series; this drastically reduces execution time over prior art, which resorts to sampling. Finally, we show that our results generalize, beyond M/M/1 queues, to networks of M/M/k and symmetric M/D/1 queues."}}
{"id": "7FSdNEjh9ZV", "cdate": 1577836800000, "mdate": 1624735515902, "content": {"title": "Massively Distributed Graph Distances", "abstract": "Graph distance (or similarity) scores are used in several graph mining tasks, including anomaly detection, nearest neighbor and similarity search, pattern recognition, transfer learning, and clustering. Graph distances that are metrics and, in particular, satisfy the triangle inequality, have theoretical and empirical advantages. Well-known graph distances that are metrics include the chemical or the Chartrand-Kubiki-Shultz (CKS) distances. Unfortunately, both are computationally intractable. Recent efforts propose using convex relaxations of the chemical and CKS distances. Though distance computation becomes a convex optimization problem under these relaxations, the number of variables is quadratic in the graph size; this makes traditional optimization algorithms prohibitive even for small graphs. We propose a distributed method for massively parallelizing this problem using the Alternating Directions Method of Multipliers (ADMM). Our solution uses a novel, distributed bisection algorithm for computing a p-norm proximal operator as a building block. We demonstrate its scalability by conducting experiments over multiple parallel environments."}}
