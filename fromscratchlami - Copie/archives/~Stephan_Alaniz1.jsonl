{"id": "kkal-6-eQk", "cdate": 1672531200000, "mdate": 1695642614964, "content": {"title": "DeViL: Decoding Vision features into Language", "abstract": "Post-hoc explanation methods have often been criticised for abstracting away the decision-making process of deep neural networks. In this work, we would like to provide natural language descriptions for what different layers of a vision backbone have learned. Our DeViL method decodes vision features into language, not only highlighting the attribution locations but also generating textual descriptions of visual features at different layers of the network. We train a transformer network to translate individual image features of any vision layer into a prompt that a separate off-the-shelf language model decodes into natural language. By employing dropout both per-layer and per-spatial-location, our model can generalize training on image-text pairs to generate localized explanations. As it uses a pre-trained language model, our approach is fast to train, can be applied to any vision backbone, and produces textual descriptions at different layers of the vision network. Moreover, DeViL can create open-vocabulary attribution maps corresponding to words or phrases even outside the training scope of the vision model. We demonstrate that DeViL generates textual descriptions relevant to the image content on CC3M surpassing previous lightweight captioning models and attribution maps uncovering the learned concepts of the vision backbone. Finally, we show DeViL also outperforms the current state-of-the-art on the neuron-wise descriptions of the MILANNOTATIONS dataset. Code available at https://github.com/ExplainableML/DeViL"}}
{"id": "UWcRw079bI", "cdate": 1672531200000, "mdate": 1695642614960, "content": {"title": "PDiscoNet: Semantically consistent part discovery for fine-grained recognition", "abstract": "Fine-grained classification often requires recognizing specific object parts, such as beak shape and wing patterns for birds. Encouraging a fine-grained classification model to first detect such parts and then using them to infer the class could help us gauge whether the model is indeed looking at the right details better than with interpretability methods that provide a single attribution map. We propose PDiscoNet to discover object parts by using only image-level class labels along with priors encouraging the parts to be: discriminative, compact, distinct from each other, equivariant to rigid transforms, and active in at least some of the images. In addition to using the appropriate losses to encode these priors, we propose to use part-dropout, where full part feature vectors are dropped at once to prevent a single part from dominating in the classification, and part feature vector modulation, which makes the information coming from each part distinct from the perspective of the classifier. Our results on CUB, CelebA, and PartImageNet show that the proposed method provides substantially better part discovery performance than previous methods while not requiring any additional hyper-parameter tuning and without penalizing the classification performance. The code is available at https://github.com/robertdvdk/part_detection."}}
{"id": "K_6mAgTqzF", "cdate": 1672531200000, "mdate": 1695642614978, "content": {"title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases", "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases."}}
{"id": "EYOTldWxXt", "cdate": 1672531200000, "mdate": 1695642614958, "content": {"title": "Iterative Superquadric Recomposition of 3D Objects from Multiple Views", "abstract": "Humans are good at recomposing novel objects, i.e. they can identify commonalities between unknown objects from general structure to finer detail, an ability difficult to replicate by machines. We propose a framework, ISCO, to recompose an object using 3D superquadrics as semantic parts directly from 2D views without training a model that uses 3D supervision. To achieve this, we optimize the superquadric parameters that compose a specific instance of the object, comparing its rendered 3D view and 2D image silhouette. Our ISCO framework iteratively adds new superquadrics wherever the reconstruction error is high, abstracting first coarse regions and then finer details of the target object. With this simple coarse-to-fine inductive bias, ISCO provides consistent superquadrics for related object parts, despite not having any semantic supervision. Since ISCO does not train any neural network, it is also inherently robust to out-of-distribution objects. Experiments show that, compared to recent single instance superquadrics reconstruction approaches, ISCO provides consistently more accurate 3D reconstructions, even from images in the wild. Code available at https://github.com/ExplainableML/ISCO ."}}
{"id": "V5RAQKfUVR-", "cdate": 1663229938369, "mdate": 1663229938369, "content": {"title": "Abstracting Sketches through Simple Primitives", "abstract": "Humans show high-level of abstraction capabilities in games that require quickly communicating object information. They decompose the message content into multiple parts and communicate them in an interpretable protocol. Toward equipping machines with such capabilities, we propose the Primitive-based Sketch Abstraction task where the goal is to represent sketches using a fixed set of drawing primitives under the influence of a budget. To solve this task, our Primitive-Matching Network (PMN), learns interpretable abstractions of a sketch in a self supervised manner. Specifically, PMN maps each stroke of a sketch to its most similar primitive in a given set, predicting an affine transformation that aligns the selected primitive to the target stroke. We learn this stroke-to-primitive mapping end-to-end with a distance-transform loss that is minimal when the original sketch is precisely reconstructed with the predicted primitives. Our PMN abstraction empirically achieves the highest performance on sketch recognition and sketch-based image retrieval given a communication budget, while at the same time being highly interpretable. This opens up new possibilities for sketch analysis, such as comparing sketches by extracting the most relevant primitives that define an object category."}}
{"id": "HVlG62EdvWc", "cdate": 1646916789251, "mdate": null, "content": {"title": "Semantic Image Synthesis with Semantically Coupled VQ-Model", "abstract": "Semantic image synthesis enables control over unconditional image generation by allowing guidance on what is being generated. We conditionally synthesize the latent space from a vector quantized model (VQ-model) pre-trained to autoencode images. Instead of training an autoregressive Transformer on separately learned conditioning latents and image latents, we find that jointly learning the conditioning and image latents significantly improves the modeling capabilities of the Transformer model. While our jointly trained VQ-model achieves a similar reconstruction performance to a vanilla VQ-model for both semantic and image latents, tying the two modalities at the autoencoding stage proves to be an important ingredient to improve autoregressive modeling performance. We show that our model improves semantic image synthesis using autoregressive models on popular semantic image datasets ADE20k, Cityscapes and COCO-Stuff."}}
{"id": "yZEgAMaGYB", "cdate": 1640995200000, "mdate": 1668506835206, "content": {"title": "Abstracting Sketches through Simple Primitives", "abstract": "Humans show high-level of abstraction capabilities in games that require quickly communicating object information. They decompose the message content into multiple parts and communicate them in an interpretable protocol. Toward equipping machines with such capabilities, we propose the Primitive-based Sketch Abstraction task where the goal is to represent sketches using a fixed set of drawing primitives under the influence of a budget. To solve this task, our Primitive-Matching Network (PMN), learns interpretable abstractions of a sketch in a self supervised manner. Specifically, PMN maps each stroke of a sketch to its most similar primitive in a given set, predicting an affine transformation that aligns the selected primitive to the target stroke. We learn this stroke-to-primitive mapping end-to-end with a distance-transform loss that is minimal when the original sketch is precisely reconstructed with the predicted primitives. Our PMN abstraction empirically achieves the highest performance on sketch recognition and sketch-based image retrieval given a communication budget, while at the same time being highly interpretable. This opens up new possibilities for sketch analysis, such as comparing sketches by extracting the most relevant primitives that define an object category. Code is available at https://github.com/ExplainableML/sketch-primitives."}}
{"id": "yAebLaQKYF", "cdate": 1640995200000, "mdate": 1668506835205, "content": {"title": "Abstracting Sketches Through Simple Primitives", "abstract": "Humans show high-level of abstraction capabilities in games that require quickly communicating object information. They decompose the message content into multiple parts and communicate them in an interpretable protocol. Toward equipping machines with such capabilities, we propose the Primitive-based Sketch Abstraction task where the goal is to represent sketches using a fixed set of drawing primitives under the influence of a budget. To solve this task, our Primitive-Matching Network (PMN), learns interpretable abstractions of a sketch in a self supervised manner. Specifically, PMN maps each stroke of a sketch to its most similar primitive in a given set, predicting an affine transformation that aligns the selected primitive to the target stroke. We learn this stroke-to-primitive mapping end-to-end with a distance-transform loss that is minimal when the original sketch is precisely reconstructed with the predicted primitives. Our PMN abstraction empirically achieves the highest performance on sketch recognition and sketch-based image retrieval given a communication budget, while at the same time being highly interpretable. This opens up new possibilities for sketch analysis, such as comparing sketches by extracting the most relevant primitives that define an object category. Code is available at https://github.com/ExplainableML/sketch-primitives ."}}
{"id": "xgML77g-fi", "cdate": 1640995200000, "mdate": 1682334813307, "content": {"title": "Abstracting Sketches Through Simple Primitives", "abstract": "Humans show high-level of abstraction capabilities in games that require quickly communicating object information. They decompose the message content into multiple parts and communicate them in an interpretable protocol. Toward equipping machines with such capabilities, we propose the Primitive-based Sketch Abstraction task where the goal is to represent sketches using a fixed set of drawing primitives under the influence of a budget. To solve this task, our Primitive-Matching Network (PMN), learns interpretable abstractions of a sketch in a self supervised manner. Specifically, PMN maps each stroke of a sketch to its most similar primitive in a given set, predicting an affine transformation that aligns the selected primitive to the target stroke. We learn this stroke-to-primitive mapping end-to-end with a distance-transform loss that is minimal when the original sketch is precisely reconstructed with the predicted primitives. Our PMN abstraction empirically achieves the highest performance on sketch recognition and sketch-based image retrieval given a communication budget, while at the same time being highly interpretable. This opens up new possibilities for sketch analysis, such as comparing sketches by extracting the most relevant primitives that define an object category. Code is available at https://github.com/ExplainableML/sketch-primitives ."}}
{"id": "qdFYHAxwUPj", "cdate": 1640995200000, "mdate": 1682334813416, "content": {"title": "Explainability in Deep Learning by Means of Communication", "abstract": "Die Forschung in Deep Learning erlebt herausragende Fortschritte, die zu vielen zuk\u00fcnftigen Anwendungen f\u00fchren k\u00f6nnen, besonders im Bereich Computer Vision. Dennoch bleiben die meisten Teile eines auf maschinellem Lernen basierenden Systems verborgen und schwer dem Menschen zu erkl\u00e4ren. Der daraus resultierende Mangel an Verst\u00e4ndnis und Vertrauen beschr\u00e4nkt die Anwendung solcher Methoden in kritischen Szenarien wie dem Gesundheitssystem. In dieser Dissertation schlagen wir Modelle vor, die von Grund auf erkl\u00e4rbarer sind als die Basismodelle, auf denen sie aufbauen. Durch das Offenbaren von Teilen des Entscheidungsprozesses entwickeln wir Hilfsmittel, die es Menschen erlauben, die St\u00e4rken und Schw\u00e4chen der Modelle besser zu erkennen und einzusch\u00e4tzen, ob sie sich f\u00fcr einen Einsatz eignen. Die Forschung dieser Dissertation ist von der Art inspiriert wie Erkl\u00e4rungen durch Kommunikation entstehen. Wir pr\u00e4sentieren neuartige Herangehensweisen, welche verschiedene Aspekte des Kommunikationsablaufs in neuronale Netze integrieren, sodass sie interpretierbarer werden und/oder die Interaktion zwischen Mensch und Maschine verbessern. Kommunikation --- und damit auch unsere Sprache --- ist ein nat\u00fcrlicher Weg f\u00fcr Menschen eine Erkl\u00e4rung zu verfassen, sodass ein Benutzer von erkl\u00e4rbaren Systemen keiner Lernphase ausgesetzt ist, um mit der k\u00fcnstlichen Intelligenz interagieren zu k\u00f6nnen. Insbesondere schlagen wir eine Multiagenten-Kommunikationsumgebung vor, bei der die Nachrichten zwischen den Agenten einem Ja/Nein Frage-Antwort-Diskurs \u00e4hneln. Agenten, die trainiert wurden, Bilder zu klassifizieren, bauen dabei einen Entscheidungsbaum, der den Ablauf der Vorhersagen als Ganzes beschreibt. W\u00e4hrend dieser Ansatz eine einheitliche Erkl\u00e4rung f\u00fcr eine Problemstellung findet, stellen wir fest, dass Menschen vielf\u00e4ltig in ihrer Kommunikation, Sprache und Wahrnehmung sind. Aus diesem Grund erschaffen wir einen Agenten, der zielgerichtete Nachrichten erstellt und seine Vorgehensweise anpasst, indem er seinen Kommunikationspartner dabei beobachtet, wie effektiv dieser die \u00fcbertragenen Informationen in Taten umsetzen kann. Im Anschluss zeigen wir, dass effiziente Kommunikation auch als Ma\u00dfstab verwendet werden kann, der, wenn man sich ihn als Ziel setzt, zu hoch komprimierten Abstraktionen und zu interpretierbaren Erkenntnissen f\u00fchrt. Schlie\u00dflich werfen wir einen umfassenderen Blick darauf, inwiefern semantische Informationen, z.B. von Sprache, Vision-Modelle bereichern und sie erkl\u00e4rbarer machen k\u00f6nnen."}}
