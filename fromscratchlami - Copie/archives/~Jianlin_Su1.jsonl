{"id": "3dYmuLh5PD", "cdate": 1672531200000, "mdate": 1681181601393, "content": {"title": "Minimum Entropy Principle Guided Graph Neural Networks", "abstract": ""}}
{"id": "d6mf9AFoR-O", "cdate": 1652737722166, "mdate": null, "content": {"title": "Dual-discriminative Graph Neural Network for Imbalanced Graph-level Anomaly Detection", "abstract": "Graph-level anomaly detection aims to distinguish anomalous graphs in a graph dataset from normal graphs. Anomalous graphs represent a very few but essential patterns in the real world. The anomalous property of a graph may be referable to its anomalous attributes of particular nodes and anomalous substructures that refer to a subset of nodes and edges in the graph. In addition, due to the imbalance nature of anomaly problem, anomalous information will be diluted by normal graphs with overwhelming quantities. Various anomaly notions in the attributes and/or substructures and the imbalance nature together make detecting anomalous graphs a non-trivial task. In this paper, we propose a graph neural network for graph-level anomaly detection, namely iGAD. Specifically, an anomalous graph attribute-aware graph convolution and an anomalous graph substructure-aware deep Random Walk Kernel (deep RWK) are welded into a graph neural network to achieve the dual-discriminative ability on anomalous attributes and substructures. Deep RWK in iGAD makes up for the deficiency of graph convolution in distinguishing structural information caused by the simple neighborhood aggregation mechanism. Further, we propose a Point Mutual Information (PMI)-based loss function to target the problems caused by imbalance distributions. PMI-based loss function enables iGAD to capture essential correlation between input graphs and their anomalous/normal properties. We evaluate iGAD on four real-world graph datasets. Extensive experiments demonstrate the superiority of iGAD on the graph-level anomaly detection task."}}
{"id": "QzFJmwwBMd", "cdate": 1652737336704, "mdate": null, "content": {"title": "ZARTS: On Zero-order Optimization for Neural Architecture Search", "abstract": "Differentiable architecture search (DARTS) has been a popular one-shot paradigm for NAS due to its high efficiency. It introduces trainable architecture parameters to represent the importance of candidate operations and proposes first/second-order approximation to estimate their gradients, making it possible to solve NAS by gradient descent algorithm. However, our in-depth empirical results show that the approximation often distorts the loss landscape, leading to the biased objective to optimize and, in turn, inaccurate gradient estimation for architecture parameters. This work turns to zero-order optimization and proposes a novel NAS scheme, called ZARTS, to search without enforcing the above approximation. Specifically, three representative zero-order optimization methods are introduced: RS, MGS, and GLD, among which MGS performs best by balancing the accuracy and speed. Moreover, we explore the connections between RS/MGS and gradient descent algorithm and show that our ZARTS can be seen as a robust gradient-free counterpart to DARTS. Extensive experiments on multiple datasets and search spaces show the remarkable performance of our method. In particular, results on 12 benchmarks verify the outstanding robustness of ZARTS, where the performance of DARTS collapses due to its known instability issue. Also, we search on the search space of DARTS to compare with peer methods, and our discovered architecture achieves 97.54\\% accuracy on CIFAR-10 and 75.7\\% top-1 accuracy on ImageNet. Finally, we combine our ZARTS with three orthogonal variants of DARTS for faster search speed and better performance.  Source code will be made publicly available at:  \\url{https://github.com/vicFigure/ZARTS}."}}
{"id": "oJ1pouHkv7G", "cdate": 1640995200000, "mdate": 1681181601406, "content": {"title": "BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning in Sentiment Analysis", "abstract": ""}}
{"id": "lKvwZHaCsQ", "cdate": 1640995200000, "mdate": 1681181601407, "content": {"title": "ZLPR: A Novel Loss for Multi-label Classification", "abstract": ""}}
{"id": "bKUxz653fzj", "cdate": 1640995200000, "mdate": 1681181601391, "content": {"title": "Multiattention Network for Semantic Segmentation of Fine-Resolution Remote Sensing Images", "abstract": ""}}
{"id": "Uw_nDfRuCd", "cdate": 1640995200000, "mdate": 1681181601406, "content": {"title": "Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition", "abstract": ""}}
{"id": "07rmn3PIMr", "cdate": 1640995200000, "mdate": 1681181601386, "content": {"title": "Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images", "abstract": ""}}
{"id": "OQL_tkK1vqO", "cdate": 1632875485013, "mdate": null, "content": {"title": "ZARTS: On Zero-order Optimization for Neural Architecture Search", "abstract": "Differentiable architecture search (DARTS) has been a popular one-shot paradigm for NAS due to its high efficiency. It introduces trainable architecture parameters to represent the importance of candidate operations and proposes first/second-order approximation to estimate their gradients, making it possible to solve NAS by gradient descent algorithm. However, our in-depth empirical results show that the approximation will often distort the loss landscape, leading to the biased objective to optimize and in turn inaccurate gradient estimation for architecture parameters. This work turns to zero-order optimization and proposes a novel NAS scheme, called ZARTS, to search without enforcing the above approximation. Specifically, three representative zero-order optimization methods are introduced: RS, MGS, and GLD, among which MGS performs best by balancing the accuracy and speed. Moreover, we explore the connections between RS/MGS and gradient descent algorithm and show that our ZARTS can be seen as a robust gradient-free counterpart to DARTS. Extensive experiments on multiple datasets and search spaces show the remarkable performance of our method. In particular, results on 12 benchmarks verify the outstanding robustness of ZARTS, where the performance of DARTS collapses due to its known instability issue. Also, we search on the search space of DARTS to compare with peer methods, and our discovered architecture achieves 97.54% accuracy on CIFAR-10 and 75.7% top-1 accuracy on ImageNet, which are state-of-the-art performance."}}
{"id": "ujNqnURSeP", "cdate": 1609459200000, "mdate": 1681181601418, "content": {"title": "Few-Shot Learning for Chinese NLP Tasks", "abstract": ""}}
