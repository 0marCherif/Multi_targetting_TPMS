{"id": "Hs2mVP9hs-i", "cdate": 1696177163160, "mdate": 1696177163160, "content": {"title": "Distributionally Robust Model-based Reinforcement Learning with Large State Spaces", "abstract": "Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed method can be further combined with other model-free distributionally robust reinforcement learning methods to obtain a near-optimal robust policy. Experimental results demonstrate the robustness of our algorithm to distributional shifts and its superior performance in terms of the number of samples needed."}}
{"id": "zbK19JAsCU", "cdate": 1680307200000, "mdate": 1682317707276, "content": {"title": "How Bad is Selfish Driving? Bounding the Inefficiency of Equilibria in Urban Driving Games", "abstract": "We consider the interaction among agents engaging in a driving task and we model it as general-sum game. This class of games exhibits a plurality of different equilibria posing the issue of equilibrium selection. While selecting the most efficient equilibrium (in term of social cost) is often impractical from a computational standpoint, in this work we study the (in)efficiency of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">any</i> equilibrium players might agree to play. More specifically, we bound the equilibrium inefficiency by modeling driving games as particular type of congestion games over spatio-temporal resources. We obtain novel guarantees that refine existing bounds on the Price of Anarchy (PoA) as a function of problem-dependent game parameters. For instance, the relative trade-off between proximity costs and personal objectives such as comfort and progress. Although the obtained guarantees concern open-loop trajectories, we observe efficient equilibria even when agents employ closed-loop policies trained via decentralized multi-agent reinforcement learning."}}
{"id": "F-L7BxiE_V", "cdate": 1652737702962, "mdate": null, "content": {"title": "Movement Penalized Bayesian Optimization with Application to Wind Energy Systems", "abstract": "Contextual Bayesian optimization (CBO) is a powerful framework for sequential decision-making given side information, with important applications, e.g., in wind energy systems. In this setting, the learner receives context (e.g., weather conditions) at each round, and has to choose an action (e.g., turbine parameters). Standard algorithms assume no cost for switching their decisions at every round. However, in many practical applications, there is a cost associated with such changes, which should be minimized. We introduce the episodic CBO with movement costs problem and, based on the online learning approach for metrical task systems of Coester and Lee (2019), propose a novel randomized mirror descent algorithm that makes use of Gaussian Process confidence bounds. We compare its performance with the offline optimal sequence for each episode and provide rigorous regret guarantees. We further demonstrate our approach on the important real-world application of altitude optimization for Airborne Wind Energy Systems. In the presence of substantial movement costs, our algorithm consistently outperforms standard CBO algorithms."}}
{"id": "z1lQjd5b1H8", "cdate": 1640995200000, "mdate": 1682317707180, "content": {"title": "Efficient Model-based Multi-agent Reinforcement Learning via Optimistic Equilibrium Computation", "abstract": "We consider model-based multi-agent reinforcement learning, where the environment transition model is unknown and can only be learned via expensive interactions with the environment. We propose H-M..."}}
{"id": "VI-KYlOLRI", "cdate": 1640995200000, "mdate": 1682317707056, "content": {"title": "Movement Penalized Bayesian Optimization with Application to Wind Energy Systems", "abstract": "Contextual Bayesian optimization (CBO) is a powerful framework for sequential decision-making given side information, with important applications, e.g., in wind energy systems. In this setting, the learner receives context (e.g., weather conditions) at each round, and has to choose an action (e.g., turbine parameters). Standard algorithms assume no cost for switching their decisions at every round. However, in many practical applications, there is a cost associated with such changes, which should be minimized. We introduce the episodic CBO with movement costs problem and, based on the online learning approach for metrical task systems of Coester and Lee (2019), propose a novel randomized mirror descent algorithm that makes use of Gaussian Process confidence bounds. We compare its performance with the offline optimal sequence for each episode and provide rigorous regret guarantees. We further demonstrate our approach on the important real-world application of altitude optimization for Airborne Wind Energy Systems. In the presence of substantial movement costs, our algorithm consistently outperforms standard CBO algorithms."}}
{"id": "LvpWUBiTHNv", "cdate": 1640995200000, "mdate": 1682317707292, "content": {"title": "How Bad is Selfish Driving? Bounding the Inefficiency of Equilibria in Urban Driving Games", "abstract": "We consider the interaction among agents engaging in a driving task and we model it as general-sum game. This class of games exhibits a plurality of different equilibria posing the issue of equilibrium selection. While selecting the most efficient equilibrium (in term of social cost) is often impractical from a computational standpoint, in this work we study the (in)efficiency of any equilibrium players might agree to play. More specifically, we bound the equilibrium inefficiency by modeling driving games as particular type of congestion games over spatio-temporal resources. We obtain novel guarantees that refine existing bounds on the Price of Anarchy (PoA) as a function of problem-dependent game parameters. For instance, the relative trade-off between proximity costs and personal objectives such as comfort and progress. Although the obtained guarantees concern open-loop trajectories, we observe efficient equilibria even when agents employ closed-loop policies trained via decentralized multi-agent reinforcement learning."}}
{"id": "JvxY2RG8Fs", "cdate": 1640995200000, "mdate": 1682317706985, "content": {"title": "Learning and Efficiency in Multi-Agent Systems", "abstract": "Several important real-world problems involve multiple entities interacting with each other and can thus be modeled as multi-agent systems. Multi-agent systems are at the core of our society and, due to the recent advances in big data and artificial intelligence, are rapidly permeating into new application domains such as autonomous driving, e-commerce, shared mobility, etc. At the same time, however, such recent progress has brought relevant challenges related to the decision-making, learning, and efficiency of such systems which makes them less understood than their single-agent counterparts. In this thesis, we aim to partially address some of these challenges. The first part of the thesis investigates the problem of sample-efficient active data collection in multi-agent systems, i.e., how agents can acquire new data and learn about the underlying game without sacrificing performance. This problem, also known as the exploration vs. exploitation dilemma, has been extensively studied in single-agent problems but remains fairly unexplored in multi-agent domains. We propose a novel approach to this, which consists of using past observed data to exploit the correlations present in the game by means of statistical regression techniques. This allows the agents to build high-probability confidence intervals around the underlying game rewards and use these to improve their strategy via optimism in the face of uncertainty. We first instantiate this idea in normal-form games and then extend it to a newly defined class of contextual games (where agents observe contextual information before playing), Markov games, and sequential (Stackelberg) games. We provide theoretical regret bounds of the resulting algorithms, yielding provable convergence to equilibria. Moreover, we evaluate our methods in experimental case studies in traffic routing, autonomous driving interactions, and wildlife protection. Our algorithms gradually learn about the underlying game and display a significantly lower regret compared to the existing baselines that utilize solely the obtained game rewards (the so-called bandit feedback). Moreover, they often achieve comparable performance to methods that \u2013 unlike ours \u2013 require full information about the game. In the second part of the thesis, we study the system-level efficiency of multi-agent systems, i.e., the quality of their equilibria (arising from agents\u2019 selfishness) with respect to a system-level objective. There is a long history of research that upper bounds their inefficiency but this has mostly considered games with finite or discrete actions. We extend some of these results to a novel class of continuous action games displaying certain regularity conditions. Moreover, we provide more general efficiency bounds for the case of time-varying contextual games and in the presence of learning agents. Then, motivated by the obtained results and by emerging applications in shared mobility, we consider the problem of designing multi-agent systems to solve hard resource allocation problems (such as rebalancing a bike-sharing system) in a distributed fashion. We propose a novel algorithm for this task and, based on the results obtained in the previous chapters, we provide rigorous convergent and approximation guarantees."}}
{"id": "-wn1g2eA4z", "cdate": 1640995200000, "mdate": 1682317707173, "content": {"title": "Efficient Model-based Multi-agent Reinforcement Learning via Optimistic Equilibrium Computation", "abstract": "We consider model-based multi-agent reinforcement learning, where the environment transition model is unknown and can only be learned via expensive interactions with the environment. We propose H-MARL (Hallucinated Multi-Agent Reinforcement Learning), a novel sample-efficient algorithm that can efficiently balance exploration, i.e., learning about the environment, and exploitation, i.e., achieve good equilibrium performance in the underlying general-sum Markov game. H-MARL builds high-probability confidence intervals around the unknown transition model and sequentially updates them based on newly observed data. Using these, it constructs an optimistic hallucinated game for the agents for which equilibrium policies are computed at each round. We consider general statistical models (e.g., Gaussian processes, deep ensembles, etc.) and policy classes (e.g., deep neural networks), and theoretically analyze our approach by bounding the agents' dynamic regret. Moreover, we provide a convergence rate to the equilibria of the underlying Markov game. We demonstrate our approach experimentally on an autonomous driving simulation benchmark. H-MARL learns successful equilibrium policies after a few interactions with the environment and can significantly improve the performance compared to non-optimistic exploration methods."}}
{"id": "di0r7vfKrq5", "cdate": 1632875661836, "mdate": null, "content": {"title": "Boosting Search Engines with Interactive Agents", "abstract": "This paper presents first successful steps in designing agents that learn meta-strategies for iterative query refinement. \nOur approach uses machine reading to guide the selection of refinement terms from aggregated search results.\n\nAgents are then empowered with simple but effective search operators to exert fine-grained and transparent control over queries and search results.\n\nWe develop a novel way of generating synthetic search sessions, which leverages the power of transformer-based language models through (self-)supervised learning. We also present a reinforcement learning agent with dynamically constrained actions that learns interactive search strategies from scratch. \n\nWe obtain retrieval and answer quality performance comparable to recent neural methods using a traditional term-based BM25 ranking function. We provide an in-depth analysis of the search policies."}}
{"id": "3T9iLGojUD", "cdate": 1609459200000, "mdate": 1682317707131, "content": {"title": "A hybrid stochastic approach for offline train trajectory reconstruction", "abstract": "The next generation of railway systems will require more and more accurate information for the planning of rail operation. These are essential for the introduction of automatic processes of an optimized traffic planning, the optimal use of infrastructure capacity and energy, and, overall, the introduction of data-driven approaches into rail operation. Train trajectories collection constitutes a primary source of information for offline procedures such as timetable generation, driving behaviour analysis and models\u2019 calibration. Unfortunately, current train trajectory data are often affected by measurement errors, missing data and, in many cases, incongruence between dependent variables. To overcome this problem, a trajectory reconstruction problem must be solved, before using trajectories for any further purpose. In the present paper, a new hybrid stochastic trajectory reconstruction is proposed. On-board monitoring data on train position and velocity (kinematics) are combined with data on power used for traction and feasible acceleration values (dynamics). A fusion of those two types of information is performed by considering the stochastic characteristics of the data, via smoothing techniques. A promising potential use is seen especially in those cases where information on continuous train positions is not available or unreliable (e.g. tunnels, canyons, etc.)."}}
