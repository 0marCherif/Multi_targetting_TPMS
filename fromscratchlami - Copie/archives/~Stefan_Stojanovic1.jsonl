{"id": "Li_8TwVTeT1", "cdate": 1640995200000, "mdate": 1652964310649, "content": {"title": "Fast rates for noisy interpolation require rethinking the effects of inductive bias", "abstract": "Good generalization performance on high-dimensional data crucially hinges on a simple structure of the ground truth and a corresponding strong inductive bias of the estimator. Even though this intuition is valid for regularized models, in this paper we caution against a strong inductive bias for interpolation in the presence of noise: Our results suggest that, while a stronger inductive bias encourages a simpler structure that is more aligned with the ground truth, it also increases the detrimental effect of noise. Specifically, for both linear regression and classification with a sparse ground truth, we prove that minimum $\\ell_p$-norm and maximum $\\ell_p$-margin interpolators achieve fast polynomial rates up to order $1/n$ for $p > 1$ compared to a logarithmic rate for $p = 1$. Finally, we provide experimental evidence that this trade-off may also play a crucial role in understanding non-linear interpolating models used in practice."}}
{"id": "GgnaAXSo--", "cdate": 1640995200000, "mdate": 1683794218932, "content": {"title": "Tight bounds for maximum \ud835\udcc11-margin classifiers", "abstract": "Popular iterative algorithms such as boosting methods and coordinate descent on linear models converge to the maximum $\\ell_1$-margin classifier, a.k.a. sparse hard-margin SVM, in high dimensional regimes where the data is linearly separable. Previous works consistently show that many estimators relying on the $\\ell_1$-norm achieve improved statistical rates for hard sparse ground truths. We show that surprisingly, this adaptivity does not apply to the maximum $\\ell_1$-margin classifier for a standard discriminative setting. In particular, for the noiseless setting, we prove tight upper and lower bounds for the prediction error that match existing rates of order $\\frac{\\|w^*\\|_1^{2/3}}{n^{1/3}}$ for general ground truths. To complete the picture, we show that when interpolating noisy observations, the error vanishes at a rate of order $\\frac{1}{\\sqrt{\\log(d/n)}}$. We are therefore first to show benign overfitting for the maximum $\\ell_1$-margin classifier."}}
{"id": "7iJoF8Ro0br", "cdate": 1640995200000, "mdate": 1683794218933, "content": {"title": "Fast rates for noisy interpolation require rethinking the effect of inductive bias", "abstract": "Good generalization performance on high-dimensional data crucially hinges on a simple structure of the ground truth and a corresponding strong inductive bias of the estimator. Even though this intu..."}}
