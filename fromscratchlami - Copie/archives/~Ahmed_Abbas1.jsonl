{"id": "Ub9sx02rIdr", "cdate": 1672531200000, "mdate": 1681649930970, "content": {"title": "ClusterFuG: Clustering Fully connected Graphs by Multicut", "abstract": ""}}
{"id": "-QHUWgkh1OY", "cdate": 1652737408450, "mdate": null, "content": {"title": "DOGE-Train: Discrete Optimization on GPU with End-to-end Training", "abstract": "We present a fast, scalable, data-driven approach for solving linear relaxations of 0-1 integer linear programs using a graph neural network.\nOur solver is based on the Lagrange decomposition based algorithm of Abbas et al. (2022).\nWe make the algorithm differentiable and perform backpropagation through the dual update scheme for end-to-end training of its algorithmic parameters.\nThis allows to preserve the algorithm's theoretical properties including feasibility and guaranteed non-decrease in the lower bound.\nSince the method of Abbas et al. (2022) can get stuck in suboptimal fixed points, we provide additional freedom to our graph neural network to predict non-parametric update steps for escaping such points while maintaining dual feasibility.\nFor training of the graph neural network we use an unsupervised loss and perform experiments on large-scale real world datasets.\nWe train on smaller problems and test on larger ones showing strong generalization performance with a graph neural network comprising only around $10k$ parameters.\nOur solver achieves significantly faster performance and better dual objectives than its non-learned version of Abbas et al. (2022).\nIn comparison to commercial solvers our learned solver achieves close to optimal objective values of LP relaxations and is faster by up to an order of magnitude on very large problems from structured prediction and on selected combinatorial optimization problems.\nOur code will be made available upon acceptance."}}
{"id": "TLpJ6OgNNt", "cdate": 1640995200000, "mdate": 1668509118104, "content": {"title": "FastDOG: Fast Discrete Optimization on GPU", "abstract": "We present a massively parallel Lagrange decomposition method for solving 0\u20131 integer linear programs occurring in structured prediction. We propose a new iterative update scheme for solving the Lagrangean dual and a perturbation technique for decoding primal solutions. For representing subproblems we follow [40] and use binary decision diagrams (BDDs). Our primal and dual algorithms require little synchronization between subproblems and optimization over BDDs needs only elementary operations without complicated control flow. This allows us to exploit the parallelism offered by GPUs for all components of our method. We present experimental results on combinatorial problems from MAP inference for Markov Random Fields, quadratic assignment and cell tracking for developmental biology. Our highly parallel GPU implementation improves upon the running times of the algorithms from [40] by up to an order of magnitude. In particular, we come close to or outperform some state-of-the-art specialized heuristics while being problem agnostic. Our implementation is available at https://github.com/LPMP/BDD."}}
{"id": "BjMFHGqAZg", "cdate": 1640995200000, "mdate": 1668509118050, "content": {"title": "Structured Prediction Problem Archive", "abstract": "Structured prediction problems are one of the fundamental tools in machine learning. In order to facilitate algorithm development for their numerical solution, we collect in one place a large number of datasets in easy to read formats for a diverse set of problem classes. We provide archival links to datasets, description of the considered problems and problem formats, and a short summary of problem characteristics including size, number of instances etc. For reference we also give a non-exhaustive selection of algorithms proposed in the literature for their solution. We hope that this central repository will make benchmarking and comparison to established works easier. We welcome submission of interesting new datasets and algorithms for inclusion in our archive."}}
{"id": "9sjRAc-Gy9F", "cdate": 1640995200000, "mdate": 1668509118066, "content": {"title": "RAMA: A Rapid Multicut Algorithm on GPU", "abstract": "We propose a highly parallel primal-dual algorithm for the multicut (a.k.a. correlation clustering) problem, a classical graph clustering problem widely used in machine learning and computer vision. Our algorithm consists of three steps executed recursively: (1) Finding conflicted cycles that correspond to violated inequalities of the underlying multi-cut relaxation, (2) Performing message passing between the edges and cycles to optimize the Lagrange relaxation coming from the found violated cycles producing reduced costs and (3) Contracting edges with high reduced costs through matrix-matrix multiplications. Our algorithm produces primal solutions and lower bounds that estimate the distance to optimum. We implement our algorithm on GPUs and show resulting one to two orders-of-magnitudes improvements in execution speed without sac-rificing solution quality compared to traditional sequential algorithms that run on CPUs. We can solve very large scale benchmark problems with up to O(10 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">8</sup> ) variables in a few seconds with small primal-dual gaps. Our code is available at https://github.com/pawelswoboda/RAMA."}}
{"id": "6rQbm2ExrG", "cdate": 1640995200000, "mdate": 1668509118049, "content": {"title": "DOGE-Train: Discrete Optimization on GPU with End-to-end Training", "abstract": "We present a fast, scalable, data-driven approach for solving linear relaxations of 0-1 integer linear programs using a graph neural network. Our solver is based on the Lagrange decomposition based algorithm FastDOG (Abbas et al. (2022)). We make the algorithm differentiable and perform backpropagation through the dual update scheme for end-to-end training of its algorithmic parameters. This allows to preserve the algorithm's theoretical properties including feasibility and guaranteed non-decrease in the lower bound. Since FastDOG can get stuck in suboptimal fixed points, we provide additional freedom to our graph neural network to predict non-parametric update steps for escaping such points while maintaining dual feasibility. For training of the graph neural network we use an unsupervised loss and perform experiments on large-scale real world datasets. We train on smaller problems and test on larger ones showing strong generalization performance with a graph neural network comprising only around 10k parameters. Our solver achieves significantly faster performance and better dual objectives than its non-learned version. In comparison to commercial solvers our learned solver achieves close to optimal objective values of LP relaxations and is faster by up to an order of magnitude on very large problems from structured prediction and on selected combinatorial optimization problems."}}
{"id": "70eD741FHyI", "cdate": 1621630201483, "mdate": null, "content": {"title": "Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach", "abstract": "We propose a fully differentiable architecture for simultaneous semantic and instance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multiway cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a panoptic labeling. Our formulation allows to directly maximize a smooth surrogate of the panoptic quality metric by backpropagating the gradient through the optimization problem. \nExperimental evaluation shows improvement by backpropagating through the optimization problem w.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our approach of combinatorial optimization for panoptic segmentation (COPS) shows the utility of using optimization in tandem with deep learning in a challenging large scale real-world problem and showcases benefits and insights into training such an architecture."}}
{"id": "3SVcSU1Mkk8", "cdate": 1621630201483, "mdate": null, "content": {"title": "Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach", "abstract": "We propose a fully differentiable architecture for simultaneous semantic and instance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multiway cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a panoptic labeling. Our formulation allows to directly maximize a smooth surrogate of the panoptic quality metric by backpropagating the gradient through the optimization problem. \nExperimental evaluation shows improvement by backpropagating through the optimization problem w.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our approach of combinatorial optimization for panoptic segmentation (COPS) shows the utility of using optimization in tandem with deep learning in a challenging large scale real-world problem and showcases benefits and insights into training such an architecture."}}
{"id": "zNsxMXmnus", "cdate": 1609459200000, "mdate": 1668509118105, "content": {"title": "Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach", "abstract": "We propose a fully differentiable architecture for simultaneous semantic and instance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multiway cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a panoptic labeling. Our formulation allows to directly maximize a smooth surrogate of the panoptic quality metric by backpropagating the gradient through the optimization problem. Experimental evaluation shows improvement by backpropagating through the optimization problem w.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our approach of combinatorial optimization for panoptic segmentation (COPS) shows the utility of using optimization in tandem with deep learning in a challenging large scale real-world problem and showcases benefits and insights into training such an architecture."}}
{"id": "QFxRFRTpQt4", "cdate": 1546300800000, "mdate": 1668509118084, "content": {"title": "Bottleneck Potentials in Markov Random Fields", "abstract": "We consider general discrete Markov Random Fields (MRFs) with additional bottleneck potentials which penalize the maximum (instead of the sum) over local potential value taken by the MRF-assignment. Bottleneck potentials or analogous constructions have been considered in (i) combinatorial optimization (e.g. bottleneck shortest path problem, the minimum bottleneck spanning tree problem, bottleneck function minimization in greedoids), (ii) inverse problems with L <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u221e</sub> -norm regularization, and (iii) valued constraint satisfaction on the (min, max)-pre-semirings. Bottleneck potentials for general discrete MRFs are a natural generalization of the above direction of modeling work to Maximum-A-Posteriori (MAP) inference in MRFs. To this end, we propose MRFs whose objective consists of two parts: terms that factorize according to (i) (min, +), i.e. potentials as in plain MRFs, and (ii) (min, max), i.e. bottleneck potentials. To solve the ensuing inference problem, we propose high-quality relaxations and efficient algorithms for solving them. We empirically show efficacy of our approach on large scale seismic horizon tracking problems."}}
