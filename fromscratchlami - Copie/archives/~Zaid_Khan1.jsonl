{"id": "x0BPR9iXc1", "cdate": 1663850053604, "mdate": null, "content": {"title": "Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning ", "abstract": "Contrastive vision-language models (e.g. CLIP) are typically created by updating all the parameters of a vision model and language model through contrastive training. Can such models be created by a small number of parameter updates to an already-trained language model and vision model? The literature describes techniques that can create vision-language models by updating a small number of parameters in a language model, but these require already aligned visual representations and are non-contrastive, hence unusable for latency-sensitive applications such as neural search. We explore the feasibility and benefits of parameter-efficient contrastive vision-language alignment through transfer learning: creating a model such as CLIP by minimally updating an already-trained vision and language model. We find that a minimal set of parameter updates ($<$7\\%) can achieve the same performance as full-model training, and updating specific components ($<$1\\% of parameters) can match 75\\% of full-model training. We describe a series of experiments: we show that existing knowledge is conserved more strongly in parameter-efficient training and that parameter-efficient scaling scales with model and dataset size. Where paired-image text data is scarce but strong multilingual language models exist (e.g. low resource languages), parameter-efficient training is even preferable to full-model training. Given a fixed compute budget, parameter-efficient training allows training larger models on the same hardware, achieving equivalent performance in less time. Parameter-efficient training hence constitutes an energy-efficient and effective training strategy for contrastive vision-language models that may be preferable to the full-model training paradigm for common use cases.\nCode and weights at https://github.com/codezakh/LilT."}}
{"id": "uXSYoLhh73", "cdate": 1640995200000, "mdate": 1668272750589, "content": {"title": "Single-Stream Multi-level Alignment for Vision-Language Pretraining", "abstract": "Self-supervised vision-language pretraining from pure images and text with a contrastive loss is effective, but ignores fine-grained alignment due to a dual-stream architecture that aligns image and text representations only on a global level. Earlier, supervised, non-contrastive methods were capable of finer-grained alignment, but required dense annotations that were not scalable. We propose a single stream architecture that aligns images and language at multiple levels: global, fine-grained patch-token, and conceptual/semantic, using two novel tasks: symmetric cross-modality reconstruction (XMM) and a pseudo-labeled key word prediction (PSL). In XMM, we mask input tokens from one modality and use cross-modal information to reconstruct the masked token, thus improving fine-grained alignment between the two modalities. In PSL, we use attention to select keywords in a caption, use a momentum encoder to recommend other important keywords that are missing from the caption but represented in the image, and then train the visual encoder to predict the presence of those keywords, helping it learn semantic concepts that are essential for grounding a textual token to an image region. We demonstrate competitive performance and improved data efficiency on image-text retrieval, grounding, visual question answering/reasoning against larger models and models trained on more data. Code and models available at zaidkhan.me/SIMLA ."}}
{"id": "2aC0_RxkBL_", "cdate": 1632875770599, "mdate": null, "content": {"title": "Where is the bottleneck in long-tailed classification?", "abstract": "A commonly held belief in deep-learning based long-tailed classi\ufb01cation is that the representations learned from long-tailed data are \u201dgood enough\u201d and the performance bottleneck is the classi\ufb01cation head atop the representation learner. We design experiments to investigate this folk wisdom, and \ufb01nd that representations learned from long-tailed data distributions substantially differ from the representations learned from \u201dnormal\u201d data distributions. We show that the long-tailed representations are volatile and brittle with respect to the true data distribution. Compared to the representations learned from the true, balanced distributions, long-tailed representations fail to localize tail classes and display vastly worse inter-class separation and intra-class compactness when unseen samples from the true data distribution are embedded into the feature space. We provide an explanation for why data augmentation helps long-tailed classi\ufb01cation despite leaving the dataset imbalance unchanged \u2014 it promotes inter-class separation, intra-class compactness, and improves localization of tail classes w.r.t to the true data distribution."}}
{"id": "hZZie0cWsR", "cdate": 1609459200000, "mdate": 1675227253244, "content": {"title": "Exploiting BERT for Multimodal Target Sentiment Classification through Input Space Translation", "abstract": "Multimodal target/aspect sentiment classification combines multimodal sentiment analysis and aspect/target sentiment classification. The goal of the task is to combine vision and language to understand the sentiment towards a target entity in a sentence. Twitter is an ideal setting for the task because it is inherently multimodal, highly emotional, and affects real world events. However, multimodal tweets are short and accompanied by complex, possibly irrelevant images. We introduce a two-stream model that translates images in input space using an object-aware transformer followed by a single-pass non-autoregressive text generation approach. We then leverage the translation to construct an auxiliary sentence that provides multimodal information to a language model. Our approach increases the amount of text available to the language model and distills the object-level information in complex images. We achieve state-of-the-art performance on two multimodal Twitter datasets without modifying the internals of the language model to accept multimodal data, demonstrating the effectiveness of our translation. In addition, we explain a failure mode of a popular approach for aspect sentiment analysis when applied to tweets. Our code is available at https://github.com/codezakh/exploiting-BERT-thru-translation."}}
