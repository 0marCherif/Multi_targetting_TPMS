{"id": "pC17nxGmRlB", "cdate": 1672531200000, "mdate": 1699218148083, "content": {"title": "Kernel Neural Optimal Transport", "abstract": ""}}
{"id": "Va_afHKs_l9", "cdate": 1672531200000, "mdate": 1684316735599, "content": {"title": "Extremal Domain Translation with Neural Optimal Transport", "abstract": "We propose the extremal transport (ET) which is a mathematical formalization of the theoretically best possible unpaired translation between a pair of domains w.r.t. the given similarity function. Inspired by the recent advances in neural optimal transport (OT), we propose a scalable algorithm to approximate ET maps as a limit of partial OT maps. We test our algorithm on toy examples and on the unpaired image-to-image translation task."}}
{"id": "M_oQMJdHke", "cdate": 1672531200000, "mdate": 1699218148072, "content": {"title": "Neural Optimal Transport", "abstract": ""}}
{"id": "Zuc_MHtUma4", "cdate": 1663849923580, "mdate": null, "content": {"title": "Kernel Neural Optimal Transport", "abstract": "We study the Neural Optimal Transport (NOT) algorithm which uses the general optimal transport formulation and learns stochastic transport plans. We show that NOT with the weak quadratic cost may learn fake plans which are not optimal. To resolve this issue, we introduce kernel weak quadratic costs. We show that they provide improved theoretical guarantees and practical performance. We test NOT with kernel costs on the unpaired image-to-image translation task."}}
{"id": "d8CBRlWNkqH", "cdate": 1663849923456, "mdate": null, "content": {"title": "Neural Optimal Transport", "abstract": "We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs. To justify the usage of neural networks, we prove that they are universal approximators of transport plans between probability distributions. We evaluate the performance of our optimal transport algorithm on toy examples and on the unpaired image-to-image translation."}}
{"id": "teHNseHbr-6", "cdate": 1640995200000, "mdate": 1668715842260, "content": {"title": "Kernel Neural Optimal Transport", "abstract": "We study the Neural Optimal Transport (NOT) algorithm which uses the general optimal transport formulation and learns stochastic transport plans. We show that NOT with the weak quadratic cost might learn fake plans which are not optimal. To resolve this issue, we introduce kernel weak quadratic costs. We show that they provide improved theoretical guarantees and practical performance. We test NOT with kernel costs on the unpaired image-to-image translation task."}}
{"id": "GWioFeALnsz1", "cdate": 1640995200000, "mdate": 1668715842277, "content": {"title": "Neural Optimal Transport", "abstract": "We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs. To justify the usage of neural networks, we prove that they are universal approximators of transport plans between probability distributions. We evaluate the performance of our optimal transport algorithm on toy examples and on the unpaired image-to-image translation."}}
{"id": "h6qdGplHzg", "cdate": 1609459200000, "mdate": 1668593887426, "content": {"title": "DeepRLS: A Recurrent Network Architecture with Least Squares Implicit Layers for Non-blind Image Deconvolution", "abstract": "In this work, we study the problem of non-blind image deconvolution and propose a novel recurrent network architecture that leads to very competitive restoration results of high image quality. Motivated by the computational efficiency and robustness of existing large scale linear solvers, we manage to express the solution to this problem as the solution of a series of adaptive non-negative least-squares problems. This gives rise to our proposed Recurrent Least Squares Deconvolution Network (RLSDN) architecture, which consists of an implicit layer that imposes a linear constraint between its input and output. By design, our network manages to serve two important purposes simultaneously. The first is that it implicitly models an effective image prior that can adequately characterize the set of natural images, while the second is that it recovers the corresponding maximum a posteriori (MAP) estimate. Experiments on publicly available datasets, comparing recent state-of-the-art methods, show that our proposed RLSDN approach achieves the best reported performance both for grayscale and color images for all tested scenarios. Furthermore, we introduce a novel training strategy that can be adopted by any network architecture that involves the solution of linear systems as part of its pipeline. Our strategy eliminates completely the need to unroll the iterations required by the linear solver and, thus, it reduces significantly the memory footprint during training. Consequently, this enables the training of deeper network architectures which can further improve the reconstruction results."}}
{"id": "2_Gd73Dhpcj", "cdate": 1609459200000, "mdate": 1681652389104, "content": {"title": "Ex2MCMC: Sampling through Exploration Exploitation", "abstract": ""}}
{"id": "qK-aqFTRo36", "cdate": 1546300800000, "mdate": 1648811519421, "content": {"title": "Near Optimal Methods for Minimizing Convex Functions with Lipschitz $p$-th Derivatives", "abstract": "In this merged paper, we consider the problem of minimizing a convex function with Lipschitz-continuous $p$-th order derivatives. Given an oracle which when queried at a point returns the first $p$..."}}
