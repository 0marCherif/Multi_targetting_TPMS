{"id": "BGfLS_8j5eq", "cdate": 1646077549342, "mdate": null, "content": {"title": "If You've Trained One You\u2019ve Trained Them All: Inter-Architecture Similarity Increases With Robustness", "abstract": "Previous work has shown that commonly-used metrics for comparing representations between neural networks overestimate similarity due to correlations between data points. We show that intra-example feature correlations also causes significant overestimation of network similarity and propose an image inversion technique to analyze only the features used by a network. With this technique, we find that similarity across architectures is significantly lower than commonly understood, but we surprisingly find that similarity between models with different architectures increases as the adversarial robustness of the models increase. Our findings indicate that robust networks tend towards a universal set of representations, regardless of architecture, and that the robust training criterion is a strong prior constraint on the functions that can be learned by diverse modern architectures. We also find that the representations learned by a robust network of any architecture have an asymmetric overlap with non-robust networks of many architectures, indicating that the representations used by robust neural networks are highly entangled with the representations used by non-robust networks."}}
{"id": "tzModtpOW7l", "cdate": 1624022584466, "mdate": null, "content": {"title": "Uncovering Universal Features: How Adversarial Training Improves Adversarial Transferability", "abstract": "Adversarial examples for neural networks are known to be transferable: examples optimized to be misclassified by a \u201csource\u201d network are often misclassified by other \u201cdestination\u201d networks. Here, we show that training the source network to be \u201cslightly robust\u201d---that is, robust to small-magnitude adversarial examples---substantially improves the transferability of targeted attacks, even between architectures as different as convolutional neural networks and transformers. In fact, we show that these adversarial examples can transfer representation (penultimate) layer features substantially better than adversarial examples generated with non-robust networks. We argue that this result supports a non-intuitive hypothesis: slightly robust networks exhibit universal features---ones that tend to overlap with the features learned by all other networks trained on the same dataset. This suggests that the features of a single slightly-robust neural network may be useful to derive insight about the features of every non-robust neural network trained on the same distribution."}}
{"id": "XXxoCgHsiRv", "cdate": 1621629930940, "mdate": null, "content": {"title": "A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks", "abstract": "Adversarial examples for neural network image classifiers are known to be transferable: examples optimized to be misclassified by a source classifier are often misclassified as well by classifiers with different architectures. However, targeted adversarial examples\u2014optimized to be classified as a chosen target class\u2014tend to be less transferable between architectures. While prior research on constructing transferable targeted attacks has focused on improving the optimization procedure, in this work we examine the role of the source classifier. Here, we show that training the source classifier to be \"slightly robust\"\u2014that is, robust to small-magnitude adversarial examples\u2014substantially improves the transferability of class-targeted and representation-targeted adversarial attacks, even between architectures as different as convolutional neural networks and transformers. The results we present provide insight into the nature of adversarial examples as well as the mechanisms underlying so-called \"robust\" classifiers."}}
{"id": "uKZsVyFKbaj", "cdate": 1601308277817, "mdate": null, "content": {"title": "It's Hard for Neural Networks to Learn the Game of Life", "abstract": "Efforts to improve the learning abilities of neural networks have focused mostly on the role of optimization methods rather than on weight initializations. Recent findings, however, suggest that neural networks rely on lucky random initial weights of subnetworks called \"lottery tickets\" that converge quickly to a solution. To investigate how weight initializations affect performance, we examine small convolutional networks that are trained to predict $n$ steps of the two-dimensional cellular automaton Conway\u2019s Game of Life, the update rules of which can be implemented efficiently in a small CNN. We find that networks of this architecture trained on this task rarely converge. Rather, networks require substantially more parameters to consistently converge. Furthermore, we find that the initialization parameters that gradient descent converges to a solution are sensitive to small perturbations, such as a single sign change. Finally, we observe a critical value $d_0$ such that training minimal networks with examples in which cells are alive with probability $d_0$ dramatically increases the chance of convergence to a solution. Our results are consistent with the lottery ticket hypothesis."}}
{"id": "7ehDLD1yoE0", "cdate": 1601308107436, "mdate": null, "content": {"title": "STRATA: Simple, Gradient-free Attacks for Models of Code", "abstract": "Adversarial examples are imperceptible perturbations in the input to a neural model that result in misclassification. Generating adversarial examples for source code poses an additional challenge compared to the domains of images and natural language, because source code perturbations must adhere to strict semantic guidelines so the resulting programs retain the functional meaning of the code. We propose a simple and efficient gradient-free method for generating state-of-the-art adversarial examples on models of code that can be applied in a white-box or black-box setting. Our method generates untargeted and targeted attacks, and empirically outperforms competing gradient-based methods with less information and less computational effort."}}
