{"id": "6ARTdRjK62", "cdate": 1640995200000, "mdate": 1667094843950, "content": {"title": "Calibrating Deep Neural Networks by Pairwise Constraints", "abstract": "It is well known that deep neural networks (DNNs) pro-duce poorly calibrated estimates of class-posterior prob-abilities. We hypothesize that this is due to the limited calibration supervision provided by the cross-entropy loss, which places all emphasis on the probability of the true class and mostly ignores the remaining. We consider how each example can supervise all classes and show that the calibration of a C-way classification problem is equivalent to the calibration of C(C - 1) /2 pairwise binary classifi-cation problems that can be derived from it. This suggests the hypothesis that DNN calibration can be improved by providing calibration supervision to all such binary prob-lems. An implementation of this calibration by pairwise constraints (CPC) is then proposed, based on two types of binary calibration constraints. This is finally shown to be implementable with a very minimal increase in the complex-ity of cross-entropy training. Empirical evaluations of the proposed CPC method across multiple datasets and DNN architectures demonstrate state-of-the-art calibration per-formance."}}
{"id": "0j7PfY0TZYz", "cdate": 1609459200000, "mdate": 1634369610330, "content": {"title": "Learning Deep Classifiers Consistent With Fine-Grained Novelty Detection", "abstract": "The problem of novelty detection in fine-grained visual classification (FGVC) is considered. An integrated understanding of the probabilistic and distance-based approaches to novelty detection is developed within the framework of convolutional neural networks (CNNs). It is shown that softmax CNN classifiers are inconsistent with novelty detection, because their learned class-conditional distributions and associated distance metrics are unidentifiable. A new regularization constraint, the class-conditional Gaussianity loss, is then proposed to eliminate this unidentifiability, and enforce Gaussian class-conditional distributions. This enables training Novelty Detection Consistent Classifiers (NDCCs) that are jointly optimal for classification and novelty detection. Empirical evaluations show that NDCCs achieve significant improvements over the state-of-the-art on both small- and large-scale FGVC datasets."}}
{"id": "pUHsxx0R70j", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning with Bounded Instance and Label-dependent Label Noise", "abstract": "Instance- and Label-dependent label Noise (ILN) widely exists in real-world datasets but has been rarely studied. In this paper, we focus on Bounded Instance- and Label-dependent label Noise (BILN)..."}}
