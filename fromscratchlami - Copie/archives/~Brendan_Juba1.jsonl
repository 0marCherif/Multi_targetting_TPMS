{"id": "znLUfO8VRte", "cdate": 1640995200000, "mdate": 1683785261910, "content": {"title": "An Example of the SAM+ Algorithm for Learning Action Models for Stochastic Worlds", "abstract": "In this technical report, we provide a complete example of running the SAM+ algorithm, an algorithm for learning stochastic planning action models, on a simplified PPDDL version of the Coffee problem. We provide a very brief description of the SAM+ algorithm and detailed description of our simplified version of the Coffee domain, and then describe the results of running it on the simplified Coffee domain."}}
{"id": "lpVyy9wHNm", "cdate": 1640995200000, "mdate": 1683785262909, "content": {"title": "A Scalable Shannon Entropy Estimator", "abstract": "Quantified information flow (QIF) has emerged as a rigorous approach to quantitatively measure confidentiality; the information-theoretic underpinning of QIF allows the end-users to link the computed quantities with the computational effort required on the part of the adversary to gain access to desired confidential information. In this work, we focus on the estimation of Shannon entropy for a given program $$\\varPi $$ . As a first step, we focus on the case wherein a Boolean formula $$\\varphi (X,Y)$$ captures the relationship between inputs X and output Y of $$\\varPi $$ . Such formulas $$\\varphi (X,Y)$$ have the property that for every valuation to X, there exists exactly one valuation to Y such that $$\\varphi $$ is satisfied. The existing techniques require $$\\mathcal {O}(2^m)$$ model counting queries, where $$m = |Y|$$ . We propose the first efficient algorithmic technique, called $$\\mathsf {EntropyEstimation}$$ to estimate the Shannon entropy of $$\\varphi $$ with PAC-style guarantees, i.e., the computed estimate is guaranteed to lie within a $$(1\\pm \\varepsilon )$$ -factor of the ground truth with confidence at least $$1-\\delta $$ . Furthermore, $$\\mathsf {EntropyEstimation}$$ makes only $$\\mathcal {O}(\\frac{min(m,n)}{\\varepsilon ^2})$$ counting and sampling queries, where $$m = |Y|$$ , and $$n = |X|$$ , thereby achieving a significant reduction in the number of model counting queries. We demonstrate the practical efficiency of our algorithmic framework via a detailed experimental evaluation. Our evaluation demonstrates that the proposed framework scales to the formulas beyond the reach of the previously known approaches."}}
{"id": "dcFMFB-oIhq", "cdate": 1640995200000, "mdate": 1683785262543, "content": {"title": "Hardness of Maximum Likelihood Learning of DPPs", "abstract": "Determinantal Point Processes (DPPs) are a widely used probabilistic model for negatively correlated sets. DPPs have been successfully employed in Machine Learning applications to select a diverse, yet representative subset of data. In seminal work on DPPs in Machine Learning, Kulesza conjectured in his PhD Thesis (2011) that the problem of finding a maximum likelihood DPP model for a given data set is NP-complete. In this work we prove Kulesza's conjecture. In fact, we prove the following stronger hardness of approximation result: even computing a $\\left(1-O(\\frac{1}{\\log^9{N}})\\right)$-approximation to the maximum log-likelihood of a DPP on a ground set of $N$ elements is NP-complete. At the same time, we also obtain the first polynomial-time algorithm that achieves a nontrivial worst-case approximation to the optimal log-likelihood: the approximation factor is $\\frac{1}{(1+o(1))\\log{m}}$ unconditionally (for data sets that consist of $m$ subsets), and can be improved to $1-\\frac{1+o(1)}{\\log N}$ if all $N$ elements appear in a $O(1/N)$-fraction of the subsets. In terms of techniques, we reduce approximating the maximum log-likelihood of DPPs on a data set to solving a gap instance of a \"vector coloring\" problem on a hypergraph. Such a hypergraph is built on a bounded-degree graph construction of Bogdanov, Obata and Trevisan (FOCS 2002), and is further enhanced by the strong expanders of Alon and Capalbo (FOCS 2007) to serve our purposes."}}
{"id": "_SMtwVX7Aqi", "cdate": 1640995200000, "mdate": 1683785262664, "content": {"title": "Conditional Linear Regression for Heterogeneous Covariances", "abstract": "Often machine learning and statistical models will attempt to describe the majority of the data. However, there may be situations where only a fraction of the data can be fit well by a linear regression model. Here, we are interested in a case where such inliers can be identified by a Disjunctive Normal Form (DNF) formula. We give a polynomial time algorithm for the conditional linear regression task, which identifies a DNF condition together with the linear predictor on the corresponding portion of the data. In this work, we improve on previous algorithms by removing a requirement that the covariances of the data satisfying each of the terms of the condition have to all be very similar in spectral norm to the covariance of the overall condition."}}
{"id": "Tb7lFP-1ewm", "cdate": 1640995200000, "mdate": 1674704312448, "content": {"title": "Polynomial Time Reinforcement Learning in Factored State MDPs with Linear Value Functions", "abstract": "Many reinforcement learning (RL) environments in practice feature enormous state spaces that may be described compactly by a \"factored\" structure, that may be modeled by Factored Markov Decision Processes (FMDPs). We present the first polynomial time algorithm for RL in Factored State MDPs (generalizing FMDPs) that neither relies on an oracle planner nor requires a linear transition model; it only requires a linear value function with a suitable local basis with respect to the factorization, permitting efficient variable elimination. With this assumption, we can solve this family of Factored State MDPs in polynomial time by constructing an efficient separation oracle for convex optimization. Importantly, and in contrast to prior work on FMDPs, we do not assume that the transitions on various factors are conditionally independent."}}
{"id": "NpQCzsdUSg", "cdate": 1640995200000, "mdate": 1683785262451, "content": {"title": "Hardness of Maximum Likelihood Learning of DPPs", "abstract": "Determinantal Point Processes (DPPs) are a widely used probabilistic model for negatively correlated sets. DPPs are used in Machine Learning applications to select a diverse, yet representative sub..."}}
{"id": "J1NLrMGk48", "cdate": 1640995200000, "mdate": 1683785262773, "content": {"title": "Learning Probably Approximately Complete and Safe Action Models for Stochastic Worlds", "abstract": "We consider the problem of learning action models for planning in unknown stochastic environments that can be defined using the Probabilistic Planning Domain Description Language (PPDDL). As input, we are given a set of previously executed trajectories, and the main challenge is to learn an action model that has a similar goal achievement probability to the policies used to create these trajectories. To this end, we introduce a variant of PPDDL in which there is uncertainty about the transition probabilities, specified by an interval for each factor that contains the respective true transition probabilities. Then, we present SAM+, an algorithm that learns such an imprecise-PPDDL environment model. SAM+ has a polynomial time and sample complexity, and guarantees that with high probability, the true environment is indeed captured by the defined intervals. We prove that the action model SAM+ outputs has a goal achievement probability that is almost as good or better than that of the policies used to produced the training trajectories. Then, we show how to produce a PPDDL model based on this imprecise-PPDDL model that has similar properties."}}
{"id": "HVVyR0Q3yUO", "cdate": 1640995200000, "mdate": 1683785262587, "content": {"title": "A Scalable Shannon Entropy Estimator", "abstract": "We revisit the well-studied problem of estimating the Shannon entropy of a probability distribution, now given access to a probability-revealing conditional sampling oracle. In this model, the oracle takes as input the representation of a set $S$ and returns a sample from the distribution obtained by conditioning on $S$, together with the probability of that sample in the distribution. Our work is motivated by applications of such algorithms in Quantitative Information Flow analysis (QIF) in programming-language-based security. Here, information-theoretic quantities capture the effort required on the part of an adversary to obtain access to confidential information. These applications demand accurate measurements when the entropy is small. Existing algorithms that do not use conditional samples require a number of queries that scale inversely with the entropy, which is unacceptable in this regime, and indeed, a lower bound by Batu et al.(STOC 2002) established that no algorithm using only sampling and evaluation oracles can obtain acceptable performance. On the other hand, prior work in the conditional sampling model by Chakraborty et al.(SICOMP 2016) only obtained a high-order polynomial query complexity, $\\mathcal{O}(\\frac{m^7}{\\epsilon^8}\\log\\frac{1}{\\delta})$ queries, to obtain additive $\\epsilon$-approximations on a domain of size $\\mathcal{O}(2^m)$. We obtain multiplicative $(1+\\epsilon)$-approximations using only $\\mathcal{O}(\\frac{m}{\\epsilon^2}\\log\\frac{1}{\\delta})$ queries to the probability-revealing conditional sampling oracle. Indeed, moreover, we obtain small, explicit constants, and demonstrate that our algorithm obtains a substantial improvement in practice over the previous state-of-the-art methods used for entropy estimation in QIF."}}
{"id": "-W7rXUYi1U", "cdate": 1640995200000, "mdate": 1683785262784, "content": {"title": "Hardness of Maximum Likelihood Learning of DPPs", "abstract": ""}}
{"id": "uut_j3UrRCg", "cdate": 1632875690143, "mdate": null, "content": {"title": "Provable hierarchical lifelong learning with a sketch-based modular architecture", "abstract": "We propose a modular architecture for lifelong learning of hierarchically structured tasks. Specifically, we prove that our architecture is theoretically able to learn tasks that can be solved by functions that are learnable given access to functions for other, previously learned tasks as subroutines. We show that some tasks that we can learn in this way are not learned by standard training methods in practice; indeed, prior work suggests that some such tasks cannot be learned by \\emph{any} efficient method without the aid of the simpler tasks. We also consider methods for identifying the tasks automatically, without relying on explicitly given indicators."}}
