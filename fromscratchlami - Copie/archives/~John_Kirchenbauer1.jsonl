{"id": "gKCtyR021W", "cdate": 1672531200000, "mdate": 1683750514185, "content": {"title": "A Watermark for Large Language Models", "abstract": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."}}
{"id": "KaQwnYQGbfZ", "cdate": 1672531200000, "mdate": 1682335263605, "content": {"title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery", "abstract": "The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical \"hard\" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also \"soft\" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification."}}
{"id": "pBdThOgZyh", "cdate": 1672420274766, "mdate": 1672420274766, "content": {"title": "What Is You Metric Telling You? Evaluating Classifier Calibration Under Context-Specific Definitions of Reliability", "abstract": "Classifier calibration has received recent attention from the machine learning\ncommunity due both to its practical utility in facilitating decision making, as well\nas the observation that modern neural network classifiers are poorly calibrated.\nMuch of this focus has been towards the goal of learning classifiers such that their\noutput with largest magnitude (the \u201cpredicted class\u201d) is calibrated. However, this\nnarrow interpretation of classifier outputs does not adequately capture the variety\nof practical use cases in which classifiers can aid in decision making. In this work,\nwe argue that more expressive metrics must be developed that accurately measure\ncalibration error for the specific context in which a classifier will be deployed. To\nthis end, we derive a number of different metrics using a generalization of Expected\nCalibration Error (ECE) that measure calibration error under different definitions\nof reliability. We then provide an extensive empirical evaluation of commonly\nused neural network architectures and calibration techniques with respect to these\nmetrics. We find that: 1) definitions of ECE that focus solely on the predicted\nclass fail to accurately measure calibration error under a selection of practically\nuseful definitions of reliability and 2) many common calibration techniques fail to\nimprove calibration performance uniformly across ECE metrics derived from these\ndiverse definitions of reliability."}}
{"id": "MsjB2ohCJO1", "cdate": 1663850406271, "mdate": null, "content": {"title": "How to Do a Vocab Swap?  A Study of Embedding Replacement for Pre-trained Transformers", "abstract": "There are a wide range of different tokenizers and vocabularies that have been used to train language models, and training a language model on just one of these can be prohibitively expensive.  The ability to swap the vocabulary of a model after it has been trained enables models to be adapted to different tokenizers, and even different languages, without the computational or data cost of from-scratch training. In this paper, we ask when such swaps are possible, and how to perform them effectively? The major challenge of performing a vocab swap is re-learning the parameters of the embedding layer for the vocabulary. We observe that it is possible to re-learn the embedding for a vocabulary using a naive initialization, and we investigate strong initialization strategies that enable learning of new embeddings for swapped vocabularies, even when those vocabularies come from a different source language than the original language model."}}
{"id": "z29R0uMiF3v", "cdate": 1663849879879, "mdate": null, "content": {"title": "GOAT: A Global Transformer on Large-scale Graphs", "abstract": "Graph transformers have been competitive on graph classification tasks, but they fail to outperform Graph Neural Networks (GNNs) on node classification, which is a common task performed on large-scale graphs for industrial applications. Meanwhile, existing GNN architectures are limited in their ability to perform equally well on both homophilious and heterophilious graphs as their inductive biases are generally tailored to only one setting. To address these issues, we propose GOAT, a scalable global graph transformer. In GOAT, each node conceptually attends to all the nodes in the graph and homophily/heterophily relationships can be learnt adaptively from the data. We provide theoretical justification for our approximate global self-attention scheme, and show it to be scalable to large-scale graphs. We demonstrate the competitiveness of GOAT on both heterophilious and homophilious graphs with millions of nodes."}}
{"id": "Xe2pKAgZKk", "cdate": 1652656527635, "mdate": 1652656527635, "content": {"title": "A Closer Look at Distribution Shifts and Out-of-Distribution Generalization on Graphs", "abstract": "Distribution shifts, in which the training distribution differs from the testing distribution, can significantly degrade the performance of Graph Neural Networks (GNNs). We curate GDS, a benchmark of eight datasets reflecting a diverse range of distribution shifts across graphs. We observe that: (1) most domain generalization algorithms fail to work when applied to domain shifts on graphs; and (2) combinations of powerful GNN models and augmentation techniques usually achieve the best out-of-distribution performance. These emphasize the need for domain generalization algorithms tailored for graphs and further graph augmentation techniques that enhance the robustness of predictors."}}
{"id": "vgA2AxvN8v", "cdate": 1640995200000, "mdate": 1683750514186, "content": {"title": "What is Your Metric Telling You? Evaluating Classifier Calibration under Context-Specific Definitions of Reliability", "abstract": "Classifier calibration has received recent attention from the machine learning community due both to its practical utility in facilitating decision making, as well as the observation that modern neural network classifiers are poorly calibrated. Much of this focus has been towards the goal of learning classifiers such that their output with largest magnitude (the \"predicted class\") is calibrated. However, this narrow interpretation of classifier outputs does not adequately capture the variety of practical use cases in which classifiers can aid in decision making. In this work, we argue that more expressive metrics must be developed that accurately measure calibration error for the specific context in which a classifier will be deployed. To this end, we derive a number of different metrics using a generalization of Expected Calibration Error (ECE) that measure calibration error under different definitions of reliability. We then provide an extensive empirical evaluation of commonly used neural network architectures and calibration techniques with respect to these metrics. We find that: 1) definitions of ECE that focus solely on the predicted class fail to accurately measure calibration error under a selection of practically useful definitions of reliability and 2) many common calibration techniques fail to improve calibration performance uniformly across ECE metrics derived from these diverse definitions of reliability."}}
{"id": "XvgPGWazqRH", "cdate": 1633790971121, "mdate": null, "content": {"title": "A Closer Look at Distribution Shifts and Out-of-Distribution Generalization on Graphs", "abstract": "Distribution shifts, in which the training distribution differs from the testing distribution, can significantly degrade the performance of Graph Neural Networks (GNNs). We curate GDS, a benchmark of eight datasets reflecting a diverse range of distribution shifts across graphs. We observe that: (1) most domain generalization algorithms fail to work when applied to domain shifts on graphs; and (2) combinations of powerful GNN models and augmentation techniques usually achieve the best out-of-distribution performance. These emphasize the need for domain generalization algorithms tailored for graphs and further graph augmentation techniques that enhance the robustness of predictors."}}
{"id": "2JFVnWuvrvV", "cdate": 1632875720029, "mdate": null, "content": {"title": "A Closer Look at Distribution Shifts and Out-of-Distribution Generalization on Graphs", "abstract": "Distribution shifts, in which the training distribution differs from the testing distribution, can significantly degrade the performance of Graph Neural Networks (GNNs). Although some existing graph classification benchmarks consider distribution shifts, we are far from understanding the effects of distribution shifts on graphs, and more specifically how they differ from distribution shifts in tensor data like images. We ask: (1) how useful are existing domain generalization methods for tackling distribution shifts on graph data? (2) are GNNs capable of generalizing to test graphs from unseen distributions? As a first step to answering these questions, we curate GDS, a benchmark of 8 datasets reflecting a diverse range of distribution shifts across graphs. We observe that in most cases, we need both a suitable domain generalization algorithm and a strong GNN backbone model to optimize out-of-distribution test performance. However, even if we carefully pick such combinations of models and algorithms, the out-of-distribution performance is still much lower than the in-distribution performance. This large gap emphasizes the need for domain generalization algorithms specifically tailored for graphs and strong GNNs that generalize well to out-of-distribution graphs. To facilitate further research, we provide an open-source package that administers the GDS benchmark with modular combinations of popular domain generalization algorithms and GNN backbone models."}}
