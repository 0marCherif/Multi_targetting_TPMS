{"id": "Pm_Ctepo1O", "cdate": 1672531200000, "mdate": 1680302294067, "content": {"title": "A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models", "abstract": ""}}
{"id": "Lp6BJNsd4UV", "cdate": 1672531200000, "mdate": 1682350339301, "content": {"title": "InfoNCE Loss Provably Learns Cluster-Preserving Representations", "abstract": "The goal of contrasting learning is to learn a representation that preserves underlying clusters by keeping samples with similar content, e.g. the ``dogness'' of a dog, close to each other in the space generated by the representation. A common and successful approach for tackling this unsupervised learning problem is minimizing the InfoNCE loss associated with the training samples, where each sample is associated with their augmentations (positive samples such as rotation, crop) and a batch of negative samples (unrelated samples). To the best of our knowledge, it was unanswered if the representation learned by minimizing the InfoNCE loss preserves the underlying data clusters, as it only promotes learning a representation that is faithful to augmentations, i.e., an image and its augmentations have the same representation. Our main result is to show that the representation learned by InfoNCE with a finite number of negative samples is also consistent with respect to clusters in the data, under the condition that the augmentation sets within clusters may be non-overlapping but are close and intertwined, relative to the complexity of the learning function class."}}
{"id": "yZzjANG5IB", "cdate": 1640995200000, "mdate": 1683895018001, "content": {"title": "Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation", "abstract": "We propose an algorithm that uses linear function approximation (LFA) for stochastic shortest path (SSP). Under minimal assumptions, it obtains sublinear regret, is computationally efficient, and u..."}}
{"id": "u0NlHS2y6b", "cdate": 1640995200000, "mdate": 1683895018044, "content": {"title": "Improved Algorithms for Misspecified Linear Markov Decision Processes", "abstract": "For the misspecified linear Markov decision process (MLMDP) model of Jin et al. [2020], we propose an algorithm with three desirable properties. (P1) Its regret after K episodes scales as Kmax{\\ensuremath{\\varepsilon}mis,\\ensuremath{\\varepsilon}tol}, where \\ensuremath{\\varepsilon}mis is the degree of misspecification and \\ensuremath{\\varepsilon}tol is a user-specified error tolerance. (P2) Its space and per-episode time complexities remain bounded as $K\\rightarrow\\infty$. (P3) It does not require \\ensuremath{\\varepsilon}mis as input. To our knowledge, this is the first algorithm satisfying all three properties. For concrete choices of \\ensuremath{\\varepsilon}tol, we also improve existing regret bounds (up to log factors) while achieving either (P2) or (P3) (existing algorithms satisfy neither). At a high level, our algorithm generalizes (to MLMDPs) and refines the Sup-Lin-UCB algorithm, which Takemura et al. [2021] recently showed satisfies (P3) in the contextual bandit setting. We also provide an intuitive interpretation of their result, which informs the design of our algorithm."}}
{"id": "0Ej9OVsBkh_", "cdate": 1640995200000, "mdate": 1675422011763, "content": {"title": "PAC Generalization via Invariant Representations", "abstract": "One method for obtaining generalizable solutions to machine learning tasks when presented with diverse training environments is to find \\textit{invariant representations} of the data. These are representations of the covariates such that the best model on top of the representation is invariant across training environments. In the context of linear Structural Equation Models (SEMs), invariant representations might allow us to learn models with out-of-distribution guarantees, i.e., models that are robust to interventions in the SEM. To address the invariant representation problem in a {\\em finite sample} setting, we consider the notion of $\\epsilon$-approximate invariance. We study the following question: If a representation is approximately invariant with respect to a given number of training interventions, will it continue to be approximately invariant on a larger collection of unseen SEMs? This larger collection of SEMs is generated through a parameterized family of interventions. Inspired by PAC learning, we obtain finite-sample out-of-distribution generalization guarantees for approximate invariance that holds \\textit{probabilistically} over a family of linear SEMs without faithfulness assumptions. Our results show bounds that do not scale in ambient dimension when intervention sites are restricted to lie in a constant size subset of in-degree bounded nodes. We also show how to extend our results to a linear indirect observation model that incorporates latent variables."}}
{"id": "tZC8wzB9JF", "cdate": 1609459200000, "mdate": 1683895018044, "content": {"title": "Improved Algorithms for Misspecified Linear Markov Decision Processes", "abstract": "For the misspecified linear Markov decision process (MLMDP) model of Jin et al. [2020], we propose an algorithm with three desirable properties. (P1) Its regret after $K$ episodes scales as $K \\max \\{ \\varepsilon_{\\text{mis}}, \\varepsilon_{\\text{tol}} \\}$, where $\\varepsilon_{\\text{mis}}$ is the degree of misspecification and $\\varepsilon_{\\text{tol}}$ is a user-specified error tolerance. (P2) Its space and per-episode time complexities remain bounded as $K \\rightarrow \\infty$. (P3) It does not require $\\varepsilon_{\\text{mis}}$ as input. To our knowledge, this is the first algorithm satisfying all three properties. For concrete choices of $\\varepsilon_{\\text{tol}}$, we also improve existing regret bounds (up to log factors) while achieving either (P2) or (P3) (existing algorithms satisfy neither). At a high level, our algorithm generalizes (to MLMDPs) and refines the Sup-Lin-UCB algorithm, which Takemura et al. [2021] recently showed satisfies (P3) for contextual bandits. We also provide an intuitive interpretation of their result, which informs the design of our algorithm."}}
{"id": "_DwlgumooLy", "cdate": 1609459200000, "mdate": 1683895018090, "content": {"title": "L1 Regression with Lewis Weights Subsampling", "abstract": "We consider the problem of finding an approximate solution to $\\ell_1$ regression while only observing a small number of labels. Given an $n \\times d$ unlabeled data matrix $X$, we must choose a small set of $m \\ll n$ rows to observe the labels of, then output an estimate $\\widehat{\\beta}$ whose error on the original problem is within a $1 + \\varepsilon$ factor of optimal. We show that sampling from $X$ according to its Lewis weights and outputting the empirical minimizer succeeds with probability $1-\\delta$ for $m > O(\\frac{1}{\\varepsilon^2} d \\log \\frac{d}{\\varepsilon \\delta})$. This is analogous to the performance of sampling according to leverage scores for $\\ell_2$ regression, but with exponentially better dependence on $\\delta$. We also give a corresponding lower bound of $\\Omega(\\frac{d}{\\varepsilon^2} + (d + \\frac{1}{\\varepsilon^2}) \\log\\frac{1}{\\delta})$."}}
{"id": "YYtnBWTFYu", "cdate": 1609459200000, "mdate": 1683895018043, "content": {"title": "L1 Regression with Lewis Weights Subsampling", "abstract": "We consider the problem of finding an approximate solution to \ud835\udcc1\u2081 regression while only observing a small number of labels. Given an n \u00d7 d unlabeled data matrix X, we must choose a small set of m \u226a n rows to observe the labels of, then output an estimate \u03b2\u0302 whose error on the original problem is within a 1 + \u03b5 factor of optimal. We show that sampling from X according to its Lewis weights and outputting the empirical minimizer succeeds with probability 1-\u03b4 for m > O(1/(\u03b5\u00b2) d log d/(\u03b5 \u03b4)). This is analogous to the performance of sampling according to leverage scores for \ud835\udcc1\u2082 regression, but with exponentially better dependence on \u03b4. We also give a corresponding lower bound of \u03a9(d/(\u03b5\u00b2) + (d + 1/(\u03b5\u00b2)) log 1/(\u03b4))."}}
{"id": "DAzAnYdritH", "cdate": 1609459200000, "mdate": 1683895018025, "content": {"title": "Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation", "abstract": "We propose an algorithm that uses linear function approximation (LFA) for stochastic shortest path (SSP). Under minimal assumptions, it obtains sublinear regret, is computationally efficient, and uses stationary policies. To our knowledge, this is the first such algorithm in the LFA literature (for SSP or other formulations). Our algorithm is a special case of a more general one, which achieves regret square root in the number of episodes given access to a certain computation oracle."}}
{"id": "7PtS6fh18iT", "cdate": 1609459200000, "mdate": 1683895018115, "content": {"title": "Locating Conical Degeneracies in the Spectra of Parametric Self-adjoint Matrices", "abstract": "A simple iterative scheme is proposed for locating the parameter values for which a two-parameter family of real symmetric matrices has a double eigenvalue. The convergence is proved to be quadratic. An extension of the scheme to complex Hermitian matrices (with three parameters) and to the location of triple eigenvalues (five parameters for real symmetric matrices) is also described. Algorithm convergence is illustrated in several examples: a real symmetric family, a complex Hermitian family, a family of matrices with an \u201cavoided crossing\u201d (no convergence), and a five-parameter family of real symmetric matrices with a triple eigenvalue."}}
