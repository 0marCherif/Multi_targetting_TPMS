{"id": "SqUdvYU4I7l", "cdate": 1695369090306, "mdate": 1695369090306, "content": {"title": "ATT3D: Amortized Text-to-3D Object Synthesis", "abstract": "Text-to-3D modeling has seen exciting progress by combining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion recently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a unified model, instead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimization. Our framework - Amortized Text-to-3D (ATT3D) - enables sharing of knowledge between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations."}}
{"id": "3upCnG04XQo", "cdate": 1695368861036, "mdate": 1695368861036, "content": {"title": "Neuralangelo: High-Fidelity Neural Surface Reconstruction", "abstract": "Neural surface reconstruction has shown to be powerful for recovering dense 3D surfaces via image-based neural rendering. However, current methods struggle to recover detailed structures of real-world scenes. To address the issue, we present Neuralangelo, which combines the representation power of multi-resolution 3D hash grids with neural surface rendering. Our approach is enabled by two key ingredients: (1) numerical gradients for computing higher-order derivatives as a smoothing operation and (2) coarseto-fine optimization on the hash grids controlling different levels of details. Even without auxiliary depth, Neuralangelo can effectively recover dense 3D surface structures from multi-view images with a fidelity that significantly surpasses previous methods, enabling detailed large-scale scene reconstruction from RGB video captures."}}
{"id": "Z41QMv-B5g", "cdate": 1695141989773, "mdate": 1695141989773, "content": {"title": "SPACE: Speech-driven Portrait Animation with Controllable Expression", "abstract": "Animating portraits using speech has received growing attention in recent years, with various creative and practical use cases. An ideal generated video should have good lip sync with the audio, natural facial expressions and head motions, and high frame quality. In this work, we present SPACE, which uses speech and a single image to generate high-resolution, and expressive videos with realistic head pose, without requiring a driving video. It uses a multi-stage approach, combining the controllability of facial landmarks with the high-quality synthesis power of a pretrained face generator. SPACE also allows for the control of emotions and their intensities. Our method outperforms prior methods in objective metrics for image quality and facial motions and is strongly preferred by users in pair-wise comparisons."}}
{"id": "JxdHjURqpP", "cdate": 1680088201317, "mdate": 1680088201317, "content": {"title": "Magic3D: High-Resolution Text-to-3D Content Creation", "abstract": "DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2\u00d7 faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications."}}
{"id": "mhYpv7YMz7n", "cdate": 1679903671994, "mdate": null, "content": {"title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers", "abstract": "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's \"paint-with-words\" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/"}}
{"id": "F6G37fYsQ5", "cdate": 1668589501489, "mdate": 1668589501489, "content": {"title": "Domain Stylization: A Fast Covariance Matching Framework Towards Domain Adaptation", "abstract": "Generating computer graphics (CG) rendered synthetic images has been widely used to create simulation environments for\nrobotics/autonomous driving and generate labeled data. Yet, the problem of training models purely with synthetic data remains\nchallenging due to the considerable domain gaps caused by current limitations on rendering. In this paper, we propose a simple yet\neffective domain adaptation framework towards closing such gap at image level. Unlike many GAN-based approaches, our method\naims to match the covariance of the universal feature embeddings across domains, making the adaptation a fast, convenient step and\navoiding the need for potentially difficult GAN training. To align domains more precisely, we further propose a conditional covariance\nmatching framework which iteratively estimates semantic segmentation regions and conditionally matches the class-wise feature\ncovariance given the segmentation regions. We demonstrate that both tasks can mutually refine and considerably improve each other,\nleading to state-of-the-art domain adaptation results. Extensive experiments under multiple synthetic-to-real settings show that our\napproach exceeds the performance of latest domain adaptation approaches. In addition, we offer a quantitative analysis where our\nframework shows considerable reduction in Frechet Inception distance between source and target domains, demonstrating the\neffectiveness of this work in bridging the synthetic-to-real domain gap."}}
{"id": "St5q10aqLTO", "cdate": 1652737864630, "mdate": null, "content": {"title": "Implicit Neural Representations with Levels-of-Experts", "abstract": "Coordinate-based networks, usually in the forms of MLPs, have been successfully applied to the task of predicting high-frequency but low-dimensional signals using coordinate inputs. To scale them to model large-scale signals, previous works resort to hybrid representations, combining a coordinate-based network with a grid-based representation, such as sparse voxels. However, such approaches lack a compact global latent representation in its grid, making it difficult to model a distribution of signals, which is important for generalization tasks. To address the limitation, we propose the Levels-of-Experts (LoE) framework, which is a novel coordinate-based representation consisting of an MLP with periodic, position-dependent weights arranged hierarchically. For each linear layer of the MLP, multiple candidate values of its weight matrix are tiled and replicated across the input space, with different layers replicating at different frequencies. Based on the input, only one of the weight matrices is chosen for each layer. This greatly increases the model capacity without incurring extra computation or compromising generalization capability. We show that the new representation is an efficient and competitive drop-in replacement for a wide range of tasks, including signal fitting, novel view synthesis, and generative modeling."}}
{"id": "VnAwNNJiwDb", "cdate": 1652737355084, "mdate": null, "content": {"title": "Generating Long Videos of Dynamic Scenes", "abstract": "We present a video generation model that accurately reproduces object motion, changes in camera viewpoint, and new content that arises over time. Existing video generation methods often fail to produce new content as a function of time while maintaining consistencies expected in real environments, such as plausible dynamics and object persistence. A common failure case is for content to never change due to over-reliance on inductive bias to provide temporal consistency, such as a single latent code that dictates content for the entire video. On the other extreme, without long-term consistency, generated videos may morph unrealistically between different scenes. To address these limitations, we prioritize the time axis by redesigning the temporal latent representation and learning long-term consistency from data by training on longer videos. We leverage a two-phase training strategy, where we separately train using longer videos at a low resolution and shorter videos at a high resolution. To evaluate the capabilities of our model, we introduce two new benchmark datasets with explicit focus on long-term temporal dynamics."}}
{"id": "TrsAkAbC96", "cdate": 1652737354049, "mdate": null, "content": {"title": "Implicit Warping for Animation with Image Sets", "abstract": "We present a new implicit warping framework for image animation using sets of source images through the transfer of motion of a driving video. A single cross-modal attention layer is used to find correspondences between the source images and the driving image, choose the most appropriate features from different source images, and warp the selected features. This is in contrast to the existing methods that use explicit flow-based warping, which is designed for animation using a single source and does not extend well to multiple sources. The pick-and-choose capability of our framework helps it achieve state-of-the-art results on multiple datasets for image animation using both single and multiple source images."}}
{"id": "-C5iBHQuXgU", "cdate": 1623694496288, "mdate": 1623694496288, "content": {"title": "GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds", "abstract": "We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis."}}
