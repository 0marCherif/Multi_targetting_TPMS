{"id": "bhfp5GlDtGe", "cdate": 1663850406406, "mdate": null, "content": {"title": "Adversarial Imitation Learning with Preferences", "abstract": "Designing an accurate and explainable reward function for many Reinforcement Learning tasks is a cumbersome and tedious process. \nInstead, learning policies directly from the feedback of human teachers naturally integrates human domain knowledge into the policy optimization process. \nHowever, different feedback modalities, such as demonstrations and preferences, provide distinct benefits and disadvantages. For example, demonstrations convey a lot of information about the task but are often hard or costly to obtain from real experts while preferences typically contain less information but are in most cases cheap to generate. \nHowever, existing methods centered around human feedback mostly focus on a single teaching modality, causing them to miss out on important training data while making them less intuitive to use.\nIn this paper we propose a novel method for policy learning that incorporates two different feedback types, namely \\emph{demonstrations} and \\emph{preferences}. \nTo this end, we make use of the connection between discriminator training and density ratio estimation to incorporate preferences into the popular Adversarial Imitation Learning paradigm. \nThis insight allows us to express loss functions over both demonstrations and preferences in a unified framework.\nBesides expert demonstrations, we are also able to learn from imperfect ones and combine them with preferences to achieve improved task performance.\nWe experimentally validate the effectiveness of combining both preferences and demonstrations on common benchmarks and also show that our method can efficiently learn challenging robot manipulation tasks."}}
{"id": "Mzqn__AxyA6", "cdate": 1655376336269, "mdate": null, "content": {"title": "Inferring Versatile Behavior from Demonstrations by Matching Geometric Descriptors", "abstract": "Humans intuitively solve tasks in versatile ways, varying their behavior in terms of trajectory-based planning and for individual steps. Thus, they can easily generalize and adapt to new and changing environments. Current Imitation Learning algorithms often only consider unimodal expert demonstrations and act in a state-action-based setting, making it difficult for them to imitate human behavior in case of versatile demonstrations. Instead, we combine a mixture of movement primitives with a distribution matching objective to learn versatile behaviors that match the expert\u2019s behavior and versatility. To facilitate generalization to novel task configurations, we do not directly match the agent\u2019s and expert\u2019s trajectory distributions but rather work with concise geometric descriptors which generalize well to unseen task configurations. We empirically validate our method on various robot tasks using versatile human demonstrations and compare to imitation learning algorithms in a state-action setting as well as a trajectory-based setting. We find that the geometric descriptors greatly help in generalizing to new task configurations and that combining them with our distribution-matching objective is crucial for representing and reproducing versatile behavior."}}
{"id": "RW46Ulg_UwI", "cdate": 1640995200000, "mdate": 1682325791345, "content": {"title": "Inferring Versatile Behavior from Demonstrations by Matching Geometric Descriptors", "abstract": "Humans intuitively solve tasks in versatile ways, varying their behavior in terms of trajectory-based planning and for individual steps. Thus, they can easily generalize and adapt to new and changing environments. Current Imitation Learning algorithms often only consider unimodal expert demonstrations and act in a state-action-based setting, making it difficult for them to imitate human behavior in case of versatile demonstrations. Instead, we combine a mixture of movement primitives with a distribution matching objective to learn versatile behaviors that match the expert's behavior and versatility. To facilitate generalization to novel task configurations, we do not directly match the agent's and expert's trajectory distributions but rather work with concise geometric descriptors which generalize well to unseen task configurations. We empirically validate our method on various robot tasks using versatile human demonstrations and compare to imitation learning algorithms in a state-action setting as well as a trajectory-based setting. We find that the geometric descriptors greatly help in generalizing to new task configurations and that combining them with our distribution-matching objective is crucial for representing and reproducing versatile behavior."}}
{"id": "7xS6gn6_A7", "cdate": 1640995200000, "mdate": 1682325791318, "content": {"title": "Inferring Versatile Behavior from Demonstrations by Matching Geometric Descriptors", "abstract": "Humans intuitively solve tasks in versatile ways, varying their behavior in terms of trajectory-based planning and for individual steps. Thus, they can easily generalize and adapt to new and changi..."}}
{"id": "gIGi0FDaTSA", "cdate": 1609459200000, "mdate": 1682325791394, "content": {"title": "Automatic Learning of Cognitive Exercises for Socially Assistive Robotics", "abstract": "In this paper, we present a learning approach to facilitate the teaching of new board exercises to assistive robotic systems. We formulate the problem as the learning of action models using Boolean predicates, disjunctive preconditions, and existential quantifiers from demonstrations of successful exercise executions. To be able to cope with exercises whose rules depend on a set of features that are initialized at the beginning of each play-out, we introduce the concept of dynamic context. Furthermore, we show how the learnt knowledge can be represented intuitively in a graphical interface that helps the caregiver understand what the system has learnt. As validation, we conducted a user study in which we evaluated whether and to which extent different types of feedback can affect the subjects\u2019 performance while teaching three types of exercises: (1) sorting numbers; (2) arranging letters; and (3) reproducing shapes sequences in reversed order. The results suggest that textual and graphical feedback are beneficial."}}
{"id": "r1qrye_Mo0", "cdate": 1514764800000, "mdate": 1682325791361, "content": {"title": "Adaptable Multimodal Interaction Framework for Robot-Assisted Cognitive Training", "abstract": "The size of the population with cognitive impairment is increasing worldwide, and socially assistive robotics offers a solution to the growing demand for professional carers. Adaptation to users generates more natural, human-like behavior that may be crucial for a wider robot acceptance. The focus of this work is on robot-assisted cognitive training of the patients that suffer from mild cognitive impairment (MCI) or Alzheimer. We propose a framework that adjusts the level of robot assistance and the way the robot actions are executed, according to the user input. The actions can be performed using any of the following modalities: speech, gesture, and display, or their combination. The choice of modalities depends on the availability of the required resources. The memory state of the user was implemented as a Hidden Markov Model, and it was used to determine the level of robot assistance. A pilot user study was performed to evaluate the effects of the proposed framework on the quality of interaction with the robot."}}
{"id": "eD9JZdUhNQ", "cdate": 1514764800000, "mdate": 1682325791311, "content": {"title": "Adaptive Modality Selection Algorithm in Robot-Assisted Cognitive Training", "abstract": "Interaction of socially assistive robots with users is based on social cues coming from different interaction modalities, such as speech or gestures. However, using all modalities at all times may be inefficient as it can overload the user with redundant information and increase the task completion time. Additionally, users may favor certain modalities over the other as a result of their disability or personal preference. In this paper, we propose an Adaptive Modality Selection (AMS) algorithm that chooses modalities depending on the state of the user and the environment, as well as user preferences. The variables that describe the environment and the user state are defined as resources, and we posit that modalities are successful if certain resources possess specific values during their use. Besides the resources, the proposed algorithm takes into account user preferences which it learns while interacting with users. We tested our algorithm in simulations, and we implemented it on a robotic system that provides cognitive training, specifically Sequential memory exercises. Experimental results show that it is possible to use only a subset of available modalities without compromising the interaction. Moreover, we see a trend for users to perform better when interacting with a system with implemented AMS algorithm."}}
{"id": "5O7BGA6jcG", "cdate": 1514764800000, "mdate": 1682325791308, "content": {"title": "Resource-Based Modality Selection in Robot-Assisted Cognitive Training", "abstract": "The majority of socially assistive robots interact with their users using multiple modalities. Multimodality is an important feature that can enable them to adapt to the user behavior and the environment. In this work, we propose a resource-based modality-selection algorithm that adjusts the use of the robot interaction modalities taking into account the available resources to keep the interaction with the user comfortable and safe. For example, the robot should not enter the board space while the user is occupying it, or speak while the user is speaking. We performed a pilot study in which the robot acted as a caregiver in cognitive training. We compared a system with the proposed algorithm to a baseline system that uses all modalities for all actions unconditionally. Results of the study suggest that a reduced complexity of interaction does not significantly affect the user experience, and may improve task performance."}}
