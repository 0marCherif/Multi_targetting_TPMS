{"id": "qApz5hqKcir", "cdate": 1680472428237, "mdate": 1680472428237, "content": {"title": "Improved Differentially Private Regression via Gradient Boosting", "abstract": "We revisit the problem of differentially private squared error linear regression. We observe that existing state-of-the-art methods are sensitive to the choice of hyper-parameters -- including the ``clipping threshold'' that cannot be set optimally in a data-independent way. We give a new algorithm for private linear regression based on gradient boosting. We show that our method consistently improves over the previous state of the art when the clipping threshold is taken to be fixed without knowledge of the data, rather than optimized in a non-private way -- and that even when we optimize the clipping threshold non-privately, our algorithm is no worse. In addition to a comprehensive set of experiments, we give theoretical insights to explain this behavior."}}
{"id": "uPF2bs14E3p", "cdate": 1665069636337, "mdate": null, "content": {"title": "Differentially Private Gradient Boosting on Linear Learners for Tabular Data", "abstract": "Gradient boosting takes \\emph{linear} combinations of weak base learners. Therefore, absent privacy constraints (when we can exactly optimize over the base models) it is not effective when run over base learner classes that are closed under linear combinations (e.g. linear models). As a result, gradient boosting is typically implemented with tree base learners (e.g., XGBoost), and this has become the state of the art approach in tabular data analysis. Prior work on private gradient boosting focused on taking the state of the art algorithm in the non-private regime---boosting on trees---and making it differentially private. Surprisingly, we find that when we use differentially private learners, gradient boosting over trees is not as effective as gradient boosting over linear learners. In this paper, we propose differentially private gradient-boosted linear models as a private classification method for tabular data. We empirically demonstrate that, under strict privacy constraints, it yields higher F1 scores than the private versions of gradient-boosted trees on five real-world binary classification problems. This work adds to the growing picture that the most effective learning methods under differential privacy may be quite different from the most effective learning methods without privacy."}}
{"id": "5JdyRvTrK0q", "cdate": 1652737354282, "mdate": null, "content": {"title": "Private Synthetic Data for Multitask Learning and Marginal Queries", "abstract": "We provide a differentially private algorithm for producing  synthetic data simultaneously useful for multiple tasks: marginal queries and multitask machine learning (ML). A key innovation in our algorithm is the ability to directly handle numerical features, in contrast to a number of related prior approaches which require numerical features to be first converted into {high cardinality} categorical features via {a binning strategy}. Higher binning granularity is required for better accuracy, but this negatively impacts scalability. Eliminating the need for binning allows us to produce synthetic data preserving large numbers of statistical queries such as marginals on numerical features, and class conditional linear threshold queries. Preserving the latter means that the fraction of points of each class label above a particular half-space is roughly the same in both the real and synthetic data. This is the property that is needed to train a linear classifier in a multitask setting. Our algorithm also allows us to produce high quality synthetic data for mixed marginal queries, that combine both categorical  and numerical features. Our method consistently runs 2-5x faster than the best comparable techniques, and provides significant accuracy improvements in both marginal queries and linear prediction tasks for mixed-type datasets.  "}}
{"id": "ov6Ct9IL0D2", "cdate": 1633790970596, "mdate": null, "content": {"title": "Improving Robustness in Motor Imagery Brain-Computer Interfaces", "abstract": "The method of Common Spatial Patterns (CSP) is widely used for feature extraction of electroencephalography (EEG) data such as in motor imagery brain-computer interface (BCI) systems. It is a data-driven method estimating a set of spatial filters so that the power of the filtered EEG data is maximally separated between imagery classes. This method, however, is prone to overfitting and is known to suffer from poor generalization especially with limited calibration data. In this work, we propose a novel algorithm called Spectrally Adaptive Common Spatial Patterns (SACSP) that improves CSP by learning a temporal/spectral filter for each spatial filter so that the spatial filters are concentrated on the most relevant temporal frequencies for each user. We show the efficacy of SACSP in motor imagery BCI in providing better generalizability and higher classification accuracy from calibration to online control compared to existing methods while providing\nneurophysiologically relevant information about the temporal frequencies of the filtered signals."}}
{"id": "Bm_g50y82F", "cdate": 1598658362995, "mdate": null, "content": {"title": "On Transfer Learning via Linearized Neural Networks", "abstract": "We propose to linearize neural networks for transfer learning via a first order Taylor approximation. Making neural networks linear in this way allows the optimization to become convex (or even closed form) across several tasks. Not only does this vastly simplify the problem, but it allows us to rephrase transfer learning as sharing hyper-parameters across Gaussian processes, which can be solved using standard numerical linear algebra methods. Probabilistically, the framework is interpreted as a Gaussian process model with finite Neural Tangent Kernels. Our approach is fast not only thanks to the linearization, but also because we leverage numerical results from relating the Fisher Information Matrix to the NTK."}}
{"id": "xgtGSpBwQ6X", "cdate": 1598658165412, "mdate": null, "content": {"title": "Deep Transfer Learning with Ridge Regression", "abstract": "The large amount of online data and vast array of computing resources enable current researchers in both industry and academia to employ the power of deep learning with neural networks. While deep models trained with massive amounts of data demonstrate promising generalisation ability on unseen data from relevant domains, the computational cost of finetuning gradually becomes a bottleneck in transfering the learning to new domains. We address this issue by leveraging the low-rank property of learnt feature vectors produced from deep neural networks(DNNs) with the closed-form solution provided in kernel ridge regression (KRR). This frees transfer learning from finetuning and replaces it with an ensemble of linear systems with many fewer hyperparameters.  Our method is successful on supervised and semi-supervised transfer learning tasks"}}
{"id": "ou7uDDPl0Yu", "cdate": 1598658082069, "mdate": null, "content": {"title": "Similarity of Neural Networks with Gradients", "abstract": "A suitable similarity index for comparing learnt neural networks plays an important role in understanding the behaviour of the highly-nonlinear functions, and can provide insights on further theoretical analysis and empirical studies. We define two key steps when comparing models: firstly, the representation abstracted from the learnt model, where we propose to leverage both feature vectors and gradient ones (which are largely ignored in prior work) into designing the representation of a neural network.  Secondly, we define the employed similarity index which gives desired invariance properties, and we facilitate the chosen oneswith sketching techniques for comparing various datasets efficiently.  Empirically,  we show thatthe proposed approach provides a state-of-the-art method for computing similarity of neural networks that are trained independently on different datasets and the tasks defined by the datasets."}}
{"id": "rkgFXR4KPr", "cdate": 1569439265153, "mdate": null, "content": {"title": "A Simple Recurrent Unit with Reduced Tensor Product Representations", "abstract": "Widely used recurrent units, including Long-short Term Memory (LSTM) and Gated Recurrent Unit (GRU), perform well on natural language tasks, but their ability to learn structured representations is still questionable. Exploiting reduced Tensor Product Representations (TPRs) --- distributed representations of symbolic structure in which vector-embedded symbols are bound to vector-embedded structural positions --- we propose the TPRU, a simple recurrent unit that, at each time step, explicitly executes structural-role binding and unbinding operations to incorporate structural information into learning. The gradient analysis of our proposed TPRU is conducted to support our model design, and its performance on multiple datasets shows the effectiveness of it. Furthermore, observations on linguistically grounded study demonstrate the interpretability of our TPRU."}}
{"id": "Byla224KPr", "cdate": 1569438901070, "mdate": null, "content": {"title": "An Empirical Study on Post-processing Methods for Word Embeddings", "abstract": "Word embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks."}}
{"id": "BJe1myXT-V", "cdate": 1546624791093, "mdate": null, "content": {"title": "Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding", "abstract": ""}}
