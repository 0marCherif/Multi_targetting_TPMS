{"id": "u84zejvxLU", "cdate": 1577836800000, "mdate": null, "content": {"title": "Gaussian Gated Linear Networks", "abstract": "We propose the Gaussian Gated Linear Network (G-GLN), an extension to the recently proposed GLN family of deep neural networks. Instead of using backpropagation to learn features, GLNs have a distributed and local credit assignment mechanism based on optimizing a convex objective. This gives rise to many desirable properties including universality, data-efficient online learning, trivial interpretability and robustness to catastrophic forgetting. We extend the GLN framework from classification to multiple regression and density modelling by generalizing geometric mixing to a product of Gaussian densities. The G-GLN achieves competitive or state-of-the-art performance on several univariate and multivariate regression benchmarks, and we demonstrate its applicability to practical tasks including online contextual bandits and density estimation via denoising."}}
{"id": "mhOid4MVXRv", "cdate": 1577836800000, "mdate": null, "content": {"title": "Online Learning in Contextual Bandits using Gated Linear Networks", "abstract": "We introduce a new and completely online contextual bandit algorithm called Gated Linear Contextual Bandits (GLCB). This algorithm is based on Gated Linear Networks (GLNs), a recently introduced deep learning architecture with properties well-suited to the online setting. Leveraging data-dependent gating properties of the GLN we are able to estimate prediction uncertainty with effectively zero algorithmic overhead. We empirically evaluate GLCB compared to 9 state-of-the-art algorithms that leverage deep neural networks, on a standard benchmark suite of discrete and continuous contextual bandit problems. GLCB obtains median first-place despite being the only online method, and we further support these results with a theoretical study of its convergence properties."}}
{"id": "iSH2gItKKOo", "cdate": 1577836800000, "mdate": null, "content": {"title": "Static and Dynamic Values of Computation in MCTS", "abstract": "Monte-Carlo Tree Search (MCTS) is one of the most-widely used methodsfor planning, and has powered many recent advances in artificialintelligence. In MCTS, one typically performs computations(i.e.,..."}}
{"id": "WqkM1YbQP8", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Combinatorial Perspective on Transfer Learning", "abstract": "Human intelligence is characterized not only by the capacity to learn complex skills, but the ability to rapidly adapt and acquire new skills within an ever-changing environment. In this work we study how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data. Our main postulate is that the combination of task segmentation, modular learning and memory-based ensembling can give rise to generalization on an exponentially growing number of unseen tasks. We provide a concrete instantiation of this idea using a combination of: (1) the Forget-Me-Not Process, for task segmentation and memory based ensembling; and (2) Gated Linear Networks, which in contrast to contemporary deep learning techniques use a modular and local learning mechanism. We demonstrate that this system exhibits a number of desirable continual learning properties: robustness to catastrophic forgetting, no negative transfer and increasing levels of positive transfer as more tasks are seen. We show competitive performance against both offline and online methods on standard continual learning benchmarks."}}
{"id": "DnIsY-13fmV", "cdate": 1577836800000, "mdate": null, "content": {"title": "Gaussian Gated Linear Networks", "abstract": "We propose the Gaussian Gated Linear Network (G-GLN), an extension to the recently proposed GLN family of deep neural networks. Instead of using backpropagation to learn features, GLNs have a distributed and local credit assignment mechanism based on optimizing a convex objective. This gives rise to many desirable properties including universality, data-efficient online learning, trivial interpretability and robustness to catastrophic forgetting. We extend the GLN framework from classification to multiple regression and density modelling by generalizing geometric mixing to a product of Gaussian densities. The G-GLN achieves competitive or state-of-the-art performance on several univariate and multivariate regression benchmarks, and we demonstrate its applicability to practical tasks including online contextual bandits and density estimation via denoising."}}
{"id": "9DuRdt_Hm6z", "cdate": 1577836800000, "mdate": null, "content": {"title": "Large-scale multilingual audio visual dubbing", "abstract": "We describe a system for large-scale audiovisual translation and dubbing, which translates videos from one language to another. The source language's speech content is transcribed to text, translated, and automatically synthesized into target language speech using the original speaker's voice. The visual content is translated by synthesizing lip movements for the speaker to match the translated audio, creating a seamless audiovisual experience in the target language. The audio and visual translation subsystems each contain a large-scale generic synthesis model trained on thousands of hours of data in the corresponding domain. These generic models are fine-tuned to a specific speaker before translation, either using an auxiliary corpus of data from the target speaker, or using the video to be translated itself as the input to the fine-tuning process. This report gives an architectural overview of the full system, as well as an in-depth discussion of the video dubbing component. The role of the audio and text components in relation to the full system is outlined, but their design is not discussed in detail. Translated and dubbed demo videos generated using our system can be viewed at https://www.youtube.com/playlist?list=PLSi232j2ZA6_1Exhof5vndzyfbxAhhEs5"}}
{"id": "2g2WqhVpHC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Static and Dynamic Values of Computation in MCTS", "abstract": "Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations (i.e., simulations) to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the \"myopic\" limitations of existing computation-value-based methods in two senses: (I) we are able to account for the impact of non-immediate (ie, future) computations (II) on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art."}}
{"id": "rygf-kSYwH", "cdate": 1569439481659, "mdate": null, "content": {"title": "Behaviour Suite for Reinforcement Learning", "abstract": "This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers."}}
{"id": "7Ix7l0d6A28", "cdate": 1546300800000, "mdate": null, "content": {"title": "Optimizing the depth and the direction of prospective planning using information values", "abstract": "Author summary When faced with several choices in complex environments like chess, thinking about all the potential consequences of each choice, infinitely deep into the future, is simply impossible due to time and cognitive limitations. An outstanding question is what is the best direction and depth of thinking about the future? Here we propose a mathematical algorithm that computes, along the course of planning, the benefit of thinking another step in a given direction into the future, and compares that with the cost of thinking in order to compute the net benefit. We show that this algorithm is consistent with several behavioral patterns observed in humans and animals, suggesting that they, too, make efficient use of their time and cognitive resources when deciding how deep to think."}}
{"id": "3DuXeIVmYmD", "cdate": 1483228800000, "mdate": null, "content": {"title": "Algorithms for Obtaining Parsimonious Higher Order Neurons", "abstract": "Most neurons in the central nervous system exhibit all-or-none firing behavior. This makes Boolean Functions (BFs) tractable candidates for representing computations performed by neurons, especially at finer time scales, even though BFs may fail to capture some of the richness of neuronal computations such as temporal dynamics. One biologically plausible way to realize BFs is to compute a weighted sum of products of inputs and pass it through a heaviside step function. This representation is called a Higher Order Neuron (HON). A HON can trivially represent any n-variable BF with $$2^n$$ product terms. There have been several algorithms proposed for obtaining representations with fewer product terms. In this work, we propose improvements over previous algorithms for obtaining parsimonious HON representations and present numerical comparisons. In particular, we improve the algorithm proposed by Sezener and Oztop [1] and cut down its time complexity drastically, and develop a novel hybrid algorithm by combining metaheuristic search and the deterministic algorithm of Oztop [2]."}}
