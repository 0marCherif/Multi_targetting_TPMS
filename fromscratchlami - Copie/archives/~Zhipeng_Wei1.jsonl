{"id": "cSKhkXFIaJn", "cdate": 1667440558728, "mdate": null, "content": {"title": "Towards Transferable Adversarial Attacks on Vision Transformers", "abstract": "Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifcally, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance."}}
{"id": "PUypOk-henF", "cdate": 1667440506547, "mdate": 1667440506547, "content": {"title": "Boosting the Transferability of Video Adversarial Examples via Temporal Translation", "abstract": "Although deep-learning based video recognition models have achieved remarkable success, they are vulnerable to adversarial examples that are generated by adding humanimperceptible perturbations on clean video samples. As indicated in recent studies, adversarial examples are transferable, which makes it feasible for black-box attacks in real-world applications. Nevertheless, most existing adversarial attack methods have poor transferability when attacking other video models and transfer-based attacks on video models are still unexplored. To this end, we propose to boost the transferability of video adversarial examples for black-box attacks on video recognition models. Through extensive analysis, we discover that different video recognition models rely on different discriminative temporal patterns, leading to the poor transferability of video adversarial examples. This motivates us to introduce a temporal translation attack method, which optimizes the adversarial perturbations over a set of temporal translated video clips. By generating adversarial examples over translated videos, the resulting adversarial examples are less sensitive to temporal patterns existed in the whitebox model being attacked and thus can be better transferred. Extensive experiments on the Kinetics-400 dataset and the UCF-101 dataset demonstrate that our method can signifcantly boost the transferability of video adversarial examples. For transfer-based attack against video recognition models, it achieves a 61.56% average attack success rate on the Kinetics-400 and 48.60% on the UCF-101."}}
{"id": "m_aMRElZmRu", "cdate": 1667440403455, "mdate": 1667440403455, "content": {"title": "Cross-Modal Transferable Adversarial Attacks from Images to Videos", "abstract": "Recent studies have shown that adversarial examples hand-crafted on one white-box model can be used to attack other black-box models. Such cross-model transferability makes it feasible to perform black-box attacks, which has raised security concerns for real-world DNNs applications. Nevertheless, existing works mostly focus on investigating the adversarial transferability across different deep models that share the same modality of input data. The cross-modal transferability of adversarial perturbation has never been explored. This paper investigates the transferability of adversarial perturbation across different modalities, i.e., leveraging adversarial perturbation generated on\nwhite-box image models to attack black-box video models. Specifically, motivated by the observation that the low-level feature space between images and video frames are similar, we propose a simple yet effective cross-modal attack method, named as Image To Video (I2V) attack. I2V generates adversarial frames by minimizing the cosine similarity between features of pre-trained image models from\nadversarial and benign examples, then combines the generated adversarial frames to perform black-box attacks on video recognition models. Extensive experiments demonstrate that I2V can achieve high attack success rates on different black-box video recognition models. On Kinetics-400 and UCF-101, I2V achieves an average attack success rate of 77.88% and 65.68%, respectively, which sheds light on\nthe feasibility of cross-modal adversarial attacks."}}
{"id": "go0P5gsBE2", "cdate": 1663849968519, "mdate": null, "content": {"title": "Rethinking Data Augmentation for Improving Transferable Targeted Attacks", "abstract": "Diverse input patterns induced by data augmentations prevent crafted adversarial perturbations from over-fitting to white-box models, hence improving the transferability of adversarial examples for non-targeted attacks. Nevertheless, current data augmentation methods usually perform unsatisfactory for transferable targeted attacks. In this paper, we revisit the commonly used data augmentation method - DI, which is originally proposed to improve non-targeted transferability and discover that its unsatisfactory performance in targeted transferability is mainly caused by the unreasonable restricted diversity. Besides, we also show that directly increasing the diversity of input patterns offers better transferability. In addition, our analysis of attention heatmaps suggests that incorporating more diverse input patterns into optimizing perturbations enlarges the discriminative regions of the target class in the white-box model. Therefore, these generated perturbations can activate discriminative regions of other models with high probabilities. Motivated by this observation, we propose to optimize perturbations with a set of augmented images that have various discriminative regions of the target class in the white-box model. Specifically, we design a data augmentation method, which includes multiple image transformations that can significantly change discriminative regions of the target class, to improve transferable targeted attacks by a large margin. On the ImageNet-compatible dataset, our method achieves an average of 92.5\\% targeted attack success rate in the ensemble transfer scenario, shedding light on transfer-based targeted attacks. "}}
{"id": "k0dQPRa0Mf", "cdate": 1640995200000, "mdate": 1668136229348, "content": {"title": "Adaptive Temporal Grouping for Black-box Adversarial Attacks on Videos", "abstract": ""}}
{"id": "gCG6IqDjrJN", "cdate": 1640995200000, "mdate": 1668136229336, "content": {"title": "Attacking Video Recognition Models with Bullet-Screen Comments", "abstract": ""}}
{"id": "SxbCdbBkXSF", "cdate": 1640995200000, "mdate": 1668136229706, "content": {"title": "Cross-Modal Transferable Adversarial Attacks from Images to Videos", "abstract": ""}}
{"id": "PYoQvEfMwU4", "cdate": 1640995200000, "mdate": 1668136229366, "content": {"title": "Incorporating Locality of Images to Generate Targeted Transferable Adversarial Examples", "abstract": ""}}
{"id": "8G4lz4dv60", "cdate": 1640995200000, "mdate": 1668136229299, "content": {"title": "Towards Transferable Adversarial Attacks on Vision Transformers", "abstract": ""}}
{"id": "5P-B3XKp1ga", "cdate": 1640995200000, "mdate": 1668136229378, "content": {"title": "Boosting the Transferability of Video Adversarial Examples via Temporal Translation", "abstract": ""}}
