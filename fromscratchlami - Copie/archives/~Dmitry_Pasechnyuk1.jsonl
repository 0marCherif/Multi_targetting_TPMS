{"id": "rjDziEPQLQs", "cdate": 1652737531823, "mdate": null, "content": {"title": "A Damped Newton Method Achieves Global $\\mathcal O \\left(\\frac{1}{k^2}\\right)$  and Local Quadratic  Convergence Rate", "abstract": "In this paper, we present the first stepsize schedule for Newton method resulting in fast global and local convergence guarantees. In particular, we a) prove an $\\mathcal O \\left( 1/{k^2} \\right)$ global rate, which matches the state-of-the-art global rate of cubically regularized Newton method of Polyak and Nesterov (2006) and of regularized Newton method of Mishchenko (2021), and the later variant of Doikov and Nesterov (2021), b) prove a local quadratic rate, which matches the best-known local rate of second-order methods, and c) our stepsize formula is simple, explicit, and does not require solving any subproblem. Our convergence proofs hold under affine-invariant assumptions closely related to the notion of self-concordance. Finally, our method has competitive performance when compared to existing baselines which share the same fast global convergence guarantees."}}
{"id": "N4_0aCD6CI", "cdate": 1640995200000, "mdate": 1677950024870, "content": {"title": "Oracle Complexity Separation in Convex Optimization", "abstract": ""}}
{"id": "veNboyLCjK", "cdate": 1609459200000, "mdate": 1677950024878, "content": {"title": "On the Computational Efficiency of Catalyst Accelerated Coordinate Descent", "abstract": ""}}
{"id": "m-SAWQFAl9", "cdate": 1609459200000, "mdate": 1677950024870, "content": {"title": "Inexact model: a framework for optimization and variational inequalities", "abstract": ""}}
{"id": "c9V_Zv-E0zj", "cdate": 1609459200000, "mdate": 1677950024884, "content": {"title": "Adaptive Catalyst for Smooth Convex Optimization", "abstract": ""}}
{"id": "C8Bt5zh20A", "cdate": 1609459200000, "mdate": 1677950024881, "content": {"title": "Network utility maximization by updating individual transmission rates", "abstract": ""}}
{"id": "Cq4BAEmB_N", "cdate": 1546300800000, "mdate": 1652129273856, "content": {"title": "Gradient Methods for Problems with Inexact Model of the Objective", "abstract": "We consider optimization methods for convex minimization problems under inexact information on the objective function. We introduce inexact model of the objective, which as a particular cases includes inexact oracle [16] and relative smoothness condition [36]. We analyze gradient method which uses this inexact model and obtain convergence rates for convex and strongly convex problems. To show potential applications of our general framework we consider three particular problems. The first one is clustering by electorial model introduced in [41]. The second one is approximating optimal transport distance, for which we propose a Proximal Sinkhorn algorithm. The third one is devoted to approximating optimal transport barycenter and we propose a Proximal Iterative Bregman Projections algorithm. We also illustrate the practical performance of our algorithms by numerical experiments."}}
