{"id": "xajpk9Wfqz", "cdate": 1672531200000, "mdate": 1682549358734, "content": {"title": "Stronger 3SUM-Indexing Lower Bounds", "abstract": ""}}
{"id": "mYcALwI56Sd", "cdate": 1672531200000, "mdate": 1682549358791, "content": {"title": "Super-Logarithmic Lower Bounds for Dynamic Graph Problems", "abstract": "In this work, we prove a $\\tilde{\\Omega}(\\lg^{3/2} n )$ unconditional lower bound on the maximum of the query time and update time for dynamic data structures supporting reachability queries in $n$-node directed acyclic graphs under edge insertions. This is the first super-logarithmic lower bound for any natural graph problem. In proving the lower bound, we also make novel contributions to the state-of-the-art data structure lower bound techniques that we hope may lead to further progress in proving lower bounds."}}
{"id": "inrsKW5um_", "cdate": 1672531200000, "mdate": 1682549358736, "content": {"title": "Sparse Dimensionality Reduction Revisited", "abstract": "The sparse Johnson-Lindenstrauss transform is one of the central techniques in dimensionality reduction. It supports embedding a set of $n$ points in $\\mathbb{R}^d$ into $m=O(\\varepsilon^{-2} \\lg n)$ dimensions while preserving all pairwise distances to within $1 \\pm \\varepsilon$. Each input point $x$ is embedded to $Ax$, where $A$ is an $m \\times d$ matrix having $s$ non-zeros per column, allowing for an embedding time of $O(s \\|x\\|_0)$. Since the sparsity of $A$ governs the embedding time, much work has gone into improving the sparsity $s$. The current state-of-the-art by Kane and Nelson (JACM'14) shows that $s = O(\\varepsilon ^{-1} \\lg n)$ suffices. This is almost matched by a lower bound of $s = \\Omega(\\varepsilon ^{-1} \\lg n/\\lg(1/\\varepsilon))$ by Nelson and Nguyen (STOC'13). Previous work thus suggests that we have near-optimal embeddings. In this work, we revisit sparse embeddings and identify a loophole in the lower bound. Concretely, it requires $d \\geq n$, which in many applications is unrealistic. We exploit this loophole to give a sparser embedding when $d = o(n)$, achieving $s = O(\\varepsilon^{-1}(\\lg n/\\lg(1/\\varepsilon)+\\lg^{2/3}n \\lg^{1/3} d))$. We also complement our analysis by strengthening the lower bound of Nelson and Nguyen to hold also when $d \\ll n$, thereby matching the first term in our new sparsity upper bound. Finally, we also improve the sparsity of the best oblivious subspace embeddings for optimal embedding dimensionality."}}
{"id": "bbG1o1ZGa34", "cdate": 1672531200000, "mdate": 1682549358794, "content": {"title": "AdaBoost is not an Optimal Weak to Strong Learner", "abstract": "AdaBoost is a classic boosting algorithm for combining multiple inaccurate classifiers produced by a weak learner, to produce a strong learner with arbitrarily high accuracy when given enough training data. Determining the optimal number of samples necessary to obtain a given accuracy of the strong learner, is a basic learning theoretic question. Larsen and Ritzert (NeurIPS'22) recently presented the first provably optimal weak-to-strong learner. However, their algorithm is somewhat complicated and it remains an intriguing question whether the prototypical boosting algorithm AdaBoost also makes optimal use of training samples. In this work, we answer this question in the negative. Concretely, we show that the sample complexity of AdaBoost, and other classic variations thereof, are sub-optimal by at least one logarithmic factor in the desired accuracy of the strong learner."}}
{"id": "ZgRsCX3_wU", "cdate": 1672531200000, "mdate": 1682549358732, "content": {"title": "Fast Discrepancy Minimization with Hereditary Guarantees", "abstract": "Efficiently computing low discrepancy colorings of various set systems, has been studied extensively since the breakthrough work by Bansal (FOCS 2010), who gave the first polynomial time algorithms for several important settings, including for general set systems, sparse set systems and for set systems with bounded hereditary discrepancy. The hereditary discrepancy of a set system, is the maximum discrepancy over all set systems obtainable by deleting a subset of the ground elements. While being polynomial time, Bansal's algorithms were not practical, with e.g. his algorithm for the hereditary setup running in time \u03a9(mn4.5) for set systems with m sets over a ground set of n elements. More efficient algorithms have since then been developed for general and sparse set systems, however, for the hereditary case, Bansal's algorithm remains state-of-the-art. In this work, we give a significantly faster algorithm with hereditary guarantees, running in O(mn2 lg(2 + m/n) + n3) time. Our algorithm is based on new structural insights into set systems with bounded hereditary discrepancy. We also implement our algorithm and show experimentally that it computes colorings that are significantly better than random and finishes in a reasonable amount of time, even on set systems with thousands of sets over a ground set of thousands of elements."}}
{"id": "YFngR9BeJq", "cdate": 1672531200000, "mdate": 1681593528971, "content": {"title": "Diagonalization Games", "abstract": ""}}
{"id": "T9w-y3fCOL3", "cdate": 1672531200000, "mdate": 1682549358734, "content": {"title": "The Impossibility of Parallelizing Boosting", "abstract": "The aim of boosting is to convert a sequence of weak learners into a strong learner. At their heart, these methods are fully sequential. In this paper, we investigate the possibility of parallelizing boosting. Our main contribution is a strong negative result, implying that significant parallelization of boosting requires an exponential blow-up in the total computing resources needed for training."}}
{"id": "Q0vcVkQfey", "cdate": 1672531200000, "mdate": 1681593528903, "content": {"title": "Diagonalization Games", "abstract": ""}}
{"id": "0leMQbujPZ", "cdate": 1672531200000, "mdate": 1682549358793, "content": {"title": "Barriers for Faster Dimensionality Reduction", "abstract": "The Johnson-Lindenstrauss transform allows one to embed a dataset of n points in \u211d^d into \u211d^m, while preserving the pairwise distance between any pair of points up to a factor (1 \u00b1 \u03b5), provided that m = \u03a9(\u03b5^{-2} lg n). The transform has found an overwhelming number of algorithmic applications, allowing to speed up algorithms and reducing memory consumption at the price of a small loss in accuracy. A central line of research on such transforms, focus on developing fast embedding algorithms, with the classic example being the Fast JL transform by Ailon and Chazelle. All known such algorithms have an embedding time of \u03a9(d lg d), but no lower bounds rule out a clean O(d) embedding time. In this work, we establish the first non-trivial lower bounds (of magnitude \u03a9(m lg m)) for a large class of embedding algorithms, including in particular most known upper bounds."}}
{"id": "0fp0tepVWA", "cdate": 1672531200000, "mdate": 1682549358732, "content": {"title": "How to Compress Encrypted Data", "abstract": "We study the task of obliviously compressing a vector comprised of n ciphertexts of size $$\\xi $$ bits each, where at most t of the corresponding plaintexts are non-zero. This problem commonly features in applications involving encrypted outsourced storages, such as searchable encryption or oblivious message retrieval. We present two new algorithms with provable worst-case guarantees, solving this problem by using only homomorphic additions and multiplications by constants. Both of our new constructions improve upon the state of the art asymptotically and concretely. Our first construction, based on sparse polynomials, is perfectly correct and the first to achieve an asymptotically optimal compression rate by compressing the input vector into $$\\mathcal {O}({t \\xi })$$ bits. Compression can be performed homomorphically by performing $$\\mathcal {O}({n \\log n})$$ homomorphic additions and multiplications by constants. The main drawback of this construction is a decoding complexity of $$\\varOmega (\\sqrt{n})$$ . Our second construction is based on a novel variant of invertible bloom lookup tables and is correct with probability $$1-2^{-\\kappa }$$ . It has a slightly worse compression rate compared to our first construction as it compresses the input vector into $$\\mathcal {O}({\\xi \\kappa t /\\log t})$$ bits, where $$\\kappa \\ge \\log t$$ . In exchange, both compression and decompression of this construction are highly efficient. The compression complexity is dominated by $$\\mathcal {O}({n \\kappa /\\log t})$$ homomorphic additions and multiplications by constants. The decompression complexity is dominated by $$\\mathcal {O}({\\kappa t /\\log t})$$ decryption operations and equally many inversions of a pseudorandom permutation."}}
