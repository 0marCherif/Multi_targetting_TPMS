{"id": "DkJEy-75E_", "cdate": 1672531200000, "mdate": 1682325781695, "content": {"title": "Linearization Algorithms for Fully Composite Optimization", "abstract": "This paper studies first-order algorithms for solving fully composite optimization problems over convex and compact sets. We leverage the structure of the objective by handling its differentiable and non-differentiable components separately, linearizing only the smooth parts. This provides us with new generalizations of the classical Frank-Wolfe method and the Conditional Gradient Sliding algorithm, that cater to a subclass of non-differentiable problems. Our algorithms rely on a stronger version of the linear minimization oracle, which can be efficiently implemented in several practical applications. We provide the basic version of our method with an affine-invariant analysis and prove global convergence rates for both convex and non-convex objectives. Furthermore, in the convex case, we propose an accelerated method with correspondingly improved complexity. Finally, we provide illustrative experiments to support our theoretical results."}}
{"id": "6rgewRqqDf", "cdate": 1640995200000, "mdate": 1674814164244, "content": {"title": "Faster One-Sample Stochastic Conditional Gradient Method for Composite Convex Minimization", "abstract": "We propose a stochastic conditional gradient method (CGM) for minimizing convex finite-sum objectives formed as a sum of smooth and non-smooth terms. Existing CGM variants for this template either suffer from slow convergence rates, or require carefully increasing the batch size over the course of the algorithm\u2019s execution, which leads to computing full gradients. In contrast, the proposed method, equipped with a stochastic average gradient (SAG) estimator, requires only one sample per iteration. Nevertheless, it guarantees fast convergence rates on par with more sophisticated variance reduction techniques. In applications we put special emphasis on problems with a large number of separable constraints. Such problems are prevalent among semidefinite programming (SDP) formulations arising in machine learning and theoretical computer science. We provide numerical experiments on matrix completion, unsupervised clustering, and sparsest-cut SDPs."}}
{"id": "DtXBYsSOxCD", "cdate": 1621630249728, "mdate": null, "content": {"title": "A first-order primal-dual method with adaptivity to local smoothness", "abstract": "We consider the problem of finding a saddle point for the convex-concave objective $\\min_x \\max_y f(x) + \\langle Ax, y\\rangle - g^*(y)$, where $f$ is a convex function with locally Lipschitz gradient and $g$ is convex and possibly non-smooth. We propose an adaptive version of the Condat-V\u0169 algorithm, which alternates between primal gradient steps and dual proximal steps. The method achieves stepsize adaptivity through a simple rule involving $\\|A\\|$ and the norm of recently computed gradients of $f$. Under standard assumptions, we prove an $\\mathcal{O}(k^{-1})$ ergodic convergence rate. Furthermore, when $f$ is also locally strongly convex and $A$ has full row rank we show that our method converges with a linear rate. Numerical experiments are provided for illustrating the practical performance of the algorithm."}}
{"id": "lgpwp-E_NR", "cdate": 1609459200000, "mdate": 1674814164239, "content": {"title": "A first-order primal-dual method with adaptivity to local smoothness", "abstract": "We consider the problem of finding a saddle point for the convex-concave objective $\\min_x \\max_y f(x) + \\langle Ax, y\\rangle - g^*(y)$, where $f$ is a convex function with locally Lipschitz gradient and $g$ is convex and possibly non-smooth. We propose an adaptive version of the Condat-V\u0169 algorithm, which alternates between primal gradient steps and dual proximal steps. The method achieves stepsize adaptivity through a simple rule involving $\\|A\\|$ and the norm of recently computed gradients of $f$. Under standard assumptions, we prove an $\\mathcal{O}(k^{-1})$ ergodic convergence rate. Furthermore, when $f$ is also locally strongly convex and $A$ has full row rank we show that our method converges with a linear rate. Numerical experiments are provided for illustrating the practical performance of the algorithm."}}
{"id": "FoMtrWl7Yp1", "cdate": 1609459200000, "mdate": 1682325781683, "content": {"title": "A first-order primal-dual method with adaptivity to local smoothness", "abstract": "We consider the problem of finding a saddle point for the convex-concave objective $\\min_x \\max_y f(x) + \\langle Ax, y\\rangle - g^*(y)$, where $f$ is a convex function with locally Lipschitz gradient and $g$ is convex and possibly non-smooth. We propose an adaptive version of the Condat-V\\~u algorithm, which alternates between primal gradient steps and dual proximal steps. The method achieves stepsize adaptivity through a simple rule involving $\\|A\\|$ and the norm of recently computed gradients of $f$. Under standard assumptions, we prove an $\\mathcal{O}(k^{-1})$ ergodic convergence rate. Furthermore, when $f$ is also locally strongly convex and $A$ has full row rank we show that our method converges with a linear rate. Numerical experiments are provided for illustrating the practical performance of the algorithm."}}
{"id": "ZIdkdC-tzZt", "cdate": 1577836800000, "mdate": 1657299145071, "content": {"title": "Conditional gradient methods for stochastically constrained convex minimization", "abstract": "We propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints. Instances of this template naturall..."}}
{"id": "Kat5wPkQPp1", "cdate": 1577836800000, "mdate": null, "content": {"title": "Conditional gradient methods for stochastically constrained convex minimization", "abstract": "We propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints. Instances of this template naturally arise from SDP-relaxations of combinatorial problems, which involve a number of constraints that is polynomial in the problem dimension. The most important feature of our framework is that only a subset of the constraints is processed at each iteration, thus gaining a computational advantage over prior works that require full passes. Our algorithms rely on variance reduction and smoothing used in conjunction with conditional gradient steps, and are accompanied by rigorous convergence guarantees. Preliminary numerical experiments are provided for illustrating the practical performance of the methods."}}
