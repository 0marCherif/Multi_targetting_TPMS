{"id": "bh1dqW7YjQ", "cdate": 1689005951122, "mdate": 1689005951122, "content": {"title": "Database reasoning over text", "abstract": "Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as \"List/Count all female athletes who were born in 20th century\", which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries. Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%. On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context."}}
{"id": "R82eeIF4rP_", "cdate": 1677713830886, "mdate": null, "content": {"title": "Attention-likelihood relationship in Transformers", "abstract": "We analyze how large language models (LLMs) represent out-of-context words, investigating their reliance on the given context to capture their semantics. Our likelihood-guided text perturbations reveal a correlation between token likelihood and attention values in transformer-based language models. Extensive experiments reveal that unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers. These findings have valuable implications for assessing the robustness of LLMs in real-world scenarios. Fully reproducible codebase at [url]."}}
{"id": "yd9XANx1dX", "cdate": 1672531200000, "mdate": 1696333640281, "content": {"title": "Integrating Item Relevance in Training Loss for Sequential Recommender Systems", "abstract": "Sequential Recommender Systems (SRSs) are a popular type of recommender system that learns from a user's history to predict the next item they are likely to interact with. However, user interactions can be affected by noise stemming from account sharing, inconsistent preferences, or accidental clicks. To address this issue, we (i) propose a new evaluation protocol that takes multiple future items into account and (ii) introduce a novel relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10 and 0.88% in the traditional evaluation protocol, while in the new evaluation protocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best performing models."}}
{"id": "afs1x4eSUJN", "cdate": 1672531200000, "mdate": 1696333640221, "content": {"title": "RRAML: Reinforced Retrieval Augmented Machine Learning", "abstract": "The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability. To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database. By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the computational intensity involved. Additionally we seamlessly link the retriever's task with the reasoner, mitigating hallucinations and reducing irrelevant, and potentially damaging retrieved documents. We believe that the research agenda outlined in this paper has the potential to profoundly impact the field of AI, democratizing access to and utilization of LLMs for a wide range of entities."}}
{"id": "VimxhwxL9L", "cdate": 1672531200000, "mdate": 1696513474202, "content": {"title": "The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples", "abstract": "Deep learning-based recommender systems have become an integral part of several online platforms. However, their black-box nature emphasizes the need for explainable artificial intelligence (XAI) approaches to provide human-understandable reasons why a specific item gets recommended to a given user. One such method is counterfactual explanation (CF). While CFs can be highly beneficial for users and system designers, malicious actors may also exploit these explanations to undermine the system's security. In this work, we propose H-CARS, a novel strategy to poison recommender systems via CFs. Specifically, we first train a logical-reasoning-based surrogate model on training data derived from counterfactual explanations. By reversing the learning process of the recommendation model, we thus develop a proficient greedy algorithm to generate fabricated user profiles and their associated interaction records for the aforementioned surrogate model. Our experiments, which employ a well-known CF generation method and are conducted on two distinct datasets, show that H-CARS yields significant and successful attack performance."}}
{"id": "QNoAdinxJb", "cdate": 1672531200000, "mdate": 1696513474053, "content": {"title": "Fauno: The Italian Large Language Model that will leave you senza parole!", "abstract": ""}}
{"id": "LwcOXHAMrN", "cdate": 1672531200000, "mdate": 1681656227114, "content": {"title": "Attention-likelihood relationship in transformers", "abstract": "We analyze how large language models (LLMs) represent out-of-context words, investigating their reliance on the given context to capture their semantics. Our likelihood-guided text perturbations reveal a correlation between token likelihood and attention values in transformer-based language models. Extensive experiments reveal that unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers. These findings have valuable implications for assessing the robustness of LLMs in real-world scenarios. Fully reproducible codebase at https://github.com/Flegyas/AttentionLikelihood."}}
{"id": "IsiHBTJAc-", "cdate": 1672531200000, "mdate": 1683722881507, "content": {"title": "Deep active learning for misinformation detection using geometric deep learning", "abstract": ""}}
{"id": "IqwRsKEzDP3", "cdate": 1672531200000, "mdate": 1696513474205, "content": {"title": "Fauno: The Italian Large Language Model that will leave you senza parole!", "abstract": "This paper presents Fauno, the first and largest open-source Italian conversational Large Language Model (LLM). Our goal with Fauno is to democratize the study of LLMs in Italian, demonstrating that obtaining a fine-tuned conversational bot with a single GPU is possible. In addition, we release a collection of datasets for conversational AI in Italian. The datasets on which we fine-tuned Fauno include various topics such as general question answering, computer science, and medical questions. We release our code and datasets on \\url{https://github.com/RSTLess-research/Fauno-Italian-LLM}"}}
{"id": "FEcZnoVAz", "cdate": 1672531200000, "mdate": 1696513474052, "content": {"title": "Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels", "abstract": "In this paper, we propose a new approach for addressing the challenge of training machine learning models in the presence of noisy labels. By combining a clever usage of distance to class centroids in the items' latent space with a discounting strategy to reduce the importance of samples far away from all the class centroids (i.e., outliers), our method effectively addresses the issue of noisy labels. Our approach is based on the idea that samples farther away from their respective class centroid in the early stages of training are more likely to be noisy. We demonstrate the effectiveness of our method through extensive experiments on several popular benchmark datasets. Our results show that our approach outperforms the state-of-the-art in this area, achieving significant improvements in classification accuracy when the dataset contains noisy labels."}}
