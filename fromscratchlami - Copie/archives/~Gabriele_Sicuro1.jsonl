{"id": "P4l6wOUcy1", "cdate": 1683880091209, "mdate": 1683880091209, "content": {"title": "Aligning random graphs with a sub-tree similarity message-passing algorithm ", "abstract": "The problem of aligning Erd\u02ddos\u2013R\u00b4enyi random graphs is a noisy,\naverage-case version of the graph isomorphism problem, in which a pair of correlated random graphs is observed through a random permutation of their vertices.\nWe study a polynomial time message-passing algorithm devised to solve the inference problem of partially recovering the hidden permutation, in the sparse regime\nwith constant average degrees. We perform extensive numerical simulations to\ndetermine the range of parameters in which this algorithm achieves partial recovery. We also introduce a generalized ensemble of correlated random graphs with\nprescribed degree distributions, and extend the algorithm to this case."}}
{"id": "n1aGk7wR9p", "cdate": 1671911676297, "mdate": 1671911676297, "content": {"title": "Fluctuations, Bias, Variance & Ensemble of Learners: Exact Asymptotics for Convex Losses in High-Dimension", "abstract": "     From the sampling of data to the initialisation of parameters, randomness is ubiquitous in modern Machine Learning practice. Understanding the statistical fluctuations engendered by the different sources of randomness in prediction is therefore key to understanding robust generalisation. In this manuscript we develop a quantitative and rigorous theory for the study of fluctuations in an ensemble of generalised linear models trained on different, but correlated, features in high-dimensions. In particular, we provide a complete description of the asymptotic joint distribution of the empirical risk minimiser for generic convex loss and regularisation in the high-dimensional limit. Our result encompasses a rich set of classification and regression tasks, such as the lazy regime of overparametrised neural networks, or equivalently the random features approximation of kernels. While allowing to study directly the mitigating effect of ensembling (or bagging) on the bias-variance decomposition of the test error, our analysis also helps disentangle the contribution of statistical fluctuations, and the singular role played by the interpolation threshold that are at the roots of the \"double-descent\" phenomenon. "}}
{"id": "XatzglilNDO", "cdate": 1640995200000, "mdate": 1681485248406, "content": {"title": "Fluctuations, Bias, Variance & Ensemble of Learners: Exact Asymptotics for Convex Losses in High-Dimension", "abstract": ""}}
{"id": "JhXvJV5h3KU", "cdate": 1640995200000, "mdate": 1681485248386, "content": {"title": "Fluctuations, Bias, Variance & Ensemble of Learners: Exact Asymptotics for Convex Losses in High-Dimension", "abstract": ""}}
{"id": "7yEIhu0WGp", "cdate": 1640995200000, "mdate": 1681485248328, "content": {"title": "Planted matching problems on random hypergraphs", "abstract": ""}}
{"id": "j3eGyNMPvh", "cdate": 1621629974611, "mdate": null, "content": {"title": "Learning Gaussian Mixtures with Generalized Linear Models: Precise Asymptotics in High-dimensions", "abstract": "Generalised linear models for multi-class classification problems are one of the fundamental building blocks of modern machine learning tasks. In this manuscript, we characterise the learning of a mixture of $K$ Gaussians with generic means and covariances via empirical risk minimisation (ERM) with any convex loss and regularisation. In particular, we prove exact asymptotics characterising the ERM estimator in high-dimensions, extending several previous results about Gaussian mixture classification in the literature. We exemplify our result in two tasks of interest in statistical learning: a) classification for a mixture with sparse means, where we study the efficiency of $\\ell_1$ penalty with respect to $\\ell_2$; b) max-margin multi-class classification, where we characterise the phase transition on the existence of the multi-class logistic maximum likelihood estimator for $K>2$. Finally, we discuss how our theory can be applied beyond the scope of synthetic data, showing that in different cases Gaussian mixtures capture closely the learning curve of classification tasks in real data sets."}}
{"id": "insSUtJuyu_", "cdate": 1609459200000, "mdate": 1681485248368, "content": {"title": "Learning Gaussian Mixtures with Generalised Linear Models: Precise Asymptotics in High-dimensions", "abstract": ""}}
{"id": "auj_5mO1--N", "cdate": 1609459200000, "mdate": 1681485248325, "content": {"title": "Learning Gaussian Mixtures with Generalized Linear Models: Precise Asymptotics in High-dimensions", "abstract": ""}}
{"id": "JzBnuakh1p", "cdate": 1609459200000, "mdate": 1681485248372, "content": {"title": "Aligning random graphs with a sub-tree similarity message-passing algorithm", "abstract": ""}}
{"id": "irHYDreajw", "cdate": 1577836800000, "mdate": 1681485248385, "content": {"title": "Recovery thresholds in the sparse planted matching problem", "abstract": ""}}
