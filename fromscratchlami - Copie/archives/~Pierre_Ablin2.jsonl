{"id": "iGrGI2o9jr", "cdate": 1672531200000, "mdate": 1681490235812, "content": {"title": "Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints", "abstract": ""}}
{"id": "Z24qJD_IrXA", "cdate": 1672531200000, "mdate": 1679901406719, "content": {"title": "A Near-Optimal Algorithm for Bilevel Empirical Risk Minimization", "abstract": ""}}
{"id": "MNXK4wnmeS", "cdate": 1672531200000, "mdate": 1681490235860, "content": {"title": "Monge, Bregman and Occam: Interpretable Optimal Transport in High-Dimensions with Feature-Sparse Maps", "abstract": ""}}
{"id": "1uSzacpyWLH", "cdate": 1652737787285, "mdate": null, "content": {"title": "Benchopt: Reproducible, efficient and collaborative optimization benchmarks", "abstract": "Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: $\\ell_2$-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details."}}
{"id": "1tCuRbPts3J", "cdate": 1652737736352, "mdate": null, "content": {"title": "Do Residual Neural Networks discretize Neural Ordinary Differential Equations?", "abstract": "Neural Ordinary Differential Equations (Neural ODEs) are the continuous analog of Residual Neural Networks (ResNets). We investigate whether the discrete dynamics defined by a ResNet are close to the continuous one of a Neural ODE. We first quantify the distance between the ResNet's hidden state trajectory and the solution of its corresponding Neural ODE. Our bound is tight and, on the negative side, does not go to $0$ with depth $N$ if the residual functions are not smooth with depth. On the positive side, we show that this smoothness is preserved by gradient descent for a ResNet with linear residual functions and small enough initial loss. It ensures an implicit regularization towards a limit Neural ODE at rate $\\frac1N$, uniformly with depth and optimization time. As a byproduct of our analysis, we consider the use of a memory-free discrete adjoint method to train a ResNet by recovering the activations on the fly through a backward pass of the network, and show that this method theoretically succeeds at large depth if the residual functions are Lipschitz with the input. We then show that Heun's method, a second order ODE integration scheme, allows for better gradient estimation with the adjoint method when the residual functions are smooth with depth. We experimentally validate that our adjoint method succeeds at large depth, and that Heun\u2019s method needs fewer layers to succeed. We finally use the adjoint method successfully for fine-tuning very deep ResNets without memory consumption in the residual layers."}}
{"id": "wlEOsQ917F", "cdate": 1652737351337, "mdate": null, "content": {"title": "A framework for bilevel optimization that enables  stochastic and global variance reduction algorithms", "abstract": "Bilevel optimization, the problem of minimizing a value function which involves the arg-minimum of another function, appears in many areas of machine learning. In a large scale empirical risk minimization setting where the number of samples is huge, it is crucial to develop stochastic methods, which only use a few samples at a time to progress. However, computing the gradient of the value function involves solving a linear system, which makes it difficult to derive unbiased stochastic estimates.\nTo overcome this problem we introduce a novel framework, in which the solution of the inner problem, the solution of the linear system, and the main variable evolve at the same time. These directions are written as a sum, making it straightforward to derive unbiased estimates.\nThe simplicity of our approach allows us to develop global variance reduction algorithms, where the dynamics of all variables is subject to variance reduction.\nWe demonstrate that SABA, an adaptation of the celebrated SAGA algorithm in our framework, has $O(\\frac1T)$ convergence rate, and that it achieves linear convergence under Polyak-Lojasciewicz assumption.\nThis is the first stochastic algorithm for bilevel optimization that verifies either of these properties.\nNumerical experiments validate the usefulness of our method."}}
{"id": "mY1s84NB9C", "cdate": 1640995200000, "mdate": 1681490235938, "content": {"title": "Do Residual Neural Networks discretize Neural Ordinary Differential Equations?", "abstract": ""}}
{"id": "lT2HnFBHakW", "cdate": 1640995200000, "mdate": 1681490235728, "content": {"title": "A framework for bilevel optimization that enables stochastic and global variance reduction algorithms", "abstract": ""}}
{"id": "dT9T49uu3h", "cdate": 1640995200000, "mdate": 1671902109669, "content": {"title": "Benchopt: Reproducible, efficient and collaborative optimization benchmarks", "abstract": "Numerical validation is at the core of machine learning research as it allows to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automate, reproduce and publish optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard learning tasks: $\\ell_2$-regularized logistic regression, Lasso, and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of the state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details. We hope that Benchopt will foster collaborative work in the community hence improving the reproducibility of research findings."}}
{"id": "Uk8LvM3TBj", "cdate": 1640995200000, "mdate": 1681490235944, "content": {"title": "Sinkformers: Transformers with Doubly Stochastic Attention", "abstract": ""}}
