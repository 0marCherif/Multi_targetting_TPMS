{"id": "xZG8pE7hT-J", "cdate": 1679947908442, "mdate": 1679947908442, "content": {"title": "Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation", "abstract": "The neural transducer is an end-to-end model for automatic speech recognition (ASR). While the model is well-suited for streaming ASR, the training process remains challenging. During training, the memory requirements may quickly exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence lengths. In this work, we analyze the time and space complexity of a typical transducer training setup. We propose a memory-efficient training method that computes the transducer loss and gradients sample by sample. We present optimizations to increase the efficiency and parallelism of the sample-wise method. In a set of thorough benchmarks, we show that our sample-wise method significantly reduces memory usage, and performs at competitive speed when compared to the default batched computation. As a highlight, we manage to compute the transducer loss and gradients for a batch size of 1024, and audio length of 40 seconds, using only 6 GB of memory."}}
{"id": "NCqpX1tESsj", "cdate": 1595273932062, "mdate": null, "content": {"title": "Sequence Discriminative Distributed Training of Long Short-Term Memory Recurrent Neural Networks", "abstract": "We recently showed that Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform state-of-the-art deep neural networks (DNNs) for large scale acoustic model- ing where the models were trained with the cross-entropy (CE) criterion. It has also been shown that sequence discrimina- tive training of DNNs initially trained with the CE criterion gives significant improvements. In this paper, we investigate se- quence discriminative training of LSTM RNNs in a large scale acoustic modeling task. We train the models in a distributed manner using asynchronous stochastic gradient descent opti- mization technique. We compare two sequence discriminative criteria \u2013 maximum mutual information and state-level mini- mum Bayes risk, and we investigate a number of variations of the basic training strategy to better understand issues raised by both the sequential model, and the objective function. We ob- tain significant gains over the CE trained LSTM RNN model using sequence discriminative training techniques."}}
{"id": "EHk6kBqExuY", "cdate": 1595273795371, "mdate": null, "content": {"title": "ASYNCHRONOUS STOCHASTIC OPTIMIZATION FOR SEQUENCE TRAINING OF DEEP NEURAL NETWORKS", "abstract": "This paper explores asynchronous stochastic optimization for se- quence training of deep neural networks. Sequence training requires more computation than frame-level training using pre-computed frame data. This leads to several complications for stochastic op- timization, arising from significant asynchrony in model updates under massive parallelization, and limited data shuffling due to utterance-chunked processing. We analyze the impact of these two issues on the efficiency and performance of sequence training. In particular, we suggest a framework to formalize the reasoning about the asynchrony and present experimental results on both small and large scale Voice Search tasks to validate the effectiveness and efficiency of asynchronous stochastic optimization."}}
{"id": "FM5tUeQTP5A", "cdate": 1595273681365, "mdate": null, "content": {"title": "LARGE SCALE DEEP NEURAL NETWORK ACOUSTIC MODELING WITH SEMI-SUPERVISED TRAINING DATA FOR YOUTUBE VIDEO TRANSCRIPTION", "abstract": "YouTube is a highly visited video sharing website where over one billion people watch six billion hours of video every month. Im- proving accessibility to these videos for the hearing impaired and for search and indexing purposes is an excellent application of automatic speech recognition. However, YouTube videos are extremely chal- lenging for automatic speech recognition systems. Standard adapted Gaussian Mixture Model (GMM) based acoustic models can have word error rates above 50%, making this one of the most difficult reported tasks. Since 2009, YouTube has provided automatic gener- ation of closed captions for videos detected to have English speech; the service now supports ten different languages. This paper de- scribes recent improvements to the original system, in particular the use of owner-uploaded video transcripts to generate additional semi- supervised training data and deep neural networks acoustic models with large state inventories. Applying an \u201cisland of confidence\u201d fil- tering heuristic to select useful training segments, and increasing the model size by using 44,526 context dependent states with a low- rank final layer weight matrix approximation, improved performance by about 13% relative compared to previously reported sequence trained DNN results for this task."}}
{"id": "ao_iex_U_OW", "cdate": 1595273551692, "mdate": null, "content": {"title": "A GAUSSIAN MIXTURE MODEL LAYER JOINTLY OPTIMIZED WITH DISCRIMINATIVE FEATURES WITHIN A DEEP NEURAL NETWORK ARCHITECTURE", "abstract": "This article proposes and evaluates a Gaussian Mixture Model (GMM) represented as the last layer of a Deep Neural Network (DNN) architecture and jointly optimized with all previous layers using Asynchronous Stochastic Gradient Descent (ASGD). The re- sulting \u201cDeep GMM\u201d architecture was investigated with special at- tention to the following issues: (1) The extent to which joint op- timization improves over separate optimization of the DNN-based feature extraction layers and the GMM layer; (2) The extent to which depth (measured in number of layers, for a matched total number of parameters) helps a deep generative model based on the GMM layer, compared to a vanilla DNN model; (3) Head-to-head per- formance of Deep GMM architectures vs. equivalent DNN archi- tectures of comparable depth, using the same optimization criterion (frame-level Cross Entropy (CE)) and optimization method (ASGD); (4) Expanded possibilities for modeling offered by the Deep GMM generative model. The proposed Deep GMMs were found to yield Word Error Rates (WERs) competitive with state-of-the-art DNN systems, at the cost of pre-training using standard DNNs to initial- ize the Deep GMM feature extraction layers. An extension to Deep Subspace GMMs is described, resulting in additional gains."}}
{"id": "13gOu2r6UhQ", "cdate": 1595273451321, "mdate": null, "content": {"title": "TRANSFORMER TRANSDUCER: A STREAMABLE SPEECH RECOGNITION MODEL WITH TRANSFORMER ENCODERS AND RNN-T LOSS", "abstract": "In this paper we present an end-to-end speech recognition model with Transformer encoders that can be used in a streaming speech recognition system. Transformer computation blocks based on self- attention are used to encode both audio and label sequences indepen- dently. The activations from both audio and label encoders are com- bined with a feed-forward layer to compute a probability distribution over the label space for every combination of acoustic frame position and label history. This is similar to the Recurrent Neural Network Transducer (RNN-T) model, which uses RNNs for information en- coding instead of Transformer encoders. The model is trained with the RNN-T loss well-suited to streaming decoding. We present re- sults on the LibriSpeech dataset showing that limiting the left context for self-attention in the Transformer layers makes decoding compu- tationally tractable for streaming, with only a slight degradation in accuracy. We also show that the full attention version of our model beats the-state-of-the art accuracy on the LibriSpeech benchmarks. Our results also show that we can bridge the gap between full at- tention and limited attention versions of our model by attending to a limited number of future frames."}}
{"id": "aWKX3wmhHnL", "cdate": 1595273348327, "mdate": null, "content": {"title": "A DENSITY RATIO APPROACH TO LANGUAGE MODEL FUSION IN END-TO-END AUTOMATIC SPEECH RECOGNITION", "abstract": "This article describes a density ratio approach to integrating\nexternal Language Models (LMs) into end-to-end models\nfor Automatic Speech Recognition (ASR). Applied to a Recurrent Neural Network Transducer (RNN-T) ASR model\ntrained on a given domain, a matched in-domain RNN-LM,\nand a target domain RNN-LM, the proposed method uses\nBayes\u2019 Rule to define RNN-T posteriors for the target domain, in a manner directly analogous to the classic hybrid\nmodel for ASR based on Deep Neural Networks (DNNs) or\nLSTMs in the Hidden Markov Model (HMM) framework\n(Bourlard & Morgan, 1994). The proposed approach is evaluated in cross-domain and limited-data scenarios, for which\na significant amount of target domain text data is used for\nLM training, but only limited (or no) {audio, transcript}\ntraining data pairs are used to train the RNN-T. Specifically,\nan RNN-T model trained on paired audio & transcript data\nfrom YouTube is evaluated for its ability to generalize to\nVoice Search data. The Density Ratio method was found to\nconsistently outperform the dominant approach to LM and\nend-to-end ASR integration, Shallow Fusion."}}
{"id": "HJ4uwhedWS", "cdate": 1136073600000, "mdate": null, "content": {"title": "Training Conditional Random Fields with Multivariate Evaluation Measures", "abstract": "Jun Suzuki, Erik McDermott, Hideki Isozaki. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics. 2006."}}
