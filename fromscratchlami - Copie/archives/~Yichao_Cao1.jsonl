{"id": "IlYS1pLa9y", "cdate": 1652737443535, "mdate": null, "content": {"title": "Searching for Better Spatio-temporal Alignment in Few-Shot Action Recognition", "abstract": "Spatio-Temporal feature matching and alignment are essential for few-shot action recognition as they determine the coherence and effectiveness of the temporal patterns. Nevertheless, this process could be not reliable, especially when dealing with complex video scenarios. In this paper, we propose to improve the performance of matching and alignment from the end-to-end design of models. Our solution comes at two-folds. First, we encourage to enhance the extracted Spatio-Temporal representations from few-shot videos in the perspective of architectures. With this aim, we propose a specialized transformer search method for videos, thus the spatial and temporal attention can be well-organized and optimized for stronger feature representations. Second, we also design an efficient non-parametric spatio-temporal prototype alignment strategy to better handle the high variability of motion. In particular, a query-specific class prototype will be generated for each query sample and category, which can better match query sequences against all support sequences. By doing so, our method SST enjoys significant superiority over the benchmark UCF101 and HMDB51 datasets. For example, with no pretraining, our method achieves 17.1\\% Top-1 accuracy improvement than the baseline TRX on UCF101 5-way 1-shot setting but with only 3x fewer FLOPs."}}
{"id": "p3NL2VLYoZ", "cdate": 1640995200000, "mdate": 1667402314196, "content": {"title": "ED-DRAP: Encoder-Decoder Deep Residual Attention Prediction Network for Radar Echoes", "abstract": ""}}
{"id": "ia1l6SwU8qc", "cdate": 1640995200000, "mdate": 1667402314488, "content": {"title": "Combining the Convolution and Transformer for Classification of Smoke-Like Scenes in Remote Sensing Images", "abstract": ""}}
{"id": "EsLy8mSQ9X", "cdate": 1640995200000, "mdate": 1667402314488, "content": {"title": "STCNet: spatiotemporal cross network for industrial smoke detection", "abstract": "Industrial smoke emissions present a serious threat to natural ecosystems and human health. Prior works have shown that using computer vision techniques to identify smoke is a low-cost and convenient method. However, translucent smoke detection is a challenging task because of the irregular contours and complex motion state. To overcome these problems, we propose a novel spatiotemporal cross network (STCNet) to recognize industrial smoke emissions. The proposed STCNet involves a spatial pathway to extract appearance features and a temporal pathway to capture smoke motion information. Our STCNet is more targeted and goal oriented for dealing with translucent, nonrigid smoke objects. The spatial path can easily recognize obvious nonsmoking objects such as trees and buildings, and the temporal path can highlight the obscure traces of motion smoke. Our STCNet achieves the mutual guidance of multilevel spatiotemporal information by bidirectional feature fusion on multilevel feature maps. Extensive experiments on public datasets show that our STCNet achieves clear improvements against the best competitors by 6.2%. We also perform in-depth ablation studies on STCNet to explore the impacts of different feature fusion methods for the entire model. The code will be available at https://github.com/Caoyichao/STCNet ."}}
{"id": "BnCG3rTjyiq", "cdate": 1640995200000, "mdate": 1667402314611, "content": {"title": "PSL-SLAM: a monocular SLAM system using points and structure lines in Manhattan World", "abstract": "The performance of feature matching algorithms is well known to be one of the main Achilles heels of visual SLAM algorithms, and particularly for point-based visual SLAM. Which is prone to fail in low-textured ,enarios like man-made environments where points are insufficient. Yet, many environments in which, despite being low textured, can still reliably estimate line-based geometric primitives. The line-based structural features in the Manhattan world encode useful geometric information of parallelism, orthogonality, and coplanarity in the scene. By fully exploiting these structural features, we propose a novel monocular SLAM system merging feature points and structure lines, which can provide a more accurate estimation of camera poses. To integrate the structure lines into the framework of the system, we have made efforts in the detection, parameterization, feature fusion, and optimization modules of the structure lines. First, we used the consistency of the direction of the structure lines and the vanishing points to extract the structure lines. In the optimization module, we incorporated the error model of the structure lines into the nonlinear optimization framework, and proposed a new optimization strategy. Finally, a complete SLAM system based on points and structure lines is designed. With structure lines as a new observation, the robustness of the matching algorithm between consecutive frames in low-texture scenes is increased, ensuring continuous updating of the tracking thread when the feature points are lost. Secondly, the dominant directions of structure lines can provide global effective constraints to reduce the accumulated orientation errors and the position drift in consequence. Experiments in man-made environments have demonstrated that the proposed system outperforms existing state-of-the-art monocular SLAM systems in terms of accuracy and robustness."}}
{"id": "BFnr93RRNGT", "cdate": 1640995200000, "mdate": 1667402314489, "content": {"title": "CNN-Transformer Hybrid Architecture for Early Fire Detection", "abstract": "Fire hazards bring great harm to human beings and nature. Advances in computer vision technology have made it possible to detect fire early through surveillance videos. In recent works, CNN is widely used in fire detection but it cannot model long-range dependencies and its capability of global feature processing is poor. When dealing with the problem of early fire detection, the flame target is small and the color characteristics are not obvious, so the effect of the previous fire detection methods is poor. Transformer\u2019s strong capacity of feature processing and growing success in visual field highlight its potential and provide us with new ideas, but its large calculation has a certain impact on the detection speed. Therefore, in this paper, we design a network combining CNN and Transformer, (GLCT), which can model global and local information and achieve a balance between accuracy and speed. In the backbone MobileLP, linear highlighted attention mechanism is used to reduce the amount of computation, and locality is introduced in the feed forward network. Feature fusion is carried out by combining the designed backbone with BiFPN. Equipped with YOLO Head, the whole fire detection model is constructed. By detecting the surveillance video images of early fire, our network outperforms some representative and excellent object detection works, including YOLOv4, MobileViT, PVTv2 and its variants, showing its reliability in early fire detection."}}
{"id": "7muEoO-2v2N", "cdate": 1640995200000, "mdate": 1667402314193, "content": {"title": "QuasiVSD: efficient dual-frame smoke detection", "abstract": "Smoke is a typical symptom of early fire, and the appearance of a large amount of abnormal smoke usually indicates an impending abnormal accident. A smart smoke detection method can substantially reduce damage caused by fires in cities, factories and forests, it is also an important component of intelligent surveillance system. However, existing image-based detection methods often suffer from the lack of dynamic information, and video-based methods are usually computing-expensive because more input images need to be processed. In this work, we propose a novel and efficient Quasi Video Smoke Detector (QuasiVSD) to bridge the gap between image-based and video-based smoke detection. By regarding an unannotated image as reference, QuasiVSD can obtain motion-aware attention from just two frames. Moreover, Weakly Guided Attention Module is designed to further refine the feature representation for smoke regions. Finally, extensive experiments on real-world dataset show that our QuasiVSD achieves clear improvements against the image-based best competitors (CenterNet) by 4.71 with almost same parameters and FLOPs. And the computational complexity of QuasiVSD is just a fraction of that of general video understanding framework. Code will be available at: https://github.com/Caoyichao/VSDT."}}
{"id": "2g3hwLacXVf", "cdate": 1640995200000, "mdate": 1667402314709, "content": {"title": "Contrastive Embedding Distribution Refinement and Entropy-Aware Attention for 3D Point Cloud Classification", "abstract": "Learning a powerful representation from point clouds is a fundamental and challenging problem in the field of computer vision. Different from images where RGB pixels are stored in the regular grid, for point clouds, the underlying semantic and structural information of point clouds is the spatial layout of the points. Moreover, the properties of challenging in-context and background noise pose more challenges to point cloud analysis. One assumption is that the poor performance of the classification model can be attributed to the indistinguishable embedding feature that impedes the search for the optimal classifier. This work offers a new strategy for learning powerful representations via a contrastive learning approach that can be embedded into any point cloud classification network. First, we propose a supervised contrastive classification method to implement embedding feature distribution refinement by improving the intra-class compactness and inter-class separability. Second, to solve the confusion problem caused by small inter-class compactness and inter-class separability. Second, to solve the confusion problem caused by small inter-class variations between some similar-looking categories, we propose a confusion-prone class mining strategy to alleviate the confusion effect. Finally, considering that outliers of the sample clusters in the embedding space may cause performance degradation, we design an entropy-aware attention module with information entropy theory to identify the outlier cases and the unstable samples by measuring the uncertainty of predicted probability. The results of extensive experiments demonstrate that our method outperforms the state-of-the-art approaches by achieving 82.9% accuracy on the real-world ScanObjectNN dataset and substantial performance gains up to 2.9% in DCGNN, 3.1% in PointNet++, and 2.4% in GBNet."}}
{"id": "-Yl7H3_ZSDG", "cdate": 1640995200000, "mdate": 1667402314191, "content": {"title": "EFFNet: Enhanced Feature Foreground Network for Video Smoke Source Prediction and Detection", "abstract": ""}}
{"id": "avO5FcKfkU7", "cdate": 1609459200000, "mdate": 1667402314612, "content": {"title": "Online human action recognition with spatial and temporal skeleton features using a distributed camera network", "abstract": ""}}
