{"id": "raSbs1AFoX3", "cdate": 1663850532081, "mdate": null, "content": {"title": "On the convergence of SGD under the over-parameter setting", "abstract": "With the improvement of computing power, over-parameterized models get increasingly popular in machine learning. This type of model is usually with a complicated, non-smooth, and non-convex loss function landscape. However, when we train the model, simply using the first-order optimization algorithm like stochastic gradient descent (SGD) could acquire some good results, in both training and testing, albeit that SGD is known to not guarantee convergence for non-smooth and non-convex cases. Theoretically, it was previously proved that in training, SGD converges to the global optimum with probability $1 - \\epsilon$, but only for certain models and $\\epsilon$ depends on the model complexity. It was also observed that SGD tends to choose a flat minimum, which preserves its training performance in testing. In this paper, we first prove that SGD could iterate to the global optimum almost surely under arbitrary initial value and some mild assumptions on the loss function. Then, we prove that if the learning rate is larger than a value depending on the structure of a global minimum, the probability of converging to this global optimum is zero. Finally, we acquire the asymptotic convergence rate based on the local structure of the global optimum. "}}
{"id": "u5oLvX8x4wH", "cdate": 1652737685457, "mdate": null, "content": {"title": "Revisit last-iterate convergence of mSGD under milder requirement on step size", "abstract": " Understanding convergence of SGD-based optimization algorithms can help deal with enormous machine learning problems. To ensure last-iterate convergence of   SGD and momentum-based SGD (mSGD),\n  the existing studies  usually constrain the step size $\\epsilon_{n}$ to decay as $\\sum_{n=1}^{+\\infty}\\epsilon_{n}^{2}<+\\infty$, which however is rather conservative and may lead to slow convergence in the early stage of the iteration. In this paper, we relax this requirement by studying an alternate step size for the mSGD. First, we relax the requirement of the decay on step size to $\\sum_{n=1}^{+\\infty}\\epsilon_{n}^{2+\\eta_{0}}<+\\infty\\ (0\\le\\eta_{0}<1/2)$. This implies that a larger step size, such as $\\epsilon_{n}=\\frac{1}{\\sqrt{n}}$  can  be utilized for accelerating the mSGD in the early stage.  Under this new step size and some common conditions, we prove that the  gradient norm of mSGD for non-convex loss functions   asymptotically decays to zero. In addition, we show that this step size can indeed help make the  convergence into a neighborhood of the stationary points quicker in the early stage. In addition, we establish the convergence of   mSGD  under a constant step size $\\epsilon_n\\equiv\\epsilon>0$ by removing the common requirement in the literature on the strong convexity of the loss function. \n  Some experiments are given to illustrate the developed results."}}
{"id": "g5tANwND04i", "cdate": 1632875708608, "mdate": null, "content": {"title": "On the Convergence of mSGD and AdaGrad for Stochastic Optimization", "abstract": "As one of the most fundamental stochastic optimization algorithms, stochastic gradient descent (SGD) has been intensively developed and extensively applied in machine learning in the past decade. There have been some modified SGD-type algorithms, which outperform the SGD in many competitions and applications in terms of convergence rate and accuracy, such as momentum-based SGD (mSGD) and adaptive gradient algorithm (AdaGrad). Despite these empirical successes, the theoretical properties of these algorithms have not been well established due to technical difficulties. With this motivation, we focus on convergence analysis of mSGD and AdaGrad for any smooth (possibly non-convex) loss functions in stochastic optimization. First, we prove that the iterates of mSGD are asymptotically convergent to a connected set of stationary points with probability one, which is more general than existing works on subsequence convergence or convergence of time averages. Moreover, we prove that the loss function of mSGD decays at a certain rate faster than that of SGD. In addition, we prove the iterates of AdaGrad are asymptotically convergent to a connected set of stationary points with probability one. Also, this result extends the results from the literature on subsequence convergence and the convergence of time averages. Despite the generality of the above convergence results, we have relaxed some assumptions of gradient noises, convexity of loss functions, as well as boundedness of iterates."}}
