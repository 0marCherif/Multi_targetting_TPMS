{"id": "hfUJ4ShyDEU", "cdate": 1663849813169, "mdate": null, "content": {"title": "Achieve the Minimum Width of Neural Networks for Universal Approximation", "abstract": "The universal approximation property (UAP) of neural networks is fundamental for deep learning, and it is well known that wide neural networks are universal approximators of continuous functions within both the $L^p$ norm and the continuous/uniform norm. However, the exact minimum width, $w_{\\min}$, for the UAP has not been studied thoroughly. Recently, using a decoder-memorizer-encoder scheme, \\citet{Park2021Minimum} found that $w_{\\min} = \\max(d_x+1,d_y)$ for both the $L^p$-UAP of ReLU networks and the $C$-UAP of ReLU+STEP networks, where $d_x,d_y$ are the input and output dimensions, respectively. In this paper, we consider neural networks with an arbitrary set of activation functions. We prove that both $C$-UAP and $L^p$-UAP for functions on compact domains share a universal lower bound of the minimal width; that is, $w^*_{\\min} = \\max(d_x,d_y)$. In particular, the critical width, $w^*_{\\min}$, for $L^p$-UAP can be achieved by leaky-ReLU networks, provided that the input or output dimension is larger than one. Our construction is based on the approximation power of neural ordinary differential equations and the ability to approximate flow maps by neural networks. The nonmonotone or discontinuous activation functions case and the one-dimensional case are also discussed."}}
{"id": "HkW8kn-uZB", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent", "abstract": "Despite its empirical success and recent theoretical progress, there generally lacks a quantitative analysis of the effect of batch normalization (BN) on the convergence and stability of gradient d..."}}
{"id": "SJg7IsC5KQ", "cdate": 1538087755297, "mdate": null, "content": {"title": "On the Convergence and Robustness of Batch Normalization", "abstract": "Despite its empirical success, the theoretical underpinnings of the stability, convergence and acceleration properties of batch normalization (BN) remain elusive. In this paper, we attack this problem from a modelling approach, where we perform thorough theoretical analysis on BN applied to simplified model: ordinary least squares (OLS). We discover that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, asymptotic acceleration effects, as well as insensitivity to choice of learning rates. We then demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems. This points to a new direction towards uncovering the mathematical principles that underlies batch normalization."}}
