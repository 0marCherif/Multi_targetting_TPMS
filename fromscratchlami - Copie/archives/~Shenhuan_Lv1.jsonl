{"id": "hr7dTVtP1P", "cdate": 1672531200000, "mdate": 1706755600178, "content": {"title": "On the Consistency Rate of Decision Tree Learning Algorithms", "abstract": "Decision tree learning algorithms such as CART are generally based on heuristics that maximizes the purity gain greedily. Though these algorithms are practically successful, theoretical properties ..."}}
{"id": "T6_YJ0uIeew", "cdate": 1672531200000, "mdate": 1683884712541, "content": {"title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance", "abstract": "Deep forest is a non-differentiable deep model which has achieved impressive empirical success across a wide variety of applications, especially on categorical/symbolic or mixed modeling tasks. Many of the application fields prefer explainable models, such as random forests with feature contributions that can provide local explanation for each prediction, and Mean Decrease Impurity (MDI) that can provide global feature importance. However, deep forest, as a cascade of random forests, possesses interpretability only at the first layer. From the second layer on, many of the tree splits occur on the new features generated by the previous layer, which makes existing explanatory tools for random forests inapplicable. To disclose the impact of the original features in the deep layers, we design a calculation method with an estimation step followed by a calibration step for each layer, and propose our feature contribution and MDI feature importance calculation tools for deep forest. Experimental results on both simulated data and real world data verify the effectiveness of our methods."}}
{"id": "sL7XH6-V21e", "cdate": 1652737457841, "mdate": null, "content": {"title": "Depth is More Powerful than Width with Prediction Concatenation in Deep Forest", "abstract": "Random Forest (RF) is an ensemble learning algorithm proposed by \\citet{breiman2001random} that constructs a large number of randomized decision trees individually and aggregates their predictions by naive averaging. \\citet{zhou2019deep} further propose Deep Forest (DF) algorithm with multi-layer feature transformation, which significantly outperforms random forest in various application fields. The prediction concatenation (PreConc) operation is crucial for the multi-layer feature transformation in deep forest, though little has been known about its theoretical property. In this paper, we analyze the influence of Preconc on the consistency of deep forest. Especially when the individual tree is inconsistent (as in practice, the individual tree is often set to be fully grown, i.e., there is only one sample at each leaf node), we find that the convergence rate of two-layer DF \\textit{w.r.t.} the number of trees $M$ can reach $\\mathcal{O}(1/M^2)$ under some mild conditions, while the convergence rate of RF is $\\mathcal{O}(1/M)$. Therefore, with the help of PreConc, DF with deeper layer will be more powerful than the shallower layer. Experiments confirm theoretical advantages."}}
{"id": "X1cX0EmGDSp", "cdate": 1640995200000, "mdate": 1671894742900, "content": {"title": "Improving generalization of deep neural networks by leveraging margin distribution", "abstract": ""}}
{"id": "6yCtCNGj4m", "cdate": 1640995200000, "mdate": 1683901478897, "content": {"title": "Depth is More Powerful than Width with Prediction Concatenation in Deep Forest", "abstract": "Random Forest (RF) is an ensemble learning algorithm proposed by \\citet{breiman2001random} that constructs a large number of randomized decision trees individually and aggregates their predictions by naive averaging. \\citet{zhou2019deep} further propose Deep Forest (DF) algorithm with multi-layer feature transformation, which significantly outperforms random forest in various application fields. The prediction concatenation (PreConc) operation is crucial for the multi-layer feature transformation in deep forest, though little has been known about its theoretical property. In this paper, we analyze the influence of Preconc on the consistency of deep forest. Especially when the individual tree is inconsistent (as in practice, the individual tree is often set to be fully grown, i.e., there is only one sample at each leaf node), we find that the convergence rate of two-layer DF \\textit{w.r.t.} the number of trees $M$ can reach $\\mathcal{O}(1/M^2)$ under some mild conditions, while the convergence rate of RF is $\\mathcal{O}(1/M)$. Therefore, with the help of PreConc, DF with deeper layer will be more powerful than the shallower layer. Experiments confirm theoretical advantages."}}
{"id": "MdDz-3oM4a", "cdate": 1609459200000, "mdate": 1671894742811, "content": {"title": "Improving Deep Forest by Exploiting High-order Interactions", "abstract": "Recent studies on deep forests have shown that deep learning frameworks can be built on non-differentiable modules without a backpropagation training process. However, the feature representations of deep forests only consist of predicted class probabilities. The information these class probabilities deliver is very limited and lacks diversity, especially when the number of output labels is far less than the number of input features. Besides, the prediction-based representations require us to save multiple layers of random forests to use them during testing, which is high-memory and high-time cost. In this paper, we propose a novel deep forest model that utilizes high-order interactions of input features to generate more informative and diverse feature representations. Specifically, we design a generalized version of Random Intersection Trees (gRIT) to discover stable high-order interactions and apply Activated Linear Combination (ALC) to transform them into hierarchical distributed representations. These interaction-based representations obviate the need to store random forests in the front layers, thus greatly improving the computational efficiency. Our experiments show that our method achieves highly competitive predictive performance with significantly reduced time and memory cost."}}
{"id": "B1zb94Be8B", "cdate": 1567802505390, "mdate": null, "content": {"title": "A Refined Margin Distribution Analysis for Forest Representation Learning", "abstract": "In this paper, we formulate the forest representation learning approach called \\textsc{CasDF} as an additive model which boosts the augmented feature instead of the prediction. We substantially improve the upper bound of generalization gap from $\\mathcal{O}(\\sqrt{\\ln m/m})$ to $\\mathcal{O}(\\ln m/m)$, while the margin ratio of the margin standard deviation to the margin mean is sufficiently small. This tighter upper bound inspires us to optimize the ratio. Therefore, we design a margin distribution reweighting approach for deep forest to achieve a small margin ratio by boosting the augmented feature. Experiments confirm the consistency between the margin distribution and generalization performance. We remark that this study offers a novel understanding of \\textsc{CasDF} from the perspective of the margin theory and further guides the layer-by-layer forest representation learning."}}
{"id": "HygcvsAcFX", "cdate": 1538087778084, "mdate": null, "content": {"title": "Optimal margin Distribution Network", "abstract": "Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN\nmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data."}}
