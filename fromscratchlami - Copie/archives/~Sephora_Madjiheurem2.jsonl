{"id": "wEb6VYsaYx", "cdate": 1676591079619, "mdate": null, "content": {"title": "Beyond Temporal Credit Assignment in Reinforcement Learning", "abstract": "In reinforcement learning, traditional value-based methods rely heavily on time as the main proxy for propagating information across the state space. This often results in slow learning and does not scale to large and complex environments. Here, we propose to leverage prior information about the structure of the the environment to assign credit non-temporally to improve learning efficiency. Specifically, we introduce the concept of structural neighbours, which are sets of states with similar semantic structures and which have equivalent values under the optimal policy. We augment traditional value-based RL methods (TD(0), Dyna and Dueling DQN) with a learning mechanism based on structural neighbours. Our empirical results show that by incorporating structural updates, learning efficiency can be greatly improved on a variety of environments ranging from simple tabular grid worlds to those which require function approximation, including the complex and high-dimensional game of Solitaire."}}
{"id": "VbLGbcdz16-", "cdate": 1623604622180, "mdate": null, "content": {"title": "Disentangled Predictive Representation for Meta-Reinforcement Learning", "abstract": "A major challenge in reinforcement learning is the design of agents that are able to generalize across tasks that share common  dynamics. A viable solution is  meta-reinforcement learning,   which identifies common structures among past tasks to be then generalized to new tasks (meta-test). Prior works learn meta-representation jointly while solving tasks, resulting in representations that not generalize  well across policies, leading to sampling-inefficiency during   meta-test phases. In this work, we introduce state2vec, an efficient and low-complexity unsupervised framework for learning disentangled representations that are more general.  \nThe state embedding vectors learned with state2vec capture the geometry of the underlying state space, resulting in high-quality basis functions for linear value function approximation. "}}
{"id": "HklOjkHKDr", "cdate": 1569439647894, "mdate": null, "content": {"title": "State2vec: Off-Policy Successor Feature Approximators", "abstract": "A major challenge in reinforcement learning (RL) is how to design agents that are able to generalize across tasks that share common  dynamics. A viable solution is  meta-reinforcement learning,   which identifies common structures among past tasks to be then generalized to new tasks (meta-test). In meta-training, the RL agent learns state representations that encode  prior information from a set of  tasks, used to generalize the value function approximation. This has been proposed in the literature as   successor representation approximators. While promising, these methods do not generalize  well across optimal policies,  leading to sampling-inefficiency during   meta-test phases. In this paper, we propose state2vec, an efficient and low-complexity framework for learning  successor features which (i) generalize across policies, (ii) ensure sample-efficiency during meta-test. Representing each RL tasks as a graph, we extend the well known nod2vec framework to learn graph embeddings able to capture the discounted future state transitions in RL. The proposed off-policy state2vec captures the geometry of the underlying state space, making good basis functions for linear value function approximation. "}}
