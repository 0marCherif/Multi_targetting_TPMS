{"id": "21FNoED07B", "cdate": 1696382905153, "mdate": 1696382905153, "content": {"title": "Efficient Controllable Multi-Task Architectures", "abstract": "We aim to train a multi-task model such that users can\nadjust the desired compute budget and relative importance\nof task performances after deployment, without retraining.\nThis enables optimizing performance for dynamically varying user needs, without heavy computational overhead to\ntrain and save models for various scenarios. To this end,\nwe propose a multi-task model consisting of a shared encoder and task-specific decoders where both encoder and\ndecoder channel widths are slimmable. Our key idea is to\ncontrol the task importance by varying the capacities of\ntask-specific decoders, while controlling the total computational cost by jointly adjusting the encoder capacity. This\nimproves overall accuracy by allowing a stronger encoder\nfor a given budget, increases control over computational\ncost, and delivers high-quality slimmed sub-architectures\nbased on user\u2019s constraints. Our training strategy involves a\nnovel \u2018Configuration-Invariant Knowledge Distillation\u2019 loss\nthat enforces backbone representations to be invariant under\ndifferent runtime width configurations to enhance accuracy.\nFurther, we present a simple but effective search algorithm\nthat translates user constraints to runtime width configurations of both the shared encoder and task decoders, for\nsampling the sub-architectures. The key rule for the search\nalgorithm is to provide a larger computational budget to\nthe higher preferred task decoder, while searching a shared\nencoder configuration that enhances the overall MTL performance. Various experiments on three multi-task benchmarks\n(PASCALContext, NYUDv2, and CIFAR100-MTL) with diverse backbone architectures demonstrate the advantage of\nour approach. For example, our method shows a higher\ncontrollability by \u223c 33.5% in the NYUD-v2 dataset over\nprior methods, while incurring much less compute cost."}}
{"id": "DRckHIGk8qw", "cdate": 1652737266096, "mdate": null, "content": {"title": "GAMA: Generative Adversarial Multi-Object Scene Attacks", "abstract": "The majority of methods for crafting adversarial attacks have focused on scenes with a single dominant object (e.g., images from ImageNet). On the other hand, natural scenes include multiple dominant objects that are semantically related. Thus, it is crucial to explore designing attack strategies that look beyond learning on single-object scenes or attack single-object victim classifiers. Due to their inherent property of strong transferability of perturbations to unknown models, this paper presents the first approach of using generative models for adversarial attacks on multi-object scenes. In order to represent the relationships between different objects in the input scene, we leverage upon the open-sourced pre-trained vision-language model CLIP (Contrastive Language-Image Pre-training), with the motivation to exploit the encoded semantics in the language space along with the visual space. We call this attack approach Generative Adversarial Multi-object Attacks (GAMA). GAMA demonstrates the utility of the CLIP model as an attacker's tool to train formidable perturbation generators for multi-object scenes. Using the joint image-text features to train the generator, we show that GAMA can craft potent transferable perturbations in order to fool victim classifiers in various attack settings. For example, GAMA triggers ~16% more misclassification than state-of-the-art generative approaches in black-box settings where both the classifier architecture and data distribution of the attacker are different from the victim. Our code is available here: https://abhishekaich27.github.io/gama.html"}}
{"id": "K-nW-KVo9S", "cdate": 1640995200000, "mdate": 1667889583939, "content": {"title": "Poisson2Sparse: Self-supervised Poisson Denoising from a Single Image", "abstract": "Image enhancement approaches often assume that the noise is signal independent, and approximate the degradation model as zero-mean additive Gaussian. However, this assumption does not hold for biomedical imaging systems where sensor-based sources of noise are proportional to signal strengths, and the noise is better represented as a Poisson process. In this work, we explore a sparsity and dictionary learning-based approach and present a novel self-supervised learning method for single-image denoising where the noise is approximated as a Poisson process, requiring no clean ground-truth data. Specifically, we approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network that enforces sparsity with respect to the weights of the network. Since the sparse representations are based on the underlying image, it is able to suppress the spurious components (noise) in the image patches, thereby introducing implicit regularization for denoising tasks through the network structure. Experiments on two bio-imaging datasets demonstrate that our method outperforms the state-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results demonstrate that, in addition to higher performance on standard quantitative metrics, we are able to recover much more subtle details than other compared approaches. Our code is made publicly available at https://github.com/tacalvin/Poisson2Sparse ."}}
{"id": "Bx0hSgZyXzP", "cdate": 1640995200000, "mdate": 1667889583969, "content": {"title": "GAMA: Generative Adversarial Multi-Object Scene Attacks", "abstract": "The majority of methods for crafting adversarial attacks have focused on scenes with a single dominant object (e.g., images from ImageNet). On the other hand, natural scenes include multiple dominant objects that are semantically related. Thus, it is crucial to explore designing attack strategies that look beyond learning on single-object scenes or attack single-object victim classifiers. Due to their inherent property of strong transferability of perturbations to unknown models, this paper presents the first approach of using generative models for adversarial attacks on multi-object scenes. In order to represent the relationships between different objects in the input scene, we leverage upon the open-sourced pre-trained vision-language model CLIP (Contrastive Language-Image Pre-training), with the motivation to exploit the encoded semantics in the language space along with the visual space. We call this attack approach Generative Adversarial Multi-object scene Attacks (GAMA). GAMA demonstrates the utility of the CLIP model as an attacker's tool to train formidable perturbation generators for multi-object scenes. Using the joint image-text features to train the generator, we show that GAMA can craft potent transferable perturbations in order to fool victim classifiers in various attack settings. For example, GAMA triggers ~16% more misclassification than state-of-the-art generative approaches in black-box settings where both the classifier architecture and data distribution of the attacker are different from the victim. Our code is available here: https://abhishekaich27.github.io/gama.html"}}
{"id": "-7EhrbfbK31", "cdate": 1621629920431, "mdate": null, "content": {"title": "Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations", "abstract": "When compared to the image classification models, black-box adversarial attacks against video classification models have been largely understudied. This could be possible because, with video, the temporal dimension poses significant additional challenges in gradient estimation. Query-efficient black-box attacks rely on effectively estimated gradients towards maximizing the probability of misclassifying the target video. In this work, we demonstrate that such effective gradients can be searched for by parameterizing the temporal structure of the search space with geometric transformations. Specifically, we design a novel iterative algorithm GEOmetric TRAnsformed Perturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP employs standard geometric transformation operations to reduce the search space for effective gradients into searching for a small group of parameters that define these operations. This group of parameters describes the geometric progression of gradients, resulting in a reduced and structured search space. Our algorithm inherently leads to successful perturbations with surprisingly few queries. For example, adversarial examples generated from GEO-TRAP have better attack success rates with ~73.55% fewer queries compared to the state-of-the-art method for video adversarial attacks on the widely used Jester dataset. Overall, our algorithm exposes vulnerabilities of diverse video classification models and achieves new state-of-the-art results under black-box settings on two large datasets."}}
{"id": "kAuupyMQTi", "cdate": 1609459200000, "mdate": 1667889583954, "content": {"title": "Elastic Weight Consolidation (EWC): Nuts and Bolts", "abstract": "In this report, we present a theoretical support of the continual learning method \\textbf{Elastic Weight Consolidation}, introduced in paper titled `Overcoming catastrophic forgetting in neural networks'. Being one of the most cited paper in regularized methods for continual learning, this report disentangles the underlying concept of the proposed objective function. We assume that the reader is aware of the basic terminologies of continual learning."}}
{"id": "jRDByXf2l", "cdate": 1609459200000, "mdate": 1667889584041, "content": {"title": "Spatio-Temporal Representation Factorization for Video-based Person Re-Identification", "abstract": "Despite much recent progress in video-based person re-identification (re-ID), the current state-of-the-art still suffers from common real-world challenges such as appearance similarity among various people, occlusions, and frame misalignment. To alleviate these problems, we propose Spatio-Temporal Representation Factorization (STRF), a flexible new computational unit that can be used in conjunction with most existing 3D convolutional neural network architectures for re-ID. The key innovations of STRF over prior work include explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture complementary person-specific appearance and motion information. Specifically, temporal factorization comprises two branches, one each for static features (e.g., the color of clothes) that do not change much over time, and dynamic features (e.g., walking patterns) that change over time. Further, spatial factorization also comprises two branches to learn both global (coarse segments) as well as local (finer segments) appearance features, with the local features particularly useful in cases of occlusion or spatial misalignment. These two factorization operations taken together result in a modular architecture for our parameter-wise light STRF unit that can be plugged in between any two 3D convolutional layers, resulting in an end-to-end learning framework. We empirically show that STRF improves performance of various existing baseline architectures while demonstrating new state-of-the-art results using standard person re-ID evaluation protocols on three benchmarks."}}
{"id": "Kj7Sdmj4Kf", "cdate": 1609459200000, "mdate": 1667889584040, "content": {"title": "Deep Quantized Representation for Enhanced Reconstruction", "abstract": "While machine learning approaches have shown remarkable performance in biomedical image analysis, most of these methods rely on high-quality and accurate imaging data. However, collecting such data requires intensive and careful manual effort. One of the major challenges in imaging the Shoot Apical Meristem (SAM) of Arabidopsis thaliana, is that the deeper slices in the z-stack suffer from different perpetual quality-related problems like poor contrast and blurring. These quality-related issues often lead to the disposal of the painstakingly collected data with little to no control on quality while collecting the data. Therefore, it becomes necessary to employ and design techniques that can enhance the images to make them more suitable for further analysis. In this paper, we propose a data-driven Deep Quantized Latent Representation (DQLR) methodology for high-quality image reconstruction in the Shoot Apical Meristem (SAM) of Arabidopsis thaliana. Our proposed framework utilizes multiple consecutive slices in the z-stack to learn a low dimensional latent space, quantize it and subsequently perform reconstruction using the quantized representation to obtain sharper images. Experiments on a publicly available dataset validate our methodology showing promising results."}}
{"id": "GPPgVYlHsd", "cdate": 1609459200000, "mdate": 1667889583952, "content": {"title": "Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations", "abstract": "When compared to the image classification models, black-box adversarial attacks against video classification models have been largely understudied. This could be possible because, with video, the temporal dimension poses significant additional challenges in gradient estimation. Query-efficient black-box attacks rely on effectively estimated gradients towards maximizing the probability of misclassifying the target video. In this work, we demonstrate that such effective gradients can be searched for by parameterizing the temporal structure of the search space with geometric transformations. Specifically, we design a novel iterative algorithm GEOmetric TRAnsformed Perturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP employs standard geometric transformation operations to reduce the search space for effective gradients into searching for a small group of parameters that define these operations. This group of parameters describes the geometric progression of gradients, resulting in a reduced and structured search space. Our algorithm inherently leads to successful perturbations with surprisingly few queries. For example, adversarial examples generated from GEO-TRAP have better attack success rates with ~73.55% fewer queries compared to the state-of-the-art method for video adversarial attacks on the widely used Jester dataset. Overall, our algorithm exposes vulnerabilities of diverse video classification models and achieves new state-of-the-art results under black-box settings on two large datasets."}}
{"id": "sdd-snfShnI", "cdate": 1577836800000, "mdate": 1667889583914, "content": {"title": "ALANET: Adaptive Latent Attention Network for Joint Video Deblurring and Interpolation", "abstract": ""}}
