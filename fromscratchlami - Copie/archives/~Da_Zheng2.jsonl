{"id": "meLzymorEz", "cdate": 1674892659688, "mdate": 1674892659688, "content": {"title": "Collective Multi-type Entity Alignment Between Knowledge Graphs", "abstract": "Knowledge graph (e.g. Freebase, YAGO) is a multi-relational graph\nrepresenting rich factual information among entities of various\ntypes. Entity alignment is the key step towards knowledge graph\nintegration from multiple sources. It aims to identify entities across\ndifferent knowledge graphs that refer to the same real world entity.\nHowever, current entity alignment systems overlook the sparsity\nof different knowledge graphs and can not align multi-type entities by one single model. In this paper, we present a Collective\nGraph neural network for Multi-type entity Alignment, called CGMuAlign. Different from previous work, CG-MuAlign jointly aligns\nmultiple types of entities, collectively leverages the neighborhood\ninformation and generalizes to unlabeled entity types. Specifically,\nwe propose novel collective aggregation function tailored for this\ntask, that (1) relieves the incompleteness of knowledge graphs via\nboth cross-graph and self attentions, (2) scales up efficiently with\nmini-batch training paradigm and effective neighborhood sampling\nstrategy. We conduct experiments on real world knowledge graphs\nwith millions of entities and observe the superior performance\nbeyond existing methods. In addition, the running time of our approach is much less than the current state-of-the-art deep learning\nmethods."}}
{"id": "DhICIwGint_", "cdate": 1664046170975, "mdate": null, "content": {"title": "From Local to Global: Spectral-Inspired Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) are powerful deep learning methods for Non-Euclidean data. Popular GNNs are message-passing algorithms (MPNNs) that aggregate and combine signals in a local graph neighborhood. However, shallow MPNNs tend to miss long-range signals and perform poorly on some heterophilous graphs, while deep MPNNs can suffer from issues like over-smoothing or over-squashing. To mitigate such issues, existing works typically borrow normalization techniques from training neural networks on Euclidean data or modify the graph structures. Yet these approaches are not well-understood theoretically and could increase the overall computational complexity. In this work, we draw inspirations from spectral graph embedding and propose \\texttt{PowerEmbed} --- a simple layer-wise normalization technique to boost MPNNs. We show \\texttt{PowerEmbed} can provably express the top-$k$ leading eigenvectors of the graph operator, which prevents over-smoothing and is agnostic to the graph topology; meanwhile, it produces a list of representations ranging from local features to global signals, which avoids over-squashing. We apply \\texttt{PowerEmbed} in a wide range of simulated and real graphs and demonstrate its competitive performance, particularly for heterophilous graphs."}}
{"id": "VFIH17qfUlH", "cdate": 1663873729905, "mdate": 1663873729905, "content": {"title": "TGL: A General Framework for Temporal GNN Training on Billion-Scale Graphs", "abstract": "Many real world graphs contain time domain information. Temporal Graph Neural Networks capture temporal information as well as structural and contextual information in the generated dynamic node embeddings. Researchers have shown that these embeddings achieve state-of-the-art performance in many different tasks. In this work, we propose TGL, a unified framework for large-scale offline Temporal Graph Neural Network training where users can compose various Temporal Graph Neural Networks with simple configuration files. TGL comprises five main components, a temporal sampler, a mailbox, a node memory module, a memory updater, and a message passing engine. We design a Temporal-CSR data structure and a parallel sampler to efficiently sample temporal neighbors to formtraining mini-batches. We propose a novel random chunk scheduling technique that mitigates the problem of obsolete node memory when training with a large batch size. To address the limitations of current TGNNs only being evaluated on small-scale datasets, we introduce two large-scale real-world datasets with 0.2 and 1.3 billion temporal edges. We evaluate the performance of TGL on four small-scale datasets with a single GPU and the two large datasets with multiple GPUs for both link prediction and node classification tasks. We compare TGL with the open-sourced code of five methods and show that TGL achieves similar or better accuracy with an average of 13x speedup. Our temporal parallel sampler achieves an average of 173x speedup on a multi-core CPU compared with the baselines. On a 4-GPU machine, TGL can train one epoch of more than one billion temporal edges within 1-10 hours. To the best of our knowledge, this is the first work that proposes a general framework for large-scale Temporal Graph Neural Networks training on multiple GPUs."}}
{"id": "5s2v_0F7MG", "cdate": 1663850063889, "mdate": null, "content": {"title": "OrthoReg: Improving Graph-regularized MLPs via Orthogonality Regularization", "abstract": "Graph Neural Networks (GNNs) are currently dominating in modeling graph-structure data, while their high reliance on graph structure for inference significantly impedes them from widespread applications. By contrast, graph-Regularized MLPs (GR-MLPs) implicitly inject the graph structure information into model weights, while their performance can hardly match that of GNNs in most tasks. This motivates us to study the causes of the limited performance of GR-MLPs. In this paper, we demonstrate that node embeddings learned from conventional GR-MLPs suffer from dimensional collapse, a phenomenon in which the largest a few eigenvalues dominate the embedding space, and thus the expressive power is constrained. We further propose ORTHO-REG, a novel GR-MLP model, to mitigate the dimensional collapse issue. Through a soft regularization loss on the correlation matrix of node embeddings, ORTHO-REG explicitly encourages orthogonal node representations and thus can naturally avoid dimensionally collapsed representations. Experiments on traditional transductive semi-supervised classification tasks and inductive node classification for cold-start scenarios demonstrate its effectiveness and superiority."}}
{"id": "2nm0fGwWBMr", "cdate": 1601308133444, "mdate": null, "content": {"title": "PanRep: Universal node embeddings for heterogeneous graphs", "abstract": "Learning unsupervised node embeddings facilitates several downstream tasks such as node classification and link prediction. A node embedding is universal if it is designed to be used by and benefit various downstream tasks. This work introduces PanRep, a graph neural network (GNN) model, for unsupervised learning of universal node representations for heterogenous graphs. PanRep consists of a GNN encoder that obtains node embeddings and four decoders, each capturing different topological and node feature properties. Abiding to these properties the novel unsupervised framework learns universal embeddings applicable to different downstream tasks. PanRep can be furthered fine-tuned to account for possible limited labels. In this operational setting PanRep is considered as a pretrained model for extracting node embeddings of heterogenous graph data. PanRep outperforms all unsupervised and certain supervised methods in node classification and link prediction, especially when the labeled data for the supervised methods is small. PanRep-FT (with fine-tuning) outperforms all other supervised approaches, which corroborates the merits of pretraining models. Finally, we apply PanRep-FT for discovering novel drugs for Covid-19. We showcase the advantage of universal embeddings in drug repurposing and identify several drugs used in clinical trials as possible drug candidates."}}
