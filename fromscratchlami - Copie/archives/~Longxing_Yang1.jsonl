{"id": "06rbH1zswj", "cdate": 1672531200000, "mdate": 1681654352449, "content": {"title": "PA&DA: Jointly Sampling PAth and DAta for Consistent NAS", "abstract": ""}}
{"id": "fFDM6aIZhN", "cdate": 1663850330821, "mdate": null, "content": {"title": "Weighted Regularization for Efficient Neural Network Compression", "abstract": "Regularization is widely applied to model complexity reduction and neural network compression. Existing $L_1$ and nuclear norm regularizations can achieve favorable results, but these methods treat all parameters equally and ignore the importance of the parameters. Taking the trained parameters as prior information to construct weights, a weighted regularization method is proposed in this paper. Theoretically, we establish the bounds on the estimation errors for values of the global minimum for a fully connected single hidden layer neural network. Further we prove the estimates generated from the weighted $L_1$ regularization and the weighted nuclear norm regularization can recover the sparsity and the low rank structure of a global minimum of the neural network with a high probability, respectively. The effectiveness of the algorithm is validated by conducting a numerical simulation and experiments with popular neural networks on public datasets from real-world applications."}}
{"id": "HHcl-5chhkt", "cdate": 1663850116321, "mdate": null, "content": {"title": "IT-NAS: Integrating Lite-Transformer into NAS for Architecture Seletion", "abstract": "Neural Architecture Search (NAS) aims to search for the best network in the pre-defined search space. However, much work focuses on the search strategy but little on the architecture selection process. Despite the fact that the weight-sharing based NAS has promoted the search efficiency, we notice that the architecture selection is quite unstable or circuitous. For instance, the differentiable NAS may derive the suboptimal architecture due to the performance collapse caused by bi-level optimization, or the One-shot NAS requires sampling and evaluating a large number of candidate structures. Recently, the self-attention mechanism achieves better performance in terms of the long-range modeling capabilities. Considering that different operations are widely distributed in the search space, we suggest leveraging the self-attention mechanism to extract the relationship among them and to determine which operation is superior to others. Therefore, we integrate Lite-Transformer into NAS for architecture selection. Specifically, we regard the feature map of each candidate operation as distinct patches and feed them into the Lite-Transformer module along with an additional Indicator Token (called IT). The cross attention among various operations can be extracted by the self-attention mechanism, and the importance of each candidate operation is then shown by the softmax result between the query of indicator token (IT) and other values of operational tokens. We experimentally demonstrate that our framework can select the truly representative architecture in different search spaces and achieves 2.39% test error on CIFAR-10 in DARTS search space, and 24.1% test error on ImageNet in the ProxylessNAS search space, as well as the stable and better performance in NAS-Bench-201 search space and S1-S4 search spaces, outperforming state-of-the-art NAS methods."}}
{"id": "AsSdrNJ-DZG", "cdate": 1663849969848, "mdate": null, "content": {"title": "Sweet Gradient Matters: Designing Consistent and Efficient Estimator for Zero-Shot Neural Architecture Search", "abstract": "Neural architecture search (NAS) is one of the core technologies of AutoML for designing high-performance networks. Recently, Zero-Shot NAS has gained growing interest due to its training-free property and super-fast search speed. However, existing Zero-Shot estimators commonly suffer from low consistency, which limits the reliability and applicability. In this paper, we observe that Sweet Gradient of parameters, i.e., the absolute gradient values within a certain interval, brings higher consistency in network performance compared to the overall number of parameters. We further demonstrate a positive correlation between the network depth and the parameter ratio of sweet gradients in each layer. Based on the analysis, we propose a training-free method to find the Sweet Gradient interval and obtain an estimator, named Sweetimator. Experiments show that Sweetimator has superior consistency to existing Zero-Shot estimators on four benchmarks with eight search spaces. Moreover, Sweetimator achieves state-of-the-art performance on NAS-Bench-201 and DARTS search spaces."}}
{"id": "_oGG0ZWwPjk", "cdate": 1640995200000, "mdate": 1667443298760, "content": {"title": "Searching for BurgerFormer with Micro-Meso-Macro Space Design", "abstract": "With the success of Transformers in the computer vision field, the automated design of vision Transformers has attracted significant attention. Recently, MetaFormer found that simple average poolin..."}}
{"id": "Qj4JOHa1qKp2", "cdate": 1640995200000, "mdate": 1667443298814, "content": {"title": "AGNAS: Attention-Guided Micro and Macro-Architecture Search", "abstract": "Micro- and macro-architecture search have emerged as two popular NAS paradigms recently. Existing methods leverage different search strategies for searching micro- and macro- architectures. When us..."}}
{"id": "BEFZAM0Y60y", "cdate": 1640995200000, "mdate": 1667443298609, "content": {"title": "STC-NAS: Fast neural architecture search with source-target consistency", "abstract": ""}}
{"id": "svQebCqaBH", "cdate": 1609459200000, "mdate": 1667443298751, "content": {"title": "DDSAS: Dynamic and Differentiable Space-Architecture Search", "abstract": "Neural Architecture Search (NAS) has made remarkable progress in automatically designing neural networks. However, existing differentiable NAS and stochastic NAS methods are either biased towards e..."}}
{"id": "8jWIciNGBt", "cdate": 1609459200000, "mdate": 1667443298624, "content": {"title": "DU-DARTS: Decreasing the Uncertainty of Differentiable Architecture Search", "abstract": ""}}
