{"id": "iV9Cs8s8keU", "cdate": 1663849980160, "mdate": null, "content": {"title": "Learning the Positions in CountSketch", "abstract": "We consider sketching algorithms which first compress data by multiplication with a random sketch matrix, and then apply the sketch to quickly solve an optimization problem, e.g., low-rank approximation and regression. In the learning-based sketching paradigm proposed by Indyk et al., the sketch matrix is found by choosing a random sparse matrix, e.g., CountSketch, and then the values of its non-zero entries are updated by running gradient descent on a training data set. Despite the growing body of work on this paradigm, a noticeable omission is that the locations of the non-zero entries of previous algorithms were fixed, and only their values were learned.\nIn this work, we propose the first learning-based algorithms that also optimize the locations of the non-zero entries. Our first proposed algorithm is based on a greedy algorithm. However, one drawback of the greedy algorithm is its slower training time. We fix this issue and propose approaches for learning a sketching matrix for both low-rank approximation and Hessian approximation for second-order optimization. The latter is helpful for a range of constrained optimization problems, such as LASSO and matrix estimation with a nuclear norm constraint. Both approaches achieve good accuracy with a fast running time.  Moreover, our experiments suggest that our algorithm can still reduce the error significantly even if we only have a very limited number of training matrices."}}
{"id": "9Q0B0TcUrr", "cdate": 1652714218575, "mdate": 1652714218575, "content": {"title": "Faster Fundamental Graph Algorithms via Learned Predictions", "abstract": "We consider the question of speeding up classic graph algorithms with machine-learned predictions. In this model, algorithms are furnished with extra advice learned from past or similar instances. Given the additional information, we aim to improve upon the traditional worst-case run-time guarantees. Our contributions are the following:\n(i) We give a faster algorithm for minimum-weight bipartite matching via learned duals, improving the recent result by Dinitz, Im, Lavastida, Moseley and Vassilvitskii (NeurIPS, 2021);\n(ii) We extend the learned dual approach to the single-source shortest path problem (with negative edge lengths), achieving an almost linear runtime given sufficiently accurate predictions which improves upon the classic fastest algorithm due to Goldberg (SIAM J. Comput., 1995);\n(iii) We provide a general reduction-based framework for learning-based graph algorithms, leading to new algorithms for degree-constrained subgraph and minimum-cost 0-1 flow, based on reductions to bipartite matching and the shortest path problem.\nFinally, we give a set of general learnability theorems, showing that the predictions required by our algorithms can be efficiently learned in a PAC fashion. "}}
{"id": "Hblqhv4fQ5", "cdate": 1648670641856, "mdate": 1648670641856, "content": {"title": "Fair Representation Clustering with Several Protected Classes", "abstract": "We study the problem of fair $k$-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation $k$-median problem, we are given a set of points $X$ in a metric space. Each point $x\\in X$ belongs to one of $\\ell$ groups. Further, we are given fair representation parameters $\\alpha_j$ and $\\beta_j$ for each group $j\\in [\\ell]$. We say that a $k$-clustering $C_1, \\cdots, C_k$ fairly represents all groups if the number of points from group $j$ in cluster $C_i$ is between $\\alpha_j |C_i|$ and $\\beta_j |C_i|$ for every $j\\in[\\ell]$ and $i\\in [k]$. The goal is to find a set $\\sC$ of $k$ centers and an assignment $\\phi: X\\rightarrow \\sC$ such that the clustering defined by $(\\sC, \\phi)$ fairly represents all groups and minimizes the $\\ell_1$-objective $\\sum_{x\\in X} d(x, \\phi(x))$.\n\nWe present an $O(\\log k)$-approximation algorithm that runs in time $n^{O(\\ell)}$. Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both $k$ and $\\ell$. We also consider an important special case of the problem where $\\alpha_j = \\beta_j = \\frac{f_j}{f}$ and $f_j, f \\in \\mathbb{N}$ for all $j\\in [\\ell]$. For this special case, we present an $O(\\log k)$-approximation algorithm that runs in $(kf)^{O(\\ell)}\\log n + \\poly(n)$ time."}}
{"id": "g_7y-N3MroQ", "cdate": 1609459200000, "mdate": null, "content": {"title": "Approximation Algorithms for Socially Fair Clustering", "abstract": "We present an $(e^{O(p)} \\frac{\\log \\ell}{\\log\\log\\ell})$-approximation algorithm for socially fair clustering with the $\\ell_p$-objective. In this problem, we are given a set of points in a metric space. Each point belongs to one (or several) of $\\ell$ groups. The goal is to find a $k$-medians, $k$-means, or, more generally, $\\ell_p$-clustering that is simultaneously good for all of the groups. More precisely, we need to find a set of $k$ centers $C$ so as to minimize the maximum over all groups $j$ of $\\sum_{u \\text{ in group }j} d(u,C)^p$. The socially fair clustering problem was independently proposed by Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and Venkatasubramanian [2021]. Our algorithm improves and generalizes their $O(\\ell)$-approximation algorithms for the problem. The natural LP relaxation for the problem has an integrality gap of $\\Omega(\\ell)$. In order to obtain our result, we introduce a strengthened LP relaxation and show that it has an integrality gap of $\\Theta(\\frac{\\log \\ell}{\\log\\log\\ell})$ for a fixed $p$. Additionally, we present a bicriteria approximation algorithm, which generalizes the bicriteria approximation of Abbasi et al. [2021]."}}
{"id": "RDiiCiIH3_B", "cdate": 1601308307309, "mdate": null, "content": {"title": "A framework for learned CountSketch", "abstract": "Sketching is a compression technique that can be applied to many problems to solve them quickly and approximately. The matrices used to project data to smaller dimensions are called \"sketches\". In this work, we consider the problem of optimizing sketches to obtain low approximation error over a data distribution. \n\nWe introduce a general framework for \"learning\" and applying CountSketch, a type of sparse sketch. The sketch optimization procedure has two stages: one for optimizing the placements of the sketch's non-zero entries and another for optimizing their values. Next, we provide a way to apply learned sketches that has worst-case guarantees for approximation error. \n\nWe instantiate this framework with three sketching applications: least-squares regression, low-rank approximation (LRA), and k-means clustering. Our experiments demonstrate that our approach substantially decreases approximation error compared to classical and naively learned sketches. \n\nFinally, we investigate the theoretical aspects of our approach. For regression and LRA, we show that our method obtains state-of-the art accuracy for fixed time complexity. For LRA, we prove that it is strictly better to include the first optimization stage for two standard input distributions. For k-means, we derive a more straightforward means of retaining approximation guarantees."}}
{"id": "SLcd90FCg6B", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Learned Sketches for Randomized Numerical Linear Algebra", "abstract": "We consider sketching algorithms which first quickly compress data by multiplication with a random sketch matrix, and then apply the sketch to quickly solve an optimization problem, e.g., low rank approximation. In the learning-based sketching paradigm proposed by Indyk et al. [2019], the sketch matrix is found by choosing a random sparse matrix, e.g., the CountSketch, and then updating the values of the non-zero entries by running gradient descent on a training data set. Despite the growing body of work on this paradigm, a noticeable omission is that the locations of the non-zero entries of previous algorithms were fixed, and only their values were learned. In this work we propose the first learning algorithm that also optimizes the locations of the non-zero entries. We show this algorithm gives better accuracy for low rank approximation than previous work, and apply it to other problems such as $k$-means clustering for the first time. We show that our algorithm is provably better in the spiked covariance model and for Zipfian matrices. We also show the importance of the sketch monotonicity property for combining learned sketches. Our empirical results show the importance of optimizing not only the values of the non-zero entries but also their positions."}}
{"id": "25pAl7fmAr-", "cdate": 1577836800000, "mdate": null, "content": {"title": "Individual Fairness for k-Clustering", "abstract": "We give a local search based algorithm for $k$-median and $k$-means (and more generally for any $k$-clustering with $\\ell_p$ norm cost function) from the perspective of individual fairness. More pr..."}}
{"id": "S1l5s7298H", "cdate": 1568486306385, "mdate": null, "content": {"title": "Learning-Based Low-Rank Approximations", "abstract": "We introduce a \u201clearning-based\u201d algorithm for the low-rank decomposition problem: given an $n \\times d$ matrix $A$, and a parameter $k$, compute a rank-$k$ matrix $A'$ that minimizes the approximation loss $||A- A'||_F$. The algorithm uses a training set of input matrices in order to optimize its performance. Specifically, some of the most efficient approximate algorithms for computing low-rank approximations proceed by computing a projection $SA$, where $S$ is a sparse random $m \\times n$ \u201csketching matrix\u201d, and then performing the singular value decomposition of $SA$. We show how to replace the random matrix $S$ with a \u201clearned\u201d matrix of the same sparsity to reduce the error.\n\nOur experiments show that, for multiple types of data sets, a learned sketch matrix can substantially reduce the approximation loss compared to a random matrix $S$, sometimes by one order of magnitude. We also study mixed matrices where only some of the rows are trained and the remaining ones are random, and show that matrices still offer improved performance while retaining worst-case guarantees."}}
{"id": "rJe6150HPE", "cdate": 1552439780802, "mdate": null, "content": {"title": "Scalable Fair Clustering", "abstract": ""}}
{"id": "ubUVN4RsDXP", "cdate": 1546300800000, "mdate": null, "content": {"title": "(Learned) Frequency Estimation Algorithms under Zipfian Distribution", "abstract": "begin{abstract} The frequencies of the elements in a data stream are an important statistical measure and the task of estimating them arises in many applications within data analysis and machine learning. Two of the most popular algorithms for this problem, Count-Min and Count-Sketch, are widely used in practice. In a recent work [Hsu et al., ICLR'19], it was shown empirically that augmenting Count-Min and Count-Sketch with a machine learning algorithm leads to a significant reduction of the estimation error. The experiments were complemented with an analysis of the expected error incurred by Count-Min (both the standard and the augmented version) when the input frequencies follow a Zipfian distribution. Although the authors established that the learned version of Count-Min has lower estimation error than its standard counterpart, their analysis of the standard Count-Min algorithm was not tight. Moreover, they provided no similar analysis for Count-Sketch. In this paper we resolve these problems. First, we provide a simple tight analysis of the expected error incurred by Count-Min. Second, we provide the first error bounds for both the standard and the augmented version of Count-Sketch. These bounds are nearly tight and again demonstrate an improved performance of the learned version of Count-Sketch. In addition to demonstrating tight gaps between the aforementioned algorithms, we believe that our bounds for the standard versions of Count-Min and Count-Sketch are of independent interest. In particular, it is a typical practice to set the number of hash functions in those algorithms to $\\Theta (\\log n)$. In contrast, our results show that to minimize the \\emph{expected} error, the number of hash functions should be a constant, strictly greater than $1$."}}
