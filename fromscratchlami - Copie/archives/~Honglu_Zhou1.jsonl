{"id": "gPJjfO6q7tI", "cdate": 1675209600000, "mdate": 1681767288496, "content": {"title": "A Multiscale Geospatial Dataset and an Interactive Visualization Dashboard for Computational Epidemiology and Open Scientific Research", "abstract": "The coronavirus disease (COVID-19) continued to strike as a highly infectious and fast-spreading disease in 2020 and 2021. As the research community actively responded to this pandemic, we saw the release of many COVID-19-related datasets and visualization dashboards. However, existing resources are insufficient to support multiscale and multifaceted modeling or simulation, which is suggested to be important by the computational epidemiology literature. This work presents a curated multiscale geospatial dataset with an interactive visualization dashboard under the context of COVID-19. This open dataset will allow researchers to conduct numerous projects or analyses relating to COVID-19 or simply geospatial-related scientific studies. The interactive visualization platform enables users to visualize the spread of the disease at different scales (e.g., country level to individual neighborhoods), and allows users to interact with the policies enforced at these scales (e.g., the closure of borders and lockdowns) to observe their impacts on the epidemiology."}}
{"id": "akbqXCL02w", "cdate": 1672531200000, "mdate": 1699161366753, "content": {"title": "Learning from Synthetic Human Group Activities", "abstract": "The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group activity recognition and RGB-based multi-person pose tracking. The results indicate that learning from our synthetic datasets largely improves the model performances on real-world datasets, with the highest gain of 5.59% and 7.32% respectively in group and person recognition accuracy on CAD2, as well as an improvement of 6.63 in MOTP on HiEve. Pre-training with our synthetic data also leads to faster model convergence on downstream tasks (up to 6.8% faster). Moreover, M3Act opens new research problems for 3D group activity generation. We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger group sizes and higher complexity of inter-person interactions than previous multi-person datasets. We define multiple metrics and propose a competitive baseline for the novel task."}}
{"id": "DMlIvj4Hwp", "cdate": 1672531200000, "mdate": 1681767288495, "content": {"title": "Procedure-Aware Pretraining for Instructional Video Understanding", "abstract": "Our goal is to learn a video representation that is useful for downstream procedure understanding tasks in instructional videos. Due to the small amount of available annotations, a key challenge in procedure understanding is to be able to extract from unlabeled videos the procedural knowledge such as the identity of the task (e.g., 'make latte'), its steps (e.g., 'pour milk'), or the potential next steps given partial progress in its execution. Our main insight is that instructional videos depict sequences of steps that repeat between instances of the same or different tasks, and that this structure can be well represented by a Procedural Knowledge Graph (PKG), where nodes are discrete steps and edges connect steps that occur sequentially in the instructional activities. This graph can then be used to generate pseudo labels to train a video representation that encodes the procedural knowledge in a more accessible form to generalize to multiple procedure understanding tasks. We build a PKG by combining information from a text-based procedural knowledge database and an unlabeled instructional video corpus and then use it to generate training pseudo labels with four novel pre-training objectives. We call this PKG-based pre-training procedure and the resulting model Paprika, Procedure-Aware PRe-training for Instructional Knowledge Acquisition. We evaluate Paprika on COIN and CrossTask for procedure understanding tasks such as task recognition, step recognition, and step forecasting. Paprika yields a video representation that improves over the state of the art: up to 11.23% gains in accuracy in 12 evaluation settings. Implementation is available at https://github.com/salesforce/paprika."}}
{"id": "1NBof2-YG6E", "cdate": 1672531200000, "mdate": 1699288496515, "content": {"title": "Harnessing Neighborhood Modeling and Asymmetry Preservation for Digraph Representation Learning", "abstract": "Digraph Representation Learning aims to learn representations for directed homogeneous graphs (digraphs). Prior work is largely constrained or has poor generalizability across tasks. Most Graph Neural Networks exhibit poor performance on digraphs due to the neglect of modeling neighborhoods and preserving asymmetry. In this paper, we address these notable challenges by leveraging hyperbolic collaborative learning from multi-ordered partitioned neighborhoods and asymmetry-preserving regularizers. Our resulting formalism, Digraph Hyperbolic Networks (D-HYPR), is versatile for multiple tasks including node classification, link presence prediction, and link property prediction. The efficacy of D-HYPR was meticulously examined against 21 previous techniques, using 8 real-world digraph datasets. D-HYPR statistically significantly outperforms the current state of the art. We release our code at https://github. com/hongluzhou/dhypr."}}
{"id": "pCE8rsan7f", "cdate": 1640995200000, "mdate": 1667335986414, "content": {"title": "Graph-based generative representation learning of semantically and behaviorally augmented floorplans", "abstract": "Floorplans are commonly used to represent the layout of buildings. Research works toward computational techniques that facilitate the design process, such as automated analysis and optimization, often using simple floorplan representations that ignore the space\u2019s semantics and do not consider usage-related analytics. We present a floorplan embedding technique that uses an attributed graph to model the floorplans\u2019 geometric information, design semantics, and behavioral features as the node and edge attributes. A long short-term memory (LSTM) variational autoencoder (VAE) architecture is proposed and trained to embed attributed graphs as vectors in a continuous space. A user study is conducted to evaluate the coupling of similar floorplans retrieved from the embedding space for a given input (e.g., design layout). The qualitative, quantitative, and user study evaluations show that our embedding framework produces meaningful and accurate vector representations for floorplans. Besides, our proposed model is generative. We studied and showcased its effectiveness for generating new floorplans. We also release the dataset that we have constructed. We include the design semantic attributes and simulation-generated human behavioral features for each floorplan in the dataset for further study in the community."}}
{"id": "nCvQdM0iVcU", "cdate": 1640995200000, "mdate": 1667335986399, "content": {"title": "D-HYPR: Harnessing Neighborhood Modeling and Asymmetry Preservation for Digraph Representation Learning", "abstract": "Digraph Representation Learning (DRL) aims to learn representations for directed homogeneous graphs (digraphs). Prior work in DRL is largely constrained (e.g., limited to directed acyclic graphs), or has poor generalizability across tasks (e.g., evaluated solely on one task). Most Graph Neural Networks (GNNs) exhibit poor performance on digraphs due to the neglect of modeling neighborhoods and preserving asymmetry. In this paper, we address these notable challenges by leveraging hyperbolic collaborative learning from multi-ordered and partitioned neighborhoods, and regularizers inspired by socio-psychological factors. Our resulting formalism, Digraph Hyperbolic Networks (D-HYPR) -- albeit conceptually simple -- generalizes to digraphs where cycles and non-transitive relations are common, and is applicable to multiple downstream tasks including node classification, link presence prediction, and link property prediction. In order to assess the effectiveness of D-HYPR, extensive evaluations were performed across 8 real-world digraph datasets involving 21 prior techniques. D-HYPR statistically significantly outperforms the current state of the art. We release our code at https://github.com/hongluzhou/dhypr"}}
{"id": "mSUY49xVQt", "cdate": 1640995200000, "mdate": 1667335986415, "content": {"title": "HM: Hybrid Masking for Few-Shot Segmentation", "abstract": "We study few-shot semantic segmentation that aims to segment a target object from a query image when provided with a few annotated support images of the target class. Several recent methods resort to a feature masking\u00a0(FM) technique to discard irrelevant feature activations which eventually facilitates the reliable prediction of segmentation mask. A fundamental limitation of FM is the inability to preserve the fine-grained spatial details that affect the accuracy of segmentation mask, especially for small target objects. In this paper, we develop a simple, effective, and efficient approach to enhance feature masking\u00a0(FM). We dub the enhanced FM as hybrid masking\u00a0(HM). Specifically, we compensate for the loss of fine-grained spatial details in FM technique by investigating and leveraging a complementary basic input masking method. Experiments have been conducted on three publicly available benchmarks with strong few-shot segmentation\u00a0(FSS) baselines. We empirically show improved performance against the current state-of-the-art methods by visible margins across different benchmarks. Our code and trained models are available at: https://github.com/moonsh/HM-Hybrid-Masking"}}
{"id": "fZegoq-qks", "cdate": 1640995200000, "mdate": 1681687797317, "content": {"title": "COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality", "abstract": "Group Activity Recognition detects the activity collectively performed by a group of actors, which requires compositional reasoning of actors and objects. We approach the task by modeling the video as tokens that represent the multi-scale semantic concepts in the video. We propose COMPOSER, a Multiscale Transformer based architecture that performs attention-based reasoning over tokens at each scale and learns group activity compositionally. In addition, prior works suffer from scene biases with privacy and ethical concerns. We only use the keypoint modality which reduces scene biases and prevents acquiring detailed visual data that may contain private or biased information of users. We improve the multiscale representations in COMPOSER by clustering the intermediate scale representations, while maintaining consistent cluster assignments between scales. Finally, we use techniques such as auxiliary prediction and data augmentations tailored to the keypoint signals to aid model training. We demonstrate the model\u2019s strength and interpretability on two widely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up to $$+5.4\\%$$ improvement with just the keypoint modality (Code is available at https://github.com/hongluzhou/composer .)."}}
{"id": "Y0HTqy7Q9cb", "cdate": 1640995200000, "mdate": 1699288496559, "content": {"title": "MSI: Maximize Support-Set Information for Few-Shot Segmentation", "abstract": "FSS(Few-shot segmentation) aims to segment a target class using a small number of labeled images (support set). To extract the information relevant to target class, a dominant approach in best performing FSS methods removes background features using a support mask. We observe that this feature excision through a limiting support mask introduces an information bottleneck in several challenging FSS cases, e.g., for small targets and/or inaccurate target boundaries. To this end, we present a novel method (MSI), which maximizes the support-set information by exploiting two complementary sources of features to generate super correlation maps. We validate the effectiveness of our approach by instantiating it into three recent and strong FSS methods. Experimental results on several publicly available FSS benchmarks show that our proposed method consistently improves the performance by visible margins and leads to faster convergence. Our code and models will be publicly released."}}
{"id": "QAfIaFH0enK", "cdate": 1640995200000, "mdate": 1667335986400, "content": {"title": "HMFS: Hybrid Masking for Few-Shot Segmentation", "abstract": "We study few-shot semantic segmentation that aims to segment a target object from a query image when provided with a few annotated support images of the target class. Several recent methods resort to a feature masking (FM) technique to discard irrelevant feature activations which eventually facilitates the reliable prediction of segmentation mask. A fundamental limitation of FM is the inability to preserve the fine-grained spatial details that affect the accuracy of segmentation mask, especially for small target objects. In this paper, we develop a simple, effective, and efficient approach to enhance feature masking (FM). We dub the enhanced FM as hybrid masking (HM). Specifically, we compensate for the loss of fine-grained spatial details in FM technique by investigating and leveraging a complementary basic input masking method. Experiments have been conducted on three publicly available benchmarks with strong few-shot segmentation (FSS) baselines. We empirically show improved performance against the current state-of-the-art methods by visible margins across different benchmarks. Our code and trained models are available at: https://github.com/moonsh/HM-Hybrid-Masking"}}
