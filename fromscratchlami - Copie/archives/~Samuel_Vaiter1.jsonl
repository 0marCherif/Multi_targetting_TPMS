{"id": "1tTo7jCUNFM", "cdate": 1684244204087, "mdate": 1684244204087, "content": {"title": "A sharp oracle inequality for Graph-Slope", "abstract": "Following recent success on the analysis of the Slope estimator, we provide a sharp oracle inequality in term of prediction error for Graph-Slope, a generalization of Slope to signals observed over a graph. In addition to improving upon best results obtained so far for the Total Variation denoiser (also referred to as Graph-Lasso or Generalized Lasso), we propose an efficient algorithm to compute Graph-Slope. The proposed algorithm is obtained by applying the forward-backward method to the dual formulation of the Graph-Slope optimization problem. We also provide experiments showing the interest of the method. "}}
{"id": "dONBM4KWgS", "cdate": 1672531200000, "mdate": 1681510346658, "content": {"title": "On the Robustness of Text Vectorizers", "abstract": ""}}
{"id": "Z24qJD_IrXA", "cdate": 1672531200000, "mdate": 1679901406719, "content": {"title": "A Near-Optimal Algorithm for Bilevel Empirical Risk Minimization", "abstract": ""}}
{"id": "Wj2cMnlE5Pm", "cdate": 1655740816122, "mdate": null, "content": {"title": "Supervised graph learning with bilevel optimization", "abstract": "Graph-based learning attracted attention recently due to its efficiency analyzing data lying on graphs. Unfortunately, graphs in real-world are usually either not-given, noisy, or incomplete. In this work, we design a novel algorithm that addresses this issue by training a $G2G$ (Graph to Graph) model with a bilevel optimization framework to learn a better graph in a supervised manner. The trained model operates not only on training data, but generalizes to unseen data points. A bilevel problem comprises two optimization problems, referred to as outer and inner problem. The inner problem aims to solve the downstream task, e.g., training a $GCN$ (Graph Convolutional Network) model, whereas the outer one introduces a new objective function to evaluate the inner model performance, and the $G2G$ model is trained to minimize this function. To solve this optimization, we replace the solution of the inner problem with the output of any gradient-based algorithm proven to give a good surrogate. Then, we use automatic differentiation to compute the gradient of this output w.r.t. the $G2G$ weights, which we consequently learn with a gradient-based algorithm. Experiments on semi-supervised learning datasets show that the graph learned by the $G2G$ model outperforms the original graph by a significant margin."}}
{"id": "1uSzacpyWLH", "cdate": 1652737787285, "mdate": null, "content": {"title": "Benchopt: Reproducible, efficient and collaborative optimization benchmarks", "abstract": "Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: $\\ell_2$-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details."}}
{"id": "WUMH5xloWn", "cdate": 1652737415389, "mdate": null, "content": {"title": "Automatic differentiation of nonsmooth iterative algorithms", "abstract": "Differentiation along algorithms, i.e., piggyback propagation of derivatives, is now routinely used to differentiate iterative solvers in differentiable programming. Asymptotics is well understood for many smooth problems but the nondifferentiable case is hardly considered. Is there a limiting object for nonsmooth piggyback automatic differentiation (AD)? Does it have any variational meaning and can it be used effectively in machine learning? Is there a connection with classical derivative? All these questions are addressed under appropriate contractivity conditions in the framework of conservative derivatives which has proved useful in understanding nonsmooth AD. For nonsmooth piggyback iterations, we characterize the attractor set of nonsmooth piggyback iterations as a set-valued fixed point which remains in the conservative framework. This has various consequences and in particular almost everywhere convergence of classical derivatives. Our results are illustrated on parametric convex optimization problems with forward-backward, Douglas-Rachford and Alternating Direction of Multiplier algorithms as well as the Heavy-Ball method."}}
{"id": "wlEOsQ917F", "cdate": 1652737351337, "mdate": null, "content": {"title": "A framework for bilevel optimization that enables  stochastic and global variance reduction algorithms", "abstract": "Bilevel optimization, the problem of minimizing a value function which involves the arg-minimum of another function, appears in many areas of machine learning. In a large scale empirical risk minimization setting where the number of samples is huge, it is crucial to develop stochastic methods, which only use a few samples at a time to progress. However, computing the gradient of the value function involves solving a linear system, which makes it difficult to derive unbiased stochastic estimates.\nTo overcome this problem we introduce a novel framework, in which the solution of the inner problem, the solution of the linear system, and the main variable evolve at the same time. These directions are written as a sum, making it straightforward to derive unbiased estimates.\nThe simplicity of our approach allows us to develop global variance reduction algorithms, where the dynamics of all variables is subject to variance reduction.\nWe demonstrate that SABA, an adaptation of the celebrated SAGA algorithm in our framework, has $O(\\frac1T)$ convergence rate, and that it achieves linear convergence under Polyak-Lojasciewicz assumption.\nThis is the first stochastic algorithm for bilevel optimization that verifies either of these properties.\nNumerical experiments validate the usefulness of our method."}}
{"id": "lT2HnFBHakW", "cdate": 1640995200000, "mdate": 1681490235728, "content": {"title": "A framework for bilevel optimization that enables stochastic and global variance reduction algorithms", "abstract": ""}}
{"id": "g1aIK3X1Kdb", "cdate": 1640995200000, "mdate": 1681510346671, "content": {"title": "Automatic differentiation of nonsmooth iterative algorithms", "abstract": ""}}
{"id": "dT9T49uu3h", "cdate": 1640995200000, "mdate": 1671902109669, "content": {"title": "Benchopt: Reproducible, efficient and collaborative optimization benchmarks", "abstract": "Numerical validation is at the core of machine learning research as it allows to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automate, reproduce and publish optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard learning tasks: $\\ell_2$-regularized logistic regression, Lasso, and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of the state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details. We hope that Benchopt will foster collaborative work in the community hence improving the reproducibility of research findings."}}
