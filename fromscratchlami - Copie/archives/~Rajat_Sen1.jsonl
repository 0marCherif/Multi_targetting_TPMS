{"id": "O11qR0Qrcc", "cdate": 1676827104571, "mdate": null, "content": {"title": "Dirichlet Proportions Model for Hierarchically Coherent Probabilistic Forecasting", "abstract": "Probabilistic, hierarchically coherent forecasting is a key problem in many practical forecasting applications -- the goal is to obtain coherent probabilistic predictions for a large number of  time series  arranged in a pre-specified tree hierarchy.  In this paper, we present an end-to-end deep  probabilistic model for hierarchical forecasting that is motivated by a classical top-down strategy. It jointly learns the distribution of the root time series, and the (dirichlet) proportions according to which each parent time-series is split among its children at any point in time. \nThe resulting forecasts are naturally coherent, and provide probabilistic predictions over all time series in the hierarchy. We experiment on several public datasets and demonstrate significant improvements of up to 26% on most datasets compared to state-of-the-art baselines. Finally, we also provide theoretical justification for the superiority of our top-down approach compared to the more traditional bottom-up modeling."}}
{"id": "CIMu1aVFF2b", "cdate": 1676827080948, "mdate": null, "content": {"title": "Blackbox optimization of unimodal functions", "abstract": "We provide an intuitive new algorithm for blackbox stochastic optimization of unimodal functions, a function class that we observe empirically can capture hyperparameter-tuning loss surfaces. Our method's convergence guarantee automatically adapts to Lipschitz constants and other problem difficulty parameters, recovering and extending prior results. We complement our theoretical development with experimental validation on hyperparameter tuning tasks."}}
{"id": "dMSxTUlQgrZ", "cdate": 1663849927143, "mdate": null, "content": {"title": "A deep top-down approach to hierarchically coherent probabilistic forecasting ", "abstract": "Probabilistic, hierarchically coherent forecasting is a key problem in many practical forecasting applications -- the goal is to obtain coherent probabilistic predictions for a large number of  time series  arranged in a pre-specified tree hierarchy.  In this paper, we present a probabilistic top-down approach to hierarchical forecasting that uses a novel attention-based RNN model to learn the distribution of the proportions according to which each parent prediction is split among its children nodes at any point in time. These probabilistic proportions are then coupled with an independent univariate probabilistic forecasting model for the root time series. The resulting forecasts are naturally coherent, and provide probabilistic predictions over all time series in the hierarchy. We experiment on several public datasets and demonstrate significant improvements up to 27% on most datasets compared to state-of-the-art probabilistic hierarchical models. Finally, we also provide theoretical justification for the superiority of our top-down approach compared to traditional bottom-up modeling."}}
{"id": "VHmdFPy4U_u", "cdate": 1652737775762, "mdate": null, "content": {"title": "Trimmed Maximum Likelihood Estimation for Robust Generalized Linear Model", "abstract": "We study the problem of learning generalized linear models under adversarial corruptions.\nWe analyze a classical heuristic called the \\textit{iterative trimmed maximum likelihood estimator} which is known to be effective against \\textit{label corruptions} in practice. Under label corruptions, we prove that this simple estimator achieves minimax near-optimal risk on a wide range of generalized linear models, including Gaussian regression, Poisson regression and Binomial regression. Finally, we extend the estimator to the much more challenging setting of \\textit{label and covariate corruptions} and demonstrate its robustness and optimality in that setting as well."}}
{"id": "_Vn-mKDipa1", "cdate": 1632875725410, "mdate": null, "content": {"title": "Hierarchically Regularized Deep Forecasting", "abstract": "Hierarchical forecasting is a key problem in many practical multivariate forecasting applications - the goal is to simultaneously predict a large number of correlated time series that are arranged in a pre-specified aggregation hierarchy. The main challenge is to exploit the hierarchical correlations to simultaneously obtain good prediction accuracy for time series at different levels of the hierarchy. In this paper, we propose a new approach for hierarchical forecasting which consists of two components. First, decomposing the time series along a global set of basis time series and modeling hierarchical constraints using the coefficients of the basis decomposition. And second, using a linear autoregressive model with coefficients that vary with time. Unlike past methods, our approach is scalable (inference for a specific time series only needs access to its own history) while also modeling the hierarchical structure via (approximate) coherence constraints among the time series forecasts. We experiment on several public datasets and demonstrate significantly improved overall performance on forecasts at different levels of the hierarchy, compared to existing state-of-the-art hierarchical models."}}
{"id": "zrW-LVXj2k1", "cdate": 1632875547903, "mdate": null, "content": {"title": "On the benefits of maximum likelihood estimation for Regression and Forecasting", "abstract": "We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a specific target metric. The MLE approach is better suited to capture inductive biases such as prior domain knowledge in datasets, and can output post-hoc estimators at inference time that can optimize different types of target metrics. We present theoretical results to demonstrate that our approach is competitive with any estimator for the target metric under some general conditions. In two example practical settings, Poisson and Pareto regression, we show that our competitive results can be used to prove that the MLE approach has better excess risk bounds than directly minimizing the target metric. We also demonstrate empirically that our method instantiated with a well-designed general purpose mixture likelihood family can obtain superior performance for a variety of tasks across time-series forecasting and regression datasets with different data distributions."}}
{"id": "7L4sTEEyFEU", "cdate": 1594403735982, "mdate": null, "content": {"title": "Contextual Bandits with Stochastic Experts", "abstract": "We consider the problem of contextual bandits with stochastic experts, which is a variation of the traditional stochastic contextual bandit with experts problem. In our problem setting, we assume access to a class of stochastic experts, where each expert is a conditional distribution over the arms given a context. We propose upper-confidence bound (UCB) algorithms for this problem, which employ two different importance sampling based estimators for the mean reward for each expert. Both these estimators leverage information leakage among the experts, thus using samples collected under all the experts to estimate the mean reward of any given expert. This leads to instance dependent regret bounds of $\\lambda(\\mu){\\cal M} \\log(T)/\\Delta$, where $\\lambda(\\mu)$ is a term that depends on the mean rewards of the experts, \u0394 is the smallest gap between the mean reward of the optimal expert and the rest, and ${\\cal M}$ quantifies the information leakage among the experts. We show that under some assumptions $\\lambda(\\mu)$ is typically $\\Theta(\\log N)$. We implement our algorithm with stochastic experts generated from cost-sensitive classification oracles and show superior empirical performance on real-world datasets, when compared to other state of the art contextual bandit algorithms. "}}
{"id": "r1f4YEBl8r", "cdate": 1567802492387, "mdate": null, "content": {"title": "Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting", "abstract": "Forecasting high-dimensional time series plays a crucial role in many applications such as demand forecasting and financial predictions. Modern datasets can have millions of correlated time-series that evolve together, i.e they are extremely high dimensional (one dimension for each individual time-series). There is a need for exploiting global patterns and coupling them with local calibration for better prediction. However, most recent deep learning approaches in the literature are one-dimensional, i.e, even though they are trained on the whole dataset, during prediction, the future forecast for a single dimension mainly depends on past values from the same dimension. In this paper, we seek to correct this deficiency and propose DeepGLO, a deep forecasting model which thinks globally and acts locally. In particular, DeepGLO is a hybrid model that combines a global matrix factorization model regularized by a temporal deep network with a local deep temporal model that captures patterns specific to each dimension. The global and local models are combined via a data-driven attention mechanism for each dimension. The proposed deep architecture used  is a variation of temporal convolution termed as leveled network which can be trained effectively on high-dimensional but diverse time series, where different time series can have vastly different scales, without a priori normalization or rescaling. Empirical results demonstrate that DeepGLO can scale to datasets of dimension 1M and outperform state-of-the-art approaches; for example, we see more than 30% improvement in WAPE over other methods on a public dataset that contains more than 100K-dimensional time series."}}
{"id": "SJ-R4hZOWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Multi-Fidelity Black-Box Optimization with Hierarchical Partitions", "abstract": "Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function. Multi-fidelity techniques have become popular for app..."}}
{"id": "ryW5WPZOWB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Model-Powered Conditional Independence Test", "abstract": "We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we determine whether $X \\independent Y \\vert Z$. We approach this by converting the conditional independence test into a classification problem. This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks. These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution $f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X \\independent Y \\vert Z.$ -- when given access only to i.i.d. samples from the true joint distribution $f(x,y,z)$. To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $f^{CI}$ in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d \\textit{near-independent} samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods."}}
