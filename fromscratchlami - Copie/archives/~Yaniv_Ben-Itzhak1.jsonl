{"id": "n12Bi-Nk7q", "cdate": 1672531200000, "mdate": 1680001494544, "content": {"title": "DoCoFL: Downlink Compression for Cross-Device Federated Learning", "abstract": ""}}
{"id": "04OL67rm6ok", "cdate": 1663850297127, "mdate": null, "content": {"title": "QUIC-FL: : Quick Unbiased Compression for Federated Learning", "abstract": "Distributed Mean Estimation (DME) is a fundamental building block in communication efficient federated learning. In DME, clients communicate their lossily compressed gradients to the parameter server, which estimates the average and updates the model. \nState of the art DME techniques apply either unbiased quantization methods, resulting in large estimation errors, or biased quantization methods, where unbiasing the result requires that the server decodes each gradient individually, which markedly slows the aggregation time.\nIn this paper, we propose QUIC-FL, a DME algorithm that achieves the best of all worlds. QUIC-FL is unbiased, offers fast aggregation time, and is competitive with the most accurate (slow aggregation) DME techniques. To achieve this, we formalize the problem in a novel way that allows us to use standard solvers to design near-optimal unbiased quantization schemes."}}
{"id": "lTmJ1iAtM2i", "cdate": 1640995200000, "mdate": 1674492437056, "content": {"title": "EDEN: Communication-Efficient and Robust Distributed Mean Estimation for Federated Learning", "abstract": "Distributed Mean Estimation (DME) is a central building block in federated learning, where clients send local gradients to a parameter server for averaging and updating the model. Due to communicat..."}}
{"id": "XczwXALXob", "cdate": 1640995200000, "mdate": 1680001494539, "content": {"title": "Automating In-Network Machine Learning", "abstract": ""}}
{"id": "N_BBAxHb-Li", "cdate": 1640995200000, "mdate": 1675683647144, "content": {"title": "ScionFL: Secure Quantized Aggregation for Federated Learning", "abstract": "Secure aggregation is commonly used in federated learning (FL) to alleviate privacy concerns related to the central aggregator seeing all parameter updates in the clear. Unfortunately, most existing secure aggregation schemes ignore two critical orthogonal research directions that aim to (i) significantly reduce client-server communication and (ii) mitigate the impact of malicious clients. However, both of these additional properties are essential to facilitate cross-device FL with thousands or even millions of (mobile) participants. In this paper, we unite both research directions by introducing ScionFL, the first secure aggregation framework for FL that operates efficiently on quantized inputs and simultaneously provides robustness against malicious clients. Our framework leverages (novel) multi-party computation (MPC) techniques and supports multiple linear (1-bit) quantization schemes, including ones that utilize the randomized Hadamard transform and Kashin's representation. Our theoretical results are supported by extensive evaluations. We show that with no overhead for clients and moderate overhead on the server side compared to transferring and processing quantized updates in plaintext, we obtain comparable accuracy for standard FL benchmarks. Additionally, we demonstrate the robustness of our framework against state-of-the-art poisoning attacks."}}
{"id": "Dv3lbV4FDgu", "cdate": 1640995200000, "mdate": 1680001494485, "content": {"title": "Efficient multiclass classification with duet", "abstract": ""}}
{"id": "7SV18WTWQ9", "cdate": 1640995200000, "mdate": 1674492437045, "content": {"title": "QUIC-FL: Quick Unbiased Compression for Federated Learning", "abstract": "Distributed Mean Estimation (DME), in which $n$ clients communicate vectors to a parameter server that estimates their average, is a fundamental building block in communication-efficient federated learning. In this paper, we improve on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean Squared Error (NMSE) guarantee by asymptotically improving the complexity for either encoding or decoding (or both). To achieve this, we formalize the problem in a novel way that allows us to use off-the-shelf mathematical solvers to design the quantization."}}
{"id": "2HD1PiBGLd", "cdate": 1640995200000, "mdate": 1680001494554, "content": {"title": "IIsy: Practical In-Network Classification", "abstract": ""}}
{"id": "0i0t6HXHMip", "cdate": 1640995200000, "mdate": 1682318318693, "content": {"title": "CloudCast: Characterizing Public Clouds Connectivity", "abstract": "Public clouds are one of the most thriving technologies of the past decade. Major applications over public clouds require world-wide distribution and large amounts of data exchange between their distributed servers. To that end, major cloud providers have invested tens of billions of dollars in building world-wide inter-region networking infrastructure that can support high performance communication into, out of, and across public cloud geographic regions. In this paper, we lay the foundation for a comprehensive study and real time monitoring of various characteristic of networking within and between public clouds. We start by presenting CloudCast, a world-wide and expandable measurements and analysis system, currently (January 2019)collecting data from three major public clouds (AWS, GCPand Azure), 59 regions, 1184 intra-cloud and 2238 cross-cloud links (each link represents a direct connection between a pair of regions), amounting to a total of 3422 continuously monitored links and providing active measurements every minute.CloudCast is composed of measurement agents automatically installed in each public cloud region, centralized control, measurement data base, analysis engine and visualization tools. Then we turn to analyze the latency measurement data collected over almost a year . Our analysis yields surprising results. First, each public cloud exhibits a unique set of link latency behaviors along time. Second, using a novel, fair evaluation methodology, termed similar links, we compare the three clouds. Third, we prove that more than 50% of all links do not provide the optimal RTT through the methodology of triangles. Triangles also provide a framework to get around bottlenecks, benefiting not only the majority (53%-70%) of the cross-cloud links by 30% to 70%, but also a significant portion (29%-45%) of intra-cloud links by 14%-33%."}}
{"id": "KXRTmcv3dQ8", "cdate": 1621629929205, "mdate": null, "content": {"title": "DRIVE: One-bit Distributed Mean Estimation", "abstract": "We consider the problem where $n$ clients transmit $d$-dimensional real-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the receiver to approximately reconstruct their mean. Such compression problems naturally arise in distributed and federated learning. We provide novel mathematical results and derive computationally efficient algorithms that are more accurate than previous compression techniques.  We evaluate our methods on a collection of distributed and federated learning tasks, using a variety of datasets, and show a consistent improvement over the state of the art."}}
