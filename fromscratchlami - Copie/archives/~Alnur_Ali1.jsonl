{"id": "HxbesiyLrlc", "cdate": 1645727651263, "mdate": 1645727651263, "content": {"title": "A Comment and Erratum on \"Excess Optimism: How Biased is the Apparent Error of an Estimator Tuned by SURE?\"", "abstract": "We identify and correct an error in the paper \"Excess Optimism: How Biased is the Apparent Error of an Estimator Tuned by SURE?\" This correction allows new guarantees on the excess degrees of freedom--the bias in the error estimate of Stein's unbiased risk estimate (SURE) for an estimator tuned by directly minimizing the SURE criterion--for arbitrary SURE-tuned linear estimators. Oracle inequalities follow as a consequence of these results for such estimators."}}
{"id": "HffgaRRBSx5", "cdate": 1645727445246, "mdate": 1645727445246, "content": {"title": "The Lifecycle of a Statistical Model: Model Failure Detection, Identification, and Refitting", "abstract": "The statistical machine learning community has demonstrated considerable resourcefulness over the years in developing highly expressive tools for estimation, prediction, and inference. The bedrock assumptions underlying these developments are that the data comes from a fixed population and displays little heterogeneity. But reality is significantly more complex: statistical models now routinely fail when released into real-world systems and scientific applications, where such assumptions rarely hold. Consequently, we pursue a different path in this paper vis-a-vis the well-worn trail of developing new methodology for estimation and prediction. In this paper, we develop tools and theory for detecting and identifying regions of the covariate space (subpopulations) where model performance has begun to degrade, and study intervening to fix these failures through refitting. We present empirical results with three real-world data sets -- including a time series involving forecasting the incidence of COVID-19 -- showing that our methodology generates interpretable results, is useful for tracking model performance, and can boost model performance through refitting. We complement these empirical results with theory proving that our methodology is minimax optimal for recovering anomalous subpopulations as well as refitting to improve accuracy in a structured normal means setting."}}
{"id": "HfMbAh1IHgq", "cdate": 1640995200000, "mdate": 1645727669976, "content": {"title": "Accelerated Gradient Flow: Risk, Stability, and Implicit Regularization", "abstract": "Acceleration and momentum are the de facto standard in modern applications of machine learning and optimization, yet the bulk of the work on implicit regularization focuses instead on unaccelerated methods. In this paper, we study the statistical risk of the iterates generated by Nesterov's accelerated gradient method and Polyak's heavy ball method, when applied to least squares regression, drawing several connections to explicit penalization. We carry out our analyses in continuous-time, allowing us to make sharper statements than in prior work, and revealing complex interactions between early stopping, stability, and the curvature of the loss function."}}
{"id": "BRzW0nyLBgc", "cdate": 1640995200000, "mdate": 1645727669976, "content": {"title": "Predictive Inference with Weak Supervision", "abstract": "The expense of acquiring labels in large-scale statistical machine learning makes partially and weakly-labeled data attractive, though it is not always apparent how to leverage such data for model fitting or validation. We present a methodology to bridge the gap between partial supervision and validation, developing a conformal prediction framework to provide valid predictive confidence sets -- sets that cover a true label with a prescribed probability, independent of the underlying distribution -- using weakly labeled data. To do so, we introduce a (necessary) new notion of coverage and predictive validity, then develop several application scenarios, providing efficient algorithms for classification and several large-scale structured prediction problems. We corroborate the hypothesis that the new coverage definition allows for tighter and more informative (but valid) confidence sets through several experiments."}}
{"id": "Ko7lK-3DBLF", "cdate": 1620365767824, "mdate": null, "content": {"title": "The Generalized Lasso Problem and Uniqueness", "abstract": "We study uniqueness in the generalized lasso problem, where the penalty is the \u21131 norm of a matrix D times the coefficient vector. We derive a broad result on uniqueness that places weak assumptions on the predictor matrix X and penalty matrix D; the implication is that, if D is fixed and its null space is not too large (the dimension of its null space is at most the number of samples), and X and response vector y jointly follow an absolutely continuous distribution, then the generalized lasso problem has a unique solution almost surely, regardless of the number of predictors relative to the number of samples. This effectively generalizes previous uniqueness results for the lasso problem (which corresponds to the special case D=I). Further, we extend our study to the case in which the loss is given by the negative log-likelihood from a generalized linear model. In addition to uniqueness results, we derive results on the local stability of generalized lasso solutions that might be of interest in their own right."}}
{"id": "p1cA1jK_dpP", "cdate": 1620365698271, "mdate": null, "content": {"title": "Confidence bands for a log-concave density", "abstract": "We present a new approach for inference about a log-concave distribution: Instead of using the method of maximum likelihood, we propose to incorporate the log-concavity constraint in an appropriate nonparametric confidence set for the cdf F. This approach has the advantage that it automatically provides a measure of statistical uncertainty and it thus overcomes a marked limitation of the maximum likelihood estimate. In particular, we show how to construct confidence bands for the density that have a finite sample guaranteed confidence level. The nonparametric confidence set for F which we introduce here has attractive computational and statistical properties: It allows to bring modern tools from optimization to bear on this problem via difference of convex programming, and it results in optimal statistical inference. We show that the width of the resulting confidence bands converges at nearly the parametric n\u221212 rate when the log density is k-affine."}}
{"id": "ERzxEi7pCL", "cdate": 1620365604602, "mdate": null, "content": {"title": "Minimizing Oracle-Structured Composite Functions", "abstract": "We consider the problem of minimizing a composite convex function with two different access methods: an oracle, for which we can evaluate the value and gradient, and a structured function, which we access only by solving a convex optimization problem. We are motivated by two associated technological developments. For the oracle, systems like PyTorch or TensorFlow can automatically and efficiently compute gradients, given a computation graph description. For the structured function, systems like CVXPY accept a high level domain specific language description of the problem, and automatically translate it to a standard form for efficient solution. We develop a method that makes minimal assumptions about the two functions, does not require the tuning of algorithm parameters, and works well in practice across a variety of problems. Our algorithm combines a number of well-known ideas, including a low-rank quasi-Newton approximation of curvature, piecewise affine lower bounds from bundle-type methods, and two types of damping to ensure stability. We illustrate the method on stochastic optimization, utility maximization, and risk-averse programming problems."}}
{"id": "onSUMrsdQW4", "cdate": 1609459200000, "mdate": null, "content": {"title": "Minimum-Distortion Embedding", "abstract": "We consider the vector embedding problem. We are given a finite set of items, with the goal of assigning a representative vector to each one, possibly under some constraints (such as the collection of vectors being standardized, i.e., having zero mean and unit covariance). We are given data indicating that some pairs of items are similar, and optionally, some other pairs are dissimilar. For pairs of similar items, we want the corresponding vectors to be near each other, and for dissimilar pairs, we want the corresponding vectors to not be near each other, measured in Euclidean distance. We formalize this by introducing distortion functions, defined for some pairs of the items. Our goal is to choose an embedding that minimizes the total distortion, subject to the constraints. We call this the minimum-distortion embedding (MDE) problem. The MDE framework is simple but general. It includes a wide variety of embedding methods, such as spectral embedding, principal component analysis, multidimensional scaling, dimensionality reduction methods (like Isomap and UMAP), force-directed layout, and others. It also includes new embeddings, and provides principled ways of validating historical and new embeddings alike. We develop a projected quasi-Newton method that approximately solves MDE problems and scales to large data sets. We implement this method in PyMDE, an open-source Python package. In PyMDE, users can select from a library of distortion functions and constraints or specify custom ones, making it easy to rapidly experiment with different embeddings. Our software scales to data sets with millions of items and tens of millions of distortion functions. To demonstrate our method, we compute embeddings for several real-world data sets, including images, an academic co-author network, US county demographic data, and single-cell mRNA transcriptomes."}}
{"id": "a_aE0664IyX", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Implicit Regularization of Stochastic Gradient Flow for Least Squares", "abstract": "We study the implicit regularization of mini-batch stochastic gradient descent, when applied to the fundamental problem of least squares regression. We leverage a continuous-time stochastic differe..."}}
{"id": "24fOQs5Io1z", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust Validation: Confident Predictions Even When Distributions Shift", "abstract": "While the traditional viewpoint in machine learning and statistics assumes training and testing samples come from the same population, practice belies this fiction. One strategy---coming from robust statistics and optimization---is thus to build a model robust to distributional perturbations. In this paper, we take a different approach to describe procedures for robust predictive inference, where a model provides uncertainty estimates on its predictions rather than point predictions. We present a method that produces prediction sets (almost exactly) giving the right coverage level for any test distribution in an $f$-divergence ball around the training population. The method, based on conformal inference, achieves (nearly) valid coverage in finite samples, under only the condition that the training data be exchangeable. An essential component of our methodology is to estimate the amount of expected future data shift and build robustness to it; we develop estimators and prove their consistency for protection and validity of uncertainty estimates under shifts. By experimenting on several large-scale benchmark datasets, including Recht et al.'s CIFAR-v4 and ImageNet-V2 datasets, we provide complementary empirical results that highlight the importance of robust predictive validity."}}
