{"id": "sd6ipAuYxXz", "cdate": 1672531200000, "mdate": 1682341847069, "content": {"title": "Tree Learning: Optimal Algorithms and Sample Complexity", "abstract": "We study the problem of learning a hierarchical tree representation of data from labeled samples, taken from an arbitrary (and possibly adversarial) distribution. Consider a collection of data tuples labeled according to their hierarchical structure. The smallest number of such tuples required in order to be able to accurately label subsequent tuples is of interest for data collection in machine learning. We present optimal sample complexity bounds for this problem in several learning settings, including (agnostic) PAC learning and online learning. Our results are based on tight bounds of the Natarajan and Littlestone dimensions of the associated problem. The corresponding tree classifiers can be constructed efficiently in near-linear time."}}
{"id": "iWIioBwuNV", "cdate": 1664731450408, "mdate": null, "content": {"title": "Escaping from Moderately Constrained Saddles", "abstract": "We give polynomial time algorithms for escaping from high-dimensional saddle points under a moderate number of constraints. Given gradient access to a smooth function $f \\colon \\mathbb R^d \\to \\mathbb R$ we show that (noisy) gradient descent methods can escape from saddle points under a logarithmic number of inequality constraints. This constitutes progress (without reliance on NP-oracles or altering the definitions to only account for certain constraints) on the main open question of the breakthrough work of Ge et al. who showed an analogous result for unconstrained and equality-constrained problems. Our results hold for both regular and stochastic gradient descent."}}
{"id": "j_4xH_gyhO", "cdate": 1640995200000, "mdate": 1682341846995, "content": {"title": "HOUDINI: Escaping from Moderately Constrained Saddles", "abstract": "We give the first polynomial time algorithms for escaping from high-dimensional saddle points under a moderate number of constraints. Given gradient access to a smooth function $f \\colon \\mathbb R^d \\to \\mathbb R$ we show that (noisy) gradient descent methods can escape from saddle points under a logarithmic number of inequality constraints. This constitutes the first tangible progress (without reliance on NP-oracles or altering the definitions to only account for certain constraints) on the main open question of the breakthrough work of Ge et al. who showed an analogous result for unconstrained and equality-constrained problems. Our results hold for both regular and stochastic gradient descent."}}
{"id": "Ke9lCi1vGF", "cdate": 1621630100430, "mdate": null, "content": {"title": "Escaping Saddle Points with Compressed SGD", "abstract": "Stochastic gradient descent (SGD) is a prevalent optimization technique for large-scale distributed machine learning. While SGD computation can be efficiently divided between multiple machines, communication typically becomes a bottleneck in the distributed setting. Gradient compression methods can be used to alleviate this problem, and a recent line of work shows that SGD augmented with gradient compression converges to an $\\varepsilon$-first-order stationary point. In this paper we extend these results to convergence to an $\\varepsilon$-second-order stationary point ($\\varepsilon$-SOSP), which is to the best of our knowledge the first result of this type. In addition, we show that, when the stochastic gradient is not Lipschitz, compressed SGD with RandomK compressor converges to an $\\varepsilon$-SOSP with the same number of iterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the total communication by a factor of $\\tilde \\Theta(\\sqrt{d} \\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem. We present additional results for the cases when the compressor is arbitrary and when the stochastic gradient is Lipschitz."}}
{"id": "prF4ONaS51", "cdate": 1609459200000, "mdate": 1682341846937, "content": {"title": "Escaping Saddle Points with Compressed SGD", "abstract": "Stochastic gradient descent (SGD) is a prevalent optimization technique for large-scale distributed machine learning. While SGD computation can be efficiently divided between multiple machines, communication typically becomes a bottleneck in the distributed setting. Gradient compression methods can be used to alleviate this problem, and a recent line of work shows that SGD augmented with gradient compression converges to an $\\varepsilon$-first-order stationary point. In this paper we extend these results to convergence to an $\\varepsilon$-second-order stationary point ($\\varepsilon$-SOSP), which is to the best of our knowledge the first result of this type. In addition, we show that, when the stochastic gradient is not Lipschitz, compressed SGD with RandomK compressor converges to an $\\varepsilon$-SOSP with the same number of iterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the total communication by a factor of $\\tilde \\Theta(\\sqrt{d} \\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem. We present additional results for the cases when the compressor is arbitrary and when the stochastic gradient is Lipschitz."}}
{"id": "H-sk-z-RUr", "cdate": 1609459200000, "mdate": 1682341846706, "content": {"title": "Escaping Saddle Points with Compressed SGD", "abstract": "Stochastic gradient descent (SGD) is a prevalent optimization technique for large-scale distributed machine learning. While SGD computation can be efficiently divided between multiple machines, communication typically becomes a bottleneck in the distributed setting. Gradient compression methods can be used to alleviate this problem, and a recent line of work shows that SGD augmented with gradient compression converges to an $\\varepsilon$-first-order stationary point. In this paper we extend these results to convergence to an $\\varepsilon$-second-order stationary point ($\\varepsilon$-SOSP), which is to the best of our knowledge the first result of this type. In addition, we show that, when the stochastic gradient is not Lipschitz, compressed SGD with RandomK compressor converges to an $\\varepsilon$-SOSP with the same number of iterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the total communication by a factor of $\\tilde \\Theta(\\sqrt{d} \\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem. We present additional results for the cases when the compressor is arbitrary and when the stochastic gradient is Lipschitz."}}
{"id": "CB8mNOGHEku", "cdate": 1609459200000, "mdate": 1682341846886, "content": {"title": "Objective-Based Hierarchical Clustering of Deep Embedding Vectors", "abstract": "We initiate a comprehensive experimental study of objective-based hierarchical clustering methods on massive datasets consisting of deep embedding vectors from computer vision and NLP applications. This includes a large variety of image embedding (ImageNet, ImageNetV2, NaBirds), word embedding (Twitter, Wikipedia), and sentence embedding (SST-2) vectors from several popular recent models (e.g. ResNet, ResNext, Inception V3, SBERT). Our study includes datasets with up to 4.5 million data points with embedding dimensions up to 2048. In order to address the challenge of scaling up hierarchical clustering to such large datasets we propose a new practical hierarchical clustering algorithm B++&C. It gives a 5%/20% improvement on average for the popular Moseley-Wang (MW)/ Cohen-Addad et al. (CKMM) objectives (normalized) compared to a wide range of classic methods and recent heuristics. We also introduce a theoretical algorithm B2SAT&C which achieves a 0.74-approximation for the CKMM objective in polynomial time. This is the first substantial improvement over the trivial 2/3-approximation achieved by a random binary tree. Prior to this work, the best poly-time approximation of \u22482/3 + 0.0004 was due to Charikar et al. (SODA\u201919)"}}
{"id": "r11KoGNDL_r", "cdate": 1577836800000, "mdate": 1682341847030, "content": {"title": "Objective-Based Hierarchical Clustering of Deep Embedding Vectors", "abstract": "We initiate a comprehensive experimental study of objective-based hierarchical clustering methods on massive datasets consisting of deep embedding vectors from computer vision and NLP applications. This includes a large variety of image embedding (ImageNet, ImageNetV2, NaBirds), word embedding (Twitter, Wikipedia), and sentence embedding (SST-2) vectors from several popular recent models (e.g. ResNet, ResNext, Inception V3, SBERT). Our study includes datasets with up to $4.5$ million entries with embedding dimensions up to $2048$. In order to address the challenge of scaling up hierarchical clustering to such large datasets we propose a new practical hierarchical clustering algorithm B++&C. It gives a 5%/20% improvement on average for the popular Moseley-Wang (MW) / Cohen-Addad et al. (CKMM) objectives (normalized) compared to a wide range of classic methods and recent heuristics. We also introduce a theoretical algorithm B2SAT&C which achieves a $0.74$-approximation for the CKMM objective in polynomial time. This is the first substantial improvement over the trivial $2/3$-approximation achieved by a random binary tree. Prior to this work, the best poly-time approximation of $\\approx 2/3 + 0.0004$ was due to Charikar et al. (SODA'19)."}}
{"id": "qQxY8K13OA", "cdate": 1577836800000, "mdate": 1682318570979, "content": {"title": "Fast Fourier Sparsity Testing", "abstract": "A function is s-sparse if it has at most s non-zero Fourier coefficients. Motivated by applications to fast sparse Fourier transforms over , we study efficient algorithms for the problem of approximating the \u21132-distance from a given function to the closest s-sparse function. While previous works (e.g., Gopalan et al. SICOMP 2011) study the problem of distinguishing s-sparse functions from those that are far from s-sparse under Hamming distance, to the best of our knowledge no prior work has explicitly focused on the more general problem of distance estimation in the \u21132 setting, which is particularly well-motivated for noisy Fourier spectra. Given the focus on efficiency, our main result is an algorithm that solves this problem with query complexity (s) for constant accuracy and error parameters, which is only quadratically worse than applicable lower bounds."}}
{"id": "fJAnahpMyto", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fast Fourier Sparsity Testing", "abstract": "A function is s-sparse if it has at most s non-zero Fourier coefficients. Motivated by applications to fast sparse Fourier transforms over , we study efficient algorithms for the problem of approximating the \u21132-distance from a given function to the closest s-sparse function. While previous works (e.g., Gopalan et al. SICOMP 2011) study the problem of distinguishing s-sparse functions from those that are far from s-sparse under Hamming distance, to the best of our knowledge no prior work has explicitly focused on the more general problem of distance estimation in the \u21132 setting, which is particularly well-motivated for noisy Fourier spectra. Given the focus on efficiency, our main result is an algorithm that solves this problem with query complexity (s) for constant accuracy and error parameters, which is only quadratically worse than applicable lower bounds."}}
