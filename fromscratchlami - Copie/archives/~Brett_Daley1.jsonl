{"id": "sgTlJDB39U", "cdate": 1672531200000, "mdate": 1681491644291, "content": {"title": "Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning", "abstract": ""}}
{"id": "Hl-gfdUicx5", "cdate": 1646077546210, "mdate": null, "content": {"title": "Asymmetric DQN for Partially Observable Reinforcement Learning", "abstract": "Offline training in simulated partially observable environments allows reinforcement learning methods to exploit privileged state information through a mechanism known as asymmetry.  Such privileged information has the potential to greatly improve the optimal convergence properties, if used appropriately. However, current research in asymmetric reinforcement learning is often heuristic in nature, with few connections to underlying theory or theoretical guarantees, and is primarily tested through empirical evaluation.  In this work, we develop the theory of \\emph{asymmetric policy iteration}, an exact model-based dynamic programming solution method, and then apply relaxations which eventually result in \\emph{asymmetric DQN}, a model-free deep reinforcement learning algorithm.  Our theoretical findings are complemented and validated by empirical experimentation performed in environments which exhibit significant amounts of partial observability, and require both information gathering strategies and memorization."}}
{"id": "smOiHwT7fNt", "cdate": 1640995200000, "mdate": 1681491644302, "content": {"title": "Asymmetric DQN for partially observable reinforcement learning", "abstract": ""}}
{"id": "4WnB0i8JXB", "cdate": 1640995200000, "mdate": 1682404959087, "content": {"title": "Adaptive Tree Backup Algorithms for Temporal-Difference Reinforcement Learning", "abstract": "Q($\\sigma$) is a recently proposed temporal-difference learning method that interpolates between learning from expected backups and sampled backups. It has been shown that intermediate values for the interpolation parameter $\\sigma \\in [0,1]$ perform better in practice, and therefore it is commonly believed that $\\sigma$ functions as a bias-variance trade-off parameter to achieve these improvements. In our work, we disprove this notion, showing that the choice of $\\sigma=0$ minimizes variance without increasing bias. This indicates that $\\sigma$ must have some other effect on learning that is not fully understood. As an alternative, we hypothesize the existence of a new trade-off: larger $\\sigma$-values help overcome poor initializations of the value function, at the expense of higher statistical variance. To automatically balance these considerations, we propose Adaptive Tree Backup (ATB) methods, whose weighted backups evolve as the agent gains experience. Our experiments demonstrate that adaptive strategies can be more effective than relying on fixed or time-annealed $\\sigma$-values."}}
{"id": "KDAEc2nai83", "cdate": 1632875549992, "mdate": null, "content": {"title": "Human-Level Control without Server-Grade Hardware", "abstract": "Deep Q-Network (DQN) marked a major milestone for reinforcement learning, demonstrating for the first time that human-level control policies could be learned directly from raw visual inputs via reward maximization. Even years after its introduction, DQN remains highly relevant to the research community since many of its innovations have been adopted by successor methods. Nevertheless, despite significant hardware advances in the interim, DQN's original Atari 2600 experiments remain extremely costly to replicate in full. This poses an immense barrier to researchers who cannot afford state-of-the-art hardware or lack access to large-scale cloud computing resources. To facilitate improved access to deep reinforcement learning research, we introduce a DQN implementation that leverages a novel concurrent and synchronized execution framework designed to maximally utilize a heterogeneous CPU-GPU desktop system. With just one NVIDIA GeForce GTX 1080 GPU, our implementation reduces the training time of a 200-million-frame Atari experiment from 25 hours to just 9 hours. The ideas introduced in our paper should be generalizable to a large number of off-policy deep reinforcement learning methods."}}
{"id": "6FiiOB8v0U", "cdate": 1609459200000, "mdate": 1681491644330, "content": {"title": "Stratified Experience Replay: Correcting Multiplicity Bias in Off-Policy Reinforcement Learning", "abstract": ""}}
{"id": "36YC-Viqoc", "cdate": 1609459200000, "mdate": 1681491644341, "content": {"title": "Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning", "abstract": ""}}
{"id": "BvrKnFq_454", "cdate": 1601308229038, "mdate": null, "content": {"title": "Expectigrad: Fast Stochastic Optimization with Robust Convergence Properties", "abstract": "Many popular adaptive gradient methods such as Adam and RMSProp rely on an exponential moving average (EMA) to normalize their stepsizes. While the EMA makes these methods highly responsive to new gradient information, recent research has shown that it also causes divergence on at least one convex optimization problem. We propose a novel method called Expectigrad, which adjusts stepsizes according to a per-component unweighted mean of all historical gradients and computes a bias-corrected momentum term jointly between the numerator and denominator. We prove that Expectigrad cannot diverge on every instance of the optimization problem known to cause Adam to diverge. We also establish a regret bound in the general stochastic nonconvex setting that suggests Expectigrad is less susceptible to gradient variance than existing methods are. Testing Expectigrad on several high-dimensional machine learning tasks, we find it often performs favorably to state-of-the-art methods with little hyperparameter tuning."}}
{"id": "IhqVrtowSna", "cdate": 1577836800000, "mdate": 1667935358849, "content": {"title": "Belief-Grounded Networks for Accelerated Robot Learning under Partial Observability", "abstract": "Many important robotics problems are partially observable where a single visual or force-feedback measurement is insufficient to reconstruct the state. Standard approaches involve learning a policy..."}}
{"id": "BX7ewrgVAks", "cdate": 1546300800000, "mdate": null, "content": {"title": "Reconciling \u03bb-Returns with Experience Replay", "abstract": "Modern deep reinforcement learning methods have departed from the incremental learning required for eligibility traces, rendering the implementation of the \u03bb-return difficult in this context. In particular, off-policy methods that utilize experience replay remain problematic because their random sampling of minibatches is not conducive to the efficient calculation of \u03bb-returns. Yet replay-based methods are often the most sample efficient, and incorporating \u03bb-returns into them is a viable way to achieve new state-of-the-art performance. Towards this, we propose the first method to enable practical use of \u03bb-returns in arbitrary replay-based methods without relying on other forms of decorrelation such as asynchronous gradient updates. By promoting short sequences of past transitions into a small cache within the replay memory, adjacent \u03bb-returns can be efficiently precomputed by sharing Q-values. Computation is not wasted on experiences that are never sampled, and stored \u03bb-returns behave as stable temporal-difference (TD) targets that replace the target network. Additionally, our method grants the unique ability to observe TD errors prior to sampling; for the first time, transitions can be prioritized by their true significance rather than by a proxy to it. Furthermore, we propose the novel use of the TD error to dynamically select \u03bb-values that facilitate faster learning. We show that these innovations can enhance the performance of DQN when playing Atari 2600 games, even under partial observability. While our work specifically focuses on \u03bb-returns, these ideas are applicable to any multi-step return estimator."}}
