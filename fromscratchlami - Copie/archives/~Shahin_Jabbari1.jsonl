{"id": "Sd5N8-L8Kp", "cdate": 1683912643716, "mdate": 1683912643716, "content": {"title": "Robust Lock-Down Optimization for COVID-19 Policy Guidance", "abstract": "As the COVID-19 outbreak continues to pose a serious worldwide threat, numerous governments choose to establish lockdowns in order to reduce disease transmission. However, imposing the strictest possible lock-down at all times has dire\neconomic consequences, especially in areas with widespread\npoverty. In fact, many countries and regions have started\ncharting paths to ease lock-down measures. Thus, planning\nefficient ways to tighten and relax lock-downs is a crucial and\nurgent problem. We develop a reinforcement learning based\napproach that is (1) robust to a range of parameter settings,\nand (2) optimizes multiple objectives related to different aspects of public health and economy, such as hospital capacity\nand delay of the disease. The absence of a vaccine or a cure\nfor COVID to date implies that the infected population cannot\nbe reduced through pharmaceutical interventions. However,\nnon-pharmaceutical interventions (lock-downs) can slow disease spread and keep it manageable. This work focuses on\nhow to manage the disease spread without severe economic\nconsequences."}}
{"id": "JFRvkzm7kgO", "cdate": 1672531200000, "mdate": 1681674176452, "content": {"title": "Improving Fairness in Adaptive Social Exergames via Shapley Bandits", "abstract": "Algorithmic fairness is an essential requirement as AI becomes integrated in society. In the case of social applications where AI distributes resources, algorithms often must make decisions that will benefit a subset of users, sometimes repeatedly or exclusively, while attempting to maximize specific outcomes. How should we design such systems to serve users more fairly? This paper explores this question in the case where a group of users works toward a shared goal in a social exergame called Step Heroes. We identify adverse outcomes in traditional multi-armed bandits (MABs) and formalize the Greedy Bandit Problem. We then propose a solution based on a new type of fairness-aware multi-armed bandit, Shapley Bandits. It uses the Shapley Value for increasing overall player participation and intervention adherence rather than the maximization of total group output, which is traditionally achieved by favoring only high-performing participants. We evaluate our approach via a user study (n=46). Our results indicate that our Shapley Bandits effectively mediates the Greedy Bandit Problem and achieves better user retention and motivation across the participants."}}
{"id": "rbg_o51Tl9", "cdate": 1646226079594, "mdate": null, "content": {"title": "Solving Structured Hierarchical Games Using Differential Backward Induction", "abstract": "From large-scale organizations to decentralized political systems, hierarchical strategic decision making is commonplace. We introduce a novel class of structured hierarchical games (SHGs) that formally capture such hierarchical strategic interactions. In an SHG, each player is a node in a tree, and strategic choices of players are sequenced from root to leaves, with root moving first, followed by its children, then followed by their children, and so on until the leaves. A player\u2019s utility in an SHG depends on its own decision, and on the choices of its parent and all the tree leaves. SHGs thus generalize simultaneous-move games, as well as Stackelberg games with many followers. We leverage the structure of both the sequence of player moves as well as payoff dependence to develop a novel gradient-based backpropagation-style algorithm, which we call Differential Backward Induction (DBI), for approximating equilibria of SHGs. We provide a sufficient condition for convergence of DBI and demonstrate its efficacy in finding approximate equilibrium solutions to several SHG models of hierarchical policy-making problems."}}
{"id": "BcLqJUIs5x5", "cdate": 1646077511461, "mdate": null, "content": {"title": "Solving Structured Hierarchical Games Using Differential Backward Induction", "abstract": "From large-scale organizations to decentralized political systems, hierarchical strategic decision making is commonplace. We introduce a novel class of \\emph{structured hierarchical games (SHGs)} that formally capture such hierarchical strategic interactions. In an SHG, each player is a node in a tree, and strategic choices of players are sequenced from root to leaves, with root moving first, followed by its children, then followed by their children, and so on until the leaves. A player's utility in an SHG depends on its own decision, and on the choices of its parent and \\emph{all} the tree leaves. SHGs thus generalize simultaneous-move games, as well as Stackelberg games with many followers.  We leverage the structure of both the sequence of player moves as well as payoff dependence to develop a gradient-based back propagation-style algorithm, which we call \\emph{Differential Backward Induction (DBI)}, for approximating equilibria of SHGs. We provide a sufficient condition for convergence of DBI and demonstrate its efficacy in finding approximate equilibrium solutions to several SHG models of hierarchical policy-making problems."}}
{"id": "s-lv0Jxbsq", "cdate": 1640995200000, "mdate": 1681674308458, "content": {"title": "Solving structured hierarchical games using differential backward induction", "abstract": "From large-scale organizations to decentralized political systems, hierarchical strategic decision making is commonplace. We introduce a novel class of structured hierarchical games (SHGs) that for..."}}
{"id": "Yh4Rf_z8oY", "cdate": 1640995200000, "mdate": 1681674308458, "content": {"title": "TorchFL: A Performant Library for Bootstrapping Federated Learning Experiments", "abstract": "With the increased legislation around data privacy, federated learning (FL) has emerged as a promising technique that allows the clients (end-user) to collaboratively train deep learning (DL) models without transferring and storing the data in a centralized, third-party server. We introduce TorchFL, a performant library for (i) bootstrapping the FL experiments, (ii) executing them using various hardware accelerators, (iii) profiling the performance, and (iv) logging the overall and agent-specific results on the go. Being built on a bottom-up design using PyTorch and Lightning, TorchFL provides ready-to-use abstractions for models, datasets, and FL algorithms, while allowing the developers to customize them as and when required. This paper aims to dig deeper into the architecture and design of TorchFL, elaborate on how it allows researchers to bootstrap the federated learning experience, and provide experiments and code snippets for the same. With the ready-to-use implementation of state-of-the-art DL models, datasets, and federated learning support, TorchFL aims to allow researchers with little to no engineering background to set up FL experiments with minimal coding and infrastructure overhead."}}
{"id": "QDhC3f1oaI", "cdate": 1640995200000, "mdate": 1681674176493, "content": {"title": "Designing effective masking strategies for cyberdefense through human experimentation and cognitive models", "abstract": ""}}
{"id": "7hVYtQMk31", "cdate": 1640995200000, "mdate": 1681674176454, "content": {"title": "The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective", "abstract": "As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations."}}
{"id": "twiQxN678TV", "cdate": 1609459200000, "mdate": null, "content": {"title": "Active Screening for Recurrent Diseases: A Reinforcement Learning Approach", "abstract": "Active screening is a common approach in controlling the spread of recurring infectious diseases such as tuberculosis and influenza. In this approach, health workers periodically select a subset of population for screening. However, given the limited number of health workers, only a small subset of the population can be visited in any given time period. Given the recurrent nature of the disease and rapid spreading, the goal is to minimize the number of infections over a long time horizon. Active screening can be formalized as a sequential combinatorial optimization over the network of people and their connections. The main computational challenges in this formalization arise from i) the combinatorial nature of the problem, ii) the need of sequential planning and iii) the uncertainties in the infectiousness states of the population. Previous works on active screening fail to scale to large time horizon while fully considering the future effect of current interventions. In this paper, we propose a novel reinforcement learning (RL) approach based on Deep Q-Networks (DQN), with several innovative adaptations that are designed to address the above challenges. First, we use graph convolutional networks (GCNs) to represent the Q-function that exploit the node correlations of the underlying contact network. Second, to avoid solving a combinatorial optimization problem in each time period, we decompose the node set selection as a sub-sequence of decisions, and further design a two-level RL framework that solves the problem in a hierarchical way. Finally, to speed-up the slow convergence of RL which arises from reward sparseness, we incorporate ideas from curriculum learning into our hierarchical RL approach. We evaluate our RL algorithm on several real-world networks."}}
{"id": "hrVPcThWJU", "cdate": 1609459200000, "mdate": 1681674176453, "content": {"title": "Active Screening for Recurrent Diseases: A Reinforcement Learning Approach", "abstract": ""}}
