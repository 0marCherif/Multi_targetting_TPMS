{"id": "ZTPzwWtKW7o", "cdate": 1663850225063, "mdate": null, "content": {"title": "Evaluating Robustness of Generative Models with Adversarial Networks", "abstract": "With the advent of adversarial robustness as a research area, much novel work attempts to design creative defense mechanisms against adversarial vulnerabilities that arise. While classification models are the most common target of adversarial robustness research, generative models are often underestimated though they play essential roles in many applications. This work evaluates generative models for reconstruction tasks in terms of their adversarial robustness. We constructed two frameworks: a standard and a universal-attack framework. The standard framework requires an input to find its perturbation, and the universal-attack framework generates adversarial perturbation from the distribution of a dataset. Extensive experimental evidence discussed in this paper suggests that both frameworks can effectively alter how images are reconstructed and classified using classic generative models trained on MNIST and Cropped Yale Face datasets. Further, these frameworks outperform state-of-the-art adversarial attacks. Moreover, we showcase using the proposed framework to retrain a generative model to improve its resilience against adversarial perturbations. Furthermore, for the sake of generative models, an attack may desire not to alter the latent space. Thus, we also include the analysis of the latent space."}}
{"id": "RDv1fGXGoiQ", "cdate": 1640995200000, "mdate": 1682573879241, "content": {"title": "Evaluation of adversarial attacks sensitivity of classifiers with occluded input data", "abstract": "With the noteworthy achievements of deep learning models, there are transformative applications that aim at cost reduction and the improvement in human quality of life. Nevertheless, recent work aimed at testing a classifier\u2019s ability to withstand targeted and black-box adversarial attacks demonstrated that deep learning models, in particular, are brittle and lack certain robustness that makes them particularly weak, and ultimately leading to a lack of trust. For this specific area, a question arises concerning certain regions\u2019 sensitivity in the input space against adversarial perturbations for a classification model. This paper aims to study such a problem by looking into a Sensitivity-inspired Constrained Evaluation Method (SICEM) to deterministically evaluate how much a region of the input space is vulnerable to adversarial perturbations compared to other regions and also the entire input space. Our experiments suggest that SICEM can accurately quantify region vulnerabilities on MNIST and CIFAR-10 datasets."}}
{"id": "AQcNvPhMcL", "cdate": 1640995200000, "mdate": 1682573879238, "content": {"title": "Monolith to Microservices: VAE-Based GNN Approach with Duplication Consideration", "abstract": "With the rise of cloud computing, many applications have been implemented into microservices to fully utilize cloud computing for scalability and maintainability purposes. However, there are some traditional monolith applications that developers would like to partition into microservices. Unfortunately, it is difficult to find a solution when considering multiple factors (i.e., the strong dependency in each cluster and how often different microservices communicate with each other). Further, because we allow duplications of classes in multiple microservices to reduce the communications between them, the number of duplicated classes is also another important factor for maintainability. Therefore, we need to use machine learning algorithms to approximate a good solution due to the infeasibility of finding the optimal solution. We apply the variational autoencoder to extract features of classes and use the fuzzy c means to group the classes into microservices according to their extracted features. As a result, our approach outperforms the other baselines in some significant metrics. Also, when we allow duplication, we find that it is helpful in terms of reducing the overhead of communications between microservices."}}
{"id": "YRqeSCmyRj", "cdate": 1609459200000, "mdate": 1682573879237, "content": {"title": "An Adversarial Neural Cryptography Approach to Integrity Checking: Learning to Secure Data Communications", "abstract": "Securing communications is an increasingly challenging problem. While communication channels can be secured using strong ciphers, attackers who gain access to the channel can still perform certain types of attacks. One way to mitigate such attacks is to verify the integrity of exchanging messages between two parties or more. While there are robust integrity check mechanisms currently, these lack variety and very few are based on machine learning. This paper presents a methodology for performing an integrity check inspired by recent advances in neural cryptography. We provide formal, mathematical functions and an optimization problem for training an adversarial neural cryptography architecture. The proposed neural architectures can adequately solve the problem. In our experiments, a receiver can verify if incoming messages are authentic or altered with an accuracy greater than 99%. This work expands the repertoire of integrity checking methodologies, provides a unique perspective based on neural networks, and facilitates data security and privacy."}}
{"id": "UWpkKpB39xB", "cdate": 1609459200000, "mdate": 1682573879241, "content": {"title": "Enhancing Adversarial Examples on Deep Q Networks with Previous Information", "abstract": "Reinforcement learning has been widely used in many applications (e.g., self-driving cars, games, and robots). However, it is not very efficient when there are many actions and states. Thus, many researchers have applied Deep learning to it and call it Deep reinforcement learning (DRL) to address such a drawback. Unfortunately, recently, some researchers discovered a weakness of Deep learning in the test time and called it an adversarial attack. Since DRL is also Deep learning, it has the same weakness. Hence, DRL is vulnerable to adversarial attacks. Moreover, some existing works have created adversarial attacks for DRL. Generally, they first decide whether to create an adversarial example for the current state and then determine how much perturbation to add to the current state. This limits the adversary from the information in the previous steps when determining the perturbation. Also, some of the attacks fixed the adversarial action to the worst action, and then, the behavior of the target agent did not look natural. Therefore, we propose combining those two problems into one problem to allow the adversary to receive the information from the previous steps and let the adversary pick an adversarial action that is worth adding the perturbation by formulating and optimizing the problem. At last, we construct the experiment on Atari games to investigate the behavior of the agent attacked by our approach and compare our approach to the state-of-the-art attacks in terms of the amount of added perturbation and reward. As a result, our approach can make an agent play the games as if there was no adversarial attack and outperform the previous works."}}
{"id": "wQ1u0i-onJF", "cdate": 1546300800000, "mdate": 1682573879243, "content": {"title": "Fairness-Aware Auction Mechanism for Sustainable Mobile Crowdsensing", "abstract": "With the proliferation of sensor-embedded mobile devices, mobile crowdsensing has become a paradigm of significant interest. Incentivizing sensory-data providers to keep sustainability in a mobile crowdsensing system is a critical issue nowadays, and auction-based mechanisms have been proposed to motivate providers via monetary rewards. In our work, this sustainability problem is formulated as an optimization problem maximizing providers\u2019 proportionally fair utilities with respect to their multi-dimensional fairness factors, and a fairness-aware auction mechanism is designed accordingly. To the best of our knowledge, this is the first work that considers multi-dimensional fairness of providers as the objective in selecting providers for the mobile crowdsensing system. In addition, we present rigorous theoretical analysis proving that our mechanism meets budget feasibility, individual rationality and truthfulness. Finally, simulations are performed to demonstrate the performance of our proposed mechanism."}}
{"id": "tBKZj6PY-p", "cdate": 1514764800000, "mdate": 1682573879240, "content": {"title": "Solving Data Trading Dilemma with Asymmetric Incomplete Information Using Zero-Determinant Strategy", "abstract": "Trading data between user and service provider is a promising and efficient method to promote information exchange, service quality improvement, and development of emerging applications, benefiting individual and society. Meanwhile, data resale (i.e., data secondary use) is one of the most critical privacy issues hindering the ongoing process of data trading, which, unfortunately, is ignored in many of the existing privacy-preserving schemes. In this paper, we tackle the issue of data resale from a special angle, i.e., promoting cooperation between user and service provider to prevent data secondary use. For this purpose, we design a novel game-theoretical algorithm, in which user can unilaterally persuade service provider to cooperate in data trading, achieving a \u201cwin-win\u201d situation. Besides, we validate our proposed algorithm performance through in-depth theoretical analysis and comprehensive simulations."}}
