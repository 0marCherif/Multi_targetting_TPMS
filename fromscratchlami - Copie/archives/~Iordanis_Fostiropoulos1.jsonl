{"id": "skvqz58ys1U", "cdate": 1683834494545, "mdate": null, "content": {"title": "Probing Reasoning of Language Models with Inductive In-Context Learning", "abstract": "Despite their recent success, Language Models (LMs) have brought to question whether they statistically repeat data (\u2018stochastic parrots\u2019)[Bender et al., 2021] or can learn the underlying generative process of the data. Current benchmark used to probe the reasoning ability of LMs can be in-applicable in this setting as they are curated by human annotators and the generative process can be ambiguous. Additionally, current work to probe reasoning ability of LMs introduce the same bias as the question they attempt to answer; that is whether the model has learn the statistics of the benchmark data. In this work we introduce a novel evaluation setting that we use with Inductive In-Context Learning (IIL) and a dataset, ReAnalogy, to probe the reasoning ability of LMs. ReAnalogy consists of sequences with positive examples, generated from regular expressions (regex) and contain quasi-natural language. We use regex to evaluate implicitly whether a LM can infer \u2018Rules\u2019 (regex) given limited set of examples (\u2018Facts\u2019). We use the LM to generate additional Facts to evaluate whether the generated Facts abide by the Rules. We evaluate a GPT model in our setting and compare with the same model where a Rule is injected during training to replace a Fact. We use IIL during evaluation to probe the model to infer the Rule given Facts. We then use the inferred Rule to synthesize an additional Fact. IIL improve \u2018reasoning\u2019 performance by as much as 33%. Our results suggest that LMs can learn more than statistical patterns in the data and we support our findings with ablation studies. We evaluate our dataset with existing benchmark and baselines in inductive programming and find that current state-of-the-art symbolic or neuro-symbolic approaches fail to the complexity of our dataset; while the existing dataset and benchmark in the do main are inapplicable for LMs. Our probing method and dataset are complex enough for LMs and applicable for evaluating the inductive reasoning abilities of LMs, while IIL can improve \u2018reasoning\u2019 of LMs. We open-source our dataset and the code used for our experiments."}}
{"id": "eBLV3i7PG1c", "cdate": 1681432486630, "mdate": null, "content": {"title": "ABLATOR: Robust Horizontal-Scaling of Machine Learning Ablation Experiments", "abstract": "Understanding the efficacy of a method requires ablation experiments. Current Machine Learning (ML) workflows emphasize the vertical scaling of large models with paradigms such as \u2018data-parallelism\u2019 or \u2018model-parallelism\u2019. As a consequence, there is a lack of methods for horizontal scaling of multiple experimental trials. Horizontal scaling is labor intensive when different tools are used for different experiment stages, such as for hyper-parameter optimization, distributed execution, or the consolidation of artifacts. We identify that errors in earlier stages of experimentation propagate to the analysis. Based on our observations, experimental results, and the current literature, we provide recommendations on best practices to prevent errors. To reduce the effort required to perform an accurate analysis and address common errors when scaling the execution of multiple experiments, we introduce ABLATOR. Our framework uses a stateful experiment design paradigm that provides experiment persistence and is robust to errors. Our actionable analysis artifacts are automatically produced by the experiment state and reduce the time to evaluate a hypothesis. We evaluate ABLATOR with ablation studies on a Transformer model, \u2018Tablator\u2019, where we study the effect of 6 architectural components, 8 model hyperparameters, 3 training hyperparameters, and 4 dataset preprocessing methodologies on 11 tabular datasets. We performed the largest ablation experiment for tabular data on Transformer models to date, evaluating 2,337 models in total. Finally, we open source ABLATOR; https://github.com/fostiropoulos/ablator"}}
{"id": "_JXT98mOmR", "cdate": 1680732799082, "mdate": null, "content": {"title": "Trustworthy model evaluation on a budget", "abstract": "Standard practice in Machine Learning (ML) research uses ablation studies to evaluate a novel method. We find that errors in the ablation setup can lead to incorrect explanations for which method components contribute to the performance. Previous work has shown that the majority of experiments published in top conferences are performed with few experimental trials (less than 50) and manual sampling of hyperparameters. Using the insights from our meta-analysis, we demonstrate how current practices can lead to unreliable conclusions. We simulate an ablation study experiment on an existing Neural Architecture Search (NAS) benchmark and perform an ablation study with 120 trials using ResNet50. We quantify the selection bias of Hyperparameter Optimization (HPO) strategies to show that only random sampling can produce reliable results when determining the top and mean performance of a method under a limited computational budget. \n"}}
{"id": "gjgs9MBNWmG", "cdate": 1672531200000, "mdate": 1700069428814, "content": {"title": "Lightweight Learner for Shared Knowledge Lifelong Learning", "abstract": "In Lifelong Learning (LL), agents continually learn as they encounter new conditions and tasks. Most current LL is limited to a single agent that learns tasks sequentially. Dedicated LL machinery is then deployed to mitigate the forgetting of old tasks as new tasks are learned. This is inherently slow. We propose a new Shared Knowledge Lifelong Learning (SKILL) challenge, which deploys a decentralized population of LL agents that each sequentially learn different tasks, with all agents operating independently and in parallel. After learning their respective tasks, agents share and consolidate their knowledge over a decentralized communication network, so that, in the end, all agents can master all tasks. We present one solution to SKILL which uses Lightweight Lifelong Learning (LLL) agents, where the goal is to facilitate efficient sharing by minimizing the fraction of the agent that is specialized for any given task. Each LLL agent thus consists of a common task-agnostic immutable part, where most parameters are, and individual task-specific modules that contain fewer parameters but are adapted to each task. Agents share their task-specific modules, plus summary information (\"task anchors\") representing their tasks in the common task-agnostic latent space of all agents. Receiving agents register each received task-specific module using the corresponding anchor. Thus, every agent improves its ability to solve new tasks each time new task-specific modules and anchors are received. On a new, very challenging SKILL-102 dataset with 102 image classification tasks (5,033 classes in total, 2,041,225 training, 243,464 validation, and 243,464 test images), we achieve much higher (and SOTA) accuracy over 8 LL baselines, while also achieving near perfect parallelization. Code and data can be found at https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning"}}
{"id": "arCqtYf2m7", "cdate": 1672531200000, "mdate": 1700069428824, "content": {"title": "Batch Model Consolidation: A Multi-Task Model Consolidation Framework", "abstract": "In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation ($\\textbf{BMC}$) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a $\\textit{regularization}$ phase, BMC trains multiple $\\textit{expert models}$ in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a $\\textit{base model}$ through a $\\textit{stability loss}$, and constructs a $\\textit{buffer}$ from a fraction of the task's data. During the $\\textit{consolidation}$ phase, we combine the learned knowledge on 'batches' of $\\textit{expert models}$ using a $\\textit{batched consolidation loss}$ in $\\textit{memory}$ data that aggregates all buffers. We thoroughly evaluate each component of our method in an ablation study and demonstrate the effectiveness on standardized benchmark datasets Split-CIFAR-100, Tiny-ImageNet, and the Stream dataset composed of 71 image classification tasks from diverse domains and difficulties. Our method outperforms the next best CL approach by 70% and is the only approach that can maintain performance at the end of 71 tasks; Our benchmark can be accessed at https://github.com/fostiropoulos/stream_benchmark"}}
{"id": "Ye-L-3JblR", "cdate": 1672531200000, "mdate": 1700069428835, "content": {"title": "Reproducibility Requires Consolidated Artifacts", "abstract": "Machine learning is facing a \u2018reproducibility crisis\u2019 where a significant number of works report failures when attempting to reproduce previously published results. We evaluate the sources of reproducibility failures using a meta-analysis of 142 replication studies from ReScience C and 204 code repositories. We find that missing experiment details such as hyperparameters are potential causes of unreproducibility. We experimentally show the bias of different hyperparameter selection strategies and conclude that consolidated artifacts with a unified framework can help support reproducibility."}}
{"id": "D4_lgYkFAGJ", "cdate": 1672531200000, "mdate": 1700069428889, "content": {"title": "Reproducibility Requires Consolidated Artifacts", "abstract": "Machine learning is facing a 'reproducibility crisis' where a significant number of works report failures when attempting to reproduce previously published results. We evaluate the sources of reproducibility failures using a meta-analysis of 142 replication studies from ReScience C and 204 code repositories. We find that missing experiment details such as hyperparameters are potential causes of unreproducibility. We experimentally show the bias of different hyperparameter selection strategies and conclude that consolidated artifacts with a unified framework can help support reproducibility."}}
{"id": "7ZaJfk915b1", "cdate": 1663849842872, "mdate": null, "content": {"title": "Shared Knowledge Lifelong Learning", "abstract": "In Lifelong Learning (LL), agents continually learn as they encounter new conditions and tasks. Most current LL is limited to a single agent that learns tasks sequentially. Dedicated LL machinery is then deployed to mitigate the forgetting of old tasks as new tasks are learned. This is inherently slow. We propose a new Shared Knowledge Lifelong Learning (SKILL) learning paradigm, which deploys a population of LL agents that each learn different tasks independently and in parallel. After learning their respective tasks, agents share and consolidate their knowledge over a communication network, so that, in the end, all agents can master all tasks. Our approach relies on a frozen backbone embedded in all agents at manufacturing time, so that only the last layer head plus some small adjustments to the backbone beneficial biases are learned for each task. To eliminate the need for a task oracle, agents also learn and share summary statistics about their training datasets (Gaussian Mixture Clusters), or share a few training images, to help other agents assign test samples to the correct head using a Mahalanobis task mapper. On a new, very challenging dataset with 102 image classification tasks, we achieve significant speedup over 18 LL baselines (e.g., >9,000x speedup over single-agent EWC) while also achieving higher (and SOTA) accuracy."}}
{"id": "d04jGB7795L", "cdate": 1640995200000, "mdate": 1668625067336, "content": {"title": "Implicit Feature Decoupling with Depthwise Quantization", "abstract": "Quantization has been applied to multiple domains in Deep Neural Networks (DNNs). We propose Depthwise Quantization (DQ) where $\\textit{quantization}$ is applied to a decomposed sub-tensor along the $\\textit{feature axis}$ of weak statistical dependence. The feature decomposition leads to an exponential increase in $\\textit{representation capacity}$ with a linear increase in memory and parameter cost. In addition, DQ can be directly applied to existing encoder-decoder frameworks without modification of the DNN architecture. We use DQ in the context of Hierarchical Auto-Encoder and train end-to-end on an image feature representation. We provide an analysis on cross-correlation between spatial and channel features and we propose a decomposition of the image feature representation along the channel axis. The improved performance of the depthwise operator is due to the increased representation capacity from implicit feature decoupling. We evaluate DQ on the likelihood estimation task, where it outperforms the previous state-of-the-art on CIFAR-10, ImageNet-32 and ImageNet-64. We progressively train with increasing image size a single hierarchical model that uses 69% less parameters and has a faster convergence than the previous works."}}
{"id": "MzQ8jzoXBOE", "cdate": 1640995200000, "mdate": 1668625067335, "content": {"title": "Implicit Feature Decoupling with Depthwise Quantization", "abstract": "Quantization has been applied to multiple domains in Deep Neural Networks (DNNs). We propose Depthwise Quantization (DQ) where quantization is applied to a de-composed sub-tensor along the feature axis of weak statis-tical dependence. The feature decomposition leads to an exponential increase in representation capacity with a linear increase in memory and parameter cost. In addition, DQ can be directly applied to existing encoder-decoder frame-works without modification of the DNN architecture. We use DQ in the context of Hierarchical Auto-Encoders and train end-to-end on an image feature representation. We provide an analysis of the cross-correlation between spatial and channel features and propose a decomposition of the image feature representation along the channel axis. The improved performance of the depthwise operator is due to the increased representation capacity from implicit feature decoupling. We evaluate DQ on the likelihood estimation task, where it outperforms the previous state-of-the-art on CIFAR-10, ImageNet-32 and ImageNet-64. We progressively train with increasing image size a single hierarchical model that uses 69% fewer parameters and has faster convergence than the previous work."}}
