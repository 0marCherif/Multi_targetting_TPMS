{"id": "wcEfO-alqTd", "cdate": 1672531200000, "mdate": 1681768141787, "content": {"title": "Faster high-accuracy log-concave sampling via algorithmic warm starts", "abstract": "Understanding the complexity of sampling from a strongly log-concave and log-smooth distribution $\\pi$ on $\\mathbb{R}^d$ to high accuracy is a fundamental problem, both from a practical and theoretical standpoint. In practice, high-accuracy samplers such as the classical Metropolis-adjusted Langevin algorithm (MALA) remain the de facto gold standard; and in theory, via the proximal sampler reduction, it is understood that such samplers are key for sampling even beyond log-concavity (in particular, for distributions satisfying isoperimetric assumptions). In this work, we improve the dimension dependence of this sampling problem to $\\tilde{O}(d^{1/2})$, whereas the previous best result for MALA was $\\tilde{O}(d)$. This closes the long line of work on the complexity of MALA, and moreover leads to state-of-the-art guarantees for high-accuracy sampling under strong log-concavity and beyond (thanks to the aforementioned reduction). Our starting point is that the complexity of MALA improves to $\\tilde{O}(d^{1/2})$, but only under a warm start (an initialization with constant R\\'enyi divergence w.r.t. $\\pi$). Previous algorithms took much longer to find a warm start than to use it, and closing this gap has remained an important open problem in the field. Our main technical contribution settles this problem by establishing the first $\\tilde{O}(d^{1/2})$ R\\'enyi mixing rates for the discretized underdamped Langevin diffusion. For this, we develop new differential-privacy-inspired techniques based on R\\'enyi divergences with Orlicz--Wasserstein shifts, which allow us to sidestep longstanding challenges for proving fast convergence of hypocoercive differential equations."}}
{"id": "uVQPrO3dle", "cdate": 1672531200000, "mdate": 1681768142579, "content": {"title": "Fisher information lower bounds for sampling", "abstract": "We prove two lower bounds for the complexity of non-log-concave sampling within the framework of Balasubramanian et al. (2022), who introduced the use of Fisher information ($\\mathsf{FI}$) bounds as a notion of approximate first-order stationarity in sampling. Our first lower bound shows that averaged Langevin Monte Carlo (LMC) is optimal for the regime of large $\\mathsf{FI}$ by reducing the problem of finding stationary points in non-convex optimization to sampling. Our second lower bound shows that in the regime of small $\\mathsf{FI}$, obtaining a $\\mathsf{FI}$ of at most $\\varepsilon^2$ from the target distribution requires $\\text{poly}(1/\\varepsilon)$ queries, which is surprising as it rules out the existence of high-accuracy algorithms (e.g., algorithms using Metropolis{\u2013}Hastings filters) in this context."}}
{"id": "gj6p9B7tHa", "cdate": 1672531200000, "mdate": 1681768142007, "content": {"title": "On the complexity of finding stationary points of smooth functions in one dimension", "abstract": "We characterize the query complexity of finding stationary points of one-dimensional non-convex but smooth functions. We consider four settings, based on whether the algorithms under consideration ..."}}
{"id": "kmqSo4JYYfW", "cdate": 1664310937972, "mdate": null, "content": {"title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions", "abstract": "We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL-E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs."}}
{"id": "zyLVMgsZ0U_", "cdate": 1663850410084, "mdate": null, "content": {"title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions", "abstract": "We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs."}}
{"id": "K2PTuvVTF1L", "cdate": 1652737360691, "mdate": null, "content": {"title": "Variational inference via Wasserstein gradient flows", "abstract": "Along with Markov chain Monte Carlo (MCMC) methods, variational inference (VI) has emerged as a central computational approach to large-scale Bayesian inference. Rather than sampling from the true posterior $\\pi$, VI aims at producing a simple but effective approximation $\\hat \\pi$ to $\\pi$ for which summary statistics are easy to compute. However, unlike the well-studied MCMC methodology, algorithmic guarantees for VI are still relatively less well-understood. In this work, we propose principled methods for VI, in which $\\hat \\pi$ is taken to be a Gaussian or a mixture of Gaussians, which rest upon the theory of gradient flows on the Bures--Wasserstein space of Gaussian measures. Akin to MCMC, it comes with strong theoretical guarantees when $\\pi$ is log-concave."}}
{"id": "SnEez8-mrl5", "cdate": 1645715785876, "mdate": 1645715785876, "content": {"title": "Analysis of Langevin Monte Carlo from Poincar\u00e9 to Log-Sobolev", "abstract": "Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution \u03c0 under the sole assumption that \u03c0 satisfies a Poincar\u00e9 inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or R\u00e9nyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that \u03c0 satisfies either a Lata\u0142a--Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincar\u00e9 and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions."}}
{"id": "tph6cHHOjC", "cdate": 1640995200000, "mdate": 1681768142622, "content": {"title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions", "abstract": "We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does not reduce the complexity of SGMs."}}
{"id": "nyw5x1Ht4ru", "cdate": 1640995200000, "mdate": 1681768142019, "content": {"title": "The query complexity of sampling from strongly log-concave distributions in one dimension", "abstract": "We establish the first tight lower bound of $\\Omega(\\log\\log\\kappa)$ on the query complexity of sampling from the class of strongly log-concave and log-smooth distributions with condition number $\\..."}}
{"id": "l_uEPrXqdxL", "cdate": 1640995200000, "mdate": 1681768142279, "content": {"title": "Towards a Theory of Non-Log-Concave Sampling: First-Order Stationarity Guarantees for Langevin Monte Carlo", "abstract": "For the task of sampling from a density $\\pi \\propto \\exp(-V)$ on $\\R^d$, where $V$ is possibly non-convex but $L$-gradient Lipschitz, we prove that averaged Langevin Monte Carlo outputs a sample w..."}}
