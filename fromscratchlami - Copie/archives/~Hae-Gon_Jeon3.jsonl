{"id": "bOJ4TVetdR5", "cdate": 1699182736930, "mdate": 1699182736930, "content": {"title": "High-fidelity 3D Human Digitization from Single 2K Resolution Images", "abstract": "High-quality 3D human body reconstruction requires high-fidelity and large-scale training data and appropriate network design that effectively exploits the high-resolution input images. To tackle these problems, we propose a simple yet effective 3D human digitization method called 2K2K, which constructs a large-scale 2K human dataset and infers 3D human models from 2K resolution images. The proposed method separately recovers the global shape of a human and its details. The low-resolution depth network predicts the global structure from a low-resolution image, and the part-wise image-to-normal network predicts the details of the 3D human body structure. The high-resolution depth network merges the global 3D shape and the detailed structures to infer the high-resolution front and back side depth maps. Finally, an off-the-shelf mesh generator reconstructs the full 3D human model, which are available at this https URL. In addition, we also provide 2,050 3D human models, including texture maps, 3D joints, and SMPL parameters for research purposes. In experiments, we demonstrate competitive performance over the recent works on various datasets."}}
{"id": "4N8076xCj2", "cdate": 1698634523568, "mdate": 1698634523568, "content": {"title": "Learning Depth from Focus in the Wild", "abstract": "For better photography, most recent commercial cameras including smartphones have either adopted large-aperture lens to collect more light or used a burst mode to take multiple images within short times. These interesting features lead us to examine depth from\nfocus/defocus. In this work, we present a convolutional neural networkbased depth estimation from single focal stacks. Our method differs\nfrom relevant state-of-the-art works with three unique features. First, our method allows depth maps to be inferred in an end-to-end manner even with image alignment. Second, we propose a sharp region detection module to reduce blur ambiguities in subtle focus changes and weakly texture-less regions. Third, we design an effective downsampling module to ease flows of focal information in feature extractions. In addition, for the generalization of the proposed network, we develop a simulator to realistically reproduce the features of commercial cameras, such as changes in field of view, focal length and principal points. By effectively incorporating these three unique features, our network achieves the top rank in the DDFF 12-Scene benchmark on most metrics. We also demonstrate the effectiveness of the proposed method on various quantitative evaluations and real-world images taken from various off-the-shelf cameras compared with state-of-the-art methods. Our source code is publicly available at https://github.com/wcy199705/DfFintheWild."}}
{"id": "blWduLrBH", "cdate": 1698574221060, "mdate": null, "content": {"title": "DeepGT: Deep learning-based quantification of nanosized bioparticles in bright-field micrographs of Gires-Tournois biosensor", "abstract": "Rapid and decentralized quantification of viral load profiles in infected patients is vital for assessing clinical severity and tailoring appropriate therapeutic strategies. Although microscopic imaging offers potential for label-free and amplification-free quantitative diagnostics, the small size (~100 nm in diameter) and low refractive index (n ~1.5) of bioparticles present challenges in achieving accurate estimations, consequently increasing the limit of detection (LoD). In this study, we present a novel synergistic biosensing approach, DeepGT, combining Gires-Tournois (GT) sensing platforms with deep learning algorithms to enhance nanoscale bioparticle counting accuracy. The GT sensing platform serves as a photonic resonator, increasing bioparticle visibility in bright-field microscopy and maximizing chromatic contrast. By employing a back-end with a dilated convolutional neural network architecture, DeepGT effectively refines artifacts and color deviations, significantly improving particle estimation accuracy (MAE ~2.37 across 1596 images) compared to rule-based algorithms (MAE ~ 13.47). Notably, the enhanced accuracy in detecting invisible particles (e.g., two- or three-particles) enables an LoD of 138 pg ml\u2212 1 , facilitating a dynamic linear correlation at low viral concentration ranges within the clinical spectrum of infection, from asymptomatic to severe cases. Leveraging transfer learning, DeepGT, which relies on a chromatometry-based strategy instead of a spatial resolution approach, exhibits exceptional precision when analyzing particles of diverse dimensions smaller than the microscopy system\u2019s minimum diffraction limit in visible light (< 258 nm). The DeepGT approach holds promise for early screening and triage of emerging viruses, reducing costs and time requirements in diagnostics."}}
{"id": "y7uT9PgncV", "cdate": 1668518493496, "mdate": 1668518493496, "content": {"title": "Learning Pedestrian Group Representations for Multi-modal Trajectory Prediction ", "abstract": "Modeling the dynamics of people walking is a problem of long-standing interest in computer vision. Many previous works involving pedestrian trajectory prediction define a particular set of individual actions to implicitly model group actions. In this paper, we present a novel architecture named GP-Graph which has collective group representations for effective pedestrian trajectory prediction in crowded environments, and is compatible with all types of existing approaches. A key idea of GP-Graph is to model both individual-wise and group-wise relations as graph representations. To do this, GP-Graph first learns to assign each pedestrian into the most likely behavior group. Using this assignment information, GP-Graph then forms both intra- and inter-group interactions as graphs, accounting for human-human relations within a group and group-group relations, respectively. To be specific, for the intra-group interaction, we mask pedestrian graph edges out of an associated group. We also propose group pooling &unpooling operations to represent a group with multiple pedestrians as one graph node. Lastly, GP-Graph infers a probability map for socially-acceptable future trajectories from the integrated features of both group interactions. Moreover, we introduce a group-level latent vector sampling to ensure collective inferences over a set of possible future trajectories. Extensive experiments are conducted to validate the effectiveness of our architec ture, which demonstrates consistent performance improvements with publicly available benchmarks. Code is publicly available at https://github.com/inhwanbae/GPGraph."}}
{"id": "DaXRVucaYjk", "cdate": 1668518446482, "mdate": 1668518446482, "content": {"title": "Non-Probability Sampling Network for Stochastic Human Trajectory Prediction", "abstract": "Capturing multimodal natures is essential for stochastic\npedestrian trajectory prediction, to infer a finite set of future\ntrajectories. The inferred trajectories are based on observation paths and the latent vectors of potential decisions\nof pedestrians in the inference step. However, stochastic\napproaches provide varying results for the same data and\nparameter settings, due to the random sampling of the latent\nvector. In this paper, we analyze the problem by reconstructing and comparing probabilistic distributions from prediction samples and socially-acceptable paths, respectively.\nThrough this analysis, we observe that the inferences of all\nstochastic models are biased toward the random sampling,\nand fail to generate a set of realistic paths from finite samples.\nThe problem cannot be resolved unless an infinite number of\nsamples is available, which is infeasible in practice. We introduce that the Quasi-Monte Carlo (QMC) method, ensuring\nuniform coverage on the sampling space, as an alternative\nto the conventional random sampling. With the same finite\nnumber of samples, the QMC improves all the multimodal\nprediction results. We take an additional step ahead by incorporating a learnable sampling network into the existing\nnetworks for trajectory prediction. For this purpose, we propose the Non-Probability Sampling Network (NPSN), a very\nsmall network (\u223c5K parameters) that generates purposive\nsample sequences using the past paths of pedestrians and\ntheir social interactions. Extensive experiments confirm that\nNPSN can significantly improve both the prediction accuracy\n(up to 60%) and reliability of the public pedestrian trajectory prediction benchmark. Code is publicly available at\nhttps://github.com/inhwanbae/NPSN.\n"}}
{"id": "DwZmgAJY_N", "cdate": 1668518392623, "mdate": 1668518392623, "content": {"title": "DevianceNet: Learning to Predict Deviance from A Large-scale Geo-tagged Dataset", "abstract": "Understanding how a city\u2019s physical appearance and environmental surroundings impact society traits, such as safety, is\nan essential issue in social artifcial intelligence. To demonstrate the relationship, most existing studies utilize subjective\nhuman perceptual attributes, categorization only for a few violent crimes, and images taken from still shot images. These\nlead to diffculty in identifying location-specifc characteristics for urban safety. In this work, to address this problem, we\npropose a large-scale dataset and a novel method by adopting\na concept of \u201cDeviance\u201d which explains behaviors violating\nsocial norms, both formally (e.g. crime) and informally (e.g.\ncivil complaints). We frst collect a geo-tagged dataset consisting of incident report data for seven metropolitan cities,\nwith corresponding sequential images around incident sites\nobtained from Google street view. We also design a convolutional neural network that learns spatio-temporal visual attributes of deviant streets. Experimental results show that our\nframework can reliably recognize real-world deviance in various cities. Furthermore, we analyze which visual attribute is\nimportant for deviance identifcation and severity estimation.\nWe have released our dataset and source codes at our project\npage: https://deviance-project.github.io/DevianceNet/"}}
{"id": "SQE-hFVxOpH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Depth from a Light Field Image with Learning-Based Matching Costs.", "abstract": "One of the core applications of light field imaging is depth estimation. To acquire a depth map, existing approaches apply a single photo-consistency measure to an entire light field. However, this is not an optimal choice because of the non-uniform light field degradations produced by limitations in the hardware design. In this paper, we introduce a pipeline that automatically determines the best configuration for photo-consistency measure, which leads to the most reliable depth label from the light field. We analyzed the practical factors affecting degradation in lenslet light field cameras, and designed a learning based framework that can retrieve the best cost measure and optimal depth label. To enhance the reliability of our method, we augmented an existing light field benchmark to simulate realistic source dependent noise, aberrations, and vignetting artifacts. The augmented dataset was used for the training and validation of the proposed approach. Our method was competitive with several state-of-the-art methods for the benchmark and real-world light field datasets."}}
{"id": "HmYbnpHld6S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Accurate 3D Reconstruction from Small Motion Clip for Rolling Shutter Cameras.", "abstract": "Structure from small motion has become an important topic in 3D computer vision as a method for estimating depth, since capturing the input is so user-friendly. However, major limitations exist with respect to the form of depth uncertainty, due to the narrow baseline and the rolling shutter effect. In this paper, we present a dense 3D reconstruction method from small motion clips using commercial hand-held cameras, which typically cause the undesired rolling shutter artifact. To address these problems, we introduce a novel small motion bundle adjustment that effectively compensates for the rolling shutter effect. Moreover, we propose a pipeline for a fine-scale dense 3D reconstruction that models the rolling shutter effect by utilizing both sparse 3D points and the camera trajectory from narrow-baseline images. In this reconstruction, the sparse 3D points are propagated to obtain an initial depth hypothesis using a geometry guidance term. Then, the depth information on each pixel is obtained by sweeping the plane around each depth search space near the hypothesis. The proposed framework shows accurate dense reconstruction results suitable for various sought-after applications. Both qualitative and quantitative evaluations show that our method consistently generates better depth maps compared to state-of-the-art methods."}}
{"id": "ryeYHi0ctQ", "cdate": 1538087745357, "mdate": null, "content": {"title": "DPSNet: End-to-end Deep Plane Sweep Stereo", "abstract": "Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets."}}
{"id": "SkZo_TbuZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Robust Depth Estimation From Auto Bracketed Images", "abstract": "As demand for advanced photographic applications on hand-held devices grows, these electronics require the capture of high quality depth. However, under low-light conditions, most devices still suffer from low imaging quality and inaccurate depth acquisition. To address the problem, we present a robust depth estimation method from a short burst shot with varied intensity (i.e., Auto Bracketing) or strong noise (i.e., High ISO). We introduce a geometric transformation between flow and depth tailored for burst images, enabling our learning-based multi-view stereo matching to be performed effectively. We then describe our depth estimation pipeline that incorporates the geometric transformation into our residual-flow network. It allows our framework to produce an accurate depth map even with a bracketed image sequence. We demonstrate that our method outperforms state-of-the-art methods for various datasets captured by a smartphone and a DSLR camera. Moreover, we show that the estimated depth is applicable for image quality enhancement and photographic editing."}}
