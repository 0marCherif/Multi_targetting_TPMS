{"id": "SG3ztVYDubA", "cdate": 1676957615913, "mdate": null, "content": {"title": "Explaining Multiclass Classifiers with Categorical Values: A Case Study in Radiography", "abstract": "Explainability of machine learning methods is of fundamental importance in healthcare to calibrate trust. A large branch of explainable machine learning uses tools linked to the Shapley value, which have nonetheless been found difficult to interpret and potentially misleading. Taking multiclass classification as a refer- ence task, we argue that a critical issue in these methods is that they disregard the structure of the model outputs. We develop the Categorical Shapley value as a theoretically-grounded method to explain the output of multiclass classifiers, in terms of transition (or flipping) probabilities across classes. We demonstrate on a case study composed of three example scenarios for pneumonia detection and subtyping using X-ray images.\n"}}
{"id": "acutU9EFbD", "cdate": 1672531200000, "mdate": 1682594651638, "content": {"title": "Efficient fair PCA for fair representation learning", "abstract": "We revisit the problem of fair principal component analysis (PCA), where the goal is to learn the best low-rank linear approximation of the data that obfuscates demographic information. We propose a conceptually simple approach that allows for an analytic solution similar to standard PCA and can be kernelized. Our methods have the same complexity as standard PCA, or kernel PCA, and run much faster than existing methods for fair PCA based on semidefinite programming or manifold optimization, while achieving similar results."}}
{"id": "jI9UUsas2lX", "cdate": 1640995200000, "mdate": 1682594651775, "content": {"title": "What You Like: Generating Explainable Topical Recommendations for Twitter Using Social Annotations", "abstract": "With over 500 million tweets posted per day, in Twitter, it is difficult for Twitter users to discover interesting content from the deluge of uninteresting posts. In this work, we present a novel, explainable, topical recommendation system, that utilizes social annotations, to help Twitter users discover tweets, on topics of their interest. A major challenge in using traditional rating dependent recommendation systems, like collaborative filtering and content based systems, in high volume social networks is that, due to attention scarcity most items do not get any ratings. Additionally, the fact that most Twitter users are passive consumers, with 44% users never tweeting, makes it very difficult to use user ratings for generating recommendations. Further, a key challenge in developing recommendation systems is that in many cases users reject relevant recommendations if they are totally unfamiliar with the recommended item. Providing a suitable explanation, for why the item is recommended, significantly improves the acceptability of recommendation. By virtue of being a topical recommendation system our method is able to present simple topical explanations for the generated recommendations. Comparisons with state-of-the-art matrix factorization based collaborative filtering, content based and social recommendations demonstrate the efficacy of the proposed approach."}}
{"id": "WLfDtsBSHH7", "cdate": 1640995200000, "mdate": 1682594651642, "content": {"title": "Generating Distributional Adversarial Examples to Evade Statistical Detectors", "abstract": "Deep neural networks (DNNs) are known to be highly vulnerable to adversarial examples (AEs) that include malicious perturbations. Assumptions about the statistical differences between natural and a..."}}
{"id": "Jwrs4e7UbT_", "cdate": 1640995200000, "mdate": 1682594651774, "content": {"title": "Amazon SageMaker Model Monitor: A System for Real-Time Insights into Deployed Machine Learning Models", "abstract": ""}}
{"id": "7z94TzVfihE", "cdate": 1640995200000, "mdate": 1648673213626, "content": {"title": "Diverse Counterfactual Explanations for Anomaly Detection in Time Series", "abstract": "Data-driven methods that detect anomalies in times series data are ubiquitous in practice, but they are in general unable to provide helpful explanations for the predictions they make. In this work we propose a model-agnostic algorithm that generates counterfactual ensemble explanations for time series anomaly detection models. Our method generates a set of diverse counterfactual examples, i.e, multiple perturbed versions of the original time series that are not considered anomalous by the detection model. Since the magnitude of the perturbations is limited, these counterfactuals represent an ensemble of inputs similar to the original time series that the model would deem normal. Our algorithm is applicable to any differentiable anomaly detection model. We investigate the value of our method on univariate and multivariate real-world datasets and two deep-learning-based anomaly detection models, under several explainability criteria previously proposed in other data domains such as Validity, Plausibility, Closeness and Diversity. We show that our algorithm can produce ensembles of counterfactual examples that satisfy these criteria and thanks to a novel type of visualisation, can convey a richer interpretation of a model's internal mechanism than existing methods. Moreover, we design a sparse variant of our method to improve the interpretability of counterfactual explanations for high-dimensional time series anomalies. In this setting, our explanation is localised on only a few dimensions and can therefore be communicated more efficiently to the model's user."}}
{"id": "7TZZomvGpkk", "cdate": 1640995200000, "mdate": 1682594651643, "content": {"title": "Pairwise Fairness for Ordinal Regression", "abstract": "We initiate the study of fairness for ordinal regression. We adapt two fairness notions previously considered in fair ranking and propose a strategy for training a predictor that is approximately fair according to either notion. Our predictor has the form of a threshold model, composed of a scoring function and a set of thresholds, and our strategy is based on a reduction to fair binary classification for learning the scoring function and local search for choosing the thresholds. We provide generalization guarantees on the error and fairness violation of our predictor, and we illustrate the effectiveness of our approach in extensive experiments."}}
{"id": "SggiFaQQNzL", "cdate": 1615206463783, "mdate": null, "content": {"title": " Reliable Learning by Subsuming a Trusted Model: Safe Exploration of the Space of Complex Models", "abstract": "Designing machine learning algorithms that are reliable,  safe,  and  trustworthy  is  an  important factor  when  using  predictions  to  make  critical decisions  in  real-world  applications  including healthcare, law, and self-driving cars.  A fundamental challenge faced by a practitioner is how to trade-off higher accuracy of a complex model with more reliability of a simpler, trusted model. In this paper, we propose a novel learning frame-work to tackle this challenge\u2014our key idea is to safely explore the space of complex models by subsuming a base model which is already trusted. We  achieve  this  via  enforcing  a  regularization constraint  in  the  learning  process  of  the  complex model based on the predictions of a trusted model.    Our  approach  is  generic,  allowing  us to  consider  different  trusted  models  and  different ways to enforce the regularization constraint. We demonstrate these ideas via experiments using synthetic and real-world datasets."}}
{"id": "xVVSfmF3bq", "cdate": 1609459200000, "mdate": 1682594651779, "content": {"title": "Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud", "abstract": "Understanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explainability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice."}}
{"id": "wm8bWm_vy1y", "cdate": 1609459200000, "mdate": null, "content": {"title": "Pairwise Fairness for Ordinal Regression", "abstract": "We initiate the study of fairness for ordinal regression. We adapt two fairness notions previously considered in fair ranking and propose a strategy for training a predictor that is approximately fair according to either notion. Our predictor has the form of a threshold model, composed of a scoring function and a set of thresholds, and our strategy is based on a reduction to fair binary classification for learning the scoring function and local search for choosing the thresholds. We provide generalization guarantees on the error and fairness violation of our predictor, and we illustrate the effectiveness of our approach in extensive experiments."}}
