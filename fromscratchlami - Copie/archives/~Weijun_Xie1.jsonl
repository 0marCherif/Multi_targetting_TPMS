{"id": "xhA_vStwFKI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bicriteria Approximation of Chance-Constrained Covering Problems", "abstract": "A chance-constrained optimization problem involves constraints with random data that can be violated with probability bounded from above by a prespecified small risk parameter. Such constraints are..."}}
{"id": "t7jphitoXK_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Approximation Algorithms for D-optimal Design", "abstract": "Experimental design is a classical statistics problem, and its aim is to estimate an unknown vector from linear measurements where a Gaussian noise is introduced in each measurement. For the combin..."}}
{"id": "hPSGSLQ_8PE", "cdate": 1577836800000, "mdate": null, "content": {"title": "Tractable reformulations of two-stage distributionally robust linear programs over the type-\u221e Wasserstein ball", "abstract": "This paper studies a two-stage distributionally robust stochastic linear program under the type- \u221e Wasserstein ball by providing sufficient conditions under which the program can be efficiently computed via a tractable convex program. By exploring the properties of binary variables, the developed reformulation techniques are extended to those with mixed binary random parameters. The main tractable reformulations are projected into the original decision space. The complexity analysis demonstrates that these tractable results are tight under the setting of this paper."}}
{"id": "fYnU4DZ1dV0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Best Principal Submatrix Selection for the Maximum Entropy Sampling Problem: Scalable Algorithms and Performance Guarantees", "abstract": "This paper studies a classic maximum entropy sampling problem (MESP), which aims to select the most informative principal submatrix of a prespecified size from a covariance matrix. MESP has been widely applied to many areas, including healthcare, power system, manufacturing and data science. By investigating its Lagrangian dual and primal characterization, we derive a novel convex integer program for MESP and show that its continuous relaxation yields a near-optimal solution. The results motivate us to study an efficient sampling algorithm and develop its approximation bound for MESP, which improves the best-known bound in literature. We then provide an efficient deterministic implementation of the sampling algorithm with the same approximation bound. By developing new mathematical tools for the singular matrices and analyzing the Lagrangian dual of the proposed convex integer program, we investigate the widely-used local search algorithm and prove its first-known approximation bound for MESP. The proof techniques further inspire us with an efficient implementation of the local search algorithm. Our numerical experiments demonstrate that these approximation algorithms can efficiently solve medium-sized and large-scale instances to near-optimality. Our proposed algorithms are coded and released as open-source software. Finally, we extend the analyses to the A-Optimal MESP (A-MESP), where the objective is to minimize the trace of the inverse of the selected principal submatrix."}}
{"id": "57pAOmCt0kk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Exact and Approximation Algorithms for Sparse PCA", "abstract": "Sparse PCA (SPCA) is a fundamental model in machine learning and data analytics, which has witnessed a variety of application areas such as finance, manufacturing, biology, healthcare. To select a prespecified-size principal submatrix from a covariance matrix to maximize its largest eigenvalue for the better interpretability purpose, SPCA advances the conventional PCA with both feature selection and dimensionality reduction. This paper proposes two exact mixed-integer SDPs (MISDPs) by exploiting the spectral decomposition of the covariance matrix and the properties of the largest eigenvalues. We then analyze the theoretical optimality gaps of their continuous relaxation values and prove that they are stronger than that of the state-of-art one. We further show that the continuous relaxations of two MISDPs can be recast as saddle point problems without involving semi-definite cones, and thus can be effectively solved by first-order methods such as the subgradient method. Since off-the-shelf solvers, in general, have difficulty in solving MISDPs, we approximate SPCA with arbitrary accuracy by a mixed-integer linear program (MILP) of a similar size as MISDPs. To be more scalable, we also analyze greedy and local search algorithms, prove their first-known approximation ratios, and show that the approximation ratios are tight. Our numerical study demonstrates that the continuous relaxation values of the proposed MISDPs are quite close to optimality, the proposed MILP model can solve small and medium-size instances to optimality, and the approximation algorithms work very well for all the instances. Finally, we extend the analyses to Rank-one Sparse SVD (R1-SSVD) with non-symmetric matrices and Sparse Fair PCA (SFPCA) when there are multiple covariance matrices, each corresponding to a protected group."}}
{"id": "r9X_OPVZgXf", "cdate": 1514764800000, "mdate": null, "content": {"title": "On Deterministic Reformulations of Distributionally Robust Joint Chance Constrained Optimization Problems", "abstract": "A joint chance constrained optimization problem involves multiple uncertain constraints, i.e., constraints with stochastic parameters, that are jointly required to be satisfied with probability exceeding a prespecified threshold. In a distributionally robust joint chance constrained optimization problem (DRCCP), the joint chance constraint is required to hold for all probability distributions of the stochastic parameters from a given ambiguity set. In this work, we consider DRCCPs involving convex nonlinear uncertain constraints and an ambiguity set specified by convex moment constraints. We investigate deterministic reformulations of such problems and conditions under which such deterministic reformulations are convex. In particular we show that a DRCCP can be reformulated as a convex program if one the following conditions hold: (i) there is a single uncertain constraint, (ii) the ambiguity set is defined by a single moment constraint, (iii) the ambiguity set is defined by linear moment constraints, and (iv) the uncertain and moment constraints are positively homogeneous with respect to uncertain parameters. We further show that if the decision variables are binary and the uncertain constraints are linear then a DRCCP can be reformulated as a deterministic mixed integer convex program. Finally, we present a numerical study to illustrate that the proposed mixed integer convex reformulation can be solved efficiently by existing solvers."}}
{"id": "jFGiQGPIRtn", "cdate": 1514764800000, "mdate": null, "content": {"title": "Approximate Positively Correlated Distributions and Approximation Algorithms for D-optimal Design", "abstract": "Experimental design is a classical statistics problem and its aim is to estimate an unknown $m$-dimensional vector $\\beta$ from linear measurements where a Gaussian noise is introduced in each measurement. For the combinatorial experimental design problem, the goal is to pick $k$ out of the given $n$ experiments so as to make the most accurate estimate of the unknown parameters, denoted as $\\hat{\\beta}$. In this paper, we will study one of the most robust measures of error estimation - $D$-optimality criterion, which corresponds to minimizing the volume of the confidence ellipsoid for the estimation error $\\beta-\\hat{\\beta}$. The problem gives rise to two natural variants depending on whether repetitions of experiments are allowed or not. We first propose an approximation algorithm with a $\\frac1e$-approximation for the $D$-optimal design problem with and without repetitions, giving the first constant factor approximation for the problem. We then analyze another sampling approximation algorithm and prove that it is $(1-\\epsilon)$-approximation if $k\\geq \\frac{4m}{\\epsilon}+\\frac{12}{\\epsilon^2}\\log(\\frac{1}{\\epsilon})$ for any $\\epsilon \\in (0,1)$. Finally, for $D$-optimal design with repetitions, we study a different algorithm proposed by literature and show that it can improve this asymptotic approximation ratio."}}
{"id": "cenynOd1tVT", "cdate": 1514764800000, "mdate": null, "content": {"title": "Approximate Positive Correlated Distributions and Approximation Algorithms for D-optimal Design", "abstract": "Experimental design is a classical area in statistics [21] and has also found new applications in machine learning[2]. In the combinatorial experimental design problem, the aim is to estimate an unknown m-dimensional vector x from linear measurements where a Gaussian noise is introduced in each measurement. The goal is to pick k out of the given n experiments so as to make the most accurate estimate of the unknown parameter x. Given a set S of chosen experiments, the most likelihood estimate x\u2032 can be obtained by a least squares computation. One of the robust measures of error estimation is the D-optimality criterion [27] which aims to minimize the generalized variance of the estimator. This corresponds to minimizing the volume of the standard confidence ellipsoid for the estimation error x \u2013 x\u2032. The problem gives rise to two natural variants depending on whether repetitions of experiments is allowed or not. The latter variant, while being more general, has also found applications in geographical location of sensors [19]. We show a close connection between approximation algorithms for the D-optimal design problem and constructions of approximately m-wise positively correlated distributions. This connection allows us to obtain a approximation for the D-optimal design problem with and without repetitions giving the first constant factor approximation for the problem. We then consider the case when the number of experiments chosen is much larger than the dimension m and show one can obtain (1 \u2013 \u220a)-approximation if when repetitions are allowed and if when no repetitions are allowed improving on previous work."}}
{"id": "T9OJ4_rUBIr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Relaxations and approximations of chance constraints under finite distributions", "abstract": "Optimization problems with constraints involving stochastic parameters that are required to be satisfied with a prespecified probability threshold arise in numerous applications. Such chance constrained optimization problems involve the dual challenges of stochasticity and nonconvexity. In the setting of a finite distribution of the stochastic parameters, an optimization problem with linear chance constraints can be formulated as a mixed integer linear program (MILP). The natural MILP formulation has a weak relaxation bound and is quite difficult to solve. In this paper, we review some recent results on improving the relaxation bounds and constructing approximate solutions for MILP formulations of chance constraints. We also discuss a recently introduced bicriteria approximation algorithm for covering type chance constrained problems. This algorithm uses a relaxation to construct a solution whose (constraint violation) risk level may be larger than the pre-specified threshold, but is within a constant factor of it, and whose objective value is also within a constant factor of the true optimal value. Finally, we present some new results that improve on the bicriteria approximation factors in the finite scenario setting and shed light on the effect of strong relaxations on the approximation ratios."}}
{"id": "Db3F34EBY3W", "cdate": 1514764800000, "mdate": null, "content": {"title": "On quantile cuts and their closure for chance constrained optimization problems", "abstract": "A chance constrained optimization problem over a finite distribution involves a set of scenario constraints from which a small subset can be violated. We consider the setting where all scenario constraints are mixed-integer convex. Existing works typically consider a mixed integer nonlinear programming (MINLP) formulation of this problem by introducing binary variables to indicate which constraint systems are to be satisfied or violated. A variety of cutting plane approaches for this MINLP formulation have been developed. In this paper we consider a family of cuts in the original space rather than those in the extended space of the MINLP reformulation. These cuts, known as quantile cuts, can be viewed as a projection of the well known family of mixing inequalities for the MINLP reformulation onto the original problem space. We show that the closure of the infinite family of all quantile cuts has a finite description. An important corollary of this result is that for linear chance constrained problems the quantile closure is polyhedral. We further show that a recursive application of quantile closure operations recovers the convex hull of the nonconvex chance constrained set in the limit, and in the pure integer setting the convergence is finite. We show that separation of quantile cuts is in general NP-hard, develop a heuristic separation method, and demonstrate its effectiveness through a computational study. We also study an approximation of the quantile closure and propose a generalization by grouping scenarios."}}
