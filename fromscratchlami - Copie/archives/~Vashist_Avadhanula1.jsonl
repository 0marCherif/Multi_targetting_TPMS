{"id": "vSVeCwQj8V", "cdate": 1546300800000, "mdate": null, "content": {"title": "MNL-Bandit: A Dynamic Learning Approach to Assortment Selection", "abstract": "We consider a dynamic assortment selection problem where in every round the retailer offers a subset (assortment) of N substitutable products to a consumer, who selects one of these products accord..."}}
{"id": "LItcoPozdXf", "cdate": 1546300800000, "mdate": null, "content": {"title": "Thompson Sampling for Contextual Bandit Problems with Auxiliary Safety Constraints", "abstract": "Recent advances in contextual bandit optimization and reinforcement learning have garnered interest in applying these methods to real-world sequential decision making problems. Real-world applications frequently have constraints with respect to a currently deployed policy. Many of the existing constraint-aware algorithms consider problems with a single objective (the reward) and a constraint on the reward with respect to a baseline policy. However, many important applications involve multiple competing objectives and auxiliary constraints. In this paper, we propose a novel Thompson sampling algorithm for multi-outcome contextual bandit problems with auxiliary constraints. We empirically evaluate our algorithm on a synthetic problem. Lastly, we apply our method to a real world video transcoding problem and provide a practical way for navigating the trade-off between safety and performance using Bayesian optimization."}}
{"id": "yMKQ8Xsxl9K", "cdate": 1483228800000, "mdate": null, "content": {"title": "MNL-Bandit: A Dynamic Learning Approach to Assortment Selection", "abstract": "We consider a dynamic assortment selection problem, where in every round the retailer offers a subset (assortment) of $N$ substitutable products to a consumer, who selects one of these products according to a multinomial logit (MNL) choice model. The retailer observes this choice and the objective is to dynamically learn the model parameters, while optimizing cumulative revenues over a selling horizon of length $T$. We refer to this exploration-exploitation formulation as the MNL-Bandit problem. Existing methods for this problem follow an \"explore-then-exploit\" approach, which estimate parameters to a desired accuracy and then, treating these estimates as if they are the correct parameter values, offers the optimal assortment based on these estimates. These approaches require certain a priori knowledge of \"separability\", determined by the true parameters of the underlying MNL model, and this in turn is critical in determining the length of the exploration period. (Separability refers to the distinguishability of the true optimal assortment from the other sub-optimal alternatives.) In this paper, we give an efficient algorithm that simultaneously explores and exploits, achieving performance independent of the underlying parameters. The algorithm can be implemented in a fully online manner, without knowledge of the horizon length $T$. Furthermore, the algorithm is adaptive in the sense that its performance is near-optimal in both the \"well separated\" case, as well as the general parameter setting where this separation need not hold."}}
{"id": "nsVbnZdbjOG", "cdate": 1483228800000, "mdate": null, "content": {"title": "Thompson Sampling for the MNL-Bandit", "abstract": "We consider a sequential subset selection problem under parameter uncertainty, where at each time step, the decision maker selects a subset of cardinality $K$ from $N$ possible items (arms), and observes a (bandit) feedback in the form of the index of one of the items in said subset, or none. Each item in the index set is ascribed a certain value (reward), and the feedback is governed by a Multinomial Logit (MNL) choice model whose parameters are a priori unknown. The objective of the decision maker is to maximize the expected cumulative rewards over a finite horizon $T$, or alternatively, minimize the regret relative to an oracle that knows the MNL parameters. We refer to this as the MNL-Bandit problem. This problem is representative of a larger family of exploration-exploitation problems that involve a combinatorial objective, and arise in several important application domains. We present an approach to adapt Thompson Sampling to this problem and show that it achieves near-optimal regret as well as attractive numerical performance."}}
{"id": "loZK9OAgtMp", "cdate": 1483228800000, "mdate": null, "content": {"title": "Thompson Sampling for the MNL-Bandit", "abstract": "We consider a sequential subset selection problem under parameter uncertainty, where at each time step, the decision maker selects a subset of cardinality $K$ from $N$ possible items (arms), and observes a (bandit) feedback in the form of the index of one of the items in said subset, or none. Each item in the index set is ascribed a certain value (reward), and the feedback is governed by a Multinomial Logit (MNL) choice model whose parameters are a priori unknown. The objective of the decision maker is to maximize the expected cumulative rewards over a finite horizon $T$, or alternatively, minimize the regret relative to an oracle that knows the MNL parameters. We refer to this as the MNL-Bandit problem. This problem is representative of a larger family of exploration-exploitation problems that involve a combinatorial objective, and arise in several important application domains. We present an approach to adapt Thompson Sampling to this problem and show that it achieves near-optimal regret as well as attractive numerical performance."}}
{"id": "ovXyJ7eVe3-", "cdate": 1451606400000, "mdate": null, "content": {"title": "On the tightness of an LP relaxation for rational optimization and its applications.", "abstract": "We consider the problem of optimizing a linear rational function subject to totally unimodular (TU) constraints over { 0 , 1 } variables. Such formulations arise in many applications including assortment optimization. We show that a natural extended LP relaxation of the problem is \u201ctight\u201d. In other words, any extreme point corresponds to an integral solution. We also consider more general constraints that are not TU but obtained by adding an arbitrary constraint to the set of TU constraints. Using structural insights about extreme points, we present a polynomial time approximation scheme (PTAS) for the general problem."}}
{"id": "mF-IHyL_DZD", "cdate": 1451606400000, "mdate": null, "content": {"title": "A Near-Optimal Exploration-Exploitation Approach for Assortment Selection", "abstract": "We consider an online assortment optimization problem, where in every round, the retailer offers a K-cardinality subset (assortment) of N substitutable products to a consumer, and observes the response. We model consumer choice behavior using the widely used multinomial logit (MNL) model, and consider the retailer's problem of dynamically learning the model parameters, while optimizing cumulative revenues over the selling horizon T. Formulating this as a variant of a multi-armed bandit problem, we present an algorithm based on the principle of \"optimism in the face of uncertainty.\" A naive MAB formulation would treat each of the N choose K possible assortments as a distinct \"arm\", leading to regret bounds that are exponential in K. We show that by exploiting the specific characteristics of the MNL model it is possible to design an algorithm with \u00d5(\u221aNT) regret, under a mild assumption. We demonstrate that this performance is nearly optimal, by providing a (randomized) instance of this problem on which any online algorithm would incur at least \u03a9Omega(\u221aNT/K) regret."}}
{"id": "drD5BY-DgBj", "cdate": 1388534400000, "mdate": null, "content": {"title": "A few good predictions: selective node labeling in a social network", "abstract": "Many social network applications face the following problem: given a network G=(V,E) with labels on a small subset O \\subset V of nodes and an optional set of features on nodes and edges, predict the labels of the remaining nodes. Much research has gone into designing learning models and inference algorithms for accurate predictions in this setting. However, a core hurdle to any prediction effort is that for many nodes there is insufficient evidence for inferring a label. We propose that instead of focusing on the impossible task of providing high accuracy over all nodes, we should focus on selectively making the few node predictions which will be correct with high probability. Any selective prediction strategy will require that the scores attached to node predictions be well-calibrated. Our evaluations show that existing prediction algorithms are poorly calibrated. We propose a new method of training a graphical model using a conditional likelihood objective that provides better calibration than the existing joint likelihood objective. We augment it with a decoupled confidence model created using a novel unbiased training process. Empirical evaluation on two large social networks show that we are able to select a large number of predictions with accuracy as high as 95%, even when the best overall accuracy is only 40%."}}
