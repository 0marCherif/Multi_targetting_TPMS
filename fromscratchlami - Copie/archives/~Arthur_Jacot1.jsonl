{"id": "6iDHce-0B-a", "cdate": 1663850400820, "mdate": null, "content": {"title": "Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions", "abstract": "We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising."}}
{"id": "kK200QKfvjB", "cdate": 1652737693422, "mdate": null, "content": {"title": "Feature Learning in $L_2$-regularized DNNs: Attraction/Repulsion and Sparsity", "abstract": "We study the loss surface of DNNs with $L_{2}$ regularization. We\nshow that the loss in terms of the parameters can be reformulated\ninto a loss in terms of the layerwise activations $Z_{\\ell}$ of the\ntraining set. This reformulation reveals the dynamics behind feature\nlearning: each hidden representations $Z_{\\ell}$ are optimal w.r.t.\nto an attraction/repulsion problem and interpolate between the input\nand output representations, keeping as little information from the\ninput as necessary to construct the activation of the next layer.\nFor positively homogeneous non-linearities, the loss can be further\nreformulated in terms of the covariances of the hidden representations,\nwhich takes the form of a partially convex optimization over a convex\ncone.\n\nThis second reformulation allows us to prove a sparsity result for\nhomogeneous DNNs: any local minimum of the $L_{2}$-regularized loss\ncan be achieved with at most $N(N+1)$ neurons in each hidden layer\n(where $N$ is the size of the training set). We show that this bound\nis tight by giving an example of a local minimum that requires $N^{2}/4$\nhidden neurons. But we also observe numerically that in more traditional\nsettings much less than $N^{2}$ neurons are required to reach the\nminima."}}
{"id": "HbacpAZRrW", "cdate": 1640995200000, "mdate": 1659366492157, "content": {"title": "Feature Learning in L2-regularized DNNs: Attraction/Repulsion and Sparsity", "abstract": "We study the loss surface of DNNs with $L_{2}$ regularization. We show that the loss in terms of the parameters can be reformulated into a loss in terms of the layerwise activations $Z_{\\ell}$ of the training set. This reformulation reveals the dynamics behind feature learning: each hidden representations $Z_{\\ell}$ are optimal w.r.t. to an attraction/repulsion problem and interpolate between the input and output representations, keeping as little information from the input as necessary to construct the activation of the next layer. For positively homogeneous non-linearities, the loss can be further reformulated in terms of the covariances of the hidden representations, which takes the form of a partially convex optimization over a convex cone. This second reformulation allows us to prove a sparsity result for homogeneous DNNs: any local minimum of the $L_{2}$-regularized loss can be achieved with at most $N(N+1)$ neurons in each hidden layer (where $N$ is the size of the training set). We show that this bound is tight by giving an example of a local minimum which requires $N^{2}/4$ hidden neurons. But we also observe numerically that in more traditional settings much less than $N^{2}$ neurons are required to reach the minima."}}
{"id": "DUy-qLzqvlU", "cdate": 1621629912784, "mdate": null, "content": {"title": "DNN-based Topology Optimisation:  Spatial Invariance and Neural Tangent Kernel", "abstract": "We study the Solid Isotropic Material Penalization (SIMP) method with a density field generated by a fully-connected neural network, taking the coordinates as inputs. In the large width limit, we show that the use of DNNs leads to a filtering effect similar to traditional filtering techniques for SIMP, with a filter described by the Neural Tangent Kernel (NTK). This filter is however not invariant under translation, leading to visual artifacts and non-optimal shapes. We propose two embeddings of the input coordinates, which lead  to (approximate) spatial invariance of the NTK and of the filter. We empirically confirm our theoretical observations and study how the filter size is affected by the architecture of the network. Our solution can easily be applied to any other coordinates-based generation method. "}}
{"id": "4FupV3lz7b2", "cdate": 1609459200000, "mdate": 1659366492119, "content": {"title": "Deep Linear Networks Dynamics: Low-Rank Biases Induced by Initialization Scale and L2 Regularization", "abstract": "The dynamics of Deep Linear Networks (DLNs) is dramatically affected by the variance $\\sigma^2$ of the parameters at initialization $\\theta_0$. For DLNs of width $w$, we show a phase transition w.r.t. the scaling $\\gamma$ of the variance $\\sigma^2=w^{-\\gamma}$ as $w\\to\\infty$: for large variance ($\\gamma<1$), $\\theta_0$ is very close to a global minimum but far from any saddle point, and for small variance ($\\gamma>1$), $\\theta_0$ is close to a saddle point and far from any global minimum. While the first case corresponds to the well-studied NTK regime, the second case is less understood. This motivates the study of the case $\\gamma \\to +\\infty$, where we conjecture a Saddle-to-Saddle dynamics: throughout training, gradient descent visits the neighborhoods of a sequence of saddles, each corresponding to linear maps of increasing rank, until reaching a sparse global minimum. We support this conjecture with a theorem for the dynamics between the first two saddles, as well as some numerical experiments."}}
{"id": "j6Me_mdqNfw", "cdate": 1577836800000, "mdate": null, "content": {"title": "Kernel Alignment Risk Estimator: Risk Prediction from Training Data", "abstract": "We study the risk (i.e. generalization error) of Kernel Ridge Regression (KRR) for a kernel $K$ with ridge $\\lambda&gt;0$ and i.i.d. observations. For this, we introduce two objects: the Signal Capture Threshold (SCT) and the Kernel Alignment Risk Estimator (KARE). The SCT $\\vartheta_{K,\\lambda}$ is a function of the data distribution: it can be used to identify the components of the data that the KRR predictor captures, and to approximate the (expected) KRR risk. This then leads to a KRR risk approximation by the KARE $\\rho_{K, \\lambda}$, an explicit function of the training data, agnostic of the true data distribution. We phrase the regression problem in a functional setting. The key results then follow from a finite-size adaptation of the resolvent method for general Wishart random matrices. Under a natural universality assumption (that the KRR moments depend asymptotically on the first two moments of the observations) we capture the mean and variance of the KRR predictor. We numerically investigate our findings on the Higgs and MNIST datasets for various classical kernels: the KARE gives an excellent approximation of the risk. This supports our universality hypothesis. Using the KARE, one can compare choices of Kernels and hyperparameters directly from the training set. The KARE thus provides a promising data-dependent procedure to select Kernels that generalize well."}}
{"id": "P3j1VFF3Eo-", "cdate": 1577836800000, "mdate": null, "content": {"title": "Implicit Regularization of Random Feature Models.", "abstract": "Random Feature (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with $P$ features, $N$ data points, and a ridge $\\lambda$, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an effective ridge $\\tilde{\\lambda}$. We show that $\\tilde{\\lambda} > \\lambda$ and $\\tilde{\\lambda} \\searrow \\lambda$ monotonically as $P$ grows, thus revealing the implicit regularization effect of finite RF sampling. We then compare the risk (i.e. test error) of the $\\tilde{\\lambda}$-KRR predictor with the average risk of the $\\lambda$-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically find an extremely good agreement between the test errors of the average $\\lambda$-RF predictor and $\\tilde{\\lambda}$-KRR predictor."}}
{"id": "SkgscaNYPS", "cdate": 1569439123276, "mdate": null, "content": {"title": "The asymptotic spectrum of the Hessian of DNN throughout training", "abstract": "The dynamics of DNNs during gradient descent is described by the so-called Neural Tangent Kernel (NTK). In this article, we show that the NTK allows one to gain precise insight into the Hessian of the cost of DNNs: we obtain a full characterization of the asymptotics of the spectrum of the Hessian, at initialization and during training. "}}
{"id": "ofYkKDCavkL", "cdate": 1546300800000, "mdate": null, "content": {"title": "Disentangling feature and lazy learning in deep neural networks: an empirical study.", "abstract": "Two distinct limits for deep learning have been derived as the network width $h\\rightarrow \\infty$, depending on how the weights of the last layer scale with $h$. In the Neural Tangent Kernel (NTK) limit, the dynamics becomes linear in the weights and is described by a frozen kernel $\\Theta$. By contrast, in the Mean-Field limit, the dynamics can be expressed in terms of the distribution of the parameters associated with a neuron, that follows a partial differential equation. In this work we consider deep networks where the weights in the last layer scale as $\\alpha h^{-1/2}$ at initialization. By varying $\\alpha$ and $h$, we probe the crossover between the two limits. We observe the previously identified regimes of lazy training and feature training. In the lazy-training regime, the dynamics is almost linear and the NTK barely changes after initialization. The feature-training regime includes the mean-field formulation as a limiting case and is characterized by a kernel that evolves in time, and learns some features. We perform numerical experiments on MNIST, Fashion-MNIST, EMNIST and CIFAR10 and consider various architectures. We find that (i) The two regimes are separated by an $\\alpha^*$ that scales as $h^{-1/2}$. (ii) Network architecture and data structure play an important role in determining which regime is better: in our tests, fully-connected networks perform generally better in the lazy-training regime, unlike convolutional networks. (iii) In both regimes, the fluctuations $\\delta F$ induced on the learned function by initial conditions decay as $\\delta F\\sim 1/\\sqrt{h}$, leading to a performance that increases with $h$. The same improvement can also be obtained at an intermediate width by ensemble-averaging several networks. (iv) In the feature-training regime we identify a time scale $t_1\\sim\\sqrt{h}\\alpha$, such that for $t\\ll t_1$ the dynamics is linear."}}
{"id": "Fwv09hb1xMU", "cdate": 1546300800000, "mdate": null, "content": {"title": "Scaling description of generalization with number of parameters in deep learning.", "abstract": "Supervised deep learning involves the training of neural networks with a large number $N$ of parameters. For large enough $N$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as $N$ grows past a certain threshold $N^{*}$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with $N$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations $\\|f_{N}-\\bar{f}_{N}\\|\\sim N^{-1/4}$ of the neural net output function $f_{N}$ around its expectation $\\bar{f}_{N}$. These affect the generalization error $\\epsilon_{N}$ for classification: under natural assumptions, it decays to a plateau value $\\epsilon_{\\infty}$ in a power-law fashion $\\sim N^{-1/2}$. This description breaks down at a so-called jamming transition $N=N^{*}$. At this threshold, we argue that $\\|f_{N}\\|$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at $N^{*}$. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond $N^{*}$, and averaging their outputs."}}
