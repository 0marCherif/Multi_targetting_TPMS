{"id": "_jujUPRrQfh", "cdate": 1618146545970, "mdate": null, "content": {"title": "Unsupervised Video Representation Learning by Bidirectional Feature Prediction", "abstract": "This paper introduces a novel method for self-supervised\nvideo representation learning via feature prediction. In contrast to the previous methods that focus on future feature\nprediction, we argue that a supervisory signal arising from\nunobserved past frames is complementary to one that originates from the future frames. The rationale behind our\nmethod is to encourage the network to explore the temporal structure of videos by distinguishing between future and\npast given present observations. We train our model in a\ncontrastive learning framework, where joint encoding of future and past provides us with a comprehensive set of temporal hard negatives via swapping. We empirically show\nthat utilizing both signals enriches the learned representations for the downstream task of action recognition. It outperforms independent prediction of future and past."}}
{"id": "P5xf1piti6G", "cdate": 1618146400394, "mdate": null, "content": {"title": "Self-labeled conditional GANs", "abstract": "This paper introduces a novel and fully unsupervised framework for conditional GAN training in which labels are automatically obtained from data. We incorporate a clustering network into the standard conditional GAN framework that plays against the discriminator. With the generator, it aims to find a shared structured mapping for associating pseudo-labels with the real and fake images. Our generator outperforms unconditional GANs in terms of FID with significant margins on large scale datasets like ImageNet and LSUN. It also outperforms class conditional GANs trained on human labels on CIFAR10 and CIFAR100 where fine-grained annotations or a large number of samples per class are not available. Additionally, our clustering network exceeds the state-of-the-art on CIFAR100 clustering.\n"}}
{"id": "S1Eg2JM_bH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Boosting Self-Supervised Learning via Knowledge Transfer", "abstract": "In self-supervised learning one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. A surprising result is that our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from $5.9$ to $2.6$ in object detection on PASCAL VOC 2007."}}
{"id": "HkZhN-MuZB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Representation Learning by Learning to Count", "abstract": "We introduce a novel method for representation learning that uses an artificial supervision signal based on counting visual primitives. This supervision signal is obtained from an equivariance relation, which does not require any manual annotation. We relate transformations of images to transformations of the representations. More specifically, we look for the representation that satisfies such relation rather than the transformations that match a given representation. In this paper, we use two image transformations in the context of counting: scaling and tiling. The first transformation exploits the fact that the number of visual primitives should be invariant to scale. The second transformation allows us to equate the total number of visual primitives in each tile to that in the whole image. These two transformations are combined in one constraint and used to train a neural network with a contrastive loss. The proposed task produces representations that perform on par or exceed the state of the art in transfer learning benchmarks."}}
{"id": "rkZG6Kbd-B", "cdate": 1451606400000, "mdate": null, "content": {"title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles", "abstract": "We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with $$51.8\\,\\%$$ for detection and $$68.6\\,\\%$$ for classification, and reduce the gap with supervised learning ( $$56.5\\,\\%$$ and $$78.2\\,\\%$$ respectively)."}}
