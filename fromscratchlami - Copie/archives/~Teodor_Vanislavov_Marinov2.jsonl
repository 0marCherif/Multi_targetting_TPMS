{"id": "10-VFLfJCms", "cdate": 1672531200000, "mdate": 1681491241987, "content": {"title": "Leveraging User-Triggered Supervision in Contextual Bandits", "abstract": ""}}
{"id": "VgX6ceDerh2", "cdate": 1652737568627, "mdate": null, "content": {"title": "Stochastic Online Learning with Feedback Graphs: Finite-Time and Asymptotic Optimality", "abstract": "We revisit the problem of stochastic online learning with feedback\ngraphs, with the goal of devising algorithms that are optimal, up to\nconstants, both asymptotically and in finite time. We show that,\nsurprisingly, the notion of optimal finite-time regret is not a\nuniquely defined property in this context and that, in general, it\nis decoupled from the asymptotic rate. We discuss alternative\nchoices and propose a notion of finite-time optimality that we argue\nis \\emph{meaningful}. For that notion, we give an algorithm that\nadmits quasi-optimal regret both in finite-time and asymptotically."}}
{"id": "_pXV0dD94h", "cdate": 1640995200000, "mdate": 1681491241948, "content": {"title": "Stochastic Online Learning with Feedback Graphs: Finite-Time and Asymptotic Optimality", "abstract": ""}}
{"id": "IlLrejEKMS", "cdate": 1640995200000, "mdate": 1681491241987, "content": {"title": "Dimension Independent Generalization of DP-SGD for Overparameterized Smooth Convex Optimization", "abstract": ""}}
{"id": "zkZip1vsNz", "cdate": 1609459200000, "mdate": 1681491242051, "content": {"title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning", "abstract": ""}}
{"id": "eUYZyoUH9J", "cdate": 1609459200000, "mdate": 1681491242071, "content": {"title": "The Pareto Frontier of model selection for general Contextual Bandits", "abstract": ""}}
{"id": "bYqKG34G9ie", "cdate": 1609459200000, "mdate": null, "content": {"title": "Corralling Stochastic Bandit Algorithms", "abstract": "We study the problem of corralling stochastic bandit algorithms, that is combining multiple bandit algorithms designed for a stochastic environment, with the goal of devising a corralling algorithm that performs almost as well as the best base algorithm. We give two general algorithms for this setting, which we show benefit from favorable regret guarantees. We show that the regret of the corralling algorithms is no worse than that of the best algorithm containing the arm with the highest reward, and depends on the gap between the highest reward and other rewards."}}
{"id": "WMC0rFLJM88", "cdate": 1609459200000, "mdate": 1681491242093, "content": {"title": "The Pareto Frontier of model selection for general Contextual Bandits", "abstract": ""}}
{"id": "TJhVsAGfOM", "cdate": 1609459200000, "mdate": 1671548292698, "content": {"title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning", "abstract": "We provide improved gap-dependent regret bounds for reinforcement learning in finite episodic Markov decision processes. Compared to prior work, our bounds depend on alternative definitions of gaps. These definitions are based on the insight that, in order to achieve a favorable regret, an algorithm does not need to learn how to behave optimally in states that are not reached by an optimal policy. We prove tighter upper regret bounds for optimistic algorithms and accompany them with new information-theoretic lower bounds for a large class of MDPs. Our results show that optimistic algorithms can not achieve the information-theoretic lower bounds even in deterministic MDPs unless there is a unique optimal policy."}}
{"id": "pQsNn5Rs5hX", "cdate": 1577836800000, "mdate": 1681491242113, "content": {"title": "Corralling Stochastic Bandit Algorithms", "abstract": ""}}
