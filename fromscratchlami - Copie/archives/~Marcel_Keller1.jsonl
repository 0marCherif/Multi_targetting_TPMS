{"id": "u1OAH1wSyR7", "cdate": 1640995200000, "mdate": 1681745162490, "content": {"title": "Secure Quantized Training for Deep Learning", "abstract": ""}}
{"id": "UKTeUxOpNf1", "cdate": 1640995200000, "mdate": 1681745163287, "content": {"title": "Secure Quantized Training for Deep Learning", "abstract": "We implement training of neural networks in secure multi-party computation (MPC) using quantization commonly used in said setting. We are the first to present an MNIST classifier purely trained in ..."}}
{"id": "FNBRMbACvzt", "cdate": 1640995200000, "mdate": 1681745162269, "content": {"title": "PUBA: Privacy-Preserving User-Data Bookkeeping and Analytics", "abstract": ""}}
{"id": "NiM9Q7Z95z", "cdate": 1621630124312, "mdate": null, "content": {"title": "Secure Quantized Training for Deep Learning", "abstract": "We have implemented training of neural networks in secure multi-party\ncomputation (MPC) using quantization commonly used in said setting. To\nthe best of our knowledge, we are the first to present training of\nMNIST purely implemented in MPC that comes within one percent of\naccuracy of training using plaintext computation.  We found that\ntraining with MPC is possible, but it takes more epochs and achieves a\nlower accuracy than the usual CPU/GPU computation.  More concretely,\nwe have trained a network with two convolution and two dense layers to\n98.5% accuracy in 150 epochs. This took a day in our MPC\nimplementation.\n"}}
{"id": "xduxghtNijz", "cdate": 1609459200000, "mdate": 1681745163300, "content": {"title": "Secure Quantized Training for Deep Learning", "abstract": "We implement training of neural networks in secure multi-party computation (MPC) using quantization commonly used in said setting. We are the first to present an MNIST classifier purely trained in MPC that comes within 0.2 percent of the accuracy of the same convolutional neural network trained via plaintext computation. More concretely, we have trained a network with two convolutional and two dense layers to 99.2% accuracy in 3.5 hours (under one hour for 99% accuracy). We have also implemented AlexNet for CIFAR-10, which converges in a few hours. We develop novel protocols for exponentiation and inverse square root. Finally, we present experiments in a range of MPC security models for up to ten parties, both with honest and dishonest majority as well as semi-honest and malicious security."}}
{"id": "neoG2M2gys", "cdate": 1609459200000, "mdate": 1681745162453, "content": {"title": "PUBA: Privacy-Preserving User-Data Bookkeeping and Analytics", "abstract": ""}}
{"id": "lquar-DbLo", "cdate": 1609459200000, "mdate": 1681745162463, "content": {"title": "Fantastic Four: Honest-Majority Four-Party Secure Computation With Malicious Security", "abstract": ""}}
{"id": "Uf0Ez7iaa6", "cdate": 1609459200000, "mdate": 1681745162453, "content": {"title": "Data Augmentation in Natural Language Processing: A Novel Text Generation Approach for Long and Short Text Classifiers", "abstract": "In many cases of machine learning, research suggests that the development of training data might have a higher relevance than the choice and modelling of classifiers themselves. Thus, data augmentation methods have been developed to improve classifiers by artificially created training data. In NLP, there is the challenge of establishing universal rules for text transformations which provide new linguistic patterns. In this paper, we present and evaluate a text generation method suitable to increase the performance of classifiers for long and short texts. We achieved promising improvements when evaluating short as well as long text tasks with the enhancement by our text generation method. Especially with regard to small data analytics, additive accuracy gains of up to 15.53% and 3.56% are achieved within a constructed low data regime, compared to the no augmentation baseline and another data augmentation technique. As the current track of these constructed regimes is not universally applicable, we also show major improvements in several real world low data tasks (up to +4.84 F1-score). Since we are evaluating the method from many perspectives (in total 11 datasets), we also observe situations where the method might not be suitable. We discuss implications and patterns for the successful application of our approach on different types of datasets."}}
{"id": "ywAo_gt3xuf", "cdate": 1577836800000, "mdate": 1681745163104, "content": {"title": "Improved Primitives for MPC over Mixed Arithmetic-Binary Circuits", "abstract": "This work introduces novel techniques to improve the translation between arithmetic and binary data types in secure multi-party computation. We introduce a new approach to performing these conversions using what we call extended doubly-authenticated bits (edaBits), which correspond to shared integers in the arithmetic domain whose bit decomposition is shared in the binary domain. These can be used to considerably increase the efficiency of non-linear operations such as truncation, secure comparison and bit-decomposition. Our edaBits are similar to the daBits technique introduced by Rotaru et al.\u00a0(Indocrypt 2019). However, we show that edaBits can be directly produced much more efficiently than daBits, with active security, while enabling the same benefits in higher-level applications. Our method for generating edaBits involves a novel cut-and-choose technique that may be of independent interest, and improves efficiency by exploiting natural, tamper-resilient properties of binary circuits that occur in our construction. We also show how edaBits can be applied to efficiently implement various non-linear protocols of interest, and we thoroughly analyze their correctness for both signed and unsigned integers. The results of this work can be applied to any corruption threshold, although they seem best suited to dishonest majority protocols such as SPDZ. We implement and benchmark our constructions, and experimentally verify that our technique yields a substantial increase in efficiency. EdaBits save in communication by a factor that lies between 2 and 60 for secure comparisons with respect to a purely arithmetic approach, and between 2 and 25 with respect to using daBits. Improvements in throughput per second slightly lower but still as high as a factor of\u00a047. We also apply our novel machinery to the tasks of biometric matching and convolutional neural networks, obtaining a noticeable improvement as well."}}
{"id": "vILjwJd9ugy", "cdate": 1577836800000, "mdate": null, "content": {"title": "Effectiveness of MPC-friendly Softmax Replacement", "abstract": "Softmax is widely used in deep learning to map some representation to a probability distribution. As it is based on exp/log functions that are relatively expensive in multi-party computation, Mohassel and Zhang (2017) proposed a simpler replacement based on ReLU to be used in secure computation. However, we could not reproduce the accuracy they reported for training on MNIST with three fully connected layers. Later works (e.g., Wagh et al., 2019 and 2021) used the softmax replacement not for computing the output probability distribution but for approximating the gradient in back-propagation. In this work, we analyze the two uses of the replacement and compare them to softmax, both in terms of accuracy and cost in multi-party computation. We found that the replacement only provides a significant speed-up for a one-layer network while it always reduces accuracy, sometimes significantly. Thus we conclude that its usefulness is limited and one should use the original softmax function instead."}}
