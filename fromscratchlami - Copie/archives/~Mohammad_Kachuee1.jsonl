{"id": "nlwWuy61ABk", "cdate": 1640995200000, "mdate": 1682772528572, "content": {"title": "Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational AI Systems", "abstract": "Mohammad Kachuee, Jinseok Nam, Sarthak Ahuja, Jin-Myung Won, Sungjin Lee. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track. 2022."}}
{"id": "Wv2KgE3YUB", "cdate": 1640995200000, "mdate": 1682772528579, "content": {"title": "Constrained Policy Optimization for Controlled Self-Learning in Conversational AI Systems", "abstract": "Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making abrupt policy changes that break the current user experience. In this study, we introduce a scalable framework for supporting fine-grained exploration targets for individual domains via user-defined constraints. For example, we may want to ensure fewer policy deviations in business-critical domains such as shopping, while allocating more exploration budget to domains such as music. Furthermore, we present a novel meta-gradient learning approach that is scalable and practical to address this problem. The proposed method adjusts constraint violation penalty terms adaptively through a meta objective that encourages balanced constraint satisfaction across domains. We conduct extensive experiments using data from a real-world conversational AI on a set of realistic constraint benchmarks. Based on the experimental results, we demonstrate that the proposed approach is capable of achieving the best balance between the policy value and constraint satisfaction rate."}}
{"id": "PnnK8yTR3WE", "cdate": 1640995200000, "mdate": 1682772528571, "content": {"title": "Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational AI Systems", "abstract": "Skill routing is an important component in large-scale conversational systems. In contrast to traditional rule-based skill routing, state-of-the-art systems use a model-based approach to enable natural conversations. To provide supervision signal required to train such models, ideas such as human annotation, replication of a rule-based system, relabeling based on user paraphrases, and bandit-based learning were suggested. However, these approaches: (a) do not scale in terms of the number of skills and skill on-boarding, (b) require a very costly expert annotation/rule-design, (c) introduce risks in the user experience with each model update. In this paper, we present a scalable self-learning approach to explore routing alternatives without causing abrupt policy changes that break the user experience, learn from the user interaction, and incrementally improve the routing via frequent model refreshes. To enable such robust frequent model updates, we suggest a simple and effective approach that ensures controlled policy updates for individual domains, followed by an off-policy evaluation for making deployment decisions without any need for lengthy A/B experimentation. We conduct various offline and online A/B experiments on a commercial large-scale conversational system to demonstrate the effectiveness of the proposed method in real-world production settings."}}
{"id": "OEGCqIsaDSj", "cdate": 1640995200000, "mdate": 1682772528577, "content": {"title": "Domain-Aware Contrastive Knowledge Transfer for Multi-domain Imbalanced Data", "abstract": ""}}
{"id": "5x_OJy8tK7", "cdate": 1640995200000, "mdate": 1682772528573, "content": {"title": "Domain-Aware Contrastive Knowledge Transfer for Multi-domain Imbalanced Data", "abstract": "In many real-world machine learning applications, samples belong to a set of domains e.g., for product reviews each review belongs to a product category. In this paper, we study multi-domain imbalanced learning (MIL), the scenario that there is imbalance not only in classes but also in domains. In the MIL setting, different domains exhibit different patterns and there is a varying degree of similarity and divergence among domains posing opportunities and challenges for transfer learning especially when faced with limited or insufficient training data. We propose a novel domain-aware contrastive knowledge transfer method called DCMI to (1) identify the shared domain knowledge to encourage positive transfer among similar domains (in particular from head domains to tail domains); (2) isolate the domain-specific knowledge to minimize the negative transfer from dissimilar domains. We evaluated the performance of DCMI on three different datasets showing significant improvements in different MIL scenarios."}}
{"id": "3-a_DBryNq", "cdate": 1640995200000, "mdate": 1682772528571, "content": {"title": "Constrained Policy Optimization for Controlled Contextual Bandit Exploration", "abstract": ""}}
{"id": "27ECM0vwTn9", "cdate": 1640995200000, "mdate": 1682772528579, "content": {"title": "Generative Imputation and Stochastic Prediction", "abstract": "In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 and MNIST image datasets as well as five real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values."}}
{"id": "ekRLTaxQng5", "cdate": 1609459200000, "mdate": 1682772528572, "content": {"title": "Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents", "abstract": "Mohammad Kachuee, Hao Yuan, Young-Bum Kim, Sungjin Lee. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "-xTDprtl0Sb", "cdate": 1609459200000, "mdate": 1682772528579, "content": {"title": "Decentralized Knowledge Transfer on Edge Networks for Detecting Cancer in Images", "abstract": "The proliferation of edge networks creates an opportunity to utilize extremely low latency servers for offloading computation. In the healthcare domain, these will allow for more significant opportunities for personalized medicine and health monitoring solutions that will be directly connected to a health-care provider where patient data will be analyzed. Optimally, a model fitted to local data could be shared between providers to benefit from data that is not available locally, whether to debias a model or to gain better insight on a rarely seen condition. Since patient data privacy is of utmost importance, we require a method for transferring knowledge from sources containing data of interest without exposing or sharing local data. We propose such a method based on knowledge distillation that can transfer knowledge between models in real-time. Our approach results in lower training and converging time in addition to the ability to classify targets that are rarely or never seen locally accurately. Our method emphasizes transferring knowledge without sacrificing the insights gained from local data. We show our method works on skin cancer classification, detecting tumors in brain MRI scans, and other image datasets. All assuming data is non-i.i.d."}}
{"id": "o2ko2D_uvXJ", "cdate": 1601308020948, "mdate": null, "content": {"title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets."}}
