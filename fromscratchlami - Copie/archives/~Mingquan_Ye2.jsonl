{"id": "m7_IUwkeBxY", "cdate": 1672531200000, "mdate": 1695957837721, "content": {"title": "A Nearly-Optimal Bound for Fast Regression with \u2113\u221e Guarantee", "abstract": "Given a matrix $A\\in \\mathbb{R}^{n\\times d}$ and a vector $b\\in \\mathbb{R}^n$, we consider the regression problem with $\\ell_\\infty$ guarantees: finding a vector $x\u2019\\in \\mathbb{R}^d$ such that $||x..."}}
{"id": "Z_OlyF2T2hl", "cdate": 1672531200000, "mdate": 1695957837722, "content": {"title": "Streaming Semidefinite Programs: O(\u221an) Passes, Small Space and Fast Runtime", "abstract": "We study the problem of solving semidefinite programs (SDP) in the streaming model. Specifically, $m$ constraint matrices and a target matrix $C$, all of size $n\\times n$ together with a vector $b\\in \\mathbb{R}^m$ are streamed to us one-by-one. The goal is to find a matrix $X\\in \\mathbb{R}^{n\\times n}$ such that $\\langle C, X\\rangle$ is maximized, subject to $\\langle A_i, X\\rangle=b_i$ for all $i\\in [m]$ and $X\\succeq 0$. Previous algorithmic studies of SDP primarily focus on \\emph{time-efficiency}, and all of them require a prohibitively large $\\Omega(mn^2)$ space in order to store \\emph{all the constraints}. Such space consumption is necessary for fast algorithms as it is the size of the input. In this work, we design an interior point method (IPM) that uses $\\widetilde O(m^2+n^2)$ space, which is strictly sublinear in the regime $n\\gg m$. Our algorithm takes $O(\\sqrt n\\log(1/\\epsilon))$ passes, which is standard for IPM. Moreover, when $m$ is much smaller than $n$, our algorithm also matches the time complexity of the state-of-the-art SDP solvers. To achieve such a sublinear space bound, we design a novel sketching method that enables one to compute a spectral approximation to the Hessian matrix in $O(m^2)$ space. To the best of our knowledge, this is the first method that successfully applies sketching technique to improve SDP algorithm in terms of space (also time)."}}
{"id": "YcimyDgYlfz", "cdate": 1672531200000, "mdate": 1684615864822, "content": {"title": "Efficient Asynchronize Stochastic Gradient Algorithm with Structured Data", "abstract": "Deep learning has achieved impressive success in a variety of fields because of its good generalization. However, it has been a challenging problem to quickly train a neural network with a large number of layers. The existing works utilize the locality-sensitive hashing technique or some data structures on space partitioning to alleviate the training cost in each iteration. In this work, we try accelerating the computations in each iteration from the perspective of input data points. Specifically, for a two-layer fully connected neural network, when the training data have some special properties, e.g., Kronecker structure, each iteration can be completed in sublinear time in the data dimension."}}
{"id": "SFyO0Li-V9", "cdate": 1672531200000, "mdate": 1695957837720, "content": {"title": "Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation", "abstract": "Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \\in \\mathbb{R}^{n \\times n}$, a weight matrix $W \\in \\mathbb{R}_{\\geq 0}^{n \\times n}$, a parameter $k$, the goal is to output two matrices $U, V \\in \\mathbb{R}^{n \\times k}$ such that $\\| W \\circ (M - U V^\\top) \\|_F$ is minimized, where $\\circ$ denotes the Hadamard product. Such a problem is known to be NP-hard and even hard to approximate assuming Exponential Time Hypothesis [GG11, RSW16]. Meanwhile, alternating minimization is a good heuristic solution for approximating weighted low rank approximation. The work [LLR16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization. For weighted low rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work framework is a high-accuracy multiple response regression solver together with a robust analysis of alternating minimization."}}
{"id": "OROE9P6STV", "cdate": 1672531200000, "mdate": 1683235214248, "content": {"title": "A Nearly-Optimal Bound for Fast Regression with \ud835\udcc1\u221e Guarantee", "abstract": "Given a matrix $A\\in \\mathbb{R}^{n\\times d}$ and a vector $b\\in \\mathbb{R}^n$, we consider the regression problem with $\\ell_\\infty$ guarantees: finding a vector $x'\\in \\mathbb{R}^d$ such that $ \\|x'-x^*\\|_\\infty \\leq \\frac{\\epsilon}{\\sqrt{d}}\\cdot \\|Ax^*-b\\|_2\\cdot \\|A^\\dagger\\|$ where $x^*=\\arg\\min_{x\\in \\mathbb{R}^d}\\|Ax-b\\|_2$. One popular approach for solving such $\\ell_2$ regression problem is via sketching: picking a structured random matrix $S\\in \\mathbb{R}^{m\\times n}$ with $m\\ll n$ and $SA$ can be quickly computed, solve the ``sketched'' regression problem $\\arg\\min_{x\\in \\mathbb{R}^d} \\|SAx-Sb\\|_2$. In this paper, we show that in order to obtain such $\\ell_\\infty$ guarantee for $\\ell_2$ regression, one has to use sketching matrices that are dense. To the best of our knowledge, this is the first user case in which dense sketching matrices are necessary. On the algorithmic side, we prove that there exists a distribution of dense sketching matrices with $m=\\epsilon^{-2}d\\log^3(n/\\delta)$ such that solving the sketched regression problem gives the $\\ell_\\infty$ guarantee, with probability at least $1-\\delta$. Moreover, the matrix $SA$ can be computed in time $O(nd\\log n)$. Our row count is nearly-optimal up to logarithmic factors, and significantly improves the result in [Price, Song and Woodruff, ICALP'17], in which a super-linear in $d$ rows, $m=\\Omega(\\epsilon^{-2}d^{1+\\gamma})$ for $\\gamma=\\Theta(\\sqrt{\\frac{\\log\\log n}{\\log d}})$ is required. We also develop a novel analytical framework for $\\ell_\\infty$ guarantee regression that utilizes the Oblivious Coordinate-wise Embedding (OCE) property introduced in [Song and Yu, ICML'21]. Our analysis is arguably much simpler and more general than [Price, Song and Woodruff, ICALP'17], and it extends to dense sketches for tensor product of vectors."}}
{"id": "ISbZUBjvTaI", "cdate": 1672531200000, "mdate": 1684189447155, "content": {"title": "High-Accuracy Multicommodity Flows via Iterative Refinement", "abstract": "The multicommodity flow problem is a classic problem in network flow and combinatorial optimization, with applications in transportation, communication, logistics, and supply chain management, etc. Existing algorithms often focus on low-accuracy approximate solutions, while high-accuracy algorithms typically rely on general linear program solvers. In this paper, we present efficient high-accuracy algorithms for a broad family of multicommodity flow problems on undirected graphs, demonstrating improved running times compared to general linear program solvers. Our main result shows that we can solve the $\\ell_{q, p}$-norm multicommodity flow problem to a $(1 + \\varepsilon)$ approximation in time $O_{q, p}(m^{1+o(1)} k^2 \\log(1 / \\varepsilon))$, where $k$ is the number of commodities, and $O_{q, p}(\\cdot)$ hides constants depending only on $q$ or $p$. As $q$ and $p$ approach to $1$ and infinity respectively, $\\ell_{q, p}$-norm flow tends to maximum concurrent flow. We introduce the first iterative refinement framework for $\\ell_{q, p}$-norm minimization problems, which reduces the problem to solving a series of decomposable residual problems. In the case of $k$-commodity flow, each residual problem can be decomposed into $k$ single commodity convex flow problems, each of which can be solved in almost-linear time. As many classical variants of multicommodity flows were shown to be complete for linear programs in the high-accuracy regime [Ding-Kyng-Zhang, ICALP'22], our result provides new directions for studying more efficient high-accuracy multicommodity flow algorithms."}}
{"id": "jRP3zsyMCw", "cdate": 1640995200000, "mdate": 1684189447144, "content": {"title": "Universally-Optimal Distributed Shortest Paths and Transshipment via Graph-Based \u21131-Oblivious Routing", "abstract": ""}}
{"id": "QkU2lfw_rfe", "cdate": 1640995200000, "mdate": 1684189447162, "content": {"title": "A Data-dependent Approach for High Dimensional (Robust) Wasserstein Alignment", "abstract": "Many real-world problems can be formulated as the alignment between two geometric patterns. Previously, a great amount of research focus on the alignment of 2D or 3D patterns in the field of computer vision. Recently, the alignment problem in high dimensions finds several novel applications in practice. However, the research is still rather limited in the algorithmic aspect. To the best of our knowledge, most existing approaches are just simple extensions of their counterparts for 2D and 3D cases, and often suffer from the issues such as high computational complexities. In this paper, we propose an effective framework to compress the high dimensional geometric patterns. Any existing alignment method can be applied to the compressed geometric patterns and the time complexity can be significantly reduced. Our idea is inspired by the observation that high dimensional data often has a low intrinsic dimension. Our framework is a ``data-dependent'' approach that has the complexity depending on the intrinsic dimension of the input data. Our experimental results reveal that running the alignment algorithm on compressed patterns can achieve similar qualities, comparing with the results on the original patterns, but the runtimes (including the times cost for compression) are substantially lower."}}
{"id": "UBNS5vvYF6", "cdate": 1609459200000, "mdate": 1684189447255, "content": {"title": "Universally-Optimal Distributed Shortest Paths and Transshipment via Graph-Based L1-Oblivious Routing", "abstract": "We provide universally-optimal distributed graph algorithms for $(1+\\varepsilon)$-approximate shortest path problems including shortest-path-tree and transshipment. The universal optimality of our algorithms guarantees that, on any $n$-node network $G$, our algorithm completes in $T \\cdot n^{o(1)}$ rounds whenever a $T$-round algorithm exists for $G$. This includes $D \\cdot n^{o(1)}$-round algorithms for any planar or excluded-minor network. Our algorithms never require more than $(\\sqrt{n} + D) \\cdot n^{o(1)}$ rounds, resulting in the first sub-linear-round distributed algorithm for transshipment. The key technical contribution leading to these results is the first efficient $n^{o(1)}$-competitive linear $\\ell_1$-oblivious routing operator that does not require the use of $\\ell_1$-embeddings. Our construction is simple, solely based on low-diameter decompositions, and -- in contrast to all known constructions -- directly produces an oblivious flow instead of just an approximation of the optimal flow cost. This also has the benefit of simplifying the interaction with Sherman's multiplicative weight framework [SODA'17] in the distributed setting and its subsequent rounding procedures."}}
{"id": "5ggdMxE3GoE", "cdate": 1609459200000, "mdate": 1684189447173, "content": {"title": "Minor Sparsifiers and the Distributed Laplacian Paradigm", "abstract": "We study distributed algorithms built around minor-based vertex sparsifiers, and give the first algorithm in the CONGEST model for solving linear systems in graph Laplacian matrices to high accuracy. Our Laplacian solver has a round complexity of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$O(n^{o(1)}(\\sqrt{n}+D))$</tex> , and thus almost matches the lower bound of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\widetilde{\\Omega}(\\sqrt{n}+D)$</tex> , where <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$n$</tex> is the number of nodes in the network and <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$D$</tex> is its diameter. We show that our distributed solver yields new sublinear round algorithms for several cornerstone problems in combinatorial optimization. This is achieved by leveraging the powerful algorithmic framework of Interior Point Methods (IPMs) and the Laplacian paradigm in the context of distributed graph algorithms, which entails numerically solving optimization problems on graphs via a series of Laplacian systems. Problems that benefit from our distributed algorithmic paradigm include exact mincost flow, negative weight shortest paths, maxflow, and bipartite matching on sparse directed graphs. For the maxflow problem, this is the first exact distributed algorithm that applies to directed graphs, while the previous work by [Ghaffari et al. SICOMP'18] considered the approximate setting and works only for undirected graphs. For the mincost flow and the negative weight shortest path problems, our results constitute the first exact distributed algorithms running in a sublinear number of rounds. Given that the hybrid between IPMs and the Laplacian paradigm has proven useful for tackling numerous optimization problems in the centralized setting, we believe that our distributed solver will find future applications. At the heart of our distributed Laplacian solver is the notion of spectral subspace sparsifiers of [Li, Schild FOCS'18]. We present a nontrivial distributed implementation of their construction by (i) giving a parallel variant of their algorithm that avoids the sampling of random spanning trees and uses approximate leverage scores instead, and (ii) showing that the algorithm still produces a high-quality subspace spectral sparsifier by carefully setting up and analyzing matrix martingales. Combining this vertex reduction recursively with both tree and elimination-based preconditioners leads to our algorithm for solving Laplacian systems. The construction of the elimination-based preconditioners is based on computing short random walks, and we introduce a new technique for reducing the congestion incurred by the simulation of these walks on weighted graphs."}}
