{"id": "5t1r6LlVdLF", "cdate": 1618348853525, "mdate": null, "content": {"title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm", "abstract": "In this paper, we provide finite-sample convergence guarantees for an off-policy variant of the natural actor-critic (NAC) algorithm based on Importance Sampling. In particular, we show that the algorithm converges to a global optimal policy with a sample complexity of $\\mathcal{O}(\\epsilon^{-3}\\log^2(1/\\epsilon))$ under an appropriate choice of stepsizes. In order to overcome the issue of large variance due to Importance Sampling, we propose the $Q$-trace algorithm for the critic, which is inspired by the V-trace algorithm \\citep{espeholt2018impala}. This enables us to explicitly control the bias and variance, and characterize the trade-off between them. As an advantage of off-policy sampling, a major feature of our result is that we do not need any additional assumptions, beyond the ergodicity of the Markov chain induced by the behavior policy. "}}
{"id": "rbZ6gGqoW5", "cdate": 1609459200000, "mdate": 1647186420812, "content": {"title": "Finite Sample Analysis of Two-Time-Scale Natural Actor-Critic Algorithm", "abstract": "Actor-critic style two-time-scale algorithms are one of the most popular methods in reinforcement learning, and have seen great empirical success. However, their performance is not completely understood theoretically. In this paper, we characterize the \\emph{global} convergence of an online natural actor-critic algorithm in the tabular setting using a single trajectory of samples. Our analysis applies to very general settings, as we only assume ergodicity of the underlying Markov decision process. In order to ensure enough exploration, we employ an $\\epsilon$-greedy sampling of the trajectory. For a fixed and small enough exploration parameter $\\epsilon$, we show that the two-time-scale natural actor-critic algorithm has a rate of convergence of $\\tilde{\\mathcal{O}}(1/T^{1/4})$, where $T$ is the number of samples, and this leads to a sample complexity of $\\Tilde{\\mathcal{O}}(1/\\delta^{8})$ samples to find a policy that is within an error of $\\delta$ from the \\emph{global optimum}. Moreover, by carefully decreasing the exploration parameter $\\epsilon$ as the iterations proceed, we present an improved sample complexity of $\\Tilde{\\mathcal{O}}(1/\\delta^{6})$ for convergence to the global optimum."}}
{"id": "rawbTgf9i-5", "cdate": 1609459200000, "mdate": 1647186420849, "content": {"title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic with Linear Function Approximation", "abstract": "In this paper, we develop a novel variant of off-policy natural actor-critic algorithm with linear function approximation and we establish a sample complexity of $\\mathcal{O}(\\epsilon^{-3})$, outperforming all the previously known convergence bounds of such algorithms. In order to overcome the divergence due to deadly triad in off-policy policy evaluation under function approximation, we develop a critic that employs $n$-step TD-learning algorithm with a properly chosen $n$. We present finite-sample convergence bounds on this critic under both constant and diminishing step sizes, which are of independent interest. Furthermore, we develop a variant of natural policy gradient under function approximation, with an improved convergence rate of $\\mathcal{O}(1/T)$ after $T$ iterations. Combining the finite sample error bounds of actor and the critic, we obtain the $\\mathcal{O}(\\epsilon^{-3})$ sample complexity. We derive our sample complexity bounds solely based on the assumption that the behavior policy sufficiently explores all the states and actions, which is a much lighter assumption compared to the related literature."}}
{"id": "rEe4aezqiW9", "cdate": 1609459200000, "mdate": 1647186420887, "content": {"title": "Impact of Data Processing on Fairness in Supervised Learning", "abstract": "We study the impact of pre and post processing for reducing discrimination in data-driven decision makers. We first analyze the fundamental trade-off between fairness and accuracy in a pre-processing approach, and propose a design for a pre-processing module based on a convex optimization program, which can be added before the original classifier. This leads to a fundamental lower bound on attainable discrimination, given any acceptable distortion in the outcome. Furthermore, we reformulate an existing post-processing method in terms of our accuracy and fairness measures, which allows comparing post-processing and pre-processing approaches. We show that under some mild conditions, pre-processing outperforms post-processing. Finally, we show that by appropriate choice of the discrimination measure, the optimization problem for both pre and post processing approaches will reduce to a linear program and hence can be solved efficiently."}}
{"id": "r5bpeM9sb9", "cdate": 1609459200000, "mdate": 1647186420850, "content": {"title": "Impact of Data Processing on Fairness in Supervised Learning", "abstract": "We study the impact of pre and postprocessing for reducing discrimination in data-driven decision makers. We first analyze the fundamental trade-off between fairness and accuracy in a preprocessing approach, and propose a design for a preprocessing module based on a convex optimization program, which can be added before the original classifier. This leads to a fundamental lower bound on attainable discrimination, given any acceptable distortion in the outcome. Furthermore, we reformulate an existing postprocessing method in terms of our accuracy and fairness measures, which allows comparing postprocessing and preprocessing approaches. We show that under some mild conditions, preprocessing outperforms postprocessing. Finally, we show that by the appropriate choice of the discrimination measure, the optimization problem for both pre and postprocessing approaches will reduce to a linear program and hence can be solved efficiently."}}
{"id": "r4e-agf9s-c", "cdate": 1609459200000, "mdate": 1647186420813, "content": {"title": "On the Linear Convergence of Natural Policy Gradient Algorithm", "abstract": "Markov Decision Processes are classically solved using Value Iteration and Policy Iteration algorithms. Recent interest in Reinforcement Learning has motivated the study of methods inspired by optimization, such as gradient ascent. Among these, a popular algorithm is the Natural Policy Gradient, which is a mirror descent variant for MDPs. This algorithm forms the basis of several popular RL algorithms such as Natural actor-critic, TRPO, PPO, etc, and so is being studied with growing interest. It has been shown that Natural Policy Gradient with constant step size converges with a sublinear rate of $\\mathcal{O}(1/k)$ to the global optimal. In this paper, we present improved finite time convergence bounds, and show that this algorithm has geometric (also known as linear) asymptotic convergence rate. We further improve this convergence result by introducing a variant of Natural Policy Gradient with adaptive step sizes. Finally, we compare different variants of policy gradient methods experimentally."}}
{"id": "SzbaeGqi-c", "cdate": 1609459200000, "mdate": 1647186420851, "content": {"title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm", "abstract": "In this paper, we provide finite-sample convergence guarantees for an off-policy variant of the natural actor-critic (NAC) algorithm based on Importance Sampling. In particular, we show that the algorithm converges to a global optimal policy with a sample complexity of $\\mathcal{O}(\\epsilon^{-3}\\log^2(1/\\epsilon))$ under an appropriate choice of stepsizes. In order to overcome the issue of large variance due to Importance Sampling, we propose the $Q$-trace algorithm for the critic, which is inspired by the V-trace algorithm \\cite{espeholt2018impala}. This enables us to explicitly control the bias and variance, and characterize the trade-off between them. As an advantage of off-policy sampling, a major feature of our result is that we do not need any additional assumptions, beyond the ergodicity of the Markov chain induced by the behavior policy."}}
{"id": "S0MWagM9jbc", "cdate": 1609459200000, "mdate": 1647186420779, "content": {"title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm", "abstract": "In this paper, we provide finite-sample convergence guarantees for an off-policy variant of the natural actor-critic (NAC) algorithm based on Importance Sampling. In particular, we show that the al..."}}
{"id": "Hb4TxM9i-q", "cdate": 1609459200000, "mdate": 1647186420885, "content": {"title": "Information Theoretic Measures for Fairness-aware Feature Selection", "abstract": "Machine learning algorithms are increasingly used for consequential decision making regarding individuals based on their relevant features. Features that are relevant for accurate decisions may however lead to either explicit or implicit forms of discrimination against unprivileged groups, such as those of certain race or gender. This happens due to existing biases in the training data, which are often replicated or even exacerbated by the learning algorithm. Identifying and measuring these biases at the data level is a challenging problem due to the interdependence among the features, and the decision outcome. In this work, we develop a framework for fairness-aware feature selection which takes into account the correlation among the features and the decision outcome, and is based on information theoretic measures for the accuracy and discriminatory impacts of features. In particular, we first propose information theoretic measures which quantify the impact of different subsets of features on the accuracy and discrimination of the decision outcomes. We then deduce the marginal impact of each feature using Shapley value function; a solution concept in cooperative game theory used to estimate marginal contributions of players in a coalitional game. Finally, we design a fairness utility score for each feature (for feature selection) which quantifies how this feature influences accurate as well as nondiscriminatory decisions. Our framework depends on the joint statistics of the data rather than a particular classifier design. We examine our proposed framework on real and synthetic data to evaluate its performance."}}
{"id": "HAMValM5ibc", "cdate": 1609459200000, "mdate": 1647186420887, "content": {"title": "On the Linear convergence of Natural Policy Gradient Algorithm", "abstract": "Markov Decision Processes are classically solved using Value Iteration and Policy Iteration algorithms. Recent interest in Reinforcement Learning has motivated the study of methods inspired by optimization, such as gradient ascent. Among these, a popular algorithm is the Natural Policy Gradient, which is a mirror descent variant for MDPs. This algorithm forms the basis of several popular Reinforcement Learning algorithms such as Natural actor-critic, TRPO, PPO, etc, and so is being studied with growing interest. It has been shown that Natural Policy Gradient with constant step size converges with a sublinear rate of O(1/k) to the global optimal. In this paper, we present improved finite time convergence bounds, and show that this algorithm has geometric (also known as linear) asymptotic convergence rate. We further improve this convergence result by introducing a variant of Natural Policy Gradient with adaptive step sizes. Finally, we compare different variants of policy gradient methods experimentally."}}
