{"id": "Omu1B38MZlq", "cdate": 1680000993187, "mdate": 1680000993187, "content": {"title": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals", "abstract": "The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems."}}
{"id": "LBq2y6aHM5", "cdate": 1640995200000, "mdate": 1673282838063, "content": {"title": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals", "abstract": ""}}
{"id": "zm9WY6FFS7", "cdate": 1609459200000, "mdate": 1673282838085, "content": {"title": "Reward tampering problems and solutions in reinforcement learning: a causal influence diagram perspective", "abstract": ""}}
{"id": "nV_r62S0sTr", "cdate": 1577836800000, "mdate": null, "content": {"title": "Avoiding Tampering Incentives in Deep RL via Decoupled Approval", "abstract": "How can we design agents that pursue a given objective when all feedback mechanisms are influenceable by the agent? Standard RL algorithms assume a secure reward function, and can thus perform poorly in settings where agents can tamper with the reward-generating mechanism. We present a principled solution to the problem of learning from influenceable feedback, which combines approval with a decoupled feedback collection procedure. For a natural class of corruption functions, decoupled approval algorithms have aligned incentives both at convergence and for their local updates. Empirically, they also scale to complex 3D environments where tampering is possible."}}
{"id": "YJJyWsNCZOa", "cdate": 1577836800000, "mdate": null, "content": {"title": "Avoiding Side Effects By Considering Future Tasks", "abstract": "Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions."}}
{"id": "Ky68fO24IIW", "cdate": 1577836800000, "mdate": null, "content": {"title": "Avoiding Side Effects By Considering Future Tasks", "abstract": "Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions."}}
{"id": "9LxMsvMm5n9", "cdate": 1577836800000, "mdate": null, "content": {"title": "REALab: An Embedded Perspective on Tampering", "abstract": "This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs."}}
{"id": "sL9_PfwDDW", "cdate": 1546300800000, "mdate": null, "content": {"title": "Modeling AGI Safety Frameworks with Causal Influence Diagrams", "abstract": "Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks."}}
{"id": "jGzefi9xD61", "cdate": 1546300800000, "mdate": 1673282838106, "content": {"title": "Penalizing Side Effects using Stepwise Relative Reachability", "abstract": ""}}
{"id": "TCy5-WZ-Fv1", "cdate": 1546300800000, "mdate": null, "content": {"title": "Modeling AGI Safety Frameworks with Causal Influence Diagrams", "abstract": ""}}
