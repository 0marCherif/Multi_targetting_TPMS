{"id": "2VX6xoXXUT", "cdate": 1672531200000, "mdate": 1681651067076, "content": {"title": "Rigid body flows for sampling molecular crystal structures", "abstract": ""}}
{"id": "RgR5z2eACYV", "cdate": 1652736657956, "mdate": 1652736657956, "content": {"title": "Equivariant Flows: sampling configurations for multi-body systems with symmetric energies", "abstract": "Flows are exact-likelihood generative neural networks that transform samples from\na simple prior distribution to the samples of the probability distribution of interest.\nBoltzmann Generators (BG) combine flows and statistical mechanics to sample\nequilibrium states of strongly interacting many-body systems such as proteins with\n1000 atoms. In order to scale and generalize these results, it is essential that the\nnatural symmetries of the probability density \u2013 in physics defined by the invariances\nof the energy function \u2013 are built into the flow. Here we develop theoretical tools for\nconstructing such equivariant flows and demonstrate that a BG that is equivariant\nwith respect to rotations and particle permutations can generalize to sampling\nnontrivially new configurations where a nonequivariant BG cannot."}}
{"id": "o4rFW1iRZIh", "cdate": 1640995200000, "mdate": 1681651067025, "content": {"title": "Force-matching Coarse-Graining without Forces", "abstract": ""}}
{"id": "TY8yjHTNIOS", "cdate": 1640995200000, "mdate": 1681651067049, "content": {"title": "Generating stable molecules using imitation and reinforcement learning", "abstract": ""}}
{"id": "yxsak5ND2pA", "cdate": 1621629913566, "mdate": null, "content": {"title": "Smooth Normalizing Flows", "abstract": "Normalizing flows are a promising tool for modeling probability distributions in physical systems. While state-of-the-art flows accurately approximate distributions and energies, applications in physics additionally require smooth energies to compute forces and higher-order derivatives. Furthermore, such densities are often defined on non-trivial topologies. \nA recent example are Boltzmann Generators for generating 3D-structures of peptides and small proteins. These generative models leverage the space of internal coordinates (dihedrals, angles, and bonds), which is a product of hypertori and compact intervals. \nIn this work, we introduce a class of smooth mixture transformations working on both compact intervals and hypertori.\nMixture transformations employ root-finding methods to invert them in practice, which has so far prevented bi-directional flow training. To this end, we show that parameter gradients and forces of such inverses can be computed from forward evaluations via the inverse function theorem.\nWe demonstrate two advantages of such smooth flows: they allow training by force matching to simulation data and can be used as potentials in molecular dynamics simulations. "}}
{"id": "DAlEPSVCe8N", "cdate": 1620143621341, "mdate": null, "content": {"title": "Boltzmann generators-sampling equilibrium states of many-body systems with deep learning", "abstract": "Computing equilibrium states in condensed-matter many-body systems, such as solvated\nproteins, is a long-standing challenge. Lacking methods for generating statistically\nindependent equilibrium samples in \u201cone shot,\u201d vast computational effort is invested for\nsimulating these systems in small steps, e.g., using molecular dynamics. Combining deep\nlearning and statistical mechanics, we developed Boltzmann generators, which are shown\nto generate unbiased one-shot equilibrium samples of representative condensed-matter\nsystems and proteins. Boltzmann generators use neural networks to learn a coordinate\ntransformation of the complex configurational equilibrium distribution to a distribution\nthat can be easily sampled. Accurate computation of free-energy differences and discovery\nof new configurations are demonstrated, providing a statistical mechanics tool that can\navoid rare events during sampling without prior knowledge of reaction coordinates."}}
{"id": "ph8R5D-P6S", "cdate": 1609459200000, "mdate": 1681651067075, "content": {"title": "Generating stable molecules using imitation and reinforcement learning", "abstract": ""}}
{"id": "mOzFn46LGhn", "cdate": 1609459200000, "mdate": 1681651067028, "content": {"title": "Smooth Normalizing Flows", "abstract": ""}}
{"id": "Oq81dtLEETU", "cdate": 1609459200000, "mdate": 1681651067058, "content": {"title": "Smooth Normalizing Flows", "abstract": ""}}
{"id": "RayUtcIlGz", "cdate": 1601308406485, "mdate": null, "content": {"title": "Training Invertible Linear Layers through Rank-One Perturbations", "abstract": "Many types of neural network layers rely on matrix properties such as invertibility or orthogonality.\nRetaining such properties during optimization with gradient-based stochastic optimizers is a challenging task, which is usually addressed by either reparameterization of the affected parameters or by directly optimizing on the manifold.\nThis work presents a novel approach for training invertible linear layers. In lieu of directly optimizing\nthe network parameters, we train rank-one perturbations and add them to the actual weight matrices infrequently. This P$^{4}$Inv update allows keeping track of inverses and determinants without ever explicitly computing them. We show how such invertible blocks improve the mixing and thus the mode separation of the resulting normalizing flows. Furthermore, we outline how the P$^4$ concept can be utilized to retain properties other than invertibility."}}
