{"id": "UIujesRt_Hd", "cdate": 1672531200000, "mdate": 1680103246019, "content": {"title": "To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning", "abstract": ""}}
{"id": "edffTbw0Sws", "cdate": 1652737743221, "mdate": null, "content": {"title": "Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes", "abstract": "A fundamental property of deep learning normalization techniques, such as batch normalization, is making the pre-normalization parameters scale invariant. The intrinsic domain of such parameters is the unit sphere, and therefore their gradient optimization dynamics can be represented via spherical optimization with varying effective learning rate (ELR), which was studied previously. However, the varying ELR may obscure certain characteristics of the intrinsic loss landscape structure. In this work, we investigate the properties of training scale-invariant neural networks directly on the sphere using a fixed ELR. We discover three regimes of such training depending on the ELR value: convergence, chaotic equilibrium, and divergence. We study these regimes in detail both on a theoretical examination of a toy example and on a thorough empirical analysis of real scale-invariant deep learning models. Each regime has unique features and reflects specific properties of the intrinsic loss landscape, some of which have strong parallels with previous research on both regular and scale-invariant neural networks training. Finally, we demonstrate how the discovered regimes are reflected in conventional training of normalized networks and how they can be leveraged to achieve better optima."}}
{"id": "sJGfNSNQmJ", "cdate": 1640995200000, "mdate": 1680103246030, "content": {"title": "Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes", "abstract": ""}}
{"id": "B6uDDaDoW4a", "cdate": 1621630269598, "mdate": null, "content": {"title": "On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay", "abstract": "Training neural networks with batch normalization and weight decay has become a common practice in recent years. In this work, we show that their combined use may result in a surprising periodic behavior of optimization dynamics: the training process regularly exhibits destabilizations that, however, do not lead to complete divergence but cause a new period of training. We rigorously investigate the mechanism underlying the discovered periodic behavior from both empirical and theoretical points of view and analyze the conditions in which it occurs in practice. We also demonstrate that periodic behavior can be regarded as a generalization of two previously opposing perspectives on training with batch normalization and weight decay, namely the equilibrium presumption and the instability presumption."}}
{"id": "wBLlKFzWxY", "cdate": 1609459200000, "mdate": 1680103246068, "content": {"title": "On the Memorization Properties of Contrastive Learning", "abstract": ""}}
{"id": "MxvRakPfZ1A", "cdate": 1609459200000, "mdate": 1680103246020, "content": {"title": "On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay", "abstract": ""}}
{"id": "vkM3lAp2wzv", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Power Laws in Deep Ensembles", "abstract": "Ensembles of deep neural networks are known to achieve state-of-the-art performance in uncertainty estimation and lead to accuracy improvement. In this work, we focus on a classification problem and investigate the behavior of both non-calibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble as a function of the ensemble size and the member network size. We indicate the conditions under which CNLL follows a power law w. r. t. ensemble size or member network size, and analyze the dynamics of the parameters of the discovered power laws. Our important practical finding is that one large network may perform worse than an ensemble of several medium-size networks with the same total number of parameters (we call this ensemble a memory split). Using the detected power law-like dependencies, we can predict (1) the possible gain from the ensembling of networks with given structure, (2) the optimal memory split given a memory budget, based on a relatively small number of trained networks."}}
{"id": "exVpQg80qXH", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deep Ensembles on a Fixed Memory Budget: One Wide Network or Several Thinner Ones?", "abstract": "One of the generally accepted views of modern deep learning is that increasing the number of parameters usually leads to better quality. The two easiest ways to increase the number of parameters is to increase the size of the network, e.g. width, or to train a deep ensemble; both approaches improve the performance in practice. In this work, we consider a fixed memory budget setting, and investigate, what is more effective: to train a single wide network, or to perform a memory split -- to train an ensemble of several thinner networks, with the same total number of parameters? We find that, for large enough budgets, the number of networks in the ensemble, corresponding to the optimal memory split, is usually larger than one. Interestingly, this effect holds for the commonly used sizes of the standard architectures. For example, one WideResNet-28-10 achieves significantly worse test accuracy on CIFAR-100 than an ensemble of sixteen thinner WideResNets: 80.6% and 82.52% correspondingly. We call the described effect the Memory Split Advantage and show that it holds for a variety of datasets and model architectures."}}
{"id": "eHuvT5V89Ia", "cdate": 1577836800000, "mdate": null, "content": {"title": "Structured Sparsification of Gated Recurrent Neural Networks", "abstract": "One of the most popular approaches for neural network compression is sparsification \u2014 learning sparse weight matrices. In structured sparsification, weights are set to zero by groups corresponding to structure units, e. g. neurons. We further develop the structured sparsification approach for the gated recurrent neural networks, e. g. Long Short-Term Memory (LSTM). Specifically, in addition to the sparsification of individual weights and neurons, we propose sparsifying the preactivations of gates. This makes some gates constant and simplifies an LSTM structure. We test our approach on the text classification and language modeling tasks. Our method improves the neuron-wise compression of the model in most of the tasks. We also observe that the resulting structure of gate sparsity depends on the task and connect the learned structures to the specifics of the particular tasks."}}
{"id": "0soaH4mbpW", "cdate": 1577836800000, "mdate": 1680103246096, "content": {"title": "On Power Laws in Deep Ensembles", "abstract": ""}}
