{"id": "fevcUEmVxZ", "cdate": 1690848000000, "mdate": 1696247078587, "content": {"title": "Correlated Chained Gaussian Processes for Datasets With Multiple Annotators", "abstract": "The labeling process within a supervised learning task is usually carried out by an expert, which provides the ground truth (gold standard) for each sample. However, in many real-world applications, we typically have access to annotations provided by crowds holding different and unknown expertise levels. Learning from crowds (LFC) intends to configure machine learning paradigms in the presence of multilabelers, residing on two key assumptions: the labeler\u2019s performance does not depend on the input space, and independence among the annotators is imposed. Here, we propose the correlated chained Gaussian processes from the multiple annotators (CCGPMA) approach, which models each annotator\u2019s performance as a function of the input space and exploits the correlations among experts. Experimental results associated with classification and regression tasks show that our CCGPMA performs better modeling of the labelers\u2019 behavior, indicating that it consistently outperforms other state-of-the-art LFC approaches."}}
{"id": "oC7NgHiEIzP", "cdate": 1640995200000, "mdate": 1696247078585, "content": {"title": "A Fully Natural Gradient Scheme for Improving Inference of the Heterogeneous Multioutput Gaussian Process Model", "abstract": "A recent novel extension of multioutput Gaussian processes (GPs) handles heterogeneous outputs, assuming that each output has its own likelihood function. It uses a vector-valued GP prior to jointly model all likelihoods\u2019 parameters as latent functions drawn from a GP with a linear model of coregionalization (LMC) covariance. By means of an inducing points\u2019 framework, the model is able to obtain tractable variational bounds amenable to stochastic variational inference (SVI). Nonetheless, the strong conditioning between the variational parameters and the hyperparameters burdens the adaptive gradient optimization methods used in the original approach. To overcome this issue, we borrow ideas from variational optimization introducing an exploratory distribution over the hyperparameters, allowing inference together with the posterior\u2019s variational parameters through a fully natural gradient (NG) optimization scheme. Furthermore, in this work, we introduce an extension of the heterogeneous multioutput model, where its latent functions are drawn from convolution processes. We show that our optimization scheme can achieve better local optima solutions with higher test performance rates than adaptive gradient methods for both the LMC and the convolution process model. We also show how to make the convolutional model scalable by means of SVI and how to optimize it through a fully NG scheme. We compare the performance of the different methods over the toy and real databases."}}
{"id": "6TjyhdpU1O", "cdate": 1640995200000, "mdate": 1696247078590, "content": {"title": "Correlated Chained Gaussian Processes for Modelling Citizens Mobility Using a Zero-Inflated Poisson Likelihood", "abstract": "Modelling the mobility of people in a city depends on counting data with inherent problems of overdispersion. Such dispersion issues are caused by massive amounts of data with zero values. Though traditional machine learning models have been used to overcome said problems, they lack the ability to appropriately model the spatio-temporal correlations in data. To improve the modelling of such spatio-temporal correlations, in this work we propose to model the citizens mobility, for the Chinese city of Guangzhou, by means of a Zero-inflated Poisson likelihood in conjunction with Gaussian process priors generated from convolution processes. We follow the idea of chaining the likelihood\u2019s parameters to latent functions drawn from Gaussian process priors; this way allowing a higher flexibility to model heteroscedasticity. Additionally, we derive a stochastic variational inference framework that allow us to use two types of convolution process models in the context of large datasets: correlated chained Gaussian processes with a convolution processes model, and correlated chained Gaussian processes with variational inducing kernels. We present quantitative and qualitative results comparing the performance between Negative Binomial and Zero-inflated Poisson likelihoods, both in combination with three types of Gaussian process priors: a linear model of coregionalisation, and our two proposed methods based on a convolution processes model and variational inducing kernels."}}
{"id": "aFmTM7CO62c", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Fully Natural Gradient Scheme for Improving Inference of the Heterogeneous Multi-Output Gaussian Process Model", "abstract": "A recent novel extension of multi-output Gaussian processes handles heterogeneous outputs assuming that each output has its own likelihood function. It uses a vector-valued Gaussian process prior to jointly model all likelihoods' parameters as latent functions drawn from a Gaussian process with a linear model of coregionalisation covariance. By means of an inducing points framework, the model is able to obtain tractable variational bounds amenable to stochastic variational inference. Nonetheless, the strong conditioning between the variational parameters and the hyper-parameters burdens the adaptive gradient optimisation methods used in the original approach. To overcome this issue we borrow ideas from variational optimisation introducing an exploratory distribution over the hyper-parameters, allowing inference together with the posterior's variational parameters through a fully natural gradient optimisation scheme. Furthermore, in this work we introduce an extension of the heterogeneous multi-output model, where its latent functions are drawn from convolution processes. We show that our optimisation scheme can achieve better local optima solutions with higher test performance rates than adaptive gradient methods, this for both the linear model of coregionalisation and the convolution processes model. We also show how to make the convolutional model scalable by means of stochastic variational inference and how to optimise it through a fully natural gradient scheme. We compare the performance of the different methods over toy and real databases."}}
{"id": "4uXJIgaZyon", "cdate": 1420070400000, "mdate": null, "content": {"title": "Peripheral nerve segmentation using Nonparametric Bayesian Hierarchical Clustering", "abstract": "Several cases related to chronic pain, due to accidents, illness or surgical interventions, depend on anesthesiology procedures. These procedures are assisted with ultrasound images. Although, the ultrasound images are a useful instrument in order to guide the specialist in anesthesiology, the lack of intelligibility due to speckle noise, makes the clinical intervention a difficult task. In a similar manner, some artifacts are introduced in the image capturing process, challenging the expertise of anesthesiologists for not confusing the true nerve structures. Accordingly, an assistance methodology using image processing can improve the accuracy in the anesthesia practice. This paper proposes a peripheral nerve segmentation method in medical ultrasound images, based on Nonparametric Bayesian Hierarchical Clustering. The experimental results show segmentation performances with a Mean Squared Error performance of 1.026 \u00b1 0.379 pixels for ulnar nerve, 0.704 \u00b1 0.233 pixels for median nerve and 1.698 \u00b1 0.564 pixels for peroneal nerve. Likewise, the model allows to emphasize other soft structures like muscles and aqueous tissues, that might be useful for an anesthesiologist."}}
{"id": "3x2rTCR6pTT", "cdate": 1420070400000, "mdate": null, "content": {"title": "Peripheral Nerve Segmentation Using Speckle Removal and Bayesian Shape Models", "abstract": "In the field of medicine, ultrasound images have become a useful tool for visualizing nerve structures in the process of anesthesiology. Although, these images are commonly used in medical procedures..."}}
