{"id": "zb_1NL66mz", "cdate": 1640995200000, "mdate": 1682327494614, "content": {"title": "Open-Domain Conversational Question Answering with Historical Answers", "abstract": ""}}
{"id": "xwVMW1FPiq", "cdate": 1640995200000, "mdate": 1682327494623, "content": {"title": "Controllable User Dialogue Act Augmentation for Dialogue State Tracking", "abstract": ""}}
{"id": "gSuhlj3XI6f", "cdate": 1640995200000, "mdate": 1682327495615, "content": {"title": "PLM-ICD: Automatic ICD Coding with Pretrained Language Models", "abstract": "Automatically classifying electronic health records (EHRs) into diagnostic codes has been challenging to the NLP community. State-of-the-art methods treated this problem as a multilabel classification problem and proposed various architectures to model this problem. However, these systems did not leverage the superb performance of pretrained language models, which achieved superb performance on natural language understanding tasks. Prior work has shown that pretrained language models underperformed on this task with the regular finetuning scheme. Therefore, this paper aims at analyzing the causes of the underperformance and developing a framework for automatic ICD coding with pretrained language models. We spotted three main issues through the experiments: 1) large label space, 2) long input sequences, and 3) domain mismatch between pretraining and fine-tuning. We propose PLMICD, a framework that tackles the challenges with various strategies. The experimental results show that our proposed framework can overcome the challenges and achieves state-of-the-art performance in terms of multiple metrics on the benchmark MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD"}}
{"id": "RAz6JAt_VE_", "cdate": 1609459200000, "mdate": null, "content": {"title": "Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access Track in DSTC9", "abstract": "Most prior work on task-oriented dialogue systems are restricted to a limited coverage of domain APIs, while users oftentimes have domain related requests that are not covered by the APIs. This challenge track aims to expand the coverage of task-oriented dialogue systems by incorporating external unstructured knowledge sources. We define three tasks: knowledge-seeking turn detection, knowledge selection, and knowledge-grounded response generation. We introduce the data sets and the neural baseline models for three tasks. The challenge track received a total of 105 entries from 24 participating teams. In the evaluation results, the ensemble methods with different large-scale pretrained language models achieved high performances with improved knowledge selection capability and better generalization into unseen data."}}
{"id": "Oamc8Y4uyuq", "cdate": 1609459200000, "mdate": 1637007282560, "content": {"title": "Modeling Diagnostic Label Correlation for Automatic ICD Coding", "abstract": "Shang-Chi Tsai, Chao-Wei Huang, Yun-Nung Chen. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "q-8P8GlPfr", "cdate": 1577836800000, "mdate": 1641058138410, "content": {"title": "Learning Asr-Robust Contextualized Embeddings for Spoken Language Understanding", "abstract": "Employing pre-trained language models (LM) to extract contextualized word representations has achieved state-of-the-art performance on various NLP tasks. However, applying this technique to noisy transcripts generated by automatic speech recognizer (ASR) is concerned. Therefore, this paper focuses on making contextualized representations more ASR-robust. We propose a novel confusion-aware fine-tuning method to mitigate the impact of ASR errors on pre-trained LMs. Specifically, we fine-tune LMs to produce similar representations for acoustically confusable words that are obtained from word confusion networks (WCNs) produced by ASR. Experiments on multiple benchmark datasets show that the proposed method significantly improves the performance of spoken language understanding when performing on ASR transcripts."}}
{"id": "e9WlxXSA0wm", "cdate": 1577836800000, "mdate": 1641058138418, "content": {"title": "Learning Spoken Language Representations with Neural Lattice Language Modeling", "abstract": "Chao-Wei Huang, Yun-Nung Chen. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "bty5NkX5btt", "cdate": 1577836800000, "mdate": null, "content": {"title": "Overview of the Ninth Dialog System Technology Challenge: DSTC9", "abstract": "This paper introduces the Ninth Dialog System Technology Challenge (DSTC-9). This edition of the DSTC focuses on applying end-to-end dialog technologies for four distinct tasks in dialog systems, namely, 1. Task-oriented dialog Modeling with unstructured knowledge access, 2. Multi-domain task-oriented dialog, 3. Interactive evaluation of dialog, and 4. Situated interactive multi-modal dialog. This paper describes the task definition, provided datasets, baselines and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks."}}
{"id": "KYhLNuZgC4S", "cdate": 1577836800000, "mdate": 1641058138399, "content": {"title": "Learning Multi-Level Information for Dialogue Response Selection by Highway Recurrent Transformer", "abstract": "Highlights \u2022 A new variant of attention mechanisms focuses on modeling cross-sentence attention. \u2022 A novel model integrates highway attention in Transformer for modeling dialogues. \u2022 Our model is capable of modeling complex dialogue-level information. \u2022 The results on two response selection datasets show consistent performance. Abstract With increasing research interests in dialogue modeling, there is an emerging branch that formulates this task as next sentence selection, where given the partial dialogue context, the goal is to determine the most probable next sentence. To model natural language information, recurrent models have been applied to sequence modeling and shown promising results in various NLP tasks\u00a0(Sutskever et al., 2014). Recently, the Transformer (Vaswani et\u00a0al., 2017) has advanced modeling semantics for natural language sentences via attention, achieving improvement for sequence modeling. However, the Transformer focuses on modeling the intra-sentence attention but ignores inter-sentence information. In terms of dialogue modeling, the cross-sentence information is salient to understand dialogue content, so that the response selection can be better determined. Therefore, this paper proposes a novel attention mechanism based on multi-head attention, called highway attention, in order to allow the model to pass information through multiple sentences, and then builds a recurrent model based on the Transformer and the proposed highway attention. We call this model Highway Recurrent Transformer. This model focuses on not only intra-sentence dependency, but also inter-sentence dependency in the structure of dialogues. Experiments on the response selection task of the seventh Dialog System Technology Challenge (DSTC7) demonstrate that the proposed Highway Recurrent Transformer is capable of modeling both utterance-level and dialogue-level information for achieving better performance than the original Transformer in the single positive response scenario."}}
{"id": "BijNWKZ5lc", "cdate": 1577836800000, "mdate": 1641058138565, "content": {"title": "Towards Unsupervised Language Understanding and Generation by Joint Dual Learning", "abstract": "Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
