{"id": "ArZWGF0Ifl7", "cdate": 1652737758882, "mdate": null, "content": {"title": "Archimedes Meets Privacy: On Privately Estimating Quantiles in High Dimensions Under Minimal Assumptions", "abstract": "The last few years have seen a surge of work on high dimensional statistics under privacy constraints, mostly following two main lines of work: the \"worst case\" line, which does not make any distributional assumptions on the input data; and the \"strong assumptions\" line, which assumes that the data is generated from specific families, e.g., subgaussian distributions.\nIn this work we take a middle ground, obtaining new differentially private algorithms with polynomial sample complexity for estimating quantiles in high-dimensions, as well as estimating and sampling points of high Tukey depth, all working under very mild distributional assumptions. \n\nFrom the technical perspective, our work relies upon fundamental robustness results in the convex geometry literature, demonstrating how such results can be used in a private context. Our main object of interest is the (convex) floating body (FB), a notion going back to Archimedes, which is a robust and well studied high-dimensional analogue of the interquantile range of a distribution.  We show how one can privately, and with polynomially many samples, (a) output an approximate interior point of the FB -- e.g., \"a typical user\" in a high-dimensional database -- by leveraging the robustness of the Steiner point of the FB; and at the expense of polynomially many more samples, (b) produce an approximate uniform sample from the FB, by constructing a private noisy projection oracle.\n"}}
{"id": "vQzDYi4dPwM", "cdate": 1652737431103, "mdate": null, "content": {"title": "Size and depth of monotone neural networks: interpolation and approximation", "abstract": "\tMonotone functions and data sets arise in a variety of applications. We study the interpolation problem for monotone data sets: The input is a monotone data set with $n$ points, and the goal is to find a size and depth efficient monotone neural network with \\emph{non negative parameters} and threshold units that interpolates the data set. We show that there are monotone data sets that cannot be interpolated by a monotone network of depth $2$. On the other hand, we prove that for every monotone data set with $n$ points in $\\mathbb{R}^d$, there exists an interpolating monotone network of depth $4$ and size $O(nd)$. Our interpolation result implies that every monotone function over $[0,1]^d$ can be approximated arbitrarily well by a depth-4 monotone network, improving the previous best-known construction of depth $d+1$. Finally, building on results from Boolean circuit complexity, we show that the inductive bias of having positive parameters can lead to a super-polynomial blow-up in the number of neurons when approximating monotone functions.  "}}
{"id": "IhSeZ6Zxp3Q", "cdate": 1618900807418, "mdate": null, "content": {"title": "Community detection and percolation of information in a geometric setting", "abstract": "We make the first steps towards generalizing the theory of stochastic block models, in the sparse regime, towards a model where the discrete community structure is replaced by an underlying geometry. We consider a geometric random graph over a homogeneous metric space where the probability of two vertices to be connected is an arbitrary function of the distance. We give sufficient conditions under which the locations can be recovered (up to an isomorphism of the space) in the sparse regime. Moreover, we define a geometric counterpart of the model of flow of information on trees, due to Mossel and Peres, in which one considers a branching random walk on a sphere and the goal is to recover the location of the root based on the locations of leaves. We give some sufficient conditions for percolation and for non-percolation of information in this model."}}
{"id": "I8cmVV6oG85", "cdate": 1618900728098, "mdate": null, "content": {"title": "Network size and weights size for memorization with two-layers neural networks", "abstract": "In 1988, Eric B. Baum showed that two-layers neural networks with threshold activation function can perfectly memorize the binary labels of n points in general position in Rd using only \u250cn/d\u2510 neurons. We observe that with ReLU networks, using four times as many neurons one can fit arbitrary real labels. Moreover, for approximate memorization up to error \u03f5, the neural tangent kernel can also memorize with only O(nd\u22c5log(1/\u03f5)) neurons (assuming that the data is well dispersed too). We show however that these constructions give rise to networks where the magnitude of the neurons' weights are far from optimal. In contrast we propose a new training procedure for ReLU networks, based on complex (as opposed to real) recombination of the neurons, for which we show approximate memorization with both O(nd\u22c5log(1/\u03f5)\u03f5) neurons, as well as nearly-optimal size of the weights."}}
{"id": "ID7jIjY27NQ", "cdate": 1618900642176, "mdate": null, "content": {"title": "How to Trap a Gradient Flow", "abstract": "We consider the problem of finding an $\\epsilon$-approximate stationary point of a smooth function on a compact domain of $\\R^d$. In contrast with dimension-free approaches such as gradient descent, we focus here on the case where $d$ is finite, and potentially small. This viewpoint was explored in 1993 by Vavasis, who proposed an algorithm which, for {\\em any fixed finite dimension $d$}, improves upon the $O(1/\\epsilon^2)$ oracle complexity of gradient descent. For example for $d=2$, Vavasis' approach obtains the complexity $O(1/\\epsilon)$. Moreover for $d=2$ he also proved a lower bound of $\\Omega(1/\\sqrt{\\epsilon})$ for deterministic algorithms (we extend this result to randomized algorithms).  \n\nOur main contribution is an algorithm, which we call {\\em gradient flow trapping} (GFT), and the analysis of its oracle complexity. In dimension $d=2$, GFT closes the gap with Vavasis' lower bound (up to a logarithmic factor), as we show that it has complexity $O\\left(\\sqrt{\\frac{\\log(1/\\epsilon)}{\\epsilon}}\\right)$. In dimension $d=3$, we show a complexity of $O\\left(\\frac{\\log(1/\\epsilon)}{\\epsilon}\\right)$, improving upon Vavasis' $O\\left(1 / \\epsilon^{1.2} \\right)$. In higher dimensions, GFT has the remarkable property of being a {\\em logarithmic parallel depth} strategy, in stark contrast with the polynomial depth of gradient descent or Vavasis' algorithm. In this higher dimensional regime, the total work of GFT improves quadratically upon the only other known polylogarithmic depth strategy for this problem, namely naive grid search. We augment this result with another algorithm, named \\emph{cut and flow} (CF), which improves upon Vavasis' algorithm in any fixed dimension."}}
{"id": "VW5mn7h8fl9", "cdate": 1609459200000, "mdate": null, "content": {"title": "Non-asymptotic approximations of neural networks by Gaussian processes", "abstract": "We study the extent to which wide neural networks may be approximated by Gaussian processes when initialized with random weights. It is a well-established fact that as the width of a network goes to infinity, its law converges to that of a Gaussian process. We make this quantitative by establishing explicit convergence rates for the central limit theorem in an infinite-dimensional functional space, metrized with a natural transportation distance. We identify two regimes of interest; when the activation function is polynomial, its degree determines the rate of convergence, while for non-polynomial activations, the rate is governed by the smoothness of the function."}}
{"id": "-Zxfinfu7_g", "cdate": 1577836800000, "mdate": null, "content": {"title": "Network size and size of the weights in memorization with two-layers neural networks", "abstract": "In 1988, Eric B. Baum showed that two-layers neural networks with threshold activation function can perfectly memorize the binary labels of $n$ points in general position in $\\R^d$ using only $\\ulcorner n/d \\urcorner$ neurons. We observe that with ReLU networks, using four times as many neurons one can fit arbitrary real labels. Moreover, for approximate memorization up to error $\\epsilon$, the neural tangent kernel can also memorize with only $O\\left(\\frac{n}{d} \\cdot \\log(1/\\epsilon) \\right)$ neurons (assuming that the data is well dispersed too). We show however that these constructions give rise to networks where the \\emph{magnitude} of the neurons' weights are far from optimal. In contrast we propose a new training procedure for ReLU networks, based on {\\em complex} (as opposed to {\\em real}) recombination of the neurons, for which we show approximate memorization with both $O\\left(\\frac{n}{d} \\cdot \\frac{\\log(1/\\epsilon)}{\\epsilon}\\right)$ neurons, as well as nearly-optimal size of the weights."}}
{"id": "QJvAbvZnhL7", "cdate": 1546300800000, "mdate": null, "content": {"title": "Stability of the Shannon-Stam inequality via the F\u00f6llmer process", "abstract": "We prove stability estimates for the Shannon-Stam inequality (also known as the entropy-power inequality) for log-concave random vectors in terms of entropy and transportation distance. In particular, we give the first stability estimate for general log-concave random vectors in the following form: for log-concave random vectors $X,Y \\in \\mathbb{R}^d$, the deficit in the Shannon-Stam inequality is bounded from below by the expression $$ C \\left(\\mathrm{D}\\left(X||G\\right) + \\mathrm{D}\\left(Y||G\\right)\\right), $$ where $\\mathrm{D}\\left( \\cdot ~ ||G\\right)$ denotes the relative entropy with respect to the standard Gaussian and the constant $C$ depends only on the covariance structures and the spectral gaps of $X$ and $Y$. In the case of uniformly log-concave vectors our analysis gives dimension-free bounds. Our proofs are based on a new approach which uses an entropy-minimizing process from stochastic control theory."}}
{"id": "KrcuVUebHaR", "cdate": 1451606400000, "mdate": null, "content": {"title": "Information and dimensionality of anisotropic random geometric graphs", "abstract": "This paper deals with the problem of detecting non-isotropic high-dimensional geometric structure in random graphs. Namely, we study a model of a random geometric graph in which vertices correspond to points generated randomly and independently from a non-isotropic $d$-dimensional Gaussian distribution, and two vertices are connected if the distance between them is smaller than some pre-specified threshold. We derive new notions of dimensionality which depend upon the eigenvalues of the covariance of the Gaussian distribution. If $\\alpha$ denotes the vector of eigenvalues, and $n$ is the number of vertices, then the quantities $\\left(\\frac{||\\alpha||_2}{||\\alpha||_3}\\right)^6/n^3$ and $\\left(\\frac{||\\alpha||_2}{||\\alpha||_4}\\right)^4/n^3$ determine upper and lower bounds for the possibility of detection. This generalizes a recent result by Bubeck, Ding, R\\'acz and the first named author from [BDER14] which shows that the quantity $d/n^3$ determines the boundary of detection for isotropic geometry. Our methods involve Fourier analysis and the theory of characteristic functions to investigate the underlying probabilities of the model. The proof of the lower bound uses information theoretic tools, based on the method presented in [BG15]."}}
