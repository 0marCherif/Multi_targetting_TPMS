{"id": "JgrMWFKRTFF", "cdate": 1672531200000, "mdate": 1681718659108, "content": {"title": "Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories", "abstract": "We study the consequences of mode-collapse of normalizing flows in the context of lattice field theory. Normalizing flows allow for independent sampling. For this reason, it is hoped that they can avoid the tunneling problem of local-update MCMC algorithms for multi-modal distributions. In this work, we first point out that the tunneling problem is also present for normalizing flows but is shifted from the sampling to the training phase of the algorithm. Specifically, normalizing flows often suffer from mode-collapse for which the training process assigns vanishingly low probability mass to relevant modes of the physical distribution. This may result in a significant bias when the flow is used as a sampler in a Markov-Chain or with Importance Sampling. We propose a metric to quantify the degree of mode-collapse and derive a bound on the resulting bias. Furthermore, we propose various mitigation strategies in particular in the context of estimating thermodynamic observables, such as the free energy."}}
{"id": "YVLBwkzyGF", "cdate": 1640995200000, "mdate": 1681718659247, "content": {"title": "Gradients should stay on Path: Better Estimators of the Reverse- and Forward KL Divergence for Normalizing Flows", "abstract": "We propose an algorithm to estimate the path-gradient of both the reverse and forward Kullback-Leibler divergence for an arbitrary manifestly invertible normalizing flow. The resulting path-gradient estimators are straightforward to implement, have lower variance, and lead not only to faster convergence of training but also to better overall approximation results compared to standard total gradient estimators. We also demonstrate that path-gradient training is less susceptible to mode-collapse. In light of our results, we expect that path-gradient estimators will become the new standard method to train normalizing flows for variational inference."}}
{"id": "FtmvXmo3qx", "cdate": 1640995200000, "mdate": 1681718659244, "content": {"title": "Path-Gradient Estimators for Continuous Normalizing Flows", "abstract": "Recent work has established a path-gradient estimator for simple variational Gaussian distributions and has argued that the path-gradient is particularly beneficial in the regime in which the variational distribution approaches the exact target distribution. In many applications, this regime can however not be reached by a simple Gaussian variational distribution. In this work, we overcome this crucial limitation by proposing a path-gradient estimator for the considerably more expressive variational family of continuous normalizing flows. We outline an efficient algorithm to calculate this estimator and establish its superior performance empirically."}}
{"id": "DtNabUEtHC", "cdate": 1640995200000, "mdate": 1681718659243, "content": {"title": "Path-Gradient Estimators for Continuous Normalizing Flows", "abstract": "Recent work has established a path-gradient estimator for simple variational Gaussian distributions and has argued that the path-gradient is particularly beneficial in the regime in which the varia..."}}
{"id": "vn1EJned6FC", "cdate": 1609459200000, "mdate": 1681718659328, "content": {"title": "Machine Learning of Thermodynamic Observables in the Presence of Mode Collapse", "abstract": "Estimating the free energy, as well as other thermodynamic observables, is a key task in lattice field theories. Recently, it has been pointed out that deep generative models can be used in this context [1]. Crucially, these models allow for the direct estimation of the free energy at a given point in parameter space. This is in contrast to existing methods based on Markov chains which generically require integration through parameter space. In this contribution, we will review this novel machine-learning-based estimation method. We will in detail discuss the issue of mode collapse and outline mitigation techniques which are particularly suited for applications at finite temperature."}}
{"id": "ha6ZQ8cZy4O", "cdate": 1577836800000, "mdate": 1681718659255, "content": {"title": "On Estimation of Thermodynamic Observables in Lattice Field Theories with Deep Generative Models", "abstract": "In this work, we demonstrate that applying deep generative machine learning models for lattice field theory is a promising route for solving problems where Markov Chain Monte Carlo (MCMC) methods are problematic. More specifically, we show that generative models can be used to estimate the absolute value of the free energy, which is in contrast to existing MCMC-based methods which are limited to only estimate free energy differences. We demonstrate the effectiveness of the proposed method for two-dimensional $\\phi^4$ theory and compare it to MCMC-based methods in detailed numerical experiments."}}
{"id": "tsqgq7RYGU", "cdate": 1546300800000, "mdate": 1681718659250, "content": {"title": "Asymptotically Unbiased Generative Neural Sampling", "abstract": ""}}
{"id": "1-sv-CCF6z", "cdate": 1546300800000, "mdate": 1681718659247, "content": {"title": "Comment on \"Solving Statistical Mechanics Using VANs\": Introducing saVANt - VANs Enhanced by Importance and MCMC Sampling", "abstract": "In this comment on \"Solving Statistical Mechanics Using Variational Autoregressive Networks\" by Wu et al., we propose a subtle yet powerful modification of their approach. We show that the inherent sampling error of their method can be corrected by using neural network-based MCMC or importance sampling which leads to asymptotically unbiased estimators for physical quantities. This modification is possible due to a singular property of VANs, namely that they provide the exact sample probability. With these modifications, we believe that their method could have a substantially greater impact on various important fields of physics, including strongly-interacting field theories and statistical physics."}}
