{"id": "qioya3H2nWm", "cdate": 1672531200000, "mdate": 1687862725802, "content": {"title": "NeuroX Library for Neuron Analysis of Deep NLP Models", "abstract": "Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and provides a framework for data processing and evaluation, thus making it easier for researchers and practitioners to perform neuron analysis. The Python toolkit is available at https://www.github.com/fdalvi/NeuroX. Demo Video available at https://youtu.be/mLhs2YMx4u8."}}
{"id": "fw344F9S4uF", "cdate": 1672531200000, "mdate": 1696898681757, "content": {"title": "NeuroX Library for Neuron Analysis of Deep NLP Models", "abstract": ""}}
{"id": "dkr3RWJ3Xn", "cdate": 1672531200000, "mdate": 1687862725836, "content": {"title": "NxPlain: A Web-based Tool for Discovery of Latent Concepts", "abstract": "Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Tamim Jaban, Mus\u2019ab Husaini, Ummar Abbas. Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. 2023."}}
{"id": "b2l56i61Gd", "cdate": 1672531200000, "mdate": 1696898681758, "content": {"title": "ConceptX: A Framework for Latent Concept Analysis", "abstract": "The opacity of deep neural networks remains a challenge in deploying solutions where explanation is as important as precision. We present ConceptX, a human-in-the-loop framework for interpreting and annotating latent representational space in pre-trained Language Models (pLMs). We use an unsupervised method to discover concepts learned in these models and enable a graphical interface for humans to generate explanations for the concepts. To facilitate the process, we provide auto-annotations of the concepts (based on traditional linguistic ontologies). Such annotations enable development of a linguistic resource that directly represents latent concepts learned within deep NLP models. These include not just traditional linguistic concepts, but also task-specific or sensitive concepts (words grouped based on gender or religious connotation) that helps the annotators to mark bias in the model. The framework consists of two parts (i) concept discovery and (ii) annotation platform."}}
{"id": "SQKOVCOC9F", "cdate": 1672531200000, "mdate": 1696898681779, "content": {"title": "Knowledge Graph Based Trustworthy Medical Code Recommendations", "abstract": ""}}
{"id": "EhiLM-vUfS", "cdate": 1672531200000, "mdate": 1682351086539, "content": {"title": "Evaluating Neuron Interpretation Methods of NLP Models", "abstract": "Neuron Interpretation has gained traction in the field of interpretability, and have provided fine-grained insights into what a model learns and how language knowledge is distributed amongst its different components. However, the lack of evaluation benchmark and metrics have led to siloed progress within these various methods, with very little work comparing them and highlighting their strengths and weaknesses. The reason for this discrepancy is the difficulty of creating ground truth datasets, for example, many neurons within a given model may learn the same phenomena, and hence there may not be one correct answer. Moreover, a learned phenomenon may spread across several neurons that work together -- surfacing these to create a gold standard challenging. In this work, we propose an evaluation framework that measures the compatibility of a neuron analysis method with other methods. We hypothesize that the more compatible a method is with the majority of the methods, the more confident one can be about its performance. We systematically evaluate our proposed framework and present a comparative analysis of a large set of neuron interpretation methods. We make the evaluation framework available to the community. It enables the evaluation of any new method using 20 concepts and across three pre-trained models.The code is released at https://github.com/fdalvi/neuron-comparative-analysis"}}
{"id": "BNU-Tgvgx6", "cdate": 1672531200000, "mdate": 1682351083734, "content": {"title": "NxPlain: Web-based Tool for Discovery of Latent Concepts", "abstract": "The proliferation of deep neural networks in various domains has seen an increased need for the interpretability of these models, especially in scenarios where fairness and trust are as important as model performance. A lot of independent work is being carried out to: i) analyze what linguistic and non-linguistic knowledge is learned within these models, and ii) highlight the salient parts of the input. We present NxPlain, a web application that provides an explanation of a model's prediction using latent concepts. NxPlain discovers latent concepts learned in a deep NLP model, provides an interpretation of the knowledge learned in the model, and explains its predictions based on the used concepts. The application allows users to browse through the latent concepts in an intuitive order, letting them efficiently scan through the most salient concepts with a global corpus level view and a local sentence-level view. Our tool is useful for debugging, unraveling model bias, and for highlighting spurious correlations in a model. A hosted demo is available here: https://nxplain.qcri.org."}}
{"id": "8seneaIbsw", "cdate": 1672531200000, "mdate": 1696898681759, "content": {"title": "Learning Uncertainty for Unknown Domains with Zero-Target-Assumption", "abstract": ""}}
{"id": "6pPliiyHsfq", "cdate": 1672531200000, "mdate": 1696898681757, "content": {"title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models", "abstract": ""}}
{"id": "5fYQYhwU-c", "cdate": 1672531200000, "mdate": 1682351084711, "content": {"title": "On the effect of dropping layers of pre-trained transformer models", "abstract": ""}}
