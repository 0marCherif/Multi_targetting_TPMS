{"id": "m1M8Zco0ad", "cdate": 1696159819372, "mdate": 1696159819372, "content": {"title": "Adversarially Masking Synthetic to Mimic Real: Adaptive Noise Injection for Point Cloud Segmentation Adaptation", "abstract": "This paper considers the synthetic-to-real adaptation of\npoint cloud semantic segmentation, which aims to segment\nthe real-world point clouds with only synthetic labels available. Contrary to synthetic data which is integral and\nclean, point clouds collected by real-world sensors typically contain unexpected and irregular noise because the\nsensors may be impacted by various environmental conditions. Consequently, the model trained on ideal synthetic\ndata may fail to achieve satisfactory segmentation results\non real data. Influenced by such noise, previous adversarial training methods, which are conventional for 2D adaptation tasks, become less effective. In this paper, we aim to\nmitigate the domain gap caused by target noise via learning to mask the source points during the adaptation procedure. To this end, we design a novel learnable masking\nmodule, which takes source features and 3D coordinates as\ninputs. We incorporate Gumbel-Softmax operation into the\nmasking module so that it can generate binary masks and be\ntrained end-to-end via gradient back-propagation. With the\nhelp of adversarial training, the masking module can learn\nto generate source masks to mimic the pattern of irregular\ntarget noise, thereby narrowing the domain gap. We name\nour method \u201cAdversarial Masking\u201d as adversarial training\nand learnable masking module depend on each other and\ncooperate with each other to mitigate the domain gap. Experiments on two synthetic-to-real adaptation benchmarks\nverify the effectiveness of the proposed method"}}
{"id": "nQai_B1Zrt", "cdate": 1663849825234, "mdate": null, "content": {"title": " Decompose to Generalize: Species-Generalized Animal Pose Estimation", "abstract": "This paper challenges the cross-species generalization problem for animal pose estimation, aiming to learn a pose estimator that can be well generalized to novel species. We find the relation between different joints is important with two-fold impact: 1) on the one hand, some relation is consistent across all the species and may help two joints mutually confirm each other, e.g., the eyes help confirm the nose and vice versa because they are close in all species. 2) on the other hand, some relation is inconsistent for different species due to the species variation and may bring severe distraction rather than benefit. With these two insights, we propose a Decompose-to-Generalize (D-Gen) pose estimation method to break the inconsistent relations while preserving the consistent ones. Specifically, D-Gen first decomposes the body joints into several joint concepts so that each concept contains multiple closely-related joints. Given these joint concepts, D-Gen 1) promotes the interaction between intra-concept joints to enhance their reliable mutual confirmation, and 2) suppresses the interaction between inter-concept joints to prohibit their mutual distraction.  Importantly, we explore various decomposition approaches, i.e., heuristic, geometric and attention-based approaches. Experimental results show that all these decomposition manners yield reasonable joint concepts and substantially improve cross-species generalization (and the attention-based approach is the best). "}}
{"id": "qpQu_pTegW", "cdate": 1609459200000, "mdate": 1667370138466, "content": {"title": "Domain Consensus Clustering for Universal Domain Adaptation", "abstract": "In this paper, we investigate Universal Domain Adaptation (UniDA) problem, which aims to transfer the knowledge from source to target under unaligned label space. The main challenge of UniDA lies in how to separate common classes (i.e., classes shared across domains), from private classes (i.e., classes only exist in one domain). Previous works treat the private samples in the target as one generic class but ignore their intrinsic structure. Consequently, the resulting representations are not compact enough in the latent space and can be easily confused with common samples. To better exploit the intrinsic structure of the target domain, we propose Domain Consensus Clustering(DCC), which exploits the domain consensus knowledge to discover discriminative clusters on both common samples and private ones. Specifically, we draw the domain consensus knowledge from two aspects to facilitate the clustering and the private class discovery, i.e., the semantic-level consensus, which identifies the cycle-consistent clusters as the common classes, and the sample-level consensus, which utilizes the cross-domain classification agreement to determine the number of clusters and discover the private classes. Based on DCC, we are able to separate the private classes from the common ones, and differentiate the private classes themselves. Finally, we apply a class-aware alignment technique on identified common samples to minimize the distribution shift, and a prototypical regularizer to inspire discriminative target clusters. Experiments on four benchmarks demonstrate DCC significantly outperforms previous state-of-the-arts."}}
{"id": "DOs9l94D1E", "cdate": 1609459200000, "mdate": 1668588773503, "content": {"title": "VSPW: A Large-scale Dataset for Video Scene Parsing in the Wild", "abstract": "In this paper, we present a new dataset with the target of advancing the scene parsing task from images to videos. Our dataset aims to perform Video Scene Parsing in the Wild (VSPW), which covers a wide range of real-world scenarios and categories. To be specific, our VSPW is featured from the following aspects: 1) Well-trimmed long-temporal clips. Each video contains a complete shot, lasting around 5 seconds on average. 2) Dense annotation. The pixel-level annotations are provided at a high frame rate of 15 f/s. 3) High resolution. Over 96% of the captured videos are with high spatial resolutions from 720P to 4K. We totally annotate 3,337 videos, including 239,934 frames from 124 categories. To the best of our knowledge, our VSPW is the first attempt to tackle the challenging video scene parsing task in the wild by considering diverse scenarios. Based on VSPW, we design a generic Temporal Context Blending (TCB) network, which can effectively harness long-range contextual information from the past frames to help segment the current one. Extensive experiments show that our TCB network improves both the segmentation performance and temporal stability comparing with image-/video-based state-of-the-art methods. We hope that the scale, diversity, long-temporal, and high frame rate of our VSPW can significantly advance the research of video scene parsing and beyond."}}
{"id": "eW7vGZ0i84", "cdate": 1577836800000, "mdate": 1668217125531, "content": {"title": "Content-Consistent Matching for Domain Adaptive Semantic Segmentation", "abstract": "This paper considers the adaptation of semantic segmentation from the synthetic source domain to the real target domain. Different from most previous explorations that often aim at developing adversarial-based domain alignment solutions, we tackle this challenging task from a new perspective, i.e., content-consistent matching (CCM). The target of CCM is to acquire those synthetic images that share similar distribution with the real ones in the target domain, so that the domain gap can be naturally alleviated by employing the content-consistent synthetic images for training. To be specific, we facilitate the CCM from two aspects, i.e., semantic layout matching and pixel-wise similarity matching. First, we use all the synthetic images from the source domain to train an initial segmentation model, which is then employed to produce coarse pixel-level labels for the unlabeled images in the target domain. With the coarse/accurate label maps for real/synthetic images, we construct their semantic layout matrixes from both horizontal and vertical directions and perform the matrixes matching to find out the synthetic images with similar semantic layout to real images. Second, we choose those predicted labels with high confidence to generate feature embeddings for all classes in the target domain, and further perform the pixel-wise matching on the mined layout-consistent synthetic images to harvest the appearance-consistent pixels. With the proposed CCM, only those content-consistent synthetic images are taken into account for learning the segmentation model, which can effectively alleviate the domain bias caused by those content-irrelevant synthetic images. Extensive experiments are conducted on two popular domain adaptation tasks, i.e., GTA5 $$\\xrightarrow {}$$ Cityscapes and SYNTHIA $$\\xrightarrow {}$$ Cityscapes. Our CCM yields consistent improvements over the baselines and performs favorably against previous state-of-the-arts."}}
