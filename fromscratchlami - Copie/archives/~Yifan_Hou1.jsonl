{"id": "i5CteZN1AA", "cdate": 1640995200000, "mdate": 1679905717040, "content": {"title": "A Representation Learning Framework for Property Graphs", "abstract": ""}}
{"id": "gKHGM9HddWK", "cdate": 1640995200000, "mdate": 1679905716993, "content": {"title": "Understanding Knowledge Integration in Language Models with Graph Convolutions", "abstract": ""}}
{"id": "Zi4qe1gFCP", "cdate": 1640995200000, "mdate": 1679905716983, "content": {"title": "Adapters for Enhanced Modeling of Multilingual Knowledge and Text", "abstract": ""}}
{"id": "NsGyPgncKM", "cdate": 1640995200000, "mdate": 1679905717022, "content": {"title": "Measuring and Improving the Use of Graph Information in Graph Neural Networks", "abstract": ""}}
{"id": "NBPwMFURFM_", "cdate": 1640995200000, "mdate": 1679905716980, "content": {"title": "What Has Been Enhanced in my Knowledge-Enhanced Language Model?", "abstract": ""}}
{"id": "8LQ2lE5Q6Q", "cdate": 1640995200000, "mdate": 1679905717026, "content": {"title": "Adapters for Enhanced Modeling of Multilingual Knowledge and Text", "abstract": ""}}
{"id": "3XD_rnM97s", "cdate": 1632875514304, "mdate": null, "content": {"title": "Understanding Knowledge Integration in Language Models with Graph Convolutions", "abstract": "Pretrained language models (LMs) are not very good at robustly capturing factual knowledge. This has led to the development of a number of knowledge integration (KI) methods which aim to incorporate external knowledge into pretrained LMs. Even though KI methods show some performance gains over base LMs, the efficacy and limitations of these methods are not well-understood. For instance, it is unclear how and what kind of knowledge is effectively integrated into LMs and if such integration may lead to catastrophic forgetting of already learned knowledge. In this paper, we revisit the KI process from the view of graph signal processing and show that KI could be interpreted using a graph convolution operation. We propose a simple probe model called Graph Convolution Simulator (GCS) for interpreting knowledge-enhanced LMs and exposing what kind of knowledge is integrated into these models. We conduct experiments to verify that our GCS model can indeed be used to correctly interpret the KI process, and we use it to analyze two typical knowledge-enhanced LMs: K-Adapter and ERNIE. We find that only a small amount of factual knowledge is captured in these models during integration. While K-Adapter is better at integrating simple relational knowledge, complex relational knowledge is integrated better in ERNIE. We further find that while K-Adapter struggles to integrate time-related knowledge, it successfully integrates knowledge of unpopular entities and relations. Our analysis also show some challenges in KI. In particular, we find simply increasing the size of the KI corpus may not lead to better KI and more fundamental advances may be needed."}}
{"id": "AMRs2w_Ntz", "cdate": 1609459200000, "mdate": 1679905716978, "content": {"title": "Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach", "abstract": ""}}
{"id": "2ogEpiRDHya", "cdate": 1609459200000, "mdate": 1679905717035, "content": {"title": "Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach", "abstract": ""}}
{"id": "D2spN0uwah", "cdate": 1577836800000, "mdate": 1679905717015, "content": {"title": "Understanding Graph Neural Networks from Graph Signal Denoising Perspectives", "abstract": ""}}
