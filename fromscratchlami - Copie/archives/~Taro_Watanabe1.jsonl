{"id": "NY1MYr48Ka", "cdate": 1675834966109, "mdate": 1675834966109, "content": {"title": "Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection", "abstract": "Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of data quality and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise."}}
{"id": "ybvaAS_lUl", "cdate": 1672531200000, "mdate": 1706004447618, "content": {"title": "Findings of the Word-Level AutoCompletion Shared Task in WMT 2023", "abstract": "Lemao Liu, Francisco Casacuberta, George Foster, Guoping Huang, Philipp Koehn, Geza Kovacs, Shuming Shi, Taro Watanabe, Chengqing Zong. Proceedings of the Eighth Conference on Machine Translation. 2023."}}
{"id": "vNqb8Xoo6Om", "cdate": 1672531200000, "mdate": 1700219345418, "content": {"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "abstract": "In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V&L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks."}}
{"id": "oQJoqG-QAt", "cdate": 1672531200000, "mdate": 1706312793931, "content": {"title": "Japanese Lexical Complexity for Non-Native Readers: A New Dataset", "abstract": ""}}
{"id": "nyYRouEI3Y", "cdate": 1672531200000, "mdate": 1706312793920, "content": {"title": "Second Language Acquisition of Neural Language Models", "abstract": ""}}
{"id": "ln12EZMtYv", "cdate": 1672531200000, "mdate": 1706312793893, "content": {"title": "Japanese Lexical Complexity for Non-Native Readers: A New Dataset", "abstract": "Lexical complexity prediction (LCP) is the task of predicting the complexity of words in a text on a continuous scale. It plays a vital role in simplifying or annotating complex words to assist readers. To study lexical complexity in Japanese, we construct the first Japanese LCP dataset. Our dataset provides separate complexity scores for Chinese/Korean annotators and others to address the readers' L1-specific needs. In the baseline experiment, we demonstrate the effectiveness of a BERT-based system for Japanese LCP."}}
{"id": "knrjui8PYL3", "cdate": 1672531200000, "mdate": 1700219345575, "content": {"title": "Model-based Subsampling for Knowledge Graph Completion", "abstract": "Subsampling is effective in Knowledge Graph Embedding (KGE) for reducing overfitting caused by the sparsity in Knowledge Graph (KG) datasets. However, current subsampling approaches consider only frequencies of queries that consist of entities and their relations. Thus, the existing subsampling potentially underestimates the appearance probabilities of infrequent queries even if the frequencies of their entities or relations are high. To address this problem, we propose Model-based Subsampling (MBS) and Mixed Subsampling (MIX) to estimate their appearance probabilities through predictions of KGE models. Evaluation results on datasets FB15k-237, WN18RR, and YAGO3-10 showed that our proposed subsampling methods actually improved the KG completion performances for popular KGE models, RotatE, TransE, HAKE, ComplEx, and DistMult."}}
{"id": "hN51rwvmyt", "cdate": 1672531200000, "mdate": 1706312793905, "content": {"title": "Arukikata Travelogue Dataset with Geographic Entity Mention, Coreference, and Link Annotation", "abstract": "Geoparsing is a fundamental technique for analyzing geo-entity information in text. We focus on document-level geoparsing, which considers geographic relatedness among geo-entity mentions, and presents a Japanese travelogue dataset designed for evaluating document-level geoparsing systems. Our dataset comprises 200 travelogue documents with rich geo-entity information: 12,171 mentions, 6,339 coreference clusters, and 2,551 geo-entities linked to geo-database entries."}}
{"id": "cBVJkOQEADb", "cdate": 1672531200000, "mdate": 1700219345347, "content": {"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "abstract": ""}}
{"id": "Zm0uRsNjp3", "cdate": 1672531200000, "mdate": 1706312794036, "content": {"title": "Arukikata Travelogue Dataset", "abstract": "We have constructed Arukikata Travelogue Dataset and released it free of charge for academic research. This dataset is a Japanese text dataset with a total of over 31 million words, comprising 4,672 Japanese domestic travelogues and 9,607 overseas travelogues. Before providing our dataset, there was a scarcity of widely available travelogue data for research purposes, and each researcher had to prepare their own data. This hinders the replication of existing studies and fair comparative analysis of experimental results. Our dataset enables any researchers to conduct investigation on the same data and to ensure transparency and reproducibility in research. In this paper, we describe the academic significance, characteristics, and prospects of our dataset."}}
