{"id": "YxlxXSLJlH", "cdate": 1640995200000, "mdate": 1681650209219, "content": {"title": "A Convex Formulation for Graph Convolutional Training: Two Layer Case", "abstract": ""}}
{"id": "44sjnbket55", "cdate": 1617684336555, "mdate": null, "content": {"title": "Neural Message Passing for Multi-Relational Ordered and Recursive Hypergraphs", "abstract": "Message passing neural network (MPNN) has recently emerged as a successful framework by achieving state-of-the-art performances on many graph-based learning tasks. MPNN has also recently been extended to multi-relational graphs (each edge is labelled), and hypergraphs (each edge can connect any number of vertices). However, in real-world datasets involving text and knowledge, relationships are much more complex in which hyperedges can be multi-relational, recursive, and ordered. Such structures present several unique challenges because it is not clear how to adapt MPNN to variable-sized hyperedges in them.\nIn this work, we first unify exisiting MPNNs on different structures into G-MPNN (Generalised MPNN) framework. Motivated by real-world datasets, we then propose a novel extension of the framework, MPNN-R (MPNN-Recursive) to handle recursively-structured data. Experimental results demonstrate the effectiveness of proposed G-MPNN and MPNN-R."}}
{"id": "vfzpuoASYaB", "cdate": 1609459200000, "mdate": 1664493951897, "content": {"title": "Knowledge Base Question Answering through Recursive Hypergraphs", "abstract": ""}}
{"id": "Ud6lRldVD-q", "cdate": 1609459200000, "mdate": 1664493951893, "content": {"title": "Graph Neural Networks for Soft Semi-Supervised Learning on Hypergraphs", "abstract": "Graph-based semi-supervised learning (SSL) assigns labels to initially unlabelled vertices in a graph. Graph neural networks (GNNs), esp. graph convolutional networks (GCNs), are at the core of the current-state-of-the art models for graph-based SSL problems. GCNs have recently been extended to undirected hypergraphs in which relationships go beyond pairwise associations. There is a need to extend GCNs to directed hypergraphs which represent more expressively many real-world data sets such as co-authorship networks and recommendation networks. Furthermore, labels of interest in these applications are most naturally represented by probability distributions. Motivated by these needs, in this paper, we propose a novel GNN-based method for directed hypergraphs, called Directed Hypergraph Network (DHN) for semi-supervised learning of probability distributions (Soft SSL). A key contribution of this paper is to establish generalisation error bounds for GNN-based soft SSL. In fact, our theoretical analysis is quite general and has straightforward applicability to DHN as well as to existing hypergraph methods. We demonstrate the effectiveness of our method through detailed experimentation on real-world datasets. We have made the code available."}}
{"id": "uD--uOKXHy7", "cdate": 1577836800000, "mdate": 1664493951670, "content": {"title": "Neural Message Passing for Multi-Relational Ordered and Recursive Hypergraphs", "abstract": "Message passing neural network (MPNN) has recently emerged as a successful framework by achieving state-of-the-art performances on many graph-based learning tasks. MPNN has also recently been extended to multi-relational graphs (each edge is labelled), and hypergraphs (each edge can connect any number of vertices). However, in real-world datasets involving text and knowledge, relationships are much more complex in which hyperedges can be multi-relational, recursive, and ordered. Such structures present several unique challenges because it is not clear how to adapt MPNN to variable-sized hyperedges in them.<br /> In this work, we first unify exisiting MPNNs on different structures into G-MPNN (Generalised MPNN) framework. Motivated by real-world datasets, we then propose a novel extension of the framework, MPNN-R (MPNN-Recursive) to handle recursively-structured data. Experimental results demonstrate the effectiveness of proposed G-MPNN and MPNN-R."}}
{"id": "gCIbeOOSoK", "cdate": 1577836800000, "mdate": 1664493951676, "content": {"title": "NHP: Neural Hypergraph Link Prediction", "abstract": "Link prediction insimple graphs is a fundamental problem in which new links between vertices are predicted based on the observed structure of the graph. However, in many real-world applications, there is a need to model relationships among vertices that go beyond pairwise associations. For example, in a chemical reaction, relationship among the reactants and products is inherently higher-order. Additionally, there is a need to represent the direction from reactants to products. Hypergraphs provide a natural way to represent such complex higher-order relationships. Graph Convolutional Network (GCN) has recently emerged as a powerful deep learning-based approach for link prediction over simple graphs. However, their suitability for link prediction in hypergraphs is underexplored -- we fill this gap in this paper and propose Neural Hyperlink Predictor (NHP). NHP adapts GCNs for link prediction in hypergraphs. We propose two variants of NHP -- NHP-U and NHP-D -- for link prediction over undirected and directed hypergraphs, respectively. To the best of our knowledge, NHP-D is the first-ever method for link prediction over directed hypergraphs. An important feature of NHP is that it can also be used for hyperlinks in which dissimilar vertices interact (e.g. acids reacting with bases). Another attractive feature of NHP is that it can be used to predict unseen hyperlinks at test time (inductive hyperlink prediction). Through extensive experiments on multiple real-world datasets, we show NHP's effectiveness."}}
{"id": "71AqGWrlnI3", "cdate": 1577836800000, "mdate": 1664493951701, "content": {"title": "Graph-based Deep Learning in Natural Language Processing", "abstract": "This tutorial aims to introduce recent advances in graph-based deep learning techniques such as Graph Convolutional Networks (GCNs) for Natural Language Processing (NLP). It provides a brief introduction to deep learning methods on non-Euclidean domains such as graphs and justifies their relevance in NLP. It then covers recent advances in applying graph-based deep learning methods for various NLP tasks, such as semantic role labeling, machine translation, relationship extraction, and many more."}}
{"id": "rkg6FgrtPB", "cdate": 1569439876732, "mdate": null, "content": {"title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing."}}
{"id": "ryestJBKPB", "cdate": 1569439619360, "mdate": null, "content": {"title": "Graph Neural Networks for Soft Semi-Supervised Learning on Hypergraphs", "abstract": "Graph-based semi-supervised learning (SSL) assigns labels to initially unlabelled vertices in a graph.\nGraph neural networks (GNNs), esp. graph convolutional networks (GCNs), inspired the current-state-of-the art models for graph-based SSL problems.\nGCNs inherently assume that the labels of interest are numerical or categorical variables.\nHowever, in many real-world applications such as co-authorship networks, recommendation networks, etc., vertex labels can be naturally represented by probability distributions or histograms.\nMoreover, real-world network datasets have complex relationships going beyond pairwise associations.\nThese relationships can be modelled naturally and flexibly by hypergraphs.\nIn this paper, we explore GNNs for graph-based SSL of histograms.\nMotivated by complex relationships (those going beyond pairwise) in real-world networks, we propose a novel method for directed hypergraphs.\nOur work builds upon existing works on graph-based SSL of histograms derived from the theory of optimal transportation.\nA key contribution of this paper is to establish generalisation error bounds for a one-layer GNN within the framework of algorithmic stability.\nWe also demonstrate our proposed methods' effectiveness through detailed experimentation on real-world data.\nWe have made the code available."}}
{"id": "B1eh4mYIUB", "cdate": 1568211763780, "mdate": null, "content": {"title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal\u2019s genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task through minibatches. The relative performance of the two alleles of each gene is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. NNE is also tested on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs (replacing backprop) inspired by the recent discovery of dopaminergic plasticity. "}}
