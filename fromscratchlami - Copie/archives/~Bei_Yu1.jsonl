{"id": "I0HngR6qvbp", "cdate": 1640995200000, "mdate": 1653497219362, "content": {"title": "Advice Giving in Medical Research Literature", "abstract": "Evidence-based health advice, i.e., clinical or policy recommendations, contributes greatly to guiding medical practice and public health policies. However, whether to give health advice, especially based on individual study results, is a controversial issue: on the one hand, such advice may lack a comprehensive review of all evidence and alternative practices; on the other hand, researchers have been encouraged to translate research findings to actionable practice. To date, limited attention has been given to understanding how and where health researchers give advice in their publications, which could be critical for assessing the quality of health advice in medical literature. In this study, we conducted a content analysis of all 4,866 sentences in the abstract and discussion sections in 100 individual study papers (both randomized controlled trials and observational studies), labeling each sentence as either \u201cstrong advice\u201d, \u201cweak advice\u201d, or \u201cno advice\u201d. We found that most authors gave advice in individual studies, but they rarely gave advice in abstract only. The common practice is either to give advice in discussion sections only, or in both abstracts and discussions. When giving advice in both sections, authors tended to give weak and non-specific advice in abstracts, while using more sentences in the discussion sections to give strong and more specific advice, adding conditions required for the recommendations. The result suggests that most researchers support giving advice in individual studies, but they are generally cautious in giving advice in abstracts."}}
{"id": "jxqt-lssajX", "cdate": 1609459200000, "mdate": 1653497219403, "content": {"title": "News2PubMed: A Browser Extension for Linking Health News to Medical Literature", "abstract": "This demo system presents a browser extension that allows the reader of a health news article to quickly retrieve related medical/health research papers. This system can help news editors and readers fact-check health news for incorrect or exaggerated claims, such as making causal claims from correlational findings or inference of animal studies to humans. Linking health news to the original research papers is not a trivial task, as links are largely missing in science news reports. To link health news to medical literature, our system includes a new named-entity recognition function to extract journal names, and a new Elasticsearch-based search engine to incorporate rich metadata into the search strategy. This paper also introduces a new dataset for evaluating the performance of the proposed search system."}}
{"id": "W7qnP6Uccob", "cdate": 1609459200000, "mdate": 1653497219380, "content": {"title": "Detecting Health Advice in Medical Research Literature", "abstract": "Yingya Li, Jun Wang, Bei Yu. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "75fuAZzQ8Te", "cdate": 1609459200000, "mdate": 1653497219383, "content": {"title": "Interventions to support consumer evaluation of online health information credibility: A scoping review", "abstract": ""}}
{"id": "aFN0keBCU6O", "cdate": 1577836800000, "mdate": 1653497219375, "content": {"title": "Information Quality of Reddit Link Posts on Health News", "abstract": "Inaccuracy has been a common problem in news coverage of scientific research. This problem has been particularly prevalent in health research news. Health research news usually spreads from research publications and press releases to news and social media. In this study we examined the information quality of the Reddit link posts that introduce health news stories. We developed a coding schema to annotate the inaccurate information in a sample of 250 link posts on health research news within the Reddit community r/Health in 2018. The result shows that most link posts simply copied the original news headlines verbatim, while some paraphrased the news stories by adding, deleting, replacing, and combining content. We found that 12 paraphrased link posts contained inaccurate information that may mislead the readers. The most common type of inaccuracy is exaggeration resulted from changing the original speculative claims to direct causal statements by removing the modal verbs such as \u201cmay\u201d and \u201cmight\u201d. The result shows that although the link posts of health news were generally faithful to the original news stories, exaggerated claims may lead to false hope for researchers and patients."}}
{"id": "3KwF-LO63Rb", "cdate": 1577836800000, "mdate": 1653497219375, "content": {"title": "Measuring Correlation-to-Causation Exaggeration in Press Releases", "abstract": "Bei Yu, Jun Wang, Lu Guo, Yingya Li. Proceedings of the 28th International Conference on Computational Linguistics. 2020."}}
{"id": "yoSEOGqXNIL", "cdate": 1546300800000, "mdate": 1653497219381, "content": {"title": "Detecting Causal Language Use in Science Findings", "abstract": "Bei Yu, Yingya Li, Jun Wang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
{"id": "cWBCArkMrMt", "cdate": 1546300800000, "mdate": 1653497219384, "content": {"title": "Toward Training and Assessing Reproducible Data Analysis in Data Science Education", "abstract": "Reproducibility is a cornerstone of scientific research. Data science is not an exception. In recent years scientists were concerned about a large number of irreproducible studies. Such reproducibility crisis in science could severely undermine public trust in science and science-based public policy. Recent efforts to promote reproducible research mainly focused on matured scientists and much less on student training. In this study, we conducted action research on students in data science to evaluate to what extent students are ready for communicating reproducible data analysis. The results show that although two-thirds of the students claimed they were able to reproduce results in peer reports, only one-third of reports provided all necessary information for replication. The actual replication results also include conflicting claims; some lacked comparisons of original and replication results, indicating that some students did not share a consistent understanding of what reproducibility means and how to report replication results. The findings suggest that more training is needed to help data science students communicating reproducible data analysis."}}
{"id": "7Qkxj76j26g", "cdate": 1546300800000, "mdate": 1653497219375, "content": {"title": "Identifying Finding Sentences in Conclusion Subsections of Biomedical Abstracts", "abstract": "Segmenting scientific abstracts and full-text based on their rhetorical function is an essential task in text classification. Small rhetorical segments can be useful for fine-grained literature search, summarization, and comparison. Current effort has been focusing on segmenting documents into general sections such as introduction, method, and conclusion, and much less on the roles of individual sentences within the segments. For example, not all sentences in the conclusion section are describing research findings. In this work, we developed rule-based and machine learning methods and compared their performance in identifying the finding sentences in conclusion subsections of biomedical abstracts. 1100 conclusion subsections with observational and randomized clinical trials study designs covering five common health topics were sampled from PubMed to develop and evaluate the methods. The rule-based method and the bag-of-words based machine learning method both achieved high accuracy. The better performance by the simple rule-based approach shows that although advanced machine learning approaches could capture the main patterns, human expert may still outperform on such a specialized task."}}
{"id": "6ZKskeenY2P", "cdate": 1546300800000, "mdate": 1653497219375, "content": {"title": "HClaimE: A tool for identifying health claims in health news headlines", "abstract": ""}}
