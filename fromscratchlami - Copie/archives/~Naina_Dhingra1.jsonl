{"id": "eysQXUs2n0k", "cdate": 1640995200000, "mdate": 1682335986963, "content": {"title": "HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders", "abstract": "In this paper, HeadPosr is proposed to predict the head poses using a single RGB image. \\textit{HeadPosr} uses a novel architecture which includes a transformer encoder. In concrete, it consists of: (1) backbone; (2) connector; (3) transformer encoder; (4) prediction head. The significance of using a transformer encoder for HPE is studied. An extensive ablation study is performed on varying the (1) number of encoders; (2) number of heads; (3) different position embeddings; (4) different activations; (5) input channel size, in a transformer used in HeadPosr. Further studies on using: (1) different backbones, (2) using different learning rates are also shown. The elaborated experiments and ablations studies are conducted using three different open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000, and BIWI datasets. Experiments illustrate that \\textit{HeadPosr} outperforms all the state-of-art methods including both the landmark-free and the others based on using landmark or depth estimation on the AFLW2000 dataset and BIWI datasets when trained with 300W-LP. It also outperforms when averaging the results from the compared datasets, hence setting a benchmark for the problem of HPE, also demonstrating the effectiveness of using transformers over the state-of-the-art."}}
{"id": "aboTV_yiA5", "cdate": 1640995200000, "mdate": 1682335987092, "content": {"title": "LwPosr: Lightweight Efficient Fine Grained Head Pose Estimation", "abstract": "This paper presents a lightweight network for head pose estimation (HPE) task. While previous approaches rely on convolutional neural networks, the proposed network LwPosr uses mixture of depthwise separable convolutional (DSC) and transformer encoder layers which are structured in two streams and three stages to provide fine-grained regression for predicting head poses. The quantitative and qualitative demonstration is provided to show that the proposed network is able to learn head poses efficiently while using less parameter space. Extensive ablations are conducted using three open-source datasets namely 300W-LP, AFLW2000, and BIWI datasets. To our knowledge, (1) LwPosr is the lightest network proposed for estimating head poses compared to both keypoints-based and keypoints-free approaches; (2) it sets a benchmark for both overperforming the previous lightweight network on mean absolute error and on reducing number of parameters; (3) it is first of its kind to use mixture of DSCs and transformer encoders for HPE. This approach is suitable for mobile devices which require lightweight networks."}}
{"id": "RXmGZ0Csy3", "cdate": 1640995200000, "mdate": 1682335986949, "content": {"title": "Scene Understanding and Gesture Recognition for Human-Machine Interaction", "abstract": ""}}
{"id": "M6cUJjDSFX", "cdate": 1640995200000, "mdate": 1682335986817, "content": {"title": "Language-Attention Modular-Network for Relational Referring Expression Comprehension in Videos", "abstract": "Referring expression (RE) for video domain describes the video using a natural language expression. Relational RE comprehension in a video domain localizes an object in relation to a distinguishing context object. Unlike object grounding in videos using REs, not much work has been done in videos using relational REs. In this paper, we focus on (1) relational RE comprehension for videos, and (2) demonstrating the significance of attention for the task. We propose a novel modular network based approach for relational RE comprehension in highly ambiguous settings for videos. We show the significance of the language attention in modular approach by: (1) Using two different networks, i.e., modATN consisting of attention mechanism, visual modules, and a natural language expression input, and modSTR consisting of visual modules, and structured input (subject, subject adjective, object, object adjective, action, relation); (2) Introducing a new dataset having structured RE for relation RE comprehension task in modSTR. Finally, we propose an optimised modular network that outperforms and shows significant improvements over the baseline networks."}}
{"id": "KCCQnJTNj_", "cdate": 1640995200000, "mdate": 1682335986963, "content": {"title": "LwPosr: Lightweight Efficient Fine-Grained Head Pose Estimation", "abstract": "This paper presents a lightweight network for head pose estimation (HPE) task. While previous approaches rely on convolutional neural networks, the proposed network \\textit{LwPosr} uses mixture of depthwise separable convolutional (DSC) and transformer encoder layers which are structured in two streams and three stages to provide fine-grained regression for predicting head poses. The quantitative and qualitative demonstration is provided to show that the proposed network is able to learn head poses efficiently while using less parameter space. Extensive ablations are conducted using three open-source datasets namely 300W-LP, AFLW2000, and BIWI datasets. To our knowledge, (1) \\textit{LwPosr} is the lightest network proposed for estimating head poses compared to both keypoints-based and keypoints-free approaches; (2) it sets a benchmark for both overperforming the previous lightweight network on mean absolute error and on reducing number of parameters; (3) it is first of its kind to use mixture of DSCs and transformer encoders for HPE. This approach is suitable for mobile devices which require lightweight networks."}}
{"id": "ZMyiWJzDVQ", "cdate": 1609459200000, "mdate": 1682335986963, "content": {"title": "HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders", "abstract": "Head pose estimation (HPE) is the task of estimating head pose given an RGB image, video, or RGB-D data. In this paper, a network, HeadPosr is proposed to predict the head poses using a single RGB image. HeadPosr uses a novel architecture which includes a transformer encoder. In concrete, it consists of: (1) backbone; (2) connector; (3) transformer encoder; (4) prediction head. The significance of using a transformer encoder for HPE is studied. An extensive ablation study is performed on varying the (1) number of encoders; (2) number of heads; (3) different position embeddings; (4) different activations; (5) input channel size, in a transformer used in HeadPosr. Further studies on using: (1) different backbones, (2) using different learning rates are also shown. The elaborated experiments and ablations studies are conducted using three different open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000, and BIWI datasets. Experiments illustrate that HeadPosr outperforms all the state-of-art methods including both the landmark-free and the others based on using landmark or depth estimation on the AFLW2000 dataset and BIWI datasets when trained with 300W-LP. It also outperforms when averaging the results from the compared datasets, hence setting a benchmark for the problem of HPE, also demonstrating the effectiveness of using transformers over the state-of-the-art."}}
{"id": "SPcoJqtGUcy", "cdate": 1609459200000, "mdate": 1682335987094, "content": {"title": "Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network", "abstract": "We present Border-SegGCN, a novel architecture to improve semantic segmentation by refining the border outline using graph convolutional networks (GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as a base network to have pre-segmented output. This output is converted into a graphical structure and fed into the GCN to improve the border pixel prediction of the presegmented output. We explored and studied the factors such as border thickness, number of edges for a node, and the number of features to be fed into the GCN by performing experiments. We demonstrate the effectiveness of the BorderSegGCN on the CamVid and Carla dataset, achieving a test set performance of 81.96% without any post-processing on CamVid dataset. It is higher than the reported state of the art mIoU achieved on CamVid dataset by 0.404%."}}
{"id": "SIeqpYRo6u", "cdate": 1609459200000, "mdate": 1682335987093, "content": {"title": "BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation", "abstract": "Scene graphs are nodes and edges consisting of objects and object-object relationships, respectively. Scene graph generation (SGG) aims to identify the objects and their relationships. We propose a bidirectional GRU (BiGRU) transformer network (BGT-Net) for the scene graph generation for images. This model implements novel object-object communication to enhance the object information using a BiGRU layer. Thus, the information of all objects in the image is available for the other objects, which can be leveraged later in the object prediction step. This object information is used in a transformer encoder to predict the object class as well as to create object-specific edge information via the use of another transformer encoder. To handle the dataset bias induced by the long-tailed relationship distribution, softening with a log-softmax function and adding a bias adaptation term to regulate the bias for every relation prediction individually showed to be an effective approach. We conducted an elaborate study on experiments and ablations using open-source datasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection datasets, demonstrating the effectiveness of the proposed model over state of the art."}}
{"id": "NN_F_nH4Jq", "cdate": 1609459200000, "mdate": 1682335987107, "content": {"title": "Detection and Localisation of Pointing, Pairing and Grouping Gestures for Brainstorming Meeting Applications", "abstract": "The detection of gestures and their interpretation is crucial for blind and visually impaired people (BVIP). In a card-based brainstorming meeting, sighted users use non-verbal communication when referring to cards on a common workspace using pointing, grouping, or pairing gestures. While sighted users could easily interpret such gestures, they remain inaccessible to BVIP. Thus, there is a need for capturing, interpreting and translating gestures for BVIP. To address this problem, we developed a pointing gesture detection system using Unity with the SteamVR Plugin and HTC Vive. HTC\u2019s trackers are attached to a user\u2019s hands to measure the hand position in 3D space. With pointing gestures, a user controls a virtual ray that will intersect with a virtual whiteboard. This virtual whiteboard is invisible to the sighted users, but its position and size corresponds to a physical whiteboard. The intersection of the ray with the virtual whiteboard is calculated, resulting in a pointing trajectory on it. The shape of the trajectory is analyzed to determine, which artifacts are selected by the pointing gesture. A pointing gesture is detected when a user is pointing at a card on the screen and then ending the gesture by pointing outside of the screen. A pairing gesture is detected when pointing at one artifact and then on another one before leaving the screen. The grouping gesture is detected when performing an encircling gesture around multiple artifacts before leaving the screen."}}
{"id": "JvG-fQSiCvn", "cdate": 1609459200000, "mdate": 1682335987124, "content": {"title": "Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network", "abstract": "We present Border-SegGCN, a novel architecture to improve semantic segmentation by refining the border outline using graph convolutional networks (GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as a base network to have pre-segmented output. This output is converted into a graphical structure and fed into the GCN to improve the border pixel prediction of the pre-segmented output. We explored and studied the factors such as border thickness, number of edges for a node, and the number of features to be fed into the GCN by performing experiments. We demonstrate the effectiveness of the Border-SegGCN on the CamVid and Carla dataset, achieving a test set performance of 81.96% without any post-processing on CamVid dataset. It is higher than the reported state of the art mIoU achieved on CamVid dataset by 0.404%"}}
