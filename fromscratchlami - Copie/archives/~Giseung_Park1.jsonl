{"id": "ZQBTRBVnLX", "cdate": 1640995200000, "mdate": 1682319373290, "content": {"title": "Blockwise Sequential Model Learning for Partially Observable Reinforcement Learning", "abstract": "This paper proposes a new sequential model learning architecture to solve partially observable Markov decision problems. Rather than compressing sequential information at every timestep as in conventional recurrent neural network-based methods, the proposed architecture generates a latent variable in each data block with a length of multiple timesteps and passes the most relevant information to the next block for policy optimization. The proposed blockwise sequential model is implemented based on self-attention, making the model capable of detailed sequential learning in partial observable settings. The proposed model builds an additional learning network to efficiently implement gradient estimation by using self-normalized importance sampling, which does not require the complex blockwise input data reconstruction in the model learning. Numerical results show that the proposed method significantly outperforms previous methods in various partially observable environments."}}
{"id": "A8VDD4fq8K-", "cdate": 1609459200000, "mdate": 1682319372916, "content": {"title": "Blockwise Sequential Model Learning for Partially Observable Reinforcement Learning", "abstract": "This paper proposes a new sequential model learning architecture to solve partially observable Markov decision problems. Rather than compressing sequential information at every timestep as in conventional recurrent neural network-based methods, the proposed architecture generates a latent variable in each data block with a length of multiple timesteps and passes the most relevant information to the next block for policy optimization. The proposed blockwise sequential model is implemented based on self-attention, making the model capable of detailed sequential learning in partial observable settings. The proposed model builds an additional learning network to efficiently implement gradient estimation by using self-normalized importance sampling, which does not require the complex blockwise input data reconstruction in the model learning. Numerical results show that the proposed method significantly outperforms previous methods in various partially observable environments."}}
{"id": "4emQEegFhSy", "cdate": 1601308128150, "mdate": null, "content": {"title": "Adaptive Multi-model Fusion Learning for Sparse-Reward Reinforcement Learning", "abstract": "In this paper, we consider intrinsic reward generation for sparse-reward reinforcement learning based on model prediction errors. In typical model-prediction-error-based intrinsic reward generation, an agent has a learning model for the underlying environment. Then intrinsic reward is designed as the error between the model prediction and the actual outcome of the environment, based on the fact that for less-visited or non-visited states, the learned model yields larger prediction errors, promoting exploration helpful for reinforcement learning. This paper generalizes this model-prediction-error-based intrinsic reward generation method to multiple prediction models. We propose a new adaptive fusion method relevant to the multiple-model case, which learns optimal prediction-error fusion across the learning phase to enhance the overall learning performance.  Numerical results show that for representative locomotion tasks, the proposed intrinsic reward generation method outperforms most of the previous methods, and the gain is significant in some tasks."}}
{"id": "rtOdC5Mu3qJ", "cdate": 1577836800000, "mdate": 1681877213189, "content": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners..."}}
{"id": "fIAmimu5lsZ", "cdate": 1577836800000, "mdate": 1682319372763, "content": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged range enable faster and better policy search. Monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm. Numerical results show that the constructed algorithm outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment."}}
{"id": "SyxJU64twr", "cdate": 1569439046684, "mdate": null, "content": {"title": "Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning", "abstract": "In this paper, a new intrinsic reward generation method for sparse-reward reinforcement learning is proposed based on an ensemble of dynamics models. In the proposed method, the mixture of multiple dynamics models is used to approximate the true unknown transition probability, and the intrinsic reward is designed as the minimum of the surprise seen from each dynamics model to the mixture of the dynamics models. In order to show the effectiveness of the proposed intrinsic reward generation method, a working algorithm is constructed by combining the proposed intrinsic reward generation method with the proximal policy optimization (PPO) algorithm. Numerical results show that for representative locomotion tasks, the proposed model-ensemble-based intrinsic reward generation method outperforms the previous methods based on a single dynamics model."}}
{"id": "rJeINp4KwH", "cdate": 1569439022497, "mdate": null, "content": {"title": "Population-Guided Parallel Policy Search for Reinforcement Learning", "abstract": "In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged  range enable faster and better policy search, and monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, and numerical results show that the constructed P3S-TD3 outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment."}}
{"id": "B1eEKi0qYQ", "cdate": 1538087803608, "mdate": null, "content": {"title": "Interactive Parallel Exploration for Reinforcement Learning in Continuous Action Spaces", "abstract": "In this paper, a new interactive parallel learning scheme is proposed to enhance the performance of off-policy continuous-action reinforcement learning.  In the proposed interactive parallel learning scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The information of the best policy  is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search space by the multiple learners. The guidance by the previous best policy and the enlarged search space by the proposed interactive parallel learning scheme enable faster and better policy search in the policy parameter space. Working algorithms are constructed by  applying the proposed interactive parallel learning scheme to several off-policy reinforcement learning algorithms such as  the twin delayed deep deterministic (TD3) policy gradient algorithm and the soft actor-critic (SAC) algorithm, and numerical results show that the constructed IPE-enhanced algorithms outperform most of the current state-of-the-art reinforcement learning algorithms for continuous action control."}}
{"id": "JwEz_PUyUC8", "cdate": 1514764800000, "mdate": 1682319372899, "content": {"title": "5G K-Simulator: Link Level Simulator for 5G", "abstract": "In this paper, 5G K-SimLink is introduced. 5G K-SimLink is a link level simulator for 5G. 5G K-SimLink implements the main features of 5G, and has a modular structure. The modularity of each link function of 5G K-SimLink makes modification to user's need easy and flexible."}}
