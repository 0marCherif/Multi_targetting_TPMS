{"id": "_qd8bm6Q5ef", "cdate": 1684224226465, "mdate": 1684224226465, "content": {"title": "Optimizing Sequential Experimental Design with Deep Reinforcement Learning", "abstract": "Bayesian approaches developed to solve the optimal design of sequential experiments are mathematically elegant but computationally challenging. Recently, techniques using amortization have been proposed to make these Bayesian approaches practical, by training a parameterized policy that proposes designs efficiently at deployment time. However, these methods may not sufficiently explore the design space, require access to a differentiable probabilistic model and can only optimize over continuous design spaces. Here, we address these limitations by showing that the problem of optimizing policies can be reduced to solving a Markov decision process (MDP). We solve the equivalent MDP with modern deep reinforcement learning techniques. Our experiments show that our approach is also computationally efficient at deployment time and exhibits state-of-the-art performance on both continuous and discrete design spaces, even when the probabilistic model is a black box.\n"}}
{"id": "aWYdvQK-fTy", "cdate": 1684224182095, "mdate": 1684224182095, "content": {"title": "Bayesian Curiosity for Efficient Exploration in Reinforcement Learning", "abstract": "Balancing exploration and exploitation is a fundamental part of reinforcement learning, yet most state-of-theart algorithms use a naive exploration protocol like \u000f-greedy.\nThis contributes to the problem of high sample complexity, as\nthe algorithm wastes effort by repeatedly visiting parts of the\nstate space that have already been explored. We introduce a\nnovel method based on Bayesian linear regression and latent\nspace embedding to generate an intrinsic reward signal that\nencourages the learning agent to seek out unexplored parts of\nthe state space. This method is computationally efficient, simple\nto implement, and can extend any state-of-the-art reinforcement\nlearning algorithm. We evaluate the method on a range of\nalgorithms and challenging control tasks, on both simulated\nand physical robots, demonstrating how the proposed method\ncan significantly improve sample complexity."}}
{"id": "yrr7xZcQvd", "cdate": 1684224137758, "mdate": 1684224137758, "content": {"title": "Improving reinforcement learning pre-training with variational dropout", "abstract": "Improving reinforcement learning pre-training with variational dropout"}}
{"id": "eDut2cYKs1", "cdate": 1684224090375, "mdate": 1684224090375, "content": {"title": "Learning from Demonstration without Demonstrations", "abstract": "State-of-the-art reinforcement learning (RL) algorithms suffer from high sample complexity, particularly in the sparse reward case. A popular strategy for mitigating this problem is to learn control policies by imitating a set of expert demonstrations. The drawback of such approaches is that an expert needs to produce demonstrations, which may be costly in practice. To address this shortcoming, we propose Probabilistic Planning for Demonstration Discovery (P2D2), a technique for automatically discovering demonstrations without access to an expert. We formulate discovering demonstrations as a search problem and leverage widely-used planning algorithms such as Rapidly-exploring Random Tree to find demonstration trajectories. These demonstrations are used to initialize a policy, then refined by a generic RL algorithm. We provide theoretical guarantees of P2D2 finding successful trajectories, as well as bounds for its sampling complexity. We experimentally demonstrate the method outperforms classic and intrinsic exploration RL techniques in a range of classic control and robotics tasks, requiring only a fraction of exploration samples and achieving better asymptotic performance.\n"}}
