{"id": "erHHjuxwojZ", "cdate": 1672765400216, "mdate": 1672765400216, "content": {"title": "EDICT: Exact Diffusion Inversion via Coupled Transformations", "abstract": "Finding an initial noise vector that produces an input\nimage when fed into the diffusion process (known as inversion) is an important problem in denoising diffusion models (DDMs), with applications for real image editing. The\nstate-of-the-art approach for real image editing with inversion uses denoising diffusion implicit models (DDIMs [28])\nto deterministically noise the image to the intermediate\nstate along the path that the denoising would follow given\nthe original conditioning. However, DDIM inversion for\nreal images is unstable as it relies on local linearization assumptions, which result in the propagation of errors, leading to incorrect image reconstruction and loss of content. To\nalleviate these problems, we propose Exact Diffusion Inversion via Coupled Transformations (EDICT), an inversion\nmethod that draws inspiration from affine coupling layers.\nEDICT enables mathematically exact inversion of real and\nmodel-generated images by maintaining two coupled noise\nvectors which are used to invert each other in an alternating fashion. Using Stable Diffusion [24], a state-of-the-art\nlatent diffusion model, we demonstrate that EDICT successfully reconstructs real images with high fidelity. On complex\nimage datasets like MS-COCO, EDICT reconstruction significantly outperforms DDIM, improving the mean square\nerror of reconstruction by a factor of two. Using noise\nvectors inverted from real images, EDICT enables a wide\nrange of image edits\u2014from local and global semantic edits to image stylization\u2014while maintaining fidelity to the\noriginal image structure. EDICT requires no model training/finetuning, prompt tuning, or extra data and can be\ncombined with any pretrained DDM. Code is available at\nhttps://github.com/salesforce/EDICT."}}
{"id": "Nc7EsfpZ7C", "cdate": 1665285245313, "mdate": null, "content": {"title": "Designing active and thermostable enzymes with sequence-only predictive models", "abstract": "Data-driven models of fitness can be useful in designing novel proteins with desired properties, but many questions remain regarding how and in what settings they should be used.  Here, we ask: How can we use predictive models of protein fitness, whose predictions we might not always trust, to design protein sequences enhanced for multiple fitness functions?  We propose a general approach for doing so, and apply it to design novel variants of eight different acylphosphatase and lysozyme wild types, intended to be more thermostable and at least as catalytically active as the wild types.  Our method does not require a structure, experimental measurements of activity, curation of homologous sequences, or family-specific thermostability data.  Experimental characterizations of our designed sequences, as well as sequences designed by PROSS, a competitive baseline method for improving protein thermostability, are currently underway and forthcoming."}}
{"id": "ZOn4HXehSJ6", "cdate": 1663850026806, "mdate": null, "content": {"title": "ProGen2: Exploring the Boundaries of Protein Language Models", "abstract": "Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional fine-tuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/anonymized-research/progen2."}}
{"id": "adxVPA1VNpr", "cdate": 1652578681778, "mdate": 1652578681778, "content": {"title": "CLIP-Lite: Information Efficient Visual Representation Learning from Textual Annotations", "abstract": "We propose CLIP-Lite, an information efficient method for visual representation learning by feature alignment with textual annotations. Compared to the previously proposed CLIP model, CLIP-Lite requires only one negative image-text sample pair for every positive image-text sample during the optimization of its contrastive learning objective. We accomplish this by taking advantage of an information efficient lower-bound to maximize the mutual information between the two input modalities. This allows CLIP-Lite to be trained with significantly reduced amounts of data and batch sizes while obtaining better performance than CLIP. We evaluate CLIP-Lite by pretraining on the COCO-Captions dataset and testing transfer learning to other datasets. CLIP-Lite obtains a +15.4% mAP absolute gain in performance on Pascal VOC classification, and a +22.1% top-1 accuracy gain on ImageNet, while being comparable or superior to other, more complex, text-supervised models. CLIP-Lite is also superior to CLIP on image and text retrieval, zero-shot classification, and visual grounding. Finally, by performing explicit image-text alignment during representation learning, we show that CLIP-Lite can leverage language semantics to encourage bias-free visual representations that can be used in downstream tasks. "}}
{"id": "hHmtmT58pSL", "cdate": 1632875755622, "mdate": null, "content": {"title": "Don\u2019t throw away that linear head: Few-shot protein fitness prediction with generative models", "abstract": "Predicting the fitness, i.e. functional value, of a protein sequence is an important and challenging task in biology, particularly due to the scarcity of assay-labeled data. Traditional approaches utilize transfer learning from evolutionary data, yet discard useful information from a generative model\u2019s learned probability distribution. We propose generative fitness fine-tuning, termed gf-tuning, to utilize the generative model\u2019s log probabilities as logits for a pairwise ranking loss---allowing for the full distribution learned in unsupervised training to be repurposed for fine-tuning on assay-labeled fitness data. We demonstrate that gf-tuning achieves better performance than existing baselines across a variety of few-shot fitness prediction settings, including both low homology and highly epistatic systems as well as generalizing from single to multiple mutations. Generative fitness finetuning offers an effective strategy for few-shot fitness prediction which could enable advances to better understand and engineer proteins."}}
{"id": "NCDMYD2y5kK", "cdate": 1621630087956, "mdate": null, "content": {"title": "Deep Extrapolation for Attribute-Enhanced Generation", "abstract": "Attribute extrapolation in sample generation is challenging for deep neural networks operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein stability dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training. We release our benchmark tasks and models to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry."}}
{"id": "kvqPFy0hbF", "cdate": 1603141806495, "mdate": null, "content": {"title": "DIME: An Information-Theoretic Difficulty Measure for AI Datasets", "abstract": "Evaluating the relative difficulty of widely-used benchmark datasets across time and across data modalities is important for accurately measuring progress in machine learning. To help tackle this problem, we propose DIME, an information-theoretic DIfficulty MEasure for datasets, based on Fano\u2019s inequality and a neural network estimation of the conditional entropy of the sample-label distribution. DIME can be decomposed into components attributable to the data distribution and the number of samples. DIME can also compute per-class difficulty scores. Through extensive experiments on both vision and language datasets, we show that DIME is well aligned with empirically observed performance of state-of-the-art machine learning models. We hope that DIME can aid future dataset design and model-training strategies."}}
{"id": "H1lNb0NtPH", "cdate": 1569439227826, "mdate": null, "content": {"title": "DIME: AN INFORMATION-THEORETIC DIFFICULTY MEASURE FOR AI DATASETS", "abstract": "Evaluating the relative difficulty of widely-used benchmark datasets across time and across data modalities is important for accurately measuring progress in machine learning.  To help tackle this problem, we proposeDIME, an information-theoretic DIfficulty MEasure for datasets, based on conditional entropy estimation of the sample-label distribution.  Theoretically,  we prove a model-agnostic and modality-agnostic lower bound on the 0-1 error by extending Fano\u2019s inequality to the common supervised learning scenario where labels are discrete and features are continuous. Empirically, we estimate this lower bound using a neural network to compute DIME. DIME can be decomposed into components attributable to the data distribution and the number of samples.  DIME can also compute per-class difficulty scores. Through extensive experiments on both vision and language datasets, we show that DIME is well-aligned with empirically observed performance of state-of-the-art machine learning models. We hope that DIME can aid future dataset design and model-training strategies."}}
{"id": "BJypUGZ0Z", "cdate": 1518730162936, "mdate": null, "content": {"title": "Accelerating Neural Architecture Search using Performance Prediction", "abstract": "Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations."}}
{"id": "HJqk3N1vG", "cdate": 1518451682308, "mdate": null, "content": {"title": "Accelerating Neural Architecture Search using Performance Prediction", "abstract": "Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations."}}
