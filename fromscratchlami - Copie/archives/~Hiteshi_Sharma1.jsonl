{"id": "F7pKvRWU8C", "cdate": 1577836800000, "mdate": 1684360414746, "content": {"title": "Finite Time Guarantees for Continuous State MDPs with Generative Model", "abstract": "In this paper, we present Online Empirical Value Learning (ONEVaL), an `online' reinforcement learning algorithm for continuous MDPs that is `quasi-model-free' (needs a generative/simulation model but not the model per se) that can compute nearly-optimal policies and comes with nonasymptotic performance guarantees including prescriptions on required sample complexity for specified performance bounds. The algorithm relies on use of a `fully' randomized policy that will generate a \u03b2-mixing sample trajectory. It also relies on randomized function approximation in an RKHS for arbitrarily small function approximation error, and an `empirical' estimate of value from the next state by several samples of the next state from the generative model. We demonstrate its' good numerical performance on some benchmark problems. We note that the algorithm requires no hyper-parameter tuning, and is also robust to other concerns that seem to plague Deep RL algorithms."}}
{"id": "F-lpnYBoIv0", "cdate": 1577836800000, "mdate": 1684360414746, "content": {"title": "A Universal Empirical Dynamic Programming Algorithm for Continuous State MDPs", "abstract": "We propose universal randomized function approximation-based empirical value learning (EVL) algorithms for Markov decision processes. The \u201cempirical\u201d nature comes from each iteration being done empirically from samples available from simulations of the next state. This makes the Bellman operator a random operator. A parametric and a nonparametric method for function approximation using a parametric function space and a reproducing kernel Hilbert space respectively are then combined with EVL. Both function spaces have the universal function approximation property. Basis functions are picked randomly. Convergence analysis is performed using a random operator framework with techniques from the theory of stochastic dominance. Finite time sample complexity bounds are derived for both universal approximate dynamic programming algorithms. Numerical experiments support the versatility and computational tractability of this approach."}}
{"id": "BT9I5fVtkUd", "cdate": 1577836800000, "mdate": 1684360414745, "content": {"title": "Randomized Policy Learning for Continuous State and Action MDPs", "abstract": "Deep reinforcement learning methods have achieved state-of-the-art results in a variety of challenging, high-dimensional domains ranging from video games to locomotion. The key to success has been the use of deep neural networks used to approximate the policy and value function. Yet, substantial tuning of weights is required for good results. We instead use randomized function approximation. Such networks are not only cheaper than training fully connected networks but also improve the numerical performance. We present \\texttt{RANDPOL}, a generalized policy iteration algorithm for MDPs with continuous state and action spaces. Both the policy and value functions are represented with randomized networks. We also give finite time guarantees on the performance of the algorithm. Then we show the numerical performance on challenging environments and compare them with deep neural network based algorithms."}}
{"id": "9dHa82XrGIW", "cdate": 1577836800000, "mdate": 1684360414748, "content": {"title": "Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes", "abstract": "Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning in..."}}
{"id": "zII5fIZSwV", "cdate": 1546300800000, "mdate": 1684360414754, "content": {"title": "An Empirical Relative Value Learning Algorithm for Non-parametric MDPs with Continuous State Space", "abstract": "We propose an empirical relative value learning (ERVL) algorithm for non-parametric MDPs with continuous state space and finite actions and average reward criterion. The ERVL algorithm relies on function approximation via nearest neighbors, and minibatch samples for value function update. It is universal (will work for any MDP), computationally quite simple and yet provides arbitrarily good approximation with high probability in finite time. This is the first such algorithm for non-parametric (and continuous state space) MDPs with average reward criteria with these provable properties as far as we know. Numerical evaluation on a benchmark problem of optimal replacement suggests good performance."}}
{"id": "blHu_kCDla", "cdate": 1546300800000, "mdate": 1684360414748, "content": {"title": "Empirical Algorithms for General Stochastic Systems with Continuous States and Actions", "abstract": "In this paper, we present Randomized Empirical Value Learning (RAEVL) algorithm for MDPs with continuous state and action spaces. This algorithm combines the ideas of random search over action space with randomized function approximation method to generalize the value functions over state space . Our theoretical analysis is done under a random operator framework combined with stochastic dominance argument. This provides finite-time analysis of the proposed algorithm as well as give the sample complexity."}}
{"id": "RLIDeM4gFll", "cdate": 1546300800000, "mdate": null, "content": {"title": "Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes", "abstract": "Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm reduces the problem to the discounted-reward version and achieves $\\mathcal{O}(T^{2/3})$ regret after $T$ steps, under the minimal assumption of weakly communicating MDPs. To our knowledge, this is the first model-free algorithm for general MDPs in this setting. The second algorithm makes use of recent advances in adaptive algorithms for adversarial multi-armed bandits and improves the regret to $\\mathcal{O}(\\sqrt{T})$, albeit with a stronger ergodic assumption. This result significantly improves over the $\\mathcal{O}(T^{3/4})$ regret achieved by the only existing model-free algorithm by Abbasi-Yadkori et al. (2019a) for ergodic MDPs in the infinite-horizon average-reward setting."}}
{"id": "KC2joJcXdd5", "cdate": 1546300800000, "mdate": 1684360414750, "content": {"title": "An Approximately Optimal Relative Value Learning Algorithm for Averaged MDPs with Continuous States and Actions", "abstract": "It has long been a challenging problem to design algorithms for Markov decision processes (MDPs) with continuous states and actions that are provably approximately optimal and can provide arbitrarily good approximation for any MDP. In this paper, we propose an empirical value learning algorithm for average MDPs with continuous states and actions that combines empirical value iteration with n function-parametric approximation and approximation of transition probability distribution with kernel density estimation. We view each iteration as operation of random operator and argue convergence using the probabilistic contraction analysis method that the authors (along with others) have recently developed."}}
{"id": "C5gvUU_B9T", "cdate": 1546300800000, "mdate": 1684360414751, "content": {"title": "Approximate Relative Value Learning for Average-reward Continuous State MDPs", "abstract": "In this paper, we propose an approximate relative value learning (ARVL) algorithm for non- parametric MDPs with continuous state space and finite actions and average reward criterion. It is a sampl..."}}
{"id": "7yvlerbX_D", "cdate": 1483228800000, "mdate": 1684360414750, "content": {"title": "Randomized function fitting-based empirical value iteration", "abstract": "Randomization is notable for being much less computationally expensive than optimization but often yielding comparable numerical performance. In this paper, we consider randomized function fitting combined with empirical value iteration for approximate dynamic programming on continuous state spaces. The method we propose is universal (i.e., not problem-dependent) and yields good approximations with high probability. A random operator theoretic framework is introduced for convergence analysis which uses a novel stochastic dominance argument. A non-asymptotic rate of convergence is obtained as a byproduct of the analysis. Numerical experiments confirm good performance of the algorithm proposed."}}
