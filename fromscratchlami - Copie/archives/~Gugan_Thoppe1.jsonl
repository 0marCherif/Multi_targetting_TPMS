{"id": "n9vGnTdJVB8", "cdate": 1676827105550, "mdate": null, "content": {"title": "Does Momentum Help in Stochastic Optimization? A Sample Complexity Analysis.", "abstract": "Stochastic Heavy Ball (SHB) and Nesterov's Accelerated Stochastic Gradient (ASG) are popular momentum methods in optimization. While the benefits of these acceleration ideas in deterministic settings are well understood, their advantages in stochastic optimization are unclear. Several works have recently claimed that SHB and ASG always help in stochastic optimization. Our work shows that i.) these claims are either flawed or one-sided (e.g., consider only the bias term but not the variance), and ii.) when both these terms are accounted for, SHB and ASG do not always help. Specifically, for \\textit{any} quadratic optimization, we obtain a lower bound on the sample complexity of SHB and ASG, accounting for both bias and variance, and show that the vanilla SGD can achieve the same bound."}}
{"id": "4aFQF_ctga", "cdate": 1672531200000, "mdate": 1682126667357, "content": {"title": "Online Learning with Adversaries: A Differential Inclusion Analysis", "abstract": "We consider the measurement model $Y = AX,$ where $X$ and, hence, $Y$ are random variables and $A$ is an a priori known tall matrix. At each time instance, a sample of one of $Y$'s coordinates is available, and the goal is to estimate $\\mu := \\mathbb{E}[X]$ via these samples. However, the challenge is that a small but unknown subset of $Y$'s coordinates are controlled by adversaries with infinite power: they can return any real number each time they are queried for a sample. For such an adversarial setting, we propose the first asynchronous online algorithm that converges to $\\mu$ almost surely. We prove this result using a novel differential inclusion based two-timescale analysis. Two key highlights of our proof include: (a) the use of a novel Lyapunov function for showing that $\\mu$ is the unique global attractor for our algorithm's limiting dynamics, and (b) the use of martingale and stopping time theory to show that our algorithm's iterates are almost surely bounded."}}
{"id": "0DW0Kp7e3l", "cdate": 1672531200000, "mdate": 1681724082335, "content": {"title": "SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree Search", "abstract": "Despite the popularity of policy gradient methods, they are known to suffer from large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax -- a generalization of softmax that takes planning into account. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We consider two variants of SoftTreeMax, one for cumulative reward and one for exponentiated reward. For both, we analyze the gradient variance and reveal for the first time the role of a tree expansion policy in mitigating this variance. We prove that the resulting variance decays exponentially with the planning horizon as a function of the expansion policy. Specifically, we show that the closer the resulting state transitions are to uniform, the faster the decay. In a practical implementation, we utilize a parallelized GPU-based simulator for fast and efficient tree search. Our differentiable tree-based policy leverages all gradients at the tree leaves in each environment step instead of the traditional single-sample-based gradient. We then show in simulation how the variance of the gradient is reduced by three orders of magnitude, leading to better sample complexity compared to the standard policy gradient. On Atari, SoftTreeMax demonstrates up to 5x better performance in a faster run time compared to distributed PPO. Lastly, we demonstrate that high reward correlates with lower variance."}}
{"id": "Ms1Zs8s7rg", "cdate": 1663850109777, "mdate": null, "content": {"title": "Demystifying Approximate RL with $\\epsilon$-greedy Exploration: A Differential Inclusion View", "abstract": "Q-learning and SARSA(0) with $\\epsilon$-greedy exploration are leading reinforcement learning methods, and their tabular forms converge to the optimal Q-function under reasonable conditions. However, with function approximation, they exhibit unexpected behaviors, such as i.) policy oscillation and chattering, and ii.) convergence to different attractors (possibly even the worst policy) on different runs, ii.) multiple attractors, and iii.) worst policy convergence, apart from the textbook instability. Accordingly, a theory to explain these phenomena has been a long-standing open problem, even for basic linear function approximation (Sutton, 1999). Our work uses differential inclusion theory to provide the first framework for resolving this problem. We further illustrate via numerical examples how this framework helps explain these algorithms' asymptotic behaviors."}}
{"id": "_ctgRLN1NIw", "cdate": 1640995200000, "mdate": 1682126667358, "content": {"title": "Online algorithms for estimating change rates of web pages", "abstract": ""}}
{"id": "UEpJGgrhkB", "cdate": 1640995200000, "mdate": 1682126667357, "content": {"title": "Approximate Q-learning and SARSA(0) under the \u03b5-greedy Policy: a Differential Inclusion Analysis", "abstract": "Q-learning and SARSA with $\\epsilon$-greedy exploration are leading reinforcement learning methods. Their tabular forms converge to the optimal Q-function under reasonable conditions. However, with function approximation, these methods exhibit strange behaviors such as policy oscillation, chattering, and convergence to different attractors (possibly even the worst policy) on different runs, apart from the usual instability. A theory to explain these phenomena has been a long-standing open problem, even for basic linear function approximation (Sutton, 1999). Our work uses differential inclusion to provide the first framework for resolving this problem. We also provide numerical examples to illustrate our framework's prowess in explaining these algorithms' behaviors."}}
{"id": "HZRRCB5uz_c", "cdate": 1640995200000, "mdate": 1682126667358, "content": {"title": "Improving Sample Efficiency in Evolutionary RL Using Off-Policy Ranking", "abstract": "Evolution Strategy (ES) is a powerful black-box optimization technique based on the idea of natural evolution. In each of its iterations, a key step entails ranking candidate solutions based on some fitness score. For an ES method in Reinforcement Learning (RL), this ranking step requires evaluating multiple policies. This is presently done via on-policy approaches: each policy's score is estimated by interacting several times with the environment using that policy. This leads to a lot of wasteful interactions since, once the ranking is done, only the data associated with the top-ranked policies is used for subsequent learning. To improve sample efficiency, we propose a novel off-policy alternative for ranking, based on a local approximation for the fitness function. We demonstrate our idea in the context of a state-of-the-art ES method called the Augmented Random Search (ARS). Simulations in MuJoCo tasks show that, compared to the original ARS, our off-policy variant has similar running times for reaching reward thresholds but needs only around 70% as much data. It also outperforms the recent Trust Region ES. We believe our ideas should be extendable to other ES methods as well."}}
{"id": "6veB3MCD-bu", "cdate": 1621630257628, "mdate": null, "content": {"title": "A Law of Iterated Logarithm for Multi-Agent Reinforcement Learning", "abstract": "In Multi-Agent Reinforcement Learning (MARL), multiple agents interact with a common environment, as also with each other, for solving a shared problem in sequential decision-making. It has wide-ranging applications in gaming, robotics, finance, communication, etc. In this work, we derive a novel law of iterated logarithm for a family of  distributed nonlinear stochastic approximation schemes that is useful in MARL. In particular, our result describes the convergence rate on almost every sample path where the algorithm converges. This result is the first of its kind in the distributed setup and provides deeper insights than the existing ones, which only discuss convergence rates in the expected or the CLT sense. Importantly, our result holds under significantly weaker assumptions: neither the gossip matrix needs to be doubly stochastic nor the stepsizes square summable. As an application, we show  that, for the stepsize $n^{-\\gamma}$ with $\\gamma \\in (0, 1),$ the distributed TD(0) algorithm with linear function approximation has a convergence rate of $O(\\sqrt{n^{-\\gamma} \\ln n })$ a.s.; for the $1/n$ type stepsize, the same is $O(\\sqrt{n^{-1} \\ln \\ln n})$ a.s. These decay rates do not depend on the graph depicting the interactions among the different agents. "}}
{"id": "6w4rGOrHfC", "cdate": 1609459200000, "mdate": 1682126667359, "content": {"title": "A Law of Iterated Logarithm for Multi-Agent Reinforcement Learning", "abstract": "In Multi-Agent Reinforcement Learning (MARL), multiple agents interact with a common environment, as also with each other, for solving a shared problem in sequential decision-making. It has wide-ranging applications in gaming, robotics, finance, communication, etc. In this work, we derive a novel law of iterated logarithm for a family of distributed nonlinear stochastic approximation schemes that is useful in MARL. In particular, our result describes the convergence rate on almost every sample path where the algorithm converges. This result is the first of its kind in the distributed setup and provides deeper insights than the existing ones, which only discuss convergence rates in the expected or the CLT sense. Importantly, our result holds under significantly weaker assumptions: neither the gossip matrix needs to be doubly stochastic nor the stepsizes square summable. As an application, we show that, for the stepsize $n^{-\\gamma}$ with $\\gamma \\in (0, 1),$ the distributed TD(0) algorithm with linear function approximation has a convergence rate of $O(\\sqrt{n^{-\\gamma} \\ln n })$ a.s.; for the $1/n$ type stepsize, the same is $O(\\sqrt{n^{-1} \\ln \\ln n})$ a.s. These decay rates do not depend on the graph depicting the interactions among the different agents."}}
{"id": "73IzOYsBso", "cdate": 1599925328894, "mdate": null, "content": {"title": "Finite Sample Analyses for TD(0) with Function Approximation", "abstract": "TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such results. Existing convergence rates for Temporal Difference (TD) methods apply only to somewhat modified versions, e.g., projected variants or ones where stepsizes depend on unknown problem parameters. Our analyses obviate these artificial alterations by exploiting strong properties of TD(0). We provide convergence rates both in expectation and with high-probability. The two are obtained via different approaches that use relatively unknown, recently developed stochastic approximation techniques."}}
