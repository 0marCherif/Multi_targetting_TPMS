{"id": "NBLYtIcuJCb", "cdate": 1696118400000, "mdate": 1695973721969, "content": {"title": "Increasing revenue in Bayesian posted price auctions through signaling", "abstract": ""}}
{"id": "EPonuZvVPw", "cdate": 1682899200000, "mdate": 1682352543180, "content": {"title": "Designing menus of contracts efficiently: The power of randomization", "abstract": ""}}
{"id": "zq4xHZq4jO3", "cdate": 1672531200000, "mdate": 1695971967086, "content": {"title": "Optimal Rates and Efficient Algorithms for Online Bayesian Persuasion", "abstract": "Bayesian persuasion studies how an informed sender should influence beliefs of rational receivers that take decisions through Bayesian updating of a common prior. We focus on the online Bayesian pe..."}}
{"id": "sgIr-rtELm", "cdate": 1672531200000, "mdate": 1695973722069, "content": {"title": "Learning Optimal Contracts: How to Exploit Small Action Spaces", "abstract": "We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme -- called contract -- in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent's action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover, it can also be employed to provide a $\\tilde{\\mathcal{O}}(T^{4/5})$ regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility, thus considerably improving previously-known regret bounds."}}
{"id": "sM_UgyDKLm", "cdate": 1672531200000, "mdate": 1682352543518, "content": {"title": "Selling Information while Being an Interested Party", "abstract": "We study the algorithmic problem faced by an information holder (seller) who wants to optimally sell such information to a budged-constrained decision maker (buyer) that has to undertake some action. Differently from previous, we consider the case in which the seller is an interested party, as the action chosen by the buyer does not only influence their utility, but also seller's one. This happens in many real-world settings, where the way in which businesses use acquired information may positively or negatively affect the seller, due to the presence of externalities on the information market. The utilities of both the seller and the buyer depend on a random state of nature, which is revealed to the seller, but it is unknown to the buyer. Thus, the seller's goal is to (partially) sell their information about the state of nature to the buyer, so as to concurrently maximize revenue and induce the buyer to take a desirable action. We study settings in which buyer's budget and utilities are determined by a random buyer's type that is unknown to the seller. In such settings, an optimal protocol for the seller must propose to the buyer a menu of information-revelation policies to choose from, with the latter acquiring one of them by paying its corresponding price. Moreover, since in our model the seller is an interested party, an optimal protocol must also prescribe the seller to pay back the buyer contingently on their action. First, we show that the problem of computing a seller-optimal protocol can be solved in polynomial time. Next, we switch the attention to the case in which a seller's protocol employs a single information-revelation policy, rather than proposing a menu, deriving both positive and negative results."}}
{"id": "nSLi4KhHvu", "cdate": 1672531200000, "mdate": 1695973722036, "content": {"title": "A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints", "abstract": "We study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical unconstrained MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with long-term constraints. Our algorithm is capable of handling settings in which rewards and constraints are selected either stochastically or adversarially, without requiring any knowledge of the underling process. Moreover, our algorithm matches state-of-the-art regret and constraint violation bounds for settings in which constraints are selected stochastically, while it is the first to provide guarantees in the case in which they are chosen adversarially."}}
{"id": "lW31IxmqN", "cdate": 1672531200000, "mdate": 1695973722050, "content": {"title": "Multi-Agent Contract Design: How to Commission Multiple Agents with Individual Outcomes", "abstract": "We study hidden-action principal-agent problems with multiple agents. These are problems in which a principal commits to an outcome-dependent payment scheme (called contract) in order to incentivize some agents to take costly, unobservable actions that lead to favorable outcomes. Previous works study models where the principal observes a single outcome determined by the actions of all the agents. This considerably limits the contracting power of the principal, since payments can only depend on the joint result achieved by the agents. In this paper, we consider a model in which each agent determines their own individual outcome as an effect of their action only, the principal observes all the individual outcomes separately, and they perceive a reward that jointly depends on all these outcomes. This considerably enhances the principal's contracting capabilities, by allowing them to pay each agent on the basis of their individual result. We analyze the computational complexity of finding principal-optimal contracts, revolving around two properties of principal's rewards, namely IR-supermodularity and DR-submodularity. The former captures settings with increasing returns, where the rewards grow faster as the agents' effort increases, while the latter models the case of diminishing returns, in which rewards grow slower instead. These naturally model diseconomies and economies of scale. We first address basic instances in which the principal knows everything about the agents, and, then, more general Bayesian instances where each agent has their own private type determining their features, such as action costs and how actions stochastically determine individual outcomes. As a preliminary result, we show that finding an optimal contract in a non-Bayesian instance can be reduced in polynomial time to a maximization problem over a matroid having a particular structure. This is needed to prove our main positive results in the rest of the paper. We start by analyzing non-Bayesian instances, where we first prove that the problem of computing a principal-optimal contract is inapproximable with either IR-supermodular or DR-submodular rewards. Nevertheless, we show that in the former case the problem becomes polynomial-time solvable under some mild regularity assumptions, while in the latter case it admits a polynomial-time (1 \u2212 1/e)-approximation algorithm. In conclusion, we extend our positive results to Bayesian instances. First, we show that the principal's optimization problem can be approximately solved by means of a linear formulation. This is non-trivial since in general the problem may not admit a maximum, but only a supremum. Then, by working on such a linear formulation, we provide algorithms based on the ellipsoid method that (almost) match the guarantees obtained for non-Bayesian instances."}}
{"id": "jO-ePijYxY", "cdate": 1672531200000, "mdate": 1695904607607, "content": {"title": "Persuading Farsighted Receivers in MDPs: the Power of Honesty", "abstract": "Bayesian persuasion studies the problem faced by an informed sender who strategically discloses information to influence the behavior of an uninformed receiver. Recently, a growing attention has been devoted to settings where the sender and the receiver interact sequentially, in which the receiver's decision-making problem is usually modeled as a Markov decision process (MDP). However, previous works focused on computing optimal information-revelation policies (a.k.a. signaling schemes) under the restrictive assumption that the receiver acts myopically, selecting actions to maximize the one-step utility and disregarding future rewards. This is justified by the fact that, when the receiver is farsighted and thus considers future rewards, finding an optimal Markovian signaling scheme is NP-hard. In this paper, we show that Markovian signaling schemes do not constitute the \"right\" class of policies. Indeed, differently from most of the MDPs settings, we prove that Markovian signaling schemes are not optimal, and general history-dependent signaling schemes should be considered. Moreover, we also show that history-dependent signaling schemes circumvent the negative complexity results affecting Markovian signaling schemes. Formally, we design an algorithm that computes an optimal and {\\epsilon}-persuasive history-dependent signaling scheme in time polynomial in 1/{\\epsilon} and in the instance size. The crucial challenge is that general history-dependent signaling schemes cannot be represented in polynomial space. Nevertheless, we introduce a convenient subclass of history-dependent signaling schemes, called promise-form, which are as powerful as general history-dependent ones and efficiently representable. Intuitively, promise-form signaling schemes compactly encode histories in the form of honest promises on future receiver's rewards."}}
{"id": "c0z-U2d9z-E", "cdate": 1672531200000, "mdate": 1681666287148, "content": {"title": "Optimal Rates and Efficient Algorithms for Online Bayesian Persuasion", "abstract": "Bayesian persuasion studies how an informed sender should influence beliefs of rational receivers who take decisions through Bayesian updating of a common prior. We focus on the online Bayesian persuasion framework, in which the sender repeatedly faces one or more receivers with unknown and adversarially selected types. First, we show how to obtain a tight $\\tilde O(T^{1/2})$ regret bound in the case in which the sender faces a single receiver and has partial feedback, improving over the best previously known bound of $\\tilde O(T^{4/5})$. Then, we provide the first no-regret guarantees for the multi-receiver setting under partial feedback. Finally, we show how to design no-regret algorithms with polynomial per-iteration running time by exploiting type reporting, thereby circumventing known intractability results on online Bayesian persuasion. We provide efficient algorithms guaranteeing a $O(T^{1/2})$ regret upper bound both in the single- and multi-receiver scenario when type reporting is allowed."}}
{"id": "_oBqBBs-BX", "cdate": 1672531200000, "mdate": 1695971967092, "content": {"title": "Constrained Phi-Equilibria", "abstract": "The computational study of equilibria involving constraints on players\u2019 strategies has been largely neglected. However, in real-world applications, players are usually subject to constraints ruling..."}}
