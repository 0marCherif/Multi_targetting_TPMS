{"id": "jBzMnpMhQb", "cdate": 1640995200000, "mdate": 1681742095259, "content": {"title": "Fenrir: Physics-Enhanced Regression for Initial Value Problems", "abstract": "We show how probabilistic numerics can be used to convert an initial value problem into a Gauss\u2013Markov process parametrised by the dynamics of the initial value problem. Consequently, the often dif..."}}
{"id": "dlY522ZpLb", "cdate": 1640995200000, "mdate": 1681742095261, "content": {"title": "Probabilistic ODE Solutions in Millions of Dimensions", "abstract": "Probabilistic solvers for ordinary differential equations (ODEs) have emerged as an efficient framework for uncertainty quantification and inference on dynamical systems. In this work, we explain t..."}}
{"id": "cHZVdl8_1CN", "cdate": 1640995200000, "mdate": 1681742095259, "content": {"title": "Pick-and-Mix Information Operators for Probabilistic ODE Solvers", "abstract": "Probabilistic numerical solvers for ordinary differential equations compute posterior distributions over the solution of an initial value problem via Bayesian inference. In this paper, we leverage their probabilistic formulation to seamlessly include additional information as general likelihood terms. We show that second-order differential equations should be directly provided to the solver, instead of transforming the problem to first order. Additionally, by including higher-order information or physical conservation laws in the model, solutions become more accurate and more physically meaningful. Lastly, we demonstrate the utility of flexible information operators by solving differential-algebraic equations. In conclusion, the probabilistic formulation of numerical solvers offers a flexible way to incorporate various types of information, thus improving the resulting solutions."}}
{"id": "B0ebOilXSl9", "cdate": 1609459200000, "mdate": 1645715616493, "content": {"title": "ProbNum: Probabilistic Numerics in Python", "abstract": "Probabilistic numerical methods (PNMs) solve numerical problems via probabilistic inference. They have been developed for linear algebra, optimization, integration and differential equation simulation. PNMs naturally incorporate prior information about a problem and quantify uncertainty due to finite computational resources as well as stochastic input. In this paper, we present ProbNum: a Python library providing state-of-the-art probabilistic numerical solvers. ProbNum enables custom composition of PNMs for specific problem classes via a modular design as well as wrappers for off-the-shelf use. Tutorials, documentation, developer guides and benchmarks are available online at www.probnum.org."}}
{"id": "1Rd2UePcLbI", "cdate": 1609459200000, "mdate": null, "content": {"title": "Calibrated Adaptive Probabilistic ODE Solvers", "abstract": "Probabilistic solvers for ordinary differential equations assign a posterior measure to the solution of an initial value problem. The joint covariance of this distribution provides an estimate of the (global) approximation error. The contraction rate of this error estimate as a function of the solver\u2019s step-size identifies it as a well-calibrated worst-case error, but its explicit numerical value for a certain step size is not automatically a good estimate of the explicit error. Addressing this issue, we introduce, discuss, and assess several probabilistically motivated ways to calibrate the uncertainty estimate. Numerical experiments demonstrate that these calibration methods interact efficiently with adaptive step-size selection, resulting in descriptive, and efficiently computable posteriors. We demonstrate the efficiency of the methodology by benchmarking against the classic, widely used Dormand-Prince 4/5 Runge-Kutta method."}}
{"id": "3Du_qlP2FDr", "cdate": 1591623882119, "mdate": null, "content": {"title": "Planning from Images with Deep Latent Gaussian Process Dynamics", "abstract": "Planning is a powerful approach to control problems with known environment dynamics. In unknown environments the agent needs to learn a model of the system dynamics to make planning applicable. This is particularly challenging when the underlying states are only indirectly observable through high-dimensional observations such as images. We propose to learn a deep latent Gaussian process dynamics (DLGPD) model that learns low-dimensional system dynamics from environment interactions with visual observations. The method infers latent state representations from observations using neural networks and models the system dynamics in the learned latent space with Gaussian processes. All parts of the model can be trained jointly by optimizing a lower bound on the likelihood of transitions in image space. We evaluate the proposed approach on the pendulum swing-up task while using the learned dynamics model for planning in latent space in order to solve the control problem. We also demonstrate that our method can quickly adapt a trained agent to changes in the system dynamics from just a few rollouts. We compare our approach to a state-of-the-art purely deep learning based method and demonstrate the advantages of combining Gaussian processes with deep learning for data efficiency and transfer learning."}}
