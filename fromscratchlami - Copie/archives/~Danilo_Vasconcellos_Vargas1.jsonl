{"id": "802RB3WAwyE", "cdate": 1668040726840, "mdate": 1668040726840, "content": {"title": "Continual General Chunking Problem and SyncMap", "abstract": "Humans possess an inherent ability to chunk sequences into\ntheir constituent parts. In fact, this ability is thought to boot-\nstrap language skills and learning of image patterns which\nmight be a key to a more animal-like type of intelligence.\nHere, we propose a continual generalization of the chunking\nproblem (an unsupervised problem), encompassing fixed and\nprobabilistic chunks, discovery of temporal and causal struc-\ntures and their continual variations. Additionally, we propose\nan algorithm called SyncMap1 that can learn and adapt to\nchanges in the problem by creating a dynamic map which pre-\nserves the correlation between variables. Results of SyncMap\nsuggest that the proposed algorithm learn near optimal solu-\ntions, despite the presence of many types of structures and their\ncontinual variation. When compared to Word2vec, PARSER\nand MRIL, SyncMap surpasses or ties with the best algorithm\non 66% of the scenarios while being the second best in the\nremaining 34%. SyncMap\u2019s model-free simple dynamics and\nthe absence of loss functions reveal that, perhaps surprisingly,\nmuch can be done with self-organization alone."}}
{"id": "xYNOuQh1Z7Y", "cdate": 1663850196922, "mdate": null, "content": {"title": "Magnum: Tackling High-Dimensional Structures with Self-Organization", "abstract": "A big challenge for dealing with real world problems is scalability. In fact, this is partially the reason behind the success of deep learning over other learning paradigms. Here, we tackle the scalability of a novel learning paradigm proposed in 2021 based solely on self organizing principles. This paradigm consists of only dynamical equations which self-organize with the input to create attractor-repeller points that are related to the patterns found in data. To achieve scalability for such a system, we propose the Magnum algorithm, which utilizes many self-organizing subsystems (SubSigma) each with subsets of the problem's variables. The main idea is that by merging SubSigmas, Magnum builds over time a variable correlation by consensus, capable of accurately predicting the structure of large groups of variables. Experiments show that Magnum surpasses or ties with other unsupervised algorithms in all of the high-dimensional chunking problems, each with distinct types of shapes and structural features. Moreover, SubSigma alone outperforms or ties with other unsupervised algorithms in six out of seven basic chunking problems. Thus, this work sheds light in how self-organization learning paradigms can be scaled up to deal with high dimensional structures and compete with current learning paradigms."}}
{"id": "ndYrOsNw_B2", "cdate": 1663850099632, "mdate": null, "content": {"title": "Dynamical Equations With Bottom-up Self-Organizing Properties Learn Accurate Dynamical Hierarchies Without Any Loss Function", "abstract": "Self-organization is ubiquitous in nature and mind. However, machine learning and theories of cognition still barely touch the subject. The hurdle is that general patterns are difficult to define in terms of dynamical equations and designing a system that could learn by reordering itself is still to be seen. Here, we propose a learning system, where patterns are defined within the realm of nonlinear dynamics with positive and negative feedback loops, allowing attractor-repeller pairs to emerge for each pattern observed. Experiments reveal that such a system can map temporal to spatial correlation, enabling hierarchical structures to be learned from sequential data. The results are accurate enough to surpass state-of-the-art unsupervised learning algorithms in seven out of eight experiments as well as two real-world problems. Interestingly, the dynamic nature of the system makes it inherently adaptive, giving rise to phenomena similar to phase transitions in chemistry/thermodynamics when the input structure changes. Thus, the work here sheds light on how self-organization can allow for pattern recognition and hints at how intelligent behavior might emerge from simple dynamic equations without an objective/loss function."}}
{"id": "xnscpQU6lvh", "cdate": 1663850097681, "mdate": null, "content": {"title": "Symmetrical SyncMap for Imbalanced General Chunking Problems", "abstract": "Recently, SyncMap (2021) pioneered an approach to learn complex structures from sequences as well as adapt to any changes in underlying structures. Such approach, inspired by neuron group behaviors, is achieved by using self-organizing dynamical equations without any loss functions. Here we propose Symmetrical SyncMap that goes beyond the original work to show how to create dynamical equations and attractor-repeller points which are stable over the long run, even dealing with imbalanced continual general chunking problems (CGCPs). The main idea is to apply equal updates from positive and negative feedback loops by symmetrical activation. We then introduce the concept of memory window to allow for more positive updates. Our algorithm surpasses or ties other unsupervised state-of-the-art baselines in all 12 imbalanced CGCPs with various difficulties, including dynamical ones. To verify its performance in real-world scenarios, we conduct experiments on several well-studied structure learning problems. The proposed method surpasses substantially other methods in all scenarios, suggesting that symmetrical activation plays a critical role in uncovering topological structures and even hierarchies encoded in temporal data."}}
{"id": "rKRBNzIrX4", "cdate": 1609459200000, "mdate": 1668075421469, "content": {"title": "Towards Evaluating the Representation Learned by Variational AutoEncoders", "abstract": "At the heart of a deep neural network is representation learning with complex latent variables. This representation learning has been improved by disentangled representations and the idea of regularization terms. However, adversarial samples show that tasks with DNNs can easily fail due to slight perturbations or transformations of the input. Variational AutoEncoder (VAE) learns <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$P(z\\vert x)$</tex> , the distribution of the latent variable <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$z$</tex> , rather than <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$P(y\\vert x)$</tex> , the distribution of the output <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$y$</tex> for the input x. Therefore, VAE is considered to be a good model for learning representations from input data. In other words, the mapping of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$x$</tex> is not directly to <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$y$</tex> , but to the latent variable <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$z$</tex> . In this paper, we propose an evaluation method to characterize the latent variables that VAE learns. Specifically, latent variables extracted from VAEs trained by two well-known data sets are analyzed by the k-nearest neighbor method(kNN). In doing so, we propose an interpretation of what kind of representation the VAE learns, and share clues about the hyperdimensional space to which the latent variables are mapped."}}
{"id": "qiVVV7k433", "cdate": 1609459200000, "mdate": 1668075421550, "content": {"title": "Parameter Optimization via CMA-ES for Implementation in the Active Control of Magnetic Pillar Arrays", "abstract": "Pillared surfaces are the products of a surface modification technique that allow the implementation of active control methods by an outer source such as magnetic fields. Pillar arrays with magnetic tips exhibit different characteristics depending on the initial positional arrangement of the pillars and/or the environmental magnetic field conditions. This study develops methods for simulation and parameter optimization by machine learning to aid the investigation of pillar behaviors in various combinations of initial positions and magnetic fields. Optimization is performed using the co-variance adaptation evolution strategy (CMA-ES). The algorithm is tested to obtain preliminary results: (1) the maximum size of the pillar pitch at a given magnetic field; (2) the initial pillar arrangement of a 3-pillar unit cell and three settings of applied magnetic field-each corresponds to a predefined contact state of a three-stage paring pattern."}}
{"id": "YeWR5KY7OK", "cdate": 1609459200000, "mdate": 1668075421520, "content": {"title": "Towards Understanding The Space of Unrobust Features of Neural Networks", "abstract": "Despite the convolutional neural network has achieved tremendous monumental success on a variety of computer vision-related tasks, it is still extremely challenging to build a neural network with doubtless reliability. Previous works have demonstrated that the deep neural network can be efficiently fooled by human imperceptible perturbation to the input, which actually revealed the instability for interpolation. Like human-beings, an ideally trained neural network should be constrained within desired inference space and maintain correctness for both interpolation and extrapolation. In this paper, we develop a technique to verify the correctness when convolutional neural networks extrapolate beyond training data distribution by generating legitimated feature broken images, and we show that the decision boundary for convolutional neural network is not well formulated based on image features for extrapolating."}}
{"id": "UrAo6hRc7qA", "cdate": 1609459200000, "mdate": 1668075421463, "content": {"title": "Continual General Chunking Problem and SyncMap", "abstract": "Humans possess an inherent ability to chunk sequences into their constituent parts. In fact, this ability is thought to bootstrap language skills and learning of image patterns which might be a key to a more animal-like type of intelligence. Here, we propose a continual generalization of the chunking problem (an unsupervised problem), encompassing fixed and probabilistic chunks, discovery of temporal and causal structures and their continual variations. Additionally, we propose an algorithm called SyncMap that can learn and adapt to changes in the problem by creating a dynamic map which preserves the correlation between variables. Results of SyncMap suggest that the proposed algorithm learn near optimal solutions, despite the presence of many types of structures and their continual variation. When compared to Word2vec, PARSER and MRIL, SyncMap surpasses or ties with the best algorithm on 66% of the scenarios while being the second best in the remaining 34%. SyncMap's model-free simple dynamics and the absence of loss functions reveal that, perhaps surprisingly, much can be done with self-organization alone."}}
{"id": "OqI2GpqLtZ", "cdate": 1609459200000, "mdate": 1668075421519, "content": {"title": "Towards Learning Hierarchical Structures with SyncMap", "abstract": "Objects or events perceived by human are often organized in a sequence that forms into chunks which exhibit hierarchical structure, e.g., words or videos. Such a sequence can be represented as a group of temporally correlated variables at multiple levels referred as chunk. In this work, an unsupervised method known as SyncMap is used to perform chunking on sequences of input data with hierarchical structure. We design a fixed and probabilistic chunk experiment to test our model capability, measured by the mutual information between the predicted chunk with the ground truth. Surprisingly, without too much modification on the original algorithm, the result has shown that SyncMap can perform chunking with hierarchical structure, although with limitation. Possible future works are proposed to overcome the limitation. Observation on the dynamic of weight map also indicates that SyncMap adapts to the low-level hierarchical representation of chunks faster than the one on the higher level."}}
{"id": "DNiazsGala", "cdate": 1609459200000, "mdate": 1668075421442, "content": {"title": "Deep neural network loses attention to adversarial images", "abstract": "Adversarial algorithms have shown to be effective against neural networks for a variety of tasks. Some adversarial algorithms perturb all the pixels in the image minimally for the image classification task in image classification. In contrast, some algorithms perturb few pixels strongly. However, very little information is available regarding why these adversarial samples so diverse from each other exist. Recently, Vargas et al. showed that the existence of these adversarial samples might be due to conflicting saliency within the neural network. We test this hypothesis of conflicting saliency by analysing the Saliency Maps (SM) and Gradient-weighted Class Activation Maps (Grad-CAM) of original and few different types of adversarial samples. We also analyse how different adversarial samples distort the attention of the neural network compared to original samples. We show that in the case of Pixel Attack, perturbed pixels either calls the network attention to themselves or divert the attention from them. Simultaneously, the Projected Gradient Descent Attack perturbs pixels so that intermediate layers inside the neural network lose attention for the correct class. We also show that both attacks affect the saliency map and activation maps differently. Thus, shedding light on why some defences successful against some attacks remain vulnerable against other attacks. We hope that this analysis will improve understanding of the existence and the effect of adversarial samples and enable the community to develop more robust neural networks."}}
