{"id": "dwtv4jsPSh", "cdate": 1672531200000, "mdate": 1683657780511, "content": {"title": "Beyond UCB: Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits", "abstract": "We consider the sequential decision-making problem where the mean outcome is a non-linear function of the chosen action. Compared with the linear model, two curious phenomena arise in non-linear models: first, in addition to the \"learning phase\" with a standard parametric rate for estimation or regret, there is an \"burn-in period\" with a fixed cost determined by the non-linear function; second, achieving the smallest burn-in cost requires new exploration algorithms. For a special family of non-linear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. In particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. In contrast, several classical algorithms, such as UCB and algorithms relying on regression oracles, are provably suboptimal."}}
{"id": "_j_p_dgoCHF", "cdate": 1672531200000, "mdate": 1681146879586, "content": {"title": "Tight Guarantees for Interactive Decision Making with the Decision-Estimation Coefficient", "abstract": ""}}
{"id": "hjqTeP05OMB", "cdate": 1652737861543, "mdate": null, "content": {"title": "Leveraging the Hints: Adaptive Bidding in Repeated First-Price Auctions", "abstract": "With the advent and increasing consolidation of e-commerce, digital advertising has very recently replaced traditional advertising as the main marketing force in the economy. In the past four years, a particularly important development in the digital advertising industry is the shift from second-price auctions to first-price auctions for online display ads. This shift immediately motivated the intellectually challenging question of how to bid in first-price auctions, because unlike in second-price auctions, bidding one's private value truthfully is no longer optimal. Following a series of recent works in this area, we consider a differentiated setup: we do not make any assumption about other bidders' maximum bid (i.e. it can be adversarial over time), and instead assume that we have access to a hint that serves as a prediction of other bidders' maximum bid, where the prediction is learned through some blackbox machine learning model. We consider two types of hints: one where a single point-prediction is available, and the other where a hint interval (representing a type of confidence region into which others' maximum bid falls) is available. We establish minimax optimal regret bounds for both cases and highlight the quantitatively different behavior between the two settings. We also provide improved regret bounds when the others' maximum bid exhibits the further structure of sparsity. Finally, we complement the theoretical results with demonstrations using real bidding data."}}
{"id": "SbHxPRHPc2u", "cdate": 1652737471026, "mdate": null, "content": {"title": "Oracle-Efficient Online Learning for Smoothed Adversaries", "abstract": "We study the design of computationally efficient online learning algorithms under smoothed analysis. In this setting, at every step, an adversary generates a sample from an adaptively chosen distribution whose density is upper bounded by $1/\\sigma$ times the uniform density. Given access to an offline optimization (ERM) oracle, we give the first computationally efficient online algorithms whose sublinear regret depends only on the pseudo/VC dimension $d$ of the class and the smoothness parameter $\\sigma$. In particular, we achieve \\emph{oracle-efficient} regret bounds of   $ O (  \\sqrt{T d\\sigma^{-1}} ) $ for learning real-valued functions and $ O (  \\sqrt{T d\\sigma^{-\\frac{1}{2}} }  )$ for learning binary-valued functions. Our results establish that online learning is computationally as easy as offline learning, under the smoothed analysis framework. This contrasts the computational separation between online learning with worst-case adversaries and offline learning established by [HK16].\nOur algorithms also achieve improved bounds for some settings with binary-valued functions and worst-case adversaries.  These include an oracle-efficient algorithm with $O ( \\sqrt{T(d |\\mathcal{X}|)^{1/2} })$ regret that refines the earlier $O ( \\sqrt{T|\\mathcal{X}|})$ bound of [DS16] for finite domains, and an oracle-efficient algorithm with $O(T^{3/4} d^{1/2})$ regret for the transductive setting.  "}}
{"id": "q16HXpXtjJn", "cdate": 1652737400706, "mdate": null, "content": {"title": "Beyond the Best:  Distribution Functional Estimation in Infinite-Armed Bandits", "abstract": "In the infinite-armed bandit problem, each arm's average reward is sampled from an unknown distribution, and each arm can be sampled further to obtain noisy estimates of the average reward of that arm. Prior work focuses on the best arm, i.e. estimating the maximum of the average reward distribution. We consider a general class of distribution functionals beyond the maximum and obtain optimal sample complexities in both offline and online settings. We show that online estimation, where the learner can sequentially choose whether to sample a new or existing arm, offers no advantage over the offline setting for estimating the mean functional, but significantly reduces the sample complexity for other functionals such as the median, maximum, and trimmed mean. We propose unified meta algorithms for the online and offline settings and derive matching lower bounds using different Wasserstein distances. For the special case of median estimation, we identify a curious thresholding phenomenon on the indistinguishability between Gaussian convolutions with respect to the noise level, which may be of independent interest."}}
{"id": "Qp-Vex3_Cw", "cdate": 1640995200000, "mdate": 1683611367055, "content": {"title": "Oracle-Efficient Online Learning for Smoothed Adversaries", "abstract": "We study the design of computationally efficient online learning algorithms under smoothed analysis. In this setting, at every step, an adversary generates a sample from an adaptively chosen distribution whose density is upper bounded by $1/\\sigma$ times the uniform density. Given access to an offline optimization (ERM) oracle, we give the first computationally efficient online algorithms whose sublinear regret depends only on the pseudo/VC dimension $d$ of the class and the smoothness parameter $\\sigma$. In particular, we achieve \\emph{oracle-efficient} regret bounds of $ O ( \\sqrt{T d\\sigma^{-1}} ) $ for learning real-valued functions and $ O ( \\sqrt{T d\\sigma^{-\\frac{1}{2}} } )$ for learning binary-valued functions. Our results establish that online learning is computationally as easy as offline learning, under the smoothed analysis framework. This contrasts the computational separation between online learning with worst-case adversaries and offline learning established by [HK16].Our algorithms also achieve improved bounds for some settings with binary-valued functions and worst-case adversaries. These include an oracle-efficient algorithm with $O ( \\sqrt{T(d |\\mathcal{X}|)^{1/2} })$ regret that refines the earlier $O ( \\sqrt{T|\\mathcal{X}|})$ bound of [DS16] for finite domains, and an oracle-efficient algorithm with $O(T^{3/4} d^{1/2})$ regret for the transductive setting."}}
{"id": "G-yAIG8kzaR", "cdate": 1640995200000, "mdate": 1654693721096, "content": {"title": "On the Statistical Complexity of Sample Amplification", "abstract": "Given $n$ i.i.d. samples drawn from an unknown distribution $P$, when is it possible to produce a larger set of $n+m$ samples which cannot be distinguished from $n+m$ i.i.d. samples drawn from $P$? (Axelrod et al. 2019) formalized this question as the sample amplification problem, and gave optimal amplification procedures for discrete distributions and Gaussian location models. However, these procedures and associated lower bounds are tailored to the specific distribution classes, and a general statistical understanding of sample amplification is still largely missing. In this work, we place the sample amplification problem on a firm statistical foundation by deriving generally applicable amplification procedures, lower bound techniques and connections to existing statistical notions. Our techniques apply to a large class of distributions including the exponential family, and establish a rigorous connection between sample amplification and distribution learning."}}
{"id": "dHc1p5eoecb", "cdate": 1621630348052, "mdate": null, "content": {"title": "Optimal prediction of Markov chains with and without spectral gap", "abstract": "We study the following learning problem with dependent data: Given a trajectory of length $n$ from a stationary Markov chain with $k$ states, the goal is to predict the distribution of the next state. For $3 \\leq k \\leq O(\\sqrt{n})$, the optimal prediction risk in the Kullback-Leibler divergence is shown to be $\\Theta(\\frac{k^2}{n}\\log \\frac{n}{k^2})$, in contrast to the optimal rate of $\\Theta(\\frac{\\log \\log n}{n})$ for $k=2$ previously shown in Falahatgar et al in 2016. These nonparametric rates can be attributed to the memory in the data, as the spectral gap of the Markov chain can be arbitrarily small. To quantify the memory effect, we study irreducible reversible chains with a prescribed spectral gap. In addition to characterizing the optimal prediction risk for two states, we show that, as long as the spectral gap is not excessively small, the prediction risk in the Markov model is $O(\\frac{k^2}{n})$, which coincides with that of an iid model with the same number of parameters. "}}
{"id": "Xa9Ba6NsJ6", "cdate": 1621630324133, "mdate": null, "content": {"title": "On the Value of Interaction and Function Approximation in Imitation Learning", "abstract": "We study the statistical guarantees for the Imitation Learning (IL) problem in episodic MDPs.\nRajaraman et al. (2020) show an information theoretic lower bound that in the worst case, a learner which can even actively query the expert policy suffers from a suboptimality growing quadratically in the length of the horizon, $H$. We study imitation learning under the $\\mu$-recoverability assumption of Ross et al. (2011) which assumes that the difference in the $Q$-value under the expert policy across different actions in a state do not deviate beyond $\\mu$ from the maximum. We show that the reduction proposed by Ross et al. (2010) is statistically optimal: the resulting algorithm upon interacting with the MDP for $N$ episodes results in a suboptimality bound of $\\widetilde{\\mathcal{O}} \\left( \\mu |\\mathcal{S}| H / N \\right)$ which we show is optimal up to log-factors. In contrast, we show that any algorithm which does not interact with the MDP and uses an offline dataset of $N$ expert trajectories must incur suboptimality growing as $\\gtrsim |\\mathcal{S}| H^2/N$ even under the $\\mu$-recoverability assumption. This establishes a clear and provable separation of the minimax rates between the active setting and the no-interaction setting. We also study IL with linear function approximation. When the expert plays actions according to a linear classifier of known state-action features, we use the reduction to multi-class classification to show that with high probability, the suboptimality of behavior cloning is  $\\widetilde{O}(dH^2/N)$ given $N$ rollouts from the optimal policy. This is optimal up to log-factors but can be improved to $\\widetilde{O}(dH/N)$ if we have a linear expert with parameter-sharing across time steps. In contrast, when the MDP transition structure is known to the learner such as in the case of simulators, we demonstrate fundamental differences compared to the tabular setting in terms of the performance of an optimal algorithm, Mimic-MD (Rajaraman et al. (2020)) when extended to the function approximation setting. Here, we introduce a new problem called confidence set linear classification, that can be used to construct sample-efficient IL algorithms."}}
{"id": "rHCXjnwKV9_", "cdate": 1609459200000, "mdate": 1683657780531, "content": {"title": "Adversarial Combinatorial Bandits with General Non-linear Reward Functions", "abstract": "In this paper we study the adversarial combinatorial bandit with a known non-linear reward function, extending existing work on adversarial linear combinatorial bandit. {The adversarial combinatori..."}}
