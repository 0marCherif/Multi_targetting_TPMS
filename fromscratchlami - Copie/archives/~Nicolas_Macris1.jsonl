{"id": "O2cW5Q3bH_M", "cdate": 1663850308842, "mdate": null, "content": {"title": "Gradient flow in the gaussian covariate model: exact solution of learning curves and multiple descent structures", "abstract": "A recent line of work has shown remarkable behaviors of the generalization error curves in simple learning models. Even the least-squares regression has shown atypical features such as the model-wise double descent, and further works have observed triple or multiple descents. Another important characteristic are the epoch-wise descent structures which emerge during training. The observations of model-wise and epoch-wise descents have been analytically derived in limited theoretical settings (such as the random feature model) and are otherwise experimental. In this work, we provide a full and unified analysis of the whole time-evolution of the generalization curve, in the asymptotic large-dimensional regime and under gradient-flow, within a wider theoretical setting stemming from a gaussian covariate model. In particular, we cover most cases already disparately observed in the literature, and also provide examples of the existence of multiple descent structures as a function of a model parameter or time. Furthermore, we show that our theoretical predictions adequately match the learning curves obtained by gradient descent over realistic datasets.\nTechnically we compute averages of rational expressions involving random matrices using recent developments in random matrix theory based on \"linear pencils\". Another contribution, which is also of independent interest in random matrix theory, is a new derivation of related fixed point equations (and an extension there-off) using Dyson brownian motions."}}
{"id": "ae33sQe7xVf", "cdate": 1640995200000, "mdate": 1662816709136, "content": {"title": "Mismatched Estimation of Non-Symmetric Rank-One Matrices Under Gaussian Noise", "abstract": "We consider the estimation of a n\u00d7m matrix u <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u2217</sup> v <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u2217T</sup> observed through an additive Gaussian noise channel, a problem that frequently arises in statistics and machine learning. We investigate a scenario involving mismatched Bayesian inference in which the statistician is unaware of true prior and uses an assumed prior. We derive the exact analytic expression for the asymptotic mean squared error (MSE) in the large system size limit for the particular case of Gaussian priors and additive noise. Our formulas demonstrate that in the mismatched case, estimation is still possible. Additionally, the minimum MSE (MMSE) can be obtained by selecting a non-trivial set of parameters beyond the matched parameters. Our technique is based on the asymptotic behavior of spherical integrals for rectangular matrices. Our method can be extended to non-rotation-invariant distributions for the true prior but requires rotation invariance for the statistician\u2019s assumed prior."}}
{"id": "luCVRHASXC0", "cdate": 1621630218083, "mdate": null, "content": {"title": "Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model", "abstract": "Recent evidence has shown the existence of a so-called double-descent and even triple-descent behavior for the generalization error of deep-learning models. This important phenomenon commonly appears in implemented neural network architectures, and also seems to emerge in epoch-wise curves during the training process. A recent line of research has highlighted that random matrix tools can be used to obtain precise analytical asymptotics of the generalization (and training) errors of the random feature model. In this contribution, we analyze the whole temporal behavior of the generalization and training errors under gradient flow for the random feature model. We show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. This allows us to observe how the double and triple descents develop over time, if and when early stopping is an option, and also observe time-wise descent structures. Our techniques are based on Cauchy complex integral representations of the errors together with recent random matrix methods based on linear pencils. "}}
{"id": "ILkcpB76rGc", "cdate": 1621000834905, "mdate": null, "content": {"title": "Information theoretic limits of learning a sparse rule", "abstract": "We consider generalized linear models in regimes where the number of non zero components of the signal and accessible data points are sublinear with respect to the size of the signal. We prove a variational formula for the asymptotic mutual information per sample when the system size grows to infinity. This result allows us to derive an expression for the minimum mean-square error (MMSE) of the Bayesian estimator when the signal entries have a discrete distribution with finite support.   We find that,  for such signals and suitable vanishing scalings of the sparsity and sampling rate, the MMSE is non increasing piecewise constant.  In specific instances the MMSE even displays an all-or-nothing phase transition,that is, the MMSE sharply jumps from its maximum value to zero at a critical sampling rate. The all-or-nothing phenomenon has previously been shown to occurin high-dimensional linear regression. Our analysis goes beyond the linear case andapplies to learning the weights of a perceptron with general activation function in a teacher-student scenario. In particular, we discuss an all-or-nothing phenomenon for the generalization error with a sublinear set of training examples."}}
{"id": "kw74CHPYPj5", "cdate": 1621000716029, "mdate": null, "content": {"title": "The layered structure of tensor estimation and its mutual information", "abstract": "We consider rank-one non-symmetric tensor estimation and derive simple formulas for the mutual information. We start by the order 2 problem, namely matrix factorization. We treat it completely in a simpler fashion than previous proofs using a new type of interpolation method developed in [1]. We then show how to harness the structure in \u201clayers\u201d of tensor estimation in order to obtain a formula for the mutual information for the order 3 problem from the knowledge of the formula for the order 2 problem, still using the same kind of interpolation. Our proof technique straightforwardly generalizes and allows to rigorously obtain the mutual information at any order in a recursive way."}}
{"id": "vnkDg6qSDY", "cdate": 1621000332044, "mdate": null, "content": {"title": "The committee machine: computational to statistical gaps in learning a two-layers neural network", "abstract": "Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine, under a technical assumption. We also introduce aversion of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap."}}
{"id": "gN2YalM9QI1", "cdate": 1609459200000, "mdate": 1662816709140, "content": {"title": "Rank-one matrix estimation: analytic time evolution of gradient descent dynamics", "abstract": "We consider a rank-one symmetric matrix corrupted by additive noise. The rank-one matrix is formed by an n-component unknown vector on the sphere of radius $\\sqrt{n}$, and we consider the problem o..."}}
{"id": "_6CzwavoMjc", "cdate": 1609459200000, "mdate": 1662816709160, "content": {"title": "Bell Diagonal and Werner State Generation: Entanglement, Non-Locality, Steering and Discord on the IBM Quantum Computer", "abstract": "We propose the first correct special-purpose quantum circuits for preparation of Bell diagonal states (BDS), and implement them on the IBM Quantum computer, characterizing and testing complex aspects of their quantum correlations in the full parameter space. Among the circuits proposed, one involves only two quantum bits but requires adapted quantum tomography routines handling classical bits in parallel. The entire class of Bell diagonal states is generated, and several characteristic indicators, namely entanglement of formation and concurrence, CHSH non-locality, steering and discord, are experimentally evaluated over the full parameter space and compared with theory. As a by-product of this work, we also find a remarkable general inequality between \u201cquantum discord\u201d and \u201casymmetric relative entropy of discord\u201d: the former never exceeds the latter. We also prove that for all BDS the two coincide."}}
{"id": "X_jVCcggEi", "cdate": 1609459200000, "mdate": 1662816709137, "content": {"title": "Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model", "abstract": "Recent evidence has shown the existence of a so-called double-descent and even triple-descent behavior for the generalization error of deep-learning models. This important phenomenon commonly appears in implemented neural network architectures, and also seems to emerge in epoch-wise curves during the training process. A recent line of research has highlighted that random matrix tools can be used to obtain precise analytical asymptotics of the generalization (and training) errors of the random feature model. In this contribution, we analyze the whole temporal behavior of the generalization and training errors under gradient flow for the random feature model. We show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. This allows us to observe how the double and triple descents develop over time, if and when early stopping is an option, and also observe time-wise descent structures. Our techniques are based on Cauchy complex integral representations of the errors together with recent random matrix methods based on linear pencils."}}
{"id": "W9Qe5KRipQI", "cdate": 1609459200000, "mdate": 1662816709162, "content": {"title": "Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model", "abstract": "Recent evidence has shown the existence of a so-called double-descent and even triple-descent behavior for the generalization error of deep-learning models. This important phenomenon commonly appears in implemented neural network architectures, and also seems to emerge in epoch-wise curves during the training process. A recent line of research has highlighted that random matrix tools can be used to obtain precise analytical asymptotics of the generalization (and training) errors of the random feature model. In this contribution, we analyze the whole temporal behavior of the generalization and training errors under gradient flow for the random feature model. We show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. This allows us to observe how the double and triple descents develop over time, if and when early stopping is an option, and also observe time-wise descent structures. Our techniques are based on Cauchy complex integral representations of the errors together with recent random matrix methods based on linear pencils."}}
