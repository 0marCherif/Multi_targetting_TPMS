{"id": "dp9gy4Vh6Y", "cdate": 1640995200000, "mdate": 1668181716526, "content": {"title": "Bi-directional Contrastive Learning for Domain Adaptive Semantic Segmentation", "abstract": ""}}
{"id": "A2GHgsRMq_R", "cdate": 1640995200000, "mdate": 1667436402061, "content": {"title": "Bi-directional Contrastive Learning for Domain Adaptive Semantic Segmentation", "abstract": "We present a novel unsupervised domain adaptation method for semantic segmentation that generalizes a model trained with source images and corresponding ground-truth labels to a target domain. A key to domain adaptive semantic segmentation is to learn domain-invariant and discriminative features without target ground-truth labels. To this end, we propose a bi-directional pixel-prototype contrastive learning framework that minimizes intra-class variations of features for the same object class, while maximizing inter-class variations for different ones, regardless of domains. Specifically, our framework aligns pixel-level features and a prototype of the same object class in target and source images (i.e., positive pairs), respectively, sets them apart for different classes (i.e., negative pairs), and performs the alignment and separation processes toward the other direction with pixel-level features in the source image and a prototype in the target image. The cross-domain matching encourages domain-invariant feature representations, while the bidirectional pixel-prototype correspondences aggregate features for the same object class, providing discriminative features. To establish training pairs for contrastive learning, we propose to generate dynamic pseudo labels of target images using a non-parametric label transfer, that is, pixel-prototype correspondences across different domains. We also present a calibration method compensating class-wise domain biases of prototypes gradually during training."}}
{"id": "6cwngiXjyN", "cdate": 1640995200000, "mdate": 1682320686881, "content": {"title": "Disentangled Representations for Short-Term and Long-Term Person Re-Identification", "abstract": "We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons could have the same attribute, and persons\u2019 appearances look different, e.g., with viewpoint changes. Recent reID methods focus on learning person features discriminative only for a particular factor of variations (e.g., human pose), which also requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to factorize person images into identity-related and -unrelated features. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose). To this end, we propose a new generative adversarial network, dubbed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">identity shuffle GAN</i> (IS-GAN). It disentangles identity-related and -unrelated features from person images through an identity-shuffling technique that exploits identification labels alone without any auxiliary supervisory signals. We restrict the distribution of identity-unrelated features, or encourage the identity-related and -unrelated features to be uncorrelated, facilitating the disentanglement process. Experimental results validate the effectiveness of IS-GAN, showing state-of-the-art performance on standard reID benchmarks, including Market-1501, CUHK03 and DukeMTMC-reID. We further demonstrate the advantages of disentangling person representations on a long-term reID task, setting a new state of the art on a Celeb-reID dataset. Our code and models are available online: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://cvlab-yonsei.github.io/projects/ISGAN/</uri> ."}}
{"id": "tgcDrub-a6N", "cdate": 1609459200000, "mdate": 1667436402075, "content": {"title": "Video-based Person Re-identification with Spatial and Temporal Memory Networks", "abstract": "Video-based person re-identification (reID) aims to retrieve person videos with the same identity as a query person across multiple cameras. Spatial and temporal distractors in person videos, such as background clutter and partial occlusions over frames, respectively, make this task much more challenging than image-based person reID. We observe that spatial distractors appear consistently in a particular location, and temporal distractors show several patterns, e.g., partial occlusions occur in the first few frames, where such patterns provide informative cues for predicting which frames to focus on (i.e., temporal attentions). Based on this, we introduce a novel Spatial and Temporal Memory Networks (STMN). The spatial memory stores features for spatial distractors that frequently emerge across video frames, while the temporal memory saves attentions which are optimized for typical temporal patterns in person videos. We leverage the spatial and temporal memories to refine frame-level person representations and to aggregate the refined frame-level features into a sequence-level person representation, respectively, effectively handling spatial and temporal distractors in person videos. We also introduce a memory spread loss preventing our model from addressing particular items only in the memories. Experimental results on standard benchmarks, including MARS, DukeMTMC-VideoReID, and LSVID, demonstrate the effectiveness of our method."}}
{"id": "MhpOJ87L4", "cdate": 1578712413918, "mdate": null, "content": {"title": "Learning Disentangled Representation for Robust Person Re-identification", "abstract": "We address the problem of person re-identification (reID), that is, retrieving person\nimages from a large dataset, given a query image of the person of interest. A\nkey challenge is to learn person representations robust to intra-class variations, as\ndifferent persons can have the same attribute and the same person\u2019s appearance\nlooks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human\npose), which requires corresponding supervisory signals (e.g., pose annotations).\nTo tackle this problem, we propose to disentangle identity-related and -unrelated\nfeatures from person images. Identity-related features contain information useful\nfor specifying a particular person (e.g., clothing), while identity-unrelated ones\nhold other factors (e.g., human pose, scale changes). To this end, we introduce a\nnew generative adversarial network, dubbed identity shuffle GAN (IS-GAN), that\nfactorizes these features using identification labels without any auxiliary information. We also propose an identity-shuffling technique to regularize the disentangled\nfeatures. Experimental results demonstrate the effectiveness of IS-GAN, significantly outperforming the state of the art on standard reID benchmarks including the\nMarket-1501, CUHK03 and DukeMTMC-reID. Our code and models are available\nonline: https://cvlab-yonsei.github.io/projects/ISGAN/."}}
{"id": "01T5Y9j8Ubi", "cdate": 1577836800000, "mdate": 1667436402057, "content": {"title": "Temporally Consistent Depth Prediction With Flow-Guided Memory Units", "abstract": "Predicting depth from a monocular video sequence is an important task for autonomous driving. Although it has advanced considerably in the past few years, recent methods based on convolutional neural networks (CNNs) discard temporal coherence in the video sequence and estimate depth independently for each frame, which often leads to undesired inconsistent results over time. To address this problem, we propose to memorize temporal consistency in the video sequence, and leverage it for the task of depth prediction. To this end, we introduce a two-stream CNN with a flow-guided memory module, where each stream encodes visual and temporal features, respectively. The memory module, implemented using convolutional gated recurrent units (ConvGRUs), inputs visual and temporal features sequentially together with optical flow tailored to our task. It memorizes trajectories of individual features selectively and propagates spatial information over time, enforcing a long-term temporal consistency to prediction results. We evaluate our method on the KITTI benchmark dataset in terms of depth prediction accuracy, temporal consistency and runtime, and achieve a new state of the art. We also provide an extensive experimental analysis, clearly demonstrating the effectiveness of our approach to memorizing temporal consistency for depth prediction."}}
{"id": "r1xTFEHgLr", "cdate": 1567802501151, "mdate": null, "content": {"title": "Learning Disentangled Representation for Robust Person Re-identification", "abstract": "We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. The key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same person's appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human pose) and this requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person (e.g.,clothing), while identity-unrelated ones hold other factors (e.g., human pose, scale changes). To this end, we introduce a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN), that factorizes these features using identification labels without any auxiliary information. We also propose an identity shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, largely outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID. Our code and models will be available online at the time of the publication. "}}
