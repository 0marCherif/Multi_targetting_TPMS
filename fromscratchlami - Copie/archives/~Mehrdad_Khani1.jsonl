{"id": "XE3BTiKYCO", "cdate": 1672531200000, "mdate": 1693587398532, "content": {"title": "RECL: Responsive Resource-Efficient Continuous Learning for Video Analytics", "abstract": ""}}
{"id": "AGw1_LIZYG", "cdate": 1640995200000, "mdate": 1696409097476, "content": {"title": "Gemino: Practical and Robust Neural Compression for Video Conferencing", "abstract": "Video conferencing systems suffer from poor user experience when network conditions deteriorate because current video codecs simply cannot operate at extremely low bitrates. Recently, several neural alternatives have been proposed that reconstruct talking head videos at very low bitrates using sparse representations of each frame such as facial landmark information. However, these approaches produce poor reconstructions in scenarios with major movement or occlusions over the course of a call, and do not scale to higher resolutions. We design Gemino, a new neural compression system for video conferencing based on a novel high-frequency-conditional super-resolution pipeline. Gemino upsamples a very low-resolution version of each target frame while enhancing high-frequency details (e.g., skin texture, hair, etc.) based on information extracted from a single high-resolution reference image. We use a multi-scale architecture that runs different components of the model at different resolutions, allowing it to scale to resolutions comparable to 720p, and we personalize the model to learn specific details of each person, achieving much better fidelity at low bitrates. We implement Gemino atop aiortc, an open-source Python implementation of WebRTC, and show that it operates on 1024x1024 videos in real-time on a Titan X GPU, and achieves 2.2-5x lower bitrate than traditional video codecs for the same perceptual quality."}}
{"id": "S3GbVziVfm9", "cdate": 1609459200000, "mdate": 1648671500179, "content": {"title": "Efficient Video Compression via Content-Adaptive Super-Resolution", "abstract": "Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 20% of the bits-per-pixel of H.265 in slow mode, and 3% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on an NVIDIA V100 GPU."}}
{"id": "H4x-NfiNf7q", "cdate": 1609459200000, "mdate": 1648671500192, "content": {"title": "Real-Time Video Inference on Edge Devices via Adaptive Model Streaming", "abstract": "Real-time video inference on edge devices like mobile phones and drones is challenging due to the high computation cost of Deep Neural Networks. We present Adaptive Model Streaming (AMS), a new approach to improving the performance of efficient lightweight models for video inference on edge devices. AMS uses a remote server to continually train and adapt a small model running on the edge device, boosting its performance on the live video using online knowledge distillation from a large, state-of-the-art model. We discuss the challenges of over-the-network model adaptation for video inference and present several techniques to reduce communication the cost of this approach: avoiding excessive overfitting, updating a small fraction of important model parameters, and adaptive sampling of training frames at edge devices. On the task of video semantic segmentation, our experimental results show 0.4\u201317.8 percent mean Intersection-over-Union improvement compared to a pretrained model across several video datasets. Our prototype can perform video segmentation at 30 frames-per-second with 40 milliseconds camera-to-label latency on a Samsung Galaxy S10+ mobile phone, using less than 300 Kbps uplink and downlink bandwidth on the device."}}
{"id": "H-E-EGoVzm5", "cdate": 1609459200000, "mdate": 1648671500193, "content": {"title": "SiP-ML: high-bandwidth optical network interconnects for machine learning training", "abstract": "This paper proposes optical network interconnects as a key enabler for building high-bandwidth ML training clusters with strong scaling properties. Our design, called SiP-ML, accelerates the training time of popular DNN models using silicon photonics links capable of providing multiple terabits-per-second of bandwidth per GPU. SiP-ML partitions the training job across GPUs with hybrid data and model parallelism while ensuring the communication pattern can be supported efficiently on the network interconnect. We develop task partitioning and device placement methods that take the degree and reconfiguration latency of optical interconnects into account. Simulations using real DNN models show that, compared to the state-of-the-art electrical networks, our approach improves training time by 1.3--9.1x."}}
{"id": "SnVZ4Ms4fX5", "cdate": 1577836800000, "mdate": 1648671500179, "content": {"title": "Adaptive Neural Signal Detection for Massive MIMO", "abstract": "Traditional symbol detection algorithms either perform poorly or are impractical to implement for Massive Multiple-Input Multiple-Output (MIMO) systems. Recently, several learning-based approaches have achieved promising results on simple channel models (e.g., i.i.d. Gaussian channel coefficients), but as we show, their performance degrades on real-world channels with spatial correlation. We propose MMNet, a deep learning MIMO detection scheme that significantly outperforms existing approaches on realistic channels with the same or lower computational complexity. MMNet's design builds on the theory of iterative soft-thresholding algorithms, and uses a novel training algorithm that leverages temporal and spectral correlation in real channels to accelerate training. These innovations make it practical to train MMNet online for every realization of the channel. On i.i.d. Gaussian channels, MMNet requires two orders of magnitude fewer operations than existing deep learning schemes but achieves near-optimal performance. On spatiallycorrelated channels, it achieves the same error rate as the next-best learning scheme (OAMPNet) at 2.5dB lower signalto-noise ratio (SNR), and with at least 10\u00d7 less computational complexity. MMNet is also 4-8dB better overall than a classic linear scheme like the minimum mean square error (MMSE) detector."}}
{"id": "BkgfRbEPsE", "cdate": 1556722121781, "mdate": null, "content": {"title": "Park: An Open Platform for Learning Augmented Computer Systems", "abstract": "This paper presents Park, an open extensible platform that uses a common interface to connect to a suite of real world computer systems for RL augmented optimizations. These systems cover a wide spectrum of problems, including both global vs. distributed control, and fast control loop vs. long term planning. This dataset unveils unique challenges that the existing off-the-shelf RL techniques cannot solve. The challenges occur in the representation and search of the state-action space, the special property of the decision process and the reality gap between simulations and actual systems. To understand the effect of these challenges, we benchmark several existing RL algorithms in Park with comparing heuristic baselines."}}
{"id": "HKLZEzjEfmq", "cdate": 1546300800000, "mdate": 1648671500180, "content": {"title": "Park: An Open Platform for Learning-Augmented Computer Systems", "abstract": "We present Park, a platform for researchers to experiment with Reinforcement Learning (RL) for computer systems. Using RL for improving the performance of systems has a lot of potential, but is also in many ways very different from, for example, using RL for games. Thus, in this work we first discuss the unique challenges RL for systems has, and then propose Park an open extensible platform, which makes it easier for ML researchers to work on systems problems. Currently, Park consists of 12 real world system-centric optimization problems with one common easy to use interface. Finally, we present the performance of existing RL approaches over those 12 problems and outline potential areas of future work."}}
