{"id": "nYbD8214iY", "cdate": 1701388800000, "mdate": 1696084543546, "content": {"title": "A learnable self-supervised task for unsupervised domain adaptation on point cloud classification and segmentation", "abstract": "To address the UDA problem for point clouds, we propose a novel learnable self-supervised task that helps the adapted neural network extract transferable features. Specifically, we propose a learnable point cloud transformation and use it in a point cloud destruction-reconstruction self-supervised auxiliary task. We train the main task network and the auxiliary task network, which share an encoder, so that the encoder extracts features that are highly transferable to the target domain. We further propose a multi-region transformation strategy to make the network focus on local features, which are more transferable. New state-of-the-art performance is achieved on the point cloud classification and segmentation UDA benchmarks."}}
{"id": "qeMv5qvbVM", "cdate": 1690848000000, "mdate": 1696084543546, "content": {"title": "AIM-MEF: Multi-exposure image fusion based on adaptive information mining in both spatial and frequency domains", "abstract": ""}}
{"id": "DfJBJ7wbxa", "cdate": 1688169600000, "mdate": 1696084543605, "content": {"title": "Dual-Branch Deep Point Cloud Registration Framework for Unconstrained Rotation", "abstract": "Learning-based rigid point cloud registration (RPCR) studies have made great progress recently but most existing methods have a small convergence region and can only be used to solve the registration problem with a small rotation angle, which is usually constrained within <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$[0, 45^\\circ ]$</tex-math></inline-formula> . However, the relative rotation between point clouds is usually unconstrained in practice. To address this challenging problem, we propose a new RPCR network and integrate it into a new dual-branch registration framework for unconstrained rotation point cloud registration. The dual-branch framework consists of a large-rotation branch and a small-rotation branch, which are used to accurately register point clouds with large and small relative rotations, respectively. In addition, we propose a multiview intersection over the union module to select a better registration result from the output of the two branches. Extensive experiments on both ModelNet40 and MVP-RG datasets demonstrate that our proposed method outperforms existing state-of-the-art techniques by a large margin."}}
{"id": "cUn3O9p2TRS", "cdate": 1682899200000, "mdate": 1683788593362, "content": {"title": "Robust Point Cloud Registration Framework Based on Deep Graph Matching", "abstract": "3D point cloud registration is a fundamental problem in computer vision and robotics. Recently, learning-based point cloud registration methods have made great progress. However, these methods are sensitive to outliers, which lead to more incorrect correspondences. In this paper, we propose a novel deep graph matching-based framework for point cloud registration. Specifically, we first transform point clouds into graphs and extract deep features for each point. Then, we develop a module based on deep graph matching to calculate a soft correspondence matrix. By using graph matching, not only the local geometry of each point but also its structure and topology in a larger range are considered in establishing correspondences, so that more correct correspondences are found. We train the network with a loss directly defined on the correspondences, and in the test stage the soft correspondences are transformed into hard one-to-one correspondences so that registration can be performed by a correspondence-based solver. Furthermore, we introduce a transformer-based method to generate edges for graph construction, which further improves the quality of the correspondences. Extensive experiments on object-level and scene-level benchmark datasets show that the proposed method achieves state-of-the-art performance."}}
{"id": "ImqOL-4kQY", "cdate": 1680307200000, "mdate": 1681691496806, "content": {"title": "Rethinking multi-exposure image fusion with extreme and diverse exposure levels: A robust framework based on Fourier transform and contrastive learning", "abstract": ""}}
{"id": "mFPl5JhsogM", "cdate": 1672531200000, "mdate": 1681691496804, "content": {"title": "Boosting 3D Point Cloud Registration by Transferring Multi-modality Knowledge", "abstract": "The recent multi-modality models have achieved great performance in many vision tasks because the extracted features contain the multi-modality knowledge. However, most of the current registration descriptors have only concentrated on local geometric structures. This paper proposes a method to boost point cloud registration accuracy by transferring the multi-modality knowledge of pre-trained multi-modality model to a new descriptor neural network. Different to the previous multi-modality methods that requires both modalities, the proposed method only requires point clouds during inference. Specifically, we propose an ensemble descriptor neural network combining pre-trained sparse convolution branch and a new point-based convolution branch. By fine-tuning on a single modality data, the proposed method achieves new state-of-the-art results on 3DMatch and competitive accuracy on 3DLoMatch and KITTI."}}
{"id": "UMZXIz2vNxH", "cdate": 1672531200000, "mdate": 1696084543865, "content": {"title": "PointMBF: A Multi-scale Bidirectional Fusion Network for Unsupervised RGB-D Point Cloud Registration", "abstract": "Point cloud registration is a task to estimate the rigid transformation between two unaligned scans, which plays an important role in many computer vision applications. Previous learning-based works commonly focus on supervised registration, which have limitations in practice. Recently, with the advance of inexpensive RGB-D sensors, several learning-based works utilize RGB-D data to achieve unsupervised registration. However, most of existing unsupervised methods follow a cascaded design or fuse RGB-D data in a unidirectional manner, which do not fully exploit the complementary information in the RGB-D data. To leverage the complementary information more effectively, we propose a network implementing multi-scale bidirectional fusion between RGB images and point clouds generated from depth images. By bidirectionally fusing visual and geometric features in multi-scales, more distinctive deep features for correspondence estimation can be obtained, making our registration more accurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our method achieves new state-of-the-art performance. Code will be released at https://github.com/phdymz/PointMBF"}}
{"id": "QQoM6IkMH4g", "cdate": 1672531200000, "mdate": 1696084543578, "content": {"title": "Reducing Domain Gap in Frequency and Spatial Domain for Cross-Modality Domain Adaptation on Medical Image Segmentation", "abstract": "Unsupervised domain adaptation (UDA) aims to learn a model trained on source domain and performs well on unlabeled target domain. In medical image segmentation field, most existing UDA methods depend on adversarial learning to address the domain gap between different image modalities, which is ineffective due to its complicated training process. In this paper, we propose a simple yet effective UDA method based on frequency and spatial domain transfer under multi-teacher distillation framework. In the frequency domain, we first introduce non-subsampled contourlet transform for identifying domain-invariant and domain-variant frequency components (DIFs and DVFs), and then keep the DIFs unchanged while replacing the DVFs of the source domain images with that of the target domain images to narrow the domain gap. In the spatial domain, we propose a batch momentum update-based histogram matching strategy to reduce the domain-variant image style bias. Experiments on two commonly used cross-modality medical image segmentation datasets show that our proposed method achieves superior performance compared to state-of-the-art methods."}}
{"id": "PXEcn5KYAOi", "cdate": 1672531200000, "mdate": 1696084543608, "content": {"title": "Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need", "abstract": "Weakly supervised whole slide image classification is usually formulated as a multiple instance learning (MIL) problem, where each slide is treated as a bag, and the patches cut out of it are treated as instances. Existing methods either train an instance classifier through pseudo-labeling or aggregate instance features into a bag feature through attention mechanisms and then train a bag classifier, where the attention scores can be used for instance-level classification. However, the pseudo instance labels constructed by the former usually contain a lot of noise, and the attention scores constructed by the latter are not accurate enough, both of which affect their performance. In this paper, we propose an instance-level MIL framework based on contrastive learning and prototype learning to effectively accomplish both instance classification and bag classification tasks. To this end, we propose an instance-level weakly supervised contrastive learning algorithm for the first time under the MIL setting to effectively learn instance feature representation. We also propose an accurate pseudo label generation method through prototype learning. We then develop a joint training strategy for weakly supervised contrastive learning, prototype learning, and instance classifier training. Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method. Codes will be available."}}
{"id": "Ato2F56LDl", "cdate": 1672531200000, "mdate": 1696084543566, "content": {"title": "Boosting 3D Point Cloud Registration by Transferring Multi-modality Knowledge", "abstract": "The recent multi-modality models have achieved great performance in many vision tasks because the extracted features contain the multi-modality knowledge. However, most of the current registration descriptors have only concentrated on local geometric structures. This paper proposes a method to boost point cloud registration accuracy by transferring the multi-modality knowledge of pre-trained multi-modality model to a new descriptor neural network. Different to the previous multi-modality methods that requires both modalities, the proposed method only requires point clouds during inference. Specifically, we propose an ensemble descriptor neural network combining pre-trained sparse convolution branch and a new point-based convolution branch. By fine-tuning on a single modality data, the proposed method achieves new state-of-the-art results on 3DMatch and competitive accuracy on 3DLoMatch and KITTI. The code and the trained model will be released at https://github.com/phdymz/DBENet.git."}}
