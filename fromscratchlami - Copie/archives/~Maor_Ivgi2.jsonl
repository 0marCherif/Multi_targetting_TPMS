{"id": "KmHIKFULoZ", "cdate": 1672531200000, "mdate": 1681490149226, "content": {"title": "DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule", "abstract": ""}}
{"id": "HiwqoKAHGiN", "cdate": 1672531200000, "mdate": 1696122820927, "content": {"title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding", "abstract": "We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard"}}
{"id": "BVisv9lsvps", "cdate": 1672531200000, "mdate": 1696122820789, "content": {"title": "DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule", "abstract": "We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and no..."}}
{"id": "iDBX_iFoI2", "cdate": 1640995200000, "mdate": 1681532152731, "content": {"title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments", "abstract": ""}}
{"id": "eiCb_PE1oN", "cdate": 1640995200000, "mdate": 1681532152726, "content": {"title": "SCROLLS: Standardized CompaRison Over Long Language Sequences", "abstract": ""}}
{"id": "MiMcvToLA7A", "cdate": 1640995200000, "mdate": 1681532152841, "content": {"title": "Efficient Long-Text Understanding with Short-Text Models", "abstract": ""}}
{"id": "ISliqZ7BigE", "cdate": 1640995200000, "mdate": 1654068971931, "content": {"title": "SCROLLS: Standardized CompaRison Over Long Language Sequences", "abstract": "NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods."}}
{"id": "Humczw4mNO", "cdate": 1640995200000, "mdate": 1681532152728, "content": {"title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments", "abstract": ""}}
{"id": "0dV2hHjSqg", "cdate": 1640995200000, "mdate": 1696122820808, "content": {"title": "Beyond Importance Scores: Interpreting Tabular ML by Visualizing Feature Semantics", "abstract": "Interpretability is becoming an active research topic as machine learning (ML) models are more widely used to make critical decisions. Tabular data are one of the most commonly used modes of data in diverse applications such as healthcare and finance. Much of the existing interpretability methods used for tabular data only report feature-importance scores\u2014either locally (per example) or globally (per model)\u2014but they do not provide interpretation or visualization of how the features interact. We address this limitation by introducing Feature Vectors, a new global interpretability method designed for tabular datasets. In addition to providing feature-importance, Feature Vectors discovers the inherent semantic relationship among features via an intuitive feature visualization technique. Our systematic experiments demonstrate the empirical utility of this new method by applying it to several real-world datasets. We further provide an easy-to-use Python package for Feature Vectors."}}
{"id": "mm9aeWfm3Hx", "cdate": 1609459200000, "mdate": 1681532152911, "content": {"title": "Achieving Model Robustness through Discrete Adversarial Training", "abstract": ""}}
