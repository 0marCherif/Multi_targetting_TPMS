{"id": "OlAZAhtk1od", "cdate": 1684183805889, "mdate": 1684183805889, "content": {"title": "The Numerical Stability of Hyperbolic Representation Learning", "abstract": "Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\u00e9 ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\u00e9 ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\u00e9 ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Euclidean parametrization of the hyperbolic space which can alleviate these limitations. We further extend this Euclidean parametrization to hyperbolic hyperplanes and exhibits its ability in improving the performance of hyperbolic SVM."}}
{"id": "hSR5sYywxGv", "cdate": 1680698079435, "mdate": null, "content": {"title": "Graph Laplacian Learning with Exponential Family Noise", "abstract": "Graph signal processing (GSP) has generalized classical Fourier analysis to signals lying on irregular structures such as networks. However, a common challenge in applying graph signal processing is that the underlying graph of a system is unknown. Well-established methods optimize a graph representation, usually the graph adjacency matrix or the graph Laplacian, so that the total variation of given signals will be minimal on the learned graph. These methods have been developed for continuous graph signals, however inferring the graph structure for other types of data, such as discrete counts or binary signal, remains underexplored. In this talk, I will address the problem of learning the graph Laplacian from noisy data and generalize a GSP framework for learning a graph from smooth graph signals to exponential family noise distributions, allowing for the modeling of various data types. We then propose Graph Learning with Exponential family Noise (GLEN), an alternating algorithm that estimates the underlying graph Laplacian as well as the unobserved smooth representation from the noisy signals. Furthermore, we extend GLEN to the time-vertex setting to handle data with temporal correlations, e.g., time-series on a network. We demonstrate in synthetic experiments with different graph models that GLEN outperforms competing Laplacian estimation methods under noise model mismatch. Furthermore, we apply GLEN to different types of real-world data (e.g., binary questionnaires, neural activity) to further demonstrate the efficacy of our methods."}}
{"id": "SvcawuEiUVM", "cdate": 1663850249921, "mdate": null, "content": {"title": "Compositional Image Generation and Manipulation with Latent Diffusion Models", "abstract": "We propose a principled method for compositional image generation and manipulation using diffusion probabilistic models. In particular, for any pre-trained generative model with a semantic latent space, we train a latent diffusion model and auxiliary latent classifiers to help navigate latent representations in a non-linear fashion. We show that such conditional generation achieved by latent classifier guidance provably maximizes a lower bound of the conditional log-likelihood during training, and can reduce to a simple latent arithmetic method with additional assumption, which is surprisingly under-studied in the context of compositionality. We then derive a new guidance term which is shown to be crucial for maintaining the original semantics when doing manipulation. Unlike previous methods, our method is agnostic to pre-trained generative models and latent spaces, while still achieving competitive performance on compositional image generation as well as sequential manipulation of real and synthetic images."}}
{"id": "joZ4CuOyKY8", "cdate": 1652737660645, "mdate": null, "content": {"title": "DiSC: Differential Spectral Clustering of Features", "abstract": "Selecting subsets of features that differentiate between two conditions is a key task in a broad range of scientific domains. In many applications, the features of interest form clusters with similar effects on the data at hand. To recover such clusters we develop DiSC, a data-driven approach for detecting groups of features that differentiate between conditions. For each condition, we construct a graph whose nodes correspond to the features and whose weights are functions of the similarity between them for that condition. We then apply a spectral approach to compute subsets of nodes whose connectivity pattern differs significantly between the condition-specific feature graphs. On the theoretical front, we analyze our approach with a toy example based on the stochastic block model. We evaluate DiSC on a variety of datasets, including MNIST, hyperspectral imaging, simulated scRNA-seq and task fMRI, and demonstrate that DiSC uncovers features that better differentiate between conditions compared to competing methods."}}
{"id": "qtiU1bnLcB", "cdate": 1640995200000, "mdate": 1672777068839, "content": {"title": "Multi-scale affinities with missing data: Estimation and applications", "abstract": ""}}
{"id": "q9yMu3VuZP", "cdate": 1640995200000, "mdate": 1672777068840, "content": {"title": "GraFT: Graph Filtered Temporal Dictionary Learning for Functional Neural Imaging", "abstract": ""}}
{"id": "YbXnQjBbcS3", "cdate": 1640995200000, "mdate": 1672777068594, "content": {"title": "DiSC: Differential Spectral Clustering of Features", "abstract": ""}}
{"id": "Qy-hiuZV9H", "cdate": 1640995200000, "mdate": 1672777068683, "content": {"title": "Evaluating Disentanglement in Generative Models Without Knowledge of Latent Factors", "abstract": ""}}
{"id": "BJDThV_mA8", "cdate": 1640995200000, "mdate": 1672777068470, "content": {"title": "Applications and Comparison of Dimensionality Reduction Methods for Microbiome Data", "abstract": ""}}
{"id": "7zc05Ua_HOK", "cdate": 1632875750338, "mdate": null, "content": {"title": "Learning Sample Reweighting for Adversarial Robustness", "abstract": "There has been great interest in enhancing the robustness of neural network classifiers to defend against adversarial perturbations through adversarial training, while balancing the trade-off between robust accuracy and standard accuracy. We propose a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. Inspired by MAML-based approaches, we formulate weighted adversarial training as a bilevel optimization problem where the upper-level task corresponds to learning a robust classifier, and the lower-level task corresponds to learning a parametric function that maps from a sample's \\textit{multi-class margin} to an importance weight. Extensive experiments demonstrate that our approach improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines. "}}
