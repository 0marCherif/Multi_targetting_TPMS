{"id": "_Srox2nVaZ4", "cdate": 1676827076116, "mdate": null, "content": {"title": "Random Reshuffling with Variance Reduction: New Analysis and Better Rates", "abstract": "Virtually all state-of-the-art methods for training supervised machine learning models are variants of SGD, enhanced with a number of additional tricks, such as minibatching, momentum, and adaptive stepsizes. However, one of the most basic questions in the design of  successful SGD methods, one that is orthogonal to the aforementioned tricks, is the choice of the next training data point to be learning from. Standard variants of SGD employ a sampling with replacement strategy, which means that the next training data point is sampled from the entire data set, often independently of all previous samples. While standard SGD is well understood theoretically,  virtually all widely used machine learning software is based on sampling without replacement as this is often empirically superior. That is, the training data is randomly shuffled/permuted, either only once at the beginning, strategy known as random shuffling (RS), or before every epoch, strategy known as random reshuffling (RR),  and  training proceeds in the data order dictated by the shuffling. RS and RR strategies  have for a long time remained beyond the reach of  theoretical analysis that would satisfactorily explain their success. However, very recently, Mishchenko et al. [2020] provided tight sublinear convergence rates through a novel analysis, and showed that these strategies can improve upon standard SGD in certain regimes. Inspired by these results, we seek to further  improve the rates of shuffling-based methods. In particular, we show that it is possible to enhance them with a variance reduction mechanism, obtaining  linear convergence rates. To the best of our knowledge, our linear convergence rates are the best for any method based on sampling without replacement. "}}
{"id": "LvwOdSbB9Ic", "cdate": 1663849916884, "mdate": null, "content": {"title": "Minibatch Stochastic Three Points Method for Unconstrained Smooth Minimization", "abstract": "In this paper, we propose a new zero order optimization method called minibatch stochastic three points (MiSTP) method to solve an unconstrained minimization problem in a setting where only an approximation of the objective function evaluation is possible. It is based on the recently proposed stochastic three points (STP) method (Bergou et al., 2020). At each iteration, MiSTP generates a random search direction in a similar manner to STP, but chooses the next iterate based solely on the approximation of the objective function rather than its exact evaluations. We also analyze our method\u2019s complexity in the nonconvex and convex cases and evaluate its performance on multiple machine learning tasks."}}
{"id": "edkno3SvKo", "cdate": 1652737738517, "mdate": null, "content": {"title": "Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning", "abstract": "We study distributed optimization methods based on the {\\em local training (LT)} paradigm, i.e., methods which achieve communication efficiency by performing richer local gradient-based training on the clients before (expensive) parameter averaging is allowed to take place. While these methods were first proposed about a decade ago, and form the algorithmic backbone of federated learning, there is an enormous gap between their practical performance, and our theoretical understanding. Looking back at the progress of the field, we {\\em identify 5 generations of LT methods}: 1) heuristic, 2) homogeneous, 3) sublinear, 4) linear, and 5) accelerated. The 5${}^{\\rm th}$ generation was initiated by the ProxSkip method of Mishchenko et al. (2022), whose analysis provided the first theoretical confirmation that LT is a communication acceleration mechanism. Inspired by this recent progress, we contribute to the 5${}^{\\rm th}$ generation of LT methods by showing that it is possible to enhance ProxSkip further using {\\em variance reduction}. While all previous theoretical results for LT methods ignore the cost of local work altogether, and are framed purely in terms of the number of communication rounds, we construct a method that can be substantially faster in terms of the {\\em total training time} than the state-of-the-art method ProxSkip in theory and practice in the regime when local computation is sufficiently expensive. We characterize this threshold theoretically, and confirm our theoretical predictions with empirical results. Our treatment of variance reduction is generic, and can work with a large number of variance reduction techniques, which may lead to future applications in the future. Finally, we corroborate our theoretical results with carefully engineered proof-of-concept experiments."}}
{"id": "-RQQ5cbXVPR", "cdate": 1652457812966, "mdate": 1652457812966, "content": {"title": "From Local SGD to Local Fixed-Point Methods for Federated Learning", "abstract": "Most algorithms for solving optimization prob- lems or finding saddle points of convex\u2013concave functions are fixed-point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approxima- tion thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the com- putations done locally on a mobile device. We investigate two strategies to achieve such a con- sensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We per- form convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach."}}
{"id": "HENlQsIL46", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distributed Proximal Splitting Algorithms with Rates and Acceleration", "abstract": "We analyze several generic proximal splitting algorithms well suited for large-scale convex nonsmooth optimization. We derive sublinear and linear convergence results with new rates on the function value suboptimality or distance to the solution, as well as new accelerated versions, using varying stepsizes. In addition, we propose distributed variants of these algorithms, which can be accelerated as well. While most existing results are ergodic, our nonergodic results significantly broaden our understanding of primal-dual optimization algorithms."}}
{"id": "14e9QQuCFFF", "cdate": 1577836800000, "mdate": 1623608523677, "content": {"title": "From Local SGD to Local Fixed-Point Methods for Federated Learning", "abstract": "Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed-point algorithms. In this work we consider the generic problem of finding a fixed po..."}}
