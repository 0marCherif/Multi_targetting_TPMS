{"id": "g-qWfKQlL3", "cdate": 1663850179951, "mdate": null, "content": {"title": "Conditional Permutation Invariant Flows", "abstract": "We present a novel, conditional generative probabilistic model of set-valued data with a tractable log density.  This model is a continuous normalizing flow governed by permutation equivariant dynamics. These dynamics are driven by a learnable per-set-element term and pairwise interactions, both parametrized by deep neural networks.  We illustrate the utility of this model via applications including (1) complex traffic scene generation conditioned on visually specified map information, and (2) object bounding box generation conditioned directly on images.  We train our model by maximizing the expected likelihood of labeled conditional data under our flow, with the aid of a penalty that ensures the dynamics are smooth and hence efficiently solvable. Our method significantly outperforms non-permutation invariant baselines in terms of log likelihood and domain-specific metrics (offroad, collision, and combined infractions), yielding realistic samples that are difficult to distinguish from real data."}}
{"id": "ObtGcyKmwna", "cdate": 1663849925958, "mdate": null, "content": {"title": "Critic Sequential Monte Carlo", "abstract": "We introduce CriticSMC, a new algorithm for planning as inference built from a composition of sequential Monte Carlo with learned Soft-Q function heuristic factors. These heuristic factors, obtained from parametric approximations of the marginal likelihood ahead, more effectively guide SMC towards the desired target distribution, which is particularly helpful for planning in environments with hard constraints placed sparsely in time. Compared with previous work, we modify the placement of such heuristic factors, which allows us to cheaply propose and evaluate large numbers of putative action particles, greatly increasing inference and planning efficiency. CriticSMC is compatible with informative priors, whose density function need not be known, and can be used as a model-free control algorithm. Our experiments on collision avoidance in a high-dimensional simulated driving task show that CriticSMC significantly reduces collision rates at a low computational cost while maintaining realism and diversity of driving behaviors across vehicles and environment scenarios."}}
{"id": "Kqoxj7_1dZ", "cdate": 1640995200000, "mdate": 1681494432407, "content": {"title": "Conditional Permutation Invariant Flows", "abstract": ""}}
{"id": "2TVKeqiCBH", "cdate": 1640995200000, "mdate": 1681494432407, "content": {"title": "Critic Sequential Monte Carlo", "abstract": ""}}
{"id": "zonr2ydyMZo", "cdate": 1609459200000, "mdate": 1668749224471, "content": {"title": "Imagining The Road Ahead: Multi-Agent Trajectory Prediction via Differentiable Simulation", "abstract": "We develop a deep generative model built on a fully differentiable simulator for multi-agent trajectory prediction. Agents are modeled with conditional recurrent variational neural networks (CVRNNs), which take as input an ego-centric birdview image representing the current state of the world and output an action, consisting of steering and acceleration, which is used to derive the subsequent agent state using a kinematic bicycle model. The full simulation state is then differentiably rendered for each agent, initiating the next time step. We achieve state-of-the-art results on the INTERACTION dataset, using standard neural architectures and a standard variational training objective, producing realistic multi-modal predictions without any ad-hoc diversity-inducing losses. We conduct ablation studies to examine individual components of the simulator, finding that both the kinematic bicycle model and the continuous feedback from the birdview image are crucial for achieving this level of performance. We name our model ITRA, for \u201cImagining the Road Ahead\u201d."}}
{"id": "jetugFa2Ct-", "cdate": 1609459200000, "mdate": 1683904859084, "content": {"title": "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation", "abstract": "Ahmad Rashid, Vasileios Lioutas, Mehdi Rezagholizadeh. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "UqAKFuG502", "cdate": 1609459200000, "mdate": 1683904859084, "content": {"title": "Towards Zero-Shot Knowledge Distillation for Natural Language Processing", "abstract": ""}}
{"id": "q--S__NbRir", "cdate": 1577836800000, "mdate": 1650661448294, "content": {"title": "Improving Word Embedding Factorization for Compression using Distilled Nonlinear Neural Decomposition", "abstract": "Vasileios Lioutas, Ahmad Rashid, Krtin Kumar, Md. Akmal Haidar, Mehdi Rezagholizadeh. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
{"id": "GD7f0awIVqt", "cdate": 1577836800000, "mdate": 1682359481578, "content": {"title": "Time-aware Large Kernel Convolutions", "abstract": "To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to gener..."}}
{"id": "Bkga90VKDB", "cdate": 1569439381101, "mdate": null, "content": {"title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n"}}
