{"id": "fsKNY17EP95", "cdate": 1640995200000, "mdate": 1648710592690, "content": {"title": "Coresets for (k, \u2113 )-Median Clustering Under the Fr\u00e9chet Distance", "abstract": "We present an algorithm for computing \n                  \n                    \n                  \n                  $$\\varepsilon $$\n                  \n                -coresets for \n                  \n                    \n                  \n                  $$(k, \\ell )$$\n                  \n                -median clustering of polygonal curves in \n                  \n                    \n                  \n                  $$\\mathbb {R}^d$$\n                  \n                 under the Fr\u00e9chet distance. This type of clustering is an adaption of Euclidean k-median clustering: we are given a set of n polygonal curves in \n                  \n                    \n                  \n                  $$\\mathbb {R}^d$$\n                  \n                , each of complexity (number of vertices) at most m, and want to compute k median curves such that the sum of distances from the given curves to their closest median curve is minimal. Additionally, we restrict the complexity of the median curves to be at most \n                  \n                    \n                  \n                  $$\\ell $$\n                  \n                 each, to suppress overfitting, a problem specific for sequential data. Our algorithm has running time linear in n, sub-quartic in m and quadratic in \n                  \n                    \n                  \n                  $$\\varepsilon ^{-1}$$\n                  \n                . With high probability it returns \n                  \n                    \n                  \n                  $$\\varepsilon $$\n                  \n                -coresets of size quadratic in \n                  \n                    \n                  \n                  $$\\varepsilon ^{-1}$$\n                  \n                 and logarithmic in n and m. We achieve this result by applying the improved \n                  \n                    \n                  \n                  $$\\varepsilon $$\n                  \n                -coreset framework by Langberg and Feldman to a generalized k-median problem over an arbitrary metric space. Later we combine this result with the recent result by Driemel et al. on the VC dimension of metric balls under the Fr\u00e9chet distance. Furthermore, our framework yields \n                  \n                    \n                  \n                  $$\\varepsilon $$\n                  \n                -coresets for any generalized k-median problem where the range space induced by the open metric balls of the underlying space has bounded VC dimension, which is of independent interest. Finally, we show that our \n                  \n                    \n                  \n                  $$\\varepsilon $$\n                  \n                -coresets can be used to improve the running time of an existing approximation algorithm for \n                  \n                    \n                  \n                  $$(1,\\ell )$$\n                  \n                -median clustering."}}
{"id": "i3xXPW7SwGG", "cdate": 1609459200000, "mdate": null, "content": {"title": "Coresets for (k, \ud835\udcc1)-Median Clustering under the Fr\u00e9chet Distance", "abstract": "We present an algorithm for computing $\\epsilon$-coresets for $(k, \\ell)$-median clustering of polygonal curves in $\\mathbb{R}^d$ under the Fr\\'echet distance. This type of clustering is an adaption of Euclidean $k$-median clustering: we are given a set of $n$ polygonal curves in $\\mathbb{R}^d$, each of complexity (number of vertices) at most $m$, and want to compute $k$ median curves such that the sum of distances from the given curves to their closest median curve is minimal. Additionally, we restrict the complexity of the median curves to be at most $\\ell$ each, to suppress overfitting, a problem specific for sequential data. Our algorithm has running time linear in $n$, sub-quartic in $m$ and quadratic in $\\epsilon^{-1}$. With high probability it returns $\\epsilon$-coresets of size quadratic in $\\epsilon^{-1}$ and logarithmic in $n$ and $m$. We achieve this result by applying the improved $\\epsilon$-coreset framework by Langberg and Feldman to a generalized $k$-median problem over an arbitrary metric space. Later we combine this result with the recent result by Driemel et al. on the VC dimension of metric balls under the Fr\\'echet distance. Furthermore, our framework yields $\\epsilon$-coresets for any generalized $k$-median problem where the range space induced by the open metric balls of the underlying space has bounded VC dimension, which is of independent interest. Finally, we show that our $\\epsilon$-coresets can be used to improve the running time of an existing approximation algorithm for $(1,\\ell)$-median clustering."}}
{"id": "RYTWZFzoGKV", "cdate": 1609459200000, "mdate": 1648710592685, "content": {"title": "Approximating Length-Restricted Means under Dynamic Time Warping", "abstract": "We study variants of the mean problem under the $p$-Dynamic Time Warping ($p$-DTW) distance, a popular and robust distance measure for sequential data. In our setting we are given a set of finite point sequences over an arbitrary metric space and we want to compute a mean point sequence of given length that minimizes the sum of $p$-DTW distances, each raised to the $q$\\textsuperscript{th} power, between the input sequences and the mean sequence. In general, the problem is $\\mathrm{NP}$-hard and known not to be fixed-parameter tractable in the number of sequences. On the positive side, we show that restricting the length of the mean sequence significantly reduces the hardness of the problem. We give an exact algorithm running in polynomial time for constant-length means. We explore various approximation algorithms that provide a trade-off between the approximation factor and the running time. Our approximation algorithms have a running time with only linear dependency on the number of input sequences. In addition, we use our mean algorithms to obtain clustering algorithms with theoretical guarantees."}}
{"id": "JYpkj_ztzHl", "cdate": 1609459200000, "mdate": null, "content": {"title": "Approximating (k, \u2113-Median Clustering for Polygonal Curves", "abstract": "In 2015, Driemel, Krivo\u0161ija and Sohler introduced the (k, \u2113)-median problem for clustering polygonal curves under the Fr\u00e9chet distance. Given a set of input curves, the problem asks to find k median curves of at most \u2113 vertices each that minimize the sum of Fr\u00e9chet distances over all input curves to their closest median curve. A major shortcoming of their algorithm is that the input curves are restricted to lie on the real line. In this paper, we present a randomized bicriteria-approximation algorithm that works for polygonal curves in \u211dd and achieves approximation factor (1 + \u220a) with respect to the clustering costs. The algorithm has worst-case running-time linear in the number of curves, polynomial in the maximum number of vertices per curve, i.e. their complexity, and exponential in d, \u2113, \u220a and \u03b4, i.e., the failure probability. We achieve this result through a shortcutting lemma, which guarantees the existence of a polygonal curve with similar cost as an optimal median curve of complexity \u2113, but of complexity at most 2\u2113 \u2013 2, and whose vertices can be computed efficiently. We combine this lemma with the superset-sampling technique by Kumar et al. to derive our clustering result. In doing so, we describe and analyze a generalization of the algorithm by Ackermann et al., which may be of independent interest."}}
{"id": "jgVFNn5wwsw", "cdate": 1577836800000, "mdate": null, "content": {"title": "Approximating (k, \ud835\udcc1)-Median Clustering for Polygonal Curves", "abstract": "In 2015, Driemel, Krivo\\v{s}ija and Sohler introduced the $(k,\\ell)$-median problem for clustering polygonal curves under the Fr\\'echet distance. Given a set of input curves, the problem asks to find $k$ median curves of at most $\\ell$ vertices each that minimize the sum of Fr\\'echet distances over all input curves to their closest median curve. A major shortcoming of their algorithm is that the input curves are restricted to lie on the real line. In this paper, we present a randomized bicriteria-approximation algorithm that works for polygonal curves in $\\mathbb{R}^d$ and achieves approximation factor $(1+\\epsilon)$ with respect to the clustering costs. The algorithm has worst-case running-time linear in the number of curves, polynomial in the maximum number of vertices per curve, i.e. their complexity, and exponential in $d$, $\\ell$, $\\epsilon$ and $\\delta$, i.e., the failure probability. We achieve this result through a shortcutting lemma, which guarantees the existence of a polygonal curve with similar cost as an optimal median curve of complexity $\\ell$, but of complexity at most $2\\ell-2$, and whose vertices can be computed efficiently. We combine this lemma with the superset-sampling technique by Kumar et al. to derive our clustering result. In doing so, we describe and analyze a generalization of the algorithm by Ackermann et al., which may be of independent interest."}}
{"id": "HklhcHrlIr", "cdate": 1567802771528, "mdate": null, "content": {"title": "Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves", "abstract": "We study the center and median clustering problems for high-dimensional polygonal curves with finite but unbounded complexity. We tackle the computational issue that arises from the high number of dimensions by defining a Johnson-Lindenstrauss projection for polygonal curves. We analyze the resulting error in terms of the Fr\u00e9chet distance, which is a natural dissimilarity measure for curves. Our algorithms for the median clustering achieve sublinear dependency on the number of input curves via subsampling. For the center clustering we utilize \\cite{approx_k_l_center} algorithm that achieves linear running-time in the number of input curves. We evaluate our results empirically utilizing a fast, CUDA-parallelized  variant of the Alt and Godau algorithm for the Fr\u00e9chet distance. Our experiments show that our clustering algorithms have fast and accurate practical implementations that yield meaningful results on real world data from various physical domains."}}
{"id": "h8X3rKYM2U", "cdate": 1546300800000, "mdate": 1648710592693, "content": {"title": "Random projections and sampling algorithms for clustering of high-dimensional polygonal curves", "abstract": "We study the $k$-median clustering problem for high-dimensional polygonal curves with finite but unbounded number of vertices. We tackle the computational issue that arises from the high number of dimensions by defining a Johnson-Lindenstrauss projection for polygonal curves. We analyze the resulting error in terms of the Fr\\'echet distance, which is a tractable and natural dissimilarity measure for curves. Our clustering algorithms achieve sublinear dependency on the number of input curves via subsampling. Also, we show that the Fr\\'echet distance can not be approximated within any factor of less than $\\sqrt{2}$ by probabilistically reducing the dependency on the number of vertices of the curves. As a consequence we provide a fast, CUDA-parallelized version of the Alt and Godau algorithm for computing the Fr\\'echet distance and use it to evaluate our results empirically."}}
{"id": "UuJ0uwUNs3m", "cdate": 1514764800000, "mdate": 1648710592689, "content": {"title": "A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice", "abstract": "In the $k$-nearest neighborhood model ($k$-NN), we are given a set of points $P$, and we shall answer queries $q$ by returning the $k$ nearest neighbors of $q$ in $P$ according to some metric. This concept is crucial in many areas of data analysis and data processing, e.g., computer vision, document retrieval and machine learning. Many $k$-NN algorithms have been published and implemented, but often the relation between parameters and accuracy of the computed $k$-NN is not explicit. We study property testing of $k$-NN graphs in theory and evaluate it empirically: given a point set $P \\subset \\mathbb{R}^\\delta$ and a directed graph $G=(P,E)$, is $G$ a $k$-NN graph, i.e., every point $p \\in P$ has outgoing edges to its $k$ nearest neighbors, or is it $\\epsilon$-far from being a $k$-NN graph? Here, $\\epsilon$-far means that one has to change more than an $\\epsilon$-fraction of the edges in order to make $G$ a $k$-NN graph. We develop a randomized algorithm with one-sided error that decides this question, i.e., a property tester for the $k$-NN property, with complexity $O(\\sqrt{n} k^2 / \\epsilon^2)$ measured in terms of the number of vertices and edges it inspects, and we prove a lower bound of $\\Omega(\\sqrt{n / \\epsilon k})$. We evaluate our tester empirically on the $k$-NN models computed by various algorithms and show that it can be used to detect $k$-NN models with bad accuracy in significantly less time than the building time of the $k$-NN model."}}
{"id": "BkEpZ_-d-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice", "abstract": "In the $k$-nearest neighborhood model ($k$-NN), we are given a set of points $P$, and we shall answer queries $q$ by returning the $k$ nearest neighbors of $q$ in $P$ according to some metric. This concept is crucial in many areas of data analysis and data processing, e.g., computer vision, document retrieval and machine learning. Many $k$-NN algorithms have been published and implemented, but often the relation between parameters and accuracy of the computed $k$-NN is not explicit. We study property testing of $k$-NN graphs in theory and evaluate it empirically: given a point set $P \\subset \\mathbb{R}^\\delta$ and a directed graph $G=(P,E)$, is $G$ a $k$-NN graph, i.e., every point $p \\in P$ has outgoing edges to its $k$ nearest neighbors, or is it $\\epsilon$-far from being a $k$-NN graph? Here, $\\epsilon$-far means that one has to change more than an $\\epsilon$-fraction of the edges in order to make $G$ a $k$-NN graph. We develop a randomized algorithm with one-sided error that decides this question, i.e., a property tester for the $k$-NN property, with complexity $O(\\sqrt{n} k^2 / \\epsilon^2)$ measured in terms of the number of vertices and edges it inspects, and we prove a lower bound of $\\Omega(\\sqrt{n / \\epsilon k})$. We evaluate our tester empirically on the $k$-NN models computed by various algorithms and show that it can be used to detect $k$-NN models with bad accuracy in significantly less time than the building time of the $k$-NN model."}}
