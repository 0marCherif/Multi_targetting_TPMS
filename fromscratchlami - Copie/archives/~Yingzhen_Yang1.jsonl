{"id": "-ShKZHg0Md9", "cdate": 1676827113760, "mdate": null, "content": {"title": "Locally Regularized Sparse Graph by Fast Proximal Gradient Descent", "abstract": "Sparse graphs built by sparse representation has been demonstrated to be effective in clustering high-dimensional data. Albeit the compelling empirical performance, the vanilla sparse graph ignores the geometric information of the data by performing sparse representation for each datum separately. In order to obtain a sparse graph aligned with the local geometric structure of data, we propose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data clustering. SRSG encourages local smoothness on the neighborhoods of nearby data points by a well-defined support regularization term. We propose a fast proximal gradient descent method to solve the non-convex optimization problem of SRSG with the convergence matching the Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Extensive experimental results on various real data sets demonstrate the superiority of SRSG over other competing clustering methods."}}
{"id": "V0gA6pAp5S", "cdate": 1676472362768, "mdate": null, "content": {"title": "Robust Neural Architecture Search by Cross-Layer Knowledge Distillation", "abstract": "Deep Neural Networks are vulnerable to adversarial attacks. Neural Architecture Search (NAS), one of the driving tools of deep neural networks, demonstrates superior performance in prediction accuracy in various machine learning applications. However, it is unclear how it performs against adversarial attacks. Given the presence of a robust teacher, it would be interesting to investigate if NAS would produce robust neural architecture by inheriting robustness from the teacher. In this paper, we propose Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel NAS algorithm that improves the robustness of NAS by learning from a robust teacher through cross-layer knowledge distillation. Unlike previous knowledge distillation methods that encourage close student/teacher output only in the last layer, RNAS-CL automatically searches for the best teacher layer to supervise each student layer. Experimental result evidences the effectiveness of RNAS-CL and shows that RNAS-CL produces small and robust neural architecture."}}
{"id": "yEsj8pGNl1", "cdate": 1663850431912, "mdate": null, "content": {"title": "Projective Proximal Gradient Descent for Nonconvex Nonsmooth Optimization: Fast Convergence Without Kurdyka-Lojasiewicz (KL) Property", "abstract": "Nonconvex and nonsmooth optimization problems are important and challenging for statistics and machine learning. In this paper, we propose Projected Proximal Gradient Descent (PPGD) which solves a class of nonconvex and nonsmooth optimization problems, where the nonconvexity and nonsmoothness come from a nonsmooth regularization term which is nonconvex but piecewise convex. In contrast with existing convergence analysis of accelerated PGD methods for nonconvex and nonsmooth problems based on the Kurdyka-\\L{}ojasiewicz (K\\L{}) property, we provide a new theoretical analysis showing local fast convergence of PPGD. It is proved that PPGD achieves a fast convergence rate of $O(1/k^2)$ when the iteration number $k \\ge k_0$ for a finite $k_0$ on a class of nonconvex and nonsmooth problems under mild assumptions, which is locally the Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Experimental results demonstrate the effectiveness of PPGD."}}
{"id": "ZHyTtEd4lXz", "cdate": 1663850431079, "mdate": null, "content": {"title": "Differentiable Channel Selection for Self-Attention", "abstract": "Self-attention has been widely used in deep learning, and recent efforts have been devoted to incorporating self-attention modules into convolutional neural networks for computer vision. In this paper, we propose a novel attention module termed Differentiable Channel Selection (DCS). In contrast with conventional self-attention, DCS searches for the locations and key dimension of channels in a continuous space by a novel differentiable searching method. Our DCS module is compatible with either fixed neural network backbone or learnable backbone with Differentiable Neural Architecture Search (DNAS), leading to DCS with Fixed Backbone (DCS-FB) and DCS-DNAS respectively. We apply DCS-FB and DCS-DNAS to three computer vision tasks, person Re-IDentification methods (Re-ID), object detection, and image classification, with state-of-the-art results on standard benchmarks and compact architecture compared to competing methods, revealing the advantage of DCS."}}
{"id": "l5XHUBGrBkD", "cdate": 1663850023433, "mdate": null, "content": {"title": "RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation", "abstract": "Deep Neural Networks are vulnerable to adversarial attacks. Neural Architecture Search (NAS), one of the driving tools of deep neural networks, demonstrates superior performance in prediction accuracy in various machine learning applications. However, it is unclear how it performs against adversarial attacks. Given the presence of a robust teacher, it would be interesting to investigate if NAS would produce robust neural architecture by inheriting robustness from the teacher. In this paper, we propose Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel NAS algorithm that improves the robustness of NAS by learning from a robust teacher through cross-layer knowledge distillation. Unlike previous knowledge distillation methods that encourage close student/teacher output only in the last layer, RNAS-CL automatically searches for the best teacher layer to supervise each student layer. Experimental result evidences the effectiveness of RNAS-CL and shows that RNAS-CL produces small and robust neural architecture."}}
{"id": "Xecc-oeRzMr", "cdate": 1663849832366, "mdate": null, "content": {"title": "Bayesian Robust Graph Contrastive Learning", "abstract": "Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies. In this work, we propose a novel and robust method, Bayesian Robust Graph Contrastive Learning (BRGCL), which trains a GNN encoder to learn robust node representations. The BRGCL encoder is a completely unsupervised encoder. Two steps are iteratively executed at each epoch of training the BRGCL encoder: (1) estimating confident nodes and computing robust cluster prototypes of node representations through a novel Bayesian nonparametric method; (2) prototypical contrastive learning between the node representations and the robust cluster prototypes. Experiments on public benchmarks demonstrate the superior performance of BRGCL and the robustness of the learned node representations."}}
{"id": "BnzMjDLs5e9", "cdate": 1646077538803, "mdate": null, "content": {"title": "Noisy $\\ell^{0}$-Sparse Subspace Clustering on Dimensionality Reduced Data", "abstract": "Sparse subspace clustering methods with sparsity induced by $\\ell^{0}$-norm, such as $\\ell^{0}$-Sparse Subspace Clustering ($\\ell^{0}$-SSC)~\\citep{YangFJYH16-L0SSC-ijcv}, are demonstrated to be more effective than its $\\ell^{1}$ counterpart such as Sparse Subspace Clustering (SSC)~\\citep{ElhamifarV13}. However, the theoretical analysis of $\\ell^{0}$-SSC is restricted to clean data that lie exactly in subspaces. Real data often suffer from noise and they may lie close to subspaces. In this paper, we show that an optimal solution to the optimization problem of noisy $\\ell^{0}$-SSC achieves subspace detection property (SDP), a key element with which data from different subspaces are separated, under deterministic and semi-random model. Our results provide theoretical guarantee on the correctness of noisy $\\ell^{0}$-SSC in terms of SDP on noisy data for the first time, which reveals the advantage of noisy $\\ell^{0}$-SSC in terms of much less restrictive condition on subspace affinity. In order to improve the efficiency of noisy $\\ell^{0}$-SSC, we propose Noisy-DR-$\\ell^{0}$-SSC which provably recovers the subspaces on dimensionality reduced data. Noisy-DR-$\\ell^{0}$-SSC first projects the data onto a lower dimensional space by random projection, then performs noisy $\\ell^{0}$-SSC on the projected data for improved efficiency. Experimental results demonstrate the effectiveness of Noisy-DR-$\\ell^{0}$-SSC."}}
{"id": "_V7e0PfB3jM", "cdate": 1632875757746, "mdate": null, "content": {"title": "Noisy $\\ell^{0}$-Sparse Subspace Clustering on Dimensionality Reduced Data", "abstract": "High-dimensional data often lie in or close to low-dimensional subspaces. Sparse subspace clustering methods with sparsity induced by $\\ell^{0}$-norm, such as $\\ell^{0}$-Sparse Subspace Clustering ($\\ell^{0}$-SSC) \\citep{YangFJYH16-L0SSC}, are demonstrated to be more effective than its $\\ell^{1}$ counterpart such as Sparse Subspace Clustering (SSC) \\citep{ElhamifarV13}. However, the theoretical analysis of $\\ell^{0}$-SSC is restricted to clean data that lie exactly in subspaces. Real data often suffer from noise and they may lie close to subspaces. In this paper, we show that an optimal solution to the optimization problem of noisy $\\ell^{0}$-SSC achieves Subspace Detection Property (SDP), a key element with which data from different subspaces are separated, under deterministic and randomized models. Our results provide theoretical guarantee on the correctness of noisy $\\ell^{0}$-SSC in terms of SDP on noisy data for the first time. In order to improve the efficiency of noisy $\\ell^{0}$-SSC, we propose Noisy-DR-$\\ell^{0}$-SSC and Noisy-DR-$\\ell^{0}$-SSC-OSNAP which provably recover the subspaces on dimensionality reduced data. Both algorithms first project the data onto a lower dimensional space by linear transformation, then perform noisy $\\ell^{0}$-SSC on the dimensionality reduced data for improved efficiency. Experimental results demonstrate the effectiveness of Noisy-DR-$\\ell^{0}$-SSC and Noisy-DR-$\\ell^{0}$-SSC-OSNAP."}}
{"id": "kj0_45Y4r9i", "cdate": 1632875756187, "mdate": null, "content": {"title": "Discriminative Similarity for Data Clustering", "abstract": "Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose {\\em Clustering by  Discriminative Similarity (CDS)}, a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised similarity-based classifier from each data partition, and searches for the optimal partition of the data by minimizing the generalization error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as the sum of discriminative similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK), with its effectiveness demonstrated by experimental results."}}
{"id": "u2JeVfXIQa", "cdate": 1632875674650, "mdate": null, "content": {"title": "Adaptive Cross-Layer Attention for Image Restoration", "abstract": "Non-local attention module has been proven to be crucial for image restoration. Conventional non-local attention processes features of each layer separately, so it risks missing correlation between features among different layers. To address this problem, we propose Cross-Layer Attention (CLA) module in this paper. Instead of \ufb01nding correlated key pixels within the same layer, each query pixel is allowed to attend to key pixels at previous layers of the network. In order to mitigate the expensive computational cost of such hierarchical attention design, only a small \ufb01xed number of keys can be selected for each query from a previous layer. We further propose a variant of CLA termed Adaptive Cross-Layer Attention (ACLA). In ACLA, the number of keys to be aggregated for each query is dynamically selected. A neural architecture search method is used to \ufb01nd the insert positions of ACLA modules to render a compact neural network with compelling performance. Extensive experiments on image restoration tasks including single image super-resolution, image denoising, image demosaicing, and image compression artifacts reduction validate the effectiveness and ef\ufb01ciency of ACLA."}}
