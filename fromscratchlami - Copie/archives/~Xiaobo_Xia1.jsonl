{"id": "wXYLXATA-o", "cdate": 1704067200000, "mdate": 1708521140509, "content": {"title": "Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision", "abstract": "Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space. However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs. In light of this, we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities. Compared to text-only weakly-supervised methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset."}}
{"id": "R0w5JsHee-", "cdate": 1701388800000, "mdate": 1695954352336, "content": {"title": "Dynamics-aware loss for learning with label noise", "abstract": ""}}
{"id": "en3Fff2ICQ", "cdate": 1677628800000, "mdate": 1691873943865, "content": {"title": "Extended $T$T: Learning With Mixed Closed-Set and Open-Set Noisy Labels", "abstract": "The  <i>noise transition matrix</i>   <inline-formula><tex-math notation=\"LaTeX\">$T$</tex-math></inline-formula> , reflecting the probabilities that true labels flip into noisy ones, is of vital importance to model label noise and build statistically consistent classifiers. The traditional transition matrix is limited to model  <i>closed-set</i>  label noise, where noisy training data have true class labels  <i>within</i>  the noisy label set. It is unfitted to employ such a transition matrix to model  <i>open-set</i>  label noise, where some true class labels are  <i>outside</i>  the noisy label set. Therefore, when considering a more realistic situation, i.e., both closed-set and open-set label noises occur, prior works will give  <i>unbelievable</i>  solutions. Besides, the traditional transition matrix is mostly limited to model instance-independent label noise, which may not perform well in practice. In this paper, we focus on learning with the mixed closed-set and open-set noisy labels. We address the aforementioned issues by extending the traditional transition matrix to be able to model mixed label noise, and further to the cluster-dependent transition matrix to better combat the instance-dependent label noise in real-world applications. We term the proposed transition matrix as the cluster-dependent extended transition matrix. An unbiased estimator (i.e., extended  <inline-formula><tex-math notation=\"LaTeX\">$T$</tex-math></inline-formula> -estimator) has been designed to estimate the cluster-dependent extended transition matrix by only exploiting the noisy data. Comprehensive experiments validate that our method can better cope with realistic label noise, following its more robust performance than the prior state-of-the-art label-noise learning methods."}}
{"id": "r2DFfIXD7CZ", "cdate": 1672531200000, "mdate": 1681649874710, "content": {"title": "HumanMAC: Masked Motion Completion for Human Motion Prediction", "abstract": ""}}
{"id": "nbDMTLPpE9i", "cdate": 1672531200000, "mdate": 1683879143774, "content": {"title": "Dynamics-Aware Loss for Learning with Label Noise", "abstract": "Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss functions which reconcile fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamics of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn beneficial patterns, then gradually overfit harmful label noise, DAL strengthens the fitting ability initially, then gradually improves robustness. Moreover, at the later stage, to further reduce the negative impact of label noise and combat underfitting simultaneously, we let DNNs put more emphasis on easy examples than hard ones and introduce a bootstrapping term. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method. Our source code can be found in https://github.com/XiuchuanLi/DAL."}}
{"id": "lZBjsWP18J", "cdate": 1672531200000, "mdate": 1708521140568, "content": {"title": "Moderate Coreset: A Universal Method of Data Selection for Real-world Data-efficient Deep Learning", "abstract": ""}}
{"id": "hz4EdBk7bT", "cdate": 1672531200000, "mdate": 1698422416664, "content": {"title": "Harnessing Out-Of-Distribution Examples via Augmenting Content and Style", "abstract": ""}}
{"id": "YFPtjgJ7MEy", "cdate": 1672531200000, "mdate": 1708521140522, "content": {"title": "One Shot Learning as Instruction Data Prospector for Large Language Models", "abstract": "Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top 1% of Nuggets-curated examples substantially outperforms conventional methods that use the full dataset. These findings advocate for a data selection paradigm that prioritizes quality, offering a more efficient pathway to align LLMs with humans."}}
{"id": "VmxzzC0FNt", "cdate": 1672531200000, "mdate": 1681692018249, "content": {"title": "Robust Generalization against Photon-Limited Corruptions via Worst-Case Sharpness Minimization", "abstract": "Robust generalization aims to tackle the most challenging data distributions which are rare in the training set and contain severe noises, i.e., photon-limited corruptions. Common solutions such as distributionally robust optimization (DRO) focus on the worst-case empirical risk to ensure low training error on the uncommon noisy distributions. However, due to the over-parameterized model being optimized on scarce worst-case data, DRO fails to produce a smooth loss landscape, thus struggling on generalizing well to the test set. Therefore, instead of focusing on the worst-case risk minimization, we propose SharpDRO by penalizing the sharpness of the worst-case distribution, which measures the loss changes around the neighbor of learning parameters. Through worst-case sharpness minimization, the proposed method successfully produces a flat loss curve on the corrupted distributions, thus achieving robust generalization. Moreover, by considering whether the distribution annotation is available, we apply SharpDRO to two problem settings and design a worst-case selection process for robust generalization. Theoretically, we show that SharpDRO has a great convergence guarantee. Experimentally, we simulate photon-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show that SharpDRO exhibits a strong generalization ability against severe corruptions and exceeds well-known baseline methods with large performance gains."}}
{"id": "Q_1VmOAh0c", "cdate": 1672531200000, "mdate": 1708521140536, "content": {"title": "Regularly Truncated M-estimators for Learning with Noisy Labels", "abstract": "The sample selection approach is very popular in learning with noisy labels. As deep networks learn pattern first, prior methods built on sample selection share a similar training procedure: the small-loss examples can be regarded as clean examples and used for helping generalization, while the large-loss examples are treated as mislabeled ones and excluded from network parameter updates. However, such a procedure is arguably debatable from two folds: (a) it does not consider the bad influence of noisy labels in selected small-loss examples; (b) it does not make good use of the discarded large-loss examples, which may be clean or have meaningful information for generalization. In this paper, we propose regularly truncated M-estimators (RTME) to address the above two issues simultaneously. Specifically, RTME can alternately switch modes between truncated M-estimators and original M-estimators. The former can adaptively select small-losses examples without knowing the noise rate and reduce the side-effects of noisy labels in them. The latter makes the possibly clean examples but with large losses involved to help generalization. Theoretically, we demonstrate that our strategies are label-noise-tolerant. Empirically, comprehensive experimental results show that our method can outperform multiple baselines and is robust to broad noise types and levels."}}
