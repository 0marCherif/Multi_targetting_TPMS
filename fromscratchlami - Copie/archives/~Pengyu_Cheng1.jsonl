{"id": "cxuCPhPdozk", "cdate": 1683906998665, "mdate": 1683906998665, "content": {"title": "Toward Fairness in Text Generation via Mutual Information Minimizationbased on Importance Sampling", "abstract": "Pretrained language models (PLMs), such as GPT-2,  have  achieved  remarkable  empirical  perfor-mance in text generation tasks.   However,  pre-trained on large-scale natural language corpora,the generated text from PLMs may exhibitsocialbiasagainst disadvantaged demographic groups.To improve the fairness of PLMs in text genera-tion, we propose to minimize the mutual informa-tion between the semantics in the generated textsentences  and  theirdemographic  polarity,i.e.,the demographic group to which the sentence isreferring. In this way, the mentioning of a demo-graphic group (e.g.,male or female) is encouragedto be independent from how it is described in thegenerated text, thus effectively alleviating the so-cial bias.   Moreover,  we propose to efficientlyestimate the upper bound of the above mutual in-formation via importance sampling, leveraging anatural language corpus. We also propose a dis-tillation mechanism that preserves the languagemodeling ability of the PLMs after debiasing. Em-pirical results on real-world benchmarks demon-strate that the proposed method yields superiorperformance in term of both fairness and languagemodeling ability."}}
{"id": "UsDZut_p2LG", "cdate": 1603141808979, "mdate": null, "content": {"title": "Estimating Total Correlation with Mutual Information Bounds", "abstract": "Total correlation (TC) is a fundamental concept in information theory to measure the statistical dependency of multiple random variables. Recently, TC has shown effectiveness as a regularizer in many machine learning tasks when minimizing/maximizing the correlation among random variables is required. However, to obtain precise TC values is challenging, especially when the closed-form distributions of variables are unknown. In this paper, we introduced several sample-based variational TC estimators. Specifically, we connect the TC with mutual information (MI) and constructed two calculation paths to decompose TC into MI terms. In our experiments, we estimated the true TC values with the proposed estimators in different simulation scenarios and analyzed the properties of the TC estimators."}}
{"id": "N6JECD-PI5w", "cdate": 1601308299346, "mdate": null, "content": {"title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders", "abstract": "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space."}}
{"id": "TgSVWXw22FQ", "cdate": 1601308216790, "mdate": null, "content": {"title": "Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning", "abstract": "Voice style transfer, also called voice conversion, seeks to modify one speaker's voice to generate speech as if it came from another (target) speaker. Previous works have made progress on voice conversion with parallel training data and pre-known speakers. However, zero-shot voice style transfer, which learns from non-parallel data and generates voices for previously unseen speakers, remains a challenging problem. In this paper we propose a novel zero-shot voice transfer method via disentangled representation learning. The proposed method first encodes speaker-related style and voice content of each input voice into separate low-dimensional embedding spaces, and then transfers to a new voice by combining the source content embedding and target style embedding through a decoder. With information-theoretic guidance, the style and content embedding spaces are representative and (ideally) independent of each other. On real-world datasets, our method outperforms other baselines and obtains state-of-the-art results in terms of transfer accuracy and voice naturalness."}}
{"id": "g75kUi1jAc_", "cdate": 1601308143440, "mdate": null, "content": {"title": "WAFFLe: Weight Anonymized Factorization for Federated Learning", "abstract": "In domains where data are sensitive or private, there is great value in methods that can learn in a distributed manner without the data ever leaving the local devices. In light of this need, federated learning has emerged as a popular training paradigm. However, many federated learning approaches trade transmitting data for communicating updated weight parameters for each local device. Therefore, a successful breach that would have otherwise directly compromised the data instead grants whitebox access to the local model, which opens the door to a number of attacks, including exposing the very data federated learning seeks to protect. Additionally, in distributed scenarios, individual client devices commonly exhibit high statistical heterogeneity. Many common federated approaches learn a single global model; while this may do well on average, performance degrades when the i.i.d. assumption is violated, underfitting individuals further from the mean and raising questions of fairness. To address these issues, we propose Weight Anonymized Factorization for Federated Learning (WAFFLe), an approach that combines the Indian Buffet Process with a shared dictionary of weight factors for neural networks. Experiments on MNIST, FashionMNIST, and CIFAR-10 demonstrate WAFFLe's significant improvement to local test performance and fairness while simultaneously providing an extra layer of security."}}
{"id": "wbTxuiXUvSw", "cdate": 1577836800000, "mdate": null, "content": {"title": "CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information", "abstract": "Mutual information (MI) minimization has gained considerable interests in various machine learning tasks. However, estimating and minimizing MI in high-dimensional spaces remains a challenging problem, especially when only samples, rather than distribution forms, are accessible. Previous works mainly focus on MI lower bound approximation, which is not applicable to MI minimization problems. In this paper, we propose a novel Contrastive Log-ratio Upper Bound (CLUB) of mutual information. We provide a theoretical analysis of the properties of CLUB and its variational approximation. Based on this upper bound, we introduce a MI minimization training scheme and further accelerate it with a negative sampling strategy. Simulation studies on Gaussian distributions show the reliable estimation ability of CLUB. Real-world MI minimization experiments, including domain adaptation and information bottleneck, demonstrate the effectiveness of the proposed method. The code is at https://github.com/Linear95/CLUB."}}
{"id": "sE4KI-IIrh5", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Disentangled Text Representation Learning with Information-Theoretic Guidance", "abstract": "Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation."}}
{"id": "m5Tux_jxvRo9", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Disentangled Text Representation Learning with Information-Theoretic Guidance", "abstract": "Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, Lawrence Carin. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "Ve7L3GlKuC", "cdate": 1577836800000, "mdate": null, "content": {"title": "WAFFLe: Weight Anonymized Factorization for Federated Learning", "abstract": "In domains where data are sensitive or private, there is great value in methods that can learn in a distributed manner without the data ever leaving the local devices. In light of this need, federated learning has emerged as a popular training paradigm. However, many federated learning approaches trade transmitting data for communicating updated weight parameters for each local device. Therefore, a successful breach that would have otherwise directly compromised the data instead grants whitebox access to the local model, which opens the door to a number of attacks, including exposing the very data federated learning seeks to protect. Additionally, in distributed scenarios, individual client devices commonly exhibit high statistical heterogeneity. Many common federated approaches learn a single global model; while this may do well on average, performance degrades when the i.i.d. assumption is violated, underfitting individuals further from the mean, and raising questions of fairness. To address these issues, we propose Weight Anonymized Factorization for Federated Learning (WAFFLe), an approach that combines the Indian Buffet Process with a shared dictionary of weight factors for neural networks. Experiments on MNIST, FashionMNIST, and CIFAR-10 demonstrate WAFFLe's significant improvement to local test performance and fairness while simultaneously providing an extra layer of security."}}
{"id": "5ZAO6IvX_5", "cdate": 1577836800000, "mdate": null, "content": {"title": "Dynamic Embedding on Textual Networks via a Gaussian Process", "abstract": "Textual network embedding aims to learn low-dimensional representations of text-annotated nodes in a graph. Prior work in this area has typically focused on fixed graph structures; however, real-world networks are often dynamic. We address this challenge with a novel end-to-end node-embedding model, called Dynamic Embedding for Textual Networks with a Gaussian Process (DetGP). After training, DetGP can be applied efficiently to dynamic graphs without re-training or backpropagation. The learned representation of each node is a combination of textual and structural embeddings. Because the structure is allowed to be dynamic, our method uses the Gaussian process to take advantage of its non-parametric properties. To use both local and global graph structures, diffusion is used to model multiple hops between neighbors. The relative importance of global versus local structure for the embeddings is learned automatically. With the non-parametric nature of the Gaussian process, updating the embeddings for a changed graph structure requires only a forward pass through the learned model. Considering link prediction and node classification, experiments demonstrate the empirical effectiveness of our method compared to baseline approaches. We further show that DetGP can be straightforwardly and efficiently applied to dynamic textual networks."}}
