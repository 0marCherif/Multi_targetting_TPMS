{"id": "NUtrt0Le5P", "cdate": 1668576266205, "mdate": 1668576266205, "content": {"title": "Using Object Information for Spotting Text", "abstract": "Text spotting, also called text detection, is a challenging computer vision task because of cluttered backgrounds, diverse imaging environments, various text sizes and similarity between some objects and characters, e.g., tyre and \u2019o\u2019. However, text spotting is a vital step in numerous AI and computer vision systems, such as autonomous robots and systems for visually impaired. Due to its potential applications and commercial values, researchers have proposed various deep architectures and methods for text spotting. These methods and architectures concentrate only on text in images, but neglect other information related to text. There exists a strong relationship between certain objects and the presence of text, such as signboards or the absence of text, such as trees. In this paper, a text spotting algorithm based on text and object dependency is proposed. The proposed algorithm consists of two sub-convolutional neural networks and three training stages. For this study, a new NTU-UTOI dataset containing over 22k non-synthetic images with 277k bounding boxes for text and 42 text-related object classes is established. According to our best knowledge, it is the second largest non-synthetic text image database. Experimental results on three benchmark datasets with clutter backgrounds, COCO-Text, MSRA-TD500 and SVT show that the proposed algorithm provides comparable performance to state-of-the-art text spotting methods. Experiments are also performed on our newly established dataset to investigate the effectiveness of object information for text spotting. The experimental results indicate that the object information contributes significantly on the performance gain.\n"}}
{"id": "sDNuHPA7Ib4", "cdate": 1663850018955, "mdate": null, "content": {"title": "Certification of Attribution Robustness for Euclidean Distance and Cosine Similarity Measure", "abstract": "Model attribution is a critical component of deep neural networks (DNNs) for its interpretability to complex models. Recent works bring up attention to the security of attributions as they are vulnerable to attribution attacks that generate similar images with dramatically different attributions. Studies have been working on empirically improving the robustness of DNNs against those attacks. However, due to their lack of certification, the actual robustness of the model for a testing point is not known. In this work, we define \\emph{certified attribution robustness} for the first time that upper bounds the dissimilarity of attributions after the samples are perturbed by any noises within a certain region while the classification results remain the same. Based on the definition, we propose different approaches to certify the attributions using Euclidean distance and cosine similarity under both $\\ell_2$ and $\\ell_\\infty$-norm perturbations constraints. The bounds developed by our theoretical study are validated on three datasets (MNIST, Fashion-MNIST and CIFAR-10), and two different types of attacks (PGD attack and IFIA attribution attack). The experimental results show that the bounds certify the model effectively."}}
{"id": "QPg5TTAdizy", "cdate": 1652737651837, "mdate": null, "content": {"title": "Exploiting the Relationship Between Kendall's Rank Correlation and Cosine Similarity for Attribution Protection", "abstract": "Model attributions are important in deep neural networks as they aid practitioners in understanding the models, but recent studies reveal that attributions can be easily perturbed by adding imperceptible noise to the input. The non-differentiable Kendall's rank correlation is a key performance index for attribution protection. In this paper, we first show that the expected Kendall's rank correlation is positively correlated to cosine similarity and then indicate that the direction of attribution is the key to attribution robustness. Based on these findings, we explore the vector space of attribution to explain the shortcomings of attribution defense methods using $\\ell_p$ norm and propose integrated gradient regularizer (IGR), which maximizes the cosine similarity between natural and perturbed attributions. Our analysis further exposes that IGR encourages neurons with the same activation states for natural samples and the corresponding perturbed samples. Our experiments on different models and datasets confirm our analysis on attribution protection and demonstrate a decent improvement in adversarial robustness."}}
{"id": "DesNW4-5ai9", "cdate": 1632875508417, "mdate": null, "content": {"title": "Transferable Adversarial Attack based on Integrated Gradients", "abstract": "The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention maps, and smoothing decision surfaces, are commonly used to craft adversarial examples. By tightly integrating the three approaches, we propose a new and simple algorithm named Transferable Attack based on Integrated Gradients (TAIG) in this paper, which can find highly transferable adversarial examples for black-box attacks. Unlike previous methods using multiple computational terms or combining with other methods, TAIG integrates the three approaches into one single term. Two versions of TAIG that compute their integrated gradients on a straight-line path and a random piecewise linear path are studied. Both versions offer strong transferability and can seamlessly work together with the previous methods. Experimental results demonstrate that TAIG outperforms the state-of-the-art methods."}}
{"id": "xwA5XVRM4Kd", "cdate": 1609459200000, "mdate": null, "content": {"title": "Pixel-wise ordinal classification for salient object grading", "abstract": "Highlights \u2022 Studying a new problem setting - salient object grading \u2022 Proposing an ordinal classification method to predict saliency levels of objects \u2022 Improving the traditional saliency detection performance by the ordinal information \u2022 Constructing two new saliency grading datasets Abstract Driven by business intelligence applications for rating attraction of products in shops, a new problem \u2014 salient object grading is studied in this paper. In computer vision, plenty of salient object detection approaches have been proposed, while most existing studies detect objects in a binary manner: salient or not. This paper focuses on a new problem setting that requires detecting all salient objects and categorizing them into different salient levels. Based on that, a pixel-wise ordinal classification method is proposed. It consists of a multi-resolution saliency detector which detects and segments objects, an ordinal classifier which grades pixels into different salient levels, and a binary saliency enhancer which sharpens the difference between non-saliency and all other salient levels. Two new image datasets with salient level labels are constructed. Experimental results demonstrate that, on the one hand, the proposed method provides effective salient level predictions and on the other hand, offers very comparable performance with state-of-the-art salient object detection methods in the traditional problem setting."}}
{"id": "-laGadbyRc", "cdate": 1582425280870, "mdate": null, "content": {"title": "Attacking Object Detectors Without Changing the Target Object", "abstract": "Object detectors, such as Faster R-CNN and YOLO, have numerous applications, including in some critical systems, e.g., self-driving cars and unmanned aerial vehicles. Their vulnerabilities have to be studied thoroughly before deploying them in critical systems to avoid irrecoverable loss caused by intentional attacks. Researchers have proposed some methods to craft adversarial examples for studying security risk in object detectors. All these methods require modifying pixels inside target objects. Some modifications are substantial and target objects are significantly distorted. In this paper, an algorithm which derives an adversarial signal placing around the border of target objects to fool objector detectors is proposed. Computationally, the algorithm seeks a border around target objects to mislead Faster R-CNN to produce a very large bounding box and finally decease its confidence to target objects. Using stop sign as a target object, adversarial borders with four different sizes are generated and evaluated on 77 videos, including five in-car videos for digital attacks and 72 videos for physical attacks. The experimental results show that adversarial border can effectively fool Faster R-CNN and YOLOv3 digitally and physically. In addition, the experimental results on YOLOv3 indicate that adversarial border is transferable, which is vital for black-box attack."}}
{"id": "_a1o4QPMB7", "cdate": 1582425156473, "mdate": null, "content": {"title": "Adversarial Signboard against Object Detector", "abstract": "Object detector is an indispensable component in many computer vision and artificial\nintelligence systems, such as autonomous robot and image analyzer for profiling social\nmedia users. Analyzing its vulnerabilities is essential for detecting and preventing attacks and minimizing potential loss. Researchers have proposed a number of adversarial\nexamples to evaluate the robustness of object detectors. All these adversarial examples\nchange pixels inside target objects to carry out attacks but only some of them are suitable\nfor physical attacks. According to the best knowledge of the authors, no published work\nsuccessfully attacks object detector without changing pixels inside the target object. In\nan unpublished work, the authors designed an adversarial border which tightly surrounds\ntarget object and successfully misleads Faster R-CNN and YOLOv3 digitally and physically. Adversarial border does not change pixels inside target object but makes it look\nweird. In this paper, a new adversarial example named adversarial signboard, which\nlooks like a signboard, is proposed. By putting it below a target object, it can mislead\nthe state-of-the-art object detectors. Using stop sign as a target object, adversarial signboard is evaluated on 48 videos with totally 5416 frames. The experimental results show\nthat adversarial signboard derived from Faster R-CNN with ResNet-101 as a backbone\nnetwork can mislead Faster R-CNN with a different backbone network, Mask R-CNN,\nYOLOv3 and R-FCN digitally and physically"}}
{"id": "o_RZg4KSKVD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Gender and Ethnicity Classification based on Palmprint and Palmar Hand Images from Uncontrolled Environment", "abstract": "Soft biometric attributes such as gender, ethnicity or age may provide useful information for biometrics and forensics applications. Researchers used, e.g., face, gait, iris, and hand, etc. to classify such attributes. Even though hand has been widely studied for biometric recognition, relatively less attention has been given to soft biometrics from hand. Previous studies of soft biometrics based on hand images focused on gender and well-controlled imaging environment. In this paper, the gender and ethnicity classification in uncontrolled environment are considered. Gender and ethnicity labels are collected and provided for subjects in a publicly available database, which contains hand images from the Internet. Five deep learning models are fine-tuned and evaluated in gender and ethnicity classification scenarios based on palmar 1) full hand, 2) segmented hand and 3) palmprint images. The experimental results indicate that for gender and ethnicity classification in uncontrolled environment, full and segmented hand images are more suitable than palmprint images."}}
{"id": "d18UItn8Jb", "cdate": 1577836800000, "mdate": null, "content": {"title": "Palmprint Recognition in Uncontrolled and Uncooperative Environment", "abstract": "Online palmprint recognition and latent palmprint identification are two branches of palmprint studies. The former uses middle-resolution images collected by a digital camera in a well-controlled or contact-based environment with user cooperation for commercial applications and the latter uses high-resolution latent palmprints collected in crime scenes for forensic investigation. However, these two branches do not cover some palmprint images which have the potential for forensic investigation. Due to the prevalence of smartphone and consumer camera, more evidence is in the form of digital images taken in uncontrolled and uncooperative environment, e.g., child pornographic images and terrorist images, where the criminals commonly hide or cover their face. However, their palms can be observable. To study palmprint identification on images collected in uncontrolled and uncooperative environment, a new palmprint database is established and an end-to-end deep learning algorithm is proposed. The new database named NTU Palmprints from the Internet (NTU-PI-v1) contains 7881 images from 2035 palms collected from the Internet. The proposed algorithm consists of an alignment network and a feature extraction network and is end-to-end trainable. The proposed algorithm is compared with the state-of-the-art online palmprint recognition methods and evaluated on three public contactless palmprint databases, IITD, CASIA, and PolyU and two new databases, NTU-PI-v1 and NTU contactless palmprint database. The experimental results showed that the proposed algorithm outperforms the existing palmprint recognition methods."}}
{"id": "Ut0kWqQUhLS", "cdate": 1577836800000, "mdate": null, "content": {"title": "A survey on image and video cosegmentation: Methods, challenges and analyses", "abstract": "Highlights \u2022 To the best of our knowledge, this paper is the first comprehensive survey covering both image and video cosegmentation. \u2022 To clarify the essence of cosegmentation methods, typical methodologies and formulations are summarized and outlined from a wide range of existing methods. \u2022 Through unveiling the relevance as well as the difference between the two co-related research areas, common ideas and challenges are identified and summarized for future study. Abstract Image and video cosegmentation is a newly emerging and rapidly progressing area, which aims at delineating common objects at pixel-level from a group of images or a set of videos. Plenty of related works have been published and implemented in varied applications, but there lacks a systematic survey on both image and video cosegmentation. This paper provides a comprehensive overview including the existing methods, applications, and challenges. Specifically, different cosegmentation problem settings are described, the formulation details of the methods are summarized and their potential applications are listed. Moreover, the benchmark datasets and standard evaluation metrics are also given; and the future directions and unsolved challenges are discussed."}}
