{"id": "YlGsTZODyjz", "cdate": 1663850495911, "mdate": null, "content": {"title": "The Tilted Variational Autoencoder: Improving Out-of-Distribution Detection", "abstract": "A problem with using the Gaussian distribution as a prior for the variational autoencoder (VAE) is that the set on which Gaussians have high probability density is small as the latent dimension increases. This is an issue because VAEs try to attain both a high likelihood with respect to a prior distribution and at the same time, separation between points for better reconstruction. Therefore, a small volume in the high-density region of the prior is problematic because it restricts the separation of latent points.  To ameliorate this, we propose a simple generalization of the Gaussian distribution, called the tilted Gaussian, which has a maximum probability density occurring on a sphere instead of a single point. The tilted Gaussian has exponentially more volume in high-density regions than the standard Gaussian as a function of the distribution dimension. We empirically demonstrate that this simple change in the prior distribution improves VAE performance on the task of detecting unsupervised out-of-distribution (OOD) samples. We also introduce a new OOD testing procedure, called the Will-It-Move test, where the tilted Gaussian achieves remarkable OOD performance."}}
{"id": "WG3vmsteqR_", "cdate": 1652737794888, "mdate": null, "content": {"title": "The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization", "abstract": "The logit outputs of a feedforward neural network at initialization are conditionally Gaussian, given a random covariance matrix defined by the penultimate layer. In this work, we study the distribution of this random matrix. Recent work has shown that shaping the activation function as network depth grows large is necessary for this covariance matrix to be non-degenerate. However, the current infinite-width-style understanding of this shaping method is unsatisfactory for large depth: infinite-width analyses ignore the microscopic fluctuations from layer to layer, but these fluctuations accumulate over many layers. \n\nTo overcome this shortcoming, we study the random covariance matrix in the shaped infinite-depth-and-width limit. We identify the precise scaling of the activation function necessary to arrive at a non-trivial limit, and show that the random covariance matrix is governed by a stochastic differential equation (SDE) that we call the Neural Covariance SDE. Using simulations, we show that the SDE closely matches the distribution of the random covariance matrix of finite networks. Additionally, we recover an if-and-only-if condition for exploding and vanishing norms of large shaped networks based on the activation function. "}}
{"id": "-h99IwQN-f", "cdate": 1621630050510, "mdate": null, "content": {"title": "The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization", "abstract": "Theoretical results show that neural networks can be approximated by Gaussian processes in the infinite-width limit. However, for fully connected networks, it has been previously shown that for any fixed network width, $n$, the Gaussian approximation gets worse as the network depth, $d$, increases. Given that modern networks are deep, this raises the question of how well modern architectures, like ResNets, are captured by the infinite-width limit. To provide a better approximation, we study ReLU ResNets in the infinite-depth-and-width limit, where \\emph{both} depth and width tend to infinity as their ratio, $d/n$, remains constant. In contrast to the Gaussian infinite-width limit, we show theoretically that the network exhibits log-Gaussian behaviour at initialization in the infinite-depth-and-width limit, with parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we demonstrate that even basic properties of standard ResNet architectures are poorly captured by the Gaussian limit, but remarkably well captured by our log-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at initialization are hypoactivated: fewer than half of the ReLUs are activated. Additionally, we calculate the interlayer correlations, which have the effect of exponentially increasing the variance of the network output. Based on our analysis, we introduce \\emph{Balanced ResNets}, a simple architecture modification, which eliminates hypoactivation and interlayer correlations and is more amenable to theoretical analysis."}}
{"id": "DX3m3fpHaHX", "cdate": 1577836800000, "mdate": null, "content": {"title": "Solving Elliptic Equations with Brownian Motion: Bias Reduction and Temporal Difference Learning", "abstract": "The Feynman-Kac formula provides a way to understand solutions to elliptic partial differential equations in terms of expectations of continuous time Markov processes. This connection allows for the creation of numerical schemes for solutions based on samples of these Markov processes which have advantages over traditional numerical methods in some cases. However, na\\\"ive numerical implementations suffer from statistical bias and sampling error. We present methods to discretize the stochastic process appearing in the Feynman-Kac formula that reduce the bias of the numerical scheme. We also propose using temporal difference learning to assemble information from random samples in a way that is more efficient than the traditional Monte Carlo method."}}
{"id": "BFGH0-a5jVq", "cdate": 1577836800000, "mdate": null, "content": {"title": "A derivative-free method for solving elliptic partial differential equations with deep neural networks", "abstract": "We introduce a deep neural network based method for solving a class of elliptic partial differential equations. We approximate the solution of the PDE with a deep neural network which is trained under the guidance of a probabilistic representation of the PDE in the spirit of the Feynman-Kac formula. The solution is given by an expectation of a martingale process driven by a Brownian motion. As Brownian walkers explore the domain, the deep neural network is iteratively trained using a form of reinforcement learning. Our method is a \u2018Derivative-Free Loss Method\u2019 since it does not require the explicit calculation of the derivatives of the neural network with respect to the input neurons in order to compute the training loss. The advantages of our method are showcased in a series of test problems: a corner singularity problem, a high-dimensional Poisson's equation, an interface problem, and an application to a chemotaxis population model."}}
{"id": "SJgndT4KwB", "cdate": 1569439091530, "mdate": null, "content": {"title": "Finite Depth and Width Corrections to the Neural Tangent Kernel", "abstract": "We prove the precise scaling, at finite depth and width, for the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The standard deviation is exponential in the ratio of network depth to width. Thus, even in the limit of infinite overparameterization, the NTK is not deterministic if depth and width simultaneously tend to infinity. Moreover, we prove that for such deep and wide networks, the NTK has a non-trivial evolution during training by showing that the mean of its first SGD update is also exponential in the ratio of network depth to width. This is sharp contrast to the regime where depth is fixed and network width is very large. Our results suggest that, unlike relatively shallow and wide networks, deep and wide ReLU networks are capable of learning data-dependent features even in the so-called lazy training regime. "}}
{"id": "sBieEavUWS", "cdate": 1420070400000, "mdate": null, "content": {"title": "Optimal Strategy in \"Guess Who?\"", "abstract": "Guess Who?\" is a popular two player game where players ask \"Yes\"/\"No\" questions to search for their opponent's secret identity from a pool of possible candidates. This is modeled as a simple stochastic game. Using this model, the optimal strategy is explicitly found. Contrary to popular belief, performing a binary search is \\emph{not} always optimal. Instead, the optimal strategy for the player who trails is to make certain bold plays in an attempt catch up. This is discovered by first analyzing a continuous version of the game where players play indefinitely and the winner is never decided after finitely many rounds."}}
