{"id": "44l6hQIoRG", "cdate": 1685982300528, "mdate": null, "content": {"title": "Understanding Deep Neural Function Approximation in Reinforcement Learning via $\\epsilon$-Greedy Exploration", "abstract": "This paper provides a theoretical study of deep neural function approximation in reinforcement learning (RL) with the $\\epsilon$-greedy exploration under the online setting. This problem setting is motivated by the successful deep Q-networks (DQN) framework that falls in this regime. In this work, we provide an initial attempt on theoretical understanding deep RL from the perspective of function class and neural networks architectures (e.g., width and depth) beyond the \"linear'' regime. To be specific, we focus on the value based algorithm with the $\\epsilon$-greedy exploration via deep (and two-layer) neural networks endowed by Besov (and Barron) function spaces, respectively, which aims at approximating an $\\alpha$-smooth Q-function in a $d$-dimensional feature space. We prove that, with $T$ episodes, scaling the width $m = \\widetilde{\\mathcal{O}}(T^{\\frac{d}{2\\alpha + d}})$ and the depth $L=\\mathcal{O}(\\log T)$ of the neural network for deep RL is sufficient for learning with sublinear regret in Besov spaces. Moreover, for a two layer neural network endowed by the Barron space, scaling the width $\\Omega(\\sqrt{T})$ is sufficient. To achieve this, the key issue in our analysis is how to estimate the temporal difference error under deep neural function approximation as the $\\epsilon$-greedy exploration is not enough to ensure ``optimism''. Our analysis reformulates the temporal difference error in an $L^2(\\mathrm{d}\\mu)$-integrable space over a certain averaged measure $\\mu$, and transforms it to a generalization problem under the non-iid setting. This might have its own interest in RL theory for better understanding $\\epsilon$-greedy exploration in deep RL."}}
{"id": "yqRoo7JTfs", "cdate": 1685532021810, "mdate": null, "content": {"title": "Proximal Point Imitation Learning", "abstract": "This work develops new algorithms with rigorous efficiency guarantees for infinite horizon imitation learning (IL) with linear function approximation without restrictive coherence assumptions. We begin with the minimax formulation of the problem and then outline how to leverage classical tools from optimization, in particular, the proximal-point method (PPM) and dual smoothing, for online and offline IL, respectively. Thanks to PPM, we avoid nested policy evaluation and cost updates for online IL appearing in the prior literature. In particular, we do away with the conventional alternating updates by the optimization of a single convex and smooth objective over both cost and $Q$-functions. When solved inexactly, we relate the optimization errors to the suboptimality of the recovered policy. As an added bonus, by re-interpreting PPM as dual smoothing with the expert policy as a center point, we also obtain an offline IL algorithm enjoying theoretical guarantees in terms of required expert trajectories. Finally, we achieve convincing empirical performance for both linear and neural network function approximation."}}
{"id": "4iEoOIQ7nL", "cdate": 1652737706813, "mdate": null, "content": {"title": "Proximal Point Imitation Learning", "abstract": "This work develops new algorithms with rigorous efficiency guarantees for infinite horizon imitation learning (IL) with linear function approximation without restrictive coherence assumptions. We begin with the minimax formulation of the problem and then outline how to leverage classical tools from optimization, in particular, the proximal-point method (PPM) and dual smoothing, for online and offline IL, respectively. Thanks to PPM, we avoid nested policy evaluation and cost updates for online IL appearing in the prior literature. In particular, we do away with the conventional alternating updates by the optimization of a single convex and smooth objective over both cost and $Q$-functions. When solved inexactly, we relate the optimization errors to the suboptimality of the recovered policy. As an added bonus, by re-interpreting PPM as dual smoothing with the expert policy as a center point, we also obtain an offline IL algorithm enjoying theoretical guarantees in terms of required expert trajectories. Finally, we achieve convincing empirical performance for both linear and neural network function approximation."}}
{"id": "o8vYKDWMnq1", "cdate": 1652737421982, "mdate": null, "content": {"title": "Understanding Deep Neural Function Approximation in Reinforcement Learning via $\\epsilon$-Greedy Exploration", "abstract": "This paper provides a theoretical study of deep neural function approximation in reinforcement learning (RL) with the $\\epsilon$-greedy exploration under the online setting. This problem setting is motivated by the successful deep Q-networks (DQN) framework that falls in this regime. In this work, we provide an initial attempt on theoretical understanding deep RL from the perspective of function class and neural networks architectures (e.g., width and depth) beyond the ``linear'' regime. To be specific, we focus on the value based algorithm with the $\\epsilon$-greedy exploration via deep (and two-layer) neural networks endowed by Besov (and Barron) function spaces, respectively, which aims at approximating an $\\alpha$-smooth Q-function in a $d$-dimensional feature space. We prove that, with $T$ episodes, scaling the width $m = \\widetilde{\\mathcal{O}}(T^{\\frac{d}{2\\alpha + d}})$ and the depth $L=\\mathcal{O}(\\log T)$ of the neural network for deep RL is sufficient for learning with sublinear regret in Besov spaces. Moreover, for a two layer neural network endowed by the Barron space, scaling the width $\\Omega(\\sqrt{T})$ is sufficient. To achieve this, the key issue in our analysis is how to estimate the temporal difference error under deep neural function approximation as the $\\epsilon$-greedy exploration is not enough to ensure \"optimism\". Our analysis reformulates the temporal difference error in an $L^2(\\mathrm{d}\\mu)$-integrable space over a certain averaged measure $\\mu$, and transforms it to a generalization problem under the non-iid setting. This might have its own interest in RL theory for better understanding $\\epsilon$-greedy exploration in deep RL."}}
{"id": "KFxIsdIvUj", "cdate": 1652737324418, "mdate": null, "content": {"title": "Identifiability and generalizability from multiple experts in Inverse Reinforcement Learning", "abstract": "While Reinforcement Learning (RL) aims to train an agent from a reward function in a given environment, Inverse Reinforcement Learning (IRL) seeks to recover the reward function from observing an expert's behavior. It is well known that, in general, various reward functions can lead to the same optimal policy, and hence, IRL is ill-defined. However, \\cite{cao2021identifiability} showed that, if we observe two or more experts with different discount factors or acting in different environments, the reward function can under certain conditions be identified up to a constant. This work starts by showing an equivalent identifiability statement from multiple experts in tabular MDPs based on a rank condition, which is easily verifiable and is shown to be also necessary. We then extend our result to various different scenarios, i.e., we characterize reward identifiability in the case where the reward function can be represented as a linear combination of given features, making it more interpretable, or when we have access to approximate transition matrices. Even when the reward is not identifiable, we provide conditions characterizing when data on multiple experts in a given environment allows to generalize and train an optimal agent in a new environment. Our theoretical results on reward identifiability and generalizability are validated in various numerical experiments."}}
{"id": "gRWbKhvzSQK", "cdate": 1640995200000, "mdate": 1657298982832, "content": {"title": "A Natural Actor-Critic Framework for Zero-Sum Markov Games", "abstract": "We introduce algorithms based on natural actor-critic and analyze their sample complexity for solving two player zero-sum Markov games in the tabular case. Our results improve the best-known sample..."}}
{"id": "WN3GlWfVwY", "cdate": 1640995200000, "mdate": 1681494601269, "content": {"title": "Understanding Deep Neural Function Approximation in Reinforcement Learning via \u03b5-Greedy Exploration", "abstract": ""}}
{"id": "VT8Hx6RbKs", "cdate": 1640995200000, "mdate": 1681494601271, "content": {"title": "Robust Learning from Observation with Model Misspecification", "abstract": ""}}
{"id": "ULts5giZlnm", "cdate": 1640995200000, "mdate": 1681494601271, "content": {"title": "Neural NID Rules", "abstract": ""}}
{"id": "QBwEiBWN-l", "cdate": 1640995200000, "mdate": 1681494601271, "content": {"title": "Identifiability and generalizability from multiple experts in Inverse Reinforcement Learning", "abstract": ""}}
