{"id": "WCZysfor4S", "cdate": 1677628800000, "mdate": 1679978896259, "content": {"title": "Optimization Induced Equilibrium Networks: An Explicit Optimization Perspective for Understanding Equilibrium Models", "abstract": ""}}
{"id": "UzTGdyAZ6Si", "cdate": 1640995200000, "mdate": 1664887170322, "content": {"title": "Global Convergence of Over-parameterized Deep Equilibrium Models", "abstract": "A deep equilibrium model (DEQ) is implicitly defined through an equilibrium point of an infinite-depth weight-tied model with an input-injection. Instead of infinite computations, it solves an equilibrium point directly with root-finding and computes gradients with implicit differentiation. The training dynamics of over-parameterized DEQs are investigated in this study. By supposing a condition on the initial equilibrium point, we show that the unique equilibrium point always exists during the training process, and the gradient descent is proved to converge to a globally optimal solution at a linear convergence rate for the quadratic loss function. In order to show that the required initial condition is satisfied via mild over-parameterization, we perform a fine-grained analysis on random DEQs. We propose a novel probabilistic framework to overcome the technical difficulty in the non-asymptotic analysis of infinite-depth weight-tied models."}}
{"id": "5v-QnZceoWo", "cdate": 1640995200000, "mdate": 1679978896247, "content": {"title": "Exploring Image Regions Not Well Encoded by an INN", "abstract": ""}}
{"id": "k9WzY5fg2eK", "cdate": 1546300800000, "mdate": 1679978896244, "content": {"title": "Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks", "abstract": ""}}
{"id": "rJGgFjA9FQ", "cdate": 1538087800298, "mdate": null, "content": {"title": "Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks", "abstract": "This paper presents two methods to disentangle and interpret contextual effects that are encoded in a pre-trained deep neural network. Unlike convolutional studies that visualize image appearances corresponding to the network output or a neural activation from a global perspective, our research aims to clarify how a certain input unit (dimension) collaborates with other units (dimensions) to constitute inference patterns of the neural network and thus contribute to the network output. The analysis of local contextual effects w.r.t. certain input units is of special values in real applications. In particular, we used our methods to explain the gaming strategy of the alphaGo Zero model in experiments, and our method successfully disentangled the rationale of each move during the game."}}
{"id": "QWacxF9G3r", "cdate": 1514764800000, "mdate": 1681702430501, "content": {"title": "Spectrum concentration in deep residual learning: a free probability appproach", "abstract": "We revisit the initialization of deep residual networks (ResNets) by introducing a novel analytical tool in free probability to the community of deep learning. This tool deals with non-Hermitian random matrices, rather than their conventional Hermitian counterparts in the literature. As a consequence, this new tool enables us to evaluate the singular value spectrum of the input-output Jacobian of a fully-connected deep ResNet for both linear and nonlinear cases. With the powerful tool of free probability, we conduct an asymptotic analysis of the spectrum on the single-layer case, and then extend this analysis to the multi-layer case of an arbitrary number of layers. In particular, we propose to rescale the classical random initialization by the number of residual units, so that the spectrum has the order of $O(1)$, when compared with the large width and depth of the network. We empirically demonstrate that the proposed initialization scheme learns at a speed of orders of magnitudes faster than the classical ones, and thus attests a strong practical relevance of this investigation."}}
