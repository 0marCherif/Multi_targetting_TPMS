{"id": "c8RkqpuVXR", "cdate": 1672531200000, "mdate": 1695969304603, "content": {"title": "Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization", "abstract": ""}}
{"id": "Vaqx9ADKntX", "cdate": 1672531200000, "mdate": 1684341284969, "content": {"title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality", "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality."}}
{"id": "KHbQ_FGv86a", "cdate": 1672531200000, "mdate": 1695969304603, "content": {"title": "Knowledge Guided Representation Learning and Causal Structure Learning in Soil Science", "abstract": ""}}
{"id": "J7-YO5kaLYq", "cdate": 1672531200000, "mdate": 1681852931701, "content": {"title": "An Open-Source Suite of Causal AI Tools and Libraries", "abstract": "We propose to accelerate use-inspired basic research in causal AI through a suite of causal tools and libraries that simultaneously provides core causal AI functionality to practitioners and creates a platform for research advances to be rapidly deployed. In this presentation, we describe our contributions towards an open-source causal AI suite. We describe some of their applications, the lessons learned from their usage, and what is next."}}
{"id": "F6cQ3NY5o2F", "cdate": 1672531200000, "mdate": 1695969304606, "content": {"title": "Food Choice Mimicry on a Large University Campus", "abstract": "Social influence is a strong determinant of food consumption, which in turn influences health. Although consistent observations have been made on the role of social factors in driving similarities in food consumption, much less is known about the precise governing mechanisms. We study social influence on food choice through carefully designed causal analyses, leveraging the sequential nature of shop queues on a major university campus. In particular, we consider a large number of adjacent purchases where a focal user immediately follows another user (\"partner\") in the checkout queue and both make a purchase. Identifying the partner's impact on the focal user, we find strong evidence of a specific behavioral mechanism for how dietary similarities between individuals arise: purchasing mimicry, a phenomenon where the focal user copies the partner's purchases. For instance, across food additions purchased during lunchtime together with a meal, we find that the focal user is significantly more likely to purchase the food item when the partner buys the item, v.s. when the partner does not, increasing the purchasing probability by 14% in absolute terms, or by 83% in relative terms. The effect is observed across all food types, but largest for condiments, and smallest for soft drinks. We find that no such effect is observed when a focal user is compared to a random (rather than directly preceding) partner. Furthermore, purchasing mimicry is present across age, gender, and status subpopulations, but strongest for students and the youngest persons. Finally, we find a dose-response relationship whereby mimicry decreases as proximity in the purchasing queue decreases. The results of this study elucidate the behavioral mechanism of purchasing mimicry and have further implications for understanding and improving dietary behaviors on campus."}}
{"id": "DJaNPZiIlm", "cdate": 1672531200000, "mdate": 1683916044695, "content": {"title": "Language Model Decoding as Likelihood-Utility Alignment", "abstract": "Martin Josifoski, Maxime Peyrard, Frano Raji\u010d, Jiheng Wei, Debjit Paul, Valentin Hartmann, Barun Patra, Vishrav Chaudhary, Emre Kiciman, Boi Faltings. Findings of the Association for Computational Linguistics: EACL 2023. 2023."}}
{"id": "-9MFGblmPD", "cdate": 1672531200000, "mdate": 1681852931816, "content": {"title": "Counterfactual (Non-)identifiability of Learned Structural Causal Models", "abstract": "Recent advances in probabilistic generative modeling have motivated learning Structural Causal Models (SCM) from observational datasets using deep conditional generative models, also known as Deep Structural Causal Models (DSCM). If successful, DSCMs can be utilized for causal estimation tasks, e.g., for answering counterfactual queries. In this work, we warn practitioners about non-identifiability of counterfactual inference from observational data, even in the absence of unobserved confounding and assuming known causal structure. We prove counterfactual identifiability of monotonic generation mechanisms with single dimensional exogenous variables. For general generation mechanisms with multi-dimensional exogenous variables, we provide an impossibility result for counterfactual identifiability, motivating the need for parametric assumptions. As a practical approach, we propose a method for estimating worst-case errors of learned DSCMs' counterfactual predictions. The size of this error can be an essential metric for deciding whether or not DSCMs are a viable approach for counterfactual inference in a specific problem setting. In evaluation, our method confirms negligible counterfactual errors for an identifiable SCM from prior work, and also provides informative error bounds on counterfactual errors for a non-identifiable synthetic SCM."}}
{"id": "EI8lfDkPHI", "cdate": 1664928790763, "mdate": null, "content": {"title": "Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Systems", "abstract": "Given a user's input text, text-matching recommender systems output relevant items by comparing the input text to available items' description, such as product-to-product recommendation on e-commerce platforms. As users' interests and item inventory are expected to change, it is important for a text-matching system to generalize to data shifts, a task known as out-of-distribution (OOD) generalization. However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization. For a product recommendation task, fine-tuning  obtains worse accuracy than the base model when recommending items in a new category or for a future time period. To explain this  generalization failure, we consider an intervention-based importance metric, which shows that  a fine-tuned model captures spurious correlations  and fails to learn the causal features that determine the relevance between any two text inputs. Moreover, standard methods for causal regularization do not apply in this setting, because unlike in images, there exist no universally spurious features in a text-matching task (the same token may be spurious or causal depending on the text it is being matched to). For OOD generalization on text inputs, therefore, we highlight a different goal: avoiding high importance scores for certain features. We do so using an intervention-based regularizer that constraints the importance score of any token on the model's relevance score to be similar to the base model. Results on Amazon product  and 3 question recommendation datasets show that our proposed regularizer improves generalization for both in-distribution and OOD evaluation, especially in difficult scenarios when the base model is not accurate."}}
{"id": "5cCX_xZSeEl", "cdate": 1664815581566, "mdate": null, "content": {"title": "Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Systems", "abstract": "Given a user's input text, text-matching recommender systems output relevant items by comparing the input text to available items' description, such as product-to-product recommendation on e-commerce platforms. As users' interests and item inventory are expected to change, it is important for a text-matching system to generalize to data shifts, a task known as out-of-distribution (OOD) generalization. However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization. For a product recommendation task, fine-tuning  obtains worse accuracy than the base model when recommending items in a new category or for a future time period. To explain this  generalization failure, we consider an intervention-based importance metric, which shows that  a fine-tuned model captures spurious correlations  and fails to learn the causal features that determine the relevance between any two text inputs. Moreover, standard methods for causal regularization do not apply in this setting, because unlike in images, there exist no universally spurious features in a text-matching task (the same token may be spurious or causal depending on the text it is being matched to). For OOD generalization on text inputs, therefore, we highlight a different goal: avoiding high importance scores for certain features. We do so using an intervention-based regularizer that constraints the causal effect of any token on the model's relevance score to be similar to the base model. Results on Amazon product  and 3 question recommendation datasets show that our proposed regularizer improves generalization for both in-distribution and OOD evaluation, especially in difficult scenarios when the base model is not accurate."}}
{"id": "-gVJ1_lD1RH", "cdate": 1664815580530, "mdate": null, "content": {"title": "A Causal AI Suite for Decision-Making", "abstract": "Critical data science and decision-making questions across a wide variety of domains are fundamentally causal questions. We present a suite of open-source causal tools and libraries that aims to simultaneously provide core causal AI functionality to practitioners and create a platform for research advances to be rapidly deployed. In this paper, we describe our contributions towards such a comprehensive causal AI suite of tools and libraries, its design, and lessons we are learning from its growing adoption. We hope that our work accelerates use-inspired basic research for improvement of causal AI."}}
