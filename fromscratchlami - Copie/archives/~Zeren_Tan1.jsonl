{"id": "kciCTrtbzVl", "cdate": 1663850005464, "mdate": null, "content": {"title": "Koopman neural operator for learning non-linear partial differential equations", "abstract": "The lacking of analytic solutions of diverse partial differential equations (PDEs) gives birth to series of computational techniques for numerical solutions. In machine learning, numerous latest advances of solver designs are accomplished in developing neural operators, a kind of mesh-free approximators of the infinite-dimensional operators that map between different parameterization spaces of equation solutions. Although neural operators exhibit generalization capacities for learning an entire PDE family simultaneously, they become less accurate and explainable while learning long-term behaviours of non-linear PDE families. In this paper, we propose Koopman neural operator (KNO), a new neural operator, to overcome these challenges. With the same objective of learning an infinite-dimensional mapping between Banach spaces that serves as the solution operator of target PDE family, our approach differs from existing models by formulating a non-linear dynamic system of equation solution. By approximating the Koopman operator, an infinite-dimensional linear operator governing all possible observations of the dynamic system, to act on the flow mapping of dynamic system, we can equivalently learn the solution of an entire non-linear PDE family by solving simple linear prediction problems. In zero-shot prediction and long-term prediction experiments on representative PDEs (e.g., the Navier-Stokes equation), KNO exhibits notable advantages in breaking the  tradeoff between accuracy and efficiency (e.g., model size) while previous state-of-the-art models are limited."}}
{"id": "9czfKu1QqcN", "cdate": 1663850003284, "mdate": null, "content": {"title": "ErGOT: entropy-regularized graph optimal transport", "abstract": "Graph comparison is a fundamental task, which not only relates to graph matching, an NP-hard problem, but also has various applications in graph learning. We tackle this task by studying optimal graph representation and the entropy-regularized optimal transport between graphs (ErGOT). First, we analytically derive a family of Gaussian variables that optimally represent graph topology and node relation. Second, we realize graph comparison by formulating ErGOT, a framework with low sample complexity, on represented graph information. Third, we control biases in the solution by defining ErGOT with a 2-Sinkhorn divergence, whose closed-form expression can be derived on the manifold of Gaussian variables. As the Gaussian geometry changes with entropy regularization magnitude, ErGOT defined with 2-Sinkhorn divergence wanders between pure optimal transport and maximum mean discrepancy among graphs. We demonstrate that these statistically efficient, principally unbiased, and in-between properties ensure theoretically faster convergence of our approach to empirically higher performance than the state-of-art algorithms on graph alignment, sketching, and retrieval tasks. "}}
{"id": "uHExO8MOmV", "cdate": 1609459200000, "mdate": 1681652470726, "content": {"title": "T-SCI: A Two-Stage Conformal Inference Algorithm with Guaranteed Coverage for Cox-MLP", "abstract": ""}}
{"id": "CY5KIWppbt4", "cdate": 1609459200000, "mdate": 1681652470733, "content": {"title": "T-SCI: A Two-Stage Conformal Inference Algorithm with Guaranteed Coverage for Cox-MLP", "abstract": ""}}
