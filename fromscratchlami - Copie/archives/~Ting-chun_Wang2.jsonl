{"id": "Z41QMv-B5g", "cdate": 1695141989773, "mdate": 1695141989773, "content": {"title": "SPACE: Speech-driven Portrait Animation with Controllable Expression", "abstract": "Animating portraits using speech has received growing attention in recent years, with various creative and practical use cases. An ideal generated video should have good lip sync with the audio, natural facial expressions and head motions, and high frame quality. In this work, we present SPACE, which uses speech and a single image to generate high-resolution, and expressive videos with realistic head pose, without requiring a driving video. It uses a multi-stage approach, combining the controllability of facial landmarks with the high-quality synthesis power of a pretrained face generator. SPACE also allows for the control of emotions and their intensities. Our method outperforms prior methods in objective metrics for image quality and facial motions and is strongly preferred by users in pair-wise comparisons."}}
{"id": "e5QXl6B31Vu", "cdate": 1694970293934, "mdate": 1694970293934, "content": {"title": "SPACE: Speech-driven Portrait Animation with Controllable Expression", "abstract": "Animating portraits using speech has received growing attention in recent years, with various creative and practical use cases. An ideal generated video should have good lip sync with the audio, natural facial expressions and head motions, and high frame quality. In this work, we present SPACE, which uses speech and a single image to generate high-resolution, and expressive videos with realistic head pose, without requiring a driving video. It uses a multi-stage approach, combining the controllability of facial landmarks with the high-quality synthesis power of a pretrained face generator. SPACE also allows for the control of emotions and their intensities. Our method outperforms prior methods in objective metrics for image quality and facial motions and is strongly preferred by users in pair-wise comparisons."}}
{"id": "eXS5gh7Qkd", "cdate": 1694970205010, "mdate": 1694970205010, "content": {"title": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion", "abstract": "We present DreamPose, a diffusion-based method for generating animated fashion videos from still images. Given an image and a sequence of human body poses, our method synthesizes a video containing both human and fabric motion. To achieve this, we transform a pretrained text-to-image model (Stable Diffusion) into a pose-and-image guided video synthesis model, using a novel finetuning strategy, a set of architectural changes to support the added conditioning signals, and techniques to encourage temporal consistency. We fine-tune on a collection of fashion videos from the UBC Fashion dataset. We evaluate our method on a variety of clothing styles and poses, and demonstrate that our method produces state-of-the-art results on fashion video animation. Video results are available on our project page."}}
{"id": "qCHarZlvePO", "cdate": 1682899200000, "mdate": 1695403007750, "content": {"title": "Partial Convolution for Padding, Inpainting, and Image Synthesis", "abstract": "Partial convolution weights convolutions with binary masks and renormalizes on valid pixels. It was originally proposed for image inpainting task because a corrupted image processed by a standard convolutional often leads to artifacts. Therefore, binary masks are constructed that define the valid and corrupted pixels, so that partial convolution results are only calculated based on valid pixels. It has been also used for conditional image synthesis task, so that when a scene is generated, convolution results of an instance depend only on the feature values that belong to the same instance. One of the unexplored applications for partial convolution is padding which is a critical component of modern convolutional networks. Common padding schemes make strong assumptions about how the padded data should be extrapolated. We show that these padding schemes impair model accuracy, whereas partial convolution based padding provides consistent improvements across a range of tasks. In this article, we review partial convolution applications under one framework. We conduct a comprehensive study of the partial convolution based padding on a variety of computer vision tasks, including image classification, 3D-convolution-based action recognition, and semantic segmentation. Our results suggest that partial convolution-based padding shows promising improvements over strong baselines."}}
{"id": "1E0K-98gQXW", "cdate": 1672531200000, "mdate": 1695403008046, "content": {"title": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion", "abstract": "We present DreamPose, a diffusion-based method for generating animated fashion videos from still images. Given an image and a sequence of human body poses, our method synthesizes a video containing both human and fabric motion. To achieve this, we transform a pretrained text-to-image model (Stable Diffusion) into a pose-and-image guided video synthesis model, using a novel finetuning strategy, a set of architectural changes to support the added conditioning signals, and techniques to encourage temporal consistency. We fine-tune on a collection of fashion videos from the UBC Fashion dataset. We evaluate our method on a variety of clothing styles and poses, and demonstrate that our method produces state-of-the-art results on fashion video animation. Video results are available on our project page."}}
{"id": "F6G37fYsQ5", "cdate": 1668589501489, "mdate": 1668589501489, "content": {"title": "Domain Stylization: A Fast Covariance Matching Framework Towards Domain Adaptation", "abstract": "Generating computer graphics (CG) rendered synthetic images has been widely used to create simulation environments for\nrobotics/autonomous driving and generate labeled data. Yet, the problem of training models purely with synthetic data remains\nchallenging due to the considerable domain gaps caused by current limitations on rendering. In this paper, we propose a simple yet\neffective domain adaptation framework towards closing such gap at image level. Unlike many GAN-based approaches, our method\naims to match the covariance of the universal feature embeddings across domains, making the adaptation a fast, convenient step and\navoiding the need for potentially difficult GAN training. To align domains more precisely, we further propose a conditional covariance\nmatching framework which iteratively estimates semantic segmentation regions and conditionally matches the class-wise feature\ncovariance given the segmentation regions. We demonstrate that both tasks can mutually refine and considerably improve each other,\nleading to state-of-the-art domain adaptation results. Extensive experiments under multiple synthetic-to-real settings show that our\napproach exceeds the performance of latest domain adaptation approaches. In addition, we offer a quantitative analysis where our\nframework shows considerable reduction in Frechet Inception distance between source and target domains, demonstrating the\neffectiveness of this work in bridging the synthetic-to-real domain gap."}}
{"id": "VnAwNNJiwDb", "cdate": 1652737355084, "mdate": null, "content": {"title": "Generating Long Videos of Dynamic Scenes", "abstract": "We present a video generation model that accurately reproduces object motion, changes in camera viewpoint, and new content that arises over time. Existing video generation methods often fail to produce new content as a function of time while maintaining consistencies expected in real environments, such as plausible dynamics and object persistence. A common failure case is for content to never change due to over-reliance on inductive bias to provide temporal consistency, such as a single latent code that dictates content for the entire video. On the other extreme, without long-term consistency, generated videos may morph unrealistically between different scenes. To address these limitations, we prioritize the time axis by redesigning the temporal latent representation and learning long-term consistency from data by training on longer videos. We leverage a two-phase training strategy, where we separately train using longer videos at a low resolution and shorter videos at a high resolution. To evaluate the capabilities of our model, we introduce two new benchmark datasets with explicit focus on long-term temporal dynamics."}}
{"id": "TrsAkAbC96", "cdate": 1652737354049, "mdate": null, "content": {"title": "Implicit Warping for Animation with Image Sets", "abstract": "We present a new implicit warping framework for image animation using sets of source images through the transfer of motion of a driving video. A single cross-modal attention layer is used to find correspondences between the source images and the driving image, choose the most appropriate features from different source images, and warp the selected features. This is in contrast to the existing methods that use explicit flow-based warping, which is designed for animation using a single source and does not extend well to multiple sources. The pick-and-choose capability of our framework helps it achieve state-of-the-art results on multiple datasets for image animation using both single and multiple source images."}}
{"id": "nlhx7T09Sl", "cdate": 1640995200000, "mdate": 1684166887550, "content": {"title": "Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation", "abstract": "Given a portrait image of a person and an environment map of the target lighting, portrait relighting aims to re-illuminate the person in the image as if the person appeared in an environment with the target lighting. To achieve high-quality results, recent methods rely on deep learning. An effective approach is to supervise the training of deep neural networks with a high-fidelity dataset of desired input-output pairs, captured with a light stage. However, acquiring such data requires an expensive special capture rig and time-consuming efforts, limiting access to only a few resourceful laboratories. To address the limitation, we propose a new approach that can perform on par with the state-of-the-art (SOTA) relighting methods without requiring a light stage. Our approach is based on the realization that a successful relighting of a portrait image depends on two conditions. First, the method needs to mimic the behaviors of physically-based relighting. Second, the output has to be photorealistic. To meet the first condition, we propose to train the relighting network with training data generated by a virtual light stage that performs physically-based rendering on various 3D synthetic humans under different environment maps. To meet the second condition, we develop a novel synthetic-to-real approach to bring photorealism to the relighting network output. In addition to achieving SOTA results, our approach offers several advantages over the prior methods, including controllable glares on glasses and more temporally-consistent results for relighting videos."}}
{"id": "d5oQ_PSxR0f", "cdate": 1640995200000, "mdate": 1695403007720, "content": {"title": "Implicit Warping for Animation with Image Sets", "abstract": "We present a new implicit warping framework for image animation using sets of source images through the transfer of motion of a driving video. A single cross-modal attention layer is used to find correspondences between the source images and the driving image, choose the most appropriate features from different source images, and warp the selected features. This is in contrast to the existing methods that use explicit flow-based warping, which is designed for animation using a single source and does not extend well to multiple sources. The pick-and-choose capability of our framework helps it achieve state-of-the-art results on multiple datasets for image animation using both single and multiple source images."}}
