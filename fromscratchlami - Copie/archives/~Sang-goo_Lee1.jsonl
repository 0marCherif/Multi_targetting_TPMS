{"id": "MP55EKiUXU", "cdate": 1708395254643, "mdate": 1708395254643, "content": {"title": "Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning", "abstract": "When deploying machine learning systems to the wild, it is highly desirable for them to effectively leverage prior knowledge to the unfamiliar domain while also firing alarms to anomalous inputs. In order to address these requirements, Universal Domain Adaptation (UniDA) has emerged as a novel research area in computer vision, focusing on achieving both adaptation ability and robustness (i.e., the ability to detect out-of-distribution samples). While UniDA has led significant progress in computer vision, its application on language input still needs to be explored despite its feasibility. In this paper, we propose a comprehensive benchmark for natural language that offers thorough viewpoints of the model\u2019s generalizability and robustness. Our benchmark encompasses multiple datasets with varying difficulty levels and characteristics, including temporal shifts and diverse domains. On top of our testbed, we validate existing UniDA methods from computer vision and state-of-the-art domain adaptation techniques from NLP literature, yielding valuable findings: We observe that UniDA methods originally designed for image input can be effectively transferred to the natural language domain while also underscoring the effect of adaptation difficulty in determining the model\u2019s performance."}}
{"id": "fkipwrCvLp", "cdate": 1708394986173, "mdate": 1708394986173, "content": {"title": "Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble", "abstract": "Out-of-distribution (OOD) detection aims to discern outliers from the intended data distribution, which is crucial to maintaining high reliability and a good user experience. Most recent studies in OOD detection utilize the information from a single representation that resides in the penultimate layer to determine whether the input is anomalous or not. Although such a method is straightforward, the potential of diverse information in the intermediate layers is overlooked. In this paper, we propose a novel framework based on contrastive learning that encourages intermediate features to learn layer-specialized representations and assembles them implicitly into a single representation to absorb rich information in the pre-trained language model. Extensive experiments in various intent classification and OOD datasets demonstrate that our approach is significantly more effective than other works."}}
{"id": "-W0JidJ2ipV", "cdate": 1708394783894, "mdate": 1708394783894, "content": {"title": "Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP", "abstract": "When deploying machine learning systems to the wild, it is highly desirable for them to effectively leverage prior knowledge to the unfamiliar domain while also firing alarms to anomalous inputs. In order to address these requirements, Universal Domain Adaptation (UniDA) has emerged as a novel research area in computer vision, focusing on achieving both adaptation ability and robustness (i.e., the ability to detect out-of-distribution samples). While UniDA has led significant progress in computer vision, its application on language input still needs to be explored despite its feasibility. In this paper, we propose a comprehensive benchmark for natural language that offers thorough viewpoints of the model\u2019s generalizability and robustness. Our benchmark encompasses multiple datasets with varying difficulty levels and characteristics, including temporal shifts and diverse domains. On top of our testbed, we validate existing UniDA methods from computer vision and state-of-the-art domain adaptation techniques from NLP literature, yielding valuable findings: We observe that UniDA methods originally designed for image input can be effectively transferred to the natural language domain while also underscoring the effect of adaptation difficulty in determining the model\u2019s performance."}}
{"id": "qmdOPKZYvQ", "cdate": 1686205365913, "mdate": 1686205365913, "content": {"title": "CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels", "abstract": "Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis, a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels). Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold: (1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robust model on the top of LM, which learns an accurate decision boundary from an extracted noisy dataset. Throughout in-depth investigations on various datasets, we demonstrated that CELDA reaches new state-of-the-art in weakly-supervised text classification and narrows the gap with a fully-supervised model. Additionally, our proposed methodology can be applied universally to any LM and has the potential to scale to larger models, making it a more viable option for utilizing large LMs."}}
{"id": "_xAAzrQMfPl", "cdate": 1686205274805, "mdate": 1686205274805, "content": {"title": "Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners", "abstract": "Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified that PALP significantly enhances the input representations closing the gap between ICL in the data-hungry scenario and fine-tuning in the data-abundant scenario with little training overhead, potentially making PALP a strong alternative in a black-box scenario."}}
{"id": "hiW3ubshzE", "cdate": 1672531200000, "mdate": 1680488008554, "content": {"title": "Multi-scale Contrastive Learning for Complex Scene Generation", "abstract": ""}}
{"id": "oru0QDDeKIz", "cdate": 1640995200000, "mdate": 1680488008553, "content": {"title": "Exploiting Session Information in BERT-based Session-aware Sequential Recommendation", "abstract": ""}}
{"id": "lSmm5Vaivx", "cdate": 1640995200000, "mdate": 1680488008740, "content": {"title": "Retrieval-Augmented Response Generation for Knowledge-Grounded Conversation in the Wild", "abstract": ""}}
{"id": "aQ6o1m8yqFk", "cdate": 1640995200000, "mdate": 1680488008554, "content": {"title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations", "abstract": ""}}
{"id": "ZPUP9OVyj79", "cdate": 1640995200000, "mdate": 1680488008554, "content": {"title": "Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble", "abstract": ""}}
