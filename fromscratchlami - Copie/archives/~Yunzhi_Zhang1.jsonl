{"id": "4eC4CSqTcw", "cdate": 1672531200000, "mdate": 1681490133915, "content": {"title": "IKEA-Manual: Seeing Shape Assembly Step by Step", "abstract": ""}}
{"id": "D-pG5nGGBlw", "cdate": 1668670314959, "mdate": 1668670314959, "content": {"title": "Translating a Visual LEGO Manual to a Machine-Executable Plan", "abstract": "We study the problem of translating an image-based, step-by-step assembly manual created by human designers into machine-interpretable instructions. We formulate this problem as a sequential prediction task: at each step, our model reads the manual, locates the components to be added to the current shape, and infers their 3D poses. This task poses the challenge of establishing a 2D-3D correspondence between the manual image and the real 3D object, and 3D pose estimation for unseen 3D objects, since a new component to be added in a step can be an object built from previous steps. To address these two challenges, we present a novel learning-based framework, the Manual-to-Executable-Plan Network (MEPNet), which reconstructs the assembly steps from a sequence of manual images. The key idea is to integrate neural 2D keypoint detection modules and 2D-3D projection algorithms for high-precision prediction and strong generalization to unseen components. The MEPNet outperforms existing methods on three newly collected LEGO manual datasets and a Minecraft house dataset."}}
{"id": "QAV2CcLEDh", "cdate": 1663850162768, "mdate": null, "content": {"title": "MaskViT: Masked Visual Pre-Training for Video Prediction", "abstract": "The ability to predict future visual observations conditioned on past observations and motor commands can enable embodied agents to plan solutions to a variety of tasks in complex environments. This work shows that we can create good video prediction models by pre-training transformers via masked visual modeling. Our approach, named MaskViT, is based on two simple design decisions. First, for memory and training efficiency, we use two types of window attention: spatial and spatiotemporal. Second, during training, we mask a variable percentage of tokens instead of a fixed mask ratio. For inference, MaskViT generates all tokens via iterative refinement where we incrementally decrease the masking ratio following a mask scheduling function. On several datasets we demonstrate that MaskViT outperforms prior works in video prediction, is parameter efficient, and can generate high resolution videos ($256 \\times $256). Further, we demonstrate the benefits of inference speedup (up to $512 \\times$) due to iterative decoding by using MaskViT for planning on a real robot. Our work suggests that we can endow embodied agents with powerful predictive models by leveraging the general framework of masked visual modeling with minimal domain knowledge. "}}
{"id": "ZeeswGSOw7r", "cdate": 1654427645687, "mdate": null, "content": {"title": "IKEA-Manual: Seeing Shape Assembly Step by Step", "abstract": "Human-designed visual manuals are crucial components in shape assembly activities. They provide step-by-step guidance on how we should move and connect different parts in a convenient and physically-realizable way. While there has been an ongoing effort in building agents that perform assembly tasks, the information in human-design manuals has been largely overlooked. We identify that this is due to 1) a lack of realistic 3D assembly objects that have paired manuals and 2) the difficulty of extracting structured information from purely image-based manuals. Motivated by this observation, we present IKEA-Manual, a dataset consisting of 102 IKEA objects paired with assembly manuals. We provide fine-grained annotations on the IKEA objects and assembly manuals, including decomposed assembly parts, assembly plans, manual segmentation, and 2D-3D correspondence between 3D parts and visual manuals. We illustrate the broad application of our dataset on four tasks related to shape assembly: assembly plan generation, part segmentation, pose estimationand 3D part assembly."}}
{"id": "dtLJ6Swfdhb", "cdate": 1640995200000, "mdate": 1681490136588, "content": {"title": "MaskViT: Masked Visual Pre-Training for Video Prediction", "abstract": ""}}
{"id": "dMu7Y290nh-", "cdate": 1640995200000, "mdate": 1668670782094, "content": {"title": "Translating a Visual LEGO Manual to a Machine-Executable Plan", "abstract": "We study the problem of translating an image-based, step-by-step assembly manual created by human designers into machine-interpretable instructions. We formulate this problem as a sequential prediction task: at each step, our model reads the manual, locates the components to be added to the current shape, and infers their 3D poses. This task poses the challenge of establishing a 2D-3D correspondence between the manual image and the real 3D object, and 3D pose estimation for unseen 3D objects, since a new component to be added in a step can be an object built from previous steps. To address these two challenges, we present a novel learning-based framework, the Manual-to-Executable-Plan Network (MEPNet), which reconstructs the assembly steps from a sequence of manual images. The key idea is to integrate neural 2D keypoint detection modules and 2D-3D projection algorithms for high-precision prediction and strong generalization to unseen components. The MEPNet outperforms existing methods on three newly collected LEGO manual datasets and a Minecraft house dataset."}}
{"id": "Zyhwc0x5de", "cdate": 1640995200000, "mdate": 1681490136788, "content": {"title": "Translating a Visual LEGO Manual to a Machine-Executable Plan", "abstract": ""}}
{"id": "CXUO43qpwaY", "cdate": 1640995200000, "mdate": 1681490136662, "content": {"title": "Video Extrapolation in Space and Time", "abstract": ""}}
{"id": "BNwcwObac47", "cdate": 1640995200000, "mdate": 1681490136384, "content": {"title": "Video Extrapolation in Space and Time", "abstract": ""}}
{"id": "-IrKCRsKb9v", "cdate": 1640995200000, "mdate": 1675646618612, "content": {"title": "Seeing a Rose in Five Thousand Ways", "abstract": "What is a rose, visually? A rose comprises its intrinsics, including the distribution of geometry, texture, and material specific to its object category. With knowledge of these intrinsic properties, we may render roses of different sizes and shapes, in different poses, and under different lighting conditions. In this work, we build a generative model that learns to capture such object intrinsics from a single image, such as a photo of a bouquet. Such an image includes multiple instances of an object type. These instances all share the same intrinsics, but appear different due to a combination of variance within these intrinsics and differences in extrinsic factors, such as pose and illumination. Experiments show that our model successfully learns object intrinsics (distribution of geometry, texture, and material) for a wide range of objects, each from a single Internet image. Our method achieves superior results on multiple downstream tasks, including intrinsic image decomposition, shape and image generation, view synthesis, and relighting."}}
