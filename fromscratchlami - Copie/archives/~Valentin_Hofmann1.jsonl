{"id": "zQvwARsz-h", "cdate": 1640995200000, "mdate": 1681661710845, "content": {"title": "Geographic Adaptation of Pretrained Language Models", "abstract": "Geographic features are commonly used to improve the performance of pretrained language models (PLMs) on NLP tasks where they are intuitively beneficial (e.g., geolocation prediction, dialect feature prediction). Existing methods, however, leverage geographic information in task-specific fine-tuning and fail to integrate it into the geo-linguistic knowledge encoded by PLMs, which would make it transferable across different tasks. In this paper, we introduce an approach to task-agnostic geoadaptation of PLMs that forces them to learn associations between linguistic phenomena and geographic locations. Geoadaptation is an intermediate training step that couples language modeling and geolocation prediction in a multi-task learning setup. In our main set of experiments, we geoadapt BERTi\\'{c}, a PLM for Bosnian-Croatian-Montenegrin-Serbian (BCMS), using a corpus of geotagged BCMS tweets. Evaluation on three tasks, namely fine-tuned as well as zero-shot geolocation prediction and zero-shot prediction of dialect features, shows that geoadaptation is very effective: e.g., we obtain state-of-the-art performance in supervised geolocation prediction and report massive gains over geographically uninformed PLMs on zero-shot geolocation prediction. Moreover, in follow-up experiments we successfully geoadapt two other PLMs, specifically ScandiBERT on Norwegian, Swedish, and Danish tweets and GermanBERT on Jodel posts in German from Austria, Germany, and Switzerland, proving that the benefits of geoadaptation are not limited to a particular language area and PLM."}}
{"id": "njyqFiH48q", "cdate": 1640995200000, "mdate": 1681661710842, "content": {"title": "The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative", "abstract": ""}}
{"id": "SOfJ86pgUMx", "cdate": 1640995200000, "mdate": 1681661710978, "content": {"title": "Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology", "abstract": "We propose a fully unsupervised method to detect bias in contextualized embeddings. The method leverages the assortative information latently encoded by social networks and combines orthogonality r..."}}
{"id": "JlvO97QOlE", "cdate": 1640995200000, "mdate": 1664919513572, "content": {"title": "CaMEL: Case Marker Extraction without Labels", "abstract": ""}}
{"id": "GZSXHo1Yhtv", "cdate": 1640995200000, "mdate": 1681661710845, "content": {"title": "Modeling Ideological Salience and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity", "abstract": ""}}
{"id": "ArLjgwwSO5x", "cdate": 1640995200000, "mdate": 1681661710839, "content": {"title": "The Reddit Politosphere: A Large-Scale Text and Network Resource of Online Political Discourse", "abstract": "We introduce the Reddit Politosphere, a large-scale resource of online political discourse covering more than 600 political discussion groups over a period of 12 years. It is to the best of our knowledge the largest and ideologically most comprehensive dataset of its type now available. One key feature of the Reddit Politosphere is that it consists of both text and network data, allowing for methodologically-diverse analyses. We describe in detail how we create the Reddit Politosphere, present descriptive statistics, and sketch potential directions for future research based on the resource."}}
{"id": "2HAqOjxCago", "cdate": 1640995200000, "mdate": 1681661710948, "content": {"title": "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers", "abstract": ""}}
{"id": "Lo9lHfYTqH", "cdate": 1609459200000, "mdate": 1636883552443, "content": {"title": "Dynamic Contextualized Word Embeddings", "abstract": "Valentin Hofmann, Janet Pierrehumbert, Hinrich Sch\u00fctze. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "HheXjMVsogd", "cdate": 1609459200000, "mdate": 1636883552256, "content": {"title": "Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words", "abstract": "Valentin Hofmann, Janet Pierrehumbert, Hinrich Sch\u00fctze. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "hnt_2RQ3abY", "cdate": 1577836800000, "mdate": 1636883552150, "content": {"title": "A Graph Auto-encoder Model of Derivational Morphology", "abstract": "Valentin Hofmann, Hinrich Sch\u00fctze, Janet Pierrehumbert. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
