{"id": "xdyqePsI5GC", "cdate": 1640995200000, "mdate": 1682325737330, "content": {"title": "Truncated Edge-based Color Constancy", "abstract": "In this paper we propose the truncated edge-based color constancy. It is based on, and extends, the edge-based framework by introducing the use of truncated Gaussian filters. The truncation level can be controlled with the use of a dedicated parameter that is added to the other three parameters existing in the edge-based framework, namely the derivative order, the standard deviation of the Gaussian filter, and the Minkowski norm. Experimental results on two standard dataset for color constancy show that the truncated edge-based framework allows to achieve the same or higher illuminant estimation accuracy of the edge-based framework considerably reducing the number of operations."}}
{"id": "vYuwMkKpp9", "cdate": 1640995200000, "mdate": 1682325737606, "content": {"title": "Designing an AI-Based Virtual Try-On Web Application", "abstract": "In the last few years, Augmented Reality, Virtual Reality, and Artificial Intelligence (AI) have been increasingly employed in different application domains. Among them, the retail market presents the opportunity to allow people to check the appearance of accessories, makeup, hairstyle, hair color, and clothes on themselves, exploiting virtual try-on applications. In this paper, we propose an eyewear virtual try-on experience based on a framework that leverages advanced deep learning-based computer vision techniques. The virtual try-on is performed on a 3D face reconstructed from a single input image. In designing our system, we started by studying the underlying architecture, components, and their interactions. Then, we assessed and compared existing face reconstruction approaches. To this end, we performed an extensive analysis and experiments for evaluating their design, complexity, geometry reconstruction errors, and reconstructed texture quality. The experiments allowed us to select the most suitable approach for our proposed try-on framework. Our system considers actual glasses and face sizes to provide a realistic fit estimation using a markerless approach. The user interacts with the system by using a web application optimized for desktop and mobile devices. Finally, we performed a usability study that showed an above-average score of our eyewear virtual try-on application."}}
{"id": "qQ__xdRJzt", "cdate": 1640995200000, "mdate": 1667372933637, "content": {"title": "AIM 2022 Challenge on Super-Resolution of Compressed Image and Video: Dataset, Methods and Results", "abstract": "This paper reviews the Challenge on Super-Resolution of Compressed Image and Video at AIM 2022. This challenge includes two tracks. Track 1 aims at the super-resolution of compressed image, and Track~2 targets the super-resolution of compressed video. In Track 1, we use the popular dataset DIV2K as the training, validation and test sets. In Track 2, we propose the LDV 3.0 dataset, which contains 365 videos, including the LDV 2.0 dataset (335 videos) and 30 additional videos. In this challenge, there are 12 teams and 2 teams that submitted the final results to Track 1 and Track 2, respectively. The proposed methods and solutions gauge the state-of-the-art of super-resolution on compressed image and video. The proposed LDV 3.0 dataset is available at https://github.com/RenYang-home/LDV_dataset. The homepage of this challenge is at https://github.com/RenYang-home/AIM22_CompressSR."}}
{"id": "pbKM_GWdiE_", "cdate": 1640995200000, "mdate": 1682325737143, "content": {"title": "A Framework for Contrast Enhancement Algorithms Optimization", "abstract": "We present a general-purpose framework for the optimization of parametric contrast enhancement algorithms. We first define a regression module for image acceptability, which is based on deep neural features and which is trained on a large dataset of user-expressed preferences. This regression module is then used as the objective function of a Bayesian optimization process, guiding the search for the optimal parameters of a given contrast enhancement algorithm. In our experiments we optimize three different contrast enhancement algorithms of varying levels of complexity. The effectiveness of our optimization framework is experimentally confirmed by evaluating the output of the optimized contrast enhancement algorithms with respect to reference enhanced images."}}
{"id": "pF21bxqU-Z", "cdate": 1640995200000, "mdate": 1668204249113, "content": {"title": "Semi-supervised cross-lingual speech emotion recognition", "abstract": "Speech emotion recognition (SER) on a single language has achieved remarkable results through deep learning approaches over the last decade. However, cross-lingual SER remains a challenge in real-world applications due to (i) a large difference between the source and target domain distributions, (ii) the availability of few labeled and many unlabeled utterances for the new language. Taking into account previous aspects, we propose a Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition when a few labels from the new language are available. Based on a Convolutional Neural Network (CNN), our method adapts to a new language by exploiting a pseudo-labeling strategy for the unlabeled utterances. In particular, the use of a hard and soft pseudo-labels approach is investigated. We thoroughly evaluate the performance of the method in a speaker-independent setup on both the source and the new language and show its robustness across five languages belonging to different linguistic strains."}}
{"id": "ohPLf0hfdC", "cdate": 1640995200000, "mdate": 1681790042120, "content": {"title": "AIM 2022 Challenge on Super-Resolution of Compressed Image and Video: Dataset, Methods and Results", "abstract": "This paper reviews the Challenge on Super-Resolution of Compressed Image and Video at AIM 2022. This challenge includes two tracks. Track 1 aims at the super-resolution of compressed image, and Track\u00a02 targets the super-resolution of compressed video. In Track 1, we use the popular dataset DIV2K as the training, validation and test sets. In Track 2, we propose the LDV 3.0 dataset, which contains 365 videos, including the LDV 2.0 dataset (335 videos) and 30 additional videos. In this challenge, there are 12 teams and 2 teams that submitted the final results to Track 1 and Track 2, respectively. The proposed methods and solutions gauge the state-of-the-art of super-resolution on compressed image and video. The proposed LDV 3.0 dataset is available at https://github.com/RenYang-home/LDV_dataset . The homepage of this challenge is at https://github.com/RenYang-home/AIM22_CompressSR ."}}
{"id": "kh6-hWxp927", "cdate": 1640995200000, "mdate": 1682325737159, "content": {"title": "SfM Flow: A comprehensive toolset for the evaluation of 3D reconstruction pipelines", "abstract": ""}}
{"id": "jJikrnBQ_k6", "cdate": 1640995200000, "mdate": 1682325737139, "content": {"title": "COCOA: Combining Color Constancy Algorithms for Images and Videos", "abstract": "We present an efficient combination strategy for color constancy algorithms. We define a compact neural network architecture to process and combine the illuminant estimations of individual algorithms, that may be based on different assumptions over the input scene content. Our solution can be specialized to the image domain, thus expecting a single frame input, and to the video domain, exploiting a Long Short-Term Memory module (LSTM) to handle varying-length sequences. To prove the effectiveness of our combining method we limit ourselves to combine only learning-free color constancy algorithms based on simple image statistics. We experiment on the standard Shi-Gehler and NUS datasets for still images, and on the recent Burst Color Constancy dataset for videos. Experimental results show that our method outperforms other combination strategies, and reaches an illuminant estimation accuracy comparable to more sophisticated and computationally-demanding solutions when the standard dataset split is used. Furthermore, our solution is proven to be effective even when the number of training instances available is reduced. As a further analysis, we assess the individual contribution of each underlying method towards the final illuminant estimation."}}
{"id": "h_UuVmKf35", "cdate": 1640995200000, "mdate": 1682325737152, "content": {"title": "A comparison of temporal aggregators for speaker verification", "abstract": "Speaker verification is the task of examining a speech signal to authenticate the claimed identity of a speaker as true or false. In order to deal with utterances having different lengths, and to accumulate information along the time dimension, different temporal aggregators have been proposed inside speaker verification pipelines. In this paper we investigate the behavior of five different temporal aggregators in the state of art, namely Temporal Average Pooling (TAP), Global Statistical Pooling (GSP), Self-Attentive Pooling (SAP), Attentive Statistical Pooling (ASP), and Vector of Locally Aggregated Descriptors (VLAD) at varying lengths of the two utterances. Starting from a speaker verification method in the state of the art, the experimental results on the VoxCeleb2 dataset show that there is a sweet spot for utterance length where speaker verification performance is higher independently from the temporal aggregator used."}}
{"id": "hJRkm9EDp2B", "cdate": 1640995200000, "mdate": 1668071279907, "content": {"title": "NTIRE 2022 Challenge on Night Photography Rendering", "abstract": "This paper reviews the NTIRE 2022 challenge on night photography rendering. The challenge solicited solutions that processed RAW camera images captured in night scenes to produce a photo-finished output image encoded in the standard RGB (sRGB) space. Given the subjective nature of this task, the proposed solutions were evaluated based on the mean opinions of viewers asked to judge the visual appearance of the results. Michael Freeman, a world-renowned photographer, further ranked the solutions with the highest mean opinion scores. A total of 13 teams competed in the final phase of the challenge. The proposed methods provided by the participating teams represent state-of-the-art performance in nighttime photography. Results from the various teams can be found here: https://nightimaging.org/"}}
