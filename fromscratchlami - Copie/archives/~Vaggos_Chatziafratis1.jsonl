{"id": "zkIm6n5P_aU", "cdate": 1681852129383, "mdate": 1681852129383, "content": {"title": "Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem", "abstract": "Given a target function $f$, how large must a neural network be in order to approximate $f$? Recent works examine this basic question on neural network \\textit{expressivity} from the lens of dynamical systems and provide novel \"depth-vs-width\" tradeoffs for a large family of functions $f$. They suggest that such tradeoffs are governed by the existence of \\textit{periodic} points or \\emph{cycles} in $f$. Our work, by further deploying dynamical systems concepts, illuminates a more subtle connection between periodicity and expressivity: we prove that periodic points alone lead to suboptimal depth-width tradeoffs and we improve upon them by demonstrating that certain \"chaotic itineraries\" give stronger exponential tradeoffs, even in regimes where previous analyses only imply polynomial gaps. Contrary to prior works, our bounds are nearly-optimal, tighten as the period increases, and handle strong notions of inapproximability (e.g., constant $L_1$ error). More broadly, we identify a phase transition to the \\textit{chaotic regime} that exactly coincides with an abrupt shift in other notions of function complexity, including VC-dimension and topological entropy."}}
{"id": "mjzm6btqgV", "cdate": 1663850190275, "mdate": null, "content": {"title": "Efficiently Computing Nash Equilibria in Adversarial Team Markov Games", "abstract": "    Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, in light of computational intractability barriers in general-sum games, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications.\n    \n    In this work, we depart from those prior results by investigating infinite-horizon \\emph{adversarial team Markov games}, a natural and well-motivated class of games in which a team of identically-interested players---in the absence of any explicit coordination or communication---is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary $\\epsilon$-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as $1/\\epsilon$.\n    \n    The proposed algorithm is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers."}}
{"id": "k5idxiVdJ3p", "cdate": 1652737594679, "mdate": null, "content": {"title": "On Scrambling Phenomena for Randomly Initialized Recurrent Networks ", "abstract": "Recurrent Neural Networks (RNNs) frequently exhibit complicated dynamics, and their sensitivity to the initialization process often renders them notoriously hard to train. Recent works have shed light on such phenomena analyzing when exploding or vanishing gradients may occur, either of which is detrimental for training dynamics. In this paper, we point to a formal connection between RNNs and chaotic dynamical systems and prove a qualitatively stronger phenomenon about RNNs than what exploding gradients seem to suggest. Our main result proves that under standard initialization (e.g., He, Xavier etc.), RNNs will exhibit \\textit{Li-Yorke chaos} with \\textit{constant} probability \\textit{independent} of the network's width. This explains the experimentally observed phenomenon of \\textit{scrambling}, under which trajectories of nearby points may appear to be arbitrarily close during some timesteps, yet will be far away in future timesteps. In stark contrast to their feedforward counterparts, we show that chaotic behavior in RNNs is preserved under small perturbations and that their expressive power remains exponential in the number of feedback iterations. Our technical arguments rely on viewing RNNs as random walks under non-linear activations, and studying the existence of certain types of higher-order fixed points called \\textit{periodic points} in order to establish phase transitions from order to chaos."}}
{"id": "BJe55gBtvH", "cdate": 1569439890190, "mdate": null, "content": {"title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions."}}
{"id": "rJba7nZdbH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Hierarchical Clustering with Structural Constraints", "abstract": "Hierarchical clustering is a popular unsupervised data analysis method. For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the c..."}}
