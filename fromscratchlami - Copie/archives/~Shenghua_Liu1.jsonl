{"id": "5ucJaaq8ixd", "cdate": 1675005858336, "mdate": 1675005858336, "content": {"title": "CatchCore: Catching Hierarchical Dense Subtensor", "abstract": " Dense subtensor detection gains remarkable success in spotting anomaly and fraudulent behaviors for the multi-aspect data (i.e., tensors), like in social media and event streams. Existing methods detect the densest subtensors\nflatly and separately, with an underlying assumption that those subtensors are\nexclusive. However, many real-world tensors usually present hierarchical properties, e.g., the core-periphery structure or dynamic communities in networks. In\nthis paper, we propose CatchCore, a novel framework to effectively find the\nhierarchical dense subtensors. We first design a unified metric for dense subtensor detection, which can be optimized with gradient-based methods. With the\nproposed metric, CatchCore detects hierarchical dense subtensors through the\nhierarchy-wise alternative optimization. Finally, we utilize the minimum description length principle to measure the quality of detection result and select the\noptimal hierarchical dense subtensors. Extensive experiments on synthetic and\nreal-world datasets demonstrate that CatchCore outperforms the top competitors in accuracy for detecting dense subtensors and anomaly patterns. Additionally, CatchCore successfully identified a hierarchical researcher co-authorship\ngroup with intense interactions in DBLP dataset. Meanwhile, CatchCore also\nscales linearly with all aspects of tensors."}}
{"id": "IfKYm2b41a", "cdate": 1675005794843, "mdate": 1675005794843, "content": {"title": "FlowScope: Spotting Money Laundering Based on Graphs", "abstract": "Given a graph of the money transfers between accounts of\na bank, how can we detect money laundering? Money laundering refers to criminals using the bank\u2019s services to move\nmassive amounts of illegal money to untraceable destination\naccounts, in order to inject their illegal money into the legitimate financial system. Existing graph fraud detection approaches focus on dense subgraph detection, without considering the fact that money laundering involves high-volume\nflows of funds through chains of bank accounts, thereby\ndecreasing their detection accuracy. Instead, we propose to\nmodel the transactions using a multipartite graph, and detect the complete flow of money from source to destination\nusing a scalable algorithm, FlowScope. Theoretical analysis\nshows that FlowScope provides guarantees in terms of the\namount of money that fraudsters can transfer without being\ndetected. FlowScope outperforms state-of-the-art baselines in\naccurately detecting the accounts involved in money laundering, in both injected and real-world data settings."}}
{"id": "-DJduBphgMo", "cdate": 1675005716280, "mdate": 1675005716280, "content": {"title": "EagleMine: Vision-guided Micro-clusters recognition and collective anomaly detection", "abstract": "Given a graph with millions of nodes, what patterns exist in the distributions of node characteristics?\nHow can we detect them and separate anomalous nodes in a way similar to human visual perception?\nMore generally, how can we identify micro-clusters in a histogram and spot some interesting patterns?\nIn this paper, we propose a vision-guided algorithm, EagleMine, to recognize and summarize node\ngroups in a histogram constructed from some correlated features. EagleMine hierarchically discovers\nnode groups, which form internally connected dense areas in the histogram, by utilizing a waterlevel tree with multiple resolutions according to the rule of the visual recognition. EagleMine uses\nthe statistical hypothesis test to determine the optimal groups while exploring the tree and simultaneously performs vocabulary-based summarization. Moreover, EagleMine can identify anomalous\nmicro-clusters, consisting of nodes that exhibit very similar and suspicious behavior, deviate away from\nthe majority. Experiments on the real-world datasets show that our method can recognize intuitive\nnode groups as human vision does; it achieves the best summarization performance compared to\nbaselines. In terms of anomaly detection, EagleMine also outperforms the state-of-the-art graph-based\nmethods with significantly improving accuracy in a micro-blog dataset. Moreover, EagleMine can be\nused for other applications, e.g., to detect the synchronized patterns in the temporal retweet event."}}
{"id": "90oH-F78-7", "cdate": 1675005568707, "mdate": 1675005568707, "content": {"title": "Provably Robust Node Classification via Low-Pass Message Passing", "abstract": "Graph Convolutional Networks (GCNs) have\nachieved state-of-the-art performance on node classification.\nHowever, recent works have shown that GCNs are vulnerable to adversarial attacks, such as additions or deletions of\nadversarially-chosen edges in the graph, in order to mislead\nthe node classification algorithms. How can we design robust\nGCNs that are resistant to such adversarial attacks? More\nchallengingly, how can we do this in a way that is provably\nrobust? We propose a robust node classification approach based\non a low-pass \u2018message passing\u2019 mechanism, that (a) reduces\nthe effectiveness of adversarial attacks in experiments, and\n(b) provides theoretical guarantees against adversarial attacks.\nOur approach can be embedded into the existing GCN architectures to enhance their robustness. Empirical results show that\nour loss-pass method effectively improves the performance of\nmultiple GCNs under miscellaneous perturbations and helps\nthem to achieve superior performance on various graphs."}}
{"id": "6YbmQy3JNI", "cdate": 1675005541154, "mdate": 1675005541154, "content": {"title": "DPGS: Degree-Preserving Graph Summarization", "abstract": "Given a large graph, how can we summarize it with fewer\nnodes and edges while maintaining its key properties, e.g.\nnode degrees and graph spectrum? As a solution, graph\nsummarization, which aims to find the compact representation for optimally describing and reconstructing a given\ngraph, has received much attention, and numerous methods\nhave been developed for it. However, many existing methods\nadopt the uniform reconstruction scheme, which is an unrealistic assumption as most real-world graphs have highly\nskewed node degrees, even within communities. Therefore\nwe propose a degree-preserving graph summarization model,\nDPGS, with a novel reconstruction scheme based on the\nconfiguration model. To optimize the Minimum Description\nLength of our model, we deisgn a linearly scalable algorithm\nusing hashing techniques. We theoretically show that the\nminimized reconstruction error bounds the perturbation of\ngraph spectral information. Extensive experiments on realworld datasets show that DPGS yields more accurate summary graphs than several well-known baselines. Moreover,\nour reduced summary graphs can effectively train graph neural networks (GNNs) while saving computational cost."}}
{"id": "6K-beK8xuBS", "cdate": 1675005462466, "mdate": 1675005462466, "content": {"title": "SPECGREEDY: Unified Dense Subgraph Detection", "abstract": "How can we effectively detect fake reviews or fraudulent connections on a website? How can we spot communities that suddenly appear based on users\u2019 interaction? And how can we efficiently find the minimum cut in a big graph? All of these are related to the problem of finding dense subgraphs, an important primitive problem in graph data analysis with extensive applications across various domains.\n\nWe focus on formulating the problem of detecting the densest subgraph in real-world large graphs, and we theoretically compare and contrast several closely related problems. Moreover, we propose a unified framework for the densest subgraph detection (GENDS) and devise a simple and computationally efficient algorithm, SPECGREEDY, to solve it by leveraging the graph spectral properties with a greedy approach. We conduct thorough experiments on 40 real-world networks with up to 1.47 billion edges from various domains, and demonstrate that our algorithm yields up to 58.6\u00d7\n speedup and achieves better or approximately equal-quality solutions for the densest subgraph detection compared to the baselines. Moreover, SPECGREEDY scales linearly with the graph size and is proved effective in applications, such as finding collaborations that appear suddenly in a big, time-evolving co-authorship network."}}
{"id": "Y4nLJq3G7xv", "cdate": 1675005421361, "mdate": 1675005421361, "content": {"title": "Time Series Anomaly Detection with Adversarial Reconstruction Networks", "abstract": "Time series data naturally exist in many domains including medical data analysis, infrastructure sensor monitoring, and\nmotion tracking. However, a very small portion of anomalous time series can be observed, comparing to the whole data. Most existing\napproaches are based on the supervised classification model requiring representative labels for anomaly class(es), which is\nchallenging in real-world problems. So can we learn how to detect anomalous time ticks in an effective yet efficient way, given mostly\nnormal time series data? Therefore, we propose an unsupervised reconstruction model named BeatGAN which learns to detect\nanomalies based on normal data, or data which majority of samples are normal. BeatGAN provides a framework to adversarially learn\nto reconstruct, which can cooperate with both 1-d CNN and RNN. Rarely observed anomalies can result in larger reconstruction errors,\nwhich are then detected based on extreme value theory. Moreover, data augmentation with dynamic time warping regularizes\nreconstruction and provides robustness. In the experiments, effectiveness and sensitivity are studied in both synthetic data and various\nreal-world time series. BeatGAN achieves better accuracy and fast inference."}}
{"id": "giG9GzhnXRr", "cdate": 1675005061719, "mdate": 1675005061719, "content": {"title": "MonLAD: Money Laundering Agents Detection in Transaction Streams", "abstract": "Given a stream of money transactions between accounts in a bank,\nhow can we accurately detect money laundering agent accounts\nand suspected behaviors in real-time? Money laundering agents\ntry to hide the origin of illegally obtained money by dispersive\nmultiple small transactions and evade detection by smart strategies. Therefore, it is challenging to accurately catch such fraudsters\nin an unsupervised manner. Existing approaches do not consider\nthe characteristics of those agent accounts and are not suitable\nto the streaming settings. Therefore, we propose MonLAD and\nMonLAD-W to detect money laundering agent accounts in a transaction stream by keeping track of their residuals and other features;\nwe devise AnoScore algorithm to! nd anomalies based on the\nrobust measure of statistical deviation. Experimental results show\nthat MonLAD outperforms the state-of-the-art baselines on realworld data and! nds various suspicious behavior patterns of money\nlaundering. Additionally, several detected suspected accounts have\nbeen manually-veri!ed as agents in real money laundering scenario."}}
{"id": "B1-yaVfuZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "NeuCast: Seasonal Neural Forecast of Power Grid Time Series", "abstract": "In the smart power grid, short-term load forecasting (STLF) is a crucial step in scheduling and planning for future load, so as to improve the reliability, cost, and emissions of the power grid. Different from traditional time series forecast, STLF is a more challenging task, because of the complex demand of active and reactive power from numerous categories of electrical loads and the effects of environment. Therefore, we propose NeuCast, a seasonal neural forecasting method, which dynamically models various loads as co-evolving time series in a hidden space, as well as extra weather conditions, in a neural network structure. NeuCast captures seasonality and patterns of the time series by integrating factor modeling and hidden state recognition. NeuCast can also detect anomalies and forecast under different temperature assumptions. Extensive experiments on 134 real-word datasets show the improvements of NeuCast over the stateof-the-art methods."}}
{"id": "HkN-9NfubH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Single-Pass PCA of Large High-Dimensional Data", "abstract": "Principal component analysis (PCA) is a fundamental dimension reduction tool in statistics and machine learning. For large and high-dimensional data, computing the PCA (i.e., the top singular vectors of the data matrix) becomes a challenging task. In this work, a single-pass randomized algorithm is proposed to compute PCA with only one pass over the data. It is suitable for processing extremely large and high-dimensional data stored in slow memory (hard disk) or the data generated in a streaming fashion. Experiments with synthetic and real data validate the algorithm's accuracy, which has orders of magnitude smaller error than an existing single-pass algorithm. For a set of high-dimensional data stored as a 150 GB file, the algorithm is able to compute the first 50 principal components in just 24 minutes on a typical 24-core computer, with less than 1 GB memory cost."}}
