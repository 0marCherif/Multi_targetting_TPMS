{"id": "yyTNV4CcIR9", "cdate": 1663849962519, "mdate": null, "content": {"title": "A Unimodal, Uncertainty-Aware Deep Learning Approach for Ordinal Regression", "abstract": "Ordinal regression is an important area in machine learning, and many algorithms were proposed to approach it. In this work, we propose an ordinal regression prediction algorithm, based on deep learning machinery and inspired by the well-known Proportional Odds model. Our proposed approach has three key components: first, it is designed to guarantee unimodal output probabilities, which is a desired element in many real world applications. Second, we argue that the standard maximum likelihood is sub-optimal for ordinal regression problems and train our model using optimal transport loss, as it naturally captures the order of the classes. Third, we design a novel regularizer aiming to make the model uncertainty-aware, in the sense of making the model more confident about correct predictions, comparing to wrong predictions. In addition, we propose a novel uncertainty-awareness evaluation measure. Experimental results on eight real-world datasets\ndemonstrate that our proposed approach consistently performs on par with and often better than several recently proposed deep learning approaches for ordinal regression, in terms of both accuracy and uncertainty-awareness, while having a guarantee on the output unimodality."}}
{"id": "Owz3dDKM32p", "cdate": 1652737527165, "mdate": null, "content": {"title": "Discovery of Single Independent Latent Variable", "abstract": "Latent variable discovery is a central problem in data analysis with a broad range of applications in applied science.\nIn this work, we consider data given as an invertible mixture of two statistically independent components, and assume that one of the components is observed while the other is hidden. Our goal is to recover the hidden component.\nFor this purpose, we propose an autoencoder equipped with a discriminator.\nUnlike the standard nonlinear ICA problem, which was shown to be non-identifiable, in the  special case of ICA we consider here, we show that our approach can recover the component of interest up to entropy-preserving transformation.\nWe demonstrate the performance of the proposed approach in several tasks, including image synthesis, voice cloning, and fetal ECG extraction. "}}
{"id": "p1_pv4uV3pq", "cdate": 1640995200000, "mdate": 1682610118350, "content": {"title": "SG-VAD: Stochastic Gates Based Speech Activity Detection", "abstract": "We propose a novel voice activity detection (VAD) model in a low-resource environment. Our key idea is to model VAD as a denoising task, and construct a network that is designed to identify nuisance features for a speech classification task. We train the model to simultaneously identify irrelevant features while predicting the type of speech event. Our model contains only 7.8K parameters, outperforms the previously proposed methods on the AVA-Speech evaluation set, and provides comparative results on the HAVIC dataset. We present its architecture, experimental results, and ablation study on the model's components. We publish the code and the models here https://www.github.com/jsvir/vad."}}
{"id": "OFmZOETPZ2_", "cdate": 1640995200000, "mdate": 1682610118340, "content": {"title": "Deep unsupervised feature selection by discarding nuisance and correlated features", "abstract": ""}}
{"id": "OUH25e12YyH", "cdate": 1621630215630, "mdate": null, "content": {"title": "Differentiable Unsupervised Feature Selection based on a Gated Laplacian", "abstract": "Scientific observations may consist of a large number of variables (features). Selecting a subset of meaningful features is often crucial for identifying patterns hidden in the ambient space. In this paper, we present a method for unsupervised feature selection, and we demonstrate its advantage in clustering, a common unsupervised task. We propose a differentiable loss that combines a graph Laplacian-based score that favors low-frequency features with a gating mechanism for removing nuisance features. Our method improves upon the naive graph Laplacian score by replacing it with a gated variant computed on a subset of low-frequency features. We identify this subset by learning the parameters of continuously relaxed Bernoulli variables, which gate the entire feature space. We mathematically motivate the proposed approach and demonstrate that it is crucial to compute the graph Laplacian on the gated inputs rather than on the full feature set in the high noise regime. Using several real-world examples, we demonstrate the efficacy and advantage of the proposed approach over leading baselines."}}
{"id": "ZU_3Z3DDuBl", "cdate": 1609459200000, "mdate": 1649186847651, "content": {"title": "Single Independent Component Recovery and Applications", "abstract": "Latent variable discovery is a central problem in data analysis with a broad range of applications in applied science. In this work, we consider data given as an invertible mixture of two statistically independent components, and assume that one of the components is observed while the other is hidden. Our goal is to recover the hidden component. For this purpose, we propose an autoencoder equipped with a discriminator. Unlike the standard nonlinear ICA problem, which was shown to be non-identifiable, in the special case of ICA we consider here, we show that our approach can recover the component of interest up to entropy-preserving transformation. We demonstrate the performance of the proposed approach on several datasets, including image synthesis, voice cloning, and fetal ECG extraction."}}
{"id": "UWFGPJaqXeV", "cdate": 1609459200000, "mdate": 1649186847661, "content": {"title": "Deep Unsupervised Feature Selection by Discarding Nuisance and Correlated Features", "abstract": "Modern datasets often contain large subsets of correlated features and nuisance features, which are not or loosely related to the main underlying structures of the data. Nuisance features can be identified using the Laplacian score criterion, which evaluates the importance of a given feature via its consistency with the Graph Laplacians' leading eigenvectors. We demonstrate that in the presence of large numbers of nuisance features, the Laplacian must be computed on the subset of selected features rather than on the complete feature set. To do this, we propose a fully differentiable approach for unsupervised feature selection, utilizing the Laplacian score criterion to avoid the selection of nuisance features. We employ an autoencoder architecture to cope with correlated features, trained to reconstruct the data from the subset of selected features. Building on the recently proposed concrete layer that allows controlling for the number of selected features via architectural design, simplifying the optimization process. Experimenting on several real-world datasets, we demonstrate that our proposed approach outperforms similar approaches designed to avoid only correlated or nuisance features, but not both. Several state-of-the-art clustering results are reported."}}
{"id": "LtOpEUqyi2K", "cdate": 1609459200000, "mdate": 1682610118488, "content": {"title": "Differentiable Unsupervised Feature Selection based on a Gated Laplacian", "abstract": "Scientific observations may consist of a large number of variables (features). Selecting a subset of meaningful features is often crucial for identifying patterns hidden in the ambient space. In this paper, we present a method for unsupervised feature selection, and we demonstrate its advantage in clustering, a common unsupervised task. We propose a differentiable loss that combines a graph Laplacian-based score that favors low-frequency features with a gating mechanism for removing nuisance features. Our method improves upon the naive graph Laplacian score by replacing it with a gated variant computed on a subset of low-frequency features. We identify this subset by learning the parameters of continuously relaxed Bernoulli variables, which gate the entire feature space. We mathematically motivate the proposed approach and demonstrate that it is crucial to compute the graph Laplacian on the gated inputs rather than on the full feature set in the high noise regime. Using several real-world examples, we demonstrate the efficacy and advantage of the proposed approach over leading baselines."}}
{"id": "TnuYH8VKFo5", "cdate": 1577836800000, "mdate": 1649186847636, "content": {"title": "Let the Data Choose its Features: Differentiable Unsupervised Feature Selection", "abstract": "Scientific observations may consist of a large number of variables (features). Identifying a subset of meaningful features is often ignored in unsupervised learning, despite its potential for unraveling clear patterns hidden in the ambient space. In this paper, we present a method for unsupervised feature selection, and we demonstrate its use for the task of clustering. We propose a differentiable loss function that combines the Laplacian score, which favors low-frequency features, with a gating mechanism for feature selection. We improve the Laplacian score, by replacing it with a gated variant computed on a subset of features. This subset is obtained using a continuous approximation of Bernoulli variables whose parameters are trained to gate the full feature space. We mathematically motivate the proposed approach and demonstrate that in the high noise regime, it is crucial to compute the Laplacian on the gated inputs, rather than on the full feature set. Experimental demonstration of the efficacy of the proposed approach and its advantage over current baselines is provided using several real-world examples."}}
