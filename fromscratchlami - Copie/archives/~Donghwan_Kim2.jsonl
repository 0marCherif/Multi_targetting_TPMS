{"id": "U7vVeHydyR", "cdate": 1621629836393, "mdate": null, "content": {"title": "Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems", "abstract": "Modern minimax problems, such as generative adversarial network and adversarial training, are often under a nonconvex-nonconcave setting, and developing an efficient method for such setting is of interest. Recently, two variants of the extragradient (EG) method are studied in that direction. First, a two-time-scale variant of the EG, named EG+, was proposed under a smooth structured nonconvex-nonconcave setting, with a slow $\\mathcal{O}(1/k)$ rate on the squared gradient norm, where $k$ denotes the number of iterations. Second, another variant of EG with an anchoring technique, named extra anchored gradient (EAG), was studied under a smooth convex-concave setting, yielding a fast $\\mathcal{O}(1/k^2)$ rate on the squared gradient norm. Built upon EG+ and EAG, this paper proposes a two-time-scale EG with anchoring, named fast extragradient (FEG), that has a fast $\\mathcal{O}(1/k^2)$ rate on the squared gradient norm for smooth structured nonconvex-nonconcave problems; the corresponding saddle-gradient operator satisfies the negative comonotonicity condition. This paper further develops its backtracking line-search version, named FEG-A, for the case where the problem parameters are not available. The stochastic analysis of FEG is also provided."}}
{"id": "AYAgKFl78z", "cdate": 1621629836393, "mdate": null, "content": {"title": "Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems", "abstract": "Modern minimax problems, such as generative adversarial network and adversarial training, are often under a nonconvex-nonconcave setting, and developing an efficient method for such setting is of interest. Recently, two variants of the extragradient (EG) method are studied in that direction. First, a two-time-scale variant of the EG, named EG+, was proposed under a smooth structured nonconvex-nonconcave setting, with a slow $\\mathcal{O}(1/k)$ rate on the squared gradient norm, where $k$ denotes the number of iterations. Second, another variant of EG with an anchoring technique, named extra anchored gradient (EAG), was studied under a smooth convex-concave setting, yielding a fast $\\mathcal{O}(1/k^2)$ rate on the squared gradient norm. Built upon EG+ and EAG, this paper proposes a two-time-scale EG with anchoring, named fast extragradient (FEG), that has a fast $\\mathcal{O}(1/k^2)$ rate on the squared gradient norm for smooth structured nonconvex-nonconcave problems; the corresponding saddle-gradient operator satisfies the negative comonotonicity condition. This paper further develops its backtracking line-search version, named FEG-A, for the case where the problem parameters are not available. The stochastic analysis of FEG is also provided."}}
{"id": "P7Q44Mq6wLz", "cdate": 1609459200000, "mdate": 1682688419208, "content": {"title": "Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems", "abstract": "Modern minimax problems, such as generative adversarial network and adversarial training, are often under a nonconvex-nonconcave setting, and developing an efficient method for such setting is of interest. Recently, two variants of the extragradient (EG) method are studied in that direction. First, a two-time-scale variant of the EG, named EG+, was proposed under a smooth structured nonconvex-nonconcave setting, with a slow $\\mathcal{O}(1/k)$ rate on the squared gradient norm, where $k$ denotes the number of iterations. Second, another variant of EG with an anchoring technique, named extra anchored gradient (EAG), was studied under a smooth convex-concave setting, yielding a fast $\\mathcal{O}(1/k^2)$ rate on the squared gradient norm. Built upon EG+ and EAG, this paper proposes a two-time-scale EG with anchoring, named fast extragradient (FEG), that has a fast $\\mathcal{O}(1/k^2)$ rate on the squared gradient norm for smooth structured nonconvex-nonconcave problems; the corresponding saddle-gradient operator satisfies the negative comonotonicity condition. This paper further develops its backtracking line-search version, named FEG-A, for the case where the problem parameters are not available. The stochastic analysis of FEG is also provided."}}
{"id": "E73N7CrRwjx", "cdate": 1609459200000, "mdate": 1682688419204, "content": {"title": "Optimizing the Efficiency of First-Order Methods for Decreasing the Gradient of Smooth Convex Functions", "abstract": "This paper optimizes the step coefficients of first-order methods for smooth convex minimization in terms of the worst-case convergence bound (i.e., efficiency) of the decrease in the gradient norm. This work is based on the performance estimation problem approach. The worst-case gradient bound of the resulting method is optimal up to a constant for large-dimensional smooth convex minimization problems, under the initial bounded condition on the cost function value. This paper then illustrates that the proposed method has a computationally efficient form that is similar to the optimized gradient method."}}
{"id": "CHZIe-gtM5", "cdate": 1609459200000, "mdate": 1682688419272, "content": {"title": "Accelerated proximal point method for maximally monotone operators", "abstract": "This paper proposes an accelerated proximal point method for maximally monotone operators. The proof is computer-assisted via the performance estimation problem approach. The proximal point method includes various well-known convex optimization methods, such as the proximal method of multipliers and the alternating direction method of multipliers, and thus the proposed acceleration has wide applications. Numerical experiments are presented to demonstrate the accelerating behaviors."}}
{"id": "siiadbjlgQQ", "cdate": 1514764800000, "mdate": null, "content": {"title": "Another Look at the Fast Iterative Shrinkage/Thresholding Algorithm (FISTA)", "abstract": "This paper provides a new way of developing the fast iterative shrinkage/thresh-olding algorithm (FISTA) [A. Beck and M. Teboulle, SIAM J. Imaging Sci., 2 (2009), pp. 183--202] that is widely used for minimizing composite convex functions with a nonsmooth term such as the $\\ell_1$ regularizer. In particular, this paper shows that FISTA corresponds to an optimized approach to accelerating the proximal gradient method with respect to a worst-case bound of the cost function. This paper then proposes a new algorithm that is derived by instead optimizing the step coefficients of the proximal gradient method with respect to a worst-case bound of the composite gradient mapping. The proof is based on the worst-case analysis called the performance estimation problem in [Y. Drori and M. Teboulle, Math. Program., 145 (2014), pp. 451--482]."}}
{"id": "DYuabvA1HCB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Adaptive Restart of the Optimized Gradient Method for Convex Optimization", "abstract": "First-order methods with momentum, such as Nesterov\u2019s fast gradient method, are very useful for convex optimization problems, but can exhibit undesirable oscillations yielding slow convergence rates for some applications. An adaptive restarting scheme can improve the convergence rate of the fast gradient method, when the parameter of a strongly convex cost function is unknown or when the iterates of the algorithm enter a locally strongly convex region. Recently, we introduced the optimized gradient method, a first-order algorithm that has an inexpensive per-iteration computational cost similar to that of the fast gradient method, yet has a worst-case cost function rate that is twice faster than that of the fast gradient method and that is optimal for large-dimensional smooth convex problems. Building upon the success of accelerating the fast gradient method using adaptive restart, this paper investigates similar heuristic acceleration of the optimized gradient method. We first derive a new first-order method that resembles the optimized gradient method for strongly convex quadratic problems with known function parameters, yielding a linear convergence rate that is faster than that of the analogous version of the fast gradient method. We then provide a heuristic analysis and numerical experiments that illustrate that adaptive restart can accelerate the convergence of the optimized gradient method. Numerical results also illustrate that adaptive restart is helpful for a proximal version of the optimized gradient method for nonsmooth composite convex functions."}}
{"id": "-uPBsGbqk-z", "cdate": 1514764800000, "mdate": null, "content": {"title": "Generalizing the Optimized Gradient Method for Smooth Convex Minimization", "abstract": "This paper generalizes the optimized gradient method (OGM) [Y. Drori and M. Teboulle, Math. Program., 145 (2014), pp. 451--482], [D. Kim and J. A. Fessler, Math. Program., 159 (2016), pp. 81--107], [D. Kim and J. A. Fessler, J. Optim. Theory Appl., 172 (2017), pp. 187--205] that achieves the optimal worst-case cost function bound of first-order methods for smooth convex minimization [Y. Drori, J. Complexity, 39 (2017), pp. 1--16]. Specifically, this paper studies a generalized formulation of OGM and analyzes its worst-case rates in terms of both the function value and the norm of the function gradient. This paper also develops a new algorithm called OGM-OG that is in the generalized family of OGM and that has the best known analytical worst-case bound with rate $O(1/N^{1.5})$ on the decrease of the gradient norm among fixed-step first-order methods. This paper also proves that Nesterov's fast gradient method [Y. Nesterov, Dokl. Akad. Nauk. USSR, 269 (1983), pp. 543--547], [Y. Nesterov, Math. Program., 103 (2005), pp. 127--152] has an $O(1/N^{1.5})$ worst-case gradient norm rate but with constant larger than OGM-OG. The proof is based on the worst-case analysis called the Performance Estimation Problem in [Y. Drori and M. Teboulle, Math. Program., 145 (2014), pp. 451--482]."}}
{"id": "Jrb5zLFfXR", "cdate": 1483228800000, "mdate": null, "content": {"title": "On the Convergence Analysis of the Optimized Gradient Method", "abstract": "This paper considers the problem of unconstrained minimization of smooth convex functions having Lipschitz continuous gradients with known Lipschitz constant. We recently proposed the optimized gradient method for this problem and showed that it has a worst-case convergence bound for the cost function decrease that is twice as small as that of Nesterov\u2019s fast gradient method, yet has a similarly efficient practical implementation. Drori showed recently that the optimized gradient method has optimal complexity for the cost function decrease over the general class of first-order methods. This optimality makes it important to study fully the convergence properties of the optimized gradient method. The previous worst-case convergence bound for the optimized gradient method was derived for only the last iterate of a secondary sequence. This paper provides an analytic convergence bound for the primary sequence generated by the optimized gradient method. We then discuss additional convergence properties of the optimized gradient method, including the interesting fact that the optimized gradient method has two types of worst-case functions: a piecewise affine-quadratic function and a quadratic function. These results help complete the theory of an optimal first-order method for smooth convex minimization."}}
{"id": "Dix1oJUksWN", "cdate": 1483228800000, "mdate": null, "content": {"title": "Accelerated dual gradient-based methods for total variation image denoising/deblurring problems", "abstract": "We study accelerated dual gradient-based methods for image denoising/deblurring problems based on the total variation (TV) model. For the TV-based denoising problem, combining the dual approach and Nesterov's fast gradient projection (FGP) method has been found effective. The corresponding denoising method minimizes the dual function with FGP's optimal rate O(1/k <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> ) where k denotes the number of iterations, and guarantees a rate O(1/k) for the primal function decrease. Considering that the dual projected gradient decrease is closely related to the primal function decrease, this paper proposes new accelerated gradient projection methods that decrease the projected gradient norm with a fast rate O(1/k <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1.5</sup> ) and that are as efficient as FGP. The proposed approach also decreases the primal function with a faster rate O(1/k <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1.5</sup> ). We provide preliminary results on image denoising/deblurring problems with a TV regularizer, where the fast and efficient denoising solver is iteratively used for solving a deblurring problem as the inner proximal update of a fast iterative shrinkage/thresholding algorithm (FISTA)."}}
