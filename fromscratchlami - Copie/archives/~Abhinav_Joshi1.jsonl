{"id": "9nzg6ptyzlW", "cdate": 1671985247362, "mdate": 1671985247362, "content": {"title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN", "abstract": "Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person\u2019s emotions are influenced by the other speaker\u2019s utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi- modal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the- art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels."}}
{"id": "qSERY7CWW7", "cdate": 1670998518602, "mdate": 1670998518602, "content": {"title": "CISLR: Corpus for Indian Sign Language Recognition", "abstract": "Indian Sign Language, though used by a diverse community, still lacks well-annotated resources for developing systems that would enable sign language processing. In recent years researchers have actively worked for sign languages like American Sign Languages, however, Indian Sign language is still far from data-driven tasks like machine translation. To address this gap, in this paper, we introduce a new dataset CISLR (Corpus for Indian Sign Language Recognition) for word-level recognition in Indian Sign Language using videos. The corpus has a large vocabulary of around 4700 words covering different topics and domains. Further, we propose a baseline model for word recognition from sign language videos. To handle the low resource problem in the Indian Sign Language, the proposed model consists of a prototype-based one-shot learner that leverages resource rich American Sign Language to learn generalized features for improving predictions in Indian Sign Language. Our experiments show that gesture features learned in another sign language can help perform one-shot predictions in CISLR. "}}
{"id": "yMHzGXgcQeg", "cdate": 1664358386002, "mdate": null, "content": {"title": "ScriptWorld: A Scripts-based RL Environment", "abstract": "Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning algorithms. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: A text-based environment for teaching agents about real-world daily chores, imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that considers data written by humans (scripts datasets) to create procedural games for daily real-world human activities. We provide gaming environments for 10 daily activities and perform a detailed analysis to capture the richness of the proposed environment. We also test the developed environment using human gameplay experiments and reinforcement learning algorithms as baselines. Our experiments show that the flexibility of the proposed environment makes it a suitable testbed for reinforcement learning algorithms to learn the underlying procedural knowledge in daily human chores."}}
{"id": "tpXEsOjYUHR", "cdate": 1640995200000, "mdate": 1681531963277, "content": {"title": "CISLR: Corpus for Indian Sign Language Recognition", "abstract": ""}}
{"id": "dJRzS0smfC", "cdate": 1640995200000, "mdate": 1681531963545, "content": {"title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN", "abstract": ""}}
{"id": "bbjarbvG7aJ", "cdate": 1640995200000, "mdate": 1681531963275, "content": {"title": "Generalized Product-of-Experts for Learning Multimodal Representations in Noisy Environments", "abstract": ""}}
{"id": "ULIUoUEMww", "cdate": 1640995200000, "mdate": 1681531963550, "content": {"title": "Generalized Product-of-Experts for Learning Multimodal Representations in Noisy Environments", "abstract": ""}}
{"id": "NMKMieAio48", "cdate": 1640995200000, "mdate": 1681531963276, "content": {"title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN", "abstract": ""}}
{"id": "MxdYfM6KPG", "cdate": 1640995200000, "mdate": 1681531963278, "content": {"title": "Multimodal Representation Learning For Real-World Applications", "abstract": ""}}
{"id": "l4_Y_9v1MKA", "cdate": 1609459200000, "mdate": 1640929662761, "content": {"title": "Shapes of Emotions: Multimodal Emotion Recognition in Conversations via Emotion Shifts", "abstract": "Emotion Recognition in Conversations (ERC) is an important and active research problem. Recent work has shown the benefits of using multiple modalities (e.g., text, audio, and video) for the ERC task. In a conversation, participants tend to maintain a particular emotional state unless some external stimuli evokes a change. There is a continuous ebb and flow of emotions in a conversation. Inspired by this observation, we propose a multimodal ERC model and augment it with an emotion-shift component. The proposed emotion-shift component is modular and can be added to any existing multimodal ERC model (with a few modifications), to improve emotion recognition. We experiment with different variants of the model, and results show that the inclusion of emotion shift signal helps the model to outperform existing multimodal models for ERC and hence showing the state-of-the-art performance on MOSEI and IEMOCAP datasets."}}
