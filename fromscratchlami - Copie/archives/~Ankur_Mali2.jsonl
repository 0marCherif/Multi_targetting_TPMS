{"id": "ccYOWWNa5v2", "cdate": 1652737568676, "mdate": null, "content": {"title": "Lifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting", "abstract": "In lifelong learning systems based on artificial neural networks, one of the biggest obstacles is the inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we propose a new kind of connectionist architecture, the Sequential Neural Coding Network, that is robust to forgetting when learning from streams of data points and, unlike networks of today, does not learn via the popular back-propagation of errors. Grounded in the neurocognitive theory of predictive coding, our model adapts its synapses in a biologically-plausible fashion while another neural system learns to direct and control this cortex-like structure, mimicking some of the task-executive control functionality of the basal ganglia. In our experiments, we demonstrate that our self-organizing system experiences significantly less forgetting compared to standard neural models, outperforming a swath of previously proposed methods, including rehearsal/data buffer-based methods, on both standard (SplitMNIST, Split Fashion MNIST, etc.) and custom benchmarks even though it is trained in a stream-like fashion. Our work offers evidence that emulating mechanisms in real neuronal systems, e.g., local learning, lateral competition, can yield new directions and possibilities for tackling the grand challenge of lifelong machine learning."}}
{"id": "Lw6DRegbar9", "cdate": 1598720100528, "mdate": null, "content": {"title": "A Neural Temporal Model for Human Motion Prediction", "abstract": "We propose novel neural temporal models for predicting and synthesizing human motion, achieving state-of-theart in modeling long-term motion trajectories while being\ncompetitive with prior work in short-term prediction and\nrequiring significantly less computation. Key aspects of our\nproposed system include: 1) a novel, two-level processing architecture that aids in generating planned trajectories, 2) a simple set of easily computable features that integrate derivative information, and 3) a novel multi-objective\nloss function that helps the model to slowly progress from\nsimple next-step prediction to the harder task of multistep, closed-loop prediction. Our results demonstrate that\nthese innovations improve the modeling of long-term motion trajectories. Finally, we propose a novel metric, called\nNormalized Power Spectrum Similarity (NPSS), to evaluate\nthe long-term predictive ability of motion synthesis models, complementing the popular mean-squared error (MSE)\nmeasure of Euler joint angles over time. We conduct a\nuser study to determine if the proposed NPSS correlates\nwith human evaluation of long-term motion more strongly\nthan MSE and find that it indeed does. We release code\nand additional results (visualizations) for this paper at:\nhttps://github.com/cr7anand/neural temporal models"}}
{"id": "pWnvsWlOH2w", "cdate": 1598719977144, "mdate": null, "content": {"title": "Biologically Motivated Algorithms for Propagating Local Target Representations.", "abstract": "Finding biologically plausible alternatives to back-propagation\nof errors is a fundamentally important challenge in artificial\nneural network research. In this paper, we propose a learning\nalgorithm called error-driven Local Representation Alignment\n(LRA-E), which has strong connections to predictive coding, a\ntheory that offers a mechanistic way of describing neurocomputational\nmachinery. In addition, we propose an improved\nvariant of Difference Target Propagation, another procedure\nthat comes from the same family of algorithms as LRA-E.\nWe compare our procedures to several other biologicallymotivated\nalgorithms, including two feedback alignment algorithms\nand Equilibrium Propagation. In two benchmarks, we\nfind that both of our proposed algorithms yield stable performance\nand strong generalization compared to other competing\nback-propagation alternatives when training deeper, highly\nnonlinear networks, with LRA-E performing the best overall."}}
{"id": "roBj9FuVT_go", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Sibling Neural Estimator: Improving Iterative Image Decoding with Gradient Communication", "abstract": "For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations. The system links two recurrent networks that \"help\" each other reconstruct the same target image patches using complementary portions of the spatial context, communicating with each other via gradient signals. This dual agent system builds upon prior work that proposed an iterative refinement algorithm for recurrent neural network (RNN) based decoding. Our approach works with any neural or non-neural encoder. Our system progressively reduces image patch reconstruction error over a fixed number of steps. Experiments with variations of RNN memory cells show that our system consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe gains of 1:64 decibel (dB) over JPEG, a 1:46 dB over JPEG2000, a 1:34 dB over the GOOG neural baseline, 0:36 over E2E (a modern competitive neural compression model), and 0:37 over a single iterative neural decoder."}}
{"id": "Nns1l-vI4tD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Provably Stable Interpretable Encodings of Context Free Grammars in RNNs with a Differentiable Stack", "abstract": "We introduce a neural stack architecture, including a differentiable parametrized stack operator that approximates stack push and pop operations for suitable choices of parameters that explicitly represents a stack. We prove the stability of this stack architecture: after arbitrarily many stack operations, the state of the neural stack still closely resembles the state of the discrete stack. Using the neural stack with a recurrent neural network, we introduce a neural network Pushdown Automaton (nnPDA) and prove that nnPDA with finite/bounded neurons and time can simulate any PDA. Furthermore, we extend our construction and propose new architecture neural state Turing Machine (nnTM). We prove that differentiable nnTM with bounded neurons can simulate Turing Machine (TM) in real-time. Just like the neural stack, these architectures are also stable. Finally, we extend our construction to show that differentiable nnTM is equivalent to Universal Turing Machine (UTM) and can simulate any TM with only \\textbf{seven finite/bounded precision} neurons. This work provides a new theoretical bound for the computational capability of bounded precision RNNs augmented with memory."}}
{"id": "Cx47xAfDoiH", "cdate": 1577836800000, "mdate": null, "content": {"title": "Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack", "abstract": "Recurrent neural networks (RNNs) are a widely used deep architecture for sequence modeling, generation, and prediction. Despite success in applications such as machine translation and voice recognition, these stateful models have several critical shortcomings. Specifically, RNNs generalize poorly over very long sequences, which limits their applicability to many important temporal processing and time series forecasting problems. For example, RNNs struggle in recognizing complex context free languages (CFLs), never reaching 100% accuracy on training. One way to address these shortcomings is to couple an RNN with an external, differentiable memory structure, such as a stack. However, differentiable memories in prior work have neither been extensively studied on CFLs nor tested on sequences longer than those seen in training. The few efforts that have studied them have shown that continuous differentiable memory structures yield poor generalization for complex CFLs, making the RNN less interpretable. In this paper, we improve the memory-augmented RNN with important architectural and state updating mechanisms that ensure that the model learns to properly balance the use of its latent states with external memory. Our improved RNN models exhibit better generalization performance and are able to classify long strings generated by complex hierarchical context free grammars (CFGs). We evaluate our models on CGGs, including the Dyck languages, as well as on the Penn Treebank language modelling task, and achieve stable, robust performance across these benchmarks. Furthermore, we show that only our memory-augmented networks are capable of retaining memory for a longer duration up to strings of length 160."}}
{"id": "7bTZgdmrdmN", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reducing the Computational Burden of Deep Learning with Recursive Local Representation Alignment", "abstract": "Training deep neural networks on large-scale datasets requires significant hardware resources whose costs (even on cloud platforms) put them out of reach of smaller organizations, groups, and individuals. Backpropagation, the workhorse for training these networks, is an inherently sequential process that is difficult to parallelize. Furthermore, it requires researchers to continually develop various tricks, such as specialized weight initializations and activation functions, in order to ensure a stable parameter optimization. Our goal is to seek an effective, neuro-biologically-plausible alternative to backprop that can be used to train deep networks. In this paper, we propose a gradient-free learning procedure, recursive local representation alignment, for training large-scale neural architectures. Experiments with residual networks on CIFAR-10 and the large benchmark, ImageNet, show that our algorithm generalizes as well as backprop while converging sooner due to weight updates that are parallelizable and computationally less demanding. This is empirical evidence that a backprop-free algorithm can scale up to larger datasets."}}
{"id": "nFfQT11zfyy", "cdate": 1546300800000, "mdate": null, "content": {"title": "Lifelong Neural Predictive Coding: Sparsity Yields Less Forgetting when Learning Cumulatively", "abstract": "In lifelong learning systems based on artificial neural networks, one of the biggest obstacles is the inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we propose a new kind of connectionist architecture, the Sequential Neural Coding Network, that is robust to forgetting when learning from streams of data points and, unlike networks of today, does not learn via the popular back-propagation of errors. Grounded in the neurocognitive theory of predictive processing, our model adapts synapses in a biologically-plausible fashion while another neural system learns to direct and control this cortex-like structure, mimicking some of the task-executive control functionality of the basal ganglia. In our experiments, we demonstrate that our self-organizing system experiences significantly less forgetting compared to standard neural models, outperforming a swath of previously proposed methods, including rehearsal/data buffer-based methods, on both standard (SplitMNIST, Split Fashion MNIST, etc.) and custom benchmarks even though it is trained in a stream-like fashion. Our work offers evidence that emulating mechanisms in real neuronal systems, e.g., local learning, lateral competition, can yield new directions and possibilities for tackling the grand challenge of lifelong machine learning."}}
{"id": "_F_Ytjghmy", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Neural State Pushdown Automata", "abstract": "In order to learn complex grammars, recurrent neural networks (RNNs) require sufficient computational resources to ensure correct grammar recognition. A widely-used approach to expand model capacity would be to couple an RNN to an external memory stack. Here, we introduce a \"neural state\" pushdown automaton (NSPDA), which consists of a digital stack, instead of an analog one, that is coupled to a neural network state machine. We empirically show its effectiveness in recognizing various context-free grammars (CFGs). First, we develop the underlying mechanics of the proposed higher order recurrent network and its manipulation of a stack as well as how to stably program its underlying pushdown automaton (PDA) to achieve desired finite-state network dynamics. Next, we introduce a noise regularization scheme for higher-order (tensor) networks, to our knowledge the first of its kind, and design an algorithm for improved incremental learning. Finally, we design a method for inserting grammar rules into a NSPDA and empirically show that this prior knowledge improves its training convergence time by an order of magnitude and, in some cases, leads to better generalization. The NSPDA is also compared to a classical analog stack neural network pushdown automaton (NNPDA) as well as a wide array of first and second-order RNNs with and without external memory, trained using different learning algorithms. Our results show that, for Dyck(2) languages, prior rule-based knowledge is critical for optimization convergence and for ensuring generalization to longer sequences at test time. We observe that many RNNs with and without memory, but no prior knowledge, fail to converge and generalize poorly on CFGs."}}
{"id": "SssbU0MeO6H", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Neural Temporal Model for Human Motion Prediction.", "abstract": "We propose novel neural temporal models for predicting and synthesizing human motion, achieving state-of-the-art in modeling long-term motion trajectories while being competitive with prior work in short-term prediction and requiring significantly less computation. Key aspects of our proposed system include: 1) a novel, two-level processing architecture that aids in generating planned trajectories, 2) a simple set of easily computable features that integrate derivative information, and 3) a novel multi-objective loss function that helps the model to slowly progress from simple next-step prediction to the harder task of multi-step, closed-loop prediction. Our results demonstrate that these innovations improve the modeling of long-term motion trajectories. Finally, we propose a novel metric, called Normalized Power Spectrum Similarity (NPSS), to evaluate the long-term predictive ability of motion synthesis models, complementing the popular mean-squared error (MSE) measure of Euler joint angles over time. We conduct a user study to determine if the proposed NPSS correlates with human evaluation of long-term motion more strongly than MSE and find that it indeed does. We release code and additional results (visualizations) for this paper at: https://github.com/cr7anand/neural_temporal_models"}}
