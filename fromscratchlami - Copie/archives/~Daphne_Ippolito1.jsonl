{"id": "qY5DfQ7ZfW", "cdate": 1668719448195, "mdate": 1668719448195, "content": {"title": "Understanding image motion with group representations", "abstract": "Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain"}}
{"id": "PvOo1sHKzf", "cdate": 1663850164823, "mdate": null, "content": {"title": "Counterfactual Memorization in Neural Language Models", "abstract": "Modern neural language models widely used in tasks across NLP risk memorizing sensitive information from their training data. As models continue to scale up in parameters, training data, and compute, understanding memorization in language models is both important from a learning-theoretical point of view, and is practically crucial in real world applications. An open question in previous studies of memorization in language models is how to filter out \"common\" memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing \"common\" memorization such as familiar phrases, public knowledge or templated texts. In this paper, we provide a principled perspective inspired by a taxonomy of human memory in Psychology. From this perspective, we formulate a notion of counterfactual memorization, which characterizes how a model's predictions change if a particular document is omitted during training. We identify and study counterfactually-memorized training examples in standard text datasets. We further estimate the influence of each training example on the validation set and on generated texts, and show that this can provide direct evidence of the source of memorization at test time."}}
{"id": "TatRHT_1cK", "cdate": 1663850164469, "mdate": null, "content": {"title": "Quantifying Memorization Across Neural Language Models", "abstract": "Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).\nWe describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations."}}
{"id": "7bJizxLKrR", "cdate": 1663850163656, "mdate": null, "content": {"title": "Measuring Forgetting of Memorized Training Examples", "abstract": "Machine learning models exhibit two seemingly contradictory phenomena: training data memorization and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena.\nWe propose a technique to measure to what extent models ``forget'' the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently.\nWe show that, while non-convexity can prevent forgetting from happening in the worst-case, standard image,speech, and language models empirically do forget examples over time.\nWe identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget.\nOur results suggest that examples seen early when training with extremely large datasets---for instance those examples used to pre-train a model---may observe privacy benefits at the expense of examples seen later."}}
{"id": "EKwH-BMlkzq", "cdate": 1654191669286, "mdate": null, "content": {"title": "Dungeons and Dragons as a Dialogue Challenge for Artificial Intelligence", "abstract": "AI researchers have posited Dungeons and Dragons (D\\&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D\\&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history.  We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words.  We automatically annotate the data with partial state information about the game play.  We train a large language model to generate the next game turn, conditioning it on different information.  The LM can respond as a particular character or as the player who runs the game\u2014i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy).  We perform a human evaluation to determine what factors make the generated output plausible and interesting.  We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output.  "}}
{"id": "EcIKWj2i43I", "cdate": 1635880961500, "mdate": 1635880961500, "content": {"title": "Comparison of diverse decoding methods from conditional language models", "abstract": "While conditional language models have greatly improved in their ability to output high-quality natural language, many NLP ap- plications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given- sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that re-rank and com- bine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has fo- cused on increasing diversity in these meth- ods. In this work, we perform an extensive survey of decoding-time strategies for generat- ing diverse outputs from conditional language models. We also show how diversity can be improved without sacrificing quality by over- sampling additional candidates, then filtering to the desired number."}}
{"id": "Fkpr2RYDvI1", "cdate": 1629418650409, "mdate": null, "content": {"title": "SynthBio: A Case Study in Faster Curation of Text Datasets", "abstract": "NLP researchers need more, higher-quality text datasets. Human-labeled datasets are expensive to collect, while datasets collected via automatic retrieval from the web such as WikiBio [Lebret 2016] are noisy and can include undesired biases. Moreover, data sourced from the web is often included in datasets used to pretrain models, leading to inadvertent cross-contamination of training and test sets. In this work we introduce a novel method for efficient dataset curation: we use a large language model to provide seed generations to human raters, thereby changing dataset authoring from a writing task to an editing task. We use our method to curate SynthBio - a new evaluation set for WikiBio - comprised of structured attribute lists describing fictional individuals, mapped to natural language biographies. We show that our dataset of fictional biographies is less noisy than WikiBio, and also more balanced with respect to gender and nationality."}}
{"id": "rJbdpQ-OZS", "cdate": 1546300800000, "mdate": null, "content": {"title": "ChatEval: A Tool for Chatbot Evaluation", "abstract": "Jo\u00e3o Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai Thirani, Lyle Ungar, Chris Callison-Burch. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). 2019."}}
{"id": "SJLlmG-AZ", "cdate": 1518730164752, "mdate": null, "content": {"title": "Understanding image motion with group representations ", "abstract": "Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain."}}
{"id": "ry-2Tox_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Translations via Images with a Massively Multilingual Image Dataset", "abstract": ""}}
