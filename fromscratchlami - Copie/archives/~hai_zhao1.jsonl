{"id": "yEaOsRabHR", "cdate": 1683893348379, "mdate": 1683893348379, "content": {"title": "Heat Up The Sentiment Learning With Ice", "abstract": "Recently, dramatic gains have been made on the task of aspect sentiment triplet extraction (ASTE). In this paper, we introduce a straightforward pipeline model to perform two-stage sequence labeling, including aspect and opinion terms identification and aspect-opinion pair classification. To exploit the cross-sentence context information to the maximum extent possible, we propose the instance cooperative enhancement (ICE) by introducing unsupervised clustering methods. Through experimenting with various clustering methods, we found that GSDMM unleashes the potential of cross-sentence information to the most degree. Compared to current state-of-the-art models, the results show the effectiveness of our proposed framework on ASTE-Data-V2."}}
{"id": "wFxjFCHUkS", "cdate": 1677713803819, "mdate": null, "content": {"title": "Heat Up The Sentiment Learning With ICE", "abstract": "Recently, dramatic gains have been made on the task of aspect sentiment triplet extraction (ASTE). In this paper, we introduce a straightforward pipeline model to perform two-stage sequence labeling, including aspect and opinion terms identification and aspect-opinion pair classification. To exploit the cross-sentence context information to the maximum extent possible, we propose the instance cooperative enhancement (ICE) by introducing unsupervised clustering methods. Through experimenting with various clustering methods, we found that GSDMM unleashes the potential of cross-sentence information to the most degree. Compared to current state-of-the-art models, the results show the effectiveness of our proposed framework on ASTE-Data-V2."}}
{"id": "xZD10GhCvM", "cdate": 1663850064865, "mdate": null, "content": {"title": "Toward Adversarial Training on Contextualized Language Representation", "abstract": "Beyond the success story of adversarial training (AT) in the recent text domain on top of pre-trained language models (PLMs), our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition. This paper investigates AT from the perspective of the contextualized language representation outputted by PLM encoders. We find the current AT attacks lean to generate sub-optimal adversarial examples that can fool the decoder part but have a minor effect on the encoder. However, we find it necessary to effectively deviate the latter one to allow AT to gain. Based on the observation, we propose simple yet effective \\textit{Contextualized representation-Adversarial Training} (CreAT), in which the attack is explicitly optimized to deviate the contextualized representation of the encoder. It allows a global optimization of adversarial examples that can fool the entire model. We also find CreAT gives rise to a better direction to optimize the adversarial examples, to let them less sensitive to hyperparameters. Compared to AT, CreAT produces consistent performance gains on a wider range of tasks and is proven to be more effective for language pre-training where only the encoder part is kept for downstream tasks. We achieve the new state-of-the-art performances on a series of challenging benchmarks, e.g. AdvGLUE (59.1 $ \\rightarrow $ 61.1), HellaSWAG (93.0  $ \\rightarrow $ 94.9), ANLI (68.1  $ \\rightarrow $ 69.3)."}}
{"id": "p7Bfc_wsDtH", "cdate": 1663849908469, "mdate": null, "content": {"title": "Logic-aware Pre-training of Language Models", "abstract": "Pre-trained language models (PrLMs) have been shown useful for enhancing a broad range of natural language understanding (NLU) tasks. However, the capacity for capturing logic relations in challenging NLU still remains a bottleneck even for state-of-the-art PrLM enhancement, which greatly stalls their reasoning abilities. To bridge the gap, we propose logic pre-training of language models to equip PrLMs with logical reasoning ability. To let logic pre-training perform on a clear, accurate, and generalized knowledge basis, we introduce \\textit{fact} instead of the plain language unit in previous PrLMs. The \\textit{fact} is extracted through syntactic parsing in avoidance of unnecessary complex knowledge injection. Meanwhile, it enables training logic-aware models to be conducted on a more general language text. To explicitly guide the PrLM to capture logic relations, three complementary self-supervised pre-training objectives are introduced: 1) logical structure completion to accurately capture fact-level logic from the original context, 2) logical path prediction on a logical graph to uncover global logic relationships among facts, 3) logical connectives masking to capture discourse-level for fact groups. We evaluate our model on a broad range of NLP tasks, including natural language inference, relation extraction, and machine reading comprehension with logical reasoning. Experimental results show that our model achieves significant performance in all the downstream tasks, especially in logical reasoning-related tasks. "}}
{"id": "MdSGM9PEQ7", "cdate": 1663849870002, "mdate": null, "content": {"title": "Admeta: A Novel Double Exponential Moving Average to Adaptive and Non-adaptive Momentum Optimizers with Bidirectional Looking", "abstract": "Optimizer is an essential component for the success of deep learning, which guides the neural network to update the parameters according to the loss on the training set. SGD and Adam are two classical and effective optimizers on which researchers have proposed many variants, such as SGDM and RAdam. In this paper, we innovatively combine the backward-looking and forward-looking aspects of the optimizer algorithm and propose a novel \\textsc{Admeta} (\\textbf{A} \\textbf{D}ouble exponential \\textbf{M}oving averag\\textbf{E} \\textbf{T}o \\textbf{A}daptive and non-adaptive momentum) optimizer framework. For backward-looking part, we propose a DEMA variant scheme, which is motivated by a metric in the stock market, to replace the common exponential moving average scheme. While in the forward-looking part, we present a dynamic lookahead strategy which asymptotically approaching a set value, maintaining its speed at early stage and high convergence performance at final stage. Based on this idea, we provide two optimizer implementations, \\textsc{AdmetaR} and \\textsc{AdmetaS}, the former based on RAdam and the latter based on SGDM. Through extensive experiments on diverse tasks, we find that the proposed \\textsc{Admeta} optimizer outperforms our base optimizers and shows advantages over recently proposed competitive optimizers. We also provide theoretical proof of these two algorithms, which verifies the convergence of our proposed \\textsc{Admeta}."}}
{"id": "lP11WtZwquE", "cdate": 1632875665769, "mdate": null, "content": {"title": "Language Model Pre-training on True Negatives", "abstract": "Discriminative pre-trained language models (PrLMs) learn to predict original texts from intentionally corrupted ones. Taking the former text as positive and the latter as negative samples, the discriminative PrLM can be trained effectively for contextualized representation. However, though the training of such a type of PrLMs highly relies on the quality of the automatically constructed samples, existing PrLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false negative issue where training is carried out on wrong data and leads to less efficiency and less robustness in the resulting PrLMs.\nThus in this work, on the basis of defining the false negative issue in discriminative PrLMs that has been ignored for a long time, we design enhanced pre-training methods to counteract false negative predictions and encourage pre-training language models on true negatives, by correcting the harmful gradient updates subject to false negative predictions. Experimental results on GLUE and SQuAD benchmarks show that our counter-false-negative pre-training methods indeed bring about better performance together with stronger robustness."}}
{"id": "q4pQkTlImdk", "cdate": 1632875655934, "mdate": null, "content": {"title": "Not All Attention Is All You Need", "abstract": "Dropout has shown an effective mediation to alleviate the over-fitting of neural models by forcedly blocking less helpful connections. However, common dropout has to be done crudely over all neural structures with the same dropout pattern once for all to dodge huge search space of tuning every individual structures. Thus in terms of meta-learning, we propose $AttendOut$ which is capable of performing smart unit-specific dropout for attention models. The proposed smart dropout is nearly parameter-free and makes it possible to achieve even stronger performances with a faster tuning circle even though we evaluate our proposed method on state-of-the-art pre-trained language models. Eventually, we verify the universality of our approach on extensive downstream tasks in both pre-training and fine-tuning stages."}}
{"id": "1gEb_H1DEqZ", "cdate": 1632875655452, "mdate": null, "content": {"title": "Logic Pre-Training of Language Models", "abstract": "Pre-trained language models (PrLMs) have been shown useful for enhancing a broad range of natural language understanding (NLU) tasks. However, the capacity for capturing logic relations in challenging NLU still remains a bottleneck even for state-of-the-art PrLM enhancement, which greatly stalled their reasoning abilities. Thus we propose logic pre-training of language models, leading to the logic reasoning ability equipped PrLM, \\textsc{Prophet}. To let logic pre-training perform on a clear, accurate, and generalized knowledge basis, we introduce \\textit{fact} instead of the plain language unit in previous PrLMs. The \\textit{fact} is extracted through syntactic parsing in avoidance of unnecessary complex knowledge injection. Meanwhile, it enables training logic-aware models to be conducted on a more general language text. To explicitly guide the PrLM to capture logic relations, three pre-training objectives are introduced: 1) logical connectives masking to capture sentence-level logics, 2) logical structure completion to accurately capture facts from the original context, 3) logical path prediction on a logical graph to uncover global logic relationships among facts. We evaluate our model on a broad range of NLP and NLU tasks, including natural language inference, relation extraction, and machine reading comprehension with logical reasoning. Results show that the extracted fact and the newly introduced pre-training tasks can help \\textsc{Prophet} achieve significant performance in all the downstream tasks, especially in logic reasoning related tasks. "}}
{"id": "gKWxifgJVP", "cdate": 1632875629380, "mdate": null, "content": {"title": "Fact-driven Logical Reasoning", "abstract": "Recent years have witnessed an increasing interest in training machines with reasoning ability, which deeply relies on accurate, clearly presented clue forms that are usually modeled as entity-like knowledge in existing studies. However, in real hierarchical reasoning motivated machine reading comprehension, such one-sided modeling is insufficient for those indispensable local complete facts or events when only \"global\" knowledge is really paid attention to. Thus, in view of language being a complete knowledge/clue carrier, we propose a general formalism to support representing logic units by extracting backbone constituents of the sentence such as the subject-verb-object formed \"facts\", covering both global and local knowledge pieces that are necessary as the basis for logical reasoning. Beyond building the ad-hoc graphs, we propose a more general and convenient fact-driven approach to construct a supergraph on top of our newly defined fact units, benefiting from both sides of the connections between facts and internal knowledge such as concepts or actions inside a fact. Experiments on two challenging logical reasoning benchmarks show that our proposed model, \\textsc{Focal Reasoner}, outperforms the baseline models dramatically and achieves state-of-the-art results."}}
{"id": "5JIAKpVrmZK", "cdate": 1621629915420, "mdate": null, "content": {"title": "Multilingual Pre-training with Universal Dependency Learning", "abstract": "The pre-trained language model (PrLM) demonstrates domination in downstream natural language processing tasks, in which multilingual PrLM takes advantage of language universality to alleviate the issue of limited resources for low-resource languages. Despite its successes, the performance of multilingual PrLM is still unsatisfactory, when multilingual PrLMs only focus on plain text and ignore obvious universal linguistic structure clues. Existing PrLMs have shown that monolingual linguistic structure knowledge may bring about better performance. Thus we propose a novel multilingual PrLM that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in our model, which brings unprecedented PrLM interpretability and convenience in downstream task use. Our model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets, demonstrating the effectiveness and stronger cross-lingual modeling capabilities of our approach."}}
