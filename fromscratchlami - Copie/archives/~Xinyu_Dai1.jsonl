{"id": "i3d7yPu8cQ", "cdate": 1676885550583, "mdate": 1676885550583, "content": {"title": "Modeling Users' Contextualized Page-wise Feedback for Click-Through Rate Prediction in E-commerce Search", "abstract": "Modeling users\u2019 historical feedback is essential for Click-ThroughRate Prediction in personalized search and recommendation. Existing methods usually only model users\u2019 positive feedback information such as click sequences which neglects the context information of the feedback. In this paper, we propose a new perspective for context-aware users\u2019 behavior modeling by including the whole page-wisely exposed products and the corresponding feedback as contextualized page-wise feedback sequence. The intra-page context information and inter-page interest evolution can be captured to learn more specific user preference. We design a novel neural ranking model RACP(Recurrent Attention over ContextualizedPage sequence), which utilizes page-context aware attention to model the intra-page context. A recurrent attention process is used to model the cross-page interest convergence evolution as denoising the interest in the previous pages. Experiments on public and real-world industrial datasets verify our model\u2019s effectiveness."}}
{"id": "Z-k91NB8Eh", "cdate": 1664928782707, "mdate": null, "content": {"title": "Out-of-Distribution Generalization Challenge in Dialog State Tracking", "abstract": "Dialog State Tracking (DST) is a core component for multi-turn Task-Oriented Dialog (TOD) systems to understand the dialogs. DST models need to generalize to Out-of-Distribution (OOD) utterances due to the open environments dialog systems face. Unfortunately, utterances in TOD are multi-labeled, and most of them appear in specific contexts (i.e., the dialog histories). Both characteristics make them different from the conventional focus of OOD generalization research and remain unexplored. In this paper, we formally define OOD utterances in TOD and evaluate the generalizability of existing competitive DST models on the OOD utterances. Our experimental result shows that the performance of all models drops considerably in dialogs with OOD utterances, indicating an OOD generalization challenge in DST."}}
{"id": "H_xAgRM7I5N", "cdate": 1652737473923, "mdate": null, "content": {"title": "Zero-Shot 3D Drug Design by Sketching and Generating", "abstract": "Drug design is a crucial step in the drug discovery cycle. Recently, various deep learning-based methods design drugs by generating novel molecules from scratch, avoiding traversing large-scale drug libraries. However, they depend on scarce experimental data or time-consuming docking simulation, leading to overfitting issues with limited training data and slow generation speed. In this study, we propose the zero-shot drug design method DESERT (Drug dEsign by SkEtching and geneRaTing). Specifically, DESERT splits the design process into two stages: sketching and generating, and bridges them with the molecular shape. The two-stage fashion enables our method to utilize the large-scale molecular database to reduce the need for experimental data and docking simulation. Experiments show that DESERT achieves a new state-of-the-art at a fast speed."}}
{"id": "1O33PNH4t9v", "cdate": 1648781313931, "mdate": null, "content": {"title": "A Synthetic Approach for Recommendation: Combining Ratings, Social Relations, and Reviews", "abstract": "Recommender systems (RSs) provide an effective way of alleviating the information overload problem by selecting personalized choices. Online social networks and user-generated content provide diverse sources for recommendation beyond ratings, which present opportunities as well as challenges for traditional RSs. Although social matrix factorization (Social MF) can integrate ratings with social relations and topic matrix factorization can integrate ratings with item reviews, both of them ignore some useful information. In this paper, we investigate the effective data fusion by combining the two approaches, in two steps. First, we extend Social MF to exploit the graph structure of neighbors. Second, we propose a novel framework MR3 to jointly model these three types of information effectively for rating prediction by aligning latent factors and hidden topics. We achieve more accurate rating prediction on two real-life datasets. Furthermore, we measure the contribution of each data source to the proposed framework.\n"}}
{"id": "SylurJHFPS", "cdate": 1569439552489, "mdate": null, "content": {"title": "The Detection of Distributional Discrepancy for Text Generation", "abstract": "The text generated by neural language models is not as good as the real text. This means that their distributions are different. Generative Adversarial Nets (GAN) are used to alleviate it. However, some researchers argue that GAN variants do not work at all. When both sample quality (such as Bleu) and sample diversity (such as self-Bleu) are taken into account, the GAN variants even are worse than a well-adjusted language model. But, Bleu and self-Bleu can not precisely measure this distributional discrepancy. In fact, how to measure the distributional discrepancy between real text and generated text is still an open problem. In this paper, we theoretically propose two metric functions to measure the distributional difference between real text and generated text. Besides that, a method is put forward to estimate them. First, we evaluate language model with these two functions and find the difference is huge. Then, we try several methods to use the detected discrepancy signal to improve the generator. However the difference becomes even bigger than before. Experimenting on two existing language GANs, the distributional discrepancy between real text and generated text increases with more adversarial learning rounds. It demonstrates both of these language GANs fail. "}}
{"id": "HkxQRTNYPH", "cdate": 1569439179135, "mdate": null, "content": {"title": "Mirror-Generative Neural Machine Translation", "abstract": "Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in a variety of scenarios and language pairs, including resource-rich and low-resource situations. "}}
{"id": "HJbO47WubS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Online Distilling from Checkpoints for Neural Machine Translation", "abstract": "Hao-Ran Wei, Shujian Huang, Ran Wang, Xin-yu Dai, Jiajun Chen. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "H1bvpmbObH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling", "abstract": "Zhifang Fan, Zhen Wu, Xin-Yu Dai, Shujian Huang, Jiajun Chen. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "H14BPXbubB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Exploiting Noisy Data in Distant Supervision Relation Classification", "abstract": "Kaijia Yang, Liang He, Xin-yu Dai, Shujian Huang, Jiajun Chen. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "r1b1XQ-uZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attention", "abstract": "This thesis proposes theoretical and numerical contributions to use Entropy-regularized Optimal Transport (EOT) for machine learning. We introduce Sinkhorn Divergences (SD), a class of discrepancies between probability measures based on EOT which interpolates between two other well-known discrepancies: Optimal Transport (OT) and Maximum Mean Discrepancies (MMD). We develop an efficient numerical method to use SD for density fitting tasks, showing that a suitable choice of regularization can improve performance over existing methods. We derive a sample complexity theorem for SD which proves that choosing a large enough regularization parameter allows to break the curse of dimensionality from OT, and recover asymptotic rates similar to MMD. We propose and analyze stochastic optimization solvers for EOT, which yield online methods that can cope with arbitrary measures and are well suited to large scale problems, contrarily to existing discrete batch solvers."}}
