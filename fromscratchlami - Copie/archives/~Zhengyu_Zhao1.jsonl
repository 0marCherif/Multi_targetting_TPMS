{"id": "bhFbuw4eynO", "cdate": 1672531200000, "mdate": 1681115701990, "content": {"title": "The Importance of Image Interpretation: Patterns of Semantic Misclassification in Real-World Adversarial Images", "abstract": ""}}
{"id": "S6VwbbhY84", "cdate": 1672531200000, "mdate": 1681115702610, "content": {"title": "Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression", "abstract": ""}}
{"id": "zKvm1ETDOq", "cdate": 1663850170452, "mdate": null, "content": {"title": "Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?", "abstract": "Indiscriminate data poisoning can decrease the clean test accuracy of a deep learning model by slightly perturbing its training samples.\nThere is a consensus that such poisons can hardly harm adversarially-trained (AT) models when the adversarial training budget is no less than the poison budget, i.e., $\\epsilon_\\mathrm{adv}\\geq\\epsilon_\\mathrm{poi}$. This consensus, however, is challenged in this paper based on our new attack strategy that induces \\textit{entangled features} (EntF). The existence of entangled features makes the poisoned data become less useful for training a model, no matter if AT is applied or not. We demonstrate that for attacking a CIFAR-10 AT model under a reasonable setting with $\\epsilon_\\mathrm{adv}=\\epsilon_\\mathrm{poi}=8/255$, our EntF yields an accuracy drop of $13.31\\%$, which is $7\\times$ better than existing methods and equal to discarding $83\\%$ training data. We further show the generalizability of EntF to more challenging settings, e.g., higher AT budgets, partial poisoning, unseen model architectures, and stronger (ensemble or adaptive) defenses. We finally provide new insights into the distinct roles of non-robust vs. robust features in poisoning standard vs. AT models and demonstrate the possibility of using a hybrid attack to poison standard and AT models simultaneously. Our code is available at~\\url{https://github.com/WenRuiUSTC/EntF}."}}
{"id": "vvNfacohAuF", "cdate": 1640995200000, "mdate": 1681115702029, "content": {"title": "Towards Good Practices in Evaluating Transfer Adversarial Attacks", "abstract": ""}}
{"id": "s9hDFtZKA0j", "cdate": 1640995200000, "mdate": 1681115702478, "content": {"title": "Generative Poisoning Using Random Discriminators", "abstract": ""}}
{"id": "baFbtm3gpMF", "cdate": 1640995200000, "mdate": 1681115702137, "content": {"title": "Membership Inference Attacks by Exploiting Loss Trajectory", "abstract": ""}}
{"id": "3I57JLURUbu", "cdate": 1624022584596, "mdate": null, "content": {"title": "On Success and Simplicity: A Second Look at Transferable Targeted Attacks", "abstract": "Achieving transferability of targeted attacks is reputed to be remarkably difficult, and state-of-the-art approaches are resource-intensive due to training target-specific model(s) with additional data. In our work, we find, however, that simple transferable attacks which require neither additional data nor model training can achieve surprisingly high targeted transferability. This insight has been overlooked mainly due to the widespread practice of unreasonably restricting attack optimization to few iterations. In particular, we, for the first time, identify the state-of-the-art performance of a simple logit loss. Our investigation is conducted in a wide range of transfer settings, especially including three new, realistic settings: ensemble transfer with little model similarity, transfer to low-ranked target classes, and transfer to the real-world Google Cloud Vision API. Results in these new settings demonstrate that the commonly adopted, easy settings cannot fully reveal the actual properties of different attacks and may cause misleading comparisons. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability."}}
{"id": "aHK-onEhYRg", "cdate": 1621629993485, "mdate": null, "content": {"title": "On Success and Simplicity: A Second Look at Transferable Targeted Attacks", "abstract": "Achieving transferability of targeted attacks is reputed to be remarkably difficult. The current state of the art has resorted to resource-intensive solutions that necessitate training model(s) for each target class with additional data. In our investigation, we find, however, that simple transferable attacks which require neither model training nor additional data can achieve surprisingly strong targeted transferability. This insight has been overlooked until now, mainly because the widespread practice of attacking with only few iterations has largely limited the attack convergence to optimal targeted transferability. In particular, we, for the first time, identify that a very simple logit loss can largely surpass the commonly adopted cross-entropy loss, and yield even better results than the resource-intensive state of the art. Our analysis spans a variety of transfer scenarios, especially including three new, realistic scenarios: an ensemble transfer scenario with little model similarity, a worse-case scenario with low-ranked target classes, and also a real-world attack on the Google Cloud Vision API. Results in these new transfer scenarios demonstrate that the commonly adopted, easy scenarios cannot fully reveal the actual strength of different attacks and may cause misleading comparative results. We also show the usefulness of the simple logit loss for generating targeted universal adversarial perturbations in a data-free manner. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability. Code is available at https://github.com/ZhengyuZhao/Targeted-Tansfer."}}
{"id": "LVWcGZr-8h", "cdate": 1621629993485, "mdate": null, "content": {"title": "On Success and Simplicity: A Second Look at Transferable Targeted Attacks", "abstract": "Achieving transferability of targeted attacks is reputed to be remarkably difficult. The current state of the art has resorted to resource-intensive solutions that necessitate training model(s) for each target class with additional data. In our investigation, we find, however, that simple transferable attacks which require neither model training nor additional data can achieve surprisingly strong targeted transferability. This insight has been overlooked until now, mainly because the widespread practice of attacking with only few iterations has largely limited the attack convergence to optimal targeted transferability. In particular, we, for the first time, identify that a very simple logit loss can largely surpass the commonly adopted cross-entropy loss, and yield even better results than the resource-intensive state of the art. Our analysis spans a variety of transfer scenarios, especially including three new, realistic scenarios: an ensemble transfer scenario with little model similarity, a worse-case scenario with low-ranked target classes, and also a real-world attack on the Google Cloud Vision API. Results in these new transfer scenarios demonstrate that the commonly adopted, easy scenarios cannot fully reveal the actual strength of different attacks and may cause misleading comparative results. We also show the usefulness of the simple logit loss for generating targeted universal adversarial perturbations in a data-free manner. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability. Code is available at https://github.com/ZhengyuZhao/Targeted-Tansfer."}}
{"id": "s6m6Z_dEk1", "cdate": 1609459200000, "mdate": 1668146744941, "content": {"title": "On Success and Simplicity: A Second Look at Transferable Targeted Attacks", "abstract": ""}}
