{"id": "K47UapKPuWq", "cdate": 1684747637131, "mdate": 1684747637131, "content": {"title": "On Deep Generative Models for Approximation and Estimation of Distributions on Manifolds", "abstract": "Generative networks have experienced great empirical successes in distribution learning. Many existing experiments have demonstrated that generative networks can generate high-dimensional complex data from a low-dimensional easy-to-sample distribution. However, this phenomenon can not be justified by existing theories. The widely held manifold hypothesis speculates that real-world data sets, such as natural images and signals, exhibit low-dimensional geometric structures. In this paper, we take such low-dimensional data structures into consideration by assuming that data distributions are supported on a low-dimensional manifold. We prove statistical guarantees of generative networks under the Wasserstein-1 loss. We show that the Wasserstein-1 loss converges to zero at a fast rate depending on the intrinsic dimension instead of the ambient data dimension. Our theory leverages the low-dimensional geometric structures in data sets and justifies the practical power of generative networks. We require no smoothness assumptions on the data distribution which is desirable in practice."}}
{"id": "v_e7WqmYpyO", "cdate": 1672531200000, "mdate": 1681664057905, "content": {"title": "Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness", "abstract": "Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\\displaystyle n^{-\\frac{2}{d+2}}\\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly depends on the ambient dimension and noise level. We further extend our theory on data with noise containing both normal and tangential components, where chart autoencoders still exhibit a denoising effect for the normal component. As a special case, our theory also applies to classical autoencoders, as long as the data manifold has a global parametrization. Our results provide a solid theoretical foundation for the effectiveness of autoencoders, which is further validated through several numerical experiments."}}
{"id": "HSFDz8iOT_", "cdate": 1672531200000, "mdate": 1681664057979, "content": {"title": "On Deep Generative Models for Approximation and Estimation of Distributions on Manifolds", "abstract": "Generative networks have experienced great empirical successes in distribution learning. Many existing experiments have demonstrated that generative networks can generate high-dimensional complex data from a low-dimensional easy-to-sample distribution. However, this phenomenon can not be justified by existing theories. The widely held manifold hypothesis speculates that real-world data sets, such as natural images and signals, exhibit low-dimensional geometric structures. In this paper, we take such low-dimensional data structures into consideration by assuming that data distributions are supported on a low-dimensional manifold. We prove statistical guarantees of generative networks under the Wasserstein-1 loss. We show that the Wasserstein-1 loss converges to zero at a fast rate depending on the intrinsic dimension instead of the ambient data dimension. Our theory leverages the low-dimensional geometric structures in data sets and justifies the practical power of generative networks. We require no smoothness assumptions on the data distribution which is desirable in practice."}}
{"id": "fbvDUa8-MTm", "cdate": 1665081437622, "mdate": null, "content": {"title": "Benefits of Overparameterized Convolutional Residual Networks: Function Approximation under Smoothness Constraint", "abstract": "Overparameterized neural networks enjoy great representation power on complex data, and more importantly yield sufficiently smooth output, which is crucial to their generalization and robustness. Most existing function approximation theories suggest that with sufficiently many parameters, neural networks can well approximate certain classes of functions in terms of the function value. The neural network themselves, however, can be highly nonsmooth. To bridge this gap, we take convolutional residual networks (ConvResNets) as an example, and prove that large ConvResNets can not only approximate a target function in terms of function value, but also exhibit sufficient first-order smoothness. Moreover, we extend our theory to approximating functions supported on a low-dimensional manifold. Our theory partially justifies the benefits of using deep and wide networks in practice. Numerical experiments on adversarial robust image classification are provided to support our theory."}}
{"id": "4n1PS9WvdYv", "cdate": 1652737609603, "mdate": null, "content": {"title": "On Deep Generative Models for Approximation and Estimation of Distributions on Manifolds", "abstract": "Deep generative models have experienced great empirical successes in distribution learning. Many existing experiments have demonstrated that deep generative networks can efficiently generate high-dimensional complex data from a low-dimensional easy-to-sample distribution. However, this phenomenon can not be justified by existing theories. The widely held manifold hypothesis speculates that real-world data sets, such as natural images and signals, exhibit low-dimensional geometric structures. In this paper, we take such low-dimensional data structures into consideration by assuming that data distributions are supported on a low-dimensional manifold. We prove approximation and estimation theories of deep generative networks for estimating distributions on a low-dimensional manifold under the Wasserstein-1 loss. We show that the Wasserstein-1 loss converges to zero at a fast rate depending on the intrinsic dimension instead of the ambient data dimension. Our theory leverages the low-dimensional geometric structures in data sets and justifies the practical power of deep generative models. We require no smoothness assumptions on the data distribution which is desirable in practice."}}
{"id": "ukRb1rQkWAp", "cdate": 1640995200000, "mdate": 1681664057969, "content": {"title": "A Manifold Two-Sample Test Study: Integral Probability Metric with Neural Networks", "abstract": "Two-sample tests are important areas aiming to determine whether two collections of observations follow the same distribution or not. We propose two-sample tests based on integral probability metric (IPM) for high-dimensional samples supported on a low-dimensional manifold. We characterize the properties of proposed tests with respect to the number of samples $n$ and the structure of the manifold with intrinsic dimension $d$. When an atlas is given, we propose two-step test to identify the difference between general distributions, which achieves the type-II risk in the order of $n^{-1/\\max\\{d,2\\}}$. When an atlas is not given, we propose H\\\"older IPM test that applies for data distributions with $(s,\\beta)$-H\\\"older densities, which achieves the type-II risk in the order of $n^{-(s+\\beta)/d}$. To mitigate the heavy computation burden of evaluating the H\\\"older IPM, we approximate the H\\\"older function class using neural networks. Based on the approximation theory of neural networks, we show that the neural network IPM test has the type-II risk in the order of $n^{-(s+\\beta)/d}$, which is in the same order of the type-II risk as the H\\\"older IPM test. Our proposed tests are adaptive to low-dimensional geometric structure because their performance crucially depends on the intrinsic dimension instead of the data dimension."}}
{"id": "u5hYto9NRCS", "cdate": 1640995200000, "mdate": 1681664057976, "content": {"title": "Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces", "abstract": "Learning operators between infinitely dimensional spaces is an important learning task arising in wide applications in machine learning, imaging science, mathematical modeling and simulations, etc. This paper studies the nonparametric estimation of Lipschitz operators using deep neural networks. Non-asymptotic upper bounds are derived for the generalization error of the empirical risk minimizer over a properly chosen network class. Under the assumption that the target operator exhibits a low dimensional structure, our error bounds decay as the training sample size increases, with an attractive fast rate depending on the intrinsic dimension in our estimation. Our assumptions cover most scenarios in real applications and our results give rise to fast rates by exploiting low dimensional structures of data in operator estimation. We also investigate the influence of network structures (e.g., network width, depth, and sparsity) on the generalization error of the neural network estimator and propose a general suggestion on the choice of network structures to maximize the learning efficiency quantitatively."}}
{"id": "ifh-Py41FXT", "cdate": 1640995200000, "mdate": 1681664058141, "content": {"title": "Benefits of Overparameterized Convolutional Residual Networks: Function Approximation under Smoothness Constraint", "abstract": "Overparameterized neural networks enjoy great representation power on complex data, and more importantly yield sufficiently smooth output, which is crucial to their generalization and robustness. M..."}}
{"id": "dct47ThtZ9-", "cdate": 1640995200000, "mdate": 1681664058053, "content": {"title": "Robust Identification of Differential Equations by Numerical Techniques from a Single Set of Noisy Observation", "abstract": "We propose robust methods to identify the underlying Partial Differential Equation (PDE) from a given single set of noisy time-dependent data. We assume that the governing equation of the PDE is a linear combination of a few linear and nonlinear differential terms in a prescribed dictionary. Noisy data make such identification particularly challenging. Our objective is to develop robust methods against a high level of noise and approximate the underlying noise-free dynamics well. We first introduce a Successively Denoised Differentiation (SDD) scheme to stabilize the amplified noise in numerical differentiation. SDD effectively denoises the given data and the corresponding derivatives. Second, we present two algorithms for PDE identification: Subspace pursuit Time evolution (ST) error and Subspace pursuit Cross-validation (SC). Our general strategy is to first find a candidate set using the Subspace Pursuit (SP) greedy algorithm, then choose the best one via time evolution or cross-validation. ST uses a multishooting numerical time evolution and selects the PDE which yields the least evolution error. SC evaluates the cross-validation error in the least-squares fitting and picks the PDE that gives the smallest validation error. We present various numerical experiments to validate our methods. Both methods are efficient and robust to noise."}}
{"id": "MPPd82fofG5", "cdate": 1640995200000, "mdate": 1681664058151, "content": {"title": "WeakIdent: Weak formulation for Identifying Differential Equations using Narrow-fit and Trimming", "abstract": "Data-driven identification of differential equations is an interesting but challenging problem, especially when the given data are corrupted by noise. When the governing differential equation is a linear combination of various differential terms, the identification problem can be formulated as solving a linear system, with the feature matrix consisting of linear and nonlinear terms multiplied by a coefficient vector. This product is equal to the time derivative term, and thus generates dynamical behaviors. The goal is to identify the correct terms that form the equation to capture the dynamics of the given data. We propose a general and robust framework to recover differential equations using a weak formulation, for both ordinary and partial differential equations (ODEs and PDEs). The weak formulation facilitates an efficient and robust way to handle noise. For a robust recovery against noise and the choice of hyper-parameters, we introduce two new mechanisms, narrow-fit and trimming, for the coefficient support and value recovery, respectively. For each sparsity level, Subspace Pursuit is utilized to find an initial set of support from the large dictionary. Then, we focus on highly dynamic regions (rows of the feature matrix), and error normalize the feature matrix in the narrow-fit step. The support is further updated via trimming of the terms that contribute the least. Finally, the support set of features with the smallest Cross-Validation error is chosen as the result. A comprehensive set of numerical experiments are presented for both systems of ODEs and PDEs with various noise levels. The proposed method gives a robust recovery of the coefficients, and a significant denoising effect which can handle up to $100\\%$ noise-to-signal ratio for some equations. We compare the proposed method with several state-of-the-art algorithms for the recovery of differential equations."}}
