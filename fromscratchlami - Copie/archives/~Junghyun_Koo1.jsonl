{"id": "bECTuPXiP7D", "cdate": 1672531200000, "mdate": 1696257508813, "content": {"title": "Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data", "abstract": "Music source separation (MSS) faces challenges due to the limited availability of correctly-labeled individual instrument tracks. With the push to acquire larger datasets to improve MSS performance, the inevitability of encountering mislabeled individual instrument tracks becomes a significant challenge to address. This paper introduces an automated technique for refining the labels in a partially mislabeled dataset. Our proposed self-refining technique, employed with a noisy-labeled dataset, results in only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data in MSS model training and shows that utilizing the refined dataset leads to comparable results derived from a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on a self-refined dataset even outperform those trained on a dataset refined with a classifier trained on clean labels."}}
{"id": "QcDAapmfsci", "cdate": 1672531200000, "mdate": 1696257508815, "content": {"title": "Exploiting Time-Frequency Conformers for Music Audio Enhancement", "abstract": "With the proliferation of video platforms on the internet, recording musical performances by mobile devices has become commonplace. However, these recordings often suffer from degradation such as noise and reverberation, which negatively impact the listening experience. Consequently, the necessity for music audio enhancement (referred to as music enhancement from this point onward), involving the transformation of degraded audio recordings into pristine high-quality music, has surged to augment the auditory experience. To address this issue, we propose a music enhancement system based on the Conformer architecture that has demonstrated outstanding performance in speech enhancement tasks. Our approach explores the attention mechanisms of the Conformer and examines their performance to discover the best approach for the music enhancement task. Our experimental results show that our proposed model achieves state-of-the-art performance on single-stem music enhancement. Furthermore, our system can perform general music enhancement with multi-track mixtures, which has not been examined in previous work."}}
{"id": "mjosVeDDksf", "cdate": 1640995200000, "mdate": 1674347230661, "content": {"title": "Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects", "abstract": "We propose an end-to-end music mixing style transfer system that converts the mixing style of an input multitrack to that of a reference song. This is achieved with an encoder pre-trained with a contrastive objective to extract only audio effects related information from a reference music recording. All our models are trained in a self-supervised manner from an already-processed wet multitrack dataset with an effective data preprocessing method that alleviates the data scarcity of obtaining unprocessed dry data. We analyze the proposed encoder for the disentanglement capability of audio effects and also validate its performance for mixing style transfer through both objective and subjective evaluations. From the results, we show the proposed system not only converts the mixing style of multitrack audio close to a reference but is also robust with mixture-wise style transfer upon using a music source separation model."}}
{"id": "i4_ZWg4XNlN", "cdate": 1640995200000, "mdate": 1681650684203, "content": {"title": "Representation Selective Self-distillation and wav2vec 2.0 Feature Exploration for Spoof-aware Speaker Verification", "abstract": ""}}
{"id": "IikCVkdx_9", "cdate": 1640995200000, "mdate": 1681650684226, "content": {"title": "End-To-End Music Remastering System Using Self-Supervised And Adversarial Training", "abstract": ""}}
{"id": "ANNAoBPsCF", "cdate": 1609459200000, "mdate": 1681650684190, "content": {"title": "Reverb Conversion Of Mixed Vocal Tracks Using An End-To-End Convolutional Deep Neural Network", "abstract": ""}}
{"id": "xyZS2k1g-X", "cdate": 1577836800000, "mdate": null, "content": {"title": "Exploiting Multi-Modal Features from Pre-Trained Networks for Alzheimer's Dementia Recognition", "abstract": "Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient\u2019s medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer\u2019s Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer\u2019s Dementia by providing acoustic and textual data. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. Our test results surpass baseline\u2019s accuracy by 18.75%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70%."}}
{"id": "L1SUA7o8-Jb", "cdate": 1577836800000, "mdate": null, "content": {"title": "Disentangling Timbre and Singing Style with Multi-Singer Singing Synthesis System", "abstract": "In this study, we define the identity of the singer with two independent concepts \u2013 timbre and singing style \u2013 and propose a multi-singer singing synthesis system that can model them separately. To this end, we extend our single-singer model into a multi-singer model in the following ways: first, we design a singer identity encoder that can adequately reflect the identity of a singer. Second, we use encoded singer identity to condition the two independent decoders that model timbre and singing style, respectively. Through a user study with the listening tests, we experimentally verify that the proposed framework is capable of generating a natural singing voice of high quality while independently controlling the timbre and singing style. Also, by using the method of changing singing styles while fixing the timbre, we suggest that our proposed network can produce a more expressive singing voice."}}
{"id": "Ils6CIsEGi", "cdate": 1546300800000, "mdate": null, "content": {"title": "Adversarially Trained End-to-End Korean Singing Voice Synthesis System", "abstract": "In this paper, we propose an end-to-end Korean singing voice synthesis system from lyrics and a symbolic melody using the following three novel approaches: 1) phonetic enhancement masking, 2) local conditioning of text and pitch to the super-resolution network, and 3) conditional adversarial training. The proposed system consists of two main modules; a mel-synthesis network that generates a mel-spectrogram from the given input information, and a super-resolution network that upsamples the generated mel-spectrogram into a linear-spectrogram. In the mel-synthesis network, phonetic enhancement masking is applied to generate implicit formant masks solely from the input text, which enables a more accurate phonetic control of singing voice. In addition, we show that two other proposed methods \u2014 local conditioning of text and pitch, and conditional adversarial training \u2014 are crucial for a realistic generation of the human singing voice in the super-resolution process. Finally, both quantitative and qualitative evaluations are conducted, confirming the validity of all proposed methods."}}
