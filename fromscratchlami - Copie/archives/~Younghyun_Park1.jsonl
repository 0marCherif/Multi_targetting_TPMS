{"id": "_g-_ts2q7jr", "cdate": 1683968739165, "mdate": 1683968739165, "content": {"title": "Distribution Aware Active Learning via Gaussian Mixtures", "abstract": "In this paper, we propose a distribution-aware active learning strategy that captures\nand mitigates the distribution discrepancy between the labeled and unlabeled sets\nto cope with overfitting. By taking advantage of gaussian mixture models (GMM)\nand Wasserstein distance, we first design a distribution-aware training strategy to\nimprove the model performance. Then, we introduce a hybrid informativeness\nmetric for active learning which considers both likelihood-based and model-based\ninformation simultaneously. Experimental results on four different datasets show\nthe effectiveness of our method against existing active learning baselines."}}
{"id": "4DsbHZmdnrQ", "cdate": 1676472363612, "mdate": null, "content": {"title": "Distribution Aware Active Learning via Gaussian Mixtures", "abstract": "In this paper, we propose a distribution-aware active learning strategy that captures and mitigates the distribution discrepancy between the labeled and unlabeled sets to cope with overfitting. By taking advantage of gaussian mixture models (GMM) and Wasserstein distance, we first design a distribution-aware training strategy to improve the model performance. Then, we introduce a hybrid informativeness metric for active learning which considers both likelihood-based and model-based information simultaneously. Experimental results on four different datasets show the effectiveness of our method against existing active learning baselines."}}
{"id": "reavxawuWwe", "cdate": 1672531200000, "mdate": 1696508921345, "content": {"title": "Active Learning for Object Detection with Evidential Deep Learning and Hierarchical Uncertainty Aggregation", "abstract": ""}}
{"id": "MnEjsw-vj-X", "cdate": 1663850272641, "mdate": null, "content": {"title": "Active Learning for Object Detection with Evidential Deep Learning and Hierarchical Uncertainty Aggregation", "abstract": "Despite the huge success of object detection, the training process still requires an immense amount of labeled data. Although various active learning solutions for object detection have been proposed, most existing works do not take advantage of epistemic uncertainty, which is an important metric for capturing the usefulness of the sample. Also, previous works pay little attention to the attributes of each bounding box (e.g., nearest object, box size) when computing the informativeness of an image. In this paper, we propose a new active learning strategy for object detection that overcomes the shortcomings of prior works. To make use of epistemic uncertainty, we adopt evidential deep learning (EDL) and propose a new module termed model evidence head (MEH), that makes EDL highly compatible with object detection. Based on the computed epistemic uncertainty of each bounding box, we propose hierarchical uncertainty aggregation (HUA) for obtaining the informativeness of an image. HUA realigns all bounding boxes into multiple levels based on the attributes and aggregates uncertainties in a bottom-up order, to effectively capture the context within the image. Experimental results show that our method outperforms existing state-of-the-art methods by a considerable margin."}}
{"id": "mY9x_i4dog8", "cdate": 1640995200000, "mdate": 1681733089907, "content": {"title": "Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot Segmentation", "abstract": "Few-shot learning allows machines to classify novel classes using only a few labeled samples. Recently, few-shot segmentation aiming at semantic segmentation on low sample data has also seen great interest. In this paper, we propose a learnable module that can be placed on top of existing segmentation networks for performing few-shot segmentation. This module, called the task-adaptive feature transformer (TAFT), linearly transforms task-specific high-level features to a set of task agnostic features well-suited to conducting few-shot segmentation. The task-conditioned feature transformation allows an effective utilization of the semantic information in novel classes to generate tight segmentation masks. We also propose a semantic enrichment (SE) module that utilizes a pixel-wise attention module for high-level feature and an auxiliary loss from an auxiliary segmentation network conducting the semantic segmentation for all training classes. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets confirm that the added modules successfully extend the capability of existing segmentators to yield highly competitive few-shot segmentation performances."}}
{"id": "BIQBDiYFrJ", "cdate": 1640995200000, "mdate": 1681673863784, "content": {"title": "Active Object Detection with Epistemic Uncertainty and Hierarchical Information Aggregation", "abstract": "Despite the huge success of object detection, the training process still requires an immense amount of labeled data. Active learning has been proposed as a practical solution, but existing works on active object detection do not utilize the concept of epistemic uncertainty, which is an important metric for capturing the usefulness of the sample. Previous works also pay little attention to the relation between bounding boxes when computing the informativeness of an image. In this paper, we propose a new active object detection strategy that improves these two shortcomings of existing methods. We specifically consider a Bayesian framework and propose a new module termed model evidence head (MEH), to take advantage of epistemic uncertainty in object detection. We also propose hierarchical uncertainty aggregation (HUA), which realigns all bounding boxes into multiple levels and aggregates uncertainties in a bottom-up order, to compute the informativeness of an image. Experimental results show that our method outperforms existing state-of-the-art methods by a considerable margin."}}
{"id": "ZgUZmeV1Mtu", "cdate": 1621630096841, "mdate": null, "content": {"title": "Few-Round Learning for Federated Learning", "abstract": "In federated learning (FL), a number of distributed clients targeting the same task collaborate to train a single global model without sharing their data. The learning process typically starts from a randomly initialized or some pretrained model. In this paper, we aim at designing an initial model based on which an arbitrary group of clients can obtain a global model for its own purpose, within only a few rounds of FL. The key challenge here is that the downstream tasks for which the pretrained model will be used are generally unknown when the initial model is prepared. Our idea is to take a meta-learning approach to construct the initial model so that any group with a possibly unseen task can obtain a high-accuracy global model within only R rounds of FL. Our meta-learning itself could be done via federated learning among willing participants and is based on an episodic arrangement to mimic the R rounds of FL followed by inference in each episode. Extensive experimental results show that our method generalizes well for arbitrary groups of clients and provides large performance improvements given the same overall communication/computation resources, compared to other baselines relying on known pretraining methods."}}
{"id": "EKRZjOhHJv", "cdate": 1609459200000, "mdate": 1668590548538, "content": {"title": "Few-Round Learning for Federated Learning", "abstract": "In federated learning (FL), a number of distributed clients targeting the same task collaborate to train a single global model without sharing their data. The learning process typically starts from a randomly initialized or some pretrained model. In this paper, we aim at designing an initial model based on which an arbitrary group of clients can obtain a global model for its own purpose, within only a few rounds of FL. The key challenge here is that the downstream tasks for which the pretrained model will be used are generally unknown when the initial model is prepared. Our idea is to take a meta-learning approach to construct the initial model so that any group with a possibly unseen task can obtain a high-accuracy global model within only R rounds of FL. Our meta-learning itself could be done via federated learning among willing participants and is based on an episodic arrangement to mimic the R rounds of FL followed by inference in each episode. Extensive experimental results show that our method generalizes well for arbitrary groups of clients and provides large performance improvements given the same overall communication/computation resources, compared to other baselines relying on known pretraining methods."}}
{"id": "5KaW5V3WSh6", "cdate": 1609459200000, "mdate": 1681733089788, "content": {"title": "CAFENet: Class-Agnostic Few-Shot Edge Detection Network", "abstract": ""}}
{"id": "gMRZ4wLqlkJ", "cdate": 1601308194927, "mdate": null, "content": {"title": "Few-Round Learning for Federated Learning", "abstract": "Federated learning (FL) presents an appealing opportunity for individuals who are willing to make their private data available for building a communal model without revealing their data contents to anyone else. Of central issues that may limit a widespread adoption of FL is the significant communication resources required in the exchange of updated model parameters between the server and individual clients over many communication rounds. In this work, we focus on limiting the number of model exchange rounds in FL to some small fixed number $R$, to control the communication burden. Following the spirit of meta-learning for few-shot learning, we take a meta-learning strategy to train the model so that once the meta-training phase is over, only $R$ rounds of FL would produce a model that will satisfy the needs of all participating clients. A key advantage of employing meta-training is that the main labeled dataset used in training could differ significantly (e.g., different classes of images) from the actual data sample presented at inference time. Compared to the meta-training approaches to optimize personalized local models at distributed devices, our method better handles the potential lack of data variability at individual nodes. Extensive experimental results indicate that meta-training geared to few-round learning provide large performance improvements compared to various baselines."}}
