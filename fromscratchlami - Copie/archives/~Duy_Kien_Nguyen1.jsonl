{"id": "J_PSiFUwcEs", "cdate": 1640995200000, "mdate": 1667923893747, "content": {"title": "BoxeR: Box-Attention for 2D and 3D Transformers", "abstract": "In this paper, we propose a simple attention mechanism, we call Box-Attention. It enables spatial interaction between grid features, as sampled from boxes of interest, and improves the learning capability of transformers for several vision tasks. Specifically, we present BoxeR, short for Box Transformer, which attends to a set of boxes by predicting their transformation from a reference window on an input feature map. The BoxeR computes attention weights on these boxes by considering its grid structure. Notably, BoxeR-2D naturally reasons about box information within its attention module, making it suitable for end-to-end instance detection and segmentation tasks. By learning invariance to rotation in the box-attention module, BoxeR-3D is capable of generating discriminative information from a bird's-eye view plane for 3D end-to-end object detection. Our experiments demonstrate that the proposed BoxeR-2D achieves state-of-the-art results on COCO detection and instance segmentation. Besides, BoxeR-3D improves over the end-to-end 3D object detection baseline and already obtains a compelling performance for the vehicle category of Waymo Open, without any class-specific optimization. Code is available at https://github.com/kienduynguyen/BoxeR."}}
{"id": "2nuJo2g5UP", "cdate": 1609459200000, "mdate": 1667923893764, "content": {"title": "MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond", "abstract": "This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for \u2018number\u2019 related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting."}}
{"id": "8e6BrwU6AjQ", "cdate": 1601308029819, "mdate": null, "content": {"title": "MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond", "abstract": "This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for \u2018number\u2019 related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting."}}
{"id": "tyzTSnkFjf", "cdate": 1546300800000, "mdate": 1667923893765, "content": {"title": "DAME WEB: DynAmic MEan with Whitening Ensemble Binarization for Landmark Retrieval without Human Annotation", "abstract": "In this work, we propose a simple yet effective module called DynAmic MEan (DAME) which allows a neural network to dynamically learn to aggregate feature maps at the pooling stage based on the input image, in order to generate global descriptors suitable for landmark retrieval. In contrast to Generalized Mean (GeM), which uses a predefined and static norm for pooling features into descriptors, we use a dynamic p-norm, with the p value being generated online by the model for each image. In addition, we utilize the introduced dynamic pooling method, to propose a novel feature whitening technique, Whitening Ensemble Binarization (WEB), to discover complementary information through multiple statistical projections. The memory cost of the proposed global binary descriptor is 8 times smaller than the state-of-the-art, while exhibiting similar or improved performance. To further demonstrate the power of DAME, we use it with features extracted from a fixed, pre-trained classification network, and illustrate that our dynamic p-norm is capable of learning to pool the classification features into global descriptors suitable for retrieval. Finally, by combining DAME with WEB, we achieve state-of-the-art results on challenging large-scale landmark retrieval benchmarks."}}
{"id": "F9YgcMwdHJ", "cdate": 1546300800000, "mdate": 1667923893767, "content": {"title": "Multi-Task Learning of Hierarchical Vision-Language Representation", "abstract": "It is still challenging to build an AI system that can perform tasks that involve vision and language at human level. So far, researchers have singled out individual tasks separately, for each of which they have designed networks and trained them on its dedicated datasets. Although this approach has seen a certain degree of success, it comes with difficulties of understanding relations among different tasks and transferring the knowledge learned for a task to others. We propose a multi-task learning approach that enables to learn vision-language representation that is shared by many tasks from their diverse datasets. The representation is hierarchical, and prediction for each task is computed from the representation at its corresponding level of the hierarchy. We show through experiments that our method consistently outperforms previous single-task-learning methods on image caption retrieval, visual question answering, and visual grounding. We also analyze the learned hierarchical representation by visualizing attention maps generated in our network."}}
{"id": "OtjcoDPfozF", "cdate": 1514764800000, "mdate": 1667924391654, "content": {"title": "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering", "abstract": "A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction."}}
