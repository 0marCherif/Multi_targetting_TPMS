{"id": "khjL3s9X7G", "cdate": 1640995200000, "mdate": 1681744310776, "content": {"title": "Updated Headline Generation: Creating Updated Summaries for Evolving News Stories", "abstract": ""}}
{"id": "h36qOz7OKJ", "cdate": 1640995200000, "mdate": 1681744310492, "content": {"title": "CoditT5: Pretraining for Source Code and Natural Language Editing", "abstract": "Pretrained language models have been shown to be effective in many software-related generation tasks; however, they are not well-suited for editing tasks as they are not designed to reason about edits. To address this, we propose a novel pretraining objective which explicitly models edits and use it to build CoditT5, a large language model for software-related editing tasks that is pretrained on large amounts of source code and natural language comments. We fine-tune it on various downstream editing tasks, including comment updating, bug fixing, and automated code review. By outperforming standard generation-based models, we demonstrate the generalizability of our approach and its suitability for editing tasks. We also show how a standard generation model and our edit-based model can complement one another through simple reranking strategies, with which we achieve state-of-the-art performance for the three downstream editing tasks."}}
{"id": "_lEzjfkNfe3", "cdate": 1640995200000, "mdate": 1681744310736, "content": {"title": "Learning to Describe Solutions for Bug Reports Based on Developer Discussions", "abstract": ""}}
{"id": "WlqM0xKQtfk", "cdate": 1640995200000, "mdate": 1681744310494, "content": {"title": "CoditT5: Pretraining for Source Code and Natural Language Editing", "abstract": "Pretrained language models have been shown to be effective in many software-related generation tasks; however, they are not well-suited for editing tasks as they are not designed to reason about edits. To address this, we propose a novel pretraining objective which explicitly models edits and use it to build CoditT5, a large language model for software-related editing tasks that is pretrained on large amounts of source code and natural language comments. We fine-tune it on various downstream editing tasks, including comment updating, bug fixing, and automated code review. By outperforming standard generation-based models, we demonstrate the generalizability of our approach and its suitability for editing tasks. We also show how a standard generation model and our edit-based model can complement one another through simple reranking strategies, with which we achieve state-of-the-art performance for the three downstream editing tasks."}}
{"id": "K7EiiAUd1K", "cdate": 1640995200000, "mdate": 1681744310497, "content": {"title": "Using Developer Discussions to Guide Fixing Bugs in Software", "abstract": "Automatically fixing software bugs is a challenging task. While recent work showed that natural language context is useful in guiding bug-fixing models, the approach required prompting developers to provide this context, which was simulated through commit messages written after the bug-fixing code changes were made. We instead propose using bug report discussions, which are available before the task is performed and are also naturally occurring, avoiding the need for any additional information from developers. For this, we augment standard bug-fixing datasets with bug report discussions. Using these newly compiled datasets, we demonstrate that various forms of natural language context derived from such discussions can aid bug-fixing, even leading to improved performance over using commit messages corresponding to the oracle bug-fixing commits."}}
{"id": "Cc1W81nQMn", "cdate": 1640995200000, "mdate": 1681744310565, "content": {"title": "Using Developer Discussions to Guide Fixing Bugs in Software", "abstract": ""}}
{"id": "Yx8nj6am8z", "cdate": 1609459200000, "mdate": null, "content": {"title": "Learning to Generate Code Comments from Class Hierarchies", "abstract": "Descriptive code comments are essential for supporting code comprehension and maintenance. We propose the task of automatically generating comments for overriding methods. We formulate a novel framework which accommodates the unique contextual and linguistic reasoning that is required for performing this task. Our approach features: (1) incorporating context from the class hierarchy; (2) conditioning on learned, latent representations of specificity to generate comments that capture the more specialized behavior of the overriding method; and (3) unlikelihood training to discourage predictions which do not conform to invariant characteristics of the comment corresponding to the overridden method. Our experiments show that the proposed approach is able to generate comments for overriding methods of higher quality compared to prevailing comment generation techniques."}}
{"id": "Y5egCgbt26r", "cdate": 1609459200000, "mdate": 1681744310605, "content": {"title": "Deep Just-In-Time Inconsistency Detection Between Comments and Source Code", "abstract": "Natural language comments convey key aspects of source code such as implementation, usage, and pre- and post-conditions. Failure to update comments accordingly when the corresponding code is modified introduces inconsistencies, which is known to lead to confusion and software bugs. In this paper, we aim to detect whether a comment becomes inconsistent as a result of changes to the corresponding body of code, in order to catch potential inconsistencies just-in-time, i.e., before they are committed to a code base. To achieve this, we develop a deep-learning approach that learns to correlate a comment with code changes. By evaluating on a large corpus of comment/code pairs spanning various comment types, we show that our model outperforms multiple baselines by significant margins. For extrinsic evaluation, we show the usefulness of our approach by combining it with a comment update model to build a more comprehensive automatic comment maintenance system which can both detect and resolve inconsistent comments based on code changes."}}
{"id": "Qm2MAlUpi3", "cdate": 1609459200000, "mdate": 1681744310488, "content": {"title": "Copy That! Editing Sequences by Copying Spans", "abstract": "Neural sequence-to-sequence models are finding increasing use in editing of documents, for example in correcting a text document or repairing source code. In this paper, we argue that common seq2seq models (with a facility to copy single tokens) are not a natural fit for such tasks, as they have to explicitly copy each unchanged token. We present an extension of seq2seq models capable of copying entire spans of the input to the output in one step, greatly reducing the number of decisions required during inference. This extension means that there are now many ways of generating the same output, which we handle by deriving a new objective for training and a variation of beam search for inference that explicitly handles this problem. In our experiments on a range of editing tasks of natural language and source code, we show that our new model consistently outperforms simpler baselines."}}
{"id": "03RYIRhzbvR", "cdate": 1609459200000, "mdate": 1681744310701, "content": {"title": "Learning to Describe Solutions for Bug Reports Based on Developer Discussions", "abstract": "When a software bug is reported, developers engage in a discussion to collaboratively resolve it. While the solution is likely formulated within the discussion, it is often buried in a large amount of text, making it difficult to comprehend and delaying its implementation. To expedite bug resolution, we propose generating a concise natural language description of the solution by synthesizing relevant content within the discussion, which encompasses both natural language and source code. We build a corpus for this task using a novel technique for obtaining noisy supervision from repository changes linked to bug reports, with which we establish benchmarks. We also design two systems for generating a description during an ongoing discussion by classifying when sufficient context for performing the task emerges in real-time. With automated and human evaluation, we find this task to form an ideal testbed for complex reasoning in long, bimodal dialogue context."}}
