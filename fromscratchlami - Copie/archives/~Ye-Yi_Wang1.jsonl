{"id": "Ydss0WTsgF9", "cdate": 1650477651129, "mdate": 1650477651129, "content": {"title": "Metadata-Induced Contrastive Learning for Zero-Shot Extreme Multi-Label Text Classification", "abstract": "Extreme multi-label text classification (XMTC) aims to associate a document with its relevant labels from a large candidate set. Most existing XMTC approaches rely on massive human-annotated training data, which are often costly to obtain and suffer from a long-tailed label distribution (i.e., many labels occur only a few times in the training set). In this paper, we study XMTC under the zero-shot setting, which does not require any annotated documents with labels and only relies on label surface names and descriptions. To train a classifier that calculates the similarity score between a document and a label, we propose a novel metadata-induced contrastive learning (MICoL) method. Different from previous textbased contrastive learning techniques, MICoL exploits document metadata (e.g., authors, venue, and references between documents), which are widely available on the Web, to derive similar document\u2013document pairs. Experimental results on two large-scale datasets show that: (1) MICoL significantly outperforms strong zero-shot text classification and contrastive learning baselines by up to 6.0%; (2) MICoL is on par with the supervised metadata-aware XMTC method trained on 10K\u2013200K labeled documents; and (3) MICoL tends to predict more infrequent labels than supervised methods, thus alleviates the deteriorated performance on long-tailed labels."}}
{"id": "rkVxIQb_bB", "cdate": 1420070400000, "mdate": null, "content": {"title": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval", "abstract": "Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation."}}
{"id": "HJNZ4U-uWr", "cdate": 1230768000000, "mdate": null, "content": {"title": "Extracting structured information from user queries with semi-supervised conditional random fields", "abstract": "When search is against structured documents, it is beneficial to extract information from user queries in a format that is consistent with the backend data structure. As one step toward this goal, we study the problem of query tagging which is to assign each query term to a pre-defined category. Our problem could be approached by learning a conditional random field (CRF) model (or other statistical models) in a supervised fashion, but this would require substantial human-annotation effort. In this work, we focus on a semi-supervised learning method for CRFs that utilizes two data sources: (1) a small amount of manually-labeled queries, and (2) a large amount of queries in which some word tokens have derived labels, i.e., label information automatically obtained from additional resources. We present two principled ways of encoding derived label information in a CRF model. Such information is viewed as hard evidence in one setting and as soft evidence in the other. In addition to the general methodology of how to use derived labels in semi-supervised CRFs, we also present a practical method on how to obtain them by leveraging user click data and an in-domain database that contains structured documents. Evaluation on product search queries shows the effectiveness of our approach in improving tagging accuracies."}}
{"id": "rkEs7HWObr", "cdate": 1199145600000, "mdate": null, "content": {"title": "Learning query intent from regularized click graphs", "abstract": "This work presents the use of click graphs in improving query intent classifiers, which are critical if vertical search and general-purpose search services are to be offered in a unified user interface. Previous works on query classification have primarily focused on improving feature representation of queries, e.g., by augmenting queries with search engine results. In this work, we investigate a completely orthogonal approach --- instead of enriching feature representation, we aim at drastically increasing the amounts of training data by semi-supervised learning with click graphs. Specifically, we infer class memberships of unlabeled queries from those of labeled ones according to their proximities in a click graph. Moreover, we regularize the learning with click graphs by content-based classification to avoid propagating erroneous labels. We demonstrate the effectiveness of our algorithms in two different applications, product intent and job intent classification. In both cases, we expand the training data with automatically labeled queries by over two orders of magnitude, leading to significant improvements in classification performance. An additional finding is that with a large amount of training data obtained in this fashion, classifiers using only query words/phrases as features can work remarkably well."}}
{"id": "S1VHfXWdbH", "cdate": 1167609600000, "mdate": null, "content": {"title": "Voice-Rate: A Dialog System for Consumer Ratings", "abstract": "Geoffrey Zweig, Y.C. Ju, Patrick Nguyen, Dong Yu, Ye-Yi Wang, Alex Acero. Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT). 2007."}}
{"id": "rkVzd2eO-B", "cdate": 1136073600000, "mdate": null, "content": {"title": "Combining Statistical and Knowledge-Based Spoken Language Understanding in Conditional Models", "abstract": "Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance. The traditional knowledge-based approach to this problem is very expensive --- it requires joint expertise in natural language processing and speech recognition, and best practices in language engineering for every new domain. On the other hand, a statistical learning approach needs a large amount of annotated data for model training, which is seldom available in practical applications outside of large research labs. A generative HMM/CFG composite model, which integrates easy-to-obtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement. The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework. We also study and compare conditional random fields (CRFs) with perceptron learning for SLU. Experimental results show that the conditional models achieve more than 20% relative reduction in slot error rate over the HMM/CFG model, which had already achieved an SLU accuracy at the same level as the best results reported on the ATIS data."}}
{"id": "HJVjMXZuWr", "cdate": 1072915200000, "mdate": null, "content": {"title": "Use and Acquisition of Semantic Language Model", "abstract": "Semantic language model is a technique that utilizes the semantic structure of an utterance to better rank the likelihood of words composing the sentence. When used in a conversational system, one can dynamically integrate the dialog state and domain semantics into the semantic language model to better guide the speech recognizer executing the decoding process. We describe one such application that employs semantic language model to cope with spontaneous speech in a robust manner. The semantic language model, though can be manually crafted without data, can benefit significantly from data driven machine learning techniques. An example based approach is also described here to demonstrate a viable approach."}}
{"id": "rybYkaldbS", "cdate": 883612800000, "mdate": null, "content": {"title": "Modeling with Structures in Statistical Machine Translation", "abstract": "Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based alignment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task."}}
{"id": "HJWlsnlOWH", "cdate": 852076800000, "mdate": null, "content": {"title": "Decoding Algorithm in Statistical Machine Translation", "abstract": "Decoding algorithm is a crucial part in statistical machine translation. We describe a stack decoding algorithm in this paper. We present the hypothesis scoring method and the heuristics used in our algorithm. We report several techniques deployed to improve the performance of the decoder. We also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process. We evaluate and compare these techniques/models in our statistical machine translation system."}}
{"id": "rkbJZaxuWr", "cdate": 757382400000, "mdate": null, "content": {"title": "Dual-Coding Theory and Connectionist Lexical Selection", "abstract": "We introduce the bilingual dual-coding theory as a model for bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation."}}
