{"id": "g6tA0v_RnO", "cdate": 1697449761019, "mdate": 1697449761019, "content": {"title": "Personalized ranking for non-uniformly sampled items", "abstract": "We develop an adapted version of the Bayesian Personalized Ranking (BPR) optimization criterion (Rendle et\u00c2 al., \u00c2 2009) that takes the non-uniform sampling of negative test items\u2013as in track 2 of the KDD Cup 2011\u2013into account. Furthermore, we present a modified version of the generic BPR learning algorithm that maximizes the new criterion. We use it to train ranking matrix factorization models as components of an ensemble. Additionally, we combine the ranking predictions with rating prediction models to also take into account rating data. With an ensemble of such combined models, we ranked 8th (out of more than 300 teams) in track 2 of the KDD Cup 2011, without using the additional taxonomic information offered by the competition organizers."}}
{"id": "Y4ZQN1O75O6", "cdate": 1697449687170, "mdate": 1697449687170, "content": {"title": "Learning attribute-to-feature mappings for cold-start recommendations", "abstract": "Cold-start scenarios in recommender systems are\nsituations in which no prior events, like ratings or clicks, are\nknown for certain users or items. To compute predictions in\nsuch cases, additional information about users (user attributes,\ne.g. gender, age, geographical location, occupation) and items\n(item attributes, e.g. genres, product categories, keywords) must\nbe used.\nWe describe a method that maps such entity (e.g. user or\nitem) attributes to the latent features of a matrix (or higher-\ndimensional) factorization model. With such mappings, the\nfactors of a MF model trained by standard techniques can\nbe applied to the new-user and the new-item problem, while\nretaining its advantages, in particular speed and predictive\naccuracy.\nWe use the mapping concept to construct an attribute-\naware matrix factorization model for item recommendation\nfrom implicit, positive-only feedback. Experiments on the new-\nitem problem show that this approach provides good predictive\naccuracy, while the prediction time only grows by a constant\nfactor."}}
{"id": "zLQBFqtimc", "cdate": 1697449545046, "mdate": 1697449545046, "content": {"title": "MyMediaLite: A free recommender system library", "abstract": "MyMediaLite is a fast and scalable, multi-purpose library of recommender system algorithms, aimed both at recommender system researchers and practitioners. It addresses two common scenarios in collaborative filtering: rating prediction (e.g. on a scale of 1 to 5 stars) and item prediction from positive-only implicit feedback (e.g. from clicks or purchase actions). The library offers state-of-the-art algorithms for those two tasks. Programs that expose most of the library's functionality, plus a GUI demo, are included in the package. Efficient data structures and a common API are used by the implemented algorithms, and may be used to implement further algorithms. The API also contains methods for real-time updates and loading/storing of already trained recommender models."}}
{"id": "a-bD9-0ycs0", "cdate": 1663850579424, "mdate": null, "content": {"title": "Latent Linear ODEs with Neural Kalman Filtering for Irregular Time Series Forecasting", "abstract": "Over the past four years, models based on Neural Ordinary Differential Equations have become state of the art in the forecasting of irregularly sampled time series. Describing the data-generating process as a dynamical system in continuous time allows predictions at arbitrary time points. However, the numerical integration of Neural ODEs typically comes with a high computational burden or may even fail completely. We propose a novel Neural ODE model that embeds the observations into a latent space with dynamics governed by a linear ODE. Consequently, we do not require any specialized numerical integrator but only an implementation of the matrix exponential readily available in many numerical linear algebra libraries. We also introduce a novel state update component inspired by the classical Kalman filter, which, to our knowledge, makes our model the first Neural ODE variant to explicitly satisfy a specific self-consistency property. It allows forecasting irregularly sampled time series with missing values and comes with some numerical stability guarantees. We evaluate the performance on medical and climate benchmark datasets, where the model outperforms the state of the art by margins up to 30%."}}
{"id": "wfD-fMD4StU", "cdate": 1633015336104, "mdate": null, "content": {"title": "Transfer Learning for Bayesian HPO with End-to-End Landmark Meta-Features", "abstract": "Hyperparameter optimization (HPO) is a crucial component of deploying machine learning models, however, it remains an open problem due to the resource-constrained number of possible hyperparameter evaluations. As a result, prior work focus on exploring the direction of transfer learning for tackling the sample inefficiency of HPO. In contrast to existing approaches, we propose a novel Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. We design DKLM to capture the similarity between hyperparameter configurations with an end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance. As a result, our novel DKLM can learn contextualized dataset-specific similarity representations for hyperparameter configurations. We experimentally validate the performance of DKLM in a wide range of HPO meta-datasets from OpenML and demonstrate the empirical superiority of our method against a series of state-of-the-art baselines."}}
{"id": "4l5iO9eoh3f", "cdate": 1632875640413, "mdate": null, "content": {"title": "Supervised Permutation Invariant Networks for solving the CVRP with bounded fleet size", "abstract": "Learning to solve combinatorial optimization problems, such as the vehicle routing problem, offers great computational advantages over classical operation research solvers and heuristics. The recently developed deep reinforcement learning approaches either improve an initially given solution iteratively or sequentially construct a set of individual tours. \nHowever, all existing learning-based approaches are not able to work for a fixed number of vehicles and thus bypass the NP-hardness of the original problem. On the other hand, this makes them less suitable for real applications, as many logistic service providers rely on solutions provided for a specific bounded fleet size and cannot accommodate short term changes to the number of vehicles.\nIn contrast we propose a powerful supervised deep learning framework that constructs a complete tour plan from scratch while respecting an apriori fixed number of vehicles. \nIn combination with an efficient post-processing scheme, our supervised approach is not only much faster and easier to train but also achieves competitive results that incorporate the practical aspect of vehicle costs.\nIn thorough controlled experiments we re-evaluate and compare our method to multiple state-of-the-art approaches where we demonstrate stable performance and shed some light on existent inconsistencies in the experimentation protocols of the related work."}}
{"id": "dYUdt59fJ0e", "cdate": 1632875547405, "mdate": null, "content": {"title": "Yformer: U-Net Inspired Transformer Architecture for Far Horizon Time Series Forecasting", "abstract": "Time series data is ubiquitous in research as well as in a wide variety of industrial applications. Effectively analyzing the available historical data and providing insights into the far future allows us to make effective decisions. Recent research has witnessed the superior performance of transformer-based architectures, especially in the regime of far horizon time series forecasting. However, the current state of the art sparse Transformer architectures fail to couple down- and upsampling procedures to produce outputs in a similar resolution as the input. We propose the Yformer model, based on a novel Y-shaped encoder-decoder architecture that (1) uses direct connection from the downscaled encoder layer to the corresponding upsampled decoder layer in a U-Net inspired architecture, (2) Combines the downscaling/upsampling with sparse attention to capture long-range effects, and (3) stabilizes the encoder-decoder stacks with the addition of an auxiliary reconstruction loss. Extensive experiments have been conducted with relevant baselines on four benchmark datasets, demonstrating an average improvement of 19.82, 18.41 percentage MSE and 13.62, 11.85 percentage MAE in comparison to the current state of the art for the univariate and the multivariate settings respectively."}}
{"id": "wronZ3Mx_d", "cdate": 1632875435840, "mdate": null, "content": {"title": "Transfer Learning for Bayesian HPO with End-to-End Meta-Features", "abstract": "Hyperparameter optimization (HPO) is a crucial component of deploying machine learning models, however, it remains an open problem due to the resource-constrained number of possible hyperparameter evaluations. As a result, prior work focus on exploring the direction of transfer learning for tackling the sample inefficiency of HPO. In contrast to existing approaches, we propose a novel Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) that can be jointly meta-trained on a set of source tasks and then transferred efficiently on a new (unseen) target task. We design DKLM to capture the similarity between hyperparameter configurations with an end-to-end meta-feature network that embeds the set of evaluated configurations and their respective performance. As a result, our novel DKLM can learn contextualized dataset-specific similarity representations for hyperparameter configurations. We experimentally validate the performance of DKLM in a wide range of HPO meta-datasets from OpenML and demonstrate the empirical superiority of our method against a series of state-of-the-art baselines."}}
{"id": "X2V7RW3Sul", "cdate": 1632875431865, "mdate": null, "content": {"title": "Improving Hyperparameter Optimization by Planning Ahead", "abstract": "Hyperparameter optimization (HPO) is generally treated as a bi-level optimization problem that involves fitting a (probabilistic) surrogate model to a set of observed hyperparameter responses, e.g. validation loss, and consequently maximizing an acquisition function using a surrogate model to identify good hyperparameter candidates for evaluation. The choice of a surrogate and/or acquisition function can be further improved via knowledge transfer across related tasks. In this paper, we propose a novel transfer learning approach, defined within the context of model-based reinforcement learning, where we represent the surrogate as an ensemble of probabilistic models that allows trajectory sampling. We further propose a new variant of model predictive control which employs a simple look-ahead strategy as a policy that optimizes a sequence of actions, representing hyperparameter candidates to expedite HPO. Our experiments on three meta-datasets comparing to state-of-the-art HPO algorithms including a model-free reinforcement learning approach show that the proposed method can outperform all baselines by exploiting a simple planning-based policy. "}}
{"id": "8wRDNRsI2HA", "cdate": 1623402197600, "mdate": null, "content": {"title": "Hyperparameter Optimization with Differentiable Metafeatures", "abstract": "Metafeatures, or dataset characteristics, have been shown to improve the performance of hyperparameter optimization (HPO). Conventionally, metafeatures are precomputed and used to measure the similarity between datasets, leading to a better initialization of HPO models. In this paper, we propose a cross dataset surrogate model called Differentiable Metafeature-based Surrogate (DMFBS), that predicts the hyperparameter response, i.e. validation loss, of a model trained on the dataset at hand. In contrast to existing models, DMFBS i) integrates a differentiable metafeature extractor and ii) is optimized using a novel multi-task loss, linking manifold regularization with a dataset similarity measure learned via an auxiliary dataset identification meta-task, effectively enforcing the response approximation for similar datasets to be similar. We compare DMFBS against several recent models for HPO on three large meta-datasets and show that it consistently outperforms all of them with an average 10% improvement. Finally, we provide an extensive ablation study that examines the different components of our approach."}}
