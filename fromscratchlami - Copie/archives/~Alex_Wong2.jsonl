{"id": "1zE9GSjxno", "cdate": 1683912726328, "mdate": 1683912726328, "content": {"title": "Small Lesion Segmentation in Brain MRIs with Subpixel Embedding", "abstract": "We present a method to segment MRI scans of the human brain into ischemic stroke lesion and normal tissues. We propose a neural network architecture in the form of a standard encoder-decoder where predictions are guided by a spatial expansion embedding network. Our embedding network learns features that can resolve detailed structures in the brain without the need for high-resolution training images, which are often unavailable and expensive to acquire. Alternatively, the encoder-decoder learns global structures by means of striding and max pooling. Our embedding network complements the encoder-decoder architecture by guiding the decoder with fine-grained details lost to spatial downsampling during the encoder stage. Unlike previous works, our decoder outputs at 2 times the input resolution, where a single pixel in the input resolution is predicted by four neighboring subpixels in our output. To obtain the output at the original scale, we propose a learnable downsampler (as opposed to hand-crafted ones e.g. bilinear) that combines subpixel predictions. Our approach improves the baseline architecture by approximately 11.7% and achieves the state of the art on the ATLAS public benchmark dataset with a smaller memory footprint and faster runtime than the best competing method. Our source code has been made available at: https://github.com/alexklwong/subpixel-embedding-segmentation."}}
{"id": "Sv-JmNUSsV", "cdate": 1668888511716, "mdate": 1668888511716, "content": {"title": "Stereoscopic Universal Perturbations across Different Architectures and Datasets", "abstract": "We study the effect of adversarial perturbations of images on deep stereo matching networks for the disparity estimation task. We present a method to craft a single set of perturbations that, when added to any stereo image pair in a dataset, can fool a stereo network to significantly alter the perceived scene geometry. Our perturbation images are \u201cuniversal\u201d in that they not only corrupt estimates\nof the network on the dataset they are optimized for, but also generalize to different architectures trained on different datasets. We evaluate our approach on multiple benchmark datasets where our perturbations can increase the D1-error (akin to fooling rate) of state-of-the-art stereo networks from 1% to as much as 87%. We investigate the effect of perturbations on the estimated scene geometry and identify object classes that are most vulnerable. Our analysis on the activations of registered points between left and right images led us to find architectural components that can increase robustness against adversaries. By simply designing networks with such components, one can reduce the effect of adversaries by up to 60.5%, which rivals the robustness of networks fine-tuned with costly adversarial data augmentation. Our design principle also improves their robustness against common image corruptions by an average of 70%"}}
{"id": "sTa1nyGuxs", "cdate": 1668888194382, "mdate": 1668888194382, "content": {"title": "Monitored Distillation for Positive Congruent Depth Completion", "abstract": "We propose a method to infer a dense depth map from a single image, its calibration, and the associated sparse point cloud. In order\nto leverage existing models (teachers) that produce putative depth maps, we propose an adaptive knowledge distillation approach that yields a positive congruent training process, wherein a student model avoids learning the error modes of the teachers. In the absence of ground truth for model selection and training, our method, termed Monitored Distillation, allows a student to exploit a blind ensemble of teachers by selectively learning from predictions that best minimize the reconstruction error for a given image. Monitored Distillation yields a distilled depth map and a confidence map, or \u201cmonitor\u201d, for how well a prediction from a particular teacher fits the observed image. The monitor adaptively weights the distilled depth where if all of the teachers exhibit high residuals, the standard unsupervised image reconstruction loss takes over as the supervisory signal. On indoor scenes (VOID), we outperform blind ensembling baselines by 17.53% and unsupervised methods by 24.25%; we boast a 79% model size reduction while maintaining comparable performance to the best supervised method. For outdoors (KITTI), we tie for 5th overall on the benchmark despite not using ground truth. Code available at:\nhttps://github.com/alexklwong/mondi-python."}}
{"id": "Q69Aj3P0PtV", "cdate": 1668734801605, "mdate": null, "content": {"title": "Evaluating Worst Case Adversarial Weather Perturbations Robustness", "abstract": "Several algorithms are proposed to improve the robustness of deep neural networks against adversarial perturbations beyond $\\ell_p$ cases, i.e. weather perturbations. However, evaluations of existing robust training algorithms are over-optimistic. This is in part due to the lack of a standardized evaluation protocol across various robust training algorithms, leading to ad-hoc methods that test robustness on either random perturbations or the adversarial samples from generative models that are used for robust training, which is either uninformative of the worst case, or is heavily biased.\nIn this paper, we identify such evaluation bias in these existing works and propose the first standardized and fair evaluation that compares various robust training algorithms by using physics simulators for common adverse weather effects i.e. rain and snow.\nWith this framework, we evaluated several existing robust training algorithms on two streetview classification datasets (BIC\\_GSV, Places365) and show the evaluation bias in experiments."}}
{"id": "fOfUe90j0r", "cdate": 1667590197239, "mdate": 1667590197239, "content": {"title": "Not Just Streaks: Towards Ground Truth for Single Image Deraining", "abstract": "We propose a large-scale dataset of real-world rainy and clean image pairs and a method to remove degradations, induced by rain streaks and rain accumulation, from the image. As there exists no real-world dataset for deraining, current state-of-the-art methods rely on synthetic data and thus are limited by the sim2real domain gap; moreover, rigorous evaluation remains a challenge due to the absence of a real paired dataset. We fill this gap by collecting a real paired deraining dataset through meticulous control of non-rain variations. Our dataset enables paired training and quantitative evaluation for diverse real-world rain phenomena (e.g. rain streaks and rain accumulation). To learn a representation robust to rain phenomena, we propose a deep neural network that reconstructs the underlying scene by minimizing a rain-robust loss between rainy and clean images. Extensive experiments demonstrate that our model outperforms the state-of-the-art deraining methods on real rainy images under various conditions. Project website: https://visual.ee.ucla.edu/gt_rain.htm/."}}
{"id": "Pk_di2bPAop", "cdate": 1663850530281, "mdate": null, "content": {"title": "On the Adversarial Robustness against Natural Weather Perturbations", "abstract": "Several algorithms are proposed to improve the robustness of deep neural networks against adversarial perturbations beyond $\\ell_p$ cases, i.e. weather perturbations. However, evaluations of existing robust training algorithms are over-optimistic. This is in part due to the lack of a standardized evaluation protocol across various robust training algorithms, leading to ad-hoc methods that test robustness on either random perturbations or the adversarial samples from generative models that are used for robust training, which is either uninformative of the worst case, or is heavily biased.\nIn this paper, we identify such evaluation bias in these existing works and propose the first standardized and fair evaluation that compares various robust training algorithms by using physics simulators for common adverse weather effects i.e. rain and snow. Additionally, our framework identified the lack of diversity in existing robust training algorithms. As a step to address this, we propose a light-weight generative adversarial network (GAN) with improved diverse weather effects controlled by latent codes that can be used in robust training.\nThe proposed robust training algorithm is evaluated on two streetview classification datasets (BIC\\_GSV, Places365), where it outperforms other robust training approaches based on generative models for worst-case adversarial rain and snow attacks."}}
{"id": "wMQJrezwC9K", "cdate": 1640995200000, "mdate": 1668537163923, "content": {"title": "Does Monocular Depth Estimation Provide Better Pre-training than Classification for Semantic Segmentation?", "abstract": "Training a deep neural network for semantic segmentation is labor intensive, so it is common to pre-train on a different task for which data is abundant, typically image-level classification, and then fine-tune with a small annotated dataset. There is empirical evidence showing that incorporating depth information during training may improve semantic segmentation, but the extent of this effect has yet to be fully characterized. In this paper, we study whether monocular depth estimation can serve as pre-training for semantic segmentation, ideally eliminating the need for manually supervised pre-training. Using common benchmarks such as KITTI, Cityscapes, and NYU-V2, we evaluate pre-training using depth estimation vs. classification, measuring their effects on downstream semantic segmentation. The former edges out the latter by 5.8\\% mIoU and 5.2\\% pixel accuracy. We analyze the impact of different forms of supervision for depth estimation, training pipelines, and data resolution on semantic fine-tuning. Additionally, we find that other forms of self-supervision are less effective than depth pre-training, including optical flow, despite sharing the same loss, namely the photometric reprojection error."}}
{"id": "m74YpC8odx", "cdate": 1640995200000, "mdate": 1668537163922, "content": {"title": "Stereoscopic Universal Perturbations across Different Architectures and Datasets", "abstract": "We study the effect of adversarial perturbations of images on deep stereo matching networks for the disparity estimation task. We present a method to craft a single set of perturbations that, when added to any stereo image pair in a dataset, can fool a stereo network to significantly alter the perceived scene geometry. Our perturbation images are \u201cuniversal\u201d in that they not only corrupt estimates of the network on the dataset they are optimized for, but also generalize to different architectures trained on different datasets. We evaluate our approach on multiple benchmark datasets where our perturbations can increase the D1-error (akin to fooling rate) of state-of-the-art stereo networks from 1% to as much as 87%. We investigate the effect of perturbations on the estimated scene geometry and identify object classes that are most vulnerable. Our analysis on the activations of registered points between left and right images led us to find architectural components that can increase robustness against adversaries. By simply designing networks with such components, one can reduce the effect of adversaries by up to 60.5%, which rivals the robustness of networks finetuned with costly adversarial data augmentation. Our design principle also improves their robustness against common image corruptions by an average of 70%."}}
{"id": "ZuUf2di6Qd", "cdate": 1640995200000, "mdate": 1668537163922, "content": {"title": "Monitored Distillation for Positive Congruent Depth Completion", "abstract": "We propose a method to infer a dense depth map from a single image, its calibration, and the associated sparse point cloud. In order to leverage existing models (teachers) that produce putative depth maps, we propose an adaptive knowledge distillation approach that yields a positive congruent training process, wherein a student model avoids learning the error modes of the teachers. In the absence of ground truth for model selection and training, our method, termed Monitored Distillation, allows a student to exploit a blind ensemble of teachers by selectively learning from predictions that best minimize the reconstruction error for a given image. Monitored Distillation yields a distilled depth map and a confidence map, or ``monitor'', for how well a prediction from a particular teacher fits the observed image. The monitor adaptively weights the distilled depth where if all of the teachers exhibit high residuals, the standard unsupervised image reconstruction loss takes over as the supervisory signal. On indoor scenes (VOID), we outperform blind ensembling baselines by 17.53% and unsupervised methods by 24.25%; we boast a 79% model size reduction while maintaining comparable performance to the best supervised method. For outdoors (KITTI), we tie for 5th overall on the benchmark despite not using ground truth. Code available at: https://github.com/alexklwong/mondi-python."}}
{"id": "VwRF7ga0cy", "cdate": 1640995200000, "mdate": 1668537163923, "content": {"title": "Towards Ground Truth for Single Image Deraining", "abstract": "We propose a large-scale dataset of real-world rainy and clean image pairs and a method to remove degradations, induced by rain streaks and rain accumulation, from the image. As there exists no real-world dataset for deraining, current state-of-the-art methods rely on synthetic data and thus are limited by the sim2real domain gap; moreover, rigorous evaluation remains a challenge due to the absence of a real paired dataset. We fill this gap by collecting a real paired deraining dataset through meticulous control of non-rain variations. Our dataset enables paired training and quantitative evaluation for diverse real-world rain phenomena (e.g. rain streaks and rain accumulation). To learn a representation robust to rain phenomena, we propose a deep neural network that reconstructs the underlying scene by minimizing a rain-robust loss between rainy and clean images. Extensive experiments demonstrate that our model outperforms the state-of-the-art deraining methods on real rainy images under various conditions. Project website: https://visual.ee.ucla.edu/gt_rain.htm/."}}
