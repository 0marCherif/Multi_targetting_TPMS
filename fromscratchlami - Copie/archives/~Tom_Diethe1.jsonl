{"id": "C7gt7kVmMQc", "cdate": 1633790966433, "mdate": null, "content": {"title": "Continual Density Ratio Estimation", "abstract": "In online applications with streaming data, awareness of how far the empirical training or test data has shifted away from its original data distribution can be crucial to the performance of the model. However, historical samples in the data stream may not be kept either due to space requirements or for regulatory reasons. To cope with such situations, we propose Continual Density Ratio Estimation (CDRE), for estimating density ratios between the initial and latest distributions (p/q_t) of a data stream without the need of storing past samples, where q_t shifted away from p after a time period t. In particular, CDRE is more accurate than standard DRE when the two distributions are less similar, despite not requiring samples from the original distribution. CDRE can be applied in scenarios of online or continual learning, such as importance weighted covariate shift, measuring dataset changes for better decision making.  "}}
{"id": "P1oKj7dPzP", "cdate": 1609459200000, "mdate": 1681679281902, "content": {"title": "Continual Density Ratio Estimation in an Online Setting", "abstract": "In online applications with streaming data, awareness of how far the training or test set has shifted away from the original dataset can be crucial to the performance of the model. However, we may not have access to historical samples in the data stream. To cope with such situations, we propose a novel method, Continual Density Ratio Estimation (CDRE), for estimating density ratios between the initial and current distributions ($p/q_t$) of a data stream in an iterative fashion without the need of storing past samples, where $q_t$ is shifting away from $p$ over time $t$. We demonstrate that CDRE can be more accurate than standard DRE in terms of estimating divergences between distributions, despite not requiring samples from the original distribution. CDRE can be applied in scenarios of online learning, such as importance weighted covariate shift, tracing dataset changes for better decision making. In addition, (CDRE) enables the evaluation of generative models under the setting of continual learning. To the best of our knowledge, there is no existing method that can evaluate generative models in continual learning without storing samples from the original distribution."}}
{"id": "KG4igOosnw8", "cdate": 1601308120851, "mdate": null, "content": {"title": "Discriminative Representation Loss (DRL): A More Efficient Approach than Gradient Re-Projection in Continual Learning", "abstract": "The use of episodic memories in continual learning has been shown to be effective in terms of alleviating catastrophic forgetting. In recent studies, several gradient-based approaches have been developed to make more efficient use of compact episodic memories, which constrain the gradients resulting from new samples with those from memorized samples, aiming to reduce the diversity of gradients from different tasks.  In this paper, we reveal the relation between diversity of gradients and discriminativeness of representations, demonstrating connections between Deep Metric Learning and continual learning. Based on these findings, we propose a simple yet efficient method -- Discriminative Representation Loss (DRL) -- for continual learning. In comparison with several state-of-the-art methods, this method shows effectiveness with  low computational cost on multiple benchmark experiments in the setting of online continual learning."}}
{"id": "vL6FgOWre9n", "cdate": 1598940709015, "mdate": null, "content": {"title": "Facilitating Bayesian Continual Learning by Natural Gradients and Stein Gradients", "abstract": "Continual learning aims to enable machine learning models to learn a general solution space for past and future tasks in a sequential manner. Conventional models tend to forget the knowledge of previous tasks while learning a new task, a phenomenon known as catastrophic forgetting. When using Bayesian models in continual learning, knowledge from previous tasks can be retained in two ways: 1). posterior distributions over the parameters, containing the knowledge gained from inference in previous tasks, which then serve as the priors for the following task; 2). coresets, containing knowledge of data distributions of previous tasks. Here, we show that Bayesian continual learning can be facilitated in terms of these two means through the use of natural gradients and Stein gradients respectively."}}
{"id": "ou7uDDPl0Yu", "cdate": 1598658082069, "mdate": null, "content": {"title": "Similarity of Neural Networks with Gradients", "abstract": "A suitable similarity index for comparing learnt neural networks plays an important role in understanding the behaviour of the highly-nonlinear functions, and can provide insights on further theoretical analysis and empirical studies. We define two key steps when comparing models: firstly, the representation abstracted from the learnt model, where we propose to leverage both feature vectors and gradient ones (which are largely ignored in prior work) into designing the representation of a neural network.  Secondly, we define the employed similarity index which gives desired invariance properties, and we facilitate the chosen oneswith sketching techniques for comparing various datasets efficiently.  Empirically,  we show thatthe proposed approach provides a state-of-the-art method for computing similarity of neural networks that are trained independently on different datasets and the tasks defined by the datasets."}}
{"id": "p-LrlIT5Xjw", "cdate": 1577836800000, "mdate": 1681679282029, "content": {"title": "Automatic Discovery of Privacy-Utility Pareto Fronts", "abstract": ""}}
{"id": "oxF8fnlN6T4", "cdate": 1577836800000, "mdate": null, "content": {"title": "Hyperbolic Embeddings for Preserving Privacy and Utility in Text", "abstract": ""}}
{"id": "oVmH1umwsaK", "cdate": 1577836800000, "mdate": null, "content": {"title": "Preserving Privacy in Analyses of Textual Data", "abstract": ""}}
{"id": "k3dpSAQO6_l", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimal Continual Learning has Perfect Memory and is NP-hard", "abstract": "Continual Learning (CL) algorithms incrementally learn a predictor or representation across multiple sequentially observed tasks. Designing CL algorithms that perform reliably and avoid so-called catastrophic forgetting has proven a persistent challenge. The current paper develops a theoretical approach that explains why. In particular, we derive the computational properties which CL algorithms would have to possess in order to avoid catastrophic forgetting. Our main finding is that such optimal CL algorithms generally solve an NP-hard problem and will require perfect memory to do so. The findings are of theoretical interest, but also explain the excellent performance of CL algorithms using experience replay, episodic memory and core sets relative to regularization-based approaches."}}
{"id": "YOSIcxzoE6", "cdate": 1577836800000, "mdate": 1681679281897, "content": {"title": "Interpretable Anomaly Detection with Mondrian P{\u00f3}lya Forests on Data Streams", "abstract": "Anomaly detection at scale is an extremely challenging problem of great practicality. When data is large and high-dimensional, it can be difficult to detect which observations do not fit the expected behaviour. Recent work has coalesced on variations of (random) $k$\\emph{d-trees} to summarise data for anomaly detection. However, these methods rely on ad-hoc score functions that are not easy to interpret, making it difficult to asses the severity of the detected anomalies or select a reasonable threshold in the absence of labelled anomalies. To solve these issues, we contextualise these methods in a probabilistic framework which we call the Mondrian \\Polya{} Forest for estimating the underlying probability density function generating the data and enabling greater interpretability than prior work. In addition, we develop a memory efficient variant able to operate in the modern streaming environments. Our experiments show that these methods achieves state-of-the-art performance while providing statistically interpretable anomaly scores."}}
