{"id": "Db6dKI87lO", "cdate": 1693484778198, "mdate": 1693484778198, "content": {"title": "Accelerated Methods for Riemannian Min-Max Optimization Ensuring Bounded Geometric Penalties", "abstract": "In this work, we study optimization problems of the form $\\min_x \\max_y f(x, y)$, where $f(x, y)$ is defined on a product Riemannian manifold $\\mathcal{M} \\times \\mathcal{N}$ and is $\\mux$-strongly geodesically convex (g-convex) in $x$ and $\\muy$-strongly g-concave in $y$, for $\\mux, \\muy \\geq 0$. We design accelerated methods when $f$ is $(L_x, L_y, L_{xy})$-smooth and $\\mathcal{M}$, $\\mathcal{N}$ are Hadamard. To that aim we introduce new g-convex optimization results, of independent interest: we show global linear convergence for metric-projected Riemannian gradient descent and improve existing accelerated methods by reducing geometric constants. Additionally, we complete the analysis of two previous works applying to the Riemannian min-max case by removing an assumption about iterates staying in a pre-specified compact set."}}
{"id": "tJpQgGk7iZk", "cdate": 1680010616312, "mdate": 1680010616312, "content": {"title": "Accelerated and Sparse Algorithms for Approximate Personalized PageRank and Beyond", "abstract": "It has recently been shown that ISTA, an unaccelerated optimization method, presents sparse updates for the $\\ell_1$-regularized personalized PageRank problem, leading to cheap iteration complexity and providing the same guarantees as the approximate personalized PageRank algorithm (APPR) [FRS+19]. In this work, we design an accelerated optimization algorithm for this problem that also performs sparse updates, providing an affirmative answer to the COLT 2022 open question of [FY22]. Acceleration provides a reduced dependence on the condition number, while the dependence on the sparsity in our updates differs from the ISTA approach. Further, we design another algorithm by using conjugate directions to achieve an exact solution while exploiting sparsity. Both algorithms lead to faster convergence for certain parameter regimes. Our findings apply beyond PageRank and work for any quadratic objective whose Hessian is a positive-definite $M$-matrix. "}}
{"id": "S4MIdf2-6u", "cdate": 1680010515406, "mdate": 1680010515406, "content": {"title": "Accelerated and Sparse Algorithms for Approximate Personalized PageRank and Beyond", "abstract": "It has recently been shown that ISTA, an unaccelerated optimization method, presents sparse updates for the $\\ell_1$-regularized personalized PageRank problem, leading to cheap iteration complexity and providing the same guarantees as the approximate personalized PageRank algorithm (APPR) [FRS+19]. In this work, we design an accelerated optimization algorithm for this problem that also performs sparse updates, providing an affirmative answer to the COLT 2022 open question of [FY22]. Acceleration provides a reduced dependence on the condition number, while the dependence on the sparsity in our updates differs from the ISTA approach. Further, we design another algorithm by using conjugate directions to achieve an exact solution while exploiting sparsity. Both algorithms lead to faster convergence for certain parameter regimes. Our findings apply beyond PageRank and work for any quadratic objective whose Hessian is a positive-definite $M$-matrix. "}}
{"id": "kzXRQRnG4gwe", "cdate": 1664731445057, "mdate": null, "content": {"title": "Accelerated Riemannian Optimization: Handling Constraints to Bound Geometric Penalties", "abstract": "We propose a globally-accelerated, first-order method for the optimization of smooth and (strongly or not) geodesically-convex functions in Hadamard manifolds. Our algorithm enjoys the same convergence rates as Nesterov's accelerated gradient descent, up to a multiplicative geometric penalty and log factors.  \n    Crucially, we can enforce our method to stay within a compact set we define. Prior fully accelerated works resort to assuming that the iterates of their algorithms stay in some pre-specified compact set, except for two previous methods, whose applicability is limited to local optimization and to spaces of constant curvature, respectively. Achieving global and general Riemannian acceleration without iterates assumptively staying in the feasible set was asked as an open question in (Kim & Yang, 2022), which we solve for Hadamard manifolds.\n    In our solution, we show that we can use a linearly convergent algorithm for constrained strongly g-convex smooth problems to implement a Riemannian inexact proximal point operator that we use as a subroutine, which is of independent interest."}}
{"id": "05rBhFU3mLX", "cdate": 1663850091671, "mdate": null, "content": {"title": "Accelerated Riemannian Optimization: Handling Constraints to Bound Geometric Penalties", "abstract": "    We propose a globally-accelerated, first-order method for the optimization of smooth and (strongly or not) geodesically-convex functions in Hadamard manifolds. Our algorithm enjoys the same convergence rates as Nesterov's accelerated gradient descent, up to a multiplicative geometric penalty and log factors.  \n    Crucially, we can enforce our method to stay within a compact set we define. Prior fully accelerated works resort to assuming that the iterates of their algorithms stay in some pre-specified compact set, except for two previous methods, whose applicability is limited to local optimization and to spaces of constant curvature, respectively. Achieving global and general Riemannian acceleration without iterates assumptively staying in the feasible set was asked as an open question in (Kim & Yang, 2022), which we solve for Hadamard manifolds.\n    In our solution, we show that we can use a linearly convergent algorithm for constrained strongly g-convex smooth problems to implement a Riemannian inexact proximal point operator that we use as a subroutine, which is of independent interest."}}
{"id": "Mn_HoKBcWK", "cdate": 1652737525750, "mdate": null, "content": {"title": "Fast Algorithms for Packing Proportional Fairness and its Dual", "abstract": "The proportional fair resource allocation problem is a major problem studied in flow control of networks, operations research, and economic theory, where it has found numerous applications. This problem, defined as the constrained maximization of $\\sum_i \\log x_i$, is known as the packing proportional fairness problem when the feasible set is defined by positive linear constraints and $x \\in \\mathbb{R}_{\\geq 0}^n$. In this work, we present a distributed accelerated first-order method for this problem which improves upon previous approaches. We also design an algorithm for the optimization of its dual problem. Both algorithms are width-independent."}}
{"id": "UPZCt9perOn", "cdate": 1652737405229, "mdate": null, "content": {"title": "Metric-Projected Accelerated Riemannian Optimization: Handling Constraints to Bound Geometric Penalties", "abstract": "We propose an accelerated first-order method for the optimization of smooth and (strongly or not) geodesically-convex functions over a compact and geodesically-convex set in Hadamard manifolds, that we access to via a metric-projection oracle. It enjoys the same rates of convergence as Nesterov's accelerated gradient descent, up to a multiplicative geometric penalty and log factors. Even without in-manifold constraints, all prior fully accelerated works require their iterates to remain in some specified compact set (which is needed in worse-case analyses due to a lower bound), while only two previous methods are able to enforce this condition and these, in contrast, have limited applicability like to local optimization or to spaces of constant curvature. Our results solve an open question in (Kim and Yang, 2022) and an another question related to one posed in (Zhang and Sra, 2016). In our solution, we show we can use projected Riemannian gradient descent to implement an inexact proximal point operator that we use as a subroutine, which is of independent interest. \n"}}
{"id": "YgYRhOFGlu8", "cdate": 1648928279280, "mdate": 1648928279280, "content": {"title": "Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group", "abstract": "We introduce a novel approach to perform first-order optimization with orthogonal and unitary constraints. This approach is based on a parametrization stemming from Lie group theory through the exponential map. The parametrization transforms the constrained optimization problem into an unconstrained one over a Euclidean space, for which common first-order optimization methods can be used. The theoretical results presented are general enough to cover the special orthogonal group, the unitary group and, in general, any connected compact Lie group. We discuss how this and other parametrizations can be computed efficiently through an implementation trick, making numerically complex parametrizations usable at a negligible runtime cost in neural networks. In particular, we apply our results to RNNs with orthogonal recurrent weights, yielding a new architecture called expRNN. We demonstrate how our method constitutes a more robust approach to optimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test RNNs. "}}
{"id": "bRh4u7WBfK1", "cdate": 1648928209241, "mdate": 1648928209241, "content": {"title": "Fast Algorithms for Packing Proportional Fairness and its Dual", "abstract": " The proportional fair resource allocation problem is a major problem studied in flow control of networks, operations research, and economic theory, where P it has found numerous applications. This problem, defined as the constrained maximization of $\\sum_i \\log x_i$ , is known as the packing proportional fairness problem when the feasible set is defined by positive linear constraints and x\u2208R\u22650 . In this work, we present a distributed accelerated first-order method for this problem which improves upon previous approaches. We also design an algorithm for the optimization of its dual problem. Both algorithms are width-independent. "}}
{"id": "WUNF4WVPvMy", "cdate": 1601308082619, "mdate": null, "content": {"title": "Acceleration in Hyperbolic and Spherical Spaces", "abstract": "    We further research on the acceleration phenomenon on Riemannian manifolds by introducing the first global first-order method that achieves the same rates as accelerated gradient descent in the Euclidean space for the optimization of smooth and geodesically convex (g-convex) or strongly g-convex functions defined on the hyperbolic space or a subset of the sphere, up to constants and log factors. To the best of our knowledge, this is the first method that is proved to achieve these rates globally on functions defined on a Riemannian manifold $\\mathcal{M}$ other than the Euclidean space.\n    Additionally, for any Riemannian manifold of bounded sectional curvature, we provide reductions from optimization methods for smooth and g-convex functions to methods for smooth and strongly g-convex functions and vice versa."}}
