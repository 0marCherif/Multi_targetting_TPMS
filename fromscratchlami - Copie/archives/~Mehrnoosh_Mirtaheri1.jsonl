{"id": "XrdeH9F5tR7", "cdate": 1663002171074, "mdate": 1663002171074, "content": {"title": "StATIK: Structure and Text for Inductive Knowledge Graph Completion", "abstract": "Knowledge graphs (KGs) often represent knowledge bases that are incomplete. Machine learning models can alleviate this by helping automate graph completion. Recently, there has been growing interest in completing knowledge bases that are dynamic, where previously unseen entities may be added to the KG with many missing links. In this paper, we present \\textbf{StATIK}--\\textbf{St}ructure \\textbf{A}nd \\textbf{T}ext for \\textbf{I}nductive \\textbf{K}nowledge Completion. \\texttt{StATIK} uses Language Models to extract the semantic information from text descriptions, while using Message Passing Neural Networks to capture the structural information. \\texttt{StATIK} achieves state of the art results on three challenging inductive baselines. We further analyze our hybrid model through detailed ablation studies. "}}
{"id": "GF8wO8MFQOr", "cdate": 1624392519259, "mdate": null, "content": {"title": "One-shot Learning for Temporal Knowledge Graphs", "abstract": "Most real-world knowledge graphs are characterized by a  frequency distribution with a long-tail where a significant fraction  of relations occurs only a handful of times. This observation has given rise to recent interest in low-shot learning methods that are able to generalize from only a few examples per relation. The existing approaches, however, are tailored to static knowledge graphs and do not easily generalize to temporal settings, where data scarcity poses even bigger problems, e.g., due to occurrence of new, previously unseen relations. We address this shortcoming by proposing a one-shot learning framework for link prediction in temporal knowledge graphs. Our proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and  a network  to compute a similarity score between a given query and a (one-shot) example. Our experiments show that the proposed algorithm outperforms the state of the art baselines for two well-studied benchmarks while achieving significantly better performance for sparse relations."}}
{"id": "6DOZ8XNNfGN", "cdate": 1601308101067, "mdate": null, "content": {"title": "Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning", "abstract": "Graph Representation Learning (GRL) methods have impacted fields from chemistry to social science. However, their algorithmic implementations are specialized to specific use-cases e.g. \"message passing\" methods are run differently from \"node embedding\" ones. Despite their apparent differences, all these methods utilize the graph structure,  and therefore, their learning can be approximated with stochastic graph traversals.  We propose Graph Traversal via Tensor Functionals (GTTF), a unifying meta-algorithm framework for easing the implementation of diverse graph algorithms and enabling transparent and efficient scaling to large graphs.  GTTF is founded upon a data structure (stored as a sparse tensor) and a stochastic graph traversal algorithm (described using tensor operations). The algorithm is a functional that accept two functions, and can be specialized to obtain a variety of GRL models and objectives, simply by changing those two functions. We show for a wide class of methods, our algorithm learns in an unbiased fashion and, in expectation, approximates the learning as if the specialized implementations were run directly.\nWith these capabilities, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries -- with only a handful of lines of code for each method specialization."}}
