{"id": "UgBYfuBt9c", "cdate": 1695933757316, "mdate": 1695933757316, "content": {"title": "The Dynamics of Functional Diversity throughout Neural Network Training", "abstract": "Deep ensembles offer reduced generalization error and improved predictive uncertainty estimates.\nThese performance gains are attributed to functional diversity among the component models that\nmake up the ensembles: ensemble performance increases with the diversity of the components. A\nstandard way to generate a diversity of components is to train multiple networks on the same data,\nusing different minibatch orders, augmentations, etc. In this work, we focus on how and when this\ntype of diversity in the learned predictor decreases throughout training.\nIn order to study the diversity of networks still accessible via SGD after t iterations, we first train a\nsingle network for t iterations, then duplicate the state of the optimizer and finish the remainder of\ntraining k times, with independent randomness (minibatches, augmentations, etc) for each duplicated\nnetwork. The result is k distinct networks whose training has been coupled for t iterations. We use\nthis methodology\u2014recently exploited for k = 2 to study linear mode connectivity\u2014to construct a\nnovel probe for studying diversity.\nWe find that coupling k for even a few epochs severely restricts the diversity of functions accessible\nby SGD, as measured by the KL divergence between the predicted label distributions as well as the\ncalibration and test error of k-ensembles. We also find that the number of forgetting events [1] drops\noff rapidly.\nThe amount of independent training time decreases with coupling time t however. To control for this\nconfounder, we study extending the number of iterations of high-learning-rate optimization for an\nadditional t iterations post-coupling. We find that this does not restore functional diversity.\nWe also study how functional diversity is affected by retraining after reinitializing the weights in some\nlayers. We find that we recover significantly more diversity by reinitializing layers closer to the input\nlayer, compared to reinitializing layers closer to the output. In this case, we see that reinitialization\nupsets linear mode connectivity. This observation agrees with the performance improvements seen by\narchitectures that share the core of a network but train multiple instantiations of the input layers [2]."}}
{"id": "LzbrVf-l0Xq", "cdate": 1652737802943, "mdate": null, "content": {"title": "Implications of Model Indeterminacy for Explanations of Automated Decisions", "abstract": "There has been a significant research effort focused on explaining predictive models, for example through post-hoc explainability and recourse methods. Most of the proposed techniques operate upon a single, fixed, predictive model. However, it is well-known that given a dataset and a predictive task, there may be a multiplicity of models that solve the problem (nearly) equally well. In this work, we investigate the implications of this kind of model indeterminacy on the post-hoc explanations of predictive models. We show how it can lead to explanatory multiplicity, and we explore the underlying drivers. We show how predictive multiplicity, and the related concept of epistemic uncertainty, are not reliable indicators of explanatory multiplicity. We further illustrate how a set of models showing very similar aggregate performance on a test dataset may show large variations in their local explanations, i.e., for a specific input. We explore these effects for Shapley value based explanations on three risk assessment datasets. Our results indicate that model indeterminacy may have a substantial impact on explanations in practice, leading to inconsistent and even contradicting explanations."}}
{"id": "o_bs6hQD5eG", "cdate": 1640995200000, "mdate": 1682018670498, "content": {"title": "Assessing AI Fairness in Finance", "abstract": ""}}
{"id": "pDr9UmwO_9_", "cdate": 1577836800000, "mdate": 1681612137146, "content": {"title": "RelatIF: Identifying Explanatory Training Examples via Relative Influence", "abstract": ""}}
{"id": "3lSF382tTQl", "cdate": 1577836800000, "mdate": 1681612137144, "content": {"title": "RelatIF: Identifying Explanatory Training Samples via Relative Influence", "abstract": ""}}
{"id": "SJZBooWdbH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Understanding the Origins of Bias in Word Embeddings", "abstract": "Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems can amplify stereotypes in important contexts..."}}
{"id": "_Ce7jilymC1", "cdate": 1514764800000, "mdate": 1682018670544, "content": {"title": "Understanding the Origins of Bias in Word Embeddings", "abstract": "The power of machine learning systems not only promises great technical progress, but risks societal harm. As a recent example, researchers have shown that popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems, from automated translation services to curriculum vitae scanners, can amplify stereotypes in important contexts. Although methods have been developed to measure these biases and alter word embeddings to mitigate their biased representations, there is a lack of understanding in how word embedding bias depends on the training data. In this work, we develop a technique for understanding the origins of bias in word embeddings. Given a word embedding trained on a corpus, our method identifies how perturbing the corpus will affect the bias of the resulting embedding. This can be used to trace the origins of word embedding bias back to the original training documents. Using our method, one can investigate trends in the bias of the underlying corpus and identify subsets of documents whose removal would most reduce bias. We demonstrate our techniques on both a New York Times and Wikipedia corpus and find that our influence function-based approximations are very accurate."}}
{"id": "PpbiYIDT02w", "cdate": 1420070400000, "mdate": null, "content": {"title": "Online Cost-Sharing Mechanism Design for Demand-Responsive Transport Systems", "abstract": "Demand-responsive transport (DRT) systems provide flexible transport services for passengers who request door-to-door rides in shared-ride mode without fixed routes and schedules. DRT systems face interesting coordination challenges. For example, one has to design cost-sharing mechanisms for offering fare quotes to potential passengers so that all passengers are treated fairly. The main issue is how the operating costs of the DRT system should be shared among the passengers (given that different passengers cause different amounts of inconvenience to the other passengers), taking into account that DRT systems should provide fare quotes instantaneously without knowing future ride request submissions. We determine properties of cost-sharing mechanisms that make DRT systems attractive to both the transport providers and passengers, namely online fairness, immediate response, individual rationality, budget balance, and ex-post incentive compatibility. We propose a novel cost-sharing mechanism, which is called Proportional Online Cost Sharing (POCS), which provides passengers with upper bounds on their fares immediately after their ride request submissions despite missing knowledge of future ride request submissions, allowing them to accept their fare quotes or drop out. We examine how POCS satisfies these properties in theory and computational experiments."}}
