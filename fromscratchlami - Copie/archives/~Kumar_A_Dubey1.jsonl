{"id": "XMQgwiJ7KSX", "cdate": 1686324863293, "mdate": null, "content": {"title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink)."}}
{"id": "thirVlDJ2IL", "cdate": 1652737565991, "mdate": null, "content": {"title": "A Fourier Approach to Mixture Learning", "abstract": "We revisit the problem of learning mixtures of spherical Gaussians. Given samples from a mixture $\\frac{1}{k}\\sum_{j=1}^{k}\\mathcal{N}(\\mu_j, I_d)$, the goal is to estimate the means $\\mu_1, \\mu_2, \\ldots, \\mu_k \\in \\mathbb{R}^d$ up to a small error. The hardness of this learning problem can be measured by the \\emph{separation} $\\Delta$ defined as the minimum distance between all pairs of means. Regev and Vijayaraghavan (2017) showed that with $\\Delta = \\Omega(\\sqrt{\\log k})$ separation, the means can be learned using $\\mathrm{poly}(k, d)$ samples, whereas super-polynomially many samples are required if $\\Delta = o(\\sqrt{\\log k})$ and $d = \\Omega(\\log k)$. This leaves open the low-dimensional regime where $d = o(\\log k)$.\n    \nIn this work, we give an algorithm that efficiently learns the means in $d = O(\\log k/\\log\\log k)$ dimensions under separation $d/\\sqrt{\\log k}$ (modulo doubly logarithmic factors). This separation is strictly smaller than $\\sqrt{\\log k}$, and is also shown to be necessary. Along with the results of Regev and Vijayaraghavan (2017), our work almost pins down the critical separation threshold at which efficient parameter learning becomes possible for spherical Gaussian mixtures. More generally, our algorithm runs in time $\\mathrm{poly}(k)\\cdot f(d, \\Delta, \\epsilon)$, and is thus fixed-parameter tractable in parameters $d$, $\\Delta$ and $\\epsilon$.\n    \nOur approach is based on estimating the Fourier transform of the mixture at carefully chosen frequencies, and both the algorithm and its analysis are simple and elementary. Our positive results can be easily extended to learning mixtures of non-Gaussian distributions, under a mild condition on the Fourier spectrum of the distribution."}}
{"id": "vRwCvlvd8eA", "cdate": 1652737406733, "mdate": null, "content": {"title": "Chefs' Random Tables: Non-Trigonometric Random Features", "abstract": "We introduce chefs' random tables (CRTs), a new class of non-trigonometric random features (RFs) to approximate Gaussian and softmax kernels. CRTs are an alternative to standard random kitchen sink (RKS) methods, which inherently rely on the trigonometric maps. We present variants of CRTs where RFs are positive, a key requirement for applications in recent low-rank Transformers. Further variance reduction is possible by leveraging statistics which are simple to compute. One instantiation of CRTs, the optimal positive random features (OPRFs), is to our knowledge the first RF method for unbiased softmax kernel estimation with positive and bounded RFs, resulting in exponentially small tails and much lower variance than its counterparts. As we show, orthogonal random features applied in OPRFs provide additional variance reduction for any dimensionality $d$ (not only asymptotically for sufficiently large $d$, as for RKS). We test CRTs on many tasks ranging from non-parametric classification to training Transformers for text, speech and image data, obtaining new state-of-the-art results for low-rank text Transformers, while providing linear space and time complexity."}}
{"id": "H4ph4BG-Hfs", "cdate": 1635535329337, "mdate": null, "content": {"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is the\nquadratic dependency (mainly in terms of memory) on the sequence length due to\ntheir full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse\nattention mechanism that reduces this quadratic dependency to linear. We show\nthat BIGBIRD is a universal approximator of sequence functions and is Turing\ncomplete, thereby preserving these properties of the quadratic, full attention model.\nAlong the way, our theoretical analysis reveals some of the benefits of having\nO(1) global tokens (such as CLS), that attend to the entire sequence as part of the\nsparse attention mechanism. The proposed sparse attention can handle sequences\nof length up to 8x of what was previously possible using similar hardware. As\na consequence of the capability to handle longer context, BIGBIRD drastically\nimproves performance on various NLP tasks such as question answering and\nsummarization. We also propose novel applications to genomics data."}}
{"id": "C7ViqmpuBl", "cdate": 1632875615064, "mdate": null, "content": {"title": "On Learning the Transformer Kernel", "abstract": "In this work we introduce Kernelised Transformer, a generic, scalable, data driven framework for learning the kernel function in Transformers. Our framework approximates the Transformer kernel as a dot product between spectral feature maps and learns the kernel by learning the spectral distribution. This not only helps in learning a generic kernel end-to-end, but also reduces the time and space complexity of Transformers from quadratic to linear. We show that Kernelized Transformers achieve performance comparable to existing efficient Transformer architectures, both in terms of accuracy as well as computational efficiency. Our study also demonstrates that the choice of the kernel has a substantial impact on performance, and kernel learning variants are competitive alternatives to fixed kernel Transformers, both in long as well as short sequence tasks."}}
{"id": "zdP3h_BcZ1C", "cdate": 1600184832558, "mdate": null, "content": {"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."}}
{"id": "r1RF3ExCb", "cdate": 1518730184224, "mdate": null, "content": {"title": "Transformation Autoregressive Networks", "abstract": "The fundamental task of general density estimation has been of keen interest to machine learning. Recent advances in density estimation have either: a) proposed using a flexible model to estimate the conditional factors of the chain rule; or b) used flexible, non-linear transformations of variables of a simple base distribution. Instead, this work jointly leverages transformations of variables and autoregressive conditional models, and proposes novel methods for both. We provide a deeper understanding of our models, showing a considerable improvement with our methods through a comprehensive study over both real world and synthetic data. Moreover, we illustrate the use of our models in outlier detection and image modeling task."}}
{"id": "HJUOHGWRb", "cdate": 1518730163387, "mdate": null, "content": {"title": "Contextual Explanation Networks", "abstract": "We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations. CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly. Our approach offers two major advantages: (i) for each prediction, valid instance-specific explanations are generated with no computational overhead and (ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings. We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations. Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support."}}
{"id": "rJ-ifo-OZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Transformation Autoregressive Networks", "abstract": "The fundamental task of general density estimation $p(x)$ has been of keen interest to machine learning. In this work, we attempt to systematically characterize methods for density estimation. Broa..."}}
{"id": "SkEJyFbu-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems", "abstract": "As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software, and with domain knowledge encoded as rules. As a case study, we present such a system that learns to parse Newtonian physics problems in textbooks. This system, Nuts&amp;Bolts, learns a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules. It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data. Our approach achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. Finally, we also show how Nuts&amp;Bolts can be used to achieve improvements on a relation extraction task and on the end task of answering Newtonian physics problems."}}
