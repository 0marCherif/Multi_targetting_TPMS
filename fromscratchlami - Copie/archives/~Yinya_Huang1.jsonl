{"id": "pVU6b2UUL7o", "cdate": 1640995200000, "mdate": 1682326845799, "content": {"title": "MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure", "abstract": ""}}
{"id": "mi3VUqVMfVv", "cdate": 1640995200000, "mdate": 1682326845784, "content": {"title": "Discourse-Aware Graph Networks for Textual Logical Reasoning", "abstract": "Textual logical reasoning, especially question-answering (QA) tasks with logical reasoning, requires awareness of particular logical structures. The passage-level logical relations represent entailment or contradiction between propositional units (e.g., a concluding sentence). However, such structures are unexplored as current QA systems focus on entity-based relations. In this work, we propose logic structural-constraint modeling to solve the logical reasoning QA and introduce discourse-aware graph networks (DAGNs). The networks first construct logic graphs leveraging in-line discourse connectives and generic logic theories, then learn logic representations by end-to-end evolving the logic relations with an edge-reasoning mechanism and updating the graph features. This pipeline is applied to a general encoder, whose fundamental features are joined with the high-level logic features for answer prediction. Experiments on three textual logical reasoning datasets demonstrate the reasonability of the logical structures built in DAGNs and the effectiveness of the learned logic features. Moreover, zero-shot transfer results show the features' generality to unseen logical texts."}}
{"id": "dEI8nOxMQdz", "cdate": 1640995200000, "mdate": 1669117626206, "content": {"title": "PathReasoner: Explainable reasoning paths for commonsense question answering", "abstract": ""}}
{"id": "NaXNZyaove2", "cdate": 1640995200000, "mdate": 1682326845788, "content": {"title": "MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure", "abstract": "In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models."}}
{"id": "sxZ5_pNbJVV", "cdate": 1609459200000, "mdate": 1668075550902, "content": {"title": "REM-Net: Recursive Erasure Memory Network for Commonsense Evidence Refinement", "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. While recent works retrieve supporting facts/evidence from commonsense knowledge bases to supply additional information to each question, there is still ample opportunity to advance it on the quality of the evidence. It is crucial since the quality of the evidence is the key to answering common- sense questions, and even determines the upper bound on the QA systems\u2019 performance. In this paper, we propose a recursive erasure memory network (REM-Net) to cope with the quality improvement of evidence. To address this, REM-Net is equipped with a module to refine the evidence by recursively erasing the low-quality evidence that does not explain the question answering. Besides, instead of retrieving evidence from existing knowledge bases, REM-Net leverages a pre-trained generative model to generate candidate evidence customized for the question. We conduct experiments on two commonsense question answering datasets, WIQA and CosmosQA. The results demonstrate the performance of REM- Net and show that the refined evidence is explainable."}}
{"id": "r5Zr8y3ab9", "cdate": 1609459200000, "mdate": 1647325005114, "content": {"title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning", "abstract": "Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, Xiaodan Liang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "6d7XNIejF8", "cdate": 1609459200000, "mdate": 1682326845791, "content": {"title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning", "abstract": "Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourse-aware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, ReClor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/Eleanor-H/DAGN."}}
{"id": "sFSLSBney18", "cdate": 1577836800000, "mdate": 1668693559735, "content": {"title": "REM-Net: Recursive Erasure Memory Network for Commonsense Evidence Refinement", "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. While recent works retrieve supporting facts/evidence from commonsense knowledge bases to supply additional information to each question, there is still ample opportunity to advance it on the quality of the evidence. It is crucial since the quality of the evidence is the key to answering commonsense questions, and even determines the upper bound on the QA systems performance. In this paper, we propose a recursive erasure memory network (REM-Net) to cope with the quality improvement of evidence. To address this, REM-Net is equipped with a module to refine the evidence by recursively erasing the low-quality evidence that does not explain the question answering. Besides, instead of retrieving evidence from existing knowledge bases, REM-Net leverages a pre-trained generative model to generate candidate evidence customized for the question. We conduct experiments on two commonsense question answering datasets, WIQA and CosmosQA. The results demonstrate the performance of REM-Net and show that the refined evidence is explainable."}}
