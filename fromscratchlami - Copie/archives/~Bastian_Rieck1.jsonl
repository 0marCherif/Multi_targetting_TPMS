{"id": "n5bDGlH4FY", "cdate": 1672531200000, "mdate": 1681492542781, "content": {"title": "On the Expressivity of Persistent Homology in Graph Learning", "abstract": ""}}
{"id": "X6WZziw93d_", "cdate": 1672531200000, "mdate": 1681492542781, "content": {"title": "Euler Characteristic Transform Based Topological Loss for Reconstructing 3D Images from Single 2D Slices", "abstract": ""}}
{"id": "Bjo04EGhBH", "cdate": 1672531200000, "mdate": 1675665464817, "content": {"title": "Curvature Filtrations for Graph Generative Model Evaluation", "abstract": "Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property of graphs, and has recently started to prove useful in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models."}}
{"id": "Bol45H5FAc", "cdate": 1664028936736, "mdate": null, "content": {"title": "Approximate Bayesian Computation for Panel Data with Signature Maximum Mean Discrepancies", "abstract": "Simulation models are becoming a staple tool across application domains from economics to biology. When such models are stochastic, evaluating their likelihood functions in a reasonable time is typically infeasible or even impossible. In these settings, simulation-based inference procedures are a convenient means to approximating conventional parameter calibration procedures. A popular example is approximate Bayesian computation, in which the observed data is compared to the simulation output at different parameter values through some distance function. While many such methods exist, few are compatible with panel data of various kinds, as might appear in medical settings, for example; many methods instead assume iid observations in both the simulated and observed data. We seek to address this gap through the use of signature maximum mean discrepancies as distance measures in approximate Bayesian computation. Through experiments with a dynamical model of functional brain networks, we demonstrate that such an approach can flexibly operate on panel data of various kinds, for example dynamic graph data arising from multiple patients/subjects in fMRI settings."}}
{"id": "sPCKNl5qDps", "cdate": 1663850137254, "mdate": null, "content": {"title": "Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework", "abstract": "Bridging geometry and topology, curvature is a powerful and expressive invariant. While the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. On graphs, the Ollivier-Ricci curvature measures differences between random walks via Wasserstein distances, thus grounding a geometric concept in ideas from probability theory and optimal transport. We develop Orchid, a flexible framework generalizing Ollivier-Ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. Through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that Orchid curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice."}}
{"id": "BN_P4LNiK2", "cdate": 1663850104605, "mdate": null, "content": {"title": "TOAST: Topological Algorithm for Singularity Tracking", "abstract": "The manifold hypothesis, which assumes that data lie on or close to an unknown manifold of low intrinsic dimensionality, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibit distinct non-manifold structures, which result in singularities that can lead to erroneous conclusions about the data. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address detecting singularities by developing (i) persistent local homology, a new topology-driven framework for quantifying the intrinsic dimension of a data set locally, and (ii) Euclidicity, a topology-based multi-scale measure for assessing the \u2018manifoldness\u2019 of individual points. We show that our approach can reliably identify singularities of complex spaces, while also capturing singular structures in real-world data sets."}}
{"id": "EM-Z3QFj8n", "cdate": 1662812631329, "mdate": null, "content": {"title": "Taxonomy of Benchmarks in Graph Representation Learning", "abstract": "Graph Neural Networks (GNNs) extend the success of neural networks to graph-structured data by accounting for their intrinsic geometry. While extensive research has been done on developing GNN models with superior performance according to a collection of graph representation learning benchmarks, it is currently not well understood what aspects of a given model are probed by them. For example, to what extent do they test the ability of a model to leverage graph structure vs. node features? Here, we develop a principled approach to taxonomize benchmarking datasets according to a $\\textit{sensitivity profile}$ that is based on how much GNN performance changes due to a collection of graph perturbations. Our data-driven analysis provides a deeper understanding of which benchmarking data characteristics are leveraged by GNNs. Consequently, our taxonomy can aid in selection and development of adequate graph benchmarks, and better informed evaluation of future GNN methods. Finally, our approach is designed to be extendable to multiple graph prediction task types and future datasets."}}
{"id": "l2CVt1ySC2Q", "cdate": 1652737719364, "mdate": null, "content": {"title": "On Measuring Excess Capacity in Neural Networks", "abstract": "We study the excess capacity of deep networks in the context of supervised classification. That is, given a capacity measure of the underlying hypothesis class - in our case, empirical Rademacher complexity - to what extent can we (a priori) constrain this class while retaining an empirical error on a par with the unconstrained regime? To assess excess capacity in modern architectures (such as residual networks), we extend and unify prior Rademacher complexity bounds to accommodate function composition and addition, as well as the structure of convolutions. The capacity-driving terms in our bounds are the Lipschitz constants of the layers and a (2,1) group norm distance to the initializations of the convolution weights. Experiments on benchmark datasets of varying task difficulty indicate that (1) there is a substantial amount of excess capacity per task, and (2) capacity can be kept at a surprisingly similar level across tasks. Overall, this suggests a notion of compressibility with respect to weight norms, complementary to classic compression via weight pruning. Source code is available at https://github.com/rkwitt/excess_capacity."}}
{"id": "AYII8AkvD1e", "cdate": 1652737625301, "mdate": null, "content": {"title": "Diffusion Curvature for Estimating Local Curvature in High Dimensional Data", "abstract": "We introduce a new intrinsic measure of local curvature on point-cloud data called diffusion curvature. Our measure uses the framework of diffusion maps, including the data diffusion operator, to structure point cloud data and define local curvature based on the laziness of a random walk starting at a point or region of the data. We show that this laziness directly relates to volume comparison results from Riemannian geometry. We then extend this scalar curvature notion to an entire quadratic form using neural network estimations based on the diffusion map of point-cloud data. We show applications of both estimations on toy data, single-cell data, and on estimating local Hessian matrices of neural network loss landscapes."}}
{"id": "nSEqVlDXKAm", "cdate": 1649232627402, "mdate": 1649232627402, "content": {"title": "Graph Kernels: State-of-the-Art and Future Challenges", "abstract": "Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels."}}
