{"id": "_pzyPgx0q1", "cdate": 1640995200000, "mdate": 1683904129190, "content": {"title": "Privacy Implications of Shuffling", "abstract": "ldp deployments are vulnerable to inference attacks as an adversary can link the noisy responses to their identity and subsequently, auxiliary information using the \\textit{order} of the data. An alternative model, shuffle \\textsf{DP}, prevents this by shuffling the noisy responses uniformly at random. However, this limits the data learnability -- only symmetric functions (input order agnostic) can be learned. In this paper, we strike a balance and show that systematic shuffling of the noisy responses can thwart specific inference attacks while retaining some meaningful data learnability. To this end, we propose a novel privacy guarantee, \\name-privacy, that captures the privacy of the order of a data sequence. \\name-privacy allows tuning the granularity at which the ordinal information is maintained, which formalizes the degree the resistance to inference attacks trading it off with data learnability. Additionally, we propose a novel shuffling mechanism that can achieve \\name-privacy and demonstrate the practicality of our mechanism via evaluation on real-world datasets."}}
{"id": "M_Ca8TWH-h", "cdate": 1640995200000, "mdate": 1683752640659, "content": {"title": "Privacy Amplification by Subsampling in Time Domain", "abstract": "Aggregate time-series data like traffic flow and site occupancy repeatedly sample statistics from a population across time. Such data can be profoundly useful for understanding trends within a given population, but also pose a significant privacy risk, potentially revealing e.g., who spends time where. Producing a private version of a time-series satisfying the standard definition of Differential Privacy (DP) is challenging due to the large influence a single participant can have on the sequence: if an individual can contribute to each time step, the amount of additive noise needed to satisfy privacy increases linearly with the number of time steps sampled. As such, if a signal spans a long duration or is oversampled, an excessive amount of noise must be added, drowning out underlying trends. However, in many applications an individual realistically cannot participate at every time step. When this is the case, we observe that the influence of a single participant (sensitivity) can be reduced by subsampling and/or filtering in time, while still meeting privacy requirements. Using a novel analysis, we show this significant reduction in sensitivity and propose a corresponding class of privacy mechanisms. We demonstrate the utility benefits of these techniques empirically with real-world and synthetic time-series data."}}
{"id": "GYrvWvdxA2", "cdate": 1640995200000, "mdate": 1683904129097, "content": {"title": "Sentence-level Privacy for Document Embeddings", "abstract": ""}}
{"id": "5i2f-aR6B8H", "cdate": 1632875471862, "mdate": null, "content": {"title": "Privacy Implications of Shuffling", "abstract": "\\ldp deployments are vulnerable to inference attacks as an adversary can link the noisy responses to their identity and subsequently, auxiliary information using the \\textit{order} of the data. An alternative model, shuffle \\textsf{DP}, prevents this by shuffling the noisy responses uniformly at random.  However, this limits the data learnability -- only symmetric functions (input order agnostic) can be learned. In this paper, we strike a balance and show that systematic shuffling of the noisy responses can thwart specific inference attacks while retaining some meaningful data learnability. To this end, we propose a novel privacy guarantee, \\name-privacy, that captures the privacy of the order of a data sequence. \\name-privacy allows tuning the granularity at which the ordinal information is maintained, which formalizes the degree the resistance to inference attacks trading it off with data learnability.  Additionally, we propose a novel shuffling mechanism that can achieve \\name-privacy and demonstrate the practicality of our mechanism via evaluation on real-world datasets. "}}
{"id": "fyeous9ip5", "cdate": 1632324382534, "mdate": 1632324382534, "content": {"title": "A Non-Parametric Test to Detect Data-Copying in Generative Models", "abstract": "Detecting overfitting in generative models is\nan important challenge in machine learning.\nIn this work, we formalize a form of overfitting\nthat we call data-copying \u2013 where the generative model memorizes and outputs training\nsamples or small variations thereof. We provide a three sample non-parametric test for\ndetecting data-copying that uses the training\nset, a separate sample from the target distribution, and a generated sample from the\nmodel, and study the performance of our test\non several canonical models and datasets."}}
{"id": "h741XFvHqz-", "cdate": 1632324311654, "mdate": null, "content": {"title": "Location Trace Privacy Under Conditional Priors", "abstract": "Providing meaningful privacy to users of location based services is particularly challenging\nwhen multiple locations are revealed in a short period of time. This is primarily due to the\ntremendous degree of dependence that can be anticipated between points. We propose a\nR\u00e9nyi differentially private framework for bounding expected privacy loss for conditionally\ndependent data. Additionally, we demonstrate an algorithm for achieving this privacy under\nGaussian process conditional priors. This framework both exemplifies why conditionally\ndependent data is so challenging to protect and offers a strategy for preserving privacy to\nwithin a fixed radius for every user location in a trace."}}
{"id": "49PFfpzqyU", "cdate": 1609459200000, "mdate": 1683904129105, "content": {"title": "Location Trace Privacy Under Conditional Priors", "abstract": "Providing meaningful privacy to users of location based services is particularly challenging when multiple locations are revealed in a short period of time. This is primarily due to the tremendous degree of dependence that can be anticipated between points. We propose a R\u00e9nyi divergence based privacy framework for bounding expected privacy loss for conditionally dependent data. Additionally, we demonstrate an algorithm for achieving this privacy under Gaussian process conditional priors. This framework both exemplifies why conditionally dependent data is so challenging to protect and offers a strategy for preserving privacy to within a fixed radius for sensitive locations in a user\u2019s trace."}}
{"id": "Tcmf0uCDEv", "cdate": 1577836800000, "mdate": 1683904129122, "content": {"title": "A Three Sample Hypothesis Test for Evaluating Generative Models", "abstract": "Detecting overfitting in generative models is an important challenge in machine learning. In this work, we formalize a form of overfitting that we call {\\em{data-copying}} \u2013 where the generative mo..."}}
