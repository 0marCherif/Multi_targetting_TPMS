{"id": "wd2GcXpLMUB", "cdate": 1577836800000, "mdate": 1663205494764, "content": {"title": "Attention-Based Deep Metric Learning for Near-Duplicate Video Retrieval", "abstract": "Near-duplicate video retrieval (NDVR) is an important and challenging problem due to the increasing amount of videos uploaded to the Internet. In this paper, we propose an attention-based deep metric learning method for NDVR. Our method is based on well-established principles: We leverage two-stream networks to combine RGB and optical flow features, and incorporate an attention module to effectively deal with distractor frames commonly observed in near duplicate videos. We further aggregate the features corresponding to multiple video segments to enhance the discriminative power. The whole system is trained using a deep metric learning objective with a Siamese architecture. Our experiments show that the attention module helps eliminate redundant and noisy frames, while focusing on visually relevant frames for solving NVDR. We evaluate our approach on recent large-scale NDVR datasets, CC_WEB_VIDEO, VCDB, FIVR and SVD. To demonstrate the generalization ability of our approach, we report results in both within- and cross-dataset settings, and show that the proposed method significantly outperforms state-of-the-art approaches."}}
{"id": "BbPV372opcE", "cdate": 1514764800000, "mdate": 1667257056500, "content": {"title": "LightPainter: Creating Long-Exposure Imagery from Videos", "abstract": "This article presents LightPainter, an interactive tool that promotes creative long-exposure photography through an intuitive drawing metaphor and flexible spatiotemporal mapping from videos to composite images. We discuss the power of software-defined exposure and the tools capability to facilitate creating sophisticated long-exposure effects in challenging scenarios."}}
{"id": "wruAhwi9aQ", "cdate": 1483228800000, "mdate": 1667436077143, "content": {"title": "Learning to Compose with Professional Photographs on the Web", "abstract": "Photo composition is an important factor affecting the aesthetics in photography. However, it is a highly challenging task to model the aesthetic properties of good compositions due to the lack of globally applicable rules to the wide variety of photographic styles. Inspired by the thinking process of photo taking, we formulate the photo composition problem as a view finding process which successively examines pairs of views and determines their aesthetic preferences. We further exploit the rich professional photographs on the web to mine unlimited high-quality ranking samples and demonstrate that an aesthetics-aware deep ranking network can be trained without explicitly modeling any photographic rules. The resulting model is simple and effective in terms of its architectural design and data sampling method. It is also generic since it naturally learns any photographic rules implicitly encoded in professional photographs. The experiments show that the proposed view finding network achieves state-of-the-art performance with sliding window search strategy on two image cropping datasets."}}
{"id": "Z5bCdThUnfQ", "cdate": 1483228800000, "mdate": 1665999648574, "content": {"title": "Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset and Comparative Study", "abstract": "Automatic photo cropping is an important tool for improving visual quality of digital photos without resorting to tedious manual selection. Traditionally, photo cropping is accomplished by determining the best proposal window through visual quality assessment or saliency detection. In essence, the performance of an image cropper highly depends on the ability to correctly rank a number of visually similar proposal windows. Despite the ranking nature of automatic photo cropping, little attention has been paid to learning-to-rank algorithms in tackling such a problem. In this work, we conduct an extensive study on traditional approaches as well as ranking-based croppers trained on various image features. In addition, a new dataset consisting of high quality cropping and pairwise ranking annotations is presented to evaluate the performance of various baselines. The experimental results on the new dataset provide useful insights into the design of better photo cropping algorithms."}}
{"id": "8nNrrQyJYM0p", "cdate": 1483228800000, "mdate": 1665999650385, "content": {"title": "Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset and Comparative Study", "abstract": "Automatic photo cropping is an important tool for improving visual quality of digital photos without resorting to tedious manual selection. Traditionally, photo cropping is accomplished by determining the best proposal window through visual quality assessment or saliency detection. In essence, the performance of an image cropper highly depends on the ability to correctly rank a number of visually similar proposal windows. Despite the ranking nature of automatic photo cropping, little attention has been paid to learning-to-rank algorithms in tackling such a problem. In this work, we conduct an extensive study on traditional approaches as well as ranking-based croppers trained on various image features. In addition, a new dataset consisting of high quality cropping and pairwise ranking annotations is presented to evaluate the performance of various baselines. The experimental results on the new dataset provide useful insights into the design of better photo cropping algorithms."}}
{"id": "-Svw2rfW3_", "cdate": 1483228800000, "mdate": 1667436077152, "content": {"title": "Learning to Compose with Professional Photographs on the Web", "abstract": "Photo composition is an important factor affecting the aesthetics in photography. However, it is a highly challenging task to model the aesthetic properties of good compositions due to the lack of globally applicable rules to the wide variety of photographic styles. Inspired by the thinking process of photo taking, we formulate the photo composition problem as a view finding process which successively examines pairs of views and determines their aesthetic preferences. We further exploit the rich professional photographs on the web to mine unlimited high-quality ranking samples and demonstrate that an aesthetics-aware deep ranking network can be trained without explicitly modeling any photographic rules. The resulting model is simple and effective in terms of its architectural design and data sampling method. It is also generic since it naturally learns any photographic rules implicitly encoded in professional photographs. The experiments show that the proposed view finding network achieves state-of-the-art performance with sliding window search strategy on two image cropping datasets."}}
{"id": "zVcRryml_X", "cdate": 1451606400000, "mdate": 1682354668715, "content": {"title": "EdgeVib: Effective Alphanumeric Character Output Using a Wrist-Worn Tactile Display", "abstract": "This paper presents EdgeVib, a system of spatiotemporal vibration patterns for delivering alphanumeric characters on wrist-worn vibrotactile displays. We first investigated spatiotemporal pattern delivery through a watch-back tactile display by performing a series of user studies. The results reveal that employing a 2\u00d72 vibrotactile array is more effective than employing a 3\u00d73 one, because the lower-resolution array creates clearer tactile sensations in less time consumption. We then deployed EdgeWrite patterns on a 2\u00d72 vibrotactile array to determine any difficulties of delivering alphanumerical characters, and then modified the unistroke patterns into multistroke EdgeVib ones on the basis of the findings. The results of a 24-participant user study reveal that the recognition rates of the modified multistroke patterns were significantly higher than the original unistroke ones in both alphabet (85.9% vs. 70.7%) and digits (88.6% vs. 78.5%) delivery, and a further study indicated that the techniques can be generalized to deliver two-character compound messages with recognition rates higher than 83.3%. The guidelines derived from our study can be used for designing watch-back tactile displays for alphanumeric character output."}}
{"id": "xOEJrxarwU", "cdate": 1451606400000, "mdate": null, "content": {"title": "Retargeting 3D Objects and Scenes with a General Framework", "abstract": "In this paper, we introduce an interactive method suitable for retargeting both 3D objects and scenes. Initially, the input object or scene is decomposed into a collection of constituent components e..."}}
{"id": "QKrafJaTfMv", "cdate": 1451606400000, "mdate": 1682354668998, "content": {"title": "Large-scale rapid-prototyping with zometool", "abstract": "In recent years, personalized fabrication has attracted much attention due to the greatly improved accessibility of consumer-level 3D printers. However, 3D printers still suffer from the relatively long production time and limited output size, which are undesirable for large-scale rapid-prototyping. Zometool, which is a popular building block system widely used for education and entertainment, is potentially suitable for providing an alternative solution to the aforementioned scenarios. However, even for 3D models of moderate complexity, novice users may still have difficulty in building visually plausible results by themselves. Therefore, the goal of this work is to develop an automatic system to assist users to realize Zometool rapid prototyping with a specified 3D shape. Compared with the previous work [Zimmer and Kobbelt 2014], our method may achieve the ease of assembly and economic usage of building units since we focus on generating the Zometool structures through a higher level of shape abstraction."}}
{"id": "hhf-ZvDcVP0", "cdate": 1420070400000, "mdate": 1682354668856, "content": {"title": "Cyclops: Wearable and Single-Piece Full-Body Gesture Input Devices", "abstract": "This paper presents Cyclops, a single-piece wearable device that sees its user's whole body postures through an ego-centric view of the user that is obtained through a fisheye lens at the center of the user's body, allowing it to see only the user's limbs and interpret body postures effectively. Unlike currently available body gesture input systems that depend on external cameras or distributed motion sensors across the user's body, Cyclops is a single-piece wearable device that is worn as a pendant or a badge. The main idea proposed in this paper is the observation of limbs from a central location of the body. Owing to the ego-centric view, Cyclops turns posture recognition into a highly controllable computer vision problem. This paper demonstrates a proof-of-concept device, and an algorithm for recognizing static and moving bodily gestures based on motion history images (MHI) and a random decision forest (RDF). Four example applications of interactive bodily workout, a mobile racing game that involves hands and feet, a full-body virtual reality system, and interaction with a tangible toy are presented. The experiment on the bodily workout demonstrates that, from a database of 20 body workout gestures that were collected from 20 participants, Cyclops achieved a recognition rate of 79% using MHI and simple template matching, which increased to 92% with the more advanced machine learning approach of RDF."}}
