{"id": "Du06rWqW8za", "cdate": 1671946894479, "mdate": 1671946894479, "content": {"title": "Loss landscapes and optimization in over-parameterized non-linear systems and neural networks", "abstract": "The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL\u204e, a variant of the Polyak-\u0141ojasiewicz condition [32], [25] on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL\u204e condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL\u204e-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL\u204e condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL\u204e condition applicable to \u201calmost\u201d over-parameterized systems."}}
{"id": "GNFimGDfEiV", "cdate": 1663849977492, "mdate": null, "content": {"title": "Quadratic models for understanding neural network dynamics", "abstract": "In this work, we show that recently proposed quadratic models capture optimization and generalization properties of wide neural networks that cannot be captured by linear models.  In particular, we prove that quadratic models for shallow ReLU networks exhibit the \"catapult phase\" from Lewkowycz et al. (2020) that arises when training such models with large learning rates. We then empirically show that the behaviour of quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models are an effective tool for analysis of neural networks. "}}
{"id": "Tean8bBjlbB", "cdate": 1652737641350, "mdate": null, "content": {"title": "Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture", "abstract": "In this paper we show that feedforward neural networks corresponding to arbitrary directed acyclic graphs undergo transition to linearity as their ``width'' approaches infinity. The width of these general networks is characterized by the minimum in-degree of their neurons, except for the input and first layers. Our results identify the mathematical structure underlying transition to linearity and generalize a number of recent works aimed at characterizing transition to linearity or constancy of the Neural Tangent Kernel for standard architectures. "}}
{"id": "CyKHoKyvgnp", "cdate": 1632875653632, "mdate": null, "content": {"title": "Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models", "abstract": "Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the neural networks become wide? \nIn this work, we provide a new perspective on this \"transition to linearity\" by considering a neural network as an assembly model recursively built from a set of sub-models corresponding to individual neurons. In this view, we show that the linearity of wide neural networks is, in fact, an emerging property of assembling a large number of diverse ``weak'' sub-models, none of which dominate the assembly. "}}
{"id": "zCcN_4w8FqM", "cdate": 1620699801343, "mdate": null, "content": {"title": "On the linearity of large non-linear models: when and why the tangent kernel is constant", "abstract": "The goal of this work is to shed light on the remarkable phenomenon of\" transition to linearity\" of certain neural networks as their width approaches infinity. We show that the\" transition to linearity''of the model and, equivalently, constancy of the (neural) tangent kernel (NTK) result from the scaling properties of the norm of the Hessian matrix of the network as a function of the network width. We present a general framework for understanding the constancy of the tangent kernel via Hessian scaling applicable to the standard classes of neural networks. Our analysis provides a new perspective on the phenomenon of constant tangent kernel, which is different from the widely accepted\" lazy training''. Furthermore, we show that the\" transition to linearity\" is not a general property of wide neural networks and does not hold when the last layer of the network is non-linear. It is also not necessary for successful optimization by gradient descent."}}
{"id": "0Il9xzzHc1J", "cdate": 1577836800000, "mdate": null, "content": {"title": "Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning", "abstract": "The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization problems corresponding to such systems are generally not convex, even locally. We argue that instead they satisfy PL$^*$, a variant of the Polyak-Lojasiewicz condition on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL$^*$ condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL$^*$-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL$^*$ condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL$^*$ condition applicable to \"almost\" over-parameterized systems."}}
{"id": "r1gixp4FPH", "cdate": 1569438962657, "mdate": null, "content": {"title": "Accelerating SGD with momentum for over-parameterized learning", "abstract": "\nNesterov SGD is widely used for training modern neural networks and other machine learning models. Yet, its advantages over SGD have not been theoretically clarified. Indeed, as we show  in this paper, both theoretically and empirically, Nesterov SGD with any parameter selection does not in general provide acceleration over ordinary SGD. Furthermore, Nesterov SGD may diverge for step sizes that ensure convergence of ordinary SGD. This is in contrast to the classical results in the deterministic setting, where the same step size ensures accelerated convergence of the Nesterov's method over optimal gradient descent.\n\nTo address the non-acceleration issue, we  introduce a compensation term to Nesterov SGD. The resulting  algorithm, which we call MaSS, converges  for same step sizes as SGD. We prove that MaSS obtains an accelerated convergence rates over SGD for any mini-batch size in the linear setting.  For full batch, the convergence rate of MaSS matches the well-known accelerated rate of the Nesterov's method. \n\nWe also analyze the  practically important question of the dependence of the convergence rate and  optimal hyper-parameters on the mini-batch size, demonstrating three distinct regimes: linear scaling, diminishing returns and saturation.\n\nExperimental evaluation of MaSS for several standard  architectures of deep networks, including ResNet and convolutional networks, shows improved performance over SGD, Nesterov SGD  and Adam. "}}
{"id": "kw3bPjMm7e", "cdate": 1514764800000, "mdate": null, "content": {"title": "Parametrized Accelerated Methods Free of Condition Number", "abstract": "Analyses of accelerated (momentum-based) gradient descent usually assume bounded condition number to obtain exponential convergence rates. However, in many real problems, e.g., kernel methods or deep neural networks, the condition number, even locally, can be unbounded, unknown or mis-estimated. This poses problems in both implementing and analyzing accelerated algorithms. In this paper, we address this issue by proposing parametrized accelerated methods by considering the condition number as a free parameter. We provide spectral-level analysis for several important accelerated algorithms, obtain explicit expressions and improve worst case convergence rates. Moreover, we show that those algorithm converge exponentially even when the condition number is unknown or mis-estimated."}}
{"id": "A299U8DJCbe", "cdate": 1514764800000, "mdate": null, "content": {"title": "MaSS: an Accelerated Stochastic Method for Over-parametrized Learning", "abstract": "Nesterov SGD is widely used for training modern neural networks and other machine learning models. Yet, its advantages over SGD have not been theoretically clarified. Indeed, as we show in our paper, both theoretically and empirically, Nesterov SGD with any parameter selection does not in general provide acceleration over ordinary SGD. Furthermore, Nesterov SGD may diverge for step sizes that ensure convergence of ordinary SGD. This is in contrast to the classical results in the deterministic scenario, where the same step size ensures accelerated convergence of the Nesterov's method over optimal gradient descent. To address the non-acceleration issue, we introduce a compensation term to Nesterov SGD. The resulting algorithm, which we call MaSS, converges for same step sizes as SGD. We prove that MaSS obtains an accelerated convergence rates over SGD for any mini-batch size in the linear setting. For full batch, the convergence rate of MaSS matches the well-known accelerated rate of the Nesterov's method. We also analyze the practically important question of the dependence of the convergence rate and optimal hyper-parameters on the mini-batch size, demonstrating three distinct regimes: linear scaling, diminishing returns and saturation. Experimental evaluation of MaSS for several standard architectures of deep networks, including ResNet and convolutional networks, shows improved performance over SGD, Nesterov SGD and Adam."}}
{"id": "SyNfCdWuWr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Clustering with Bregman Divergences: an Asymptotic Analysis", "abstract": "Clustering, in particular $k$-means clustering, is a central topic in data analysis. Clustering with Bregman divergences is a recently proposed generalization of $k$-means clustering which has already been widely used in applications. In this paper we analyze theoretical properties of Bregman clustering when the number of the clusters $k$ is large. We establish quantization rates and describe the limiting distribution of the centers as $k\\to \\infty$, extending well-known results for $k$-means clustering."}}
