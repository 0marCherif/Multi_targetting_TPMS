{"id": "nmL0SDl60Ne", "cdate": 1640995200000, "mdate": 1674967871271, "content": {"title": "NeuroSketch: Fast and Approximate Evaluation of Range Aggregate Queries with Neural Networks", "abstract": "Range aggregate queries (RAQs) are an integral part of many real-world applications, where, often, fast and approximate answers for the queries are desired. Recent work has studied answering RAQs using machine learning (ML) models, where a model of the data is learned to answer the queries. However, there is no theoretical understanding of why and when the ML based approaches perform well. Furthermore, since the ML approaches model the data, they fail to capitalize on any query specific information to improve performance in practice. In this paper, we focus on modeling ``queries'' rather than data and train neural networks to learn the query answers. This change of focus allows us to theoretically study our ML approach to provide a distribution and query dependent error bound for neural networks when answering RAQs. We confirm our theoretical results by developing NeuroSketch, a neural network framework to answer RAQs in practice. Extensive experimental study on real-world, TPC-benchmark and synthetic datasets show that NeuroSketch answers RAQs multiple orders of magnitude faster than state-of-the-art and with better accuracy."}}
{"id": "img67DcGlhI", "cdate": 1640995200000, "mdate": 1652976695113, "content": {"title": "Efficient Convex Optimization Requires Superlinear Memory", "abstract": "We show that any memory-constrained, first-order algorithm which minimizes $d$-dimensional, $1$-Lipschitz convex functions over the unit ball to $1/\\mathrm{poly}(d)$ accuracy using at most $d^{1.25 - \\delta}$ bits of memory must make at least $\\tilde{\\Omega}(d^{1 + (4/3)\\delta})$ first-order queries (for any constant $\\delta \\in [0, 1/4]$). Consequently, the performance of such memory-constrained algorithms are a polynomial factor worse than the optimal $\\tilde{O}(d)$ query bound for this problem obtained by cutting plane methods that use $\\tilde{O}(d^2)$ memory. This resolves a COLT 2019 open problem of Woodworth and Srebro."}}
{"id": "JySuMU-rNlSN", "cdate": 1640995200000, "mdate": 1652723215287, "content": {"title": "Multicalibrated Partitions for Importance Weights", "abstract": "The ratio between the probability that two distributions assign to points in the domain are called importance weights or density ratios and they play a fundamental role in machine learning and info..."}}
{"id": "G_Ab2S5K7Qv", "cdate": 1640995200000, "mdate": 1652723215326, "content": {"title": "KL Divergence Estimation with Multi-group Attribution", "abstract": "Estimating the Kullback-Leibler (KL) divergence between two distributions given samples from them is well-studied in machine learning and information theory. Motivated by considerations of multi-group fairness, we seek KL divergence estimates that accurately reflect the contributions of sub-populations to the overall divergence. We model the sub-populations coming from a rich (possibly infinite) family $\\mathcal{C}$ of overlapping subsets of the domain. We propose the notion of multi-group attribution for $\\mathcal{C}$, which requires that the estimated divergence conditioned on every sub-population in $\\mathcal{C}$ satisfies some natural accuracy and fairness desiderata, such as ensuring that sub-populations where the model predicts significant divergence do diverge significantly in the two distributions. Our main technical contribution is to show that multi-group attribution can be derived from the recently introduced notion of multi-calibration for importance weights [HKRR18, GRSW21]. We provide experimental evidence to support our theoretical results, and show that multi-group attribution provides better KL divergence estimates when conditioned on sub-populations than other popular algorithms."}}
{"id": "G-yAIG8kzaR", "cdate": 1640995200000, "mdate": 1654693721096, "content": {"title": "On the Statistical Complexity of Sample Amplification", "abstract": "Given $n$ i.i.d. samples drawn from an unknown distribution $P$, when is it possible to produce a larger set of $n+m$ samples which cannot be distinguished from $n+m$ i.i.d. samples drawn from $P$? (Axelrod et al. 2019) formalized this question as the sample amplification problem, and gave optimal amplification procedures for discrete distributions and Gaussian location models. However, these procedures and associated lower bounds are tailored to the specific distribution classes, and a general statistical understanding of sample amplification is still largely missing. In this work, we place the sample amplification problem on a firm statistical foundation by deriving generally applicable amplification procedures, lower bound techniques and connections to existing statistical notions. Our techniques apply to a large class of distributions including the exponential family, and establish a rigorous connection between sample amplification and distribution learning."}}
{"id": "DTGf6Qzm7z", "cdate": 1640995200000, "mdate": 1674967871484, "content": {"title": "Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales", "abstract": "We provide new gradient-based methods for efficiently solving a broad class of ill-conditioned optimization problems. We consider the problem of minimizing a function $f : \\mathbb{R}^d \\rightarrow ..."}}
{"id": "3oaRViV-BB", "cdate": 1640995200000, "mdate": 1674967871710, "content": {"title": "Efficient Convex Optimization Requires Superlinear Memory", "abstract": "We show that any memory-constrained, first-order algorithm which minimizes $d$-dimensional, $1$-Lipschitz convex functions over the unit ball to $1/\\mathrm{poly}(d)$ accuracy using at most $d^{1.25..."}}
{"id": "1ampFVRXYVD", "cdate": 1640995200000, "mdate": 1652723215288, "content": {"title": "Omnipredictors", "abstract": "Loss minimization is a dominant paradigm in machine learning, where a predictor is trained to minimize some loss function that depends on an uncertain event (e.g., \"will it rain tomorrow?\"). Different loss functions imply different learning algorithms and, at times, very different predictors. While widespread and appealing, a clear drawback of this approach is that the loss function may not be known at the time of learning, requiring the algorithm to use a best-guess loss function. Alternatively, the same classifier may be used to inform multiple decisions, which correspond to multiple loss functions, requiring multiple learning algorithms to be run on the same data. We suggest a rigorous new paradigm for loss minimization in machine learning where the loss function can be ignored at the time of learning and only be taken into account when deciding an action. We introduce the notion of an (L,\ud835\udc9e)-omnipredictor, which could be used to optimize any loss in a family L. Once the loss function is set, the outputs of the predictor can be post-processed (a simple univariate data-independent transformation of individual predictions) to do well compared with any hypothesis from the class C. The post processing is essentially what one would perform if the outputs of the predictor were true probabilities of the uncertain events. In a sense, omnipredictors extract all the predictive power from the class \ud835\udc9e, irrespective of the loss function in L. We show that such \"loss-oblivious\" learning is feasible through a connection to multicalibration, a notion introduced in the context of algorithmic fairness. A multicalibrated predictor doesn\u2019t aim to minimize some loss function, but rather to make calibrated predictions, even when conditioned on inputs lying in certain sets c belonging to a family \ud835\udc9e which is weakly learnable. We show that a \ud835\udc9e-multicalibrated predictor is also an (L,\ud835\udc9e)-omnipredictor, where L contains all convex loss functions with some mild Lipschitz conditions. The predictors are even omnipredictors with respect to sparse linear combinations of functions in \ud835\udc9e. As a corollary, we deduce that distribution-specific weak agnostic learning is complete for a large class of loss minimization tasks. In addition, we show how multicalibration can be viewed as a solution concept for agnostic boosting, shedding new light on past results. Finally, we transfer our insights back to the context of algorithmic fairness by providing omnipredictors for multi-group loss minimization."}}
{"id": "sI3AVTTJ3bI", "cdate": 1609459200000, "mdate": 1652976694771, "content": {"title": "Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales", "abstract": "We provide new gradient-based methods for efficiently solving a broad class of ill-conditioned optimization problems. We consider the problem of minimizing a function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ which is implicitly decomposable as the sum of $m$ unknown non-interacting smooth, strongly convex functions and provide a method which solves this problem with a number of gradient evaluations that scales (up to logarithmic factors) as the product of the square-root of the condition numbers of the components. This complexity bound (which we prove is nearly optimal) can improve almost exponentially on that of accelerated gradient methods, which grow as the square root of the condition number of $f$. Additionally, we provide efficient methods for solving stochastic, quadratic variants of this multiscale optimization problem. Rather than learn the decomposition of $f$ (which would be prohibitively expensive), our methods apply a clean recursive \"Big-Step-Little-Step\" interleaving of standard methods. The resulting algorithms use $\\tilde{\\mathcal{O}}(d m)$ space, are numerically stable, and open the door to a more fine-grained understanding of the complexity of convex optimization beyond condition number."}}
{"id": "rkQ7gUcFVn", "cdate": 1609459200000, "mdate": 1674967871736, "content": {"title": "One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks", "abstract": "Can deep learning solve multiple, very different tasks simultaneously? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks---for example, when the distinct tasks are encoded by well-separated clusters or decision trees over some task-code attributes. Indeed, more strongly, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that learning many tasks can be provably hard, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation."}}
