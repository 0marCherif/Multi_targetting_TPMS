{"id": "cnt-Mp1YWS", "cdate": 1672531200000, "mdate": 1696174562337, "content": {"title": "The ACM PODS Alberto O. Mendelzon Test-of-Time Award 2023", "abstract": ""}}
{"id": "HeIASYtsNF7", "cdate": 1672531200000, "mdate": 1696174562340, "content": {"title": "On the Complexity of SHAP-Score-Based Explanations: Tractability via Knowledge Compilation and Non-Approximability Results", "abstract": "Scores based on Shapley values are widely used for providing explanations to classification results over machine learning models. A prime example of this is the influential~ Shap-score, a version of the Shapley value that can help explain the result of a learned model on a specific entity by assigning a score to every feature. While in general computing Shapley values is a computationally intractable problem, we prove a strong positive result stating that the Shap-score can be computed in polynomial time over deterministic and decomposable Boolean circuits under the so-called product distributions on entities. Such circuits are studied in the field of Knowledge Compilation and generalize a wide range of Boolean circuits and binary decision diagrams classes, including binary decision trees, Ordered Binary Decision Diagrams (OBDDs) and Free Binary Decision Diagrams (FBDDs). Our positive result extends even beyond binary classifiers, as it continues to hold if each feature is associated with a finite domain of possible values. We also establish the computational limits of the notion of Shap-score by observing that, under a mild condition, computing it over a class of Boolean models is always polynomially as hard as the model counting problem for that class. This implies that both determinism and decomposability are essential properties for the circuits that we consider, as removing one or the other renders the problem of computing the Shap-score intractable (namely, $\\#P$-hard). It also implies that computing Shap-scores is $\\#P$-hard even over the class of propositional formulas in DNF. Based on this negative result, we look for the existence of fully-polynomial randomized approximation schemes (FPRAS) for computing Shap-scores over such class. In stark contrast to the model counting problem for DNF formulas, which admits an FPRAS, we prove that no such FPRAS exists (under widely believed complexity assumptions) for the computation of Shap-scores. Surprisingly, this negative result holds even for the class of monotone formulas in DNF. These techniques can be further extended to prove another strong negative result: Under widely believed complexity assumptions, there is no polynomial-time algorithm that checks, given a monotone DNF formula $\\varphi$ and features $x,y$, whether the Shap-score of $x$ in $\\varphi$ is smaller than the Shap-score of $y$ in $\\varphi$."}}
{"id": "zD65Zdh6ZhI", "cdate": 1652737819976, "mdate": null, "content": {"title": "On Computing Probabilistic Explanations for Decision Trees", "abstract": "  Formal XAI (explainable AI) is a growing area that focuses on computing explanations with mathematical guarantees for the decisions made by ML models. Inside formal XAI, one of the most studied cases is that of explaining the choices taken by decision trees, as they are traditionally deemed as one of the most interpretable classes of models. Recent work has focused on studying the computation of sufficient reasons, a kind of explanation in which given a decision tree $T$ and an instance $x$, one explains the decision $T(x)$ by providing a subset $y$ of the features of $x$ such that for any other instance $z$ compatible with $y$, it holds that  $T(z) = T(x)$, intuitively meaning that the features in $y$ are already enough to fully justify the classification of $x$ by $T$. \nIt has been argued, however, that sufficient reasons constitute a restrictive notion of explanation. For such a reason, the community has started to study their probabilistic counterpart, in which one requires that the probability of $T(z) = T(x)$ must be at least some value $\\delta \\in (0, 1]$, where $z$ is a random instance that is compatible with $y$. Our paper settles the computational complexity of $\\delta$-sufficient-reasons over decision trees, showing that both (1) finding $\\delta$-sufficient-reasons  that are minimal in size, and (2) finding $\\delta$-sufficient-reasons that are minimal inclusion-wise, do not admit polynomial-time algorithms (unless P = NP).\n   This is in stark contrast with the deterministic case ($\\delta = 1$) where inclusion-wise minimal sufficient-reasons are easy to compute. By doing this, we answer two open problems originally raised by Izza et al., and extend the hardness of explanations for Boolean circuits presented by W{\\\"a}ldchen et al. to the more restricted case of decision trees. On the positive side, we identify structural restrictions of decision trees that make the problem tractable, and show how SAT solvers might be able to tackle these problems in practical settings."}}
{"id": "qaQbt9fAC2p", "cdate": 1640995200000, "mdate": 1681565118900, "content": {"title": "On Computing Probabilistic Explanations for Decision Trees", "abstract": ""}}
{"id": "q0SsiPOoWP", "cdate": 1640995200000, "mdate": 1681652066071, "content": {"title": "Counting the Answers to a Query", "abstract": ""}}
{"id": "oGDdy9evLvC", "cdate": 1640995200000, "mdate": 1681652066107, "content": {"title": "Counting the Solutions to a Query (Invited Talk)", "abstract": ""}}
{"id": "fN1TTjQGhc", "cdate": 1640995200000, "mdate": 1681652066085, "content": {"title": "Temporal Regular Path Queries", "abstract": ""}}
{"id": "DrCT2KB8M--", "cdate": 1640995200000, "mdate": 1696174562341, "content": {"title": "On Computing Probabilistic Explanations for Decision Trees", "abstract": "Formal XAI (explainable AI) is a growing area that focuses on computing explanations with mathematical guarantees for the decisions made by ML models. Inside formal XAI, one of the most studied cases is that of explaining the choices taken by decision trees, as they are traditionally deemed as one of the most interpretable classes of models. Recent work has focused on studying the computation of sufficient reasons, a kind of explanation in which given a decision tree $T$ and an instance $x$, one explains the decision $T(x)$ by providing a subset $y$ of the features of $x$ such that for any other instance $z$ compatible with $y$, it holds that $T(z) = T(x)$, intuitively meaning that the features in $y$ are already enough to fully justify the classification of $x$ by $T$. It has been argued, however, that sufficient reasons constitute a restrictive notion of explanation. For such a reason, the community has started to study their probabilistic counterpart, in which one requires that the probability of $T(z) = T(x)$ must be at least some value $\\delta \\in (0, 1]$, where $z$ is a random instance that is compatible with $y$. Our paper settles the computational complexity of $\\delta$-sufficient-reasons over decision trees, showing that both (1) finding $\\delta$-sufficient-reasons that are minimal in size, and (2) finding $\\delta$-sufficient-reasons that are minimal inclusion-wise, do not admit polynomial-time algorithms (unless P = NP). This is in stark contrast with the deterministic case ($\\delta = 1$) where inclusion-wise minimal sufficient-reasons are easy to compute. By doing this, we answer two open problems originally raised by Izza et al., and extend the hardness of explanations for Boolean circuits presented by W{\\\"a}ldchen et al. to the more restricted case of decision trees. On the positive side, we identify structural restrictions of decision trees that make the problem tractable, and show how SAT solvers might be able to tackle these problems in practical settings."}}
{"id": "Jyxmk4wUoQV", "cdate": 1621630150235, "mdate": null, "content": {"title": "Foundations of Symbolic Languages for Model Interpretability", "abstract": "Several queries and scores have recently been proposed to explain individual predictions over ML models. Examples include queries based on \u201canchors\u201d, which are parts of an instance that are sufficient to justify its classification, and \u201cfeature-perturbation\u201d scores such as SHAP. Given the need for flexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, which allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over two classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated.  We also present a prototype implementation of FOIL wrapped in a high-level declarative language and perform experiments showing that such a language can be used in practice."}}
{"id": "vSVjpXL7oZQ", "cdate": 1609459200000, "mdate": 1681652066264, "content": {"title": "Temporal Regular Path Queries: Syntax, Semantics, and Complexity", "abstract": ""}}
