{"id": "PAFEQQtDf8s", "cdate": 1655376329792, "mdate": null, "content": {"title": "CoBEVT: Cooperative Bird\u2019s Eye View Semantic Segmentation with Sparse Transformers", "abstract": "Bird\u2019s eye view (BEV) semantic segmentation plays a crucial role in spatial sensing for autonomous driving. Although recent literature has made significant progress on BEV map understanding, they are all based on single-agent camera-based systems. These solutions sometimes have difficulty handling occlusions or detecting distant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V) communication technologies have enabled autonomous vehicles to share sensing information, dramatically improving the perception performance and range compared to single-agent systems. In this paper, we propose CoBEVT, the first generic multi-agent multi-camera perception framework that can cooperatively generate BEV map predictions. To efficiently fuse camera features from multi-view and multi-agent data in an underlying Transformer architecture, we design a fused axial attention module (FAX), which captures sparsely local and global spatial interactions across views and agents. The extensive experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT achieves state-of-the-art performance for cooperative BEV semantic segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks, including 1) BEV segmentation with single-agent multi-camera and 2) 3D object detection with multi-agent LiDAR systems, achieving state-of-the-art performance with real-time inference speed. The code is available at https://github.com/DerrickXuNu/CoBEVT."}}
{"id": "tGtFhi2tTw", "cdate": 1640995200000, "mdate": 1668267231040, "content": {"title": "Pik-Fix: Restoring and Colorizing Old Photos", "abstract": "Restoring and inpainting the visual memories that are present, but often impaired, in old photos remains an intriguing but unsolved research topic. Decades-old photos often suffer from severe and commingled degradation such as cracks, defocus, and color-fading, which are difficult to treat individually and harder to repair when they interact. Deep learning presents a plausible avenue, but the lack of large-scale datasets of old photos makes addressing this restoration task very challenging. Here we present a novel reference-based end-to-end learning framework that is able to both repair and colorize old, degraded pictures. Our proposed framework consists of three modules: a restoration sub-network that conducts restoration from degradations, a similarity network that performs color histogram matching and color transfer, and a colorization subnet that learns to predict the chroma elements of images conditioned on chromatic reference signals. The overall system makes uses of color histogram priors from reference images, which greatly reduces the need for large-scale training data. We have also created a first-of-a-kind public dataset of real old photos that are paired with ground truth ''pristine'' photos that have been manually restored by PhotoShop experts. We conducted extensive experiments on this dataset and synthetic datasets, and found that our method significantly outperforms previous state-of-the-art models using both qualitative comparisons and quantitative measurements. The code is available at https://github.com/DerrickXuNu/Pik-Fix."}}
{"id": "t3vI4B10dzo", "cdate": 1640995200000, "mdate": 1668267231028, "content": {"title": "Cooperative Perception for Estimating and Predicting Microscopic Traffic States to Manage Connected and Automated Traffic", "abstract": "Real-time traffic state estimation and prediction are of importance to the traffic management systems. New opportunities are enabled by the emerging sensing and automation technologies to manage connected and automated traffic, particularly in terms of controlling trajectories of automated vehicles. Traffic information from connected and automated vehicles (CAV) and roadside detectors (RSD) can be fused and has great potential for providing detailed microscopic traffic states (i.e., vehicle speeds, positions) of all vehicles. In this paper, we propose a cooperative perception framework for this purpose. The proposed framework based on particle filtering is developed to provide an accurate estimation and prediction of the microscopic states of partially observed traffic systems, while accounting for different sources of errors that intrinsically exist in the system, including those from sensor data, vehicle movement, and process models. Selected freeway and arterial vehicle trajectory datasets from the Next Generation Simulation (NGSIM) program and CAV traffic simulation are applied to test the proposed methodological framework. The accuracy of position and speed estimation is between 50% and 70% when the CAV market penetration rate (MPR) is 12.5%, and between 80% and 90% when the MPR is 50%. The incorporation of RSD data can further increase the accuracy by up to 10% under low CAV MPRs. The framework can also provide an accurate short-term prediction (i.e., 5 \u2013 15 seconds) of position and speed with 60% to 90% accuracy. The proposed framework provides efficient and accurate estimations and predictions of detailed microscopic traffic states, even at low CAV MPRs, creating dynamic traffic environment world models to enable fine control and management of the connected and automated traffic systems."}}
{"id": "kdIFxtpC1IA", "cdate": 1640995200000, "mdate": 1668267231098, "content": {"title": "Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather", "abstract": "Most object detection methods for autonomous driving usually assume a consistent feature distribution between training and testing data, which is not always the case when weathers differ significantly. The object detection model trained under clear weather might not be effective enough in foggy weather because of the domain gap. This paper proposes a novel domain adaptive object detection framework for autonomous driving under foggy weather. Our method leverages both image-level and object-level adaptation to diminish the domain discrepancy in image style and object appearance. To further enhance the model's capabilities under challenging samples, we also come up with a new adversarial gradient reversal layer to perform adversarial mining for the hard examples together with domain adaptation. Moreover, we propose to generate an auxiliary domain by data augmentation to enforce a new domain-level metric regularization. Experimental results on public benchmarks show the effectiveness and accuracy of the proposed method. The code is available at https://github.com/jinlong17/DA-Detect."}}
{"id": "areihyqKFS", "cdate": 1640995200000, "mdate": 1668267231103, "content": {"title": "Model-Agnostic Multi-Agent Perception Framework", "abstract": "Existing multi-agent perception systems assume that every agent utilizes the same model with identical parameters and architecture. The performance can be degraded with different perception models due to the mismatch in their confidence scores. In this work, we propose a model-agnostic multi-agent perception framework to reduce the negative effect caused by the model discrepancies without sharing the model information. Specifically, we propose a confidence calibrator that can eliminate the prediction confidence score bias. Each agent performs such calibration independently on a standard public database to protect intellectual property. We also propose a corresponding bounding box aggregation algorithm that considers the confidence scores and the spatial agreement of neighboring boxes. Our experiments shed light on the necessity of model calibration across different agents, and the results show that the proposed framework improves the baseline 3D object detection performance of heterogeneous agents."}}
{"id": "WmZLfx1vbjW", "cdate": 1640995200000, "mdate": 1668267231136, "content": {"title": "CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers", "abstract": "Bird's eye view (BEV) semantic segmentation plays a crucial role in spatial sensing for autonomous driving. Although recent literature has made significant progress on BEV map understanding, they are all based on single-agent camera-based systems. These solutions sometimes have difficulty handling occlusions or detecting distant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V) communication technologies have enabled autonomous vehicles to share sensing information, dramatically improving the perception performance and range compared to single-agent systems. In this paper, we propose CoBEVT, the first generic multi-agent multi-camera perception framework that can cooperatively generate BEV map predictions. To efficiently fuse camera features from multi-view and multi-agent data in an underlying Transformer architecture, we design a fused axial attention module (FAX), which captures sparsely local and global spatial interactions across views and agents. The extensive experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT achieves state-of-the-art performance for cooperative BEV semantic segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks, including 1) BEV segmentation with single-agent multi-camera and 2) 3D object detection with multi-agent LiDAR systems, achieving state-of-the-art performance with real-time inference speed. The code is available at https://github.com/DerrickXuNu/CoBEVT."}}
{"id": "SyxP1kCjHx", "cdate": 1640995200000, "mdate": 1668267231031, "content": {"title": "Evaluating the Effectiveness of Integrated Connected Automated Vehicle Applications Applied to Freeway Managed Lanes", "abstract": "The purpose of this study is to define an operational concept involving connected automated vehicle (CAV) operation on freeway managed lanes. Despite the low projected market penetration of CAVs during the next decade, the use of managed lane facilities has the potential to support the realization of increased mobility benefits by their very nature. The proposed CAV operation involves platoons of equipped vehicles governed by integrated CAV applications, including cooperative adaptive cruise control (CACC), cooperative merge, and speed harmonization. This study proposes an algorithm for integrating CAV applications. Through microscopic simulation, the study particularly examines the effectiveness of CACC, CACC plus cooperative merge, and the addition of speed harmonization under different penetration rates. Simulation results show the effectiveness of the bundled application to enhance system throughput and reduce delay, even with low CAV penetration rates. The speed harmonization shows the greatest effects on delay reduction at medium-to-high penetration rates and some benefits even at low penetration rates. The conclusions provide operational insights and guidance for traffic management centers to implement CAV-based traffic control in the future."}}
{"id": "RQlHX0lPKW", "cdate": 1640995200000, "mdate": 1668267231042, "content": {"title": "OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication", "abstract": "Employing Vehicle-to-Vehicle communication to enhance perception performance in self-driving technology has attracted considerable attention recently; however, the absence of a suitable open dataset for benchmarking algorithms has made it difficult to develop and assess cooperative perception technologies. To this end, we present the first large-scale open simulated dataset for Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles. We then construct a comprehensive benchmark with a total of 16 implemented models to evaluate several information fusion strategies (i.e. early, late, and intermediate fusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose a new Attentive Intermediate Fusion pipeline to aggregate information from multiple connected vehicles. Our experiments show that the proposed pipeline can be easily integrated with existing 3D LiDAR detectors and achieve outstanding performance even with large compression rates. To encourage more researchers to investigate Vehicle-to-Vehicle perception, we will release the dataset, benchmark methods, and all related codes in https://mobility-lab.seas.ucla.edu/opv2v/."}}
{"id": "OHO52ijXf8f", "cdate": 1640995200000, "mdate": 1668267231035, "content": {"title": "ROMNet: Renovate the Old Memories", "abstract": "Renovating the memories in old photos is an intriguing research topic in computer vision fields. These legacy images often suffer from severe and commingled degradations such as cracks, noise, and color-fading, while lack of large-scale paired old photo datasets makes this restoration task very challenging. In this work, we present a novel reference-based end-to-end learning framework that can jointly repair and colorize the degraded legacy pictures. Specifically, the proposed framework consists of three modules: a restoration sub-network for degradation restoration, a similarity sub-network for color histogram matching and transfer, and a colorization subnet that learns to predict the chroma elements of the images conditioned on chromatic reference signals. The whole system takes advantage of the color histogram priors in a given reference image, which vastly reduces the dependency on large-scale training data. Apart from the proposed method, we also create, to our knowledge, the first public and real-world old photo dataset with paired ground truth for evaluating old photo restoration models, wherein each old photo is paired with a manually restored pristine image by PhotoShop experts. Our extensive experiments conducted on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-arts both quantitatively and qualitatively."}}
{"id": "LJtZNojuTZE", "cdate": 1640995200000, "mdate": 1668267231141, "content": {"title": "V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer", "abstract": "In this paper, we investigate the application of Vehicle-to-Everything (V2X) communication to improve the perception performance of autonomous vehicles. We present a robust cooperative perception framework with V2X communication using a novel vision Transformer. Specifically, we build a holistic attention model, namely V2X-ViT, to effectively fuse information across on-road agents (i.e., vehicles and infrastructure). V2X-ViT consists of alternating layers of heterogeneous multi-agent self-attention and multi-scale window self-attention, which captures inter-agent interaction and per-agent spatial relationships. These key modules are designed in a unified Transformer architecture to handle common V2X challenges, including asynchronous information sharing, pose errors, and heterogeneity of V2X components. To validate our approach, we create a large-scale V2X perception dataset using CARLA and OpenCDA. Extensive experimental results demonstrate that V2X-ViT sets new state-of-the-art performance for 3D object detection and achieves robust performance even under harsh, noisy environments. The code is available at https://github.com/DerrickXuNu/v2x-vit."}}
