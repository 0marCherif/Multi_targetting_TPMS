{"id": "hOus35-_2d", "cdate": 1672531200000, "mdate": 1688669991721, "content": {"title": "On the Unreasonable Effectiveness of Single Vector Krylov Methods for Low-Rank Approximation", "abstract": "Krylov subspace methods are a ubiquitous tool for computing near-optimal rank $k$ approximations of large matrices. While \"large block\" Krylov methods with block size at least $k$ give the best known theoretical guarantees, block size one (a single vector) or a small constant is often preferred in practice. Despite their popularity, we lack theoretical bounds on the performance of such \"small block\" Krylov methods for low-rank approximation. We address this gap between theory and practice by proving that small block Krylov methods essentially match all known low-rank approximation guarantees for large block methods. Via a black-box reduction we show, for example, that the standard single vector Krylov method run for $t$ iterations obtains the same spectral norm and Frobenius norm error bounds as a Krylov method with block size $\\ell \\geq k$ run for $O(t/\\ell)$ iterations, up to a logarithmic dependence on the smallest gap between sequential singular values. That is, for a given number of matrix-vector products, single vector methods are essentially as effective as any choice of large block size. By combining our result with tail-bounds on eigenvalue gaps in random matrices, we prove that the dependence on the smallest singular value gap can be eliminated if the input matrix is perturbed by a small random matrix. Further, we show that single vector methods match the more complex algorithm of [Bakshi et al. `22], which combines the results of multiple block sizes to achieve an improved algorithm for Schatten $p$-norm low-rank approximation."}}
{"id": "T7t1BxZ-eJ3", "cdate": 1672531200000, "mdate": 1694900827346, "content": {"title": "Hutchinson's Estimator is Bad at Kronecker-Trace-Estimation", "abstract": "We study the problem of estimating the trace of a matrix $\\mathbf{A}$ that can only be accessed through Kronecker-matrix-vector products. That is, for any Kronecker-structured vector $\\boldsymbol{\\mathrm{x}} = \\otimes_{i=1}^k \\boldsymbol{\\mathrm{x}}_i$, we can compute $\\mathbf{A}\\boldsymbol{\\mathrm{x}}$. We focus on the natural generalization of Hutchinson's Estimator to this setting, proving tight rates for the number of matrix-vector products this estimator needs to find a $(1\\pm\\varepsilon)$ approximation to the trace of $\\mathbf{A}$. We find an exact equation for the variance of the estimator when using a Kronecker of Gaussian vectors, revealing an intimate relationship between Hutchinson's Estimator, the partial trace operator, and the partial transpose operator. Using this equation, we show that when using real vectors, in the worst case, this estimator needs $O(\\frac{3^k}{\\varepsilon^2})$ products to recover a $(1\\pm\\varepsilon)$ approximation of the trace of any PSD $\\mathbf{A}$, and a matching lower bound for certain PSD $\\mathbf{A}$. However, when using complex vectors, this can be exponentially improved to $\\Theta(\\frac{2^k}{\\varepsilon^2})$. We show that Hutchinson's Estimator converges slowest when $\\mathbf{A}$ itself also has Kronecker structure. We conclude with some theoretical evidence suggesting that, by combining Hutchinson's Estimator with other techniques, it may be possible to avoid the exponential dependence on $k$."}}
{"id": "60jITslU90", "cdate": 1672531200000, "mdate": 1681766656374, "content": {"title": "Near-Linear Sample Complexity for L Polynomial Regression.p", "abstract": "We study Lp polynomial regression. Given query access to a function f : [\u22121,1]\u2192\u211d, the goal is to find a degree d polynomial q\u0302 such that, for a given parameter \u03b5 > 0 Here || \u00b7 ||p is the Lp norm, \u2016g\u2016p = (\u222b1\u22121|g(t)|p dt)1/p. We show that querying f at points randomly drawn from the Chebyshev measure on [-1,1] is a near-optimal strategy for polynomial regression in all Lp norms. In particular, to find q\u0302, it suffices to sample points from [-1,1] with probabilities proportional to this measure. While the optimal sample complexity for polynomial regression was well understood for L2 and L\u221e, our result is the first that achieves sample complexity linear in d and error (1 + \u03b5) for other values of p without any assumptions. Our result requires two main technical contributions. The first concerns p \u2264 2, for which we provide explicit bounds on the Lp Lewis weight function of the infinite linear operator underlying polynomial regression. Using tools from the orthogonal polynomial literature, we show that this function is bounded by the Chebyshev density. Our second key contribution is to take advantage of the structure of polynomials to reduce the p > 2 case to the p \u2264 2 case. By doing so, we obtain a better sample complexity than what is possible for general p-norm linear regression problems, for which \u03a9(dp/2) samples are required."}}
{"id": "5CdSyCpqKk", "cdate": 1672531200000, "mdate": 1703287052340, "content": {"title": "Algorithm-agnostic low-rank approximation of operator monotone matrix functions", "abstract": "Low-rank approximation of a matrix function, $f(A)$, is an important task in computational mathematics. Most methods require direct access to $f(A)$, which is often considerably more expensive than accessing $A$. Persson and Kressner (SIMAX 2023) avoid this issue for symmetric positive semidefinite matrices by proposing funNystr\\\"om, which first constructs a Nystr\\\"om approximation to $A$ using subspace iteration, and then uses the approximation to directly obtain a low-rank approximation for $f(A)$. They prove that the method yields a near-optimal approximation whenever $f$ is a continuous operator monotone function with $f(0) = 0$. We significantly generalize the results of Persson and Kressner beyond subspace iteration. We show that if $\\widehat{A}$ is a near-optimal low-rank Nystr\\\"om approximation to $A$ then $f(\\widehat{A})$ is a near-optimal low-rank approximation to $f(A)$, independently of how $\\widehat{A}$ is computed. Further, we show sufficient conditions for a basis $Q$ to produce a near-optimal Nystr\\\"om approximation $\\widehat{A} = AQ(Q^T AQ)^{\\dagger} Q^T A$. We use these results to establish that many common low-rank approximation methods produce near-optimal Nystr\\\"om approximations to $A$ and therefore to $f(A)$."}}
{"id": "LDeZVwp6gYA", "cdate": 1640995200000, "mdate": 1681689083923, "content": {"title": "Fast Regression for Structured Inputs", "abstract": "We study the $\\ell_p$ regression problem, which requires finding $\\mathbf{x}\\in\\mathbb R^{d}$ that minimizes $\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\|_p$ for a matrix $\\mathbf{A}\\in\\mathbb R^{n \\times d}$ and response vector $\\mathbf{b}\\in\\mathbb R^{n}$. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when $n$ is very large. However, all known subsampling approaches have run time that depends exponentially on $p$, typically, $d^{\\mathcal{O}(p)}$, which can be prohibitively expensive. We improve on this work by showing that for a large class of common \\emph{structured matrices}, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for $\\ell_p$ regression that depend polynomially on $p$. For example, we give an algorithm for $\\ell_p$ regression on Vandermonde matrices that runs in time $\\mathcal{O}(n\\log^3 n+(dp^2)^{0.5+\\omega}\\cdot\\text{polylog}\\,n)$, where $\\omega$ is the exponent of matrix multiplication. The polynomial dependence on $p$ crucially allows our algorithms to extend naturally to efficient algorithms for $\\ell_\\infty$ regression, via approximation of $\\ell_\\infty$ by $\\ell_{\\mathcal{O}(\\log n)}$. Of practical interest, we also develop a new subsampling algorithm for $\\ell_p$ regression for arbitrary matrices, which is simpler than previous approaches for $p \\ge 4$."}}
{"id": "gNp54NxHUPJ", "cdate": 1632875759918, "mdate": null, "content": {"title": "Fast Regression for Structured Inputs", "abstract": "We study the $\\ell_p$ regression problem, which requires finding $\\mathbf{x}\\in\\mathbb R^{d}$ that minimizes $\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\|_p$ for a matrix $\\mathbf{A}\\in\\mathbb R^{n \\times d}$ and response vector $\\mathbf{b}\\in\\mathbb R^{n}$. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when $n$ is very large. However, all known subsampling approaches have run time that depends exponentially on $p$, typically, $d^{\\mathcal{O}(p)}$, which can be prohibitively expensive. \n\nWe improve on this work by showing that for a large class of common \\emph{structured matrices}, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for $\\ell_p$ regression that depend polynomially on $p$. For example, we give an algorithm for $\\ell_p$ regression on Vandermonde matrices that runs in time $\\mathcal{O}(n\\log^3 n+(dp^2)^{0.5+\\omega}\\cdot\\text{polylog}\\,n)$, where $\\omega$ is the exponent of matrix multiplication. The polynomial dependence on $p$ crucially allows our algorithms to extend naturally to efficient algorithms for $\\ell_\\infty$ regression, via approximation of $\\ell_\\infty$ by $\\ell_{\\mathcal{O}(\\log n)}$. Of practical interest, we also develop a new subsampling algorithm for $\\ell_p$ regression for arbitrary matrices, which is simpler than previous approaches for $p \\ge 4$."}}
{"id": "L1rKwGLR049", "cdate": 1577836800000, "mdate": null, "content": {"title": "Hutch++: Optimal Stochastic Trace Estimation", "abstract": "We study the problem of estimating the trace of a matrix $A$ that can only be accessed through matrix-vector multiplication. We introduce a new randomized algorithm, Hutch++, which computes a $(1 \\pm \\epsilon)$ approximation to $tr(A)$ for any positive semidefinite (PSD) $A$ using just $O(1/\\epsilon)$ matrix-vector products. This improves on the ubiquitous Hutchinson's estimator, which requires $O(1/\\epsilon^2)$ matrix-vector products. Our approach is based on a simple technique for reducing the variance of Hutchinson's estimator using a low-rank approximation step, and is easy to implement and analyze. Moreover, we prove that, up to a logarithmic factor, the complexity of Hutch++ is optimal amongst all matrix-vector query algorithms, even when queries can be chosen adaptively. We show that it significantly outperforms Hutchinson's method in experiments. While our theory mainly requires $A$ to be positive semidefinite, we provide generalized guarantees for general square matrices, and show empirical gains in such applications."}}
{"id": "--KCQbpBMDa", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Statistical Cost of Robust Kernel Hyperparameter Turning", "abstract": "This paper studies the statistical complexity of kernel hyperparameter tuning in the setting of active regression under adversarial noise. We consider the problem of finding the best interpolant from a class of kernels with unknown hyperparameters, assuming only that the noise is square-integrable. We provide finite-sample guarantees for the problem, characterizing how increasing the complexity of the kernel class increases the complexity of learning kernel hyperparameters. For common kernel classes (e.g. squared-exponential kernels with unknown lengthscale), our results show that hyperparameter optimization increases sample complexity by just a logarithmic factor, in comparison to the setting where optimal parameters are known in advance. Our result is based on a subsampling guarantee for linear regression under multiple design matrices which may be of independent interest."}}
{"id": "BkWJaqW_ZS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Optimality Implies Kernel Sum Classifiers are Statistically Efficient", "abstract": "We propose a novel combination of optimization tools with learning theory bounds in order to analyze the sample complexity of optimal kernel sum classifiers. This contrasts the typical learning the..."}}
{"id": "jPMVa2lg7Qx", "cdate": 1483228800000, "mdate": null, "content": {"title": "Characterizing optimal security and round-complexity for secure OR evaluation", "abstract": "Secure multi-party computation allows mutually distrusting parties to compute securely over their private data. However, even in the semi-honest two-party setting, most interesting functions cannot be computed securely in the information-theoretic plain model. Intuitively, the objective of accurately evaluating the output of such functions is inherently inimical to the privacy concerns of the parties. Securely evaluating OR of the input bits of two parties is the simplest example, and captures the essence of the hardness in securely evaluating most functions. This work studies the interplay between accuracy and privacy of secure 2-party function evaluation in the information-theoretic plain model. We provide an optimal accuracy versus privacy tradeoff for computing OR(x, y), where x and y are, respectively, the private input bits of Alice and Bob. In particular, we construct a round-optimal two-party protocol for OR that has maximum semi-honest security in the information-theoretic plain model. Prior results exhibit only weak tradeoffs that are far from the optimal. We generalize our techniques to obtain a tight accuracy-versus-privacy tradeoff characterization for a stronger notion of security, namely differentially-private semi-honest security. The technical heart of our result is a new technique to derive inequalities for distributions of transcripts generated by protocols. This approach reduces the domain of the optimization problem from an unbounded number of transcripts to a constant size while preserving the optimal solution to the original problem. We believe that these techniques for analyzing protocols in the information-theoretic plain model will be of independent interest."}}
