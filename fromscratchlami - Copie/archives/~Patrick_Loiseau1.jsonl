{"id": "NJhEwfqPjT", "cdate": 1672531200000, "mdate": 1683295447650, "content": {"title": "Dissecting Bitcoin and Ethereum Transactions: On the Lack of Transaction Contention and Prioritization Transparency in Blockchains", "abstract": "In permissionless blockchains, transaction issuers include a fee to incentivize miners to include their transactions. To accurately estimate this prioritization fee for a transaction, transaction issuers (or blockchain participants, more generally) rely on two fundamental notions of transparency, namely contention and prioritization transparency. Contention transparency implies that participants are aware of every pending transaction that will contend with a given transaction for inclusion. Prioritization transparency states that the participants are aware of the transaction or prioritization fees paid by every such contending transaction. Neither of these notions of transparency holds well today. Private relay networks, for instance, allow users to send transactions privately to miners. Besides, users can offer fees to miners via either direct transfers to miners' wallets or off-chain payments -- neither of which are public. In this work, we characterize the lack of contention and prioritization transparency in Bitcoin and Ethereum resulting from such practices. We show that private relay networks are widely used and private transactions are quite prevalent. We show that the lack of transparency facilitates miners to collude and overcharge users who may use these private relay networks despite them offering little to no guarantees on transaction prioritization. The lack of these transparencies in blockchains has crucial implications for transaction issuers as well as the stability of blockchains. Finally, we make our data sets and scripts publicly available."}}
{"id": "LffWuGtC9BE", "cdate": 1652737715788, "mdate": null, "content": {"title": "Bounding and Approximating Intersectional Fairness through Marginal Fairness", "abstract": "Discrimination in machine learning often arises along multiple dimensions (a.k.a. protected attributes); it is then desirable to ensure \\emph{intersectional fairness}---i.e., that no subgroup is discriminated against. It is known that ensuring \\emph{marginal fairness} for every dimension independently is not sufficient in general. Due to the exponential number of subgroups, however, directly measuring intersectional fairness from data is impossible. In this paper, our primary goal is to understand in detail the relationship between marginal and intersectional fairness through statistical analysis. We first identify a set of sufficient conditions under which an exact relationship can be obtained. Then, we prove bounds (easily computable through marginal fairness and other meaningful statistical quantities) in high-probability on intersectional fairness in the general case. Beyond their descriptive value, we show that these theoretical bounds can be leveraged to derive a heuristic improving the approximation and bounds of intersectional fairness by choosing, in a relevant manner, protected attributes for which we describe intersectional subgroups. Finally, we test the performance of our approximations and bounds on real and synthetic data-sets."}}
{"id": "qOyTE6nxNh", "cdate": 1640995200000, "mdate": 1673738268914, "content": {"title": "Asymptotic Degradation of Linear Regression Estimates with Strategic Data Sources", "abstract": ""}}
{"id": "cC8FdkF9R01", "cdate": 1640995200000, "mdate": 1683295447758, "content": {"title": "Fairness in Selection Problems with Strategic Candidates", "abstract": "To better understand discriminations and the effect of affirmative actions in selection problems (e.g., college admission or hiring), a recent line of research proposed a model based on differential variance. This model assumes that the decision-maker has a noisy estimate of each candidate's quality and puts forward the difference in the noise variances between different demographic groups as a key factor to explain discrimination. The literature on differential variance, however, does not consider the strategic behavior of candidates who can react to the selection procedure to improve their outcome, which is well-known to happen in many domains. In this paper, we study how the strategic aspect affects fairness in selection problems. We propose to model selection problems with strategic candidates as a contest game: A population of rational candidates compete by choosing an effort level to increase their quality. They incur a cost-of-effort but get a (random) quality whose expectation equals the chosen effort. A Bayesian decision-maker observes a noisy estimate of the quality of each candidate (with differential variance) and selects the fraction $\\alpha$ of best candidates based on their posterior expected quality; each selected candidate receives a reward $S$. We characterize the (unique) equilibrium of this game in the different parameters' regimes, both when the decision-maker is unconstrained and when they are constrained to respect the fairness notion of demographic parity. Our results reveal important impacts of the strategic behavior on the discrimination observed at equilibrium and allow us to understand the effect of imposing demographic parity in this context. In particular, we find that, in many cases, the results contrast with the non-strategic setting."}}
{"id": "U8zgcS0HuqZ", "cdate": 1640995200000, "mdate": 1683295447650, "content": {"title": "Bounding and Approximating Intersectional Fairness through Marginal Fairness", "abstract": "Discrimination in machine learning often arises along multiple dimensions (a.k.a. protected attributes); it is then desirable to ensure \\emph{intersectional fairness} -- i.e., that no subgroup is discriminated against. It is known that ensuring \\emph{marginal fairness} for every dimension independently is not sufficient in general. Due to the exponential number of subgroups, however, directly measuring intersectional fairness from data is impossible. In this paper, our primary goal is to understand in detail the relationship between marginal and intersectional fairness through statistical analysis. We first identify a set of sufficient conditions under which an exact relationship can be obtained. Then, we prove bounds (easily computable through marginal fairness and other meaningful statistical quantities) in high-probability on intersectional fairness in the general case. Beyond their descriptive value, we show that these theoretical bounds can be leveraged to derive a heuristic improving the approximation and bounds of intersectional fairness by choosing, in a relevant manner, protected attributes for which we describe intersectional subgroups. Finally, we test the performance of our approximations and bounds on real and synthetic data-sets."}}
{"id": "SWVbOZ1qUg5", "cdate": 1640995200000, "mdate": 1645809408052, "content": {"title": "On fair selection in the presence of implicit and differential variance", "abstract": "Discrimination in selection problems such as hiring or college admission is often explained by implicit bias from the decision maker against disadvantaged demographic groups. In this paper, we consider a model where the decision maker receives a noisy estimate of each candidate's quality, whose variance depends on the candidate's group\u2014we argue that such differential variance is a key feature of many selection problems. We analyze two notable settings: in the first, the noise variances are unknown to the decision maker who simply picks the candidates with the highest estimated quality independently of their group; in the second, the variances are known and the decision maker picks candidates having the highest expected quality given the noisy estimate. We show that both baseline decision makers yield discrimination, although in opposite directions: the first leads to underrepresentation of the low-variance group while the second leads to underrepresentation of the high-variance group. We study the effect on the selection utility of imposing a fairness mechanism that we term the \u03b3-rule (it is an extension of the classical four-fifths rule and it also includes demographic parity). In the first setting (with unknown variances), we prove that under mild conditions, imposing the \u03b3-rule increases the selection utility\u2014here there is no trade-off between fairness and utility. In the second setting (with known variances), imposing the \u03b3-rule decreases the utility but we prove a bound on the utility loss due to the fairness mechanism."}}
{"id": "S3iHz3ibQt", "cdate": 1640995200000, "mdate": 1683295447877, "content": {"title": "Introducing the Expohedron for Efficient Pareto-optimal Fairness-Utility Amortizations in Repeated Rankings", "abstract": "We consider the problem of computing a sequence of rankings that maximizes consumer-side utility while minimizing producer-side individual unfairness of exposure. While prior work has addressed this problem using linear or quadratic programs on bistochastic matrices, such approaches, relying on Birkhoff-von Neumann (BvN) decompositions, are too slow to be implemented at large scale. In this paper we introduce a geometrical object, a polytope that we call expohedron, whose points represent all achievable exposures of items for a Position Based Model (PBM). We exhibit some of its properties and lay out a Carath\u00e9odory decomposition algorithm with complexity $O(n^2\u0142og(n))$ able to express any point inside the expohedron as a convex sum of at most n vertices, where n is the number of items to rank. Such a decomposition makes it possible to express any feasible target exposure as a distribution over at most n rankings. Furthermore we show that we can use this polytope to recover the whole Pareto frontier of the multi-objective fairness-utility optimization problem, using a simple geometrical procedure with complexity $O(n^2\u0142og(n))$. Our approach compares favorably to linear or quadratic programming baselines in terms of algorithmic complexity and empirical runtime and is applicable to any merit that is a non-decreasing function of item relevance. Furthermore our solution can be expressed as a distribution over only $\\ndoc$ permutations, instead of the $(n-1)^2 + 1$ achieved with BvN decompositions. We perform experiments on synthetic and real-world datasets, confirming our theoretical results."}}
{"id": "RPcz9knZNeN", "cdate": 1640995200000, "mdate": 1683295447719, "content": {"title": "Introducing the Expohedron for Efficient Pareto-optimal Fairness-Utility Amortizations in Repeated Rankings", "abstract": "We consider the problem of computing a sequence of rankings that maximizes consumer-side utility while minimizing producer-side individual unfairness of exposure. While prior work has addressed this problem using linear or quadratic programs on bistochastic matrices, such approaches, relying on Birkhoff-von Neumann (BvN) decompositions, are too slow to be implemented at large scale. In this paper we introduce a geometrical object, a polytope that we call expohedron, whose points represent all achievable exposures of items for a Position Based Model (PBM). We exhibit some of its properties and lay out a Carath\\'eodory decomposition algorithm with complexity $O(n^2\\log(n))$ able to express any point inside the expohedron as a convex sum of at most $n$ vertices, where $n$ is the number of items to rank. Such a decomposition makes it possible to express any feasible target exposure as a distribution over at most $n$ rankings. Furthermore we show that we can use this polytope to recover the whole Pareto frontier of the multi-objective fairness-utility optimization problem, using a simple geometrical procedure with complexity $O(n^2\\log(n))$. Our approach compares favorably to linear or quadratic programming baselines in terms of algorithmic complexity and empirical runtime and is applicable to any merit that is a non-decreasing function of item relevance. Furthermore our solution can be expressed as a distribution over only $n$ permutations, instead of the $(n-1)^2 + 1$ achieved with BvN decompositions. We perform experiments on synthetic and real-world datasets, confirming our theoretical results."}}
{"id": "GbPj0Q4bXf", "cdate": 1640995200000, "mdate": 1683295447870, "content": {"title": "Fairness in Selection Problems with Strategic Candidates", "abstract": "To better understand discriminations and the effect of affirmative actions in selection problems (e.g., college admission or hiring), a recent line of research proposed a model based on differential variance. This model assumes that the decision-maker has a noisy estimate of each candidate's quality and puts forward the difference in the noise variances between different demographic groups as a key factor to explain discrimination. The literature on differential variance, however, does not consider the strategic behavior of candidates who can react to the selection procedure to improve their outcome, which is well-known to happen in many domains. In this paper, we study how the strategic aspect affects fairness in selection problems. We propose to model selection problems with strategic candidates as a contest game: A population of rational candidates compete by choosing an effort level to increase their quality. They incur a cost-of-effort but get a (random) quality whose expectation equals the chosen effort. A Bayesian decision-maker observes a noisy estimate of the quality of each candidate (with differential variance) and selects the fraction \u03b1 of best candidates based on their posterior expected quality; each selected candidate receives a reward S. We characterize the (unique) equilibrium of this game in the different parameters' regimes, both when the decision-maker is unconstrained and when they are constrained to respect the fairness notion of demographic parity. Our results reveal important impacts of the strategic behavior on the discrimination observed at equilibrium and allow us to understand the effect of imposing demographic parity in this context. In particular, we find that, in many cases, the results contrast with the non-strategic setting. We also find that, when the cost-of-effort depends on the demographic group (which is reasonable in many cases), then it entirely governs the observed discrimination (i.e., the noise becomes a second-order effect that does not have any impact on discrimination). Finally we find that imposing demographic parity can sometimes increase the quality of the selection at equilibrium; which surprisingly contrasts with the optimality of the Bayesian decision-maker in the non-strategic case. Our results give a new perspective on fairness in selection problems, relevant in many domains where strategic behavior is a reality."}}
{"id": "9D0OVNkgq6", "cdate": 1640995200000, "mdate": 1683295447657, "content": {"title": "Statistical Discrimination in Stable Matchings", "abstract": "Statistical discrimination results when a decision-maker observes an imperfect estimate of the quality of each candidate dependent on which demographic group they belong to [1,8]. Imperfect estimates have been modelled via noise, where the variance depends on the candidate's group ([4,6,7]). Prior literature, however, is limited to simple selection problems, where a single decision-maker tries to choose the best candidates among the applications they received. In this paper, we initiate the study of statistical discrimination in matching, where multiple decision-makers are simultaneously facing selection problems from the same pool of candidates. We consider the college admission problem as first introduced in [5] and recently extended to a model with a continuum of students [3]. We propose a model where two colleges A and B observe noisy estimates of each candidate's quality, where Ws, the vector of estimates for student s, is assumed to be a bivariate normal random variable. In this setting, the estimation noise controls a new key feature of the problem, namely correlation, \u03c1, between the estimates of the two colleges: if the noise is high, the correlation is low and if the noise is low the correlation is high. We assume that the population of students is divided into two groups G1 and G2, and that members of these two groups are subject to different correlation levels between their grades at colleges A and B. Concretely, for each student s, their grade vector (WAs, WBs) is drawn according to a centered bivariate normal distribution with variance 1 and covariance \u03c1Gs, where Gs is the group student s belongs to. We consider the stable matching induced by this distribution and characterize how key outcome characteristics vary with the parameters, in particular with the group-dependent correlation coefficient. Our results summarize as follows: We show that the probability that a student is assigned to their first choice is independent of the student's group, but that it decreases when the correlation of either group decreases. This means that higher measurement noise (inducing lower correlation) on one group hurts not only the students of that group, but the students of all groups. We show that the probability that a student is assigned to their second choice and the probability that they remain unassigned both depend on the student's group, which reveals the presence of statistical discrimination coming from the correlation effect alone. Specifically, we find that the probability that a student remains unmatched is decreasing when the correlation of their group decreases (higher measurement noise) and when the correlation of the other group increases. In other words, the higher the measurement noise of their own group, the better off students are with regard to getting assigned a college at all. This is somewhat counter-intuitive, but is explained by the observation that with high noise (i.e., low correlation) the fact that a student is rejected from one college gives only little information about the outcome at the other college. That is, a student has an independent second chance for admission. These two comparative static results give insights on the effect of correlation on the stable matching outcome for different demographic groups and show that indeed, statistical discrimination is an important theory to understand discrimination in matching problems. We also analyze a number of special cases of our model, in particular the case of a single group, to show that even in this case correlation affects the outcome. It is interesting to notice that the effect of correlation on the number of students getting their first choice in our model is the same as in [2], i.e., a higher correlation leads to more students getting their first choice. Our work is the first to investigate statistical discrimination in the context of matching. Overall we find that group-dependent measurement noises of the candidates quality---and the resulting group-dependent correlation between the colleges' estimates---play an important role in leading to unequal outcomes for different demographic groups, and in particular underrepresentation of one of the groups. Of course, we do not argue that statistical discrimination is the only possible cause of discrimination. In particular, if there is bias in the quality estimates for one group, then it will naturally also hurt the representation of that group. We do not model bias since our primary purpose is to isolate the effect of statistical discrimination. Throughout the paper, we make a number of other simplifying assumptions (e.g., focusing on two colleges) whose purpose is also to simplify our results and isolate the effect of correlation. Our analysis, however, extends to more general contexts."}}
