{"id": "IbFFIbtiU3J", "cdate": 1695617965413, "mdate": 1695617965413, "content": {"title": "On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis", "abstract": "In this paper, we study random subsampling of Gaussian process regression, one of the simplest approximation baselines, from a theoretical perspective. Although subsampling discards a large part of training data, we show provable guarantees on the accuracy of the predictive mean/variance and its generalization ability.For analysis, we consider embedding kernel matrices into graphons, which encapsulate the difference of the sample size and enables us to evaluate the approximation and generalization errors in a unified manner. The experimental results show that the subsampling approximation achieves a better trade-off regarding accuracy and runtime than the ystrom and random Fourier expansion methods."}}
{"id": "qnRlh8BdMY", "cdate": 1675827737137, "mdate": null, "content": {"title": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns", "abstract": "We present TabRet, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called retokenizing, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains. The code is available at https://github.com/pfnet-research/tabret."}}
{"id": "dYDEg9cs-t", "cdate": 1672531200000, "mdate": 1681654366934, "content": {"title": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns", "abstract": ""}}
{"id": "7lYV8JrKRxY", "cdate": 1653750180090, "mdate": null, "content": {"title": "Learning Switchable Representation with Masked Decoding and Sparse Encoding", "abstract": "In this study, we explore the unsupervised learning based on private/shared factor decomposition, which decomposes the latent space into private factors that vary only in a specific domain the shared factors that vary in all domains. We study when/how we can force the model to respect the true private/shared factor decomposition that underlies the dataset. We show that, when we train a masked decoder and an encoder with sparseness regularization in the latent space, we can identify the true private/shared decomposition up to mixing within each component. We empirically confirm this result and study the efficacy of this training strategy as a representation learning method. "}}
{"id": "rqJO9LKpNe", "cdate": 1640995200000, "mdate": 1682317788508, "content": {"title": "A Scaling Law for Syn2real Transfer: How Much Is Your Pre-training Effective?", "abstract": "Synthetic-to-real transfer learning is a framework in which a synthetically generated dataset is used to pre-train a model to improve its performance on real vision tasks. The most significant advantage of using synthetic images is that the ground-truth labels are automatically available, enabling unlimited expansion of the data size without human cost. However, synthetic data may have a huge domain gap, in which case increasing the data size does not improve the performance. How can we know that? In this study, we derive a simple scaling law that predicts the performance from the amount of pre-training data. By estimating the parameters of the law, we can judge whether we should increase the data or change the setting of image synthesis. Further, we analyze the theory of transfer learning by considering learning dynamics and confirm that the derived generalization bound is consistent with our empirical findings. We empirically validated our scaling law on various experimental settings of benchmark tasks, model sizes, and complexities of synthetic images."}}
{"id": "QhHMf5J5Jom", "cdate": 1632875446764, "mdate": null, "content": {"title": "A Scaling Law for Syn-to-Real Transfer: How Much Is Your Pre-training Effective?", "abstract": "Synthetic-to-real transfer learning is a framework in which a synthetically generated dataset is used to pre-train a model to improve its performance on real vision tasks. The most significant advantage of using synthetic images is that the ground-truth labels are automatically available, enabling unlimited data size expansion without human cost. However, synthetic data may have a huge domain gap, in which case increasing the data size does not improve the performance. How can we know that? In this study, we derive a simple scaling law that predicts the performance from the amount of pre-training data. By estimating the parameters of the law, we can judge whether we should increase the data or change the setting of image synthesis. Further, we analyze the theory of transfer learning by considering learning dynamics and confirm that the derived generalization bound is compatible with our empirical findings. We empirically validated our scaling law on various experimental settings of benchmark tasks, model sizes, and complexities of synthetic images."}}
{"id": "GouxTdU9Fbo", "cdate": 1609459200000, "mdate": 1682317788175, "content": {"title": "A Scaling Law for Synthetic-to-Real Transfer: A Measure of Pre-Training", "abstract": "Synthetic-to-real transfer learning is a framework in which a synthetically generated dataset is used to pre-train a model to improve its performance on real vision tasks. The most significant advantage of using synthetic images is that the ground-truth labels are automatically available, enabling unlimited expansion of the data size without human cost. However, synthetic data may have a huge domain gap, in which case increasing the data size does not improve the performance. How can we know that? In this study, we derive a simple scaling law that predicts the performance from the amount of pre-training data. By estimating the parameters of the law, we can judge whether we should increase the data or change the setting of image synthesis. Further, we analyze the theory of transfer learning by considering learning dynamics and confirm that the derived generalization bound is consistent with our empirical findings. We empirically validated our scaling law on various experimental settings of benchmark tasks, model sizes, and complexities of synthetic images."}}
{"id": "Y8LsNmKyC0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Weisfeiler-Lehman Embedding for Molecular Graph Neural Networks", "abstract": "A graph neural network (GNN) is a good choice for predicting the chemical properties of molecules. Compared with other deep networks, however, the current performance of a GNN is limited owing to the \"curse of depth.\" Inspired by long-established feature engineering in the field of chemistry, we expanded an atom representation using Weisfeiler-Lehman (WL) embedding, which is designed to capture local atomic patterns dominating the chemical properties of a molecule. In terms of representability, we show WL embedding can replace the first two layers of ReLU GNN -- a normal embedding and a hidden GNN layer -- with a smaller weight norm. We then demonstrate that WL embedding consistently improves the empirical performance over multiple GNN architectures and several molecular graph datasets."}}
{"id": "Ql1sm8565Z", "cdate": 1577836800000, "mdate": 1682317788386, "content": {"title": "On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis", "abstract": "In this paper, we study random subsampling of Gaussian process regression, one of the simplest approximation baselines, from a theoretical perspective. Although subsampling discards a large part of..."}}
{"id": "PFtp8qYuoVp", "cdate": 1577836800000, "mdate": 1682317788303, "content": {"title": "Testing Proximity to Subspaces: Approximate \u2113 \u221e Minimization in Constant Time", "abstract": ""}}
