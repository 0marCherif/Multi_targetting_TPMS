{"id": "r3Wi2kOv1N9", "cdate": 1672531200000, "mdate": 1695966718409, "content": {"title": "Looking through the past: better knowledge retention for generative replay in continual learning", "abstract": "In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained model to make them closer to the original data. Our method outperforms other generative replay methods in various scenarios. Code available at https://github.com/valeriya-khan/looking-through-the-past."}}
{"id": "QaZa3805LR", "cdate": 1672531200000, "mdate": 1695966718410, "content": {"title": "Learning Data Representations with Joint Diffusion Models", "abstract": "Joint machine learning models that allow synthesizing and classifying data often offer uneven performance between those tasks or are unstable to train. In this work, we depart from a set of empirical observations that indicate the usefulness of internal representations built by contemporary deep diffusion-based generative models not only for generating but also predicting. We then propose to extend the vanilla diffusion model with a classifier that allows for stable joint end-to-end training with shared parameterization between those objectives. The resulting joint diffusion model outperforms recent state-of-the-art hybrid methods in terms of both classification and generation quality on all evaluated benchmarks. On top of our joint training approach, we present how we can directly benefit from shared generative and discriminative representations by introducing a method for visual counterfactual explanations."}}
{"id": "IuFjvr8N4mC", "cdate": 1672531200000, "mdate": 1684210975150, "content": {"title": "Exploring Continual Learning of Diffusion Models", "abstract": "Diffusion models have achieved remarkable success in generating high-quality images thanks to their novel training procedures applied to unprecedented amounts of data. However, training a diffusion model from scratch is computationally expensive. This highlights the need to investigate the possibility of training these models iteratively, reusing computation while the data distribution changes. In this study, we take the first step in this direction and evaluate the continual learning (CL) properties of diffusion models. We begin by benchmarking the most common CL methods applied to Denoising Diffusion Probabilistic Models (DDPMs), where we note the strong performance of the experience replay with the reduced rehearsal coefficient. Furthermore, we provide insights into the dynamics of forgetting, which exhibit diverse behavior across diffusion timesteps. We also uncover certain pitfalls of using the bits-per-dimension metric for evaluating CL."}}
{"id": "GlDCzL-1LG", "cdate": 1672531200000, "mdate": 1680197334367, "content": {"title": "Learning Data Representations with Joint Diffusion Models", "abstract": ""}}
{"id": "nxl-IjnDCRo", "cdate": 1652737703563, "mdate": null, "content": {"title": "On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models", "abstract": "Diffusion-based Deep Generative Models (DDGMs) offer state-of-the-art performance in generative modeling. Their main strength comes from their unique setup in which a model (the backward diffusion process) is trained to reverse the forward diffusion process, which gradually adds noise to the input signal. Although DDGMs are well studied, it is still unclear how the small amount of noise is transformed during the backward diffusion process. Here, we focus on analyzing this problem to gain more insight into the behavior of DDGMs and their denoising and generative capabilities. We observe a fluid transition point that changes the functionality of the backward diffusion process from generating a (corrupted) image from noise to denoising the corrupted image to the final sample. Based on this observation, we postulate to divide a DDGM into two parts: a denoiser and a generator. The denoiser could be parameterized by a denoising auto-encoder, while the generator is a diffusion-based model with its own set of parameters. We experimentally validate our proposition, showing its pros and cons."}}
{"id": "xrqx0Hj19d4", "cdate": 1640995200000, "mdate": 1680197334413, "content": {"title": "Multiband VAE: Latent Space Alignment for Knowledge Consolidation in Continual Learning", "abstract": ""}}
{"id": "d0RI4Pxl3-", "cdate": 1640995200000, "mdate": 1680197334442, "content": {"title": "Logarithmic Continual Learning", "abstract": ""}}
{"id": "Dv8bJq5eIVt", "cdate": 1640995200000, "mdate": 1680197334530, "content": {"title": "On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models", "abstract": ""}}
{"id": "mJMCnEse-M", "cdate": 1609459200000, "mdate": 1680197334381, "content": {"title": "BinPlay: A Binary Latent Autoencoder for Generative Replay Continual Learning", "abstract": ""}}
{"id": "kotl508zi7", "cdate": 1609459200000, "mdate": 1680197334317, "content": {"title": "On Robustness of Generative Representations Against Catastrophic Forgetting", "abstract": ""}}
