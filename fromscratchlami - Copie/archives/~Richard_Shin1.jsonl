{"id": "jo9S8hTOeUh", "cdate": 1640995200000, "mdate": 1671489742757, "content": {"title": "Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation", "abstract": "We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances. Our goal is to improve a low-resource semantic parser using utterances collected through user interactions. In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs. We find that such approaches are effective despite our restrictive setup: in a low-resource setting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), we observe 33% relative improvement over a non-data-augmented baseline in top-1 match."}}
{"id": "d2TE1Szcuz", "cdate": 1640995200000, "mdate": 1671489742389, "content": {"title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Semantic Parsing", "abstract": "We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, which produces semantic outputs based on the analysis of input text through constrained decoding of a prompted or fine-tuned language model. Developers of pretrained language models currently benchmark on classification, span extraction and free-text generation tasks. Semantic parsing is neglected in language model evaluation because of the complexity of handling task-specific architectures and representations. Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. BenchCLAMP includes context-free grammars for six semantic parsing datasets with varied output meaning representations, as well as a constrained decoding interface to generate outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports both prompt-based learning as well as fine-tuning, and provides an easy-to-use toolkit for language model developers to evaluate on semantic parsing."}}
{"id": "brjehQTJPqS", "cdate": 1640995200000, "mdate": 1671489742566, "content": {"title": "Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation", "abstract": ""}}
{"id": "aK5EfVWBJwp", "cdate": 1640995200000, "mdate": 1671489742730, "content": {"title": "Guided K-best Selection for Semantic Parsing Annotation", "abstract": "Anton Belyy, Chieh-yang Huang, Jacob Andreas, Emmanouil Antonios Platanios, Sam Thomson, Richard Shin, Subhro Roy, Aleksandr Nisnevich, Charles Chen, Benjamin Van Durme. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2022."}}
{"id": "KoCOEKZBgXW", "cdate": 1640995200000, "mdate": 1671489742576, "content": {"title": "Few-Shot Semantic Parsing with Language Models Trained on Code", "abstract": ""}}
{"id": "m6A3drsfnR", "cdate": 1609459200000, "mdate": 1649908912024, "content": {"title": "Constrained Language Models Yield Few-Shot Semantic Parsers", "abstract": "Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "K8w2D9DhLV1", "cdate": 1609459200000, "mdate": 1671489742566, "content": {"title": "Few-Shot Semantic Parsing with Language Models Trained On Code", "abstract": "Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets."}}
{"id": "DfRvsIbpLJz", "cdate": 1609459200000, "mdate": 1632945187789, "content": {"title": "Constrained Language Models Yield Few-Shot Semantic Parsers", "abstract": "We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data."}}
{"id": "4ODjWoblo2B", "cdate": 1609459200000, "mdate": 1671489742708, "content": {"title": "Pruning Pretrained Encoders with a Multitask Objective", "abstract": "The sizes of pretrained language models make them challenging and expensive to use when there are multiple desired downstream tasks. In this work, we adopt recent strategies for model pruning during finetuning to explore the question of whether it is possible to prune a single encoder so that it can be used for multiple tasks. We allocate a fixed parameter budget and compare pruning a single model with a multitask objective against the best ensemble of single-task models. We find that under two pruning strategies (element-wise and rank pruning), the approach with the multitask objective outperforms training models separately when averaged across all tasks, and it is competitive on each individual one. Additional analysis finds that using a multitask objective during pruning can also be an effective method for reducing model sizes for low-resource tasks."}}
{"id": "Rix64aa1UNN", "cdate": 1577836800000, "mdate": 1632945188322, "content": {"title": "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers", "abstract": "Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
