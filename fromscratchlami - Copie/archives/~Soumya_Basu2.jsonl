{"id": "0nroZT5gHsS", "cdate": 1663850135588, "mdate": null, "content": {"title": "Generalization Properties of Retrieval-based Models", "abstract": "Many modern high-performing machine learning models such as GPT-3 primarily rely on scaling up models, e.g., transformer networks. Simultaneously, a parallel line of work aims to improve the model performance by augmenting an input instance with other (labeled) instances during inference. Examples of such augmentations include task-specific prompts and similar examples retrieved from the training data by a nonparametric component. Remarkably, retrieval-based methods have enjoyed success on a wide range of problems, ranging from standard natural language processing and vision tasks to protein folding, as demonstrated by many recent efforts, including WebGPT and AlphaFold. Despite growing literature showcasing the promise of these models, the theoretical underpinning for such models remains underexplored. In this paper, we present a formal treatment of retrieval-based models to characterize their generalization ability. In particular, we focus on two classes of retrieval-based classification approaches: First, we analyze a local learning framework that employs an explicit local empirical risk minimization based on retrieved examples for each input instance. Interestingly, we show that breaking down the underlying learning task into local sub-tasks enables the model to employ a low complexity parametric component to ensure good overall accuracy. The second class of retrieval-based approaches we explore learns a global model using kernel methods to directly map an input instance and retrieved examples to a prediction, without explicitly solving a local learning task."}}
{"id": "Mj6MVmGyMDb", "cdate": 1621629943354, "mdate": null, "content": {"title": "No Regrets for Learning the Prior in Bandits", "abstract": "We propose AdaTS, a Thompson sampling algorithm that adapts sequentially to bandit tasks that it interacts with. The key idea in AdaTS is to adapt to an unknown task prior distribution by maintaining a distribution over its parameters. When solving a bandit task, that uncertainty is marginalized out and properly accounted for. AdaTS is a fully-Bayesian algorithm that can be implemented efficiently in several classes of bandit problems. We derive upper bounds on its Bayes regret that quantify the loss due to not knowing the task prior, and show that it is small. Our theory is supported by experiments, where AdaTS outperforms prior algorithms and works well even in challenging real-world problems."}}
{"id": "ZxeqYAoQ6I", "cdate": 1612369600781, "mdate": null, "content": {"title": "Learning Mixtures of Graphs from Epidemic Cascades", "abstract": "We consider the problem of learning the weighted edges of a balanced mixture of two undirected graphs from epidemic cascades. While mixture models are popular modeling tools, algorithmic development with rigorous guarantees has lagged. Graph mixtures are apparently no exception: until now, very little is known about whether this problem is solvable. To the best of our knowledge, we establish the first necessary and sufficient conditions for this problem to be solvable in polynomial time on edge-separated graphs. When the conditions are met, ie, when the graphs are connected with at least three edges, we give an efficient algorithm for learning the weights of both graphs with optimal sample complexity (up to log factors). We give complementary results and provide sample-optimal (up to log factors) algorithms for mixtures of directed graphs of out-degree at least three, and for mixture of undirected graphs of unbalanced and/or unknown priors."}}
{"id": "JlgR7ftH97P", "cdate": 1596129697725, "mdate": null, "content": {"title": "Dominate or Delete: Decentralized Competing Bandits with Uniform Valuation", "abstract": "We study regret minimization problems in a two-sided matching market where uniformly\nvalued demand side agents (a.k.a. agents) continuously compete for getting matched with\nsupply side agents (a.k.a. arms) with unknown and heterogeneous valuations. Such markets\nabstract online matching platforms (for e.g. UpWork, TaskRabbit) and falls within the purview\nof matching bandit models introduced in Liu et al. [24]. The uniform valuation in the demand\nside admits a unique stable matching equilibrium in the system. We design the first decentralized\nalgorithm - UCB with Decentralized Dominant-arm Deletion (UCB-D3), for matching bandits\nunder uniform valuation that does not require any knowledge of reward gaps or time horizon,\nand thus partially resolves an open question in [24]. UCB-D3 works in phases of exponentially\nincreasing length. In each phase i, an agent first deletes dominated arms \u2013 the arms preferred by\nagents ranked higher than itself. Deletion follows dynamic explore-exploit using UCB algorithm\non the remaining arms for 2^i rounds. Finally, the preferred arm is broadcast in a decentralized\nfashion to other agents through pure exploitation in (N \u2212 1)K rounds with N agents and K\narms. Comparing the obtained reward with respect to the unique stable matching, we show that UCB-D3 achieves O(log(T)/\u2206^2) regret in T rounds, where \u2206 is the minimum gap across all agents and arms. We provide a (orderwise) matching regret lower-bound."}}
{"id": "zhyAqWaMIeN", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Generalization of Adaptive Methods for Over-parameterized Linear Regression", "abstract": "Over-parameterization and adaptive methods have played a crucial role in the success of deep learning in the last decade. The widespread use of over-parameterization has forced us to rethink generalization by bringing forth new phenomena, such as implicit regularization of optimization algorithms and double descent with training progression. A series of recent works have started to shed light on these areas in the quest to understand -- why do neural networks generalize well? The setting of over-parameterized linear regression has provided key insights into understanding this mysterious behavior of neural networks. In this paper, we aim to characterize the performance of adaptive methods in the over-parameterized linear regression setting. First, we focus on two sub-classes of adaptive methods depending on their generalization performance. For the first class of adaptive methods, the parameter vector remains in the span of the data and converges to the minimum norm solution like gradient descent (GD). On the other hand, for the second class of adaptive methods, the gradient rotation caused by the pre-conditioner matrix results in an in-span component of the parameter vector that converges to the minimum norm solution and the out-of-span component that saturates. Our experiments on over-parameterized linear regression and deep neural networks support this theory."}}
{"id": "sBmGIZg3tLP", "cdate": 1577836800000, "mdate": null, "content": {"title": "Contextual Blocking Bandits", "abstract": "We study a novel variant of the multi-armed bandit problem, where at each time step, the player observes an independently sampled context that determines the arms' mean rewards. However, playing an arm blocks it (across all contexts) for a fixed and known number of future time steps. The above contextual setting, which captures important scenarios such as recommendation systems or ad placement with diverse users, invalidates greedy solution techniques that are effective for its non-contextual counterpart (Basu et al., NeurIPS19). Assuming knowledge of the context distribution and the mean reward of each arm-context pair, we cast the problem as an online bipartite matching problem, where the right-vertices (contexts) arrive stochastically and the left-vertices (arms) are blocked for a finite number of rounds each time they are matched. This problem has been recently studied in the full-information case, where competitive ratio bounds have been derived. We focus on the bandit setting, where the reward distributions are initially unknown; we propose a UCB-based variant of the full-information algorithm that guarantees a $\\mathcal{O}(\\log T)$-regret w.r.t. an $\\alpha$-optimal strategy in $T$ time steps, matching the $\\Omega(\\log(T))$ regret lower bound in this setting. Due to the time correlations caused by blocking, existing techniques for upper bounding regret fail. For proving our regret bounds, we introduce the novel concepts of delayed exploitation and opportunistic subsampling and combine them with ideas from combinatorial bandits and non-stationary Markov chains coupling."}}
{"id": "eVdYFvJ6ShM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Stochastic Linear Bandits with Protected Subspace", "abstract": "We study a variant of the stochastic linear bandit problem wherein we optimize a linear objective function but rewards are accrued only orthogonal to an unknown subspace (which we interpret as a \\textit{protected space}) given only zero-order stochastic oracle access to both the objective itself and protected subspace. In particular, at each round, the learner must choose whether to query the objective or the protected subspace alongside choosing an action. Our algorithm, derived from the OFUL principle, uses some of the queries to get an estimate of the protected space, and (in almost all rounds) plays optimistically with respect to a confidence set for this space. We provide a $\\tilde{O}(sd\\sqrt{T})$ regret upper bound in the case where the action space is the complete unit ball in $\\mathbb{R}^d$, $s < d$ is the dimension of the protected subspace, and $T$ is the time horizon. Moreover, we demonstrate that a discrete action space can lead to linear regret with an optimistic algorithm, reinforcing the sub-optimality of optimism in certain settings. We also show that protection constraints imply that for certain settings, no consistent algorithm can have a regret smaller than $\\Omega(T^{3/4}).$ We finally empirically validate our results with synthetic and real datasets."}}
{"id": "L9XSKGfiogn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Warm Starting Bandits with Side Information from Confounded Data", "abstract": "We study a variant of the multi-armed bandit problem where side information in the form of bounds on the mean of each arm is provided. We develop the novel non-optimistic Global Under-Explore (GLUE) algorithm which uses the provided mean bounds (across all the arms) to infer pseudo-variances for each arm, which in turn decide the rate of exploration for the arms. We analyze the regret of GLUE and prove regret upper bounds that are never worse than that of the standard UCB algorithm. Furthermore, we show that GLUE improves upon regret guarantees that exists in literature for structured bandit problems (both theoretically and empirically). Finally, we study the practical setting of learning adaptive interventions using prior data that has been confounded by unrecorded variables that affect rewards. We show that mean bounds can be inferred naturally from such logs and can thus be used to improve the learning process. We validate our findings through semi-synthetic experiments on data derived from real data sets."}}
{"id": "GB1tNHa6ReY", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Mixtures of Graphs from Epidemic Cascades", "abstract": "We consider the problem of learning the weighted edges of a balanced mixture of two undirected graphs from epidemic cascades. While mixture models are popular modeling tools, algorithmic developmen..."}}
{"id": "-3xsgJpvjTq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Dominate or Delete: Decentralized Competing Bandits with Uniform Valuation", "abstract": "Online learning in a two-sided matching market, with demand side agents continuously competing to be matched with supply side (arms), abstracts the complex interactions under partial information on matching platforms (e.g. UpWork, TaskRabbit). We study the decentralized serial dictatorship setting, a two-sided matching market where the demand side agents have unknown and heterogeneous valuation over the supply side (arms), while the arms have known uniform preference over the demand side (agents). We design the first decentralized algorithm -- UCB with Decentralized Dominant-arm Deletion (UCB-D3), for the agents, that does not require any knowledge of reward gaps or time horizon. UCB-D3 works in phases, where in each phase, agents delete \\emph{dominated arms} -- the arms preferred by higher ranked agents, and play only from the non-dominated arms according to the UCB. At the end of the phase, agents broadcast in a decentralized fashion, their estimated preferred arms through {\\em pure exploitation}. We prove both, a new regret lower bound for the decentralized serial dictatorship model, and that UCB-D3 is order optimal."}}
