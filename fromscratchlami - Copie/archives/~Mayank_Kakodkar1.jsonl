{"id": "0TUa6mo164", "cdate": 1664248829085, "mdate": null, "content": {"title": "Conditional Invariances for Conformer Invariant Protein Representations", "abstract": "Representation learning for proteins is an emerging area in geometric deep learning. Recent works have factored in both the relational (atomic bonds) and the geometric aspects (atomic positions) of the task, notably bringing together graph neural networks (GNNs) with neural networks for point clouds. The equivariances and invariances to geometric transformations (group actions such as rotations and translations) so far treats large molecules as rigid structures. However, in many important settings, proteins can co-exist as an ensemble of multiple stable conformations. The conformations of a protein, however, cannot be described as input-independent transformations of the protein: Two proteins may require different sets of transformations in order to describe their set of viable conformations. To address this limitation, we introduce the concept of conditional transformations (CT). CT can capture protein structure, while respecting the restrictions posed by constraints on dihedral (torsion) angles and steric repulsions between atoms. We then introduce a Markov chain Monte Carlo framework to learn representations that are invariant to these conditional transformations. Our results show that endowing existing baseline models with these conditional transformations helps improve their performance without sacrificing computational cost."}}
{"id": "xOFD5BMwsB", "cdate": 1663849965503, "mdate": null, "content": {"title": "Conditional Invariances for Conformer Invariant Protein Representations", "abstract": "Representation learning for proteins is an emerging area in geometric deep learning. Recent works have factored in both the relational (atomic bonds) and the geometric aspects (atomic positions) of the task, notably bringing together graph neural networks (GNNs) with neural networks for point clouds. The equivariances and invariances to geometric transformations (group actions such as rotations and translations) so far treats large molecules as rigid structures. However, in many important settings, proteins can co-exist as an ensemble of multiple stable conformations. The conformations of a protein, however, cannot be described as input-independent transformations of the protein: Two proteins may require different sets of transformations in order to describe their set of viable conformations. To address this limitation, we introduce the concept of conditional transformations (CT). CT can capture protein structure, while respecting the restrictions posed by constraints on dihedral (torsion) angles and steric repulsions between atoms. We then introduce a Markov chain Monte Carlo framework to learn representations that are invariant to these conditional transformations. Our results show that endowing existing baseline models with these conditional transformations helps improve their performance without sacrificing computational cost."}}
{"id": "ZKwfgQYjK-4", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sequential Stratified Regeneration: MCMC for Large State Spaces with an Application to Subgraph Counting Estimation", "abstract": "This work considers the general task of estimating the sum of a bounded function over the edges of a graph, given neighborhood query access and where access to the entire network is prohibitively expensive. To estimate this sum, prior work proposes Markov chain Monte Carlo (MCMC) methods that use random walks started at some seed vertex and whose equilibrium distribution is the uniform distribution over all edges, eliminating the need to iterate over all edges. Unfortunately, these existing estimators are not scalable to massive real-world graphs. In this paper, we introduce Ripple, an MCMC-based estimator that achieves unprecedented scalability by stratifying the Markov chain state space into ordered strata with a new technique that we denote {\\em sequential stratified regenerations}. We show that the Ripple estimator is consistent, highly parallelizable, and scales well. We empirically evaluate our method by applying Ripple to the task of estimating connected, induced subgraph counts given some input graph. Therein, we demonstrate that Ripple is accurate and can estimate counts of up to $12$-node subgraphs, which is a task at a scale that has been considered unreachable, not only by prior MCMC-based methods but also by other sampling approaches. For instance, in this target application, we present results in which the Markov chain state space is as large as $10^{43}$, for which Ripple computes estimates in less than $4$ hours, on average."}}
{"id": "BhXrsgU5QCR", "cdate": 1546300800000, "mdate": null, "content": {"title": "Curated Pathways to Innovation: Personalized CS Education to Promote Diversity", "abstract": "The lack of diversity in computing is a well-known issue. This poster is a work-in-progress report on Curated Pathways to Innovation (CPI), a web-based tool which gathers existing online resources for computer science (CS) engagement and learning to allow students to learn more about CS careers and content, with a particular focus on improving participation of K-12 girls and under-represented minorities in CS. This project is a collaboration of people from academia in CS and social science, K-12 education, non-profit, and industry. We are about halfway through a 3-year pilot deployment of CPI with all students in a low-income, primarily Latino/a middle school with nearly 500 students, and smaller deployments have been undertaken and are planned for 2018-19. In addition to online content, we have created in-person experiences, including reverse science fairs, summer camps, and a hackathon, which are tracked in the CPI tool. To measure impact, we conduct regular surveys with the students measuring their interest in CS, self-efficacy, and other metrics. Our evaluation of the system based on survey data has helped inform the development of the system and curriculum, but remains preliminary. This poster also discusses the tool itself. It uses gamification in the form of badges to measure student progress. From the beginning, the vision was to use machine learning to customize recommendations based on students' demographics, background, and past performance. This integration is coming to fruition at the same time we are including more interesting visuals in the UI, such as an avatar and animations."}}
{"id": "ByNzXJbOWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine Training Through Stopping Sets", "abstract": "We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC) estimators of Restricted Boltzmann Machines (RBMs). We denote our approach Markov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange for random running times. MCLV uses a stopping set built from the training data and has maximum number of Markov chain steps K (referred as MCLV-K). We present a MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and differences between LVS-K and Contrastive Divergence (CD-K), with LVS-K significantly outperforming CD-K training RBMs over the MNIST dataset, indicating MCLV to be a promising direction in learning generative models."}}
