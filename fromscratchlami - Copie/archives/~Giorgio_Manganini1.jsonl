{"id": "Irb__XxDA0", "cdate": 1640995200000, "mdate": 1673272615697, "content": {"title": "Balancing Sample Efficiency and Suboptimality in Inverse Reinforcement Learning", "abstract": ""}}
{"id": "_P3XZtp7_k", "cdate": 1577836800000, "mdate": 1673272615693, "content": {"title": "Resource Aware Multifidelity Active Learning for Efficient Optimization", "abstract": ""}}
{"id": "_2LwmJ-RWz-", "cdate": 1546300800000, "mdate": 1673272615693, "content": {"title": "A graph-based optimization framework for the Energy Management of District Systems", "abstract": ""}}
{"id": "qLENLILPrx", "cdate": 1483228800000, "mdate": 1673272615695, "content": {"title": "A data-based approach to power capacity optimization", "abstract": ""}}
{"id": "Y9hLafpbl4W", "cdate": 1451606400000, "mdate": null, "content": {"title": "Policy Search for the Optimal Control of Markov Decision Processes: A Novel Particle-Based Iterative Scheme", "abstract": "Classical approximate dynamic programming techniques based on state-space gridding become computationally impracticable for high-dimensional problems. Policy search techniques cope with this curse of dimensionality issue by searching for the optimal control policy in a restricted parameterized policy space. We here focus on the case of discrete action space and introduce a novel policy parametrization that adopts particles to describe the map from the state space to the action space, each particle representing a region of the state space that is mapped into a certain action. The locations and actions associated with the particles describing a policy can be tuned by means of a recently introduced policy gradient method with parameter-based exploration. The task of selecting an appropriately sized set of particles is here solved through an iterative policy building scheme that adds new particles to improve the policy performance and is also capable of removing redundant particles. Experiments demonstrate the scalability of the proposed approach as the dimensionality of the state-space grows."}}
{"id": "-fwKKyO7v_", "cdate": 1451606400000, "mdate": 1673272615694, "content": {"title": "Optimal control of large scale stochastic hybrid systems with a finite control space", "abstract": ""}}
{"id": "xaB5J83Q1JC", "cdate": 1420070400000, "mdate": null, "content": {"title": "Following Newton direction in Policy Gradient with parameter exploration", "abstract": "This paper investigates the use of second-order methods to solve Markov Decision Processes (MDPs). Despite the popularity of second-order methods in optimization literature, so far little attention has been paid to the extension of such techniques to face sequential decision problems. Here we provide a model-free Reinforcement Learning method that estimates the Newton direction by sampling directly in the parameter space. In order to compute the Newton direction we provide the formulation of the Hessian of the expected return, a technique for variance reduction in the sample-based estimation and a finite sample analysis in the case of Normal distribution. Beside discussing the theoretical properties, we empirically evaluate the method on an instructional linear-quadratic regulator and on a complex dynamical quadrotor system."}}
{"id": "p0oxHReJH6", "cdate": 1420070400000, "mdate": 1673272615695, "content": {"title": "A classification-based approach to the optimal control of affine switched systems", "abstract": ""}}
{"id": "XusGt_K_b8d", "cdate": 1420070400000, "mdate": null, "content": {"title": "Optimal control to reduce emissions in gasoline engines: an iterative learning control approach for ECU calibration maps improvement", "abstract": "Control of emissions in gasoline engines has become more stringent in the last decades, especially in Europe, posing new and important problems in the control of complex nonlinear systems. In this work a preliminary investigation is conducted on the idea of exploiting Iterative Learning Control to optimize calibration maps that are commonly used in the Engine Control Unit of gasoline engines. In this spirit, starting from existing maps, we show how to refine them using a gradient-descent iterative learning control algorithm, considering additional constraints in the optimization problem. The outcome of this procedure is a control signal which can be integrated in a modified map. The performance of the proposed technique is validated on the provided training signal and cross-validated on different reference signals. Simulation results show the effectiveness of the approach."}}
{"id": "AQHl5ojC5r", "cdate": 1420070400000, "mdate": 1673272615695, "content": {"title": "A majority voting classifier with probabilistic guarantees", "abstract": ""}}
