{"id": "0pFzg-8y-o", "cdate": 1664994280495, "mdate": null, "content": {"title": "Agent-Controller Representations: Principled Offline RL with Rich Exogenous Information", "abstract": "Learning to control an agent from data collected offline in a rich pixel-based visual observation space is vital for real-world applications of reinforcement learning (RL). A major challenge in this setting is the presence of input information that is hard to model and irrelevant to controlling the agent. This problem has been approached by the theoretical RL community through the lens of exogenous information, i.e, any control-irrelevant information contained in observations. For example, a robot navigating in busy streets needs to ignore irrelevant information, such as other people walking in the background, textures of objects, or birds in the sky. In this paper, we focus on the setting with visually detailed exogenous information, and introduce new offline RL benchmarks offering the ability to study this problem. We find that contemporary representation learning techniques can fail on datasets where the noise is a complex and time dependent process, which is prevalent in practical applications. To address these, we propose to use multi-step inverse models, which have seen a great deal of interest in the RL theory community, to learn Agent-Controller Representations for Offline-RL (ACRO). Despite being simple and requiring no reward, we show theoretically and empirically that the representation created by this objective greatly outperforms baselines.  "}}
{"id": "gLl0fZQo6Vu", "cdate": 1663850047547, "mdate": null, "content": {"title": "Agent-Controller Representations: Principled Offline RL with Rich Exogenous Information", "abstract": "Learning to control an agent from data collected offline in a rich pixel-based visual observation space is vital for real-world applications of reinforcement learning (RL). A major challenge in this setting is the presence of input information that is hard to model and irrelevant to controlling the agent. This problem has been approached by the theoretical RL community through the lens of exogenous information, i.e, any control-irrelevant information contained in observations. For example, a robot navigating in busy streets needs to ignore irrelevant information, such as other people walking in the background, textures of objects, or birds in the sky. In this paper, we focus on the setting with visually detailed exogenous information, and introduce new offline RL benchmarks offering the ability to study this problem. We find that contemporary representation learning techniques can fail on datasets where the noise is a complex and time dependent process, which is prevalent in practical applications. To address these, we propose to use multi-step inverse models, which have seen a great deal of interest in the RL theory community, to learn Agent-Controller Representations for Offline-RL (ACRO). Despite being simple and requiring no reward, we show theoretically and empirically that the representation created by this objective greatly outperforms baselines.  "}}
{"id": "hQ4K9Bf4G2B", "cdate": 1663850047282, "mdate": null, "content": {"title": "Behavior Prior Representation learning for Offline Reinforcement Learning", "abstract": "Offline reinforcement learning (RL) struggles in environments with rich and noisy inputs, where the agent only has access to a fixed dataset without environment interactions. Past works have proposed common workarounds based on the pre-training of state representations, followed by policy training. In this work, we introduce a simple, yet effective approach for learning state representations. Our method, Behavior Prior Representation (BPR), learns state representations with an easy-to-integrate objective based on behavior cloning of the dataset: we first learn a state representation by mimicking actions from the dataset, and then train a policy on top of the fixed representation, using any off-the-shelf Offline RL algorithm. Theoretically, we prove that BPR carries out performance guarantees when integrated into algorithms that have either policy improvement guarantees (conservative algorithms) or produce lower bounds of the policy values (pessimistic algorithms). Empirically, we show that BPR combined with existing state-of-the-art Offline RL algorithms leads to significant improvements across several offline control benchmarks. The code is available at \\url{https://github.com/bit1029public/offline_bpr}"}}
{"id": "N6zHSyChCF2", "cdate": 1652737592760, "mdate": null, "content": {"title": "Discrete Compositional Representations as an Abstraction for Goal Conditioned Reinforcement Learning", "abstract": "Goal-conditioned reinforcement learning (RL) is a promising direction for training agents that are capable of solving multiple tasks and reach a diverse set of objectives.  How to \\textit{specify} and \\textit{ground} these goals in such a way that we can both reliably reach goals during training as well as generalize to new goals during evaluation remains an open area of research. Defining goals in the space of noisy, high-dimensional sensory inputs is one possibility, yet this poses a challenge for training goal-conditioned agents, or even for generalization to novel goals. We propose to address this by learning compositional representations of goals and processing the resulting representation via a discretization bottleneck, for coarser specification of goals, through an approach we call DGRL. We show that discretizing outputs from goal encoders through a bottleneck can work well in goal-conditioned RL setups, by experimentally evaluating this method on tasks ranging from maze environments to complex robotic navigation and manipulation tasks. Additionally, we show a theoretical result which bounds the expected return for goals not observed during training, while still allowing for specifying goals with expressive combinatorial structure."}}
{"id": "xx88J2fyi9", "cdate": 1640995200000, "mdate": 1675249397156, "content": {"title": "SimSR: Simple Distance-Based State Representations for Deep Reinforcement Learning", "abstract": "This work explores how to learn robust and generalizable state representation from image-based observations with deep reinforcement learning methods. Addressing the computational complexity, stringent assumptions and representation collapse challenges in existing work of bisimulation metric, we devise Simple State Representation (SimSR) operator. SimSR enables us to design a stochastic approximation method that can practically learn the mapping functions (encoders) from observations to latent representation space. In addition to the theoretical analysis and comparison with the existing work, we experimented and compared our work with recent state-of-the-art solutions in visual MuJoCo tasks. The results shows that our model generally achieves better performance and has better robustness and good generalization."}}
{"id": "x1zukXtL_4D", "cdate": 1640995200000, "mdate": 1681650029656, "content": {"title": "Behavior Prior Representation learning for Offline Reinforcement Learning", "abstract": ""}}
{"id": "u9tBtdzMm2A", "cdate": 1640995200000, "mdate": 1681650029592, "content": {"title": "Agent-Controller Representations: Principled Offline RL with Rich Exogenous Information", "abstract": ""}}
{"id": "n9K35GrgeKY", "cdate": 1640995200000, "mdate": 1681650029631, "content": {"title": "Discrete Factorial Representations as an Abstraction for Goal Conditioned Reinforcement Learning", "abstract": ""}}
{"id": "kExLbL1lSd", "cdate": 1640995200000, "mdate": 1681650029678, "content": {"title": "Representation Learning in Deep RL via Discrete Information Bottleneck", "abstract": ""}}
{"id": "0pdNOz579te", "cdate": 1640995200000, "mdate": 1675249397165, "content": {"title": "CHA: Categorical Hierarchy-based Attention for Next POI Recommendation", "abstract": "Next Point-of-interest (POI) recommendation is a key task in improving location-related customer experiences and business operations, but yet remains challenging due to the substantial diversity of human activities and the sparsity of the check-in records available. To address these challenges, we proposed to explore the category hierarchy knowledge graph of POIs via an attention mechanism to learn the robust representations of POIs even when there is insufficient data. We also proposed a spatial-temporal decay LSTM and a Discrete Fourier Series-based periodic attention to better facilitate the capturing of the personalized behavior pattern. Extensive experiments on two commonly adopted real-world location-based social networks (LBSNs) datasets proved that the inclusion of the aforementioned modules helps to boost the performance of next and next new POI recommendation tasks significantly. Specifically, our model in general outperforms other state-of-the-art methods by a large margin."}}
