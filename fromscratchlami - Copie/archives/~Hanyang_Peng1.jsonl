{"id": "U45w87vFQ3", "cdate": 1663850233408, "mdate": null, "content": {"title": "BinSGDM:  Extreme One-Bit Quantization for Communication Efficient Large-Scale Distributed Training ", "abstract": "To alleviate the communication bottleneck of large-scale distributed training, a rich body of prior communication-compression optimizers have been proposed. These methods  focus mainly on high compression ratio to expect  acceleration. However, some recent works pointed out, when running with distributed training frameworks ( \\emph{e.g.}, \\emph{DistributedDataParallel} in pytorch), these methods may provide no acceleration over the off-the-shelve uncompressed SGD/Adam in the typical settings, due to heavy compression/decompression computation or incompatibility with efficient communication primitives or the requirement of uncompressed warmup at the early stage. For these reasons, we propose a novel extreme one-bit quantization optimizer, dubbed \\emph{BinSGDM}. The quantization of \\emph{BinSGDM} is computed easily and lightly, and it does not need to resort to uncompressed optimizers for warmup. We also theoretically prove that it can promise the same convergence speed as the original Adam. Moreover, we specially present a hierarchical communication scheme to further lower the communication volume.  Extensive experiments are conducted on 8 to 64 GPUs (1 to 8 nodes) for distributed training with \\emph{DistributedDataParallel}, and the experimental results demonstrates that \\emph{BinSGDM} with the communication scheme can achieve up to {$\\bm{2.47 \\times}$} speedup  for training ResNet-50 and $\\bm{6.26\\times}$ speedup for training BERT-Base, compared to the full-precision optimizers."}}
{"id": "SkevphEYPB", "cdate": 1569438911053, "mdate": null, "content": {"title": "POP-Norm: A Theoretically Justified and More Accelerated Normalization Approach", "abstract": "Batch Normalization (BatchNorm) has been a default module in  modern deep networks  due to its effectiveness for accelerating training deep neural networks.  It is widely accepted that the great success of BatchNorm is owing to reduction of internal covariate shift (ICS), but recently it is demonstrated that the link between them is fairly weak. The intrinsic reason behind effectiveness of BatchNorm is still unrevealed that limits it to be made better use. In light of this,  we propose a new normalization approach,  referred to as Pre-Operation Normalization (POP-Norm), which is theoretically ensured to speed up the training convergence. Not surprisingly, POP-Norm and BatchNorm are largely the same. Hence the similarities  can help us to theoretically interpret the root of BatchNorm's effectiveness. There are still some significant distinctions between the two approaches. Just the distinctions make POP-Norm achieve faster convergence rate and better performance than BatchNorm, which are validated in extensive experiments on benchmark datasets: CIFAR10, CIFAR100 and ILSVRC2012."}}
{"id": "B1Z_-kZu-B", "cdate": 1483228800000, "mdate": null, "content": {"title": "A General Framework for Sparsity Regularized Feature Selection via Iteratively Reweighted Least Square Minimization", "abstract": "A variety of feature selection methods based on sparsity regularization have been developed with different loss functions and sparse regularization functions. Capitalizing on the existing sparsity regularized feature selection methods, we propose a general sparsity feature selection (GSR-FS) algorithm that optimizes a l 2, r (0 <\u00a0 r \u2264 2) based loss function with a l 2, p -norm (0 < p \u2264 2) sparse regularization. The l 2, r - norm (0 < \ud835\udc5f \u2264 2) based loss function brings flexibility to balance data-fitting and robustness to outliers by tuning its parameter, and the l 2, p -norm (0 < p \u2264 1) based regularization function is able to boost the sparsity for feature selection. To solve the optimization problem with multiple non-smooth and non-convex functions when , we develop an efficient solver under the general umbrella of Iterative Reweighted Least Square (IRLS) algorithms. Our algorithm has been proved to converge with a theoretical convergence order of min(2 \u2013 r, 2 \u2013 p ) at least . The experimental results have demonstrated that our method could achieve competitive feature selection performance on publicly available datasets compared with state-of-the-art feature selection methods, with reduced computational cost."}}
{"id": "HkEZkHG_ZS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Direct Sparsity Optimization Based Feature Selection for Multi-Class Classification", "abstract": "A novel sparsity optimization method is proposed to select features for multi-class classification problems by directly optimizing a l2,p -norm (0 < p \u2264 1) based sparsity function subject to data-fitting inequality constraints to obtain large between-class margins. The direct sparse optimization method circumvents the empirical tuning of regularization parameters in existing feature selection methods that adopt the sparsity model as a regularization term. To solve the direct sparsity optimization problem that is nonsmooth and non-convex when 0 < p < 1, we propose an efficient iterative algorithm with proved convergence by converting it to a convex and smooth optimization problem at every iteration step. The proposed algorithm has been evaluated based on publicly available datasets. The experiments have demonstrated that our algorithm could achieve feature selection performance competitive to state-of-the-art algorithms."}}
