{"id": "If7MXYgichc", "cdate": 1676591079494, "mdate": null, "content": {"title": "Bootstrapped Representations in Reinforcement Learning", "abstract": "In reinforcement learning (RL), state representations are key to dealing with large or continuous state spaces. While one of the promises of deep learning algorithms is to automatically construct features well-tuned for the task they try to solve, such a representation might not emerge from end-to-end training of deep RL agents. To mitigate this issue, pretrained representations are often learnt from auxiliary tasks on offline datasets as part of an unsupervised pre-training phase to improve the sample efficiency of deep RL agents in a future online phase. Bootstrapping methods are today's method of choice to make these additional predictions but it is unclear which features are being learned. In this paper, we address this gap and provide a theoretical characterization of the pre-trained representation learnt by temporal difference learning \\citep{sutton1988learning}. Surprisingly, we find that this representation differs from the features learned by pre-training with Monte Carlo and residual gradient algorithms for most transition structures of the environment. We describe the goodness of these pre-trained representations to linearly predict the value function given any downstream reward function, and use our theoretical analysis to design new unsupervised pre-training rules. We complement our theoretical results with an empirical comparison of these pre-trained representations for different cumulant functions on the four-room  \\citep{sutton99between} and Mountain Car \\citep{Moore90efficientmemory-based} domains and demonstrate that they speed up online learning."}}
{"id": "O9cJADBZT1", "cdate": 1676591079374, "mdate": null, "content": {"title": "Deep Reinforcement Learning with Plasticity Injection", "abstract": "A growing body of evidence suggests that neural networks employed in deep reinforcement learning (RL) gradually lose their plasticity, the ability to learn from new data; however, the analysis and mitigation of this phenomenon is hampered by the complex relationship between plasticity, exploration, and performance in RL. This paper introduces plasticity injection, a minimalistic intervention that increases the network plasticity without changing the number of trainable parameters or biasing the predictions. The applications of this intervention are two-fold: first, as a diagnostic tool \u2014 if injection increases the performance, we may conclude that an agent's network was losing its plasticity. This tool allows us to identify a subset of Atari environments where the lack of plasticity causes performance plateaus, motivating future studies on understanding and combating plasticity loss. Second, plasticity injection can be used to improve the computational efficiency of RL training if the agent has to re-learn from scratch due to exhausted plasticity or by growing the agent's network dynamically without compromising performance. The results on Atari show that plasticity injection attains stronger performance compared to alternative methods while being computationally efficient."}}
{"id": "Mn4IkuWamy", "cdate": 1652737407857, "mdate": null, "content": {"title": "The Nature of Temporal Difference Errors in Multi-step Distributional Reinforcement Learning", "abstract": "We study the multi-step off-policy learning approach to distributional RL. Despite the apparent similarity between value-based RL and distributional RL, our study reveals intriguing and fundamental differences between the two cases in the multi-step setting. We identify a novel notion of path-dependent distributional TD error, which is indispensable for principled multi-step distributional RL. The distinction from the value-based case bears important implications on concepts such as backward-view algorithms. Our work provides the first theoretical guarantees on multi-step off-policy distributional RL algorithms, including results that apply to the small number of existing approaches to multi-step distributional RL. In addition, we derive a novel algorithm, Quantile Regression-Retrace, which leads to a deep RL agent QR-DQN-Retrace that shows empirical improvements over QR-DQN on the Atari-57 benchmark. Collectively, we shed light on how unique challenges in multi-step distributional RL can be addressed both in theory and practice."}}
{"id": "5G7fT_tJTt", "cdate": 1634067449714, "mdate": null, "content": {"title": "Understanding and Preventing Capacity Loss in Reinforcement Learning", "abstract": "The reinforcement learning (RL) problem is rife with sources of non-stationarity\nthat can destabilize or inhibit learning progress. We identify a key mechanism\nby which this occurs in agents using neural networks as function approximators:\ncapacity loss, whereby networks trained to predict a sequence of target values lose\ntheir ability to quickly fit new functions over time. We demonstrate that capacity\nloss occurs in a broad range of RL agents and environments, and is particularly\ndamaging to learning progress in sparse-reward tasks. We then present a simple\nregularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon\nby regressing a subspace of features towards its value at initialization, improving\nperformance over a state-of-the-art model-free algorithm in the Atari 2600 suite.\nFinally, we study how this regularization affects different notions of capacity and\nevaluate other mechanisms by which it may improve performance."}}
{"id": "ZkC8wKoLbQ7", "cdate": 1632875460639, "mdate": null, "content": {"title": "Understanding and Preventing Capacity Loss in Reinforcement Learning", "abstract": "The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress.\nWe identify a key mechanism by which this occurs in agents using neural networks as function approximators: \\textit{capacity loss}, whereby networks trained to predict a sequence of target values lose their ability to quickly fit new functions over time.\nWe demonstrate that capacity loss occurs in a broad range of RL agents and environments, and is particularly damaging to learning progress in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, improving performance over a state-of-the-art model-free algorithm in the Atari 2600 suite. Finally, we study how this regularization affects different notions of capacity and evaluate other mechanisms by which it may improve performance."}}
{"id": "9DlCh34E1bN", "cdate": 1621629736348, "mdate": null, "content": {"title": "On the Expressivity of Markov Reward", "abstract": "Reward is the driving force for reinforcement-learning agents. This paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform. We frame this study around three new abstract notions of \u201ctask\u201d that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Our main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists. We conclude with an empirical study that corroborates and illustrates our theoretical findings."}}
{"id": "nPHA8fGicZk", "cdate": 1621629728060, "mdate": null, "content": {"title": "The Difficulty of Passive Learning in Deep Reinforcement Learning", "abstract": "Learning to act from observational data without active environmental interaction is a well-known challenge in Reinforcement Learning (RL). Recent approaches involve constraints on the learned policy or conservative updates, preventing strong deviations from the state-action distribution of the dataset. Although these methods are evaluated using non-linear function approximation, theoretical justifications are mostly limited to the tabular or linear cases. Given the impressive results of deep reinforcement learning, we argue for a need to more clearly understand the challenges in this setting.\n\nIn the vein of Held & Hein's classic 1963 experiment, we propose the \"tandem learning\" experimental paradigm which facilitates our empirical analysis of the difficulties in offline reinforcement learning. We identify function approximation in conjunction with fixed data distributions as the strongest factors, thereby extending but also challenging hypotheses stated in past work. Our results provide relevant insights for offline deep reinforcement learning, while also shedding new light on phenomena observed in the online case of learning control."}}
{"id": "Z4GBTwtYx9x", "cdate": 1620754962242, "mdate": null, "content": {"title": "On The Effect of Auxiliary Tasks on Representation Dynamics", "abstract": "While auxiliary tasks play a key role in shaping the representations learnt by reinforcement learning agents, much is still unknown about the mechanisms through which this is achieved. This work develops our understanding of the relationship between auxiliary tasks, environment structure, and representations by analysing the dynamics of temporal difference algorithms. Through this approach, we establish a connection between the spectral decomposition of the transition operator and the representations induced by a variety of auxiliary tasks. We then leverage insights from these theoretical results to inform the selection of auxiliary tasks for deep reinforcement learning agents in sparse-reward environments."}}
{"id": "uQzdlSVonNT", "cdate": 1620754887827, "mdate": null, "content": {"title": "The value-improvement path: Towards better representations for reinforcement learning", "abstract": "In value-based reinforcement learning (RL), unlike in supervised learning, the agent faces not a single, stationary, approximation problem, but a sequence of value prediction problems. Each time the policy improves, the nature of the problem changes, shifting both the distribution of states and their values. In this paper we take a novel perspective, arguing that the value prediction problems faced by an RL agent should not be addressed in isolation, but rather as a single, holistic, prediction problem. An RL algorithm generates a sequence of policies that, at least approximately, improve towards the optimal policy. We explicitly characterize the associated sequence of value functions and call it the value-improvement path. Our main idea is to approximate the value-improvement path holistically, rather than to solely track the value function of the current policy. Specifically, we discuss the impact that this holistic view of RL has on representation learning. We demonstrate that a representation that spans the past value-improvement path will also provide an accurate value approximation for future policy improvements. We use this insight to better understand existing approaches to auxiliary tasks and to propose new ones. To test our hypothesis empirically, we augmented a standard deep RL agent with an auxiliary task of learning the value-improvement path. In a study of Atari 2600 games, the augmented agent achieved approximately double the mean and median performance of the baseline agent.\n"}}
{"id": "cJNDFeJLaLp", "cdate": 1620754732443, "mdate": null, "content": {"title": "Conditional importance sampling for off-policy learning", "abstract": "The principal contribution of this paper is a conceptual framework for off-policy reinforcement learning, based on conditional expectations of importance sampling ratios. This framework yields new perspectives and understanding of existing off-policy algorithms, and reveals a broad space of unexplored algorithms. We theoretically analyse this space, and concretely investigate several algorithms that arise from this framework."}}
