{"id": "5-iRjd9FreV", "cdate": 1621630136421, "mdate": null, "content": {"title": "Analysis of Sensing Spectral for Signal Recovery under a Generalized Linear Model", "abstract": "We consider a nonlinear inverse problem $\\mathbf{y}= f(\\mathbf{Ax})$, where observations $\\mathbf{y} \\in \\mathbb{R}^m$ are the componentwise nonlinear transformation of $\\mathbf{Ax} \\in \\mathbb{R}^m$, $\\mathbf{x} \\in \\mathbb{R}^n$ is the signal of interest and $\\mathbf{A}$ is a known linear mapping. By properly specifying the nonlinear processing function, this model can be particularized to many signal processing problems, including compressed sensing and phase retrieval. \n\nOur main goal in this paper is to understand the impact of sensing matrices, or more specifically the spectrum of sensing matrices, on the difficulty of recovering $\\mathbf{x}$ from $\\mathbf{y}$. Towards this goal, we study the performance of one of the most successful recovery methods, i.e. the expectation propagation algorithm (EP). We define a notion for the spikiness of the spectrum of $\\mathbf{A}$ and show the importance of this measure in the performance of the EP. Whether the spikiness of the spectrum can hurt or help the recovery performance of EP depends on $f$. We define certain quantities based on the function $f$ that enables us to describe the impact of the spikiness of the spectrum on EP recovery. Based on our framework, we are able to show that for instance, in phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky (flatter) spectrums offer better recoveries. Our results unify and substantially generalize the existing results that compare sub-Gaussian and orthogonal matrices, and provide a platform toward designing optimal sensing systems."}}
{"id": "zHMRu8DukU", "cdate": 1609459200000, "mdate": 1681490889803, "content": {"title": "On the proliferation of support vectors in high dimensions", "abstract": ""}}
{"id": "nP-G7BQbxNV", "cdate": 1609459200000, "mdate": 1652396288531, "content": {"title": "When does preconditioning help or hurt generalization?", "abstract": "While second order optimizers such as natural gradient descent (NGD) often speed up optimization, their effect on generalization has been called into question. This work presents a more nuanced view on how the \\textit{implicit bias} of optimizers affects the comparison of generalization properties. We provide an exact asymptotic bias-variance decomposition of the generalization error of preconditioned ridgeless regression in the overparameterized regime, and consider the inverse population Fisher information matrix (used in NGD) as a particular example. We determine the optimal preconditioner $\\boldsymbol{P}$ for both the bias and variance, and find that the relative generalization performance of different optimizers depends on label noise and ``shape'' of the signal (true parameters): when the labels are noisy, the model is misspecified, or the signal is misaligned with the features, NGD can achieve lower risk; conversely, GD generalizes better under clean labels, a well-specified model, or aligned signal. Based on this analysis, we discuss several approaches to manage the bias-variance tradeoff, and the potential benefit of interpolating between first- and second-order updates. We then extend our analysis to regression in the reproducing kernel Hilbert space and demonstrate that preconditioning can lead to more efficient decrease in the population risk. Lastly, we empirically compare the generalization error of first- and second-order optimizers in neural network experiments, and observe robust trends matching our theoretical analysis."}}
{"id": "hl9RfaAcyB", "cdate": 1609459200000, "mdate": 1682342296879, "content": {"title": "Impact of the Sensing Spectrum on Signal Recovery in Generalized Linear Models", "abstract": "We consider an inverse problem $\\mathbf{y}= f(\\mathbf{Ax})$, where $\\mathbf{x}\\in\\mathbb{R}^n$ is the signal of interest, $\\mathbf{A}$ is the sensing matrix, $f$ is a nonlinear function and $\\mathbf{y} \\in \\mathbb{R}^m$ is the measurement vector. In many applications, we have some level of freedom to design the sensing matrix $\\mathbf{A}$, and in such circumstances we could optimize $\\mathbf{A}$ to achieve better reconstruction performance. As a first step towards optimal design, it is important to understand the impact of the sensing matrix on the difficulty of recovering $\\mathbf{x}$ from $\\mathbf{y}$. In this paper, we study the performance of one of the most successful recovery methods, i.e., the expectation propagation (EP) algorithm. We define a notion of spikiness for the spectrum of $\\bmmathbfA}$ and show the importance of this measure for the performance of EP. We show that whether a spikier spectrum can hurt or help the recovery performance depends on $f$. Based on our framework, we are able to show that, in phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky spectrums lead to better performance. Our results unify and substantially generalize existing results that compare Gaussian and orthogonal matrices, and provide a platform towards designing optimal sensing systems."}}
{"id": "7NF8deE9oCY", "cdate": 1609459200000, "mdate": 1682342297011, "content": {"title": "Analysis of Sensing Spectral for Signal Recovery under a Generalized Linear Model", "abstract": "We consider a nonlinear inverse problem $\\mathbf{y}= f(\\mathbf{Ax})$, where observations $\\mathbf{y} \\in \\mathbb{R}^m$ are the componentwise nonlinear transformation of $\\mathbf{Ax} \\in \\mathbb{R}^m$, $\\mathbf{x} \\in \\mathbb{R}^n$ is the signal of interest and $\\mathbf{A}$ is a known linear mapping. By properly specifying the nonlinear processing function, this model can be particularized to many signal processing problems, including compressed sensing and phase retrieval. Our main goal in this paper is to understand the impact of sensing matrices, or more specifically the spectrum of sensing matrices, on the difficulty of recovering $\\mathbf{x}$ from $\\mathbf{y}$. Towards this goal, we study the performance of one of the most successful recovery methods, i.e. the expectation propagation algorithm (EP). We define a notion for the spikiness of the spectrum of $\\mathbf{A}$ and show the importance of this measure in the performance of the EP. Whether the spikiness of the spectrum can hurt or help the recovery performance of EP depends on $f$. We define certain quantities based on the function $f$ that enables us to describe the impact of the spikiness of the spectrum on EP recovery. Based on our framework, we are able to show that for instance, in phase-retrieval problems, matrices with spikier spectrums are better for EP, while in 1-bit compressed sensing problems, less spiky (flatter) spectrums offer better recoveries. Our results unify and substantially generalize the existing results that compare sub-Gaussian and orthogonal matrices, and provide a platform toward designing optimal sensing systems."}}
{"id": "4obedqbWdmN", "cdate": 1609459200000, "mdate": 1682342296623, "content": {"title": "Consistent Risk Estimation in Moderately High-Dimensional Linear Regression", "abstract": "Risk estimation is at the core of many learning systems. The importance of this problem has motivated researchers to propose different schemes, such as cross validation, generalized cross validation, and Bootstrap. The theoretical properties of such estimators have been extensively studied in the low-dimensional settings, where the number of predictors p is much smaller than the number of observations n. However, a unifying methodology accompanied with a rigorous theory is lacking in high-dimensional settings. This paper studies the problem of risk estimation under the moderately high-dimensional asymptotic setting n,p \u2192 \u221e and n/p \u2192 \u03b4 > 1 ( \u03b4 is a fixed number), and proves the consistency of three risk estimators that have been successful in numerical studies, i.e., leave-one-out cross validation (LOOCV), approximate leave-one-out (ALO), and approximate message passing (AMP)-based techniques. A corner stone of our analysis is a bound that we obtain on the discrepancy of the `residuals' obtained from AMP and LOOCV. This connection not only enables us to obtain a more refined information on the estimates of AMP, ALO, and LOOCV, but also offers an upper bound on the convergence rate of each estimator."}}
{"id": "35TpiCI1OB", "cdate": 1609459200000, "mdate": 1682342296781, "content": {"title": "Spectral Method for Phase Retrieval: An Expectation Propagation Perspective", "abstract": "Phase retrieval refers to the problem of recovering a signal <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$ {x}_{\\star }\\in \\mathbb {C}^{n}$ </tex-math></inline-formula> from its phaseless measurements <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\text {y}_{\\text {i}}=| {a}_{i}^{ \\mathsf {H}} {x}_{\\star }|$ </tex-math></inline-formula> , where <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\{ {a}_{\\text {i}}\\}_{\\text {i}=1}^{ {m}}$ </tex-math></inline-formula> are the measurement vectors. Spectral method is widely used for initialization in many phase retrieval algorithms. The quality of spectral initialization can have a major impact on the overall algorithm. In this paper, we focus on the model where <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$ {A}=[ {a}_{1},\\ldots, {a}_{ {m}}]^{ \\mathsf {H}}$ </tex-math></inline-formula> has orthonormal columns, and study the spectral initialization under the asymptotic setting <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$ {m}, {n}\\to \\infty $ </tex-math></inline-formula> with <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$ {m}/ {n}\\to \\delta \\in (1,\\infty)$ </tex-math></inline-formula> . We use the expectation propagation framework to characterize the performance of spectral initialization for Haar distributed matrices. Our numerical results confirm that the predictions of the EP method are accurate for not-only Haar distributed matrices, but also for realistic Fourier based models (e.g. the coded diffraction model). The main findings of this paper are the following: 1) There exists a threshold on <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta $ </tex-math></inline-formula> (denoted as <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta _{ \\mathrm {weak}}$ </tex-math></inline-formula> ) below which the spectral method cannot produce a meaningful estimate. We show that <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta _{ \\mathrm {weak}}=2$ </tex-math></inline-formula> for the column-orthonormal model. In contrast, previous results by Mondelli and Montanari show that <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta _{ \\mathrm {weak}}=1$ </tex-math></inline-formula> for the i.i.d. Gaussian model. 2) The optimal design for the spectral method coincides with that for the i.i.d. Gaussian model, where the latter was recently introduced by Luo, Alghamdi and Lu."}}
{"id": "S724o4_WB3", "cdate": 1601308088297, "mdate": null, "content": {"title": "When does preconditioning help or hurt generalization?", "abstract": "While second order optimizers such as natural gradient descent (NGD) often speed up optimization, their effect on generalization has been called into question. This work presents a more nuanced view on how the \\textit{implicit bias} of optimizers affects the comparison of generalization properties. \nWe provide an exact asymptotic bias-variance decomposition of the generalization error of preconditioned ridgeless regression in the overparameterized regime, and consider the inverse population Fisher information matrix (used in NGD) as a particular example. We determine the optimal preconditioner $\\boldsymbol{P}$ for both the bias and variance, and find that the relative generalization performance of different optimizers depends on label noise and ``shape'' of the signal (true parameters): when the labels are noisy, the model is misspecified, or the signal is misaligned with the features, NGD can achieve lower risk; conversely, GD generalizes better under clean labels, a well-specified model, or aligned signal. \nBased on this analysis, we discuss several approaches to manage the bias-variance tradeoff, and the potential benefit of interpolating between first- and second-order updates. We then extend our analysis to regression in the reproducing kernel Hilbert space and demonstrate that preconditioning can lead to more efficient decrease in the population risk. Lastly, we empirically compare the generalization error of first- and second-order optimizers in neural network experiments, and observe robust trends matching our theoretical analysis. "}}
{"id": "S8IcjMkRWQC", "cdate": 1598980069005, "mdate": null, "content": {"title": "On the Optimal Weighted  \\ell_2 Regularization in Overparameterized Linear Regression", "abstract": "We consider the linear model $\\vy=\\vX\\vbeta_{\\star}+\\vepsilon$ with $\\vX\\in \\mathbb{R}^{n\\times p}$ in the overparameterized regime $p>n$. We estimate $\\vbeta_{\\star}$ via generalized (weighted) ridge regression: $\\hat{\\vbeta}_{\\lambda}=\\left(\\vX^{\\t}\\vX+\\lambda\\vSigma_w\\right)^{\\dagger}\\vX^{\\t}\\vy$, where $\\vSigma_w$ is the weighting matrix. Assuming a random effects model with general data covariance $\\vSigma_x$ and anisotropic prior on the true coefficients $\\vbeta_{\\star}$, i.e., $\\bbE\\vbeta_{\\star}\\vbeta_{\\star}^{\\t}=\\vSigma_\\beta$, we provide an exact characterization of the prediction risk $\\mathbb{E}(y-\\vx^{\\t}\\hat{\\vbeta}_{\\lambda})^2$ in the proportional asymptotic limit $p/n\\rightarrow \\gamma \\in (1,\\infty)$. Our general setup leads to a number of interesting findings. We provide precise conditions that decide the sign of the optimal setting $\\lambda_{\\opt}$ for the ridge parameter $\\lambda$ and confirm the implicit $\\ell_2$ regularization effect of overparameterization, which theoretically justifies the surprising empirical observation that $\\lambda_{\\opt}$ can be negative in the overparameterized regime. We also characterize the double descent phenomenon for principal component regression (PCR) when $\\vX$ and $\\vbeta_{\\star}$ are non-isotropic. Finally, we determine the optimal $\\vSigma_w$ for both the ridgeless ($\\lambda\\to 0$) and opti mally regularized ($\\lambda = \\lambda_{\\opt}$) case, and demonstrate the advantage of the weighted objective over standard ridge regression and PCR."}}
{"id": "rQta5zWSvlo", "cdate": 1577836800000, "mdate": 1652396288536, "content": {"title": "On the Optimal Weighted $\\ell_2$ Regularization in Overparameterized Linear Regression", "abstract": "We consider the linear model $\\vy=\\vX\\vbeta_{\\star}+\\vepsilon$ with $\\vX\\in \\mathbb{R}^{n\\times p}$ in the overparameterized regime $p&gt;n$. We estimate $\\vbeta_{\\star}$ via generalized (weighted) ridge regression: $\\hat{\\vbeta}_{\\lambda}=\\left(\\vX^{\\t}\\vX+\\lambda\\vSigma_w\\right)^{\\dagger}\\vX^{\\t}\\vy$, where $\\vSigma_w$ is the weighting matrix. Assuming a random effects model with general data covariance $\\vSigma_x$ and anisotropic prior on the true coefficients $\\vbeta_{\\star}$, i.e., $\\bbE\\vbeta_{\\star}\\vbeta_{\\star}^{\\t}=\\vSigma_\\beta$, we provide an exact characterization of the prediction risk $\\mathbb{E}(y-\\vx^{\\t}\\hat{\\vbeta}_{\\lambda})^2$ in the proportional asymptotic limit $p/n\\rightarrow \\gamma \\in (1,\\infty)$. Our general setup leads to a number of interesting findings. We outline precise conditions that decide the sign of the optimal setting $\\lambda_{\\opt}$ for the ridge parameter $\\lambda$ and confirm the implicit $\\ell_2$ regularization effect of overparameterization, which theoretically justifies the surprising empirical observation that $\\lambda_{\\opt}$ can be \\textit{negative} in the overparameterized regime. We also characterize the double descent phenomenon for principal component regression (PCR) when $\\vX$ and $\\vbeta_{\\star}$ are both anisotropic. Finally, we determine the optimal weighting matrix $\\vSigma_w$ for both the ridgeless ($\\lambda\\to 0$) and optimally regularized ($\\lambda = \\lambda_{\\opt}$) case, and demonstrate the advantage of the weighted objective over standard ridge regression and PCR."}}
