{"id": "-H6kKm4DVo", "cdate": 1652737508195, "mdate": null, "content": {"title": "Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?", "abstract": "As the scope of machine learning broadens, we observe a recurring theme of *algorithmic monoculture*: the same systems, or systems that share components (e.g. datasets, models), are deployed by multiple decision-makers.  While sharing offers advantages like amortizing effort, it also has risks.  We introduce and formalize one such risk, *outcome homogenization*: the extent to which particular individuals or groups experience the same outcomes across different deployments.  If the same individuals or groups exclusively experience undesirable outcomes, this may institutionalize systemic exclusion and reinscribe social hierarchy.  We relate algorithmic monoculture and outcome homogenization by proposing the *component sharing hypothesis*: if algorithmic systems are increasingly built on the same data or models, then they will increasingly homogenize outcomes.  We test this hypothesis on algorithmic fairness benchmarks, demonstrating that increased data-sharing reliably exacerbates homogenization and individual-level effects generally exceed group-level effects.  Further, given the current regime in AI of foundation models, i.e. pretrained models that can be adapted to myriad downstream tasks, we test whether model-sharing homogenizes outcomes across tasks.  We observe mixed results: we find that for both vision and language settings, the specific methods for adapting a foundation model significantly influence the degree of outcome homogenization.  We also identify societal challenges that inhibit the measurement, diagnosis, and rectification of outcome homogenization in deployed machine learning systems."}}
{"id": "qgI16hfdPi1", "cdate": 1609459200000, "mdate": 1652605998584, "content": {"title": "Generalized Optimal Linear Orders", "abstract": "The sequential structure of language, and the order of words in a sentence specifically, plays a central role in human language processing. Consequently, in designing computational models of language, the de facto approach is to present sentences to machines with the words ordered in the same order as in the original human-authored sentence. The very essence of this work is to question the implicit assumption that this is desirable and inject theoretical soundness into the consideration of word order in natural language processing. In this thesis, we begin by uniting the disparate treatments of word order in cognitive science, psycholinguistics, computational linguistics, and natural language processing under a flexible algorithmic framework. We proceed to use this heterogeneous theoretical foundation as the basis for exploring new word orders with an undercurrent of psycholinguistic optimality. In particular, we focus on notions of dependency length minimization given the difficulties in human and computational language processing in handling long-distance dependencies. We then discuss algorithms for finding optimal word orders efficiently in spite of the combinatorial space of possibilities. We conclude by addressing the implications of these word orders on human language and their downstream impacts when integrated in computational models."}}
{"id": "K1lVgqzIdvH", "cdate": 1609459200000, "mdate": 1652605998604, "content": {"title": "On the Opportunities and Risks of Foundation Models", "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature."}}
{"id": "ebXUmN02Ee", "cdate": 1577836800000, "mdate": 1652605998582, "content": {"title": "Intrinsic Evaluation of Summarization Datasets", "abstract": "Rishi Bommasani, Claire Cardie. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "AGI5Ri4C6PZ", "cdate": 1577836800000, "mdate": 1652605998605, "content": {"title": "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings", "abstract": "Rishi Bommasani, Kelly Davis, Claire Cardie. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "SJg3T2EFvr", "cdate": 1569438916224, "mdate": null, "content": {"title": "BERT Wears GloVes: Distilling Static Embeddings from Pretrained Contextual Representations", "abstract": "Contextualized word representations such as ELMo and BERT have become the de facto starting point for incorporating pretrained representations for downstream NLP tasks. In these settings, contextual representations have largely made obsolete their static embedding predecessors such as Word2Vec and GloVe. However, static embeddings do have their advantages in that they are straightforward to understand and faster to use. Additionally, embedding analysis methods for static embeddings are far more diverse and mature than those available for their dynamic counterparts. In this work, we introduce simple methods for generating static lookup table embeddings from existing pretrained contextual representations and demonstrate they outperform Word2Vec and GloVe embeddings on a variety of word similarity and word relatedness tasks. In doing so, our results also reveal insights that may be useful for subsequent downstream tasks using our embeddings or the original contextual models. Further, we demonstrate the increased potential for analysis by applying existing approaches for estimating social bias in word embeddings. Our analysis constitutes the most comprehensive study of social bias in contextual word representations (via the proxy of our distilled embeddings) and reveals a number of inconsistencies in current techniques for quantifying social bias in word embeddings. We publicly release our code and distilled word embeddings to support reproducible research and the broader NLP community. "}}
{"id": "FaeHzNONKMM", "cdate": 1546300800000, "mdate": 1652605998604, "content": {"title": "Long-Distance Dependencies Don't Have to Be Long: Simplifying through Provably (Approximately) Optimal Permutations", "abstract": "Rishi Bommasani. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. 2019."}}
