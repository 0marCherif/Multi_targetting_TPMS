{"id": "vfVvJvfgv9", "cdate": 1672169435167, "mdate": 1672169435167, "content": {"title": "Online Graph Dictionary Learning", "abstract": "Dictionary learning is a key tool for representation learning, that explains the data as linear combination of few basic elements. Yet, this analysis is not amenable in the context of graph learning, as graphs usually belong to different metric spaces. We fill this gap by proposing a new online Graph Dictionary Learning approach, which uses the Gromov Wasserstein divergence for the data fitting term. In our work, graphs are encoded through their nodes\u2019 pairwise relations and modeled as convex combination of graph atoms, i.e. dictionary elements, estimated thanks to an online stochastic algorithm, which operates on a dataset of unregistered graphs with potentially different number of nodes. Our approach naturally extends to labeled graphs, and is completed by a novel upper bound that can be used as a fast approximation of Gromov Wasserstein in the embedding space. We provide numerical evidences showing the interest of our approach for unsupervised embedding of graph datasets and for online graph subspace estimation and tracking."}}
{"id": "seYcx6CqPe", "cdate": 1652737793654, "mdate": null, "content": {"title": "Template based Graph Neural Network with Optimal Transport Distances", "abstract": "Current Graph Neural Networks (GNN) architectures generally rely on two important components: node features embedding through message passing, and aggregation with a specialized form of pooling. The structural (or topological) information is implicitly taken into account in these two steps. We propose in this work a novel point of view, which places distances to some learnable graph templates at the core of the graph representation. This distance embedding is constructed thanks to an optimal transport distance: the Fused Gromov-Wasserstein (FGW) distance, which encodes simultaneously feature and structure dissimilarities by solving a soft graph-matching problem. We postulate that the vector of FGW distances to a set of template graphs has a strong discriminative power, which is then fed to a non-linear classifier for final predictions. Distance embedding can be seen as a new layer, and can leverage on existing message passing techniques to promote sensible feature representations. Interestingly enough, in our work the optimal set of template graphs is also learnt in  an end-to-end fashion by differentiating through this layer. After describing the corresponding learning procedure, we empirically validate our claim on several synthetic and real life graph classification datasets, where our method is competitive or surpasses kernel and GNN state-of-the-art approaches. We complete our experiments by an ablation study and a sensitivity analysis to parameters."}}
{"id": "RShaMexjc-x", "cdate": 1632875533559, "mdate": null, "content": {"title": "Semi-relaxed Gromov-Wasserstein divergence and applications on graphs", "abstract": "Comparing structured objects such as graphs is a fundamental operation\ninvolved in many learning tasks. To this end, the Gromov-Wasserstein (GW)\ndistance, based on Optimal Transport (OT), has proven to be successful in\nhandling the specific nature of the associated objects. More specifically,\nthrough the nodes connectivity relations, GW operates on graphs, seen as\nprobability measures over specific spaces. At the core of OT is the idea of\nconservation of mass, which imposes a coupling between all the nodes from\nthe two considered graphs. We argue in this paper that this property can be\ndetrimental for tasks such as graph dictionary or partition learning, and we\nrelax it by proposing a new semi-relaxed Gromov-Wasserstein divergence.\nAside from immediate computational benefits, we discuss its properties, and\nshow that it can lead to an efficient graph dictionary learning algorithm.\nWe empirically demonstrate its relevance for complex tasks on graphs such as\npartitioning, clustering and completion."}}
{"id": "ByllaSHl8H", "cdate": 1567802808126, "mdate": null, "content": {"title": "Sliced Gromov-Wasserstein", "abstract": "Recently used in various machine learning contexts, the Gromov-Wasserstein distance (GW) allows for comparing distributions that do not necessarily lie in the same metric space. However, this Optimal Transport (OT) distance requires solving a complex non convex quadratic program which is most of the time very costly both in time and memory. Contrary to GW, the Wasserstein distance (W) enjoys several properties (e.g. duality) that permit large scale optimization. Among those, the Sliced Wasserstein (SW) distance exploits the direct solution of W on the line, that only requires sorting discrete samples in 1D. This paper propose a new divergence based on GW akin to SW. We first derive a closed form for GW when dealing with 1D distributions, based on a new result for the related quadratic assignment problem. We then define a novel OT discrepancy that can deal with large scale distributions via a slicing approach and we show how it relates to the GW distance while being $O(n^2)$ to compute. We illustrate the behavior of this  so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where we demonstrate its ability to tackle similar problems as GW while being several order of magnitudes faster to compute. "}}
{"id": "rkZSX3Wd-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Optimal Transport for structured data with application on graphs", "abstract": "This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transpor..."}}
