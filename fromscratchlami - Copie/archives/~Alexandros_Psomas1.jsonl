{"id": "Od4oKKwBx7Z", "cdate": 1652737842868, "mdate": null, "content": {"title": "On Infinite Separations Between Simple and Optimal Mechanisms", "abstract": "We consider a revenue-maximizing seller with $k$ heterogeneous items for sale to a single additive buyer, whose values are drawn from a known, possibly correlated prior $\\mathcal{D}$. It is known that there exist priors $\\mathcal{D}$ such that simple mechanisms --- those with bounded menu complexity --- extract an arbitrarily small fraction of the optimal revenue~(Briest et al. 2015, Hart and Nisan 2019). This paper considers the opposite direction: given a correlated distribution $\\mathcal{D}$ witnessing an infinite separation between simple and optimal mechanisms, what can be said about $\\mathcal{D}$?\n\n\\citet{hart2019selling} provides a framework for constructing such $\\mathcal{D}$: it takes as input a sequence of $k$-dimensional vectors satisfying some geometric property, and produces a $\\mathcal{D}$ witnessing an infinite gap. Our first main result establishes that this framework is without loss: every $\\mathcal{D}$ witnessing an infinite separation could have resulted from this framework. An earlier version of their work provided a more streamlined framework (Hart and Nisan 2013). Our second main result establishes that this restrictive framework is not tight. That is, we provide an instance $\\mathcal{D}$ witnessing an infinite gap, but which provably could not have resulted from the restrictive framework. \n\nAs a corollary, we discover a new kind of mechanism which can witness these infinite separations on instances where the previous ``aligned'' mechanisms do not."}}
{"id": "YiFQqYAk1xH", "cdate": 1652737416811, "mdate": null, "content": {"title": "Dynamic Fair Division with Partial Information", "abstract": "We consider the fundamental problem of fairly and efficiently allocating $T$ indivisible items among $n$ agents with additive preferences. The items become available over a sequence of rounds, and every item must be allocated immediately and irrevocably before the next one arrives. Previous work shows that when the agents' valuations for the items are drawn from known distributions, it is possible (under mild technical assumptions) to find allocations that are envy-free with high probability and Pareto efficient ex-post. \n\nWe study a \\emph{partial-information} setting, where it is possible to elicit ordinal but not cardinal information. When a new item arrives, the algorithm can query each agent for the relative rank of this item with respect to a subset of the past items. \nWhen  values are drawn from i.i.d.\\ distributions, we give an algorithm that is envy-free and $(1-\\epsilon)$-welfare-maximizing with high probability. We provide similar guarantees (envy-freeness and a constant approximation to welfare with high probability) even with minimally expressive queries that ask for a comparison to a single previous item. For independent but non-identical agents, we obtain envy-freeness and a constant approximation to Pareto efficiency with high probability. We prove that all our results are asymptotically tight. "}}
{"id": "ylila4AYSpV", "cdate": 1652737390622, "mdate": null, "content": {"title": "Simple Mechanisms for Welfare Maximization in Rich Advertising Auctions", "abstract": "Internet ad auctions have evolved from a few lines of text to richer informational layouts that include images, sitelinks, videos, etc. Ads in these new formats occupy varying amounts of space, and an advertiser can provide multiple formats, only one of which can be shown.\nThe seller is now faced with a multi-parameter mechanism design problem.\nComputing an efficient allocation is computationally intractable, and therefore the standard Vickrey-Clarke-Groves (VCG) auction, while truthful and welfare-optimal, is impractical. \n\nIn this paper, we tackle a fundamental problem in the design of modern ad auctions. We adopt a ``Myersonian'' approach and study allocation rules that are monotone both in the bid and set of rich ads. We show that such rules can be paired with a payment function to give a truthful auction. Our main technical challenge is designing a monotone rule that yields a good approximation to the optimal welfare. Monotonicity doesn't hold for standard algorithms, e.g. the incremental bang-per-buck order, that give good approximations to ``knapsack-like'' problems such as ours. In fact, we show that no deterministic monotone rule can approximate the optimal welfare within a factor better than $2$ (while there is a non-monotone FPTAS). Our main result is a new, simple, greedy and monotone allocation rule that guarantees a $3$ approximation. In ad auctions in practice, monotone allocation rules are often paired with the so-called \\emph{Generalized Second Price (GSP)} payment rule, which charges the minimum threshold price below which the allocation changes. We prove that, even though our monotone allocation rule paired with GSP is not truthful, its Price of Anarchy (PoA) is bounded. Under standard no-overbidding assumptions, we prove bounds on the a pure and Bayes-Nash PoA. Finally, we experimentally test our algorithms on real-world data."}}
{"id": "6H00JM-DZjU", "cdate": 1652737374465, "mdate": null, "content": {"title": "Fair and Efficient Allocations Without Obvious Manipulations", "abstract": "We consider the fundamental problem of allocating a set of indivisible goods among strategic agents with additive valuation functions. It is well known that, in the absence of monetary transfers, Pareto efficient and truthful rules are dictatorial, while there is no deterministic truthful mechanism that allocates all items and achieves envy-freeness up to one item (EF1), even for the case of two agents. In this paper, we investigate the interplay of fairness and efficiency under a relaxation of truthfulness called non-obvious manipulability (NOM), recently proposed by~\\citep{troyan2020obvious}. We show that this relaxation allows us to bypass the aforementioned negative results in a very strong sense. Specifically, we prove that there are deterministic and EF1 algorithms that are not obviously manipulable, and the algorithm that maximizes utilitarian social welfare (the sum of agents' utilities), which is Pareto efficient but not dictatorial, is not obviously manipulable for $n \\geq 3$ agents (but obviously manipulable for $n=2$ agents). At the same time, maximizing the egalitarian social welfare (the minimum of agents' utilities) or the Nash social welfare (the product of agents' utilities) is obviously manipulable for any number of agents and items. Our main result is an approximation preserving black-box reduction from the problem of designing EF1 and NOM mechanisms to the problem of designing EF1 algorithms. En route, we prove an interesting structural result about EF1 allocations, as well as new ``best-of-both-worlds'' results (for the problem without incentives), that might be of independent interest."}}
{"id": "rKc72lIwUb", "cdate": 1640995200000, "mdate": 1683896727657, "content": {"title": "Leakage Inversion: Towards Quantifying Privacy in Searchable Encryption", "abstract": ""}}
{"id": "oXFUfqj3fm0", "cdate": 1640995200000, "mdate": 1683896727602, "content": {"title": "Simple Mechanisms for Welfare Maximization in Rich Advertising Auctions", "abstract": "Internet ad auctions have evolved from a few lines of text to richer informational layouts that include images, sitelinks, videos, etc. Ads in these new formats occupy varying amounts of space, and an advertiser can provide multiple formats, only one of which can be shown.The seller is now faced with a multi-parameter mechanism design problem.Computing an efficient allocation is computationally intractable, and therefore the standard Vickrey-Clarke-Groves (VCG) auction, while truthful and welfare-optimal, is impractical. In this paper, we tackle a fundamental problem in the design of modern ad auctions. We adopt a ``Myersonian'' approach and study allocation rules that are monotone both in the bid and set of rich ads. We show that such rules can be paired with a payment function to give a truthful auction. Our main technical challenge is designing a monotone rule that yields a good approximation to the optimal welfare. Monotonicity doesn't hold for standard algorithms, e.g. the incremental bang-per-buck order, that give good approximations to ``knapsack-like'' problems such as ours. In fact, we show that no deterministic monotone rule can approximate the optimal welfare within a factor better than $2$ (while there is a non-monotone FPTAS). Our main result is a new, simple, greedy and monotone allocation rule that guarantees a $3$ approximation. In ad auctions in practice, monotone allocation rules are often paired with the so-called \\emph{Generalized Second Price (GSP)} payment rule, which charges the minimum threshold price below which the allocation changes. We prove that, even though our monotone allocation rule paired with GSP is not truthful, its Price of Anarchy (PoA) is bounded. Under standard no-overbidding assumptions, we prove bounds on the a pure and Bayes-Nash PoA. Finally, we experimentally test our algorithms on real-world data."}}
{"id": "nBfgvCLcJ5O", "cdate": 1640995200000, "mdate": 1681659762116, "content": {"title": "Smoothed Analysis of Social Choice Revisited", "abstract": "A canonical problem in social choice is how to aggregate ranked votes: given $n$ voters' rankings over $m$ candidates, what voting rule $f$ should we use to aggregate these votes into a single winner? One standard method for comparing voting rules is by their satisfaction of axioms - properties that we want a \"reasonable\" rule to satisfy. Unfortunately, this approach leads to several impossibilities: no voting rule can simultaneously satisfy all the properties we want, at least in the worst case over all possible inputs. Motivated by this, we consider a relaxation of these worst case requirements. We do so using a \"smoothed\" model of social choice, where votes are perturbed with small amounts of noise. If, no matter which input profile we start with, the probability (post-noise) of an axiom being satisfied is large, we will consider the axiom as good as satisfied - called \"smoothed-satisfied\" - even if it may be violated in the worst case. Our model is a mild restriction of Lirong Xia's, and corresponds closely to that in Spielman and Teng's original work on smoothed analysis. Much work has been done so far in several papers by Xia on axiom satisfaction under such noise. In our paper, we aim to give a more cohesive overview on when smoothed analysis of social choice is useful. Within our model, we give simple sufficient conditions for smoothed-satisfaction or smoothed-violation of several previously-unstudied axioms and paradoxes, plus many of those studied by Xia. We then observe that, in a practically important subclass of noise models, although convergence eventually occurs, known rates may require an extremely large number of voters. Motivated by this, we prove bounds specifically within a canonical noise model from this subclass - the Mallows model. Here, we present a more nuanced picture on exactly when smoothed analysis can help."}}
{"id": "klMAh5BM40", "cdate": 1640995200000, "mdate": 1683896727614, "content": {"title": "On Infinite Separations Between Simple and Optimal Mechanisms", "abstract": "We consider a revenue-maximizing seller with $k$ heterogeneous items for sale to a single additive buyer, whose values are drawn from a known, possibly correlated prior $\\mathcal{D}$. It is known that there exist priors $\\mathcal{D}$ such that simple mechanisms -- those with bounded menu complexity -- extract an arbitrarily small fraction of the optimal revenue. This paper considers the opposite direction: given a correlated distribution $\\mathcal{D}$ witnessing an infinite separation between simple and optimal mechanisms, what can be said about $\\mathcal{D}$? Previous work provides a framework for constructing such $\\mathcal{D}$: it takes as input a sequence of $k$-dimensional vectors satisfying some geometric property, and produces a $\\mathcal{D}$ witnessing an infinite gap. Our first main result establishes that this framework is without loss: every $\\mathcal{D}$ witnessing an infinite separation could have resulted from this framework. Even earlier work provided a more streamlined framework. Our second main result establishes that this restrictive framework is not tight. That is, we provide an instance $\\mathcal{D}$ witnessing an infinite gap, but which provably could not have resulted from the restrictive framework. As a corollary, we discover a new kind of mechanism which can witness these infinite separations on instances where the previous ''aligned'' mechanisms do not."}}
{"id": "jfmO0m4ZqI", "cdate": 1640995200000, "mdate": 1683896727649, "content": {"title": "Risk-Robust Mechanism Design for a Prospect-Theoretic Buyer", "abstract": "Consider the revenue maximization problem of a risk-neutral seller with m heterogeneous items for sale to a single additive buyer, whose values for the items are drawn from known distributions. If the buyer is also risk-neutral, it is known that a simple and natural mechanism, namely the better of selling separately or pricing only the grand bundle, gives a constant-factor approximation to the optimal revenue. In this paper we study revenue maximization without risk-neutral buyers. Specifically, we adopt cumulative prospect theory, a well established generalization of expected utility theory. Our starting observation is that such preferences give rise to a very rich space of mechanisms, allowing the seller to extract arbitrary revenue. Specifically, a seller can construct extreme lotteries that look attractive to a mildly optimistic buyer, but have arbitrarily negative true expectation. Therefore, giving the seller absolute freedom over the design space results in absurd conclusions; competing with the optimal mechanism is hopeless. Instead, in this paper we study four broad classes of mechanisms, each characterized by a distinct use of randomness. Our goal is twofold: to explore the power of randomness when the buyer is not risk-neutral, and to design simple and attitude-agnostic mechanisms\u2014mechanisms that do not depend on details of the buyer\u2019s risk attitude\u2014which are good approximations of the optimal in-class mechanism, tailored to a specific risk attitude. Our main result is that the same simple and risk-agnostic mechanism (the better of selling separately or pricing only the grand bundle) is a good approximation to the optimal non-agnostic mechanism within three of the mechanism classes we study."}}
{"id": "YxNpMdEa8Tv", "cdate": 1640995200000, "mdate": 1683896727667, "content": {"title": "Relax the Non-Collusion Assumption for Multi-Server PIR", "abstract": "A long line of research on secure computation has confirmed that anything that can be computed, can be computed securely using a set of non-colluding parties. Indeed, this non-collusion assumption makes a number of problems solvable, as well as reduces overheads and bypasses computational hardness results, and it is pervasive in the privacy-preserving computation literature. However, it remains highly susceptible to covert, undetectable collusion among computing parties. This work stems from an observation that if the number of available computing parties is much higher than the number of parties required to perform a secure computation task, collusion attempts in privacy-preserving computations could be deterred. We focus on the prominent privacy-preserving computation task of multi-server $1$-private information retrieval (PIR) that inherently assumes no pair-wise collusion. For PIR application scenarios, such as those for blockchain light clients, where the available servers can be plentiful, a single server's deviating action is not tremendously beneficial to itself. We can make deviations undesired via small amounts of rewards and penalties, thus significantly {\\em raising the bar} for collusion resistance. We design and implement a collusion mitigation mechanism on a public bulletin board with payment execution functions, considering only rational parties and no honest non-colluding servers. Privacy protection is offered for an extended period after the query executions."}}
