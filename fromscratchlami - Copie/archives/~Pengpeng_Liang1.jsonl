{"id": "wGZVq1no9dB", "cdate": 1640995200000, "mdate": 1652669195480, "content": {"title": "Pseudo Segmentation for Semantic Information-Aware Stereo Matching", "abstract": "Stereo matching plays an important role in computer vision and robotics. Though substantial progress has been made on deep learning-based algorithms, the inherent semantic information within the ground truth of the training data for stereo matching has not been well explored. In this letter, we propose to use a pseudo segmentation sub-network to extract additional semantic information. More specifically, we divide the disparity label into groups and let each group correspond to a class for pseudo segmentation. To assist stereo matching with the semantic information obtained from pseudo segmentation, we inject the feature maps at the end of the pseudo segmentation sub-network into the cost volume that is used to infer the pixel-level disparity. To validate the effectiveness of the proposed approach, we select PSMNet (Chang and Chen, 2018)and GwcNet (Guo <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i> , 2019) as baselines and enhance them with the pseudo segmentation sub-network. Comprehensive experiments are carried out on the Scene Flow, KITTI 2015, and KITTI 2012 datasets, and the results show that our proposed method can improve the performance notably."}}
{"id": "X5o7Pt_xymc", "cdate": 1640995200000, "mdate": 1652669195484, "content": {"title": "Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving", "abstract": "Detection of rare objects (e.g., traffic cones, traffic barrels and traffic warning triangles) is an important perception task to improve the safety of autonomous driving. Training of such models typically requires a large number of annotated data which is expensive and time consuming to obtain. To address the above problem, an emerging approach is to apply data augmentation to automatically generate cost-free training samples. In this work, we propose a systematic study on simple Copy-Paste data augmentation for rare object detection in autonomous driving. Specifically, local adaptive instance-level image transformation is introduced to generate realistic rare object masks from source domain to the target domain. Moreover, traffic scene context is utilized to guide the placement of masks of rare objects. To this end, our data augmentation generates training data with high quality and realistic characteristics by leveraging both local and global consistency. In addition, we build a new dataset, Rare Object Dataset (ROD), consisting 10k training images, 4k validation images and the corresponding labels with a diverse range of scenarios in autonomous driving. Experiments on ROD show that our method achieves promising results on rare object detection. We also present a thorough study to illustrate the effectiveness of our local-adaptive and global constraints based Copy-Paste data augmentation for rare object detection. The data, development kit and more information of ROD are available online at: \\url{https://nullmax-vision.github.io}."}}
{"id": "CaO5PSPO6kp", "cdate": 1640995200000, "mdate": 1652669195557, "content": {"title": "BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs", "abstract": "Semantic segmentation in bird's eye view (BEV) is an important task for autonomous driving. Though this task has attracted a large amount of research efforts, it is still challenging to flexibly cope with arbitrary (single or multiple) camera sensors equipped on the autonomous vehicle. In this paper, we present BEVSegFormer, an effective transformer-based method for BEV semantic segmentation from arbitrary camera rigs. Specifically, our method first encodes image features from arbitrary cameras with a shared backbone. These image features are then enhanced by a deformable transformer-based encoder. Moreover, we introduce a BEV transformer decoder module to parse BEV semantic segmentation results. An efficient multi-camera deformable attention unit is designed to carry out the BEV-to-image view transformation. Finally, the queries are reshaped according the layout of grids in the BEV, and upsampled to produce the semantic segmentation result in a supervised manner. We evaluate the proposed algorithm on the public nuScenes dataset and a self-collected dataset. Experimental results show that our method achieves promising performance on BEV semantic segmentation from arbitrary camera rigs. We also demonstrate the effectiveness of each component via ablation study."}}
{"id": "ozQxgqw55m", "cdate": 1609459200000, "mdate": 1652669195484, "content": {"title": "Joint Spinal Centerline Extraction and Curvature Estimation with Row-Wise Classification and Curve Graph Network", "abstract": "Spinal curvature estimation plays an important role in adolescent idiopathic scoliosis (AIS) evaluation and treatment. The Cobb angle is a well-established standard for spinal curvature estimation. Recent studies of Cobb angle estimation usually rely on detection of vertebra landmarks which requires complex post-processing of curvature calculation. Approaches directly regressing the Cobb angles apply entire image or centerline segmentation results as the network input, which limits exploring the specific curve structure of the spine. In this paper, we propose a deep learning-based approach to simultaneously estimate spine centerline and spinal curvature with shared convolutional backbone. The spine centerline extraction is formulated as a row-wise classification task. To directly regress Cobb angles, we adopt curve graph convolution to exploit curve structure of the spine centerline. In addition, given a spine centerline, a Curve Feature Pooling (CFP) module is designed to aggregate features used as the input of Curve Graph Network (CGN) to regress the Cobb angles. We evaluate our method on the accurate automated spinal curvature estimation (AASCE) challenge 2019, and the proposed approach achieves promising results on both spine centerline extraction and Cobb angles estimation tasks."}}
{"id": "oBElf-W_ptE", "cdate": 1609459200000, "mdate": 1652669195650, "content": {"title": "Ghost-Light-3dnet: Efficient Network For Heart Segmentation", "abstract": "Accurate 3D whole heart segmentation provides more details of the morphological and pathological information that could help doctors with more effective patient-specific treatments. 3D CNN network has been recognized as an important role in accurate volumetric segmentation. Typically, 3D CNN network has a large number of parameters as well as the floating point operations (FLOPs), which leads to heavy and complex computation. In this paper, we introduce an efficient 3D network (Ghost-Light-3DNet) for heart segmentation. Our solution is characterized by two key components: First, inspired by GhostNet in 2D, we extend the Ghost module to 3D which can generate more feature maps from cheap operations. Second, a sequential separable conv with residual module is applied as a light plug-and-play component to further reduce network parameters and FLOPs. For evaluation, the proposed method is validated on the MM-WHS heart segmentation Challenge 2017 datasets. Compared to state-of-the-art solution using 3D UNet-like architecture, our Ghost-Light-3DNet achieves comparable segmentation accuracy with the 2. 18x fewer parameters and 4. 48x less FLOPs, respectively."}}
{"id": "iILVr7CiJxw", "cdate": 1609459200000, "mdate": 1652669195480, "content": {"title": "Coarse-to-fine Semantic Localization with HD Map for Autonomous Driving in Structural Scenes", "abstract": "Robust and accurate localization is an essential component for robotic navigation and autonomous driving. The use of cameras for localization with high definition map (HD Map) provides an affordable localization sensor set. Existing methods suffer from pose estimation failure due to error prone data association or initialization with accurate initial pose requirement. In this paper, we propose a cost-effective vehicle localization system with HD map for autonomous driving that uses cameras as primary sensors. To this end, we formulate vision-based localization as a data association problem that maps visual semantics to landmarks in HD map. Specifically, system initialization is finished in a coarse to fine manner by combining coarse GPS (Global Positioning System) measurement and fine pose searching. In tracking stage, vehicle pose is refined by implicitly aligning the semantic segmentation result between image and landmarks in HD maps with photometric consistency. Finally, vehicle pose is computed by pose graph optimization in a sliding window fashion. We evaluate our method on two datasets and demonstrate that the proposed approach yields promising localization results in different driving scenarios. Additionally, our approach is suitable for both monocular camera and multi-cameras that provides flexibility and improves robustness for the localization system."}}
{"id": "cv97yk_Pgx", "cdate": 1609459200000, "mdate": 1652669195483, "content": {"title": "Coarse-to-fine Semantic Localization with HD Map for Autonomous Driving in Structural Scenes", "abstract": "Robust and accurate localization is an essential component for robotic navigation and autonomous driving. The use of cameras for localization with high definition map (HD Map) provides an affordable localization sensor set. Existing methods suffer from pose estimation failure due to error prone data association or initialization with accurate initial pose requirement. In this paper, we propose a cost-effective vehicle localization system with HD map for autonomous driving that uses cameras as primary sensors. To this end, we formulate vision-based localization as a data association problem that maps visual semantics to landmarks in HD map. Specifically, system initialization is finished in a coarse to fine manner by combining coarse GPS (Global Positioning System) measurement and fine pose searching. In tracking stage, vehicle pose is refined by implicitly aligning the semantic segmentation result between image and landmarks in HD maps with photometric consistency. Finally, vehicle pose is computed by pose graph optimization in a sliding window fashion. We evaluate our method on two datasets and demonstrate that the proposed approach yields promising localization results in different driving scenarios. Additionally, our approach is suitable for both monocular camera and multi-cameras that provides flexibility and improves robustness for the localization system."}}
{"id": "F5A4f1wQ_8f", "cdate": 1609459200000, "mdate": 1652669195470, "content": {"title": "Planar object tracking benchmark in the wild", "abstract": ""}}
{"id": "F-d3KzZiuPN", "cdate": 1609459200000, "mdate": 1652669195556, "content": {"title": "Learning local descriptors with multi-level feature aggregation and spatial context pyramid", "abstract": ""}}
{"id": "86cPGj3oVYk", "cdate": 1609459200000, "mdate": 1652669195479, "content": {"title": "A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking", "abstract": "Siamese trackers are shown to be vulnerable to adversarial attacks recently. However, the existing attack methods craft the perturbations for each video independently, which comes at a non-negligible computational cost. In this paper, we show the existence of universal perturbations that can enable the targeted attack, e.g., forcing a tracker to follow the ground-truth trajectory with specified offsets, to be video-agnostic and free from inference in a network. Specifically, we attack a tracker by adding a universal imperceptible perturbation to the template image and adding a fake target, i.e., a small universal adversarial patch, into the search images adhering to the predefined trajectory, so that the tracker outputs the location and size of the fake target instead of the real target. Our approach allows perturbing a novel video to come at no additional cost except the mere addition operations -- and not require gradient optimization or network inference. Experimental results on several datasets demonstrate that our approach can effectively fool the Siamese trackers in a targeted attack manner. We show that the proposed perturbations are not only universal across videos, but also generalize well across different trackers. Such perturbations are therefore doubly universal, both with respect to the data and the network architectures. We will make our code publicly available."}}
