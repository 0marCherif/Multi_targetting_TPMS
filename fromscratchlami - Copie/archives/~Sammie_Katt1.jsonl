{"id": "2y8A215u0at", "cdate": 1640995200000, "mdate": 1681491644286, "content": {"title": "BADDr: Bayes-Adaptive Deep Dropout RL for POMDPs", "abstract": ""}}
{"id": "f3eEY3p_4x", "cdate": 1577836800000, "mdate": 1682339393288, "content": {"title": "Removing Dynamic Objects for Static Scene Reconstruction using Light Fields", "abstract": ""}}
{"id": "t1KbcwBR8jD", "cdate": 1546300800000, "mdate": null, "content": {"title": "Bayesian Reinforcement Learning in Factored POMDPs", "abstract": "Model-based Bayesian Reinforcement Learning (BRL) provides a principled solution to dealing with the exploration-exploitation trade-off, but such methods typically assume a fully observable environments. The few Bayesian RL methods that are applicable in partially observable domains, such as the Bayes-Adaptive POMDP (BA-POMDP), scale poorly. To address this issue, we introduce the Factored BA-POMDP model (FBA-POMDP), a framework that is able to learn a compact model of the dynamics by exploiting the underlying structure of a POMDP. The FBA-POMDP framework casts the problem as a planning task, for which we adapt the Monte-Carlo Tree Search planning algorithm and develop a belief tracking method to approximate the joint posterior over the state and model variables. Our empirical results show that this method outperforms a number of BRL baselines and is able to learn efficiently when the factorization is known, as well as learn both the factorization and the model parameters simultaneously."}}
{"id": "dtAcR0EKRdM", "cdate": 1546300800000, "mdate": null, "content": {"title": "Online Planning for Target Object Search in Clutter under Partial Observability", "abstract": "The problem of finding and grasping a target object in a cluttered, uncertain environment, target object search, is a common and important problem in robotics. One key challenge is the uncertainty of locating and recognizing each object in a cluttered environment due to noisy perception and occlusions. Furthermore, the uncertainty in localization makes manipulation difficult and uncertain. To cope with these challenges, we formulate the target object search task as a partially observable Markov decision process (POMDP), enabling the robot to reason about perceptual and manipulation uncertainty while searching. To further address the manipulation difficulty, we propose Parameterized Action Partially Observable Monte-Carlo Planning (PA-POMCP), an algorithm that evaluates manipulation actions by taking into account the effect of the robot's current belief on the success of the action execution. In addition, a novel run-time initial belief generator and a state value estimator are introduced in this paper to facilitate the PA-POMCP algorithm. Our experiments show that our methods solve the target object search task in settings where simpler methods either take more object movements or fail."}}
{"id": "ByVkro-_WB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Learning in POMDPs with Monte Carlo Tree Search", "abstract": "The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision..."}}
{"id": "vaarzV93fW3", "cdate": 1451606400000, "mdate": 1682339415244, "content": {"title": "Anytime versus Real-Time Heuristic Search for On-Line Planning", "abstract": ""}}
