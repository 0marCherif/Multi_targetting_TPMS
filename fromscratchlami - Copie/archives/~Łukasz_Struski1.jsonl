{"id": "UTIqrYRX1Uj", "cdate": 1698658762638, "mdate": 1698658762638, "content": {"title": "Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations", "abstract": "Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies."}}
{"id": "p36db089HBP", "cdate": 1632875625121, "mdate": null, "content": {"title": "SONG: Self-Organizing Neural Graphs", "abstract": "Recent years have seen a surge in research on combining deep neural networks with other methods, including decision trees and graphs. There are at least three advantages of incorporating decision trees and graphs: they are easy to interpret since they are based on sequential decisions, they can make decisions faster, and they provide a hierarchy of classes. However, one of the well-known drawbacks of decision trees, as compared to decision graphs, is that decision trees cannot reuse the decision nodes. Nevertheless, decision graphs were not commonly used in deep learning due to the lack of efficient gradient-based training techniques. In this paper, we fill this gap and provide a general paradigm based on Markov processes, which allows for efficient training of the special type of decision graphs, which we call Self-Organizing Neural Graphs (SONG). We provide an extensive theoretical study of SONG, complemented by experiments conducted on Letter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our method performs on par or better than existing decision models."}}
{"id": "wxYPUnMSQCR", "cdate": 1591823295505, "mdate": null, "content": {"title": "Processing of incomplete images by (graph) convolutional neural networks", "abstract": "We investigate the problem of processing incomplete images by neural networks without replacing missing values. To deal with this problem, we first represent an image as a graph, in which missing pixels are entirely ignored. The graph image representation is processed using Geo-GCN -- a type of graph convolutional neural networks, which is a proper generalization of classical CNNs operating on images. On one hand, our approach avoids the problem of missing data imputation while, on the other hand, there is a natural correspondence between CNNs and Geo-GCN. Experiments confirm that our approach performs better than analogical CNNs with the imputation of missing values on typical classification and reconstruction tasks."}}
{"id": "VR6mXmaHacL", "cdate": 1591458338670, "mdate": null, "content": {"title": "Estimating conditional density of missing values using deep Gaussian mixture model", "abstract": "We consider the problem of estimating the conditional probability distribution of missing values given the observed ones. We propose an approach, which combines the flexibility of deep neural networks with the simplicity of Gaussian mixture models (GMMs). Given an incomplete data point, our neural network returns the parameters of Gaussian distribution (in the form of Factor Analyzers model) representing the corresponding conditional density. We experimentally verify that our model provides better log-likelihood than conditional GMM trained in a typical way. Moreover, imputation obtained by replacing missing values using the mean vector of our model looks visually plausible."}}
{"id": "C2nr-4elBV", "cdate": 1582750154365, "mdate": null, "content": {"title": "Can auto-encoders help with filling missing data?", "abstract": "This paper introduces an approach to filling in missing data based on deep auto-encoder models, adequate to high-dimensional data exhibiting complex dependencies, such as images. The method exploits the properties of auto-encoders' vector fields, which allows to approximate the gradient of the log-density from its reconstruction error, based on which we propose a projected gradient ascent algorithm to obtain the  conditionally most probable estimate of the missing values. Experiments performed on benchmark datasets show that imputations produced by our model are sharp and realistic."}}
{"id": "SklVqa4YwH", "cdate": 1569439116394, "mdate": null, "content": {"title": "Realism Index: Interpolation in Generative Models With Arbitrary Prior", "abstract": "In order to perform plausible interpolations in the latent space of a generative model, we need a measure that credibly reflects if a point in an interpolation is close to the data manifold being modelled, i.e. if it is convincing. In this paper, we introduce a realism index of a point, which can be constructed from an arbitrary prior density, or based on FID score approach in case a prior is not available. We propose a numerically efficient algorithm that directly maximises the realism index of an interpolation which, as we theoretically prove, leads to a search of a geodesic with respect to the corresponding Riemann structure. We show that we obtain better interpolations then the classical linear ones, in particular when either the prior density is not convex shaped, or when the soap bubble effect appears."}}
{"id": "Hke8Do0cF7", "cdate": 1538087774358, "mdate": null, "content": {"title": "Deep processing of structured data", "abstract": "We construct a general unified framework for learning representation of structured\ndata, i.e. data which cannot be represented as the fixed-length vectors (e.g. sets,\ngraphs, texts or images of varying sizes). The key factor is played by an intermediate\nnetwork called SAN (Set Aggregating Network), which maps a structured\nobject to a fixed length vector in a high dimensional latent space. Our main theoretical\nresult shows that for sufficiently large dimension of the latent space, SAN is\ncapable of learning a unique representation for every input example. Experiments\ndemonstrate that replacing pooling operation by SAN in convolutional networks\nleads to better results in classifying images with different sizes. Moreover, its direct\napplication to text and graph data allows to obtain results close to SOTA, by\nsimpler networks with smaller number of parameters than competitive models."}}
