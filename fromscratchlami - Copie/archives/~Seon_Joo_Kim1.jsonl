{"id": "6nNO2iscsQL", "cdate": 1696317887931, "mdate": null, "content": {"title": "THMM-DiT:  Talking Head Motion Modeling with Diffusion Transformer", "abstract": "Generating natural talking head motion is a challenging task due to the one-to-many nature of speech-to-motion mapping, the high dimensionality of RGB video, and the difficulty of modeling dynamic head poses. In this technical report, we propose a new approach to generating natural talking head motion that addresses these challenges. Our approach uses a diffusion model to generate a distribution of possible head poses, which is then conditioned on the given audio to produce a natural-looking talking head. We also use a face template to reduce the computational resources required to generate high-quality RGB videos. Finally, we employ long clue frames with spatio-temporal attention of transformer to generate natural long-term sequences of head poses. Our approach is able to generate dynamic head poses in the long term while accurately synchronizing mouth shapes with the given audio."}}
{"id": "EWnX3kOs2X", "cdate": 1667371202531, "mdate": 1667371202531, "content": {"title": "Error Compensation Framework for Flow-Guided Video Inpainting", "abstract": "The key to video inpainting is to use correlation information from as many reference frames as possible. Existing flow-based propagation\nmethods split the video synthesis process into multiple steps: flow completion \u2192 pixel propagation \u2192 synthesis. However, there is a significant drawback that the errors in each step continue to accumulate and amplify in the next step. To this end, we propose an Error Compensation Framework for Flow-guided Video Inpainting (ECFVI), which takes advantage of the flow-based method and offsets its weaknesses. We address the weakness with the newly designed flow completion module and the error compensation network that exploits the error guidance map. Our approach greatly improves the temporal consistency and the visual quality of the completed videos. Experimental results show the superior performance of our proposed method with the speed up of \u00d76, compared to the state-of-the-art methods. In addition, we present a new benchmark dataset for evaluation by supplementing the weaknesses of existing test datasets."}}
{"id": "0kAhIGQNHF", "cdate": 1667371106663, "mdate": 1667371106663, "content": {"title": "Deep Space-Time Video Upsampling Networks", "abstract": "Video super-resolution (VSR) and frame interpolation (FI) are traditional computer vision problems, and the performance have been improving by incorporating deep learning recently. In this paper, we investigate the problem of jointly upsampling videos both in space and time, which is becoming more important with advances in display systems. One solution for this is to run VSR and FI, one by one, independently. This is highly ine\u000ecient as heavy deep neural networks (DNN) are involved in each solution. To this end, we propose an end- to-end DNN framework for the space-time video upsampling by effciently merging VSR and FI into a joint framework. In our framework, a novel weighting scheme is proposed to fuse all input frames effectively without explicit motion compensation for efficient processing of videos. The results show better results both quantitatively and qualitatively, while reducing the computation time (x7 faster) and the number of parameters (30%) compared to baselines. Our source code is available at https://github.com/JaeYeonKang/STVUN-Pytorch."}}
{"id": "dkTmdhgapdn", "cdate": 1667358203826, "mdate": 1667358203826, "content": {"title": "Direct Association of Object Queries for Video Instance Segmentation", "abstract": "Recent Transformer-based offline Video Instance Segmentation (VIS) studies have shown that localizing the information in Transformer layers is more effective than attending to the entire spatio-temporal feature volume. From this observation, we hypothesize that explicit use of object-oriented information on spatial scenes can be a strong solution for understanding the context of the entire sequence. Thus, we introduce a new paradigm for offline VIS that learns to integrate decoded object queries from independent frames. Specifically, we propose a simple module that can be easily built on top of an off-the-shelf Transformer-based image instance segmentation model. Leaving the frame-level model to distill the rich knowledge of the spatial scene into its object queries, the proposed module directly associates and identifies the given potential objects by building temporal interactions in between. With a Swin-L backbone, our proposed method sets a record of 50.7 AP which ranks the 3rd place in Track 2-Video Instance Segmentation of the 4th Large-scale Video Object Segmentation Challenge."}}
{"id": "Jq3uTzLg9se", "cdate": 1654066966549, "mdate": null, "content": {"title": "ComMU: Dataset for Combinatorial Music Generation", "abstract": "Commercial adoption of automatic music composition requires the capability of generating diverse and high-quality music suitable for the desired context (e.g., music for romantic movies, action games, restaurants, etc.). In this paper, we introduce combinatorial music generation, a new task to create varying background music based on given conditions. Combinatorial music generation creates short samples of music with rich musical metadata, and combines them to produce a complete music. In addition, we introduce ComMU, the first symbolic music dataset consisting of short music samples and their corresponding 12 musical metadata for combinatorial music generation. Notable properties of ComMU are that (1) dataset is manually constructed by professional composers with an objective guideline that induces regularity, and (2) it has 12 musical metadata that embraces composers' intentions. Our results show that we can generate diverse high-quality music only with metadata, and that our unique metadata such as track-role and extended chord quality improves the capacity of the automatic composition. We highly recommend watching our video before reading the paper (https://pozalabs.github.io/ComMU/)."}}
{"id": "xnuN2vGmZA0", "cdate": 1652737316727, "mdate": null, "content": {"title": "VITA: Video Instance Segmentation via Object Token Association", "abstract": "We introduce a novel paradigm for offline Video Instance Segmentation (VIS), based on the hypothesis that explicit object-oriented information can be a strong clue for understanding the context of the entire sequence. To this end, we propose VITA, a simple structure built on top of an off-the-shelf Transformer-based image instance segmentation model. Specifically, we use an image object detector as a means of distilling object-specific contexts into object tokens. VITA accomplishes video-level understanding by associating frame-level object tokens without using spatio-temporal backbone features. By effectively building relationships between objects using the condensed information, VITA achieves the state-of-the-art on VIS benchmarks with a ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 & 2021, and 19.6 AP on OVIS. Moreover, thanks to its object token-based structure that is disjoint from the backbone features, VITA shows several practical advantages that previous offline VIS methods have not explored - handling long and high-resolution videos with a common GPU, and freezing a frame-level detector trained on image domain. Code is available at the link."}}
{"id": "m67FNFdgLO9", "cdate": 1652737272319, "mdate": null, "content": {"title": "Dense Interspecies Face Embedding", "abstract": "Dense Interspecies Face Embedding (DIFE) is a new direction for understanding faces of various animals by extracting common features among animal faces including human face. There are three main obstacles for interspecies face understanding: (1) lack of animal data compared to human, (2) ambiguous connection between faces of various animals, and (3) extreme shape and style variance. To cope with the lack of data, we utilize multi-teacher knowledge distillation of CSE and StyleGAN2 requiring no additional data or label. Then we synthesize pseudo pair images through the latent space exploration of StyleGAN2 to find implicit associations between different animal faces. Finally, we introduce the semantic matching loss to overcome the problem of extreme shape differences between species. To quantitatively evaluate our method over possible previous methodologies like unsupervised keypoint detection, we perform interspecies facial keypoint transfer on MAFL and AP-10K. Furthermore, the results of other applications like interspecies face image manipulation and dense keypoint transfer are provided. The code is available at https://github.com/kingsj0405/dife."}}
{"id": "s95BePNvykX", "cdate": 1621629772292, "mdate": null, "content": {"title": "Video Instance Segmentation using Inter-Frame Communication Transformers", "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. \nRecently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. \nHowever, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality.\nIn this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip.\nSpecifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene.\nThe features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.\nWe validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). \nOur method can also be applied to near-online inference for processing a video in real-time with only a small delay.\nThe code is available at https://github.com/sukjunhwang/IFC"}}
{"id": "pvjfA4wogD6", "cdate": 1621629772292, "mdate": null, "content": {"title": "Video Instance Segmentation using Inter-Frame Communication Transformers", "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. \nRecently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. \nHowever, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality.\nIn this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip.\nSpecifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene.\nThe features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.\nWe validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). \nOur method can also be applied to near-online inference for processing a video in real-time with only a small delay.\nThe code is available at https://github.com/sukjunhwang/IFC"}}
{"id": "UlocUlkgt_", "cdate": 1598841153363, "mdate": null, "content": {"title": "Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction", "abstract": "We propose a deep video prediction model conditioned on a single image and an action class. To generate future frames, we first detect keypoints of a moving object and predict future motion as a sequence of keypoints. The input image is then translated following the predicted keypoints sequence to compose future frames. Detecting the keypoints is central to our algorithm, and our method is trained to detect the keypoints of arbitrary objects in an unsupervised manner. Moreover, the detected keypoints of the original videos are used as pseudo-labels to learn the motion of objects. Experimental results show that our method is successfully applied to various datasets without the cost of labeling keypoints in videos. The detected keypoints are similar to human-annotated labels, and prediction results are more realistic compared to the previous methods."}}
