{"id": "xWpaXtz1W1p", "cdate": 1695211242693, "mdate": 1695211242693, "content": {"title": "Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics", "abstract": "Multi-modal large language models (MLLMs) are trained based on large language models (LLM), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. While they excel in multi-modal tasks, the pure NLP abilities of MLLMs are often underestimated and left untested. In this study, we get out of the box and unveil an intriguing characteristic of MLLMs -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. In releasing our code at this http URL, we aspire to foster further exploration into the intrinsic value of visual-text synergies and, in a broader scope, multi-modal interactions in alignment research."}}
{"id": "oQjWltREeRA", "cdate": 1663849805415, "mdate": null, "content": {"title": "Generalized Category Discovery via Adaptive GMMs without Knowing the Class Number", "abstract": "In this paper, we address the problem of generalized category discovery (GCD), \\ie, given a set of images where part of them are labelled and the rest are not, the task is to automatically cluster the images in the unlabelled data, leveraging the information from the labelled data, while the unlabelled data contain images from the labelled classes and also new ones. GCD is similar to semi-supervised learning (SSL) but is more realistic and challenging, as SSL assumes all the unlabelled images are from the same classes as the labelled ones. \nWe also do not assume the class number in the unlabelled data is known a-priori, making the GCD problem even harder. \nTo tackle the problem of GCD without knowing the class number, we propose an EM-like framework that alternates between representation learning and class number estimation. We propose a semi-supervised variant of the Gaussian Mixture Model (GMM) with a stochastic splitting and merging mechanism to dynamically determine the prototypes by examining the cluster compactness and separability. With these prototypes, we leverage prototypical contrastive learning for representation learning on the partially labelled data subject to the constraints imposed by the labelled data. Our framework alternates between these two steps until convergence. The cluster assignment for an unlabelled instance can then be retrieved by identifying its nearest prototype. We comprehensively evaluate our framework on both generic image classification datasets and challenging fine-grained object recognition datasets, achieving state-of-the-art performance. "}}
{"id": "Y11fVS4n8d9", "cdate": 1655045337330, "mdate": null, "content": {"title": "OOD-CV: A Benchmark for Robustness to Individual Nuisances in Real-World Out-of-Distribution Shifts", "abstract": "Enhancing the robustness of vision algorithms in real-world scenarios is challenging. One reason is that existing robustness benchmarks are limited, as they either rely on synthetic data or ignore the effects of individual nuisance factors. We introduce ROBIN, a benchmark dataset that includes out-of-distribution examples of 10 object categories in terms of pose, shape, texture, context and the weather conditions, and enables benchmarking models for image classification, object detection, and 3D pose estimation. Our experiments using popular baseline methods reveal that: 1) Some nuisance factors have a much stronger negative effect on the performance compared to others, also depending on the vision task. 2) Current approaches to enhance robustness have only marginal effects, and can even reduce robustness. 3) We do not observe significant differences between convolutional and transformer architectures. We believe our dataset provides a rich testbed to study robustness and will help push forward research in this area."}}
{"id": "H3JObxjd8S", "cdate": 1652737272121, "mdate": null, "content": {"title": "Self-Supervised Visual Representation Learning with Semantic Grouping", "abstract": "In this paper, we tackle the problem of learning visual representations from unlabeled scene-centric data. Existing works have demonstrated the potential of utilizing the underlying complex structure within scene-centric data; still, they commonly rely on hand-crafted objectness priors or specialized pretext tasks to build a learning framework, which may harm generalizability. Instead, we propose contrastive learning from data-driven semantic slots, namely SlotCon, for joint semantic grouping and representation learning. The semantic grouping is performed by assigning pixels to a set of learnable prototypes, which can adapt to each sample by attentive pooling over the feature and form new slots. Based on the learned data-dependent slots, a contrastive objective is employed for representation learning, which enhances the discriminability of features, and conversely facilitates grouping semantically coherent pixels together. Compared with previous efforts, by simultaneously optimizing the two coupled objectives of semantic grouping and contrastive learning, our approach bypasses the disadvantages of hand-crafted priors and is able to learn object/group-level representations from scene-centric images. Experiments show our approach effectively decomposes complex scenes into semantic groups for feature learning and significantly benefits downstream tasks, including object detection, instance segmentation, and semantic segmentation. Code is available at: https://github.com/CVMI-Lab/SlotCon."}}
{"id": "hOo2wiboPaz", "cdate": 1640995200000, "mdate": 1648691368469, "content": {"title": "Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective", "abstract": "This work simultaneously considers the discriminability and transferability properties of deep representations in the typical supervised learning task, i.e., image classification. By a comprehensive temporal analysis, we observe a trade-off between these two properties. The discriminability keeps increasing with the training progressing while the transferability intensely diminishes in the later training period. From the perspective of information-bottleneck theory, we reveal that the incompatibility between discriminability and transferability is attributed to the over-compression of input information. More importantly, we investigate why and how the InfoNCE loss can alleviate the over-compression, and further present a learning framework, named contrastive temporal coding~(CTC), to counteract the over-compression and alleviate the incompatibility. Extensive experiments validate that CTC successfully mitigates the incompatibility, yielding discriminative and transferable representations. Noticeable improvements are achieved on the image classification task and challenging transfer learning tasks. We hope that this work will raise the significance of the transferability property in the conventional supervised learning setting. Code is available at https://github.com/DTennant/dt-tradeoff."}}
{"id": "DSEAns80vi", "cdate": 1623376669907, "mdate": null, "content": {"title": "Temporal Context Aggregation for Video Retrieval with Contrastive Learning", "abstract": "The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly\nprocess the frames of a video as individual images or short\nclips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video\nrepresentation learning framework that incorporates longrange temporal information between frame-level features\nusing the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase\nthe capacity of negative samples. Extensive experiments\nare conducted on multiple video retrieval tasks, such as\nCC WEB VIDEO, FIVR-200K, and EVVE. The proposed\nmethod shows a significant performance advantage (\u223c 17%\nmAP on FIVR-200K) over state-of-the-art methods with\nvideo-level features, and deliver competitive results with\n22x faster inference time comparing with frame-level features."}}
{"id": "jpNAc_Z0gKr", "cdate": 1623376566933, "mdate": null, "content": {"title": "Reducing the feature divergence of RGB and near-infrared images using Switchable Normalization", "abstract": "Visual pattern recognition over agricultural areas is an important application of aerial image processing. In this paper, we consider the multi-modality nature of agricultural aerial images and show that naively combining different modalities together without taking the feature divergence into account can lead to sub-optimal results. Thus, we apply a Switchable Normalization block to our DeepLabV3 segmentation model to alleviate the feature divergence. Using the popular symmetric Kullback Leibler divergence measure, we show that our model can greatly reduce the divergence between RGB and near-infrared channels. Together with a hybrid loss function, our model achieves nearly 10\\% improvements in mean IoU over previously published baseline."}}
{"id": "YJHXfcTDaqw", "cdate": 1623152237201, "mdate": null, "content": {"title": "Rail-5k: a Real-World Dataset for Rail Surface Defects Detection", "abstract": "This paper presents the Rail-5k dataset for benchmarking the performance of visual algorithms in a real-world application scenario, namely the rail surface defect detection task.\nWe collected over 5k high-quality images from railways across China and annotated 1100 images with the help of railway experts to identify the most common 13 types of railway defects.\nThe dataset can be used for two settings both with unique challenges, the first is the fully-supervised setting using the 1k labeled images for training, fine-grained nature and long-tailed distribution of defect classes make it hard for visual algorithms to tackle.\nThe second is the semi-supervised learning setting facilitated by the 4k unlabeled images, these 4k images are uncurated containing possible image corruptions and domain shift with the labeled images, which can not be easily tackled by previous semi-supervised learning methods.\nWe believe our dataset could be a valuable benchmark for evaluating the robustness and reliability of visual algorithms."}}
{"id": "xWq1MVj7YrE", "cdate": 1621629689472, "mdate": null, "content": {"title": "Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation", "abstract": "In this paper, we tackle the problem of novel visual category discovery, i.e., grouping unlabelled images from new classes into different semantic partitions by leveraging a labelled dataset that contains images from other different but relevant categories.  This is a more realistic and challenging setting than conventional semi-supervised learning. We propose a two-branch learning framework for this problem, with one branch focusing on local part-level information and the other branch focusing on overall characteristics. To transfer knowledge from the labelled data to the unlabelled, we propose using dual ranking statistics on both branches to generate pseudo labels for training on the unlabelled data. We further introduce a mutual knowledge distillation method to allow information exchange and encourage agreement between the two branches for discovering new categories, allowing our model to enjoy the benefits of global and local features. We comprehensively evaluate our method on public benchmarks for generic object classification, as well as the more challenging datasets for fine-grained visual recognition, achieving state-of-the-art performance."}}
{"id": "lKrHK6GOUqt", "cdate": 1609459200000, "mdate": 1648691368505, "content": {"title": "Temporal Context Aggregation for Video Retrieval with Contrastive Learning", "abstract": "The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly process the frames of a video as individual images or short clips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that incorporates longrange temporal information between frame-level features using the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase the capacity of negative samples. Extensive experiments are conducted on multiple video retrieval tasks, such as CC WEB VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant performance advantage (\u223c 17% mAP on FIVR-200K) over state-of-the-art methods with video-level features, and deliver competitive results with 22x faster inference time comparing with frame-level features."}}
