{"id": "CmUWQ9s2D_", "cdate": 1685532016388, "mdate": null, "content": {"title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory", "abstract": "The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased errors over long horizons. We approach this problem from the lens of Koopman theory, where the nonlinear dynamics of the environment can be linearized in a high-dimensional latent space. This allows us to efficiently parallelize the sequential problem of long-range prediction using convolution while accounting for the agent\u2019s action at every time step. Our approach also enables stability analysis and better control over gradients through time. Taken together, these advantages result in significant improvement over the existing approaches, both in the efficiency and the accuracy of modeling dynamics over extended horizons. We also show that this model can be easily incorporated into dynamics modeling for model-based planning and model-free RL and report promising experimental results."}}
{"id": "Mrz9PgP3sT", "cdate": 1677713819302, "mdate": null, "content": {"title": "Hyperbolic Deep Reinforcement Learning for Continuous Control", "abstract": "Integrating hyperbolic representations with Deep Reinforcement Learning (DRL) has recently been proposed as a promising approach for enhancing generalization and sample-efficiency in discrete control tasks. In this work, we extend hyperbolic RL to continuous control by introducing a novel hyperbolic actor-critic model. Empirically, our simple implementation outperforms its Euclidean counterpart, with significant gains on 16/24 tasks from the DeepMind Control Suite with pixel inputs. Notably, in the low-data regime, our method even outperforms several pre-trained unsupervised RL agents. Our findings show that hyperbolic representations provide a valuable inductive bias for continuous control."}}
{"id": "pVD1k8ge25a", "cdate": 1664194183875, "mdate": null, "content": {"title": "Equivariance With Learned Canonicalization Functions", "abstract": "Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce a canonical representation of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for many groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis is that learning a neural network to perform canonicalization is better than doing it using predefined heuristics. Our results show that learning the canonicalization function indeed leads to better results and that the approach achieves great performance in practice."}}
{"id": "vWUmBjin_-o", "cdate": 1652737853527, "mdate": null, "content": {"title": "Structuring Representations Using Group Invariants", "abstract": "A finite set of invariants can identify many interesting transformation groups. For example, distances, inner products and angles are preserved by Euclidean, Orthogonal and Conformal transformations, respectively. In an equivariant representation, the group invariants should remain constant on the embedding as we transform the input. This gives a procedure for learning equivariant representations without knowing the possibly nonlinear action of the group in the input space. Rather than enforcing such hard invariance constraints on the latent space, we show how to use invariants for \"symmetry regularization\" of the latent, while guaranteeing equivariance through other means. We also show the feasibility of learning disentangled representations using this approach and provide favorable qualitative and quantitative results on downstream tasks, including world modeling and reinforcement learning."}}
{"id": "ii9X4vtZGTZ", "cdate": 1652737597482, "mdate": null, "content": {"title": "$\\alpha$-ReQ : Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay", "abstract": "Self-Supervised Learning (SSL) with large-scale unlabelled datasets enables learning useful representations for multiple downstream tasks. However, assessing the quality of such representations efficiently poses nontrivial challenges. Existing approaches train linear probes (with frozen features) to evaluate performance on a given task. This is expensive both computationally, since it requires retraining a new prediction head for each downstream task, and statistically, requires task-specific labels for multiple tasks. This poses a natural question, how do we efficiently determine the \"goodness\" of representations learned with SSL across a wide range of potential downstream tasks? In particular, a task-agnostic statistical measure of representation quality, that predicts generalization without explicit downstream task evaluation, would be highly desirable. \n   \nIn this work, we analyze characteristics of learned representations $\\mathbf{f_\\theta}$, in well-trained neural networks with canonical architectures \\& across SSL objectives. We observe that the eigenspectrum of the empirical feature covariance $\\mathrm{Cov}(\\mathbf{f_\\theta}$) can be well approximated with the family of power-law distribution. We analytically and empirically (using multiple datasets, e.g. CIFAR, STL10, MIT67, ImageNet) demonstrate that the decay coefficient $\\alpha$ serves as a measure of representation quality for tasks that are solvable with a linear readout, i.e. there exist well-defined intervals for $\\alpha$ where models exhibit excellent downstream generalization. Furthermore, our experiments suggest that key design parameters in SSL algorithms, such as BarlowTwins, implicitly modulate the decay coefficient of the eigenspectrum ($\\alpha$). As $\\alpha$ depends only on the features themselves, this measure for model selection with hyperparameter tuning for BarlowTwins enables search with less compute."}}
{"id": "WeLwN3OU5IS", "cdate": 1639069751484, "mdate": null, "content": {"title": "Generative Clearing for Deep Tissue Imaging", "abstract": "Investigating cellular level tissue architecture requires the imaging of intact biological samples from 3D volumes with high-resolution imaging methods, including confocal microscopy. A significant challenge for the quantitative analysis of such volumetric data using computer vision is the degradation of image quality at increased depths due to light scattering, absorption and optical factors. We here introduce a generative cycle consistent adversarial network (Cycle-GAN) to mitigate these effects, which exploits the property that since the tissue is self-similar, the appearance of the shallow layers can serve as a proxy for that of depth degradation-free data. The network model obtains a bi-directional mapping between the shallow and deep layers so that the restored deep layers resemble the shallow ones. We demonstrate this approach's utility on a dataset of images obtained by confocal imaging of thick cardiac tissue sections from the mouse. Our experiments show that the restored deeper layers are qualitatively and quantitatively similar to the shallow ones, that the restored tissue images are amenable to geometric analysis and that in general such an approach outperforms other methods both qualitatively, as well as quantitatively."}}
{"id": "4JlwgTbmzXQ", "cdate": 1632875758227, "mdate": null, "content": {"title": "EqR: Equivariant Representations for Data-Efficient Reinforcement Learning", "abstract": "We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agent\u2019s action, and symmetry transformations of the state-action pairs. Whereas prior work on exploiting symmetries in deep RL can only incorporate predefined linear transformations, our approach allows for non-linear symmetry transformations of state-action pairs to be learned from the data itself. This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses. We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment. Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games."}}
