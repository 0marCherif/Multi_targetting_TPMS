{"id": "AeDI0CZ8Wa", "cdate": 1671916755721, "mdate": 1671916755721, "content": {"title": "Domain-Agnostic Constrastive Representations for Learning from Label Proportions", "abstract": "We study the weak supervision learning problem of Learning from\nLabel Proportions (LLP) where the goal is to learn an instance-level\nclassifier using proportions of various class labels in a bag \u2013 a collection of input instances that often can be highly correlated. While\nrepresentation learning for weakly-supervised tasks is found to\nbe effective, they often require domain knowledge. To the best of\nour knowledge, representation learning for tabular data (unstructured data containing both continuous and categorical features)\nare not studied. In this paper, we propose to learn diverse representations of instances within the same bags to effectively utilize\nthe weak bag-level supervision. We propose a domain agnostic LLP\nmethod, called \"Self Contrastive Representation Learning for LLP\"\n(SelfCLR-LLP) that incorporates a novel self\u2013contrastive function\nas an auxiliary loss to learn representations on tabular data for LLP.\nWe show that diverse representations for instances within the same\nbags aid efficient usage of the weak bag-level LLP supervision. We\nevaluate the proposed method through extensive experiments on\nreal-world LLP datasets from e-commerce applications to demonstrate the effectiveness of our proposed SelfCLR-LLP."}}
{"id": "9_VrvV7d-FK", "cdate": 1663850395221, "mdate": null, "content": {"title": "Unsupervised Adaptation for Fairness under Covariate Shift", "abstract": "Training fair models typically involves optimizing a composite objective accounting for both prediction accuracy and some fairness measure. However, due to a shift in the distribution of the covariates at test time, the learnt fairness tradeoffs may no longer be valid, which we verify experimentally. To address this, we consider an unsupervised adaptation problem of training fair classifiers when only a small set of unlabeled test samples is available along with a large labeled training set. We propose a novel modification to the traditional composite objective by adding a weighted entropy objective on the unlabeled test dataset. This involves a min-max optimization where weights are optimized to mimic the importance weighting ratios followed by classifier optimization. We demonstrate that our weighted entropy objective provides an upper bound on the standard importance sampled training objective common in covariate shift formulations under some mild conditions. Experimentally, we demonstrate that Wasserstein distance based penalty for representation matching across protected sub groups together with the above loss outperforms existing baselines. Our method achieves the best accuracy-equalized odds tradeoff under the covariate shift setup. We find that, for the same accuracy, we get upto 2x improvement in equalized odds on notable benchmarks."}}
{"id": "idtsGjGjh4r", "cdate": 1662928260035, "mdate": 1662928260035, "content": {"title": "Multi-Variate Time Series Forecasting on Variable Subsets", "abstract": "We formulate a new inference task in the domain of multivariate time series forecasting (MTSF), called Variable Subset Forecast\n(VSF), where only a small subset of the variables is available during\ninference. Variables are absent during inference because of longterm data loss (eg. sensor failures) or high\u2192low-resource domain\nshift between train / test. To the best of our knowledge, robustness\nof MTSF models in presence of such failures, has not been studied\nin the literature. Through extensive evaluation, we first show that\nthe performance of state of the art methods degrade significantly in\nthe VSF setting. We propose a non-parametric, wrapper technique\nthat can be applied on top any existing forecast models. Through\nsystematic experiments across 4 datasets and 5 forecast models,\nwe show that our technique is able to recover close to 95% performance of the models even when only 15% of the original variables\nare present."}}
{"id": "A6rEbQnZOod", "cdate": 1655784601313, "mdate": 1655784601313, "content": {"title": "BERTops: Studying BERT Representations under a Topological Lens", "abstract": "Proposing scoring functions to effectively understand, analyze and learn various properties of high dimensional hidden representations of large-scale transformer models like BERT can be a challenging task. In this work, we explore a new direction by studying the topological features of BERT hidden representations using persistent homology (PH). We propose a novel scoring function named \"persistence scoring function (PSF)\" which: (i) accurately captures the homology of the high-dimensional hidden representations and correlates well with the test set accuracy of a wide range of datasets and outperforms existing scoring metrics, (ii) captures interesting post fine-tuning \"per-class\" level properties from both qualitative and quantitative viewpoints, (iii) is more stable to perturbations as compared to the baseline functions, which makes it a very robust proxy, and (iv) finally, also serves as a predictor of the attack success rates for a wide category of black-box and white-box adversarial attack methods. Our extensive correlation experiments demonstrate the practical utility of PSF on various NLP tasks relevant to BERT."}}
{"id": "TAGPUF_yiQF", "cdate": 1648786288856, "mdate": 1648786288856, "content": {"title": "Target Model Agnostic Adversarial Attacks with Query Budgets on Language Understanding Models", "abstract": "Despite significant improvements in natural\nlanguage understanding models with the advent of models like BERT and XLNet, these\nneural-network based classifiers are vulnerable to blackbox adversarial attacks, where the\nattacker is only allowed to query the target\nmodel outputs. We add two more realistic restrictions on the attack methods, namely limiting the number of queries allowed (query\nbudget) and crafting attacks that easily transfer across different pre-trained models (transferability), which render previous attack models impractical and ineffective. Here, we propose a target model agnostic adversarial attack\nmethod with a high degree of attack transferability across the attacked models. Our empirical studies show that in comparison to baseline\nmethods, our method generates highly transferable adversarial sentences under the restriction\nof limited query budgets."}}
{"id": "-MWyiLQuv51", "cdate": 1648786178617, "mdate": 1648786178617, "content": {"title": "A Probabilistic Framework for Knowledge Graph Data Augmentation", "abstract": "We present NNMFAug, a probabilistic framework to perform data augmentation\nfor the task of knowledge graph completion to counter the problem of data scarcity,\nwhich can enhance the learning process of neural link predictors. Our method\ncan generate potentially diverse triples with the advantage of being efficient and\nscalable as well as agnostic to the choice of the link prediction model and dataset\nused. Experiments and analysis done on popular models and benchmarks show\nthat NNMFAug can bring notable improvements over the baselines."}}
{"id": "2_XwDnOPZZt", "cdate": 1596114251144, "mdate": null, "content": {"title": "Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs", "abstract": "The recent proliferation of knowledge graphs\n(KGs) coupled with incomplete or partial information, in the form of missing relations\n(links) between entities, has fueled a lot of\nresearch on knowledge base completion (also\nknown as relation prediction). Several recent works suggest that convolutional neural\nnetwork (CNN) based models generate richer\nand more expressive feature embeddings and\nhence also perform well on relation prediction.\nHowever, we observe that these KG embeddings treat triples independently and thus fail\nto cover the complex and hidden information\nthat is inherently implicit in the local neighborhood surrounding a triple. To this effect, our\npaper proposes a novel attention-based feature\nembedding that captures both entity and relation features in any given entity\u2019s neighborhood. Additionally, we also encapsulate relation clusters and multi-hop relations in our\nmodel. Our empirical study offers insights\ninto the efficacy of our attention-based model\nand we show marked performance gains in\ncomparison to state-of-the-art methods on all\ndatasets."}}
{"id": "WLUFW66tnrY", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Representations using Spectral-Biased Random Walks on Graphs", "abstract": "Several state-of-the-art neural graph embedding methods are based on short random walks (stochastic processes) because of their ease of computation, simplicity in capturing complex local graph properties, scalability, and interpretibility. In this work, we are interested in studying how much a probabilistic bias in this stochastic process affects the quality of the nodes picked by the process. In particular, our biased walk, with a certain probability, favors movement towards nodes whose neighborhoods bear a structural resemblance to the current node's neighborhood. We succinctly capture this neighborhood as a probability measure based on the spectrum of the node's neighborhood subgraph represented as a normalized laplacian matrix. We propose the use of a paragraph vector model with a novel Wasserstein regularization term. We empirically evaluate our approach against several state-of-the-art node embedding techniques on a wide variety of real-world datasets and demonstrate that our proposed method significantly improves upon existing methods on both link prediction and node classification tasks."}}
{"id": "Bkeeca4Kvr", "cdate": 1569439111765, "mdate": null, "content": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios."}}
