{"id": "1hMhmO03wz", "cdate": 1672531200000, "mdate": 1683907410005, "content": {"title": "Visual Chain-of-Thought Diffusion Models", "abstract": "Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation."}}
{"id": "0RTJcuvHtIu", "cdate": 1652737679203, "mdate": null, "content": {"title": "Flexible Diffusion Modeling of Long Videos", "abstract": "We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames.  We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length.  We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator."}}
{"id": "z9A0L7_iz-x", "cdate": 1640995200000, "mdate": 1683879080201, "content": {"title": "Flexible Diffusion Modeling of Long Videos", "abstract": "We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator."}}
{"id": "p2qfKRh3Gc", "cdate": 1640995200000, "mdate": 1681494432331, "content": {"title": "Attention for Inference Compilation", "abstract": ""}}
{"id": "oi5ksleHIW", "cdate": 1640995200000, "mdate": 1681494432330, "content": {"title": "Near-Optimal Glimpse Sequences for Improved Hard Attention Neural Network Training", "abstract": ""}}
{"id": "Z7ImF39vhqT", "cdate": 1640995200000, "mdate": 1681494432331, "content": {"title": "Conditional Image Generation by Conditioning Variational Auto-Encoders", "abstract": ""}}
{"id": "F06sL6vSWg", "cdate": 1640995200000, "mdate": 1681494432570, "content": {"title": "Graphically Structured Diffusion Models", "abstract": ""}}
{"id": "7MV6uLzOChW", "cdate": 1632875546706, "mdate": null, "content": {"title": "Conditional Image Generation by Conditioning Variational Auto-Encoders", "abstract": "We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE. To train the conditional VAE, we only need to train an artifact to perform amortized inference over the unconditional VAE's latent variables given a conditioning input. We demonstrate our approach on tasks including image inpainting, for which it outperforms state-of-the-art GAN-based approaches at faithfully representing the inherent uncertainty. We conclude by describing a possible application of our inpainting model, in which it is used to perform Bayesian experimental design for the purpose of guiding a sensor."}}
{"id": "m4eWdSYzQLp", "cdate": 1609459200000, "mdate": null, "content": {"title": "Image Completion via Inference in Deep Generative Models", "abstract": "We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE. To train the conditional VAE, we only need to train an artifact to perform amortized inference over the unconditional VAE's latent variables given a conditioning input. We demonstrate our approach on tasks including image inpainting, for which it outperforms state-of-the-art GAN-based approaches at faithfully representing the inherent uncertainty. We conclude by describing a possible application of our inpainting model, in which it is used to perform Bayesian experimental design for the purpose of guiding a sensor."}}
{"id": "3Ql3DgBnyj", "cdate": 1609459200000, "mdate": 1681494432404, "content": {"title": "Assisting the Adversary to Improve GAN Training", "abstract": ""}}
