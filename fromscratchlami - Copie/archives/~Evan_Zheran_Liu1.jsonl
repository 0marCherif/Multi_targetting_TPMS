{"id": "ixjQB2SmVXA", "cdate": 1653752160733, "mdate": null, "content": {"title": "Giving Feedback on Interactive Student Programs with Meta-Exploration", "abstract": "Creating interactive software, such as websites or games, is a particularly engaging way to learn computer science. However, teaching and giving feedback on such software is hard \u2014 standard approaches require instructors to hand grade student-implemented interactive programs. As a result, online platforms that serve millions, like Code.org, are unable to provide any feedback on assignments for implementing interactive programs, which critically hinders students\u2019 ability to learn. Recent work proposes to train reinforcement learning agents to interact with a student\u2019s program, aiming to explore states indicative of errors. However, this approach only provides binary feedback of whether a program is correct or not, while students require finer-grained feedback on the specific errors in their programs to understand their mistakes. In this work, we show that exploring to discover errors can be cast as a meta-exploration problem. This enables us to construct a principled objective for discovering errors and an algorithm for optimizing this objective, which provides fine-grained feedback. We evaluate our approach on a set of 700K real anonymized student programs from a Code.org interactive assignment. Our approach provides feedback with 94.3% accuracy, improving over existing approaches by over 17.7% and coming within 1.5% of human-level accuracy."}}
{"id": "_AsEqoBu3s", "cdate": 1652737662108, "mdate": null, "content": {"title": "Giving Feedback on Interactive Student Programs with Meta-Exploration", "abstract": "Developing interactive software, such as websites or games, is a particularly engaging way to learn computer science. However, teaching and giving feedback on such software is time-consuming \u2014 standard approaches require instructors to manually grade student-implemented interactive programs. As a result, online platforms that serve millions, like Code.org, are unable to provide any feedback on assignments for implementing interactive programs, which critically hinders students\u2019 ability to learn. One approach toward automatic grading is to learn an agent that interacts with a student\u2019s program and explores states indicative of errors via reinforcement learning. However, existing work on this approach only provides binary feedback of whether a program is correct or not, while students require finer-grained feedback on the specific errors in their programs to understand their mistakes. In this work, we show that exploring to discover errors can be cast as a meta-exploration problem. This enables us to construct a principled objective for discovering errors and an algorithm for optimizing this objective, which provides fine-grained feedback. We evaluate our approach on a set of over 700K real anonymized student programs from a Code.org interactive assignment. Our approach provides feedback with 94.3% accuracy, improving over existing approaches by 17.7% and coming within 1.5% of human-level accuracy. Project web page: https://ezliu.github.io/dreamgrader."}}
{"id": "D4fuQ1MveDM", "cdate": 1652737600720, "mdate": null, "content": {"title": "Learning Options via Compression", "abstract": "Identifying statistical regularities in solutions to some tasks in multi-task reinforcement learning can accelerate the learning of new tasks.\nSkill learning offers one way of identifying these regularities by decomposing pre-collected experiences into a sequence of skills.\nA popular approach to skill learning is maximizing the likelihood of the pre-collected experience with latent variable models,\nwhere the latent variables represent the skills. However, there are often many solutions that maximize the likelihood equally well, including degenerate solutions. To address this underspecification, we propose a new objective that combines the maximum likelihood objective with a penalty on the description length of the skills. This penalty incentivizes the skills to maximally extract common structures from the experiences. Empirically, our objective learns skills that solve downstream tasks in fewer samples compared to skills learned from only maximizing likelihood. Further, while most prior works in the offline multi-task setting focus on tasks with low-dimensional observations, our objective can scale to challenging tasks with high-dimensional image observations."}}
{"id": "rSwTMomgCz", "cdate": 1601308312948, "mdate": null, "content": {"title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices", "abstract": "The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration's utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation."}}
{"id": "La1QuucFt8-", "cdate": 1591922549531, "mdate": null, "content": {"title": "Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning", "abstract": "We seek to efficiently learn by leveraging shared structure between different tasks and environments. For example, cooking is similar in different kitchens, even though the ingredients may change location. In principle, meta-reinforcement learning approaches can exploit this shared structure, but in practice, they fail to adapt to new environments when adaptation requires targeted exploration (e.g., exploring the cabinets to find ingredients in a new kitchen). We show that existing approaches fail due to a chicken-and-egg problem: learning what to explore requires knowing what information is critical for solving the task, but learning to solve the task requires already gathering this information via exploration. For example, exploring to find the ingredients only helps a robot prepare a meal if it already knows how to cook, but the robot can only learn to cook if it already knows where the ingredients are. To address this, we propose a new exploration objective (DREAM), based on identifying key information in the environment, independent of this information will exactly be used solve the task. By decoupling exploration from task execution, DREAM explores and consequently adapts to new environments requiring no reward signal when the task is specified via an instruction. Empirically, DREAM scales to more complex problems, such as sparse-reward 3D visual navigation, while existing approaches fail from insufficient exploration."}}
{"id": "cjV10IYMZ46r", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Abstract Models for Strategic Exploration and Fast Reward Transfer", "abstract": "Model-based reinforcement learning (RL) is appealing because (i) it enables planning and thus more strategic exploration, and (ii) by decoupling dynamics from rewards, it enables fast transfer to new reward functions. However, learning an accurate Markov Decision Process (MDP) over high-dimensional states (e.g., raw pixels) is extremely challenging because it requires function approximation, which leads to compounding errors. Instead, to avoid compounding errors, we propose learning an abstract MDP over abstract states: low-dimensional coarse representations of the state (e.g., capturing agent position, ignoring other objects). We assume access to an abstraction function that maps the concrete states to abstract states. In our approach, we construct an abstract MDP, which grows through strategic exploration via planning. Similar to hierarchical RL approaches, the abstract actions of the abstract MDP are backed by learned subpolicies that navigate between abstract states. Our approach achieves strong results on three of the hardest Arcade Learning Environment games (Montezuma's Revenge, Pitfall!, and Private Eye), including superhuman performance on Pitfall! without demonstrations. After training on one task, we can reuse the learned abstract MDP for new reward functions, achieving higher reward in 1000x fewer samples than model-free methods trained from scratch."}}
{"id": "Yo511PahRu3", "cdate": 1577836800000, "mdate": null, "content": {"title": "An Imitation Learning Approach for Cache Replacement", "abstract": "Program execution speed critically depends on increasing cache hits, as cache hits are orders of magnitude faster than misses. To increase cache hits, we focus on the problem of cache replacement: choosing which cache line to evict upon inserting a new line. This is challenging because it requires planning far ahead and currently there is no known practical solution. As a result, current replacement policies typically resort to heuristics designed for specific common access patterns, which fail on more diverse and complex access patterns. In contrast, we propose an imitation learning approach to automatically learn cache access patterns by leveraging Belady's, an oracle policy that computes the optimal eviction decision given the future cache accesses. While directly applying Belady's is infeasible since the future is unknown, we train a policy conditioned only on past accesses that accurately approximates Belady's even on diverse and complex access patterns, and call this approach Parrot. When evaluated on 13 of the most memory-intensive SPEC applications, Parrot increases cache miss rates by 20% over the current state of the art. In addition, on a large-scale web search benchmark, Parrot increases cache hit rates by 61% over a conventional LRU policy. We release a Gym environment to facilitate research in this area, as data is plentiful, and further advancements can have significant real-world impact."}}
{"id": "ryxLG2RcYX", "cdate": 1538087950272, "mdate": null, "content": {"title": "Learning Abstract Models for Long-Horizon Exploration", "abstract": "In high-dimensional reinforcement learning settings with sparse rewards, performing\neffective exploration to even obtain any reward signal is an open challenge.\nWhile model-based approaches hold promise of better exploration via planning, it\nis extremely difficult to learn a reliable enough Markov Decision Process (MDP)\nin high dimensions (e.g., over 10^100 states). In this paper, we propose learning\nan abstract MDP over a much smaller number of states (e.g., 10^5), which we can\nplan over for effective exploration. We assume we have an abstraction function\nthat maps concrete states (e.g., raw pixels) to abstract states (e.g., agent position,\nignoring other objects). In our approach, a manager maintains an abstract\nMDP over a subset of the abstract states, which grows monotonically through targeted\nexploration (possible due to the abstract MDP). Concurrently, we learn a\nworker policy to travel between abstract states; the worker deals with the messiness\nof concrete states and presents a clean abstraction to the manager. On three of\nthe hardest games from the Arcade Learning Environment (Montezuma's,\nPitfall!, and Private Eye), our approach outperforms the previous\nstate-of-the-art by over a factor of 2 in each game. In Pitfall!, our approach is\nthe first to achieve superhuman performance without demonstrations."}}
{"id": "ryTp3f-0-", "cdate": 1518730159714, "mdate": null, "content": {"title": "Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration", "abstract": "Reinforcement learning (RL) agents improve through trial-and-error, but when reward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying to emails, where a single mistake can ruin the entire sequence of actions. A common remedy is to \"warm-start\" the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level \"workflows\" which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., \"Step 1: click on a textbox; Step 2: enter some text\"). Our exploration policy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent\u2019s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x."}}
{"id": "HkNEVMMd-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Mapping natural language commands to web elements", "abstract": "The web provides a rich, open-domain environment with textual, structural, and spatial properties. We propose a new task for grounding language in this environment: given a natural language command (e.g., \"click on the second article\"), choose the correct element on the web page (e.g., a hyperlink or text box). We collected a dataset of over 50,000 commands that capture various phenomena such as functional references (e.g. \"find who made this site\"), relational reasoning (e.g. \"article by john\"), and visual reasoning (e.g. \"top-most article\"). We also implemented and analyzed three baseline models that capture different phenomena present in the dataset."}}
