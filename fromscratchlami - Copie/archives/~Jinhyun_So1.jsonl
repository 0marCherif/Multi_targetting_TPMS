{"id": "J8_l2570_T", "cdate": 1672531200000, "mdate": 1695964093800, "content": {"title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning", "abstract": "Secure aggregation is a critical component in federated learning (FL), which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL lead to leaking users' individual models within number of rounds that is linear in the number of users. To address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over any number of training rounds). Our framework also carefully accounts for the fairness and the average number of participating users at each round. Our experiments on MNIST, CIFAR-10 and CIFAR-100 datasets in the IID and the non-IID settings demonstrate the performance improvement over the baselines, both in terms of privacy protection and test accuracy."}}
{"id": "WA7I-Fm4tmP", "cdate": 1663939406897, "mdate": null, "content": {"title": "LightVeriFL: Lightweight and Verifiable Secure Federated Learning", "abstract": "Secure aggregation protocols are implemented in federated learning to protect the local models of the participating users so that the server does not obtain any information beyond the aggregate model at each iteration. However, existing secure aggregation schemes fail to protect the integrity, i.e., correctness, of the aggregate model in the possible presence of a malicious server forging the aggregation result, which motivates the need for verifiable aggregation in federated learning. Existing verifiable aggregation schemes either have a complexity that linearly grows with the model size or require time-consuming reconstruction at the server, that is quadratic in the number of users, in case of likely user dropouts. To overcome these limitations, we propose {\\texttt{LightVeriFL}}, a lightweight and communication-efficient secure verifiable aggregation protocol, that provides the same \nguarantees for verifiability against a malicious server, data privacy, and dropout-resilience as the state-of-the-art protocols without incurring substantial communication and computation overheads. The proposed \\texttt{LightVeriFL} protocol utilizes homomorphic hash and commitment functions of constant length, that are independent of the model size, to enable verification at the users. In case of dropouts, \\texttt{LightVeriFL} uses a one-shot aggregate hash recovery of the dropped users, instead of a one-by-one recovery based on secret sharing, making the verification process significantly faster than the existing approaches. We evaluate \\texttt{LightVeriFL} through experiments and show that it significantly lowers the total verification time in practical settings. "}}
{"id": "nVV6S2sb_UL", "cdate": 1652737780322, "mdate": null, "content": {"title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning", "abstract": "Secure aggregation is a critical component in federated learning, which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL may lead to leaking users' individual models within a number of rounds that is linear in the number of users. To address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over any number of training rounds). Our framework also carefully accounts for the fairness and the average number of participating users at each round. Our experiments on MNIST, CIFAR-$10$ and CIFAR-$100$ datasets in the IID and the non-IID settings demonstrate the performance improvement over the baselines, both in terms of privacy protection and test accuracy."}}
{"id": "tD6adEZGy2V", "cdate": 1649486620484, "mdate": 1649486620484, "content": {"title": "LightSecAgg: a Lightweight and Versatile Design for Secure Aggregation in Federated Learning", "abstract": "Secure model aggregation is a key component of federated learning (FL) that aims at protecting the privacy of each user's individual model while allowing for their global aggregation. It can be applied to any aggregation-based FL approach for training a global or personalized model. Model aggregation needs to also be resilient against likely user dropouts in FL systems, making its design substantially more complex. State-of-the-art secure aggregation protocols rely on secret sharing of the random-seeds used for mask generations at the users to enable the reconstruction and cancellation of those belonging to the dropped users. The complexity of such approaches, however, grows substantially with the number of dropped users. We propose a new approach, named LightSecAgg, to overcome this bottleneck by changing the design from \"random-seed reconstruction of the dropped users\" to \"one-shot aggregate-mask reconstruction of the active users via mask encoding/decoding\". We show that LightSecAgg achieves the same privacy and dropout-resiliency guarantees as the state-of-the-art protocols while significantly reducing the overhead for resiliency against dropped users. We also demonstrate that, unlike existing schemes, LightSecAgg can be applied to secure aggregation in the asynchronous FL setting. Furthermore, we provide a modular system design and optimized on-device parallelization for scalable implementation, by enabling computational overlapping between model training and on-device encoding, as well as improving the speed of concurrent receiving and sending of chunked masks. We evaluate LightSecAgg via extensive experiments for training diverse models on various datasets in a realistic FL system with large number of users and demonstrate that LightSecAgg significantly reduces the total training time.\n\n"}}
{"id": "NzRav-j6Tr", "cdate": 1640995200000, "mdate": 1681652131180, "content": {"title": "LightSecAgg: a Lightweight and Versatile Design for Secure Aggregation in Federated Learning", "abstract": ""}}
{"id": "H5WAgpmfXq", "cdate": 1640995200000, "mdate": 1648667894197, "content": {"title": "FedSpace: An Efficient Federated Learning Framework at Satellites and Ground Stations", "abstract": "Large-scale deployments of low Earth orbit (LEO) satellites collect massive amount of Earth imageries and sensor data, which can empower machine learning (ML) to address global challenges such as real-time disaster navigation and mitigation. However, it is often infeasible to download all the high-resolution images and train these ML models on the ground because of limited downlink bandwidth, sparse connectivity, and regularization constraints on the imagery resolution. To address these challenges, we leverage Federated Learning (FL), where ground stations and satellites collaboratively train a global ML model without sharing the captured images on the satellites. We show fundamental challenges in applying existing FL algorithms among satellites and ground stations, and we formulate an optimization problem which captures a unique trade-off between staleness and idleness. We propose a novel FL framework, named FedSpace, which dynamically schedules model aggregation based on the deterministic and time-varying connectivity according to satellite orbits. Extensive numerical evaluations based on real-world satellite images and satellite networks show that FedSpace reduces the training time by 1.7 days (38.6%) over the state-of-the-art FL algorithms."}}
{"id": "pKKho3FLys", "cdate": 1609459200000, "mdate": 1695964093996, "content": {"title": "Buffer-referred Data Prefetching: An Effective Approach to Coverage-Driven Prefetching", "abstract": "This study about data prefetching focuses on maximizing the performance of modern processors by hiding cache misses. This paper suggests that improving prefetch coverage is an effective approach to achieve the goal. This work proposes to employ two simple buffers, block offset buffer and block address buffer, that leverage prefetch coverage. The offset buffer stores the offset of cache blocks that are accessed recently, while the block address buffer stores the address of cache blocks that are prefetch-issued lately. For the offset buffer, we propose to adopt multiple lengths of delta history in searching offset patterns, compared to using a single length of delta history in global history buffer method. Each offset buffer is used to find a local optimum offset of a page. Besides, all the offset buffers are employed to refer to each other, mining offset patterns from others. Our second proposed buffer, the block address buffer, is hired for aggressive prefetching that can switch to adaptive burst mode occurred in the case of successive accesses to the same page. Our Buffer-referred data Prefetcher (BRP) improves an IPC speedup of 23.2% over a no-prefetching baseline. BRP outperforms the state-of-the-art Signature Path prefetcher by an IPC speedup of 4.2%."}}
{"id": "kYjBprd7fkH", "cdate": 1609459200000, "mdate": 1695964093908, "content": {"title": "Turbo-Aggregate: Breaking the Quadratic Aggregation Barrier in Secure Federated Learning", "abstract": "Federated learning is a distributed framework for training machine learning models over the data residing at mobile devices, while protecting the privacy of individual users. A major bottleneck in scaling federated learning to a large number of users is the overhead of secure model aggregation across many users. In particular, the overhead of the state-of-the-art protocols for secure model aggregation grows quadratically with the number of users. In this article, we propose the first secure aggregation framework, named Turbo-Aggregate, that in a network with N users achieves a secure aggregation overhead of O(NlogN), as opposed to O(N <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> ), while tolerating up to a user dropout rate of 50%. Turbo-Aggregate employs a multi-group circular strategy for efficient model aggregation, and leverages additive secret sharing and novel coding techniques for injecting aggregation redundancy in order to handle user dropouts while guaranteeing user privacy. We experimentally demonstrate that Turbo-Aggregate achieves a total running time that grows almost linear in the number of users, and provides up to 40\u00d7 speedup over the state-of-the-art protocols with up to N=200 users. Our experiments also demonstrate the impact of model size and bandwidth on the performance of Turbo-Aggregate."}}
{"id": "_cuVOtqSXiL", "cdate": 1609459200000, "mdate": 1695964093802, "content": {"title": "Byzantine-Resilient Secure Federated Learning", "abstract": "Secure federated learning is a privacy-preserving framework to improve machine learning models by training over large volumes of data collected by mobile users. This is achieved through an iterative process where, at each iteration, users update a global model using their local datasets. Each user then masks its local update via random keys, and the masked models are aggregated at a central server to compute the global model for the next iteration. As the local updates are protected by random masks, the server cannot observe their true values. This presents a major challenge for the resilience of the model against adversarial (Byzantine) users, who can manipulate the global model by modifying their local updates or datasets. Towards addressing this challenge, this paper presents the first single-server Byzantine-resilient secure aggregation framework (BREA) for secure federated learning. BREA is based on an integrated stochastic quantization, verifiable outlier detection, and secure model aggregation approach to guarantee Byzantine-resilience, privacy, and convergence simultaneously. We provide theoretical convergence and privacy guarantees and characterize the fundamental trade-offs in terms of the network size, user dropouts, and privacy protection. Our experiments demonstrate convergence in the presence of Byzantine users, and comparable accuracy to conventional federated learning benchmarks."}}
{"id": "T18jtDYljc", "cdate": 1609459200000, "mdate": 1695964094097, "content": {"title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning", "abstract": ""}}
