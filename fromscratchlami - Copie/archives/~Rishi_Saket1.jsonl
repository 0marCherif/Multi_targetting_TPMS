{"id": "AeDI0CZ8Wa", "cdate": 1671916755721, "mdate": 1671916755721, "content": {"title": "Domain-Agnostic Constrastive Representations for Learning from Label Proportions", "abstract": "We study the weak supervision learning problem of Learning from\nLabel Proportions (LLP) where the goal is to learn an instance-level\nclassifier using proportions of various class labels in a bag \u2013 a collection of input instances that often can be highly correlated. While\nrepresentation learning for weakly-supervised tasks is found to\nbe effective, they often require domain knowledge. To the best of\nour knowledge, representation learning for tabular data (unstructured data containing both continuous and categorical features)\nare not studied. In this paper, we propose to learn diverse representations of instances within the same bags to effectively utilize\nthe weak bag-level supervision. We propose a domain agnostic LLP\nmethod, called \"Self Contrastive Representation Learning for LLP\"\n(SelfCLR-LLP) that incorporates a novel self\u2013contrastive function\nas an auxiliary loss to learn representations on tabular data for LLP.\nWe show that diverse representations for instances within the same\nbags aid efficient usage of the weak bag-level LLP supervision. We\nevaluate the proposed method through extensive experiments on\nreal-world LLP datasets from e-commerce applications to demonstrate the effectiveness of our proposed SelfCLR-LLP."}}
{"id": "Vf2DK1Ol0ed", "cdate": 1663850533146, "mdate": null, "content": {"title": "A Benchmark Dataset for Learning from Label Proportions", "abstract": "Learning from label proportions (LLP) has recently emerged as an important technique of weakly supervised learning on aggregated labels. In LLP, a model is trained on groups (a.k.a bags) of feature-vectors and their corresponding label proportions to predict labels for individual feature-vectors. While previous works have developed a variety of techniques for LLP, including novel loss functions, model architectures and their optimization, they typically evaluated their methods on pseudo-synthetically generated LLP training data using common small scale supervised learning datasets by randomly sampling or partitioning their instances into bags.  Despite growing interest in this important task there are no large scale open source LLP benchmarks to compare various approaches. Construction of such a benchmark is hurdled by two challenges a) lack of natural large scale LLP like data, b) large number of mostly artificial methods of forming bags from instance level datasets. \nIn this paper we propose LLP-Bench: a large scale LLP benchmark constructed from   the Criteo Kaggle CTR dataset. We do an in-depth, systematic study of the Criteo dataset and propose a methodology to create a  benchmark as a collection of diverse and large scale LLP datasets. We choose the Criteo dataset since it admits multiple natural collections of bags formed by grouping  subsets of its 26 categorical features. We analyze all bag collections obtained through grouping by one or two categorical features, in terms of their bag-level statistics as well as embedding based distance metrics quantifying the geometric separation of bags. We then propose to include in LLP-Bench a few groupings to fairly represent real world bag distributions.\nWe also measure the performance of state of the art models, loss functions (adapted to LLP) and optimizers on LLP-Bench. We perform a series of ablations and explain the performance of various techniques  on LLP-Bench. To the best of our knowledge LLP-Bench is the first open source benchmark for the LLP task. We hope that the proposed benchmark and the evaluation methodology will be used by ML researchers and practitioners to better understand and hence devise state of art LLP algorithms. "}}
{"id": "idtsGjGjh4r", "cdate": 1662928260035, "mdate": 1662928260035, "content": {"title": "Multi-Variate Time Series Forecasting on Variable Subsets", "abstract": "We formulate a new inference task in the domain of multivariate time series forecasting (MTSF), called Variable Subset Forecast\n(VSF), where only a small subset of the variables is available during\ninference. Variables are absent during inference because of longterm data loss (eg. sensor failures) or high\u2192low-resource domain\nshift between train / test. To the best of our knowledge, robustness\nof MTSF models in presence of such failures, has not been studied\nin the literature. Through extensive evaluation, we first show that\nthe performance of state of the art methods degrade significantly in\nthe VSF setting. We propose a non-parametric, wrapper technique\nthat can be applied on top any existing forecast models. Through\nsystematic experiments across 4 datasets and 5 forecast models,\nwe show that our technique is able to recover close to 95% performance of the models even when only 15% of the original variables\nare present."}}
{"id": "4LZo68TuF-4", "cdate": 1652737801645, "mdate": null, "content": {"title": "Algorithms and Hardness for Learning Linear Thresholds from Label Proportions", "abstract": "We study the learnability of linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this, the feature-vector classifier is learnt from bags of feature-vectors and their corresponding observed label proportions which are satisfied by (i.e., consistent with) some unknown LTF. This problem has been investigated in recent work (Saket21)  which gave an algorithm to produce an LTF that satisfies at least $(2/5)$-fraction of a satisfiable collection of bags, each of size $\\leq 2$, by solving and rounding a natural SDP relaxation. However, this SDP relaxation is specific to at most $2$-sized bags and does not apply to bags of larger size. \n    \nIn this work we provide a fairly non-trivial SDP relaxation of a  non-quadratic formulation for bags of size $3$. We analyze its rounding procedure using novel matrix decomposition techniques to obtain an algorithm which outputs an LTF satisfying at least $(1/12)$-fraction of the bags of size $\\leq 3$. We also apply our techniques to bags of size $q \\geq 4$ to provide a $\\Omega\\left(1/q\\right)$-approximation guarantee for a weaker notion of satisfiability. We include comparative experiments on simulated data demonstrating the applicability of our algorithmic techniques.\n    \nFrom the complexity side we provide a hardness reduction to produce instances with bags of any constant size $q$. Our reduction proves the NP-hardness of satisfying  more than $({1}/{q}) + o(1)$ fraction of a satisfiable collection of such bags using as hypothesis any function of constantly many LTFs, showing thereby that the problem is harder to approximate as the bag size $q$ increases. Using a strengthened analysis, for $q=2$ we obtain a $({4}/{9}) +o(1)$ hardness factor for this problem, improving upon the $({1}/{2}) + o(1)$ factor shown by Saket21.\n"}}
{"id": "zJ59WCf9cPs", "cdate": 1640995200000, "mdate": 1681703650141, "content": {"title": "Domain-Agnostic Contrastive Representations for Learning from Label Proportions", "abstract": "We study the weak supervision learning problem of Learning from Label Proportions (LLP) where the goal is to learn an instance-level classifier using proportions of various class labels in a bag -- a collection of input instances that often can be highly correlated. While representation learning for weakly-supervised tasks is found to be effective, they often require domain knowledge. To the best of our knowledge, representation learning for tabular data (unstructured data containing both continuous and categorical features) are not studied. In this paper, we propose to learn diverse representations of instances within the same bags to effectively utilize the weak bag-level supervision. We propose a domain agnostic LLP method, called \"Self Contrastive Representation Learning for LLP\" (SelfCLR-LLP) that incorporates a novel self--contrastive function as an auxiliary loss to learn representations on tabular data for LLP. We show that diverse representations for instances within the same bags aid efficient usage of the weak bag-level LLP supervision. We evaluate the proposed method through extensive experiments on real-world LLP datasets from e-commerce applications to demonstrate the effectiveness of our proposed SelfCLR-LLP. In this paper, we propose to learn diverse representations of instances within the same bags to effectively utilize the weak bag-level supervision. We propose a domain agnostic LLP method, called \"Self Contrastive Representation Learning for LLP\" (SelfCLR-LLP) that incorporates a novel self--contrastive function as an auxiliary loss to learn representations on tabular data for LLP. We show that diverse representations for instances within the same bags aid efficient usage of the weak bag-level LLP supervision. We evaluate the proposed method through extensive experiments on real-world LLP datasets from e-commerce applications to demonstrate the effectiveness of our proposed SelfCLR-LLP."}}
{"id": "vCcWG4ys9x", "cdate": 1640995200000, "mdate": 1681703650140, "content": {"title": "On Combining Bags to Better Learn from Label Proportions", "abstract": "In the framework of learning from label proportions (LLP) the goal is to learn a good instance-level label predictor from the observed label proportions of bags of instances. Most of the LLP algorithms either explicitly or implicitly assume the nature of bag distributions with respect to the actual labels and instances, or cleverly adapt supervised learning techniques to suit LLP. In practical applications however, the scale and nature of data could render such assumptions invalid and the many of the algorithms impractical. In this paper we address the hard problem of solving LLP with provable error bounds while being bag distribution agnostic and model agnostic. We first propose the concept of generalized bags, an extension of bags and then devise an algorithm to combine bag distributions, if possible, into good generalized bag distributions. We show that (w.h.p) any classifier optimizing the squared Euclidean label-proportion loss on such a generalized bag distribution is guaranteed to minimize the instance-level loss as well. The predictive quality of our method is experimentally evaluated and it equals or betters the previous methods on pseudo-synthetic and real-world datasets."}}
{"id": "jZHlOUD05U", "cdate": 1640995200000, "mdate": 1681703649878, "content": {"title": "Multi-Variate Time Series Forecasting on Variable Subsets", "abstract": "We formulate a new inference task in the domain of multivariate time series forecasting (MTSF), called Variable Subset Forecast (VSF), where only a small subset of the variables is available during inference. Variables are absent during inference because of long-term data loss (eg. sensor failures) or high -> low-resource domain shift between train / test. To the best of our knowledge, robustness of MTSF models in presence of such failures, has not been studied in the literature. Through extensive evaluation, we first show that the performance of state of the art methods degrade significantly in the VSF setting. We propose a non-parametric, wrapper technique that can be applied on top any existing forecast models. Through systematic experiments across 4 datasets and 5 forecast models, we show that our technique is able to recover close to 95% performance of the models even when only 15% of the original variables are present."}}
{"id": "cSJ6JZXhUX", "cdate": 1640995200000, "mdate": 1683884173780, "content": {"title": "Multi-Variate Time Series Forecasting on Variable Subsets", "abstract": "We formulate a new inference task in the domain of multivariate time series forecasting (MTSF), called Variable Subset Forecast (VSF), where only a small subset of the variables is available during inference. Variables are absent during inference because of long-term data loss (eg. sensor failures) or high -> low-resource domain shift between train / test. To the best of our knowledge, robustness of MTSF models in presence of such failures, has not been studied in the literature. Through extensive evaluation, we first show that the performance of state of the art methods degrade significantly in the VSF setting. We propose a non-parametric, wrapper technique that can be applied on top any existing forecast models. Through systematic experiments across 4 datasets and 5 forecast models, we show that our technique is able to recover close to 95\\% performance of the models even when only 15\\% of the original variables are present."}}
{"id": "Dzvugrt8rKv", "cdate": 1640995200000, "mdate": 1684319303426, "content": {"title": "Algorithms and Hardness for Learning Linear Thresholds from Label Proportions", "abstract": "We study the learnability of linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this, the feature-vector classifier is learnt from bags of feature-vectors and their corresponding observed label proportions which are satisfied by (i.e., consistent with) some unknown LTF. This problem has been investigated in recent work (Saket21) which gave an algorithm to produce an LTF that satisfies at least $(2/5)$-fraction of a satisfiable collection of bags, each of size $\\leq 2$, by solving and rounding a natural SDP relaxation. However, this SDP relaxation is specific to at most $2$-sized bags and does not apply to bags of larger size. In this work we provide a fairly non-trivial SDP relaxation of a non-quadratic formulation for bags of size $3$. We analyze its rounding procedure using novel matrix decomposition techniques to obtain an algorithm which outputs an LTF satisfying at least $(1/12)$-fraction of the bags of size $\\leq 3$. We also apply our techniques to bags of size $q \\geq 4$ to provide a $\\Omega\\left(1/q\\right)$-approximation guarantee for a weaker notion of satisfiability. We include comparative experiments on simulated data demonstrating the applicability of our algorithmic techniques. From the complexity side we provide a hardness reduction to produce instances with bags of any constant size $q$. Our reduction proves the NP-hardness of satisfying more than $({1}/{q}) + o(1)$ fraction of a satisfiable collection of such bags using as hypothesis any function of constantly many LTFs, showing thereby that the problem is harder to approximate as the bag size $q$ increases. Using a strengthened analysis, for $q=2$ we obtain a $({4}/{9}) +o(1)$ hardness factor for this problem, improving upon the $({1}/{2}) + o(1)$ factor shown by Saket21."}}
{"id": "5BnaKeEwuYk", "cdate": 1621630041622, "mdate": null, "content": {"title": "Learnability of Linear Thresholds from Label Proportions", "abstract": "We study the problem of properly learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. \nFirst, we provide an algorithm that, given a collection of such bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisfied by) an unknown LTF, efficiently produces an LTF that satisfies at least $(2/5)$-fraction of the bags. If all the bags are non-monochromatic (i.e., bags of size two with differently labeled feature-vectors) the algorithm satisfies at least $(1/2)$-fraction of them. For the special case of OR over the $d$-dimensional boolean vectors, we give an algorithm which computes an LTF achieving an additional $\\Omega(1/d)$ in accuracy for the two cases.\n\nOur main result provides evidence that these algorithmic bounds cannot be significantly improved, even for learning monotone ORs using LTFs. We prove that it is NP-hard, given a collection of non-monochromatic bags which are all satisfied by some monotone OR, to compute any function of constantly many LTFs that satisfies  $(1/2 + \\varepsilon)$-fraction of the bags for any constant $\\varepsilon > 0$. This bound is tight for the non-monochromatic bags case.\n\nThe above is in contrast to the usual supervised learning setup (i.e., unit-sized bags) in which LTFs are efficiently learnable to arbitrary accuracy using linear programming, and even a trivial algorithm (any LTF or its complement) achieves an accuracy of $1/2$. These techniques however, fail in the LLP setting. Indeed, we show that the LLP learning of LTFs (even for the special case of monotone ORs) using LTFs dramatically increases in complexity as soon as bags of size two are allowed.\nOur work gives the first inapproximability for LLP learning LTFs, and a strong complexity separation between LLP  and traditional supervised learning."}}
