{"id": "oKeIIU8KDv_", "cdate": 1672531200000, "mdate": 1696492808655, "content": {"title": "YOLO5PKLot: A Parking Lot Detection Network Based on Improved YOLOv5 for Smart Parking Management System", "abstract": "In recent years, the YOLOv5 network architecture has demonstrated excellence in real-time object detection. For the purpose of applying in the smart parking management system, this paper proposes a network based on the improved YOLOv5, named YOLO5PKLot. This network focus on redesigning the backbone network with a combination of the lightweight Ghost Bottleneck and Spatial Pyramid Pooling architectures. In addition, this work also resizes the anchors and adds a detection head to optimize parking detection. The proposed network is trained and evaluated on the Parking Lot dataset. As a result, YOLO5PKLot achieved 99.6% mAP on the valuation set with only fewer network parameters and computational complexity than others."}}
{"id": "TonYyestx6s", "cdate": 1672531200000, "mdate": 1696492808657, "content": {"title": "Age Group Recognizer based on Human Face Supporting Smart Digital Advertising Platforms", "abstract": "Smart digital advertising platforms have been widely employed in public areas in big cities. An age group recognizer is indispensable to support these platforms in providing relevant advertisements for each audience. These platforms also demand a recognizer that can run properly at the minimum on a CPU device to degrade the budget during system procurement. This study proposes an age group recognizer based on human faces to predict the age group of the audience\u2019s face using an efficient architecture containing a light backbone. This work offers a residual mini multi-level module integrating two grouped convolution layers with diverse frequency levels to extract exclusive facial features maintained by residual operation. In order to improve the feature map\u2019s quality, a deep lite attention module is proposed, consisting of the deep channel and lite spatial attention part. The architecture generates few parameters with cheap operation and achieves competitive performance on the benchmark datasets. In addition, the architecture integrated with face detection, as a recognizer, can perform fast on a CPU configuration in real-time with 144 frames per second."}}
{"id": "Re5d_IThJM0", "cdate": 1672531200000, "mdate": 1699162125370, "content": {"title": "Lightweight Convolutional Neural Network for Fire Classification in Surveillance System", "abstract": "Fire is one of the worst disasters for human life. Fire can happen anywhere and the leading cause can be natural or man. Over the last century, scientists have invented sensor-based methods to minimize damage and provide early warning of fires. However, these applications are only applied in a limited space and distance. For the purpose of fire remote warning and deploying on low-computing devices, this paper proposes a vision-based method using a lightweight convolutional neural network architecture combined with the inception and attention mechanisms. This proposed network includes two main modules: a feature extractor and a classifier. The feature extractor exploits convolution layers, depthwise separable convolution layers, inception module, and attention mechanism to extract high-level feature maps. Next, the classifier applies the global average pooling layer to quickly reduce the feature map dimensions and uses the softmax function to calculate the probability of each class. The experiments performed the training and evaluation on six datasets with an accuracy of over 96%. The fire surveillance system was implemented with simulation videos on GPU, CPU, and Jetson Nano devices, with the highest speeds of 200.95 FPS, 31.08 FPS, and 14.27 FPS, respectively. A set of demonstration videos, source code, and proposed dataset are provided here: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://bit.ly/3Wlpycf</uri> ."}}
{"id": "N8sMzqvV5b", "cdate": 1672531200000, "mdate": 1696492808655, "content": {"title": "Unifying Local and Global Fourier Features for Image Classification", "abstract": "In the last decade, Convolutional Neural Networks (CNNs) have become a dominant algorithm in solving various domains such as computer vision, self-driving cars, medical imaging, and natural language processing. The core operation of the CNNs is convolution layer that can aggregate input features around local windows in a short-range manner and learn relative positions inside each window. For long-range modeling, common CNNs stack a bunch of convolutional layers that result in high computational costs to enlarge receptive field. Recently, Vision Transformers (ViTs) and its improvements have outperformed CNNs in the rankings of language, vision, and audio research. The main goal of the ViTs is that the model can extract short-range and long-range features in one layer. With this strategy, the network structure of the ViTs is simpler than CNNs. However, ViTs have quadratic complexity with the spatial length of the input feature. In the last year, many methods are proposed to relax the cost of ViTs and bring complicated designs of CNNs into ViT-based models. Inspired by the insightful properties of the ViTs and CNNs, this paper introduces a Local and Global Fourier Network (LGFNet) that jointly learns local and global receptive fields in the frequency domain rather than the spatial or time domain in conventional CNNs and ViTs. The input features, local, and global kernels are transformed to the frequency domain through Fast Fourier Transform. The local features are learned by a convolution between the input feature and local kernels. Concurrently, matrix multiplication between the input feature and global kernels is performed to extract low frequencies from the input Fourier feature. Since local and global Fourier features are complementary, the LGFNet efficiently fuses these information by summation operation based on the similarity degrees of the input signals. Therefore, our LGFNet performs unified representation from the input feature. To evaluate the effectiveness of the proposed method, we conduct experiments on the large-scale dataset ImageNet1k and the small dataset CIFAR100. As a result, the LGFNet surpasses the ViT-based models by a clear margin under similar parameters and GFLOPs."}}
{"id": "ADYVe19ZtbF", "cdate": 1672531200000, "mdate": 1696492808720, "content": {"title": "Human Face Detector with Gender Identification by Split-Based Inception Block and Regulated Attention Module", "abstract": "Smart digital advertising platforms have been widely arising. These platforms require a human face detector with gender identification to assist them in the determination of providing relevant advertisements. The detector is also prosecuted to identify the gender of a masked face in post-coronavirus situations and demanded to operate on a CPU device to lower system expenses. This work presents a lightweight Convolution Neural Network (CNN) architecture to build a gender identification integrated with face detection to respond to these issues. This work proposes a split-based inception block to efficiently extract features at various sizes by partially applying different convolution kernel sizes, levels, and regulated attention module to improve the quality of the feature map. It produces slight parameters that drive the architecture efficiency and can operate quickly in real-time. To validate the performance of the proposed architecture, UTKFace and Labeled Faces in the Wild (LFW) datasets, modified with an artificial mask, are utilized as training and validation datasets. This offered architecture is compared to different lightweight and deep architectures. Regarding the experiment results, the proposed architecture outperforms masked face gender identification on the two datasets. In addition, the proposed architecture, which integrates with face detection to become a human face detector with gender identification can run 135 frames per second in real-time on a CPU configuration."}}
{"id": "5Emi8wLhBOH", "cdate": 1672531200000, "mdate": 1696492808660, "content": {"title": "Dynamic Circular Convolution for Image Classification", "abstract": "In recent years, Vision Transformer (ViT) has achieved an outstanding landmark in disentangling diverse information of visual inputs, superseding traditional Convolutional Neural Networks (CNNs). Although CNNs have strong inductive biases such as translation equivariance and relative positions, they require deep layers to model long-range dependencies in input data. This strategy results in high model complexity. Compared to CNNs, ViT can extract global features even in earlier layers through token-to-token interactions without considering geometric location of pixels. Therefore, ViT models are data-efficient and data-hungry, in another work, learning data-dependent and producing high performances on large-scale datasets. Nonetheless, ViT has quadratic complexity with the length of the input token because of the natural dot product between query and key matrices. Different from ViTs-and-CNNs-based models, this paper proposes a Dynamic Circular Convolution Network (DCCNet) that learns token-to-token interactions in Fourier domain, relaxing model complexity to O(Nlog(N) instead of $$O(N^2)$$ in ViTs, and global Fourier filters are treated dependently and dynamically rather than independent and static weights in conventional operators. The token features, dynamic filters in spatial domain are transformed to frequency domain via Fast Fourier Transform (FFT). Dynamic circular convolution, in lieu of matrix multiplication in Fourier domain, between Fourier features and transformed filters are performed in a separable way along channel dimension. The output of circular convolution is revered back to spatial domain by Inverse Fast Fourier Transform (IFFT). Extensive experiments are conducted and evalued on large-scaled dataset ImageNet1k and small dataset CIFAR100. On ImageNet1k, the proposed model achieves 75.4% top-1 accuracy and 92.6% top-5 accuracy with the budget 7.5M paramaters under similar setting with ViT-based models, surpassing ViT and its variants. When fine-tuning the model on smaller dataset, DCCNet still works well and gets the state-of-the-art performances. Both evaluating the model on large and small datasets verifies the effectiveness and generalization capabilities of the proposed method."}}
{"id": "zPqqJx1OyL9", "cdate": 1640995200000, "mdate": 1699162125374, "content": {"title": "Driver Behaviors Recognizer Based on Light-Weight Convolutional Neural Network Architecture and Attention Mechanism", "abstract": "Driving is a set of behaviors that need high concentration. Sometimes these behaviors are dominated by other acts such as smoking, eating, drinking, talking, phone calls, adjusting the radio, or drowsiness. These are also the main causes of current traffic accidents. Therefore, developing applications to warn drivers in advance is essential. This research introduces a light-weight convolutional neural network architecture to recognize driver behaviors, helping the warning system to provide accurate information and to minimize traffic collisions. This network is a combination of feature extraction and classifier modules. The feature extraction module uses the advantages of the standard convolution layers, depthwise separable convolution layers, average pooling layers, and proposed adaptive connections to extract the feature maps. The benefit of the convolution block attention module is deployed in the feature extraction module that guides the network in learning the salient features. The classifier module is comprised of a global average pooling and softmax layer to calculate the probability of each class. The overall design optimizes the network parameters and maintains classification accuracy. The entire network is trained and evaluated on three benchmark datasets: the State Farm Distracted Driver Detection, the American University in Cairo version 1, and the American University in Cairo version 2. As a result, the accuracies on overall classes (ten classes) are 99.95%, 95.57%, and 99.61%, respectively. Also, several video tests with VGA (Video Graphics Array), HD (High Definition), and FHD (Full High Definition) resolution were conducted, and they can be seen at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://bit.ly/3GY2iJl</uri> ."}}
{"id": "v1sCm3ua3u", "cdate": 1640995200000, "mdate": 1696492808515, "content": {"title": "Multi-level Feature Reweighting and Fusion for Instance Segmentation", "abstract": "Accurate instance segmentation requires high-resolution features for performing a dense pixel-wise prediction task. However, using high-resolution feature maps results in highly expensive model complexity and ineffective receptive fields. To overcome the problems of high-resolution features, conventional methods explore multi-level feature fusion that exchanges the information between low-level features at earlier layers and high-level features at top layers. Both low and high information is extracted by the hierarchical backbone network where high-level features contain more semantic cues and low-level features encompass more specific patterns. Thus, adopting these features to the training segmentation model is necessary, and designing a more efficient multi-level feature fusion is crucial. Existing methods balance such information by using top-down and bottom-up pathway connections with more inefficient convolution layers to produce richer multi-scale features. In this work, we contribute two folds: (1) a simple but effective multilevel feature reweighting layer is proposed to strengthen deep high-level features based on channel reweighting generated from multiple features of the backbone, and (2) an efficient fusion block is proposed to process low-resolution features in a depth-to-spatial manner and combine enhanced multi-level features together. These designs enable the segmentation models to predict instance kernels for mask generation on high-level feature maps. To verify the effectiveness of the proposed method, we conduct experiments on the challenging benchmark dataset MS-COCO. Surprisingly, our simple network outperforms the baseline in both accuracy and inference speed. More specifically, we achieve 35.4% AP <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">mask</sup> at 19.5 FPS on a GPU device, becoming a state-of-the-art instance segmentation method."}}
{"id": "v0WJle9Yhx", "cdate": 1640995200000, "mdate": 1699162125371, "content": {"title": "A Fast CPU Real-Time Facial Expression Detector Using Sequential Attention Network for Human-Robot Interaction", "abstract": "Facial expression detection is a method to predict human facial emotions. This work is a trending research topic that can be implemented for human-robot interaction. More recently, deep convolutional neural network provides a robust extractor features but tends to be slow in real-time implementations and often requires a large memory and graphics processing units for fast execution. In this article, an efficient CPU-based facial expression detector is proposed using a sequential attention network to improve the baseline performance. The proposed attention network consists of three modules, global representation to capture the global features, channel representation, and dimension representation, which are focused on the channel and using spatial attention to discriminate local features. The efficient partial transfer module is also presented as a light backbone to extract facial features from an image. The entire module is trained and tested on several benchmarks to classify seven facial expressions. As a result, the proposed model reaches an accuracy of 98.18%, 98.75%, 95.63%, and 74.17% on CK+, JAFFE, KDEF, and FER-2013, respectively. It achieves competitive performance when compared to state-of-the-art methods. Lastly, it is integrated with a face detector and runs in real-time without a constraint at 69 frames per second on a CPU."}}
{"id": "loF5OLlvvtL6", "cdate": 1640995200000, "mdate": 1696492808695, "content": {"title": "Fire Warning Based on Convolutional Neural Network and Inception Mechanism", "abstract": "Fire is a dangerous disaster that takes many lives and human property. Fire happens everywhere, especially in areas with high temperatures or hot sun. Fires can be caused by humans or by nature. Therefore, an early warning of fire is necessary to reduce the damage. Research in many different fields has long been focused on fire alerts. This paper proposes a fire alarm system based on a lightweight convolutional neural network. The design takes the advantage of convolution layers, depthwise separable convolution layers, inception module, and softmax function to optimize network parameters while ensuring feature extraction and classification. This network is trained and evaluated on FireNet dataset with an accuracy of 97.14%. In addition, this work also builds and implements the fire video testing systems on low-computation devices such as CPU-Based personal computer and embedded devices."}}
