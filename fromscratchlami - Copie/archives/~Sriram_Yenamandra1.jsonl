{"id": "b-cto-fetlz", "cdate": 1686324863609, "mdate": null, "content": {"title": "HomeRobot: Open-Vocabulary Mobile Manipulation", "abstract": "HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks.\n\nOpen-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://home-robot-ovmm.github.io/."}}
{"id": "OjS3nkNATOw", "cdate": 1652737272828, "mdate": null, "content": {"title": "Adapting Self-Supervised Vision Transformers by Probing Attention-Conditioned Masking Consistency", "abstract": "Visual domain adaptation (DA) seeks to transfer trained models to unseen, unlabeled domains across distribution shift, but approaches typically focus on adapting convolutional neural network architectures initialized with supervised ImageNet representations. In this work, we shift focus to adapting modern architectures for object recognition -- the increasingly popular Vision Transformer (ViT) -- initialized with modern pretraining based on self-supervised learning (SSL). Inspired by the design of recent SSL approaches based on learning from partial image inputs generated via masking or cropping -- either by learning to predict the missing pixels, or learning representational invariances to such augmentations -- we propose PACMAC, a two-stage adaptation algorithm for self-supervised ViTs. PACMAC first performs in-domain SSL on pooled source and target data to learn task-discriminative features, and then probes the model's predictive consistency across a set of partial target inputs generated via a novel attention-conditioned masking strategy, to identify reliable candidates for self-training. Our simple approach leads to consistent performance gains over competing methods that use ViTs and self-supervised initializations on standard object recognition benchmarks. Our code is available at https://github.com/virajprabhu/PACMAC."}}
{"id": "LHzRoihoWWO", "cdate": 1640995200000, "mdate": 1652990321380, "content": {"title": "Semi-Supervised Deep Expectation-Maximization for Low-Dose Pet-Ct", "abstract": "Reducing the dose of ionizing radiation underlying combined imaging with positron emission tomography (PET) and computed tomography (CT) typically leads to reduced image quality. We propose a novel variational deep-neural-network (DNN) framework for image quality enhancement of low-dose PET-CT images, relying on Monte-Carlo expectation maximization. Unlike existing DNN-based training that pairs low-dose PET-CT images with their corresponding high-dose versions, we propose a semi-supervised learning framework that enables learning using a small number of high-dose images. We propose a robust and uncertainty-aware loss motivated by a heavy-tailed generalized-Gaussian distribution on the residuals between the DNN output and the PET-CT data, aiding our semi-supervised learning scheme. Results on publicly available data show the benefits of our framework, quantitatively and qualitatively, over existing methods."}}
{"id": "LMTpwDWiiNd", "cdate": 1609459200000, "mdate": 1652990321376, "content": {"title": "FaaSter: Accelerated Functions-as-a-Service with Heterogeneous GPUs", "abstract": "In this work, we present FaaSter, an Accelerated Functions as a Service (AFaaS) offering that unifies the function-as-a-service model with GPU acceleration resources. FaaSter provides an acceleration function library as a service, which in turn is provisioned on heterogeneous GPUs. To provide seamless access to these accelerated functions and ensure that each function has the best possible response time, we utilize GPU kernel slicing to split and execute an accelerated function instance across multiple heterogeneous GPUs. The central challenge is to be able to quickly decide the number of slices to split each function into and then map the slices to the right GPUs. To this end, we present a scheduling heuristic that is able to significantly reduce the average turn-around time of functions when compared to a non-sliceable full GPU scheduling approach. Our evaluation results show that the FaaSter scheduler achieves 62 % mean and up to 80 % improvement in average turn-around time and always performs equal to or better than non-sliceable GPU scheduling."}}
{"id": "bWinZvEU5vj", "cdate": 1577836800000, "mdate": 1652990321394, "content": {"title": "Learning Image Inpainting from Incomplete Images using Self-Supervision", "abstract": "Current approaches for semantic image inpainting rely on deep neural networks (DNNs) that learn under full supervision, i.e., using a training set comprising pairs of (i) corrupted images with holes and (ii) corresponding uncorrupted images. However, for several real-world applications, obtaining large sets of uncorrupted images is challenging or infeasible. Current methods also rely on adversarial training involving min-max optimization that is prone to instability during learning. We propose a novel self-supervised image-inpainting DNN framework that can learn in both completely unsupervised and semi-supervised modes. Moreover, our DNN learning formulation bypasses adversarial training and, thereby, lends itself to more stable training. Results on the publicly available CelebA dataset show that our method, even when learning unsupervisedly, outperforms the state of the art that learns with full supervision."}}
