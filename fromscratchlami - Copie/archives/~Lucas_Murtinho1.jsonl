{"id": "I5-Fcyzd8ZE", "cdate": 1680781732950, "mdate": 1680781732950, "content": {"title": "On the price of explainability for some clustering problems", "abstract": "The price of explainability for a clustering task can be defined as the unavoidable loss, in terms of the objective function, if we force the final partition to be explainable. Here, we study this price for the following clustering problems: $k$-means, $k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds for a natural model where explainability is achieved via decision trees. For the $k$-means and $k$-medians problems our upper bounds improve those obtained by [Dasgupta et. al, ICML 20] for low dimensions. Another contribution is a simple and efficient algorithm for building explainable clusterings for the $k$-means problem. We provide empirical evidence that its performance is better than the current state of the art for decision-tree based explainable clustering."}}
{"id": "x3jL7mfb0F", "cdate": 1680781255739, "mdate": 1680781255739, "content": {"title": "Minimization of Gini Impurity: NP-completeness and Approximation Algorithm via Connections with the $k$-means Problem", "abstract": "The Gini impurity is a very popular criterion to select attributes during decision trees construction. In the problem of finding a partition with minimum weighted Gini impurity (PMWGP), the one faced during the construction of decision trees, a set of vectors must be partitioned into $k$ different clusters such that the partition's overall Gini impurity is minimized.\n\nWe show that PMWGP is APX-hard for arbitrary $k$ and admits a randomized PTAS when the number of clusters is fixed. These results significantly improve the current knowledge on the problem. The key idea to obtain these results is to explore connections between PMWGP and the geometric $k$-means clustering problem."}}
{"id": "s8UPdxrLu2N", "cdate": 1680781061540, "mdate": 1680781061540, "content": {"title": "Nearly tight bounds on the price of explainability for the $k$-center and the maximum-spacing clustering problems", "abstract": "The price of explainability for a clustering task can be defined as the unavoidable loss, in terms of the objective function, if we force the final partition to be explainable. Here, we study this price for the $k$-centers and maximum-spacing clustering problems. We provide nearly tight bounds for a natural model where explainability is achieved via decision trees."}}
{"id": "OKpcUCcl7R3", "cdate": 1680780935631, "mdate": 1680780935631, "content": {"title": "Shallow decision trees for explainable $k$-means clustering", "abstract": "A number of recent works have employed decision trees for the construction of explainable partitions that aim to minimize the \n$k$-means cost function. These works, however, largely ignore metrics related to the depths of the leaves in the resulting tree, which is perhaps surprising considering how the explainability of a decision tree depends on these depths. To fill this gap in the literature, we propose an efficient algorithm with a penalty term in its loss function to favor the construction of shallow decision trees \u2013 i.e., trees whose leaves are not very deep, which translate to clusters that are defined by a small number of attributes and are therefore easier to explain. In experiments on 16 datasets, our algorithm yields better results than decision-tree clustering algorithms recently presented in the literature, typically achieving lower or equivalent costs with considerably shallower trees."}}
{"id": "2f0-ckFSRC4", "cdate": 1677628800000, "mdate": 1681742018988, "content": {"title": "Nearly tight bounds on the price of explainability for the k-center and the maximum-spacing clustering problems", "abstract": ""}}
{"id": "u_Ghtpswtv", "cdate": 1609459200000, "mdate": 1681742018983, "content": {"title": "On the price of explainability for some clustering problems", "abstract": "The price of explainability for a clustering task can be defined as the unavoidable loss, in terms of the objective function, if we force the final partition to be explainable. Here, we study this ..."}}
{"id": "Qvki6-BJMzI", "cdate": 1609459200000, "mdate": 1681742018984, "content": {"title": "Shallow decision trees for explainable k-means clustering", "abstract": "A number of recent works have employed decision trees for the construction of explainable partitions that aim to minimize the $k$-means cost function. These works, however, largely ignore metrics related to the depths of the leaves in the resulting tree, which is perhaps surprising considering how the explainability of a decision tree depends on these depths. To fill this gap in the literature, we propose an efficient algorithm that takes into account these metrics. In experiments on 16 datasets, our algorithm yields better results than decision-tree clustering algorithms such as the ones presented in \\cite{dasgupta2020explainable}, \\cite{frost2020exkmc}, \\cite{laber2021price} and \\cite{DBLP:conf/icml/MakarychevS21}, typically achieving lower or equivalent costs with considerably shallower trees. We also show, through a simple adaptation of existing techniques, that the problem of building explainable partitions induced by binary trees for the $k$-means cost function does not admit an $(1+\\epsilon)$-approximation in polynomial time unless $P=NP$, which justifies the quest for approximation algorithms and/or heuristics."}}
{"id": "E2Xx4rBD5c", "cdate": 1609459200000, "mdate": 1681742018985, "content": {"title": "On the price of explainability for some clustering problems", "abstract": "The price of explainability for a clustering task can be defined as the unavoidable loss,in terms of the objective function, if we force the final partition to be explainable. Here, we study this price for the following clustering problems: $k$-means, $k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds for a natural model where explainability is achieved via decision trees. For the $k$-means and $k$-medians problems our upper bounds improve those obtained by [Moshkovitz et. al, ICML 20] for low dimensions. Another contribution is a simple and efficient algorithm for building explainable clusterings for the $k$-means problem. We provide empirical evidence that its performance is better than the current state of the art for decision-tree based explainable clustering."}}
{"id": "BJE-ps-_bH", "cdate": 1546300800000, "mdate": null, "content": {"title": "New results on information theoretic clustering", "abstract": "We study the problem of optimizing the clustering of a set of vectors when the quality of the clustering is measured by the Entropy or the Gini impurity measure. Our results contribute to the state..."}}
{"id": "-SjLcE94VMl", "cdate": 1546300800000, "mdate": 1681742018987, "content": {"title": "Minimization of Gini Impurity: NP-completeness and Approximation Algorithm via Connections with the k-means Problem", "abstract": ""}}
