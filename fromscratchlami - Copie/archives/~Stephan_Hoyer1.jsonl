{"id": "Q-HOv_zn6G", "cdate": 1652737382793, "mdate": null, "content": {"title": "Efficient and Modular Implicit Differentiation", "abstract": "Automatic differentiation (autodiff) has revolutionized machine learning.  It\nallows to express complex computations by composing elementary ones in creative\nways and removes the burden of computing their derivatives by hand. More\nrecently, differentiation of optimization problem solutions has attracted\nwidespread attention with applications such as optimization layers, and in\nbi-level problems such as hyper-parameter optimization and meta-learning.\nHowever, so far, implicit differentiation remained difficult to use for\npractitioners, as it often required case-by-case tedious mathematical\nderivations and implementations. In this paper, we propose\nautomatic implicit differentiation, an efficient\nand modular approach for implicit differentiation of optimization problems. In\nour approach, the user defines directly in Python a function $F$ capturing the\noptimality conditions of the problem to be differentiated. Once this is done, we\nleverage autodiff of $F$ and the implicit function theorem to automatically\ndifferentiate the optimization problem.  Our approach thus combines the benefits\nof implicit differentiation and autodiff.  It is efficient as it can be added on\ntop of any state-of-the-art solver and modular as the optimality condition\nspecification is decoupled from the implicit differentiation mechanism.  We show\nthat seemingly simple principles allow to recover many existing implicit\ndifferentiation methods and create new ones easily.  We demonstrate the ease of\nformulating and solving bi-level optimization problems using our framework. We\nalso showcase an application to the sensitivity analysis of molecular dynamics."}}
{"id": "TQ75Md-FqQp", "cdate": 1632875747919, "mdate": null, "content": {"title": "Efficient and Modular Implicit Differentiation", "abstract": "Automatic differentiation (autodiff) has revolutionized machine learning.  It allows expressing complex computations by composing elementary ones in creative ways and removes the tedious burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted a great deal of research, with applications as a layer in a neural network, and in bi-level optimization, including hyper-parameter optimization. However, the formulae for these derivatives often involves a tedious manual derivation and implementation. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function $F$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of $F$ to automatically differentiate the optimization problem. This way, our approach combines the benefits of implicit differentiation and autodiff.  We show that seemingly simple principles allow to recover all recently proposed implicit differentiation methods and create new ones easily. We describe in details a JAX implementation of our framework and demonstrate the ease of differentiating through optimization problems thanks to it on four diverse tasks: hyperparameter optimization of multiclass SVMs, dataset distillation, task-driven dictionary learning and sensitivity analysis of molecular dynamics."}}
{"id": "D8-iwC9UN3", "cdate": 1621630335348, "mdate": null, "content": {"title": "Efficient and Modular Implicit Differentiation", "abstract": "Automatic differentiation (autodiff) has revolutionized machine learning.  It allows expressing complex computations by composing elementary ones in creative ways and removes the tedious burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted a great deal of research, with applications as a layer in a neural network, and in bi-level optimization, including hyper-parameter optimization. However, the formulae for these derivatives often involves a tedious manual derivation. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function $F$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of $F$ to automatically differentiate the optimization problem. This way, our approach combines the benefits of implicit differentiation and autodiff.  We show that seemingly simple principles allow to recover all recently proposed implicit differentiation methods and create new ones easily.  We demonstrate the ease of formulating and solving bi-level optimization problems using our framework.\n"}}
{"id": "rbVWGZ3XMmq", "cdate": 1609459200000, "mdate": 1648667641983, "content": {"title": "Variational Data Assimilation with a Learned Inverse Observation Operator", "abstract": "Variational data assimilation optimizes for an initial state of a dynamical system such that its evolution fits observational data. The physical model can subsequently be evolved into the future to..."}}
{"id": "r3VbGWnQGXc", "cdate": 1609459200000, "mdate": 1648667641977, "content": {"title": "Machine learning accelerated computational fluid dynamics", "abstract": "Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics and plasma physics. Fluids are well described by the Navier-Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large eddy simulation, our results are as accurate as baseline solvers with 8-10x finer resolution in each spatial dimension, resulting in 40-80x fold computational speedups. Our method remains stable during long simulations, and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black box machine learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization."}}
{"id": "SuObfZ37MQ9", "cdate": 1609459200000, "mdate": 1648667641973, "content": {"title": "Efficient and Modular Implicit Differentiation", "abstract": "Automatic differentiation (autodiff) has revolutionized machine learning. It allows to express complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization layers, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, so far, implicit differentiation remained difficult to use for practitioners, as it often required case-by-case tedious mathematical derivations and implementations. In this paper, we propose automatic implicit differentiation, an efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function $F$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of $F$ and the implicit function theorem to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many existing implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics."}}
{"id": "B87bG-hmfXq", "cdate": 1609459200000, "mdate": 1648667641987, "content": {"title": "Variational Data Assimilation with a Learned Inverse Observation Operator", "abstract": "Variational data assimilation optimizes for an initial state of a dynamical system such that its evolution fits observational data. The physical model can subsequently be evolved into the future to make predictions. This principle is a cornerstone of large scale forecasting applications such as numerical weather prediction. As such, it is implemented in current operational systems of weather forecasting agencies across the globe. However, finding a good initial state poses a difficult optimization problem in part due to the non-invertible relationship between physical states and their corresponding observations. We learn a mapping from observational data to physical states and show how it can be used to improve optimizability. We employ this mapping in two ways: to better initialize the non-convex optimization problem, and to reformulate the objective function in better behaved physics space instead of observation space. Our experimental results for the Lorenz96 model and a two-dimensional turbulent fluid flow demonstrate that this procedure significantly improves forecast quality for chaotic systems."}}
{"id": "hZD8qusbRB_", "cdate": 1599666916552, "mdate": null, "content": {"title": "Neural Reparameterization Improves Structural Optimization", "abstract": "Structural optimization is a popular method for designing objects such as bridge trusses, airplane wings, and optical devices. Unfortunately, the quality of solutions depends heavily on how the problem is parameterized. In this paper, we propose using the implicit bias over functions induced by neural networks to improve the parameterization of structural optimization. Rather than directly optimizing densities on a grid, we instead optimize the parameters of a neural network which outputs those densities. This reparameterization leads to different and often better solutions. On a selection of 116 structural optimization tasks, our approach produces the best design 50% more often than the best baseline method."}}
{"id": "iE8tFa4Nq", "cdate": 1582750164169, "mdate": null, "content": {"title": "Lagrangian Neural Networks", "abstract": "Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails."}}
{"id": "SazZGZ27z7c", "cdate": 1577836800000, "mdate": 1648667641963, "content": {"title": "Array programming with NumPy", "abstract": "NumPy is the primary array programming library for Python; here its fundamental concepts are reviewed and its evolution into a flexible interoperability layer between increasingly specialized computational libraries is discussed."}}
