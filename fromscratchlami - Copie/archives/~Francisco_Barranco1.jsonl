{"id": "5f8RVYEjHvm", "cdate": 1640995200000, "mdate": 1668089925518, "content": {"title": "Visual Event-Based Egocentric Human Action Recognition", "abstract": "This paper lies at the intersection of three research areas: human action recognition, egocentric vision, and visual event-based sensors. The main goal is the comparison of egocentric action recognition performance under either of two visual sources: conventional images, or event-based visual data. In this work, the events, as triggered by asynchronous event sensors or their simulation, are spatio-temporally aggregated into event frames (a grid-like representation). This allows to use exactly the same neural model for both visual sources, thus easing a fair comparison. Specifically, a hybrid neural architecture combining a convolutional neural network and a recurrent network is used. It is empirically found that this general architecture works for both, conventional gray-level frames, and event frames. This finding is relevant because it reveals that no modification or adaptation is strictly required to deal with event data for egocentric action classification. Interestingly, action recognition is found to perform better with event frames, suggesting that these data provide discriminative information that aids the neural model to learn good features."}}
{"id": "z0YtgRW7iA4", "cdate": 1609459200000, "mdate": 1668089925513, "content": {"title": "Joint direct estimation of 3D geometry and 3D motion using spatio temporal gradients", "abstract": ""}}
{"id": "ZzlDg5HQhU", "cdate": 1581700454017, "mdate": null, "content": {"title": "Contour motion estimation for asynchronous event-driven cameras", "abstract": "This paper compares image motion estimation with asynchronous event-based cameras to Computer Vision approaches using as input frame-based video sequences. Since dynamic events are triggered at significant intensity changes, which often are at the border of objects, we refer to the event-based image motion as \u201ccontour motion.\u201d Algorithms are presented for the estimation of accurate contour motion from local spatio-temporal information for two camera models: the dynamic vision sensor (DVS), which asynchronously records temporal changes of the luminance, and a family of new sensors which combine DVS data with intensity signals. These algorithms take advantage of the high temporal resolution of the DVS and achieve robustness using a multiresolution scheme in time. It is shown that, because of the coupling of velocity and luminance information in the event distribution, the image motion estimation \u2026"}}
{"id": "Dgjfldpv_e", "cdate": 1581700315713, "mdate": null, "content": {"title": "Real-time clustering and multi-target tracking using event-based sensors", "abstract": "Clustering is crucial for many computer vision applications such as robust tracking, object detection and segmentation. This work presents a real-time clustering technique that takes advantage of the unique properties of event-based vision sensors. Since event-based sensors trigger events only when the intensity changes, the data is sparse, with low redundancy. Thus, our approach redefines the well-known mean-shift clustering method using asynchronous events instead of conventional frames. The potential of our approach is demonstrated in a multi-target tracking application using Kalman filters to smooth the trajectories. We evaluated our method on an existing dataset with patterns of different shapes and speeds, and a new dataset that we collected. The sensor was attached to the Baxter robot in an eye-in-hand setup monitoring real-world objects in an action manipulation task. Clustering accuracy achieved \u2026"}}
{"id": "tVtWm6P560", "cdate": 1577836800000, "mdate": 1668089925553, "content": {"title": "Reconfigurable Cyber-Physical System for Critical Infrastructure Protection in Smart Cities via Smart Video-Surveillance", "abstract": "Automated surveillance is essential for the protection of Critical Infrastructures (CIs) in future Smart Cities. The dynamic environments and bandwidth requirements demand systems that adapt themselves to react when events of interest occur. We present a reconfigurable Cyber Physical System for the protection of CIs using distributed cloud-edge smart video surveillance. Our local edge nodes perform people detection via Deep Learning. Processing is embedded in high performance SoCs (System-on-Chip) achieving real-time performance ($\\approx$ 100 fps - frames per second) which enables efficiently managing video streams of more cameras source at lower frame rate. Cloud server gathers results from nodes to carry out biometric facial identification, tracking, and perimeter monitoring. A Quality and Resource Management module monitors data bandwidth and triggers reconfiguration adapting the transmitted video resolution. This also enables a flexible use of the network by multiple cameras while maintaining the accuracy of biometric identification. A real-world example shows a reduction of $\\approx$ 75\\% bandwidth use with respect to the no-reconfiguration scenario."}}
{"id": "kwm-NJY2KZV", "cdate": 1577836800000, "mdate": 1668089925553, "content": {"title": "Reconfigurable Cyber-Physical System for Lifestyle Video-Monitoring via Deep Learning", "abstract": "Indoor monitoring of people at their homes has become a popular application in Smart Health. With the advances in Machine Learning and hardware for embedded devices, new distributed approaches for Cyber-Physical Systems (CPSs) are enabled. Also, changing environments and need for cost reduction motivate novel reconfigurable CPS architectures. In this work, we propose an indoor monitoring reconfigurable CPS that uses embedded local nodes (Nvidia Jetson TX2). We embed Deep Learning architectures to address Human Action Recognition. Local processing at these nodes let us tackle some common issues: reduction of data bandwidth usage and preservation of privacy (no raw images are transmitted). Also real-time processing is facilitated since optimized nodes compute only its local video feed. Regarding the reconfiguration, a remote platform monitors CPS qualities and a Quality and Resource Management (QRM) tool sends commands to the CPS core to trigger its reconfiguration. Our proposal is an energy-aware system that triggers reconfiguration based on energy consumption for battery-powered nodes. Reconfiguration reduces up to 22% the local nodes energy consumption extending the device operating time, preserving similar accuracy with respect to the alternative with no reconfiguration."}}
{"id": "Aqem9sZwDC", "cdate": 1577836800000, "mdate": 1668089925558, "content": {"title": "Reconfigurable cyber-physical system for critical infrastructure protection in smart cities via smart video-surveillance", "abstract": ""}}
{"id": "swvZ5T_mVMj", "cdate": 1514764800000, "mdate": 1668089925557, "content": {"title": "Prediction of Manipulation Actions", "abstract": "By looking at a person\u2019s hands, one can often tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor\u2019s intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded videos of subjects performing different manipulation actions on the same object, such as \u201csqueezing\u201d, \u201cflipping\u201d, \u201cwashing\u201d, \u201cwiping\u201d and \u201cscratching\u201d with a sponge. In psychophysical experiments, we evaluated human observers\u2019 skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input image patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets show that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithms to predict in real time what and how a dexterous action is performed."}}
{"id": "QC-YUR93DG", "cdate": 1514764800000, "mdate": 1668089925559, "content": {"title": "Real-Time Clustering and Multi-Target Tracking Using Event-Based Sensors", "abstract": "Clustering is crucial for many computer vision applications such as robust tracking, object detection and segmentation. This work presents a real-time clustering technique that takes advantage of the unique properties of event-based vision sensors. Since event-based sensors trigger events only when the intensity changes, the data is sparse, with low redundancy. Thus, our approach redefines the well-known mean-shift clustering method using asynchronous events instead of conventional frames. The potential of our approach is demonstrated in a multi-target tracking application using Kalman filters to smooth the trajectories. We evaluated our method on an existing dataset with patterns of different shapes and speeds, and a new dataset that we collected. The sensor was attached to the Baxter robot in an eye-in-hand setup monitoring real-world objects in an action manipulation task. Clustering accuracy achieved an F-measure of 0.95, reducing the computational cost by 88% compared to the frame-based method. The average error for tracking was 2.5 pixels and the clustering achieved a consistent number of clusters along time."}}
{"id": "HjZD4JPgOTS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Prediction of Manipulation Actions.", "abstract": "By looking at a person\u2019s hands, one can often tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor\u2019s intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded videos of subjects performing different manipulation actions on the same object, such as \u201csqueezing\u201d, \u201cflipping\u201d, \u201cwashing\u201d, \u201cwiping\u201d and \u201cscratching\u201d with a sponge. In psychophysical experiments, we evaluated human observers\u2019 skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input image patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets show that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithms to predict in real time what and how a dexterous action is performed."}}
