{"id": "8uW47xXl1Nm", "cdate": 1706782139860, "mdate": 1706782139860, "content": {"title": "The devil is in the details: on the pitfalls of vocabulary selection in neural machine translation.", "abstract": "Vocabulary selection, or lexical shortlisting, is a well-known technique to improve latency of Neural Machine Translation models by constraining the set of allowed output words during inference. The chosen set is typically determined by separately trained alignment model parameters, independent of the source-sentence context at inference time. While vocabulary selection appears competitive with respect to automatic quality metrics in prior work, we show that it can fail to select the right set of output words, particularly for semantically non-compositional linguistic phenomena such as idiomatic expressions, leading to reduced translation quality as perceived by humans. Trading off latency for quality by increasing the size of the allowed set is often not an option in real-world scenarios. We propose a model of vocabulary selection, integrated into the neural translation model, that predicts the set of allowed output words from contextualized encoder representations. This restores translation quality of an unconstrained system, as measured by human evaluations on WMT newstest2020 and idiomatic expressions, at an inference latency competitive with alignment-based selection using aggressive thresholds, thereby removing the dependency on separately trained alignment models."}}
{"id": "xJd_nzd5jh", "cdate": 1706522415386, "mdate": 1706522415386, "content": {"title": "Improving the quality trade-off for neural machine translation multi-domain adaptation", "abstract": "Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original translation system. We find that we can improve over the performance trade-off offered by Elastic Weight Consolidation with a relatively simple data mixing strategy. At comparable performance on the new domains, catastrophic forgetting is mitigated significantly on strong WMT baselines. Combining both approaches improves the Pareto frontier on this task."}}
{"id": "Wf2jYZwuOG", "cdate": 1640995200000, "mdate": 1711722084422, "content": {"title": "A Hybrid Approach to Cross-lingual Product Review Summarization", "abstract": ""}}
{"id": "CJvpagEbCK", "cdate": 1640995200000, "mdate": 1705933173576, "content": {"title": "The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation", "abstract": "Tobias Domhan, Eva Hasler, Ke Tran, Sony Trenous, Bill Byrne, Felix Hieber. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "0EOUJgFgQwY", "cdate": 1640995200000, "mdate": 1665021342143, "content": {"title": "Sockeye 3: Fast Neural Machine Translation with PyTorch", "abstract": "Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine Translation (NMT). Now based on PyTorch, Sockeye 3 provides faster model implementations and more advanced features with a further streamlined codebase. This enables broader experimentation with faster iteration, efficient training of stronger and faster models, and the flexibility to move new ideas quickly from research to production. When running comparable models, Sockeye 3 is up to 126% faster than other PyTorch implementations on GPUs and up to 292% faster on CPUs. Sockeye 3 is open source software released under the Apache 2.0 license."}}
{"id": "Tipcyi4tZP", "cdate": 1577836800000, "mdate": 1711722084421, "content": {"title": "Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations", "abstract": ""}}
{"id": "Bkle6T4YvB", "cdate": 1569439159893, "mdate": null, "content": {"title": "From English to Foreign Languages: Transferring Pre-trained Language Models", "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing."}}
{"id": "Bkl1uWb0Z", "cdate": 1518730168361, "mdate": null, "content": {"title": "Inducing Grammars with and for Neural Machine Translation", "abstract": "Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru."}}
