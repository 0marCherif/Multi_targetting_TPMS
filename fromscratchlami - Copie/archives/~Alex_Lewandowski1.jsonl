{"id": "fUhxuop_Q1r", "cdate": 1632875607123, "mdate": null, "content": {"title": "Disentangling Generalization in Reinforcement Learning", "abstract": "  Generalization in Reinforcement Learning (RL) is usually measured according to\n  concepts from supervised learning. Unlike a supervised learning model however,\n  an RL agent must generalize across states, actions and observations from\n  limited reward-based feedback. We propose to measure an RL agent's capacity to\n  generalize by evaluating it in a contextual decision process that combines a\n  tabular environment with observations from a supervised learning dataset. The\n  resulting environment, while simple, necessitates function approximation for\n  state abstraction and provides ground-truth labels for optimal policies and\n  value functions. The ground truth labels provided by our environment enable us\n  to characterize generalization in RL across different axes: state-space,\n  observation-space and action-space. Putting this method to work, we combine\n  the MNIST dataset with various gridworld environments to rigorously evaluate\n  generalization of DQN and QR-DQN in state, observation and action spaces for\n  both online and offline learning. Contrary to previous reports about common\n  regularization methods, we find that dropout does not improve observation\n  generalization. We find, however, that dropout improves action generalization.\n  Our results also corroborate recent findings that QR-DQN is able to generalize\n  to new observations better than DQN in the offline setting. This success does\n  not extend to state generalization, where DQN is able to generalize better\n  than QR-DQN. These findings demonstrate the need for careful consideration\n  of generalization in RL, and we hope that this line of research will continue\n  to shed light on generalization claims in the literature.\n"}}
