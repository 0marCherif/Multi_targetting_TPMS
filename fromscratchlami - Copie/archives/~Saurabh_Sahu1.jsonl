{"id": "55IdeIjTJ5", "cdate": 1673371450773, "mdate": 1673371450773, "content": {"title": "Leveraging Local Temporal Information for Multimodal Scene Classification", "abstract": "Robust video scene classification models should capture the spatial (pixel-wise) and temporal (frame-wise) characteristics of a video effectively. Transformer models with self-attention which are designed to get contextualized representations for individual tokens given a sequence of tokens, are becoming increasingly popular in many computer vision tasks. However, the use of Transformer based models for video under-standing is still relatively unexplored. Moreover, these models fail to exploit the strong temporal relationships between the neighboring video frames to get potent frame-level representations. In this paper, we propose a novel self-attention block that leverages both local and global temporal relation-ships between the video frames to obtain better contextualized representations for the individual frames. This enables the model to understand the video at various granularities. We illustrate the performance of our models on the large-scale YoutTube-8M data set on the task of video categorization and further analyze the results to showcase improvement."}}
{"id": "Lnmu7mHBS9T", "cdate": 1673371378263, "mdate": 1673371378263, "content": {"title": "Cross-modal Non-linear Guided Attention and Temporal Coherence in Multi-modal Deep Video Models", "abstract": "Videos have data in multiple modalities, e.g., audio, video, text (captions). Understanding and modeling the interaction between different modalities is key for video analysis tasks like categorization, object detection, activity recognition, etc. However, data modalities are not always correlated --- so, learning when modalities are correlated and using that to guide the influence of one modality on the other is crucial. Another salient feature of videos is the coherence between successive frames due to continuity of video and audio, a property that we refer to as temporal coherence. We show how using non-linear guided cross-modal signals and temporal coherence can improve the performance of multi-modal machine learning (ML) models for video analysis tasks like categorization. Our experiments on the large-scale YouTube-8M dataset show how our approach significantly outperforms state-of-the-art multi-modal ML models for video categorization. The model trained on the YouTube-8M dataset also showed good performance on an internal dataset of video segments from actual Samsung TV Plus channels without retraining or fine-tuning, showing the generalization capabilities of our model."}}
{"id": "zdMZHQyofR", "cdate": 1673371241010, "mdate": 1673371241010, "content": {"title": "Modeling Feature Representations for Affective Speech Using Generative Adversarial Networks", "abstract": "Emotion recognition is a classic field of research with a typical setup extracting features and feeding them through a classifier for prediction. On the other hand, generative models jointly capture the distributional relationship between emotions and the feature profiles. Recently, Generative Adversarial Networks (GANs) have surfaced as a new class of generative models and have shown considerable success in modeling distributions in the fields of computer vision and natural language understanding. In this article, we experiment with variants of GAN architectures to generate feature vectors corresponding to an emotion in two ways: (i) A generator is trained with samples from a mixture prior. Each mixture component corresponds to an emotional class and can be sampled to generate features from the corresponding emotion. (ii) A one-hot vector corresponding to an emotion can be explicitly used to generate the features. We perform analysis on such models and also propose different metrics used to measure the performance of the GAN models in their ability to generate realistic synthetic samples. Apart from evaluation on a given dataset of interest, we perform a cross-corpus study where we study the utility of the synthetic samples as additional training data in low resource conditions."}}
{"id": "oAd2vbgy0L", "cdate": 1673371122975, "mdate": null, "content": {"title": "An Affect Prediction Approach through Depression Severity Parameter Incorporation in Neural Networks", "abstract": "Humans use emotional expressions to communicate their in-\nternal affective states. These behavioral expressions are often\nmulti-modal (e.g. facial expression, voice and gestures) and re-\nsearchers have proposed several schemes to predict the latent\naffective states based on these expressions. The relationship\nbetween the latent affective states and their expression is hy-\npothesized to be affected by several factors; depression disorder\nbeing one of them. Despite a wide interest in affect prediction,\nand several studies linking the effect of depression on affective\nexpressions, only a limited number of affect prediction models\naccount for the depression severity. In this work, we present\na novel scheme that incorporates depression severity as a pa-\nrameter in Deep Neural Networks (DNNs). In order to predict\naffective dimensions for an individual at hand, our scheme al-\nters the DNN activation function based on the subject\u2019s depres-\nsion severity. We perform experiments on affect prediction in\ntwo different sessions of the Audio-Visual Depressive language\nCorpus, which involves patients with varying degree of depres-\nsion. Our results show improvements in arousal and valence\nprediction on both the sessions using the proposed DNN model-\ning. We also present analysis of the impact of such an alteration\nin DNNs during training and testing."}}
{"id": "q307XGYBQM", "cdate": 1673371010625, "mdate": 1673371010625, "content": {"title": "Speech Features for Depression Detection", "abstract": "In this paper we discuss speech features that are useful in the\ndetection of depression. Neuro-physiological changes\nassociated with depression affect motor coordination and can\ndisrupt articulatory precision in speech. We use the Mundt\ndatabase and focus on six speakers in the database that\ntransitioned between being depressed and not depressed based\non their Hamilton depression scores. We quantify the degree\nof breathiness, jitter and shimmer computed from an AMDF\nbased parameter. Measures from sustained vowels spoken in\nisolation show that all of these attributes can increase when a\nperson is depressed. In this study, we focused on using\nfeatures from free-flowing speech to classify the depressed\nstate of an individual. To do so we looked at vowel regions\nthat look the most like sustained vowels. We train an SVM for\neach speaker and do a speaker dependent classification of the\ntest speech frames. Using the AMDF based feature we got a\nbetter accuracy (62-87% frame-wise accuracy for 5 out of 6\nspeakers) for most speakers than 13 dimensional MFCC along\nwith its velocity and acceleration coefficients. Using the\nAMDF based feature, we also trained a speaker independent\nSVM which gave an average accuracy of 77.8% for utterance\nbased classification."}}
{"id": "kZVVY0A63G", "cdate": 1673370669513, "mdate": 1673370669513, "content": {"title": "Smoothing Model Predictions Using Adversarial Training Procedures for Speech Based Emotion Recognition", "abstract": "Training discriminative classifiers involves learning a conditional distribution p(y i |x i ), given a set of feature vectors x i and the corresponding labels y i , i=1...N. For a classifier to be generalizable and not overfit to training data, the resulting conditional distribution p(y i |x i ) is desired to be smoothly varying over the inputs x i . Adversarial training procedures enforce this smoothness using manifold regularization techniques. Manifold regularization makes the model's output distribution more robust to local perturbation added to a datapoint x i . In this paper, we experiment with the application of adversarial training procedures to increase the accuracy of a deep neural network based emotion recognition system using speech cues. Specifically, we investigate two training procedures: (i) adversarial training where we determine the adversarial direction based on the given labels for the training data and, (ii) virtual adversarial training where we determine the adversarial direction based only on the output distribution of the training data. We demonstrate the efficacy of adversarial training procedures by performing a k-fold cross validation experiment on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and a cross-corpus performance analysis on three separate corpora. Results show improvement over a purely supervised approach, as well as better generalization capability to cross-corpus settings.\n"}}
{"id": "CD8Zs35I1V", "cdate": 1673370569535, "mdate": 1673370569535, "content": {"title": "Semi-Supervised and Transfer Learning Approaches for Low Resource Sentiment Classification", "abstract": "Sentiment classification involves quantifying the affective reaction of a human to a document, media item or an event. Although researchers have investigated several methods to reliably infer sentiment from lexical, speech and body language cues, training a model with a small set of labeled datasets is still a challenge. For instance, in expanding sentiment analysis to new languages and cultures, it may not always be possible to obtain comprehensive labeled datasets. In this paper, we investigate the application of semi- supervised and transfer learning methods to improve performances on low resource sentiment classification tasks. We experiment with extracting dense feature representations, pre-training and manifold regularization in enhancing the performance of sentiment classification systems. Our goal is a coherent implementation of these methods and we evaluate the gains achieved by these methods in matched setting involving training and testing on a single corpus setting as well as two cross corpora settings. In both the cases, our experiments demonstrate that the proposed methods can significantly enhance the model performance against a purely supervised approach, particularly in cases involving a handful of training data.\n"}}
{"id": "CtAC_g9-QP", "cdate": 1673370426060, "mdate": 1673370426060, "content": {"title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription", "abstract": "In this paper we plan to leverage multi-modal learning and au-\ntomated speech recognition (ASR) systems toward building a\nspeech-only emotion recognition model. Previous studies have\nshown that emotion recognition models using only acoustic fea-\ntures do not perform satisfactorily in detecting valence level.\nText analysis has been shown to be helpful for sentiment classi-\nfication. We compared classification accuracies obtained from\nan audio-only model, a text-only model and a multi-modal sys-\ntem leveraging both by performing a cross-validation analysis\non IEMOCAP dataset. Confusion matrices show it\u2019s the va-\nlence level detection thats being improved by incorporating tex-\ntual information. In the second stage of experiments, we used\ntwo ASR application programming interfaces (APIs) to get the\ntranscriptions. We compare the performances of multi-modal\nsystems using the ASR transcriptions with each other and with\nthat of one using ground truth transcription. We analyze the\nconfusion matrices to determine the effect of using ASR tran-\nscriptions instead of ground truth ones on class-wise accuracies.\nWe investigate the generalisability of such a model by perform-\ning a cross-corpus study."}}
{"id": "J47FEW8dPL", "cdate": 1673370326469, "mdate": 1673370326469, "content": {"title": "On Enhancing Speech Emotion Recognition using Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) have gained a lot of\nattention from machine learning community due to their abil-\nity to learn and mimic an input data distribution. GANs consist\nof a discriminator and a generator working in tandem playing\na min-max game to learn a target underlying data distribution;\nwhen fed with data-points sampled from a simpler distribution\n(like uniform or Gaussian distribution). Once trained, they al-\nlow synthetic generation of examples sampled from the target\ndistribution. We investigate the application of GANs to gener-\nate synthetic feature vectors used for speech emotion recogni-\ntion. Specifically, we investigate two set ups: (i) a vanilla GAN\nthat learns the distribution of a lower dimensional representa-\ntion of the actual higher dimensional feature vector and, (ii) a\nconditional GAN that learns the distribution of the higher di-\nmensional feature vectors conditioned on the labels or the emo-\ntional class to which it belongs. As a potential practical applica-\ntion of these synthetically generated samples, we measure any\nimprovement in a classifier\u2019s performance when the synthetic\ndata is used along with real data for training. We perform cross\nvalidation analyses followed by a cross-corpus study."}}
{"id": "A_uqfR3UqX", "cdate": 1673370240002, "mdate": 1673370240002, "content": {"title": "Adversarial auto-encoders for speech based emotion recognition", "abstract": "Recently, generative adversarial networks and adversarial auto-\nencoders have gained a lot of attention in machine learning\ncommunity due to their exceptional performance in tasks such\nas digit classification and face recognition. They map the auto-\nencoder\u2019s bottleneck layer output (termed as code vectors) to\ndifferent noise Probability Distribution Functions (PDFs), that\ncan be further regularized to cluster based on class informa-\ntion. In addition, they also allow a generation of synthetic sam-\nples by sampling the code vectors from the mapped PDFs. In-\nspired by these properties, we investigate the application of ad-\nversarial auto-encoders to the domain of emotion recognition.\nSpecifically, we conduct experiments on the following two as-\npects: (i) their ability to encode high dimensional feature vec-\ntor representations for emotional utterances into a compressed\nspace (with a minimal loss of emotion class discriminability in\nthe compressed space), and (ii) their ability to regenerate syn-\nthetic samples in the original feature space, to be later used for\npurposes such as training emotion recognition classifiers. We\ndemonstrate promise of adversarial auto-encoders with regards\nto these aspects on the Interactive Emotional Dyadic Motion\nCapture (IEMOCAP) corpus and present our analysis."}}
