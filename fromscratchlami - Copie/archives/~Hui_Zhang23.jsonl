{"id": "a1p3y4pQd3", "cdate": 1668632206427, "mdate": 1668632206427, "content": {"title": "Chinese Shadow Puppetry with an Interactive Interface Using the Kinect Sensor", "abstract": "This paper addresses the problem of using body gestures to control the Chinese shadow puppets with the Microsoft Kinect sensor. By analyzing the motion of the actors in the Chinese famous drama, Wusong Fights the Tiger, we propose a general framework for controlling two shadow puppets, a human model and an animal model. A performer can conduct simple actions such as turning the head, stretching the arms or kicking the legs. However, it is more difficult for a normal performer to simulate more complicated movements, for example, back flips and splits. Therefore we define some special postures to represent these difficult movements. Besides, in order to be compatible with the Chinese drama style, we use water color to paint the background scenery and the foreground characters. We show some preliminary results which demonstrate the effectiveness of this work."}}
{"id": "7u6kcI1Ogh", "cdate": 1668630674144, "mdate": 1668630674144, "content": {"title": "Image-based 3D Shape Reconstruction of Wind Turbine from Multiple Views", "abstract": "This paper addresses the problem of reconstructing depth and silhouette images of wind turbine from its photos of multiple views using deep learning approaches, which aims for wind turbine blade fault diagnosis. Some previous multi-view based methods have extracted each photo\u2019s silhouette and combined them into separate channels as the input of convolution; others use LSTM to combine a series of views for reconstruction. These approaches inevitably need a fixed number of views and the output result is divergent if the order of the input views is changed. So, we refer to a network, SiDeNet (Wiles and Zisserman, Learning to predict 3d surfaces of sculptures from single and multiple views. Int J Comp Vision, 2018), which has a flexible number of input views and will not be affected by the input order. It integrates both viewpoint and image information from each view to learn a latent 3D shape representation and use it to predict the depth of wind turbine at input views. Also, this representation could generalize to the silhouette of unseen views. We make the following contributions to SiDeNet: improving the resolution of predicted images by deepening network structure, adopting 6D camera pose to increase the degrees of freedom of viewpoint to capture a wider range of views, optimizing the loss function of silhouette by applying weights on edge points, and implementing silhouette refinement with point-wise optimizing. Additionally, we conduct a set of prediction experiments and prove the network\u2019s generalization ability to unseen views. Evaluating predicted results on a realistic wind turbine dataset confirms the high performance of the network on both given views and unseen views."}}
{"id": "yPU724m6GT", "cdate": 1668630445021, "mdate": 1668630445021, "content": {"title": "A Fast and Efficient Network for Single Image Shadow Detection", "abstract": "Shadows in images can degrade the performance of many applications. In this paper, we propose a novel multi-level feature-aware network, called TransShadow, which uses Transformer to capture both local and global context from a single image for shadow detection. Specifically, we design a multi-level feature-aware module, where multi-level features are selected and processed by the Transformer to distinguish shadowed and non-shadowed regions. To further utilize the remaining feature levels, progressive upsampling with skip connections is proposed to fuse more information for shadow detection. Experimental results show that our approach achieves comparative performance as the state-of-the-art method on benchmark datasets SBU and ISTD with the smallest model size and fastest inference speed. More importantly, our model shows the best generalization performance on the benchmark dataset UCF."}}
{"id": "W3O6Rsxtd53", "cdate": 1668630367719, "mdate": 1668630367719, "content": {"title": "RMLANet: Random Multi-Level Attention Network for Shadow Detection", "abstract": "This paper addresses the problem of shadow detection from a single image. Previous approaches have shown that exploiting both global and local contexts in deep convolutional neural network layers can greatly improve performance. However, multi-level contexts remain underexplored. To achieve this, we propose RMLANet, a novel Random Multi-Level Attention Network. Specifically, we leverage shuffled multi-level features simultaneously with guiding features, and employ the transformer to capture global context. Furthermore, to reduce the computational and memory overhead caused by the self-attention mechanism in the vanilla transformer, we propose a random sampling strategy to reduce the number of inputs to the transformer. This is motivated by observing local consistency in images, which suggests that dense attention is unnecessary. Extensive experimental results demonstrate that our method outperforms current state-of-the-art methods on three widely used benchmark datasets SBU, ISTD and UCF."}}
{"id": "f1wTq62YaO4", "cdate": 1668629973173, "mdate": 1668629973173, "content": {"title": "PSP-MVSNet: Deep Patch-based Similarity Perceptual Multi-View Stereo", "abstract": "This paper proposes PSP-MVSNet for depth inference problem in multi-view stereo (MVS). We first introduce a novel patch-based similarity perceptual (PSP) module for effectively constructing 3D cost volume. Unlike previous methods that leverage variance-based operators to fuse feature volumes of different views, our method leverages a cosine similarity measure to calculate matching scores for pairs of deep feature vectors and then treats these scores as weights for constructing the 3D cost volume. This is based on an important observation that many performance degradation factors, e.g., illumination changes or occlusions, will lead to pixel differences between multi-view images. We demonstrate that a patch-based cosine similarity can be used as explicit supervision for feature learning and can help speed up convergence. Furthermore, To adaptively set different depth ranges for different pixels, we extend an existing dynamic depth range searching method with a simple yet effective improvement. We can use this improved searching method to train our model in an end-to-end manner and further improve the performance of our method. Experimental results show that our method achieves state-of-the-art performance on the DTU dataset and comparative results on the intermediate set of Tanks and Temples dataset."}}
{"id": "mrK-d3D8x0", "cdate": 1668629878449, "mdate": 1668629878449, "content": {"title": "Camera Auto-calibration from the Steiner Conic of Fundamental Matrix", "abstract": "This paper addresses the problem of camera auto-calibration from the fundamental matrix under general motion. The fundamental matrix can be decomposed into a symmetric part (a Steiner conic) and a skew-symmetric part (a fixed point), which we find useful for fully calibrating camera parameters. We first obtain a fixed line from the image of the symmetric, skew-symmetric parts of the fundamental matrix and the image of the absolute conic. Then the properties of this fixed line are presented and proved, from which new constraints on general eigenvectors between the Steiner conic and the image of the absolute conic are derived. We thus propose a method to fully calibrate the camera. First, the three camera intrinsic parameters, i.e., the two focal lengths and the skew, can be solved from our new constraints on the imaged absolute conic obtained from at least three images. On this basis, we can initialize and then iteratively restore the optimal pair of projection centers of the Steiner conic, thereby obtaining the corresponding vanishing lines and images of circular points. Finally, all five camera parameters are fully calibrated using images of circular points obtained from at least three images. Experimental results on synthetic and real data demonstrate that our method achieves state-of-the-art performance in terms of accuracy."}}
{"id": "PMAhXmBi4Z", "cdate": 1668629601110, "mdate": 1668629601110, "content": {"title": "Homography Estimation from the Common Self-polar Triangle of Separate Ellipses", "abstract": "How to avoid ambiguity is a challenging problem for conic-based homography estimation. In this paper, we address the problem of homography estimation from two separate ellipses. We find that any two ellipses have a unique common self-polar triangle, which can provide three line correspondences. Furthermore, by investigating the location features of the common self-polar triangle, we show that one vertex of the triangle lies outside of both ellipses, while the other two vertices lies inside the ellipses separately. Accordingly, one more line correspondence can be obtained from the intersections of the conics and the common self-polar triangle. Therefore, four line correspondences can be obtained based on the common self-polar triangle, which can provide enough constraints for the homography estimation. The main contributions in this paper include: (1) A new discovery on the location features of the common self-polar triangle of separate ellipses. (2) A novel approach for homography estimation. Simulate experiments and real experiments are conducted to demonstrate the feasibility and accuracy of our approach."}}
{"id": "NGE_hRRwoFG", "cdate": 1668629358191, "mdate": null, "content": {"title": "The Common Self-polar Triangle of Concentric Circles and Its Application to Camera Calibration", "abstract": "In projective geometry, the common self-polar triangle has often been used to discuss the position relationship of two planar conics. However, there are few researches on the properties of the common self-polar triangle, especially when the two planar conics are special conics. In this paper, we explore the properties of the common self-polar triangle, when the two conics happen to be concentric circles. We show there exist infinite many common self-polar triangles of two concentric circles, and provide a method to locate the vertices of these triangles. By investigating all these triangles, we find that they encode two important properties. The first one is all triangles share one common vertex, and the opposite side of the common vertex lies on the same line, which are the circle center and the line at the infinity of the support plane. The second is all triangles are right triangles. Based on these two properties, the imaged circle center and the varnishing line of support plane can be recovered simultaneously, and many conjugate pairs on vanishing line can be obtained. These allow to induce good constraints on the image of absolute conic. We evaluate two calibration algorithms, whereby accurate results are achieved. The main contribution of this paper is that we initiate a new perspective to look into circle-based camera calibration problem. We believe that other calibration methods using different circle patterns can benefit from this perspective, especially for the patterns which involve more than two circles."}}
{"id": "mMb3cu4BqDe", "cdate": 1640995200000, "mdate": 1666991903273, "content": {"title": "PSP-MVSNet: Deep Patch-Based Similarity Perceptual for Multi-view Stereo Depth Inference", "abstract": "This paper proposes PSP-MVSNet for depth inference problem in multi-view stereo (MVS). We first introduce a novel patch-based similarity perceptual (PSP) module for effectively constructing 3D cost volume. Unlike previous methods that leverage variance-based operators to fuse feature volumes of different views, our method leverages a cosine similarity measure to calculate matching scores for pairs of deep feature vectors and then treats these scores as weights for constructing the 3D cost volume. This is based on an important observation that many performance degradation factors, e.g., illumination changes or occlusions, will lead to pixel differences between multi-view images. We demonstrate that a patch-based cosine similarity can be used as explicit supervision for feature learning and can help speed up convergence. Furthermore, To adaptively set different depth ranges for different pixels, we extend an existing dynamic depth range searching method with a simple yet effective improvement. We can use this improved searching method to train our model in an end-to-end manner and further improve the performance of our method. Experimental results show that our method achieves state-of-the-art performance on the DTU dataset and comparative results on the intermediate set of Tanks and Temples dataset."}}
{"id": "ilGqlR3mb", "cdate": 1640995200000, "mdate": 1668624921054, "content": {"title": "Camera Auto-calibration from the Steiner Conic of the Fundamental Matrix", "abstract": "This paper addresses the problem of camera auto-calibration from the fundamental matrix under general motion. The fundamental matrix can be decomposed into a symmetric part (a Steiner conic) and a skew-symmetric part (a fixed point), which we find useful for fully calibrating camera parameters. We first obtain a fixed line from the image of the symmetric, skew-symmetric parts of the fundamental matrix and the image of the absolute conic. Then the properties of this fixed line are presented and proved, from which new constraints on general eigenvectors between the Steiner conic and the image of the absolute conic are derived. We thus propose a method to fully calibrate the camera. First, the three camera intrinsic parameters, i.e., the two focal lengths and the skew, can be solved from our new constraints on the imaged absolute conic obtained from at least three images. On this basis, we can initialize and then iteratively restore the optimal pair of projection centers of the Steiner conic, thereby obtaining the corresponding vanishing lines and images of circular points. Finally, all five camera parameters are fully calibrated using images of circular points obtained from at least three images. Experimental results on synthetic and real data demonstrate that our method achieves state-of-the-art performance in terms of accuracy."}}
