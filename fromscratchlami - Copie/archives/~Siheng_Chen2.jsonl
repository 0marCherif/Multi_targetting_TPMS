{"id": "0cJ-97zBaA", "cdate": 1672215614781, "mdate": 1672215614781, "content": {"title": "Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction", "abstract": "3D skeleton-based action recognition and motion prediction are two essential problems of human activity understanding. In\nmany previous works: 1) they studied two tasks separately, neglecting internal correlations; 2) they did not capture sufficient relations\ninside the body. To address these issues, we propose a symbiotic model to handle two tasks jointly; and we propose two scales of\ngraphs to explicitly capture relations among body-joints and body-parts. Together, we propose symbiotic graph neural networks, which\ncontains a backbone, an action-recognition head, and a motion-prediction head. Two heads are trained jointly and enhance each other.\nFor the backbone, we propose multi-branch multi-scale graph convolution networks to extract spatial and temporal features. The\nmulti-scale graph convolution networks are based on joint-scale and part-scale graphs. The joint-scale graphs contain actional graphs,\ncapturing action-based relations, and structural graphs, capturing physical constraints. The part-scale graphs integrate body-joints to\nform specific parts, representing high-level relations. Moreover, dual bone-based graphs and networks are proposed to learn\ncomplementary features. We conduct extensive experiments for skeleton-based action recognition and motion prediction with four\ndatasets, NTU-RGB+D, Kinetics, Human3.6M, and CMU Mocap. Experiments show that our symbiotic graph neural networks achieve\nbetter performances on both tasks compared to the state-of-the-art methods. The code is relased at github.com/limaosen0/Sym-GNN"}}
{"id": "XxQLZtN3lV", "cdate": 1672215317261, "mdate": 1672215317261, "content": {"title": "Actional-structural graph convolutional networks for skeleton-based action recognition", "abstract": "Action recognition with skeleton data has recently attracted much attention in computer vision. Previous studies are mostly based on fixed skeleton graphs, only capturing local physical dependencies among joints, which may miss implicit joint correlations. To capture richer dependencies, we introduce an encoder-decoder structure, called A-link inference module, to capture action-specific latent dependencies, i.e. actional links, directly from actions. We also extend the existing skeleton graphs to represent higherorder dependencies, i.e. structural links. Combing the two types of links into a generalized skeleton graph, we further propose the actional-structural graph convolution network (AS-GCN), which stacks actional-structural graph convolution and temporal convolution as a basic building block, to\nlearn both spatial and temporal features for action recognition. A future pose prediction head is added in parallel to the recognition head to help capture more detailed action patterns through self-supervision. We validate AS-GCN in action recognition using two skeleton data sets, NTURGB+D and Kinetics. The proposed AS-GCN achieves consistently large improvement compared to the state-of-the-art\nmethods. As a side product, AS-GCN also shows promising results for future pose prediction. Our code is available at https://github.com/limaosen0/AS-GCN."}}
{"id": "3_589dGnLi", "cdate": 1672214200290, "mdate": 1672214200290, "content": {"title": "Learning on attribute-missing graphs", "abstract": "Graphs with complete node attributes have been widely explored recently. While in practice, there is a graph where attributes of only partial nodes could be available and those of the others might be entirely missing. This attribute-missing graph is related to numerous real-world applications and there are limited studies investigating the corresponding learning problems. Existing graph learning methods including the popular GNN cannot provide satisfied learning performance since they are not specified for attribute-missing graphs. Thereby, designing a new GNN for these graphs is a burning issue to the graph learning community. In this article, we make a <italic>shared-latent space</italic> assumption on graphs and develop a novel distribution matching-based GNN called structure-attribute transformer (SAT) for attribute-missing graphs. SAT leverages structures and attributes in a decoupled scheme and achieves the joint distribution modeling of structures and attributes by distribution matching techniques. It could not only perform the link prediction task but also the newly introduced <italic>node attribute completion</italic> task. Furthermore, practical measures are introduced to quantify the performance of <italic>node attribute completion</italic>. Extensive experiments on seven real-world datasets indicate SAT shows better performance than other methods on both link prediction and <italic>node attribute completion</italic> tasks."}}
{"id": "1tfhQMOSmx", "cdate": 1667338229034, "mdate": 1667338229034, "content": {"title": "Unsupervised Joint 3D Object Model Learning and 6D Pose Estimation for Depth-Based Instance Segmentation", "abstract": "In this work, we propose a novel unsupervised approach\nto jointly learn the 3D object model and estimate the 6D\nposes of multiple instances of a previously unknown object, with applications to depth-based instance segmentation.\nThe inputs are depth images, and the learned object model\nis represented by a 3D point cloud. Traditional 6D pose\nestimation approaches are not sufficient to address this unsupervised problem, in which neither a CAD model of the\nobject nor the ground-truth 6D poses of its instances are\navailable during training. To solve this problem, we propose\nto jointly optimize the model learning and pose estimation\nin an end-to-end deep learning framework. Specifically, our\nnetwork produces a 3D object model and a list of rigid transformations of this model to generate instances, which when\nrendered must match the observed 3D point cloud to minimize the Chamfer distance. To render the set of instance\npoint clouds with occlusions, the network automatically removes the occluded points in a given camera view. Extensive\nexperiments evaluate our technique on several object models and varying numbers of instances. We demonstrate the\napplication of our method to instance segmentation of depth\nimages of small bins of industrial parts. Compared with\npopular baselines for instance segmentation, our model not\nonly demonstrates competitive performance, but also learns\na 3D object model that is represented as a 3D point cloud"}}
{"id": "dLL4KXzKUpS", "cdate": 1652737755755, "mdate": null, "content": {"title": "Where2comm: Communication-Efficient Collaborative Perception via Spatial Confidence Maps", "abstract": "Multi-agent collaborative perception could significantly upgrade the perception performance by enabling agents to share complementary information with each other through communication. It inevitably results in a fundamental trade-off between perception performance and communication bandwidth. To tackle this bottleneck issue, we propose a spatial confidence map, which reflects the spatial heterogeneity of perceptual information. It empowers agents to only share spatially sparse, yet perceptually critical information, contributing to where to communicate. Based on this novel spatial confidence map, we propose Where2comm, a communication-efficient collaborative perception framework. Where2comm has two distinct advantages: i) it considers pragmatic compression and uses less communication to achieve higher perception performance by focusing on perceptually critical areas; and ii) it can handle varying communication bandwidth by dynamically adjusting spatial areas involved in communication. To evaluate Where2comm, we consider 3D object detection in both real-world and simulation scenarios with two modalities (camera/LiDAR) and two agent types (cars/drones) on four datasets: OPV2V, V2X-Sim, DAIR-V2X, and our original CoPerception-UAVs. Where2comm consistently outperforms previous methods; for example, it achieves more than $100,000 \\times$ lower communication volume and still outperforms DiscoNet and V2X-ViT on OPV2V. Our code is available at~\\url{https://github.com/MediaBrain-SJTU/where2comm}."}}
{"id": "xsD1AF3fnq", "cdate": 1640995200000, "mdate": 1668238561574, "content": {"title": "Dynamic-Group-Aware Networks for Multi-Agent Trajectory Prediction with Relational Reasoning", "abstract": "Demystifying the interactions among multiple agents from their past trajectories is fundamental to precise and interpretable trajectory prediction. However, previous works mainly consider static, pair-wise interactions with limited relational reasoning. To promote more comprehensive interaction modeling and relational reasoning, we propose DynGroupNet, a dynamic-group-aware network, which can i) model time-varying interactions in highly dynamic scenes; ii) capture both pair-wise and group-wise interactions; and iii) reason both interaction strength and category without direct supervision. Based on DynGroupNet, we further design a prediction system to forecast socially plausible trajectories with dynamic relational reasoning. The proposed prediction system leverages the Gaussian mixture model, multiple sampling and prediction refinement to promote prediction diversity, training stability and trajectory smoothness, respectively. Extensive experiments show that: 1)DynGroupNet can capture time-varying group behaviors, infer time-varying interaction category and interaction strength during trajectory prediction without any relation supervision on physical simulation datasets; 2)DynGroupNet outperforms the state-of-the-art trajectory prediction methods by a significant improvement of 22.6%/28.0%, 26.9%/34.9%, 5.1%/13.0% in ADE/FDE on the NBA, NFL Football and SDD datasets and achieve the state-of-the-art performance on the ETH-UCY dataset."}}
{"id": "uuJU978wqpq", "cdate": 1640995200000, "mdate": 1668238561483, "content": {"title": "A 3D Mesh-Based Lifting-and-Projection Network for Human Pose Transfer", "abstract": "Human pose transfer has typically been modeled as a 2D image-to-image translation problem. This formulation ignores the human body shape prior in 3D space and inevitably causes implausible artifacts, especially when facing occlusion. To address this issue, we propose a <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">lifting-and-projection</i> framework to perform pose transfer in the 3D mesh space. The core of our framework is a foreground generation module, that consists of two novel networks: a lifting-and-projection network (LPNet) and an appearance detail compensating network (ADCNet). To leverage the human body shape prior, LPNet exploits the topological information of the body mesh to learn an expressive visual representation for the target person in the 3D mesh space. To preserve texture details, ADCNet is further introduced to enhance the feature produced by LPNet with the source foreground image. Such design of the foreground generation module enables the model to better handle difficult cases such as those with occlusions. Experiments on the iPER and Fashion datasets empirically demonstrate that the proposed lifting-and-projection framework is effective and outperforms the existing image-to-image-based and mesh-based methods on human pose transfer task in both self-transfer and cross-transfer settings."}}
{"id": "tPRo9hRWTc", "cdate": 1640995200000, "mdate": 1668238561092, "content": {"title": "V2X-Sim: A Virtual Collaborative Perception Dataset for Autonomous Driving", "abstract": "Vehicle-to-everything (V2X) communication techniques enable the collaboration between vehicles and many other entities in the neighboring environment, which could fundamentally improve the perception system for autonomous driving. However, the lack of a public dataset significantly restricts the research progress of collaborative perception. To fill this gap, we present V2X-Sim, a comprehensive simulated multi-agent perception dataset for V2X-aided autonomous driving. V2X-Sim provides: (1) \\hl{multi-agent} sensor recordings from the road-side unit (RSU) and multiple vehicles that enable collaborative perception, (2) multi-modality sensor streams that facilitate multi-modality perception, and (3) diverse ground truths that support various perception tasks. Meanwhile, we build an open-source testbed and provide a benchmark for the state-of-the-art collaborative perception algorithms on three tasks, including detection, tracking and segmentation. V2X-Sim seeks to stimulate collaborative perception research for autonomous driving before realistic datasets become widely available. Our dataset and code are available at \\url{https://ai4ce.github.io/V2X-Sim/}."}}
{"id": "s8lcWQEPYT1", "cdate": 1640995200000, "mdate": 1668238561175, "content": {"title": "Learning on Attribute-Missing Graphs", "abstract": "Graphs with complete node attributes have been widely explored recently. While in practice, there is a graph where attributes of only partial nodes could be available and those of the others might be entirely missing. This attribute-missing graph is related to numerous real-world applications and there are limited studies investigating the corresponding learning problems. Existing graph learning methods including the popular GNN cannot provide satisfied learning performance since they are not specified for attribute-missing graphs. Thereby, designing a new GNN for these graphs is a burning issue to the graph learning community. In this article, we make a <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">shared-latent space</i> assumption on graphs and develop a novel distribution matching-based GNN called structure-attribute transformer (SAT) for attribute-missing graphs. SAT leverages structures and attributes in a decoupled scheme and achieves the joint distribution modeling of structures and attributes by distribution matching techniques. It could not only perform the link prediction task but also the newly introduced <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">node attribute completion</i> task. Furthermore, practical measures are introduced to quantify the performance of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">node attribute completion</i> . Extensive experiments on seven real-world datasets indicate SAT shows better performance than other methods on both link prediction and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">node attribute completion</i> tasks."}}
{"id": "qIDg8X89JR", "cdate": 1640995200000, "mdate": 1668238561190, "content": {"title": "Symbiotic Graph Neural Networks for 3D Skeleton-Based Human Action Recognition and Motion Prediction", "abstract": "3D skeleton-based action recognition and motion prediction are two essential problems of human activity understanding. In many previous works: 1) they studied two tasks separately, neglecting internal correlations; and 2) they did not capture sufficient relations inside the body. To address these issues, we propose a symbiotic model to handle two tasks jointly; and we propose two scales of graphs to explicitly capture relations among body-joints and body-parts. Together, we propose symbiotic graph neural networks, which contain a backbone, an action-recognition head, and a motion-prediction head. Two heads are trained jointly and enhance each other. For the backbone, we propose multi-branch multiscale graph convolution networks to extract spatial and temporal features. The multiscale graph convolution networks are based on joint-scale and part-scale graphs. The joint-scale graphs contain actional graphs, capturing action-based relations, and structural graphs, capturing physical constraints. The part-scale graphs integrate body-joints to form specific parts, representing high-level relations. Moreover, dual bone-based graphs and networks are proposed to learn complementary features. We conduct extensive experiments for skeleton-based action recognition and motion prediction with four datasets, NTU-RGB+D, Kinetics, Human3.6M, and CMU Mocap. Experiments show that our symbiotic graph neural networks achieve better performances on both tasks compared to the state-of-the-art methods."}}
