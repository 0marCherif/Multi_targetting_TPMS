{"id": "SzabDrGlNUR", "cdate": 1546300800000, "mdate": 1631055090726, "content": {"title": "Soft-Biometric Attributes from Selfie Images", "abstract": "The aim of this chapter is to discuss the soft-biometric attributes that can be extracted from selfie images acquired from mobile devices. Existing literature suggests that various features in demographics, such as gender and age, in physical, such as periocular and eyebrow, and in material, such as eyeglasses and clothing, have been extracted from selfie images for continuous user authentication and performance enhancement of primary biometric traits. Due to the limited hardware resources, low resolution of front-facing cameras, and the usage of the device in different environmental conditions, factors such as robustness to low-quality data, consent-free acquisition, lower computational complexity, and privacy, favor soft-biometric prediction in mobile devices."}}
{"id": "-PqeFsKgOsH", "cdate": 1356998400000, "mdate": 1631055090745, "content": {"title": "Clutter noise removal in binary document images", "abstract": "The paper presents a clutter detection and removal algorithm for complex document images. This distance transform based technique aims to remove irregular and independent unwanted clutter while preserving the text content. The novelty of this approach is in its approximation to the clutter\u2013content boundary when the clutter is attached to the content in irregular ways. As an intermediate step, a residual image is created, which forms the basis for clutter detection and removal. Clutter detection and removal are independent of clutter\u2019s position, size, shape, and connectivity with text. The method is tested on a collection of highly degraded and noisy, machine-printed and handwritten Arabic and English documents, and results show pixel-level accuracies of 99.18 and 98.67 % for clutter detection and removal, respectively. This approach is also extended to documents having a mix of clutter and salt-and-pepper noise."}}
{"id": "wR00z6tFrIE", "cdate": 1293840000000, "mdate": 1631055090725, "content": {"title": "Stroke-Like Pattern Noise Removal in Binary Document Images", "abstract": "This paper presents a two-phased stroke-like pattern noise (SPN) removal algorithm for binary document images. The proposed approach aims at understanding script-independent prominent text component features using supervised classification as a first step. It then uses their cohesiveness and stroke-width properties to filter and associate smaller text components with them using an unsupervised classification technique. In order to perform text extraction, and hence noise removal, at diacritic-level, this divide-and-conquer technique does not assume the availability of accurate and large amounts of ground-truth data at component-level for training purposes. The method was tested on a collection of degraded and noisy, machine-printed and handwritten binary Arabic text documents. Results show pixel-level precision and recall of 86% and 90% respectively for noise-pixels."}}
{"id": "90wxUBWasUd", "cdate": 1293840000000, "mdate": 1631055090744, "content": {"title": "Adaptive Algorithms for Automated Processing of Document Images", "abstract": "Large scale document digitization projects continue to motivate interesting document understanding technologies such as script and language identification, page classification, segmentation and enhancement. Typically, however, solutions are still limited to narrow domains or regular formats such as books, forms, articles or letters and operate best on clean documents scanned in a controlled environment. More general collections of heterogeneous documents challenge the basic assumptions of state-of-the-art technology regarding quality, script, content and layout. Our work explores the use of adaptive algorithms for the automated analysis of noisy and complex document collections. We first propose, implement and evaluate an adaptive clutter detection and removal technique for complex binary documents. Our distance transform based technique aims to remove irregular and independent unwanted foreground content while leaving text content untouched. The novelty of this approach is in its determination of best approximation to clutter-content boundary with text like structures. Second, we describe a page segmentation technique called Voronoi++ for complex layouts which builds upon the state-of-the-art method proposed by Kise [Kise1999]. Our approach does not assume structured text zones and is designed to handle multi-lingual text in both handwritten and printed form. Voronoi++ is a dynamically adaptive and contextually aware approach that considers components' separation features combined with Docstrum [O'Gorman1993] based angular and neighborhood features to form provisional zone hypotheses. These provisional zones are then verified based on the context built from local separation and high-level content features. Finally, our research proposes a generic model to segment and to recognize characters for any complex syllabic or non-syllabic script, using font-models. This concept is based on the fact that font files contain all the information necessary to render text and thus a model for how to decompose them. Instead of script-specific routines, this work is a step towards a generic character and recognition scheme for both Latin and non-Latin scripts."}}
{"id": "j5pGEB2HC5R", "cdate": 1262304000000, "mdate": 1631055090745, "content": {"title": "Context Aware On-line Diagramming Recognition", "abstract": "This paper presents a context aware, online immediate-mode diagramming recognition and beautification software for hand-sketched diagrams. The system is independent of stroke-order, -number, -direction and is invariant to scaling, translation and rotation. In our stroke-based recognition model, we propose convexity features along with spatial and temporal proximity features to prune the combinatorial search space of possible stroke configurations to form shapes. This reduces the problem of exponential complexity to polynomial one while reducing the error by 24% compared to temporal proximity based criterion. The strokes are then recognized using geometric polygonal features against a neural-net based classifier for 17 classes. The diagramming system is based on stroke-based classifier combination model where an arbitrator makes context aware decisions using suggestions from shape, connector and writing-drawing experts. We achieved an accuracy of 92.7%, 81.4% and 91.5% on the respective experts for a collection of 700,000 online shapes."}}
{"id": "fT3GIlMUhr", "cdate": 1262304000000, "mdate": 1631055090745, "content": {"title": "Performance Evaluation Tools for Zone Segmentation and Classification (PETS)", "abstract": "This paper describes a set of Performance Evaluation Tools (PETS) for document image zone segmentation and classification. The tools allow researchers and developers to evaluate, optimize and compare their algorithms by providing a variety of quantitative performance metrics. The evaluation of segmentation quality is based on the pixel-based overlaps between two sets of zones proposed by Randriamasy and Vincent. PETS extends the approach by providing a set of metrics for overlap analysis, RLE and polygonal representation of zones and introduces type-matching to evaluate zone classification. The software is available for research use."}}
{"id": "aKyhiHtB1k5", "cdate": 1262304000000, "mdate": 1631055090745, "content": {"title": "Context-aware and content-based dynamic Voronoi page segmentation", "abstract": "This paper presents a dynamic approach to document page segmentation based on inter-component relationships, local patterns and context features. State-of-the art page segmentation algorithms segment zones based on local properties of neighboring connected components such as distance and orientation, and do not typically consider additional properties other than size. Our proposed approach uses a contextually aware and dynamically adaptive page segmentation scheme. The page is first over-segmented using a dynamically adaptive scheme of separation features based on [2] and adapted from [13]. A decision to form zones is then based on the context built from these local separation features and high-level content features. Zone-based evaluation was performed on sets of printed and handwritten documents in English and Arabic scripts with multiple font types, sizes and we achieved an increase of 15% over the accuracy reported in [2]."}}
{"id": "V1y3Ard2xYG", "cdate": 1230768000000, "mdate": 1631055090696, "content": {"title": "Voronoi++: A Dynamic Page Segmentation Approach Based on Voronoi and Docstrum Features", "abstract": "This paper presents a dynamic approach to document page segmentation. Current page segmentation algorithms lack the ability to dynamically adapt local variations in the size, orientation and distance of components within a page. Our approach builds upon one of the best algorithms, Kise et. al. work based on Area Voronoi Diagrams, which adapts globally to page content to determine algorithm parameters. In our approach, local thresholds are determined dynamically based on parabolic relations between components, and Docstrum based angular and neighborhood features are integrated to improve accuracy. Zone-based evaluation was performed on four sets of printed and handwritten documents in English and Arabic scripts and an increase of 33% in accuracy is reported."}}
{"id": "OHsZCz5o9Zc", "cdate": 1230768000000, "mdate": 1631055090749, "content": {"title": "Clutter Noise Removal in Binary Document Images", "abstract": "The paper presents a clutter detection and removal algorithm for complex document images. The distance transform based approach is independent of clutter's position, size, shape and connectivity with text. Features are based on a residual image obtained by analysis of the distance transform and clutter elements, if present, are identified with an SVM classifier. Removal is restrictive, so text attached to the clutter is not deleted in the process. The method was tested on a collection of degraded and noisy, machine-printed and handwritten Arabic and English text documents. Results show pixel-level accuracies of 97.5% and 95% for clutter detection and removal respectively. This approach was also extended with a noise detection and removal model for documents having a mix of clutter and salt-n-pepper noise."}}
{"id": "kEAnKbiLxi2", "cdate": 1199145600000, "mdate": 1631055090753, "content": {"title": "Document-zone classification using partial least squares and hybrid classifiers", "abstract": "This paper introduces a novel document-zone classification algorithm. Low level image features are first extracted from document zones and partial least squares is used on pairs of classes to compute discriminating pairwise features. Rather than using the popular one-against-all and one-against-one voting schemes, we introduce a novel hybrid method which combines the benefits of the two schemes. The algorithm is applied on the University of Washington dataset and 97.3% classification accuracy is obtained."}}
