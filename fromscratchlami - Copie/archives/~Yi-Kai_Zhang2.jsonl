{"id": "16BDzjpOwe", "cdate": 1663850334139, "mdate": null, "content": {"title": "Learning Debiased Representations via Conditional Attribute Interpolation", "abstract": "An image is usually associated with more than one attribute, e.g., annotated based on both \"shape\" and \"color\". If most samples have attributes spuriously correlated with the target label, a Deep Neural Network (DNN) is prone to neglect those samples with attributes intrinsically consistent with the targets and leads to representations with large intra-class covariance. To improve the generalization ability of such a biased model, we propose a $\\chi^2$-model to fill in the intra-class blanks and learn debiased representations. First, we use a $\\chi$-shape pattern to match the training dynamics of a DNN and find Intermediate Attribute Samples (IASs) --- samples near decision boundaries when discerning various attributes, which indicate how attribute values change from one extreme to another. Then we rectify the decision boundary with a $\\chi$-branch metric learning objective. Conditional interpolation among IASs eliminates the negative effect of peripheral attributes and facilitates making intra-class samples compact. Experiments show that $\\chi^2$-model learns debiased representation effectively and achieves remarkable improvements on various datasets."}}
