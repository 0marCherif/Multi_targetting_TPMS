{"id": "w252eGbizyk", "cdate": 1675970197577, "mdate": null, "content": {"title": "How Deep Convolutional Neural Networks lose Spatial Information with training", "abstract": "A central question of machine learning is how deep nets learn tasks in high di- mensions. An appealing hypothesis is that they build a representation of the data where information irrelevant to the task is lost. For image datasets, this view is supported by the observation that after (and not before) training, the neural rep- resentation becomes less and less sensitive to diffeomorphisms acting on images as the signal propagates through the net. This loss of sensitivity correlates with performance and surprisingly correlates with a gain of sensitivity to white noise acquired over training. These facts are unexplained, and as we demonstrate still hold when white noise is added to the images of the training set. Here we (i) show empirically for various architectures that stability to diffeomorphisms is achieved due to a combination of spatial and channel pooling; (ii) introduce a model scale- detection task which reproduces our empirical observations on spatial pooling; (iii) compute analytically how the sensitivity to diffeomorphisms and noise scale with depth due to spatial pooling. In particular, we find that both trends are caused by a diffusive spreading of the neuron\u2019s receptive fields through the layers."}}
{"id": "cy554rYBzMT", "cdate": 1663850138794, "mdate": null, "content": {"title": "How deep convolutional neural networks lose spatial information with training", "abstract": "A central question of machine learning is how deep nets  manage to learn tasks in high dimensions. An appealing hypothesis is that they achieve this feat by building a representation of the data where information  irrelevant to the task is lost. For image data sets, this view is supported by the observation that after (and not before) training,  the neural representation becomes less and less sensitive to diffeomorphisms acting on images as the signal propagates through the net.  \nThis loss of sensitivity correlates with performance, and surprisingly  also correlates  with a gain of sensitivity to white noise acquired during training. These facts are unexplained, and as we demonstrate still hold when white noise is added to the images of the training set. Here, we (i) show empirically for various architectures that stability to image diffeomorphisms is achieved by spatial pooling in the first half of the net, and by channel pooling in the second half, (ii) introduce a scale-detection task for a simple model of data where pooling is learnt during training, which captures all empirical observations above and (iii) compute in this model how stability to diffeomorphisms and noise scale with depth. The scalings are found to depend on the presence of strides in the net architecture. We find that the increased sensitivity to noise is due to the perturbing noise piling up during pooling, after a ReLU non-linearity is applied to the noise in the internal layers."}}
{"id": "dZEZu7zxJBF", "cdate": 1652737813271, "mdate": null, "content": {"title": "Learning sparse features can lead to overfitting in neural networks", "abstract": "It is widely believed that the success of deep networks lies in their ability to learn a meaningful representation of the features of the data. Yet, understanding when and how this feature learning improves performance remains a challenge: for example, it is beneficial for modern architectures trained to classify images, whereas it is detrimental for fully-connected networks trained for the same task on the same data. Here we propose an explanation for this puzzle, by showing that feature learning can perform worse than lazy training (via random feature kernel or the NTK) as the former can lead to a sparser neural representation. Although sparsity is known to be essential for learning anisotropic data, it is detrimental when the target function is constant or smooth along certain directions of input space. We illustrate this phenomenon in two settings: (i) regression of Gaussian random functions on the $d$-dimensional unit sphere and  (ii) classification of benchmark datasets of images. For (i), we compute the scaling of the generalization error with number of training points, and show that methods that do not learn features generalize better, even when the dimension of the input space is large. For (ii), we show empirically that learning features can indeed lead to sparse and thereby less smooth representations of the image predictors. This fact is plausibly responsible for deteriorating the performance, which is known to be correlated with smoothness along diffeomorphisms."}}
{"id": "OgXM1Y-bg2", "cdate": 1640995200000, "mdate": 1681650247536, "content": {"title": "Learning sparse features can lead to overfitting in neural networks", "abstract": ""}}
{"id": "1aQqtQJtf5", "cdate": 1640995200000, "mdate": 1681650247476, "content": {"title": "How deep convolutional neural networks lose spatial information with training", "abstract": ""}}
{"id": "RSc-kfiLMNn", "cdate": 1621629975615, "mdate": null, "content": {"title": "Relative stability toward diffeomorphisms indicates performance in deep nets", "abstract": "Understanding why deep nets can classify data in large dimensions remains a challenge. It has been proposed that they do so by becoming stable to diffeomorphisms, yet existing empirical measurements support that it is often not the case. We revisit this question by defining a maximum-entropy distribution on diffeomorphisms, that allows to study typical diffeomorphisms of a given norm. We confirm that stability toward diffeomorphisms does not strongly correlate to performance on benchmark data sets of images. By contrast, we find that the stability toward diffeomorphisms relative to that of generic transformations $R_f$ correlates remarkably with the test error $\\epsilon_t$. It is of order unity at initialization but decreases by several decades during training for state-of-the-art architectures. For CIFAR10 and 15 known architectures, we find $\\epsilon_t\\approx 0.2\\sqrt{R_f}$, suggesting that obtaining a small $R_f$ is important to achieve good performance. We study how $R_f$ depends on the size of the training set and compare it to a simple model of invariant learning."}}
{"id": "KoKsNEv_zdD", "cdate": 1609459200000, "mdate": 1681650247694, "content": {"title": "Relative stability toward diffeomorphisms in deep nets indicates performance", "abstract": ""}}
{"id": "1yj8aiJ_o-", "cdate": 1609459200000, "mdate": 1681650247607, "content": {"title": "Relative stability toward diffeomorphisms indicates performance in deep nets", "abstract": ""}}
{"id": "87EMoAAmX-5", "cdate": 1577836800000, "mdate": 1681650247574, "content": {"title": "Compressing invariant manifolds in neural nets", "abstract": ""}}
{"id": "15QMi6p9WS", "cdate": 1577836800000, "mdate": 1681650247638, "content": {"title": "Perspective: A Phase Diagram for Deep Learning unifying Jamming, Feature Learning and Lazy Training", "abstract": ""}}
