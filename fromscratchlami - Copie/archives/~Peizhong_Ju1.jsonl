{"id": "_NcfR-W8R8C", "cdate": 1672531200000, "mdate": 1681750978210, "content": {"title": "Theory on Forgetting and Generalization of Continual Learning", "abstract": "Continual learning (CL), which aims to learn a sequence of tasks, has attracted significant recent attention. However, most work has focused on the experimental performance of CL, and theoretical studies of CL are still limited. In particular, there is a lack of understanding on what factors are important and how they affect \"catastrophic forgetting\" and generalization performance. To fill this gap, our theoretical analysis, under overparameterized linear models, provides the first-known explicit form of the expected forgetting and generalization error. Further analysis of such a key result yields a number of theoretical explanations about how overparameterization, task similarity, and task ordering affect both forgetting and generalization error of CL. More interestingly, by conducting experiments on real datasets using deep neural networks (DNNs), we show that some of these insights even go beyond the linear models and can be carried over to practical setups. In particular, we use concrete examples to show that our results not only explain some interesting empirical observations in recent studies, but also motivate better practical algorithm designs of CL."}}
{"id": "Jifob4dSh99", "cdate": 1663850289650, "mdate": null, "content": {"title": "Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning", "abstract": "Meta-learning has arisen as a successful method for improving training performance by training over many similar tasks, especially with deep neural networks (DNNs). However, the theoretical understanding of when and why overparameterized models such as DNNs can generalize well in meta-learning is still limited. As an initial step towards addressing this challenge, this paper studies the generalization performance of overfitted meta-learning under a linear regression model with Gaussian features. In contrast to a few recent studies along the same line, our framework allows the number of model parameters to be arbitrarily larger than the number of features in the ground truth signal, and hence naturally captures the overparameterized regime in practical deep meta-learning. We show that the overfitted min $\\ell_2$-norm solution of model-agnostic meta-learning (MAML) can be beneficial, which is similar to the recent remarkable findings on \"benign overfitting\" and \"double descent\" phenomenon in the classical (single-task) linear regression. However, due to the uniqueness of meta-learning such as task-specific gradient descent inner training and the diversity/fluctuation of the ground-truth signals among training tasks, we find new and interesting properties that do not exist in single-task linear regression. We first provide a high-probability upper bound (under reasonable tightness) on the generalization error, where certain terms decrease when the number of features increases. Our analysis suggests that benign overfitting is more significant and easier to observe when the noise and the diversity/fluctuation of the ground truth of each training task are large. Under this circumstance, we show that the overfitted min $\\ell_2$-norm solution can achieve an even lower generalization error than the underparameterized solution."}}
{"id": "IE32oIlhXz", "cdate": 1652737577566, "mdate": null, "content": {"title": "On the Generalization Power of the Overfitted Three-Layer Neural Tangent Kernel Model", "abstract": "In this paper, we study the generalization performance of overparameterized 3-layer NTK models. We show that, for a specific set of ground-truth functions (which we refer to as the \"learnable set\"), the test error of the overfitted 3-layer NTK is upper bounded by an expression that decreases with the number of neurons of the two hidden layers. Different from 2-layer NTK where there exists only one hidden-layer, the 3-layer NTK involves interactions between two hidden-layers. Our upper bound reveals that, between the two hidden-layers, the test error descends faster with respect to the number of neurons in the second hidden-layer (the one closer to the output) than with respect to that in the first hidden-layer (the one closer to the input). We also show that the learnable set of 3-layer NTK without bias is no smaller than that of 2-layer NTK models with various choices of bias in the neurons. However, in terms of the actual generalization performance, our results suggest that 3-layer NTK is much less sensitive to the choices of bias than 2-layer NTK, especially when the input dimension is large."}}
{"id": "MhGsCpzHTj8", "cdate": 1640995200000, "mdate": 1681750978200, "content": {"title": "Understanding the Generalization Power of Overfitted NTK Models: 3-layer vs. 2-layer (Extended Abstract)", "abstract": "Neural tangent kernel (NTK) models [1] have been recently used as an important intermediate step to understand the exceptional generalization power of overparameterized deep neural networks (DNNs). Compared to linear models with simple Gaussian or Fourier features, NTK models can capture the nonlinear features inherent in neural networks. Indeed, the work in [2] has shown that, for a 2-layer NTK model, the generalization error of an overfitted solution decreases as the number of neurons increases. Further, this descent behavior is qualitatively different from that of linear models with simple Gaussian and Fourier features, and closer to that of an actual neural network."}}
{"id": "C89y8868q_B", "cdate": 1640995200000, "mdate": 1681750978196, "content": {"title": "On the Generalization Power of the Overfitted Three-Layer Neural Tangent Kernel Model", "abstract": "In this paper, we study the generalization performance of overparameterized 3-layer NTK models. We show that, for a specific set of ground-truth functions (which we refer to as the \"learnable set\"), the test error of the overfitted 3-layer NTK is upper bounded by an expression that decreases with the number of neurons of the two hidden layers. Different from 2-layer NTK where there exists only one hidden-layer, the 3-layer NTK involves interactions between two hidden-layers. Our upper bound reveals that, between the two hidden-layers, the test error descends faster with respect to the number of neurons in the second hidden-layer (the one closer to the output) than with respect to that in the first hidden-layer (the one closer to the input). We also show that the learnable set of 3-layer NTK without bias is no smaller than that of 2-layer NTK models with various choices of bias in the neurons. However, in terms of the actual generalization performance, our results suggest that 3-layer NTK is much less sensitive to the choices of bias than 2-layer NTK, especially when the input dimension is large."}}
{"id": "3F3GP1DR37M", "cdate": 1640995200000, "mdate": 1681750978199, "content": {"title": "Distribution-level markets under high renewable energy penetration", "abstract": "We study the market structure for emerging distribution-level energy markets with high renewable energy penetration. Renewable generation is known to be uncertain and has a close-to-zero marginal cost. In this paper, we use solar energy as an example of such zero-marginal-cost resources for our focused study. We first show that, under high penetration of solar generation, the classical real-time market mechanism can either exhibit significant price-volatility (when each firm is not allowed to vary the supply quantity), or induce price-fixing (when each firm is allowed to vary the supply quantity), the latter of which leads to extreme unfairness of surplus division. To overcome these issues, we propose a new rental-market mechanism that trades the usage-right of solar panels instead of real-time solar energy. We show that the rental market produces a stable and unique price (therefore eliminating price-volatility), maintains positive surplus for both consumers and firms (therefore eliminating price-fixing), and achieves the same social welfare as the traditional real-time market. A key insight is that rental markets turn uncertainty of renewable generation from a detrimental factor (that leads to price-volatility in real-time markets) to a beneficial factor (that increases demand elasticity and contributes to the desirable rental-market outcomes)."}}
{"id": "Otxij5mW-h2", "cdate": 1609459200000, "mdate": null, "content": {"title": "On the Generalization Power of Overfitted Two-Layer Neural Tangent Kernel Models", "abstract": "In this paper, we study the generalization performance of min $\\ell_2$-norm overfitting solutions for the neural tangent kernel (NTK) model of a two-layer neural network with ReLU activation that has no bias term. We show that, depending on the ground-truth function, the test error of overfitted NTK models exhibits characteristics that are different from the \"double-descent\" of other overparameterized linear models with simple Fourier or Gaussian features. Specifically, for a class of learnable functions, we provide a new upper bound of the generalization error that approaches a small limiting value, even when the number of neurons $p$ approaches infinity. This limiting value further decreases with the number of training samples $n$. For functions outside of this class, we provide a lower bound on the generalization error that does not diminish to zero even when $n$ and $p$ are both large."}}
{"id": "K6vIzw2jS_", "cdate": 1609459200000, "mdate": 1681750978232, "content": {"title": "On the Generalization Power of Overfitted Two-Layer Neural Tangent Kernel Models", "abstract": "In this paper, we study the generalization performance of min $\\ell_2$-norm overfitting solutions for the neural tangent kernel (NTK) model of a two-layer neural network with ReLU activation that h..."}}
{"id": "YZq0E9mkpRC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Overfitting Can Be Harmless for Basis Pursuit, But Only to a Degree", "abstract": "Recently, there have been significant interests in studying the so-called \"double-descent\" of the generalization error of linear regression models under the overparameterized and overfitting regime, with the hope that such analysis may provide the first step towards understanding why overparameterized deep neural networks (DNN) still generalize well. However, to date most of these studies focused on the min L2-norm solution that overfits the data. In contrast, in this paper we study the overfitting solution that minimizes the L1-norm, which is known as Basis Pursuit (BP) in the compressed sensing literature. Under a sparse true linear regression model with p i.i.d. Gaussian features, we show that for a large range of p up to a limit that grows exponentially with the number of samples n, with high probability the model error of BP is upper bounded by a value that decreases with p. To the best of our knowledge, this is the first analytical result in the literature establishing the double-descent of overfitting BP for finite n and p. Further, our results reveal significant differences between the double-descent of BP and min L2-norm solutions. Specifically, the double-descent upper-bound of BP is independent of the signal strength, and for high SNR and sparse models the descent-floor of BP can be much lower and wider than that of min L2-norm solutions."}}
{"id": "UiEIOgF6yC5", "cdate": 1577836800000, "mdate": null, "content": {"title": "Overfitting Can Be Harmless for Basis Pursuit: Only to a Degree", "abstract": "Recently, there have been significant interests in studying the so-called \"double-descent\" of the generalization error of linear regression models under the overparameterized and overfitting regime, with the hope that such analysis may provide the first step towards understanding why overparameterized deep neural networks (DNN) still generalize well. However, to date most of these studies focused on the min $\\ell_2$-norm solution that overfits the data. In contrast, in this paper we study the overfitting solution that minimizes the $\\ell_1$-norm, which is known as Basis Pursuit (BP) in the compressed sensing literature. Under a sparse true linear regression model with $p$ i.i.d. Gaussian features, we show that for a large range of $p$ up to a limit that grows exponentially with the number of samples $n$, with high probability the model error of BP is upper bounded by a value that decreases with $p$. To the best of our knowledge, this is the first analytical result in the literature establishing the double-descent of overfitting BP for finite $n$ and $p$. Further, our results reveal significant differences between the double-descent of BP and min $\\ell_2$-norm solutions. Specifically, the double-descent upper-bound of BP is independent of the signal strength, and for high SNR and sparse models the descent-floor of BP can be much lower and wider than that of min $\\ell_2$-norm solutions."}}
