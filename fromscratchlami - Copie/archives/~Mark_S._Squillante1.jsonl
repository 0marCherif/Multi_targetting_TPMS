{"id": "0RMDK39mGg", "cdate": 1652737684336, "mdate": null, "content": {"title": "A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization", "abstract": "Bilevel optimization has been shown to be a powerful framework for formulating multi-task machine learning problems, e.g., reinforcement learning (RL) and meta-learning, where the decision variables are coupled in both levels of the minimization problems. In practice, the learning tasks would be located at different computing resource environments, and thus there is a need for deploying a decentralized training framework to implement multi-agent and multi-task learning. We develop a stochastic linearized augmented Lagrangian method (SLAM) for solving general nonconvex bilevel optimization problems over a graph, where both upper and lower optimization variables are able to achieve a consensus. We also establish that the theoretical convergence rate of the proposed SLAM to the Karush-Kuhn-Tucker (KKT) points of this class of problems is on the same order as the one achieved by the classical distributed stochastic gradient descent for only single-level nonconvex minimization problems. Numerical results tested on multi-agent RL problems showcase the superiority of SLAM compared with the benchmarks."}}
{"id": "S3e377aHvS9", "cdate": 1621630019167, "mdate": null, "content": {"title": "Efficient Generalization with Distributionally Robust Learning", "abstract": "Distributionally robust learning (DRL) is increasingly seen as a viable method to train machine learning models for improved model generalization. These min-max formulations, however, are more dif\ufb01cult to solve. We provide a new stochastic gradient descent algorithm to ef\ufb01ciently solve this DRL formulation. Our approach applies gradient descent to the outer minimization formulation and estimates the gradient of the inner maximization based on a sample average approximation. The latter uses a subset of the data sampled without replacement in each iteration, progressively increasing the subset size to ensure convergence. We rigorously establish convergence to a near-optimal solution under standard regularity assumptions and, for strongly convex losses, match the best known $O(\\epsilon{ \u22121})$ rate of convergence up to a known threshold. Empirical results demonstrate the signi\ufb01cant bene\ufb01ts of our approach over previous work in improving learning for model generalization."}}
{"id": "BJg2waqGar", "cdate": 1575296355631, "mdate": null, "content": {"title": "A Family of Robust Stochastic Operators for Reinforcement Learning", "abstract": "We consider a new family of stochastic operators for reinforcement learning with the goal of alleviating negative effects and becoming more robust to approximation or estimation errors. Various theoretical results are established, which include showing that our family of operators preserve optimality and increase the action gap in a stochastic sense. Our empirical results illustrate the strong benefits of our robust stochastic operators, significantly outperforming the classical Bellman operator and recently proposed operators."}}
{"id": "B1M-ASSgUS", "cdate": 1567802825134, "mdate": null, "content": {"title": "A Family of Robust Stochastic Operators for Reinforcement Learning", "abstract": "We consider a new family of stochastic operators for reinforcement learning with the goal of alleviating negative effects and becoming more robust to approximation or estimation errors. Various theoretical results are established, which include showing that our family of operators preserve optimality and increase the action gap in a stochastic sense. Our empirical results illustrate the strong benefits of our robust stochastic operators, significantly outperforming the classical Bellman operator and recently proposed operators."}}
{"id": "SJ-9goZO-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach", "abstract": "We propose a novel framework PROVEN to \\textbf{PRO}babilistically \\textbf{VE}rify \\textbf{N}eural network\u2019s robustness with statistical guarantees. PROVEN provides probability certificates of neura..."}}
{"id": "HJWaN-W_ZB", "cdate": 1009843200000, "mdate": null, "content": {"title": "Optimal crawling strategies for web search engines", "abstract": "Web Search Engines employ multiple so-called crawlers to maintain local copies of web pages. But these web pages are frequently updated by their owners, and therefore the crawlers must regularly revisit the web pages to maintain the freshness of their local copies. In this paper, we propose a two-part scheme to optimize this crawling process. One goal might be the minimization of the average level of staleness over all web pages, and the scheme we propose can solve this problem. Alternatively, the same basic scheme could be used to minimize a possibly more important search engine embarrassment level metric: The frequency with which a client makes a search engine query and then clicks on a returned url only to find that the result is incorrect. The first part our scheme determines the (nearly) optimal crawling frequencies, as well as the theoretically optimal times to crawl each web page. It does so within an extremely general stochastic framework, one which supports a wide range of complex update patterns found in practice. It uses techniques from probability theory and the theory of resource allocation problems which are highly computationally efficient -- crucial for practicality because the size of the problem in the web environment is immense. The second part employs these crawling frequencies and ideal crawl times as input, and creates an optimal achievable schedule for the crawlers. Our solution, based on network flow theory, is exact as well as highly efficient. An analysis of the update patterns from a highly accessed and highly dynamic web site is used to gain some insights into the properties of page updates in practice. Then, based on this analysis, we perform a set of detailed simulation experiments to demonstrate the quality and speed of our approach."}}
