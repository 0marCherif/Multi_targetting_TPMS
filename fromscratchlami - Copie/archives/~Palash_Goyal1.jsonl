{"id": "55IdeIjTJ5", "cdate": 1673371450773, "mdate": 1673371450773, "content": {"title": "Leveraging Local Temporal Information for Multimodal Scene Classification", "abstract": "Robust video scene classification models should capture the spatial (pixel-wise) and temporal (frame-wise) characteristics of a video effectively. Transformer models with self-attention which are designed to get contextualized representations for individual tokens given a sequence of tokens, are becoming increasingly popular in many computer vision tasks. However, the use of Transformer based models for video under-standing is still relatively unexplored. Moreover, these models fail to exploit the strong temporal relationships between the neighboring video frames to get potent frame-level representations. In this paper, we propose a novel self-attention block that leverages both local and global temporal relation-ships between the video frames to obtain better contextualized representations for the individual frames. This enables the model to understand the video at various granularities. We illustrate the performance of our models on the large-scale YoutTube-8M data set on the task of video categorization and further analyze the results to showcase improvement."}}
{"id": "Lnmu7mHBS9T", "cdate": 1673371378263, "mdate": 1673371378263, "content": {"title": "Cross-modal Non-linear Guided Attention and Temporal Coherence in Multi-modal Deep Video Models", "abstract": "Videos have data in multiple modalities, e.g., audio, video, text (captions). Understanding and modeling the interaction between different modalities is key for video analysis tasks like categorization, object detection, activity recognition, etc. However, data modalities are not always correlated --- so, learning when modalities are correlated and using that to guide the influence of one modality on the other is crucial. Another salient feature of videos is the coherence between successive frames due to continuity of video and audio, a property that we refer to as temporal coherence. We show how using non-linear guided cross-modal signals and temporal coherence can improve the performance of multi-modal machine learning (ML) models for video analysis tasks like categorization. Our experiments on the large-scale YouTube-8M dataset show how our approach significantly outperforms state-of-the-art multi-modal ML models for video categorization. The model trained on the YouTube-8M dataset also showed good performance on an internal dataset of video segments from actual Samsung TV Plus channels without retraining or fine-tuning, showing the generalization capabilities of our model."}}
{"id": "kcT3ia7E0o", "cdate": 1609459200000, "mdate": null, "content": {"title": "Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training", "abstract": "The introduction of Transformer model has led to tremendous advancements in sequence modeling, especially in text domain. However, the use of attention-based models for video understanding is still relatively unexplored. In this paper, we introduce Gated Adversarial Transformer (GAT) to enhance the applicability of attention-based models to videos. GAT uses a multi-level attention gate to model the relevance of a frame based on local and global contexts. This enables the model to understand the video at various granularities. Further, GAT uses adversarial training to improve model generalization. We propose temporal attention regularization scheme to improve the robustness of attention modules to adversarial examples. We illustrate the performance of GAT on the large-scale YoutTube-8M data set on the task of video categorization. We further show ablation studies along with quantitative and qualitative analysis to showcase the improvement."}}
{"id": "cSARZQI5Dph", "cdate": 1609459200000, "mdate": null, "content": {"title": "Pykg2vec: A Python Library for Knowledge Graph Embedding", "abstract": "Pykg2vec is a Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 25 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms.The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of PyTorch and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, evaluation of KGE tasks, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at https://github.com/Sujit-O/pykg2vec."}}
{"id": "xohrg8Axflb", "cdate": 1577836800000, "mdate": null, "content": {"title": "Modeling Dialogues with Hashcode Representations: A Nonparametric Approach", "abstract": "We propose a novel dialogue modeling framework, the first-ever nonparametric kernel functions based approach for dialogue modeling, which learns hashcodes as text representations; unlike traditional deep learning models, it handles well relatively small datasets, while also scaling to large ones. We also derive a novel lower bound on mutual information, used as a model-selection criterion favoring representations with better alignment between the utterances of participants in a collaborative dialogue setting, as well as higher predictability of the generated responses. As demonstrated on three real-life datasets, including prominently psychotherapy sessions, the proposed approach significantly outperforms several state-of-art neural network based dialogue systems, both in terms of computational efficiency, reducing training time from days or weeks to hours, and the response quality, achieving an order of magnitude improvement over competitors in frequency of being chosen as the best model by human evaluators."}}
{"id": "wCQ9A13iNpL", "cdate": 1577836800000, "mdate": null, "content": {"title": "Graph Representation Ensemble Learning", "abstract": "Representation learning on graphs has been gaining attention due to its wide applicability in predicting missing links and classifying and recommending nodes. Most embedding methods aim to preserve specific properties of the original graph in the low dimensional space. However, real-world graphs have a combination of several features that are difficult to characterize and capture by a single approach. In this work, we introduce the problem of graph representation ensemble learning and provide a first of its kind framework to aggregate multiple graph embedding methods efficiently. We provide analysis of our framework and analyze - theoretically and empirically - the dependence between state-of-the-art embedding methods. We test our models on the node classification task on four realworld graphs and show that proposed ensemble approaches can outperform the state-of-the-art methods by up to 20% on macro-F1. We further show that the strategy is even more beneficial for underrepresented classes with an improvement of up to 40%."}}
{"id": "sIFa7gtRQzF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Cross-modal Non-linear Guided Attention and Temporal Coherence in Multi-modal Deep Video Models", "abstract": "Videos have data in multiple modalities, e.g., audio, video, text (captions). Understanding and modeling the interaction between different modalities is key for video analysis tasks like categorization, object detection, activity recognition, etc. However, data modalities are not always correlated --- so, learning when modalities are correlated and using that to guide the influence of one modality on the other is crucial. Another salient feature of videos is the coherence between successive frames due to continuity of video and audio, a property that we refer to as temporal coherence. We show how using non-linear guided cross-modal signals and temporal coherence can improve the performance of multi-modal machine learning (ML) models for video analysis tasks like categorization. Our experiments on the large-scale YouTube-8M dataset show how our approach significantly outperforms state-of-the-art multi-modal ML models for video categorization. The model trained on the YouTube-8M dataset also showed good performance on an internal dataset of video segments from actual Samsung TV Plus channels without retraining or fine-tuning, showing the generalization capabilities of our model."}}
{"id": "f2eeyuUpSZI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Hierarchical Class-Based Curriculum Loss", "abstract": "Classification algorithms in machine learning often assume a flat label space. However, most real world data have dependencies between the labels, which can often be captured by using a hierarchy. Utilizing this relation can help develop a model capable of satisfying the dependencies and improving model accuracy and interpretability. Further, as different levels in the hierarchy correspond to different granularities, penalizing each label equally can be detrimental to model learning. In this paper, we propose a loss function, hierarchical curriculum loss, with two properties: (i) satisfy hierarchical constraints present in the label space, and (ii) provide non-uniform weights to labels based on their levels in the hierarchy, learned implicitly by the training paradigm. We theoretically show that the proposed loss function is a tighter bound of 0-1 loss compared to any other loss satisfying the hierarchical constraints. We test our loss function on real world image data sets, and show that it significantly substantially outperforms multiple baselines."}}
{"id": "N09i-NVBvkD", "cdate": 1577836800000, "mdate": null, "content": {"title": "dyngraph2vec: Capturing network dynamics using dynamic graph representation learning", "abstract": "Learning graph representations is a fundamental task aimed at capturing various properties of graphs in vector space. The most recent methods learn such representations for static networks. However, real-world networks evolve over time and have varying dynamics. Capturing such evolution is key to predicting the properties of unseen networks. To understand how the network dynamics affect the prediction performance, we propose an embedding approach which learns the structure of evolution in dynamic graphs and can predict unseen links with higher precision. Our model, dyngraph2vec, learns the temporal transitions in the network using a deep architecture composed of dense and recurrent layers. We motivate the need for capturing dynamics for the prediction on a toy dataset created using stochastic block models. We then demonstrate the efficacy of dyngraph2vec over existing state-of-the-art methods on two real-world datasets. We observe that learning dynamics can improve the quality of embedding and yield better performance in link prediction."}}
{"id": "I-p5h-zMiAU", "cdate": 1577836800000, "mdate": null, "content": {"title": "Graph embedding algorithms for attributed and temporal graphs", "abstract": "Palash Goyal is a Senior Research Scientist at Samsung Research America. In 2019, he got his doctoral degree in Computer Science from University of Southern California under the advisory of Dr. Emilio Ferrara. Over the course of his PhD, he worked with several government funded projects including IARPA and DARPA spanning domains of time series prediction, text analysis and understanding social behavior. He also applied his work on graph embedding in various industrial settings including developing automation systems in Siemens Corporate, identifying key steam injection candidate oil wells for Chevron and developing recommendation system for Target. His work on tracking temporal evolution of graphs using non-timestamped data was nominated for best paper award in ACM Hypertext. In his thesis, supervised by Dr. Emilio Ferrara (USC Information Sciences Institute), he ex- tended the work of learning low-dimensional representations of nodes in a graph in several di- rections. Firstly, he published a survey of existing graph embedding approaches and shed light into the dependency between them drawing insights into the relations effectively captured by the methods. He further proposed a benchmark to evaluate any graph embedding approach and un- derstand its applicability. Secondly, he built models to capture temporal patterns in sequential graphs and efficiently update embeddings for streaming graphs. Finally, he developed multi-modal models for learning representations through graph data as well as other forms of data available for the nodes and edges including text."}}
