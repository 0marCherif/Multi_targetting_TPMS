{"id": "FS5Ypo5wXWK", "cdate": 1680870567554, "mdate": null, "content": {"title": "On Reducing Action Labels in Planning Domains", "abstract": "Planning tasks succinctly represent labeled transition systems, with each ground action corresponding to a label. This granularity, however, is not necessary for solving planning tasks and can be harmful, especially for model-free methods. In order to apply such methods, the label sets are often manually reduced.  In this work, we propose automating this manual process. We characterize a valid label reduction for classical planning tasks and propose an automated way of obtaining such valid reductions by leveraging lifted mutex groups. Our experiments show a significant reduction in the action label space size across a wide collection of planning domains.  We demonstrate the benefit of our automated label reduction in two separate use cases: improved sample complexity of model-free reinforcement learning algorithms and speeding up successor generation in lifted planning. The code and supplementary material are available at https://github.com/IBM/Parameter-Seed-Set."}}
{"id": "mhP6mHgrg1c", "cdate": 1652737577387, "mdate": null, "content": {"title": "ORIENT: Submodular Mutual Information Measures for Data Subset Selection under Distribution Shift", "abstract": "Real-world machine-learning applications require robust models that generalize well to distribution shift settings, which is typical in real-world situations. Domain adaptation techniques aim to address this issue of distribution shift by minimizing the disparities between domains to ensure that the model trained on the source domain performs well on the target domain. Nevertheless, the existing domain adaptation methods are computationally very expensive. In this work, we aim to improve the efficiency of existing supervised domain adaptation (SDA) methods by using a subset of source data that is similar to target data for faster model training. Specifically, we propose ORIENT, a subset selection framework that uses the submodular mutual information (SMI) functions to select a source data subset similar to the target data for faster training. Additionally, we demonstrate how existing robust subset selection strategies, such as GLISTER, GRADMATCH, and CRAIG, when used with a held-out query set, fit within our proposed framework and demonstrate the connections with them. Finally, we empirically demonstrate that SDA approaches like d-SNE, CCSA, and standard Cross-entropy training, when employed together with ORIENT, achieve a) faster training and b) better performance on the target data."}}
{"id": "dIXdn5sZuw", "cdate": 1640995200000, "mdate": 1684332259401, "content": {"title": "Human-guided Collaborative Problem Solving: A Natural Language based Framework", "abstract": "We consider the problem of human-machine collaborative problem solving as a planning task coupled with natural language communication. Our framework consists of three components -- a natural language engine that parses the language utterances to a formal representation and vice-versa, a concept learner that induces generalized concepts for plans based on limited interactions with the user, and an HTN planner that solves the task based on human interaction. We illustrate the ability of this framework to address the key challenges of collaborative problem solving by demonstrating it on a collaborative building task in a Minecraft-based blocksworld domain. The accompanied demo video is available at https://youtu.be/q1pWe4aahF0."}}
{"id": "SFBkM8HnfKQ", "cdate": 1640995200000, "mdate": 1684365637087, "content": {"title": "Hybrid Deep RePReL: Integrating Relational Planning and Reinforcement Learning for Information Fusion", "abstract": "Fusion of high-level symbolic reasoning with lower level signal-based reasoning has attracted significant attention. We propose an architecture that integrates the high-level symbolic domain knowledge using a hierarchical planner with a lower level reinforcement learner that uses hybrid data (structured and unstructured). We introduce a novel neuro-symbolic system, Hybrid Deep RePReL that achieves the best of both worlds-the generalization ability of the planner with the effective learning ability of deep RL. Our results in two domains demonstrate the superiority of our approach in terms of sample efficiency as well as generalization to increased set of objects."}}
{"id": "DjIKKKU_CQ", "cdate": 1640995200000, "mdate": 1683345897503, "content": {"title": "ORIENT: Submodular Mutual Information Measures for Data Subset Selection under Distribution Shift", "abstract": "Real-world machine-learning applications require robust models that generalize well to distribution shift settings, which is typical in real-world situations. Domain adaptation techniques aim to address this issue of distribution shift by minimizing the disparities between domains to ensure that the model trained on the source domain performs well on the target domain. Nevertheless, the existing domain adaptation methods are computationally very expensive. In this work, we aim to improve the efficiency of existing supervised domain adaptation (SDA) methods by using a subset of source data that is similar to target data for faster model training. Specifically, we propose ORIENT, a subset selection framework that uses the submodular mutual information (SMI) functions to select a source data subset similar to the target data for faster training. Additionally, we demonstrate how existing robust subset selection strategies, such as GLISTER, GRADMATCH, and CRAIG, when used with a held-out query set, fit within our proposed framework and demonstrate the connections with them. Finally, we empirically demonstrate that SDA approaches like d-SNE, CCSA, and standard Cross-entropy training, when employed together with ORIENT, achieve a) faster training and b) better performance on the target data."}}
{"id": "1ISUfjKAMRs", "cdate": 1640995200000, "mdate": 1683908678284, "content": {"title": "How to Reduce Action Space for Planning Domains? (Student Abstract)", "abstract": "While AI planning and Reinforcement Learning (RL) solve sequential decision-making problems, they are based on different formalisms, which leads to a significant difference in their action spaces. When solving planning problems using RL algorithms, we have observed that a naive translation of the planning action space incurs severe degradation in sample complexity. In practice, those action spaces are often engineered manually in a domain-specific manner. In this abstract, we present a method that reduces the parameters of operators in AI planning domains by introducing a parameter seed set problem and casting it as a classical planning task. Our experiment shows that our proposed method significantly reduces the number of actions in the RL environments originating from AI planning domains."}}
{"id": "ffLKUFlsFK0", "cdate": 1634067447769, "mdate": null, "content": {"title": "Deep RePReL--Combining Planning and Deep RL for acting in relational domains", "abstract": " We consider the problem of combining symbolic planning and deep reinforcement learning (RL) to achieve the best of both worlds -- the generalization ability of the planner with the effective learning ability of deep RL. To this effect, we extend a previous work of Kokel et al. 2021, RePReL, to deep RL. As we demonstrate in experiments in two relational worlds, this combined framework enables effective learning, transfer and generalization when compared to the use of an end-to-end deep RL framework. "}}
{"id": "qIHSYXMgjOt", "cdate": 1609459200000, "mdate": 1643219453415, "content": {"title": "A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes", "abstract": "Qualitative influence statements are often provided a priori to guide learning; we answer a challenging reverse task and automatically extract them from a learned probabilistic model. We apply our Qualitative Knowledge Extraction method toward early prediction of gestational diabetes on clinical study data. Our empirical results demonstrate that the extracted rules are both interpretable and valid."}}
{"id": "M6ZFG0QXneY", "cdate": 1609459200000, "mdate": 1643219453415, "content": {"title": "RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction", "abstract": "State abstraction is necessary for better task transfer in complex reinforcement learning environments. Inspired by the benefit of state abstraction in MAXQ and building upon hybrid planner-RL architectures, we propose RePReL, a hierarchical framework that leverages a relational planner to provide useful state abstractions. Our experiments demonstrate that the abstractions enable faster learning and efficient transfer across tasks. More importantly, our framework enables the application of standard RL approaches for learning in structured domains. The benefit of using the state abstractions is critical in relational settings, where the number and/or types of objects are not fixed apriori. Our experiments clearly show that RePReL framework not only achieves better performance and efficient learning on the task at hand but also demonstrates better generalization to unseen tasks."}}
{"id": "2BSltiDn0vk", "cdate": 1609459200000, "mdate": 1643219453414, "content": {"title": "Dynamic probabilistic logic models for effective abstractions in RL", "abstract": "State abstraction enables sample-efficient learning and better task transfer in complex reinforcement learning environments. Recently, we proposed RePReL (Kokel et al. 2021), a hierarchical framework that leverages a relational planner to provide useful state abstractions for learning. We present a brief overview of this framework and the use of a dynamic probabilistic logic model to design these state abstractions. Our experiments show that RePReL not only achieves better performance and efficient learning on the task at hand but also demonstrates better generalization to unseen tasks."}}
