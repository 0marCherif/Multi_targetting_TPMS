{"id": "92ijwEQpqr", "cdate": 1672531200000, "mdate": 1682216559499, "content": {"title": "Cross-Entropy Loss Functions: Theoretical Analysis and Applications", "abstract": "Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, smooth adversarial comp-sum losses, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit $H$-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy."}}
{"id": "06OVtS901hF", "cdate": 1652737816286, "mdate": null, "content": {"title": "Multi-Class $H$-Consistency Bounds", "abstract": "We present an extensive study of $H$-consistency bounds for multi-class classification. These are upper bounds on the target loss estimation error of a predictor in a hypothesis set $H$, expressed in terms of the surrogate loss estimation error of that predictor. They are stronger and more significant guarantees than Bayes-consistency, $H$-calibration or $H$-consistency, and more informative than excess error bounds derived for $H$ being the family of all measurable functions. We give a series of new $H$-consistency bounds for surrogate multi-class losses, including max losses, sum losses, and constrained losses, both in the non-adversarial and adversarial cases, and for different differentiable or convex auxiliary functions used. We also prove that no non-trivial $H$-consistency bound can be given in some cases. To our knowledge, these are the first $H$-consistency bounds proven for the multi-class setting. Our proof techniques are also novel and likely to be useful in the analysis of other such guarantees."}}
{"id": "k-t3OwCbxc", "cdate": 1640995200000, "mdate": 1668580193422, "content": {"title": "H-Consistency Estimation Error of Surrogate Loss Minimizers", "abstract": "We present a detailed study of estimation errors in terms of surrogate loss estimation errors. We refer to such guarantees as $\\mathscr{H}$-consistency estimation error bounds, since they account for the hypothesis set $\\mathscr{H}$ adopted. These guarantees are significantly stronger than $\\mathscr{H}$-calibration or $\\mathscr{H}$-consistency. They are also more informative than similar excess error bounds derived in the literature, when $\\mathscr{H}$ is the family of all measurable functions. We prove general theorems providing such guarantees, for both the distribution-dependent and distribution-independent settings. We show that our bounds are tight, modulo a convexity assumption. We also show that previous excess error bounds can be recovered as special cases of our general results. We then present a series of explicit bounds in the case of the zero-one loss, with multiple choices of the surrogate loss and for both the family of linear functions and neural networks with one hidden-layer. We further prove more favorable distribution-dependent guarantees in that case. We also present a series of explicit bounds in the case of the adversarial loss, with surrogate losses based on the supremum of the $\\rho$-margin, hinge or sigmoid loss and for the same two general hypothesis sets. Here too, we prove several enhancements of these guarantees under natural distributional assumptions. Finally, we report the results of simulations illustrating our bounds and their tightness."}}
{"id": "R9GiK8JJTvZ", "cdate": 1640995200000, "mdate": 1668579913475, "content": {"title": "H-Consistency Bounds for Surrogate Loss Minimizers", "abstract": "We present a detailed study of estimation errors in terms of surrogate loss estimation errors. We refer to such guarantees as H-consistency bounds, since they account for the hypothesis set H adopt..."}}
{"id": "sNw3VBPL7rg", "cdate": 1621630270523, "mdate": null, "content": {"title": "Calibration and Consistency of Adversarial Surrogate Losses", "abstract": "Adversarial robustness is an increasingly critical property of classifiers in applications. The design of robust algorithms relies on surrogate losses since the optimization of the adversarial loss with most hypothesis sets is NP-hard. But, which surrogate losses should be used and when do they benefit from theoretical guarantees? We present an extensive study of this question, including a detailed analysis of the $\\mathcal{H}$-calibration and $\\mathcal{H}$-consistency of adversarial surrogate losses. We show that convex loss functions, or the supremum-based convex losses often used in applications, are not $\\mathcal{H}$-calibrated for common hypothesis sets used in machine learning. We then give a characterization of $\\mathcal{H}$-calibration and prove that some surrogate losses are indeed $\\mathcal{H}$-calibrated for the adversarial zero-one loss, with common hypothesis sets. In particular, we fix some calibration results presented in prior work for a family of linear models and significantly generalize the results to the nonlinear hypothesis sets. Next, we show that $\\mathcal{H}$-calibration is not sufficient to guarantee consistency and prove that, in the absence of any distributional assumption, no continuous surrogate loss is consistent in the adversarial setting. This, in particular, proves that a claim made in prior work is inaccurate. Next, we identify natural conditions under which some surrogate losses that we describe in detail are $\\mathcal{H}$-consistent. We also report a series of empirical results which show that many $\\mathcal{H}$-calibrated surrogate losses are indeed not $\\mathcal{H}$-consistent, and validate our theoretical assumptions. Our adversarial $\\mathcal{H}$-consistency results are novel, even for the case where $\\mathcal{H}$ is the family of all measurable functions."}}
{"id": "w_FHb2z6fLU", "cdate": 1609459200000, "mdate": 1649124251296, "content": {"title": "A Finer Calibration Analysis for Adversarial Robustness", "abstract": "We present a more general analysis of $H$-calibration for adversarially robust classification. By adopting a finer definition of calibration, we can cover settings beyond the restricted hypothesis sets studied in previous work. In particular, our results hold for most common hypothesis sets used in machine learning. We both fix some previous calibration results (Bao et al., 2020) and generalize others (Awasthi et al., 2021). Moreover, our calibration results, combined with the previous study of consistency by Awasthi et al. (2021), also lead to more general $H$-consistency results covering common hypothesis sets."}}
{"id": "sCjqCLR3Gd", "cdate": 1609459200000, "mdate": 1668579913474, "content": {"title": "Calibration and Consistency of Adversarial Surrogate Losses", "abstract": "Adversarial robustness is an increasingly critical property of classifiers in applications. The design of robust algorithms relies on surrogate losses since the optimization of the adversarial loss with most hypothesis sets is NP-hard. But, which surrogate losses should be used and when do they benefit from theoretical guarantees? We present an extensive study of this question, including a detailed analysis of the $\\mathcal{H}$-calibration and $\\mathcal{H}$-consistency of adversarial surrogate losses. We show that convex loss functions, or the supremum-based convex losses often used in applications, are not $\\mathcal{H}$-calibrated for common hypothesis sets used in machine learning. We then give a characterization of $\\mathcal{H}$-calibration and prove that some surrogate losses are indeed $\\mathcal{H}$-calibrated for the adversarial zero-one loss, with common hypothesis sets. In particular, we fix some calibration results presented in prior work for a family of linear models and significantly generalize the results to the nonlinear hypothesis sets. Next, we show that $\\mathcal{H}$-calibration is not sufficient to guarantee consistency and prove that, in the absence of any distributional assumption, no continuous surrogate loss is consistent in the adversarial setting. This, in particular, proves that a claim made in prior work is inaccurate. Next, we identify natural conditions under which some surrogate losses that we describe in detail are $\\mathcal{H}$-consistent. We also report a series of empirical results which show that many $\\mathcal{H}$-calibrated surrogate losses are indeed not $\\mathcal{H}$-consistent, and validate our theoretical assumptions. Our adversarial $\\mathcal{H}$-consistency results are novel, even for the case where $\\mathcal{H}$ is the family of all measurable functions."}}
{"id": "o5eFe8ySMSv", "cdate": 1609459200000, "mdate": 1649124251298, "content": {"title": "Calibration and Consistency of Adversarial Surrogate Losses", "abstract": "Adversarial robustness is an increasingly critical property of classifiers in applications. The design of robust algorithms relies on surrogate losses since the optimization of the adversarial loss with most hypothesis sets is NP-hard. But which surrogate losses should be used and when do they benefit from theoretical guarantees? We present an extensive study of this question, including a detailed analysis of the H-calibration and H-consistency of adversarial surrogate losses. We show that, under some general assumptions, convex loss functions, or the supremum-based convex losses often used in applications, are not H-calibrated for important hypothesis sets such as generalized linear models or one-layer neural networks. We then give a characterization of H-calibration and prove that some surrogate losses are indeed H-calibrated for the adversarial loss, with these hypothesis sets. Next, we show that H-calibration is not sufficient to guarantee consistency and prove that, in the absence of any distributional assumption, no continuous surrogate loss is consistent in the adversarial setting. This, in particular, proves that a claim presented in a COLT 2020 publication is inaccurate. (Calibration results there are correct modulo subtle definition differences, but the consistency claim does not hold.) Next, we identify natural conditions under which some surrogate losses that we describe in detail are H-consistent for hypothesis sets such as generalized linear models and one-layer neural networks. We also report a series of empirical results with simulated data, which show that many H-calibrated surrogate losses are indeed not H-consistent, and validate our theoretical assumptions."}}
{"id": "8EJQe1XJGc6", "cdate": 1577836800000, "mdate": 1649124251292, "content": {"title": "Variational training of neural network approximations of solution maps for physical models", "abstract": "Highlights \u2022 A novel unsupervised solve-training framework is proposed to train neural network representation of solution maps. \u2022 Solve-training framework achieves effective representation of the solution map adapted to the input data distribution. \u2022 Solve-training framework efficiently obtains the solution maps of linear and nonlinear elliptic equations. \u2022 Solve-training framework efficiently learns maps from potential to ground states for linear and nonlinear Schrodinger operators. Abstract A novel solve-training framework is proposed to train neural network in representing low dimensional solution maps of physical models. Solve-training framework uses the neural network as the ansatz of the solution map and trains the network variationally via loss functions from the underlying physical models. Solve-training framework avoids expensive data preparation in the traditional supervised training procedure, which prepares labels for input data, and still achieves effective representation of the solution map adapted to the input data distribution. The efficiency of solve-training framework is demonstrated through obtaining solution maps for linear and nonlinear elliptic equations, and maps from potentials to ground states of linear and nonlinear Schr\u00f6dinger equations."}}
{"id": "viQ9xoj-xv_", "cdate": 1546300800000, "mdate": 1649124251309, "content": {"title": "Variational training of neural network approximations of solution maps for physical models", "abstract": "A novel solve-training framework is proposed to train neural network in representing low dimensional solution maps of physical models. Solve-training framework uses the neural network as the ansatz of the solution map and train the network variationally via loss functions from the underlying physical models. Solve-training framework avoids expensive data preparation in the traditional supervised training procedure, which prepares labels for input data, and still achieves effective representation of the solution map adapted to the input data distribution. The efficiency of solve-training framework is demonstrated through obtaining solutions maps for linear and nonlinear elliptic equations, and maps from potentials to ground states of linear and nonlinear Schr\\\"odinger equations."}}
