{"id": "gj4ZhnMQKY", "cdate": 1680307200000, "mdate": 1683898773806, "content": {"title": "Human Variability and the Explore-Exploit Trade-Off in Recommendation", "abstract": "The enormous scale of the available information and products on the Internet has necessitated the development of algorithms that intermediate between options and human users. These algorithms attempt..."}}
{"id": "WDwxQlNbDRl", "cdate": 1640995200000, "mdate": 1683898773821, "content": {"title": "A psychological theory of explainability", "abstract": "The goal of explainable Artificial Intelligence (XAI) is to generate human-interpretable explanations, but there are no computationally precise theories of how humans interpret AI generated explanations. The lack of theory means that validation of XAI must be done empirically, on a case-by-case basis, which prevents systematic theory-building in XAI. We propose a psychological theory of how humans draw conclusions from saliency maps, the most common form of XAI explanation, which for the first time allows for precise prediction of explainee inference conditioned on explanation. Our theory posits that absent explanation humans expect the AI to make similar decisions to themselves, and that they interpret an explanation by comparison to the explanations they themselves would give. Comparison is formalized via Shepard's universal law of generalization in a similarity space, a classic theory from cognitive science. A pre-registered user study on AI image classifications with saliency map explanations demonstrate that our theory quantitatively matches participants' predictions of the AI."}}
{"id": "R-Uf29_Z7Qp", "cdate": 1640995200000, "mdate": 1683898773754, "content": {"title": "A Psychological Theory of Explainability", "abstract": "The goal of explainable Artificial Intelligence (XAI) is to generate human-interpretable explanations, but there are no computationally precise theories of how humans interpret AI generated explana..."}}
{"id": "kJRE2XdGNJ4", "cdate": 1609459200000, "mdate": 1683898773764, "content": {"title": "Mitigating belief projection in explainable artificial intelligence via Bayesian Teaching", "abstract": "State-of-the-art deep-learning systems use decision rules that are challenging for humans to model. Explainable AI (XAI) attempts to improve human understanding but rarely accounts for how people typically reason about unfamiliar agents. We propose explicitly modeling the human explainee via Bayesian Teaching, which evaluates explanations by how much they shift explainees' inferences toward a desired goal. We assess Bayesian Teaching in a binary image classification task across a variety of contexts. Absent intervention, participants predict that the AI's classifications will match their own, but explanations generated by Bayesian Teaching improve their ability to predict the AI's judgements by moving them away from this prior belief. Bayesian Teaching further allows each case to be broken down into sub-examples (here saliency maps). These sub-examples complement whole examples by improving error detection for familiar categories, whereas whole examples help predict correct AI judgements of unfamiliar cases."}}
{"id": "gU_VFo0nX4M", "cdate": 1609459200000, "mdate": 1683898773754, "content": {"title": "Abstraction, Validation, and Generalization for Explainable Artificial Intelligence", "abstract": "Neural network architectures are achieving superhuman performance on an expanding range of tasks. To effectively and safely deploy these systems, their decision-making must be understandable to a wide range of stakeholders. Methods to explain AI have been proposed to answer this challenge, but a lack of theory impedes the development of systematic abstractions which are necessary for cumulative knowledge gains. We propose Bayesian Teaching as a framework for unifying explainable AI (XAI) by integrating machine learning and human learning. Bayesian Teaching formalizes explanation as a communication act of an explainer to shift the beliefs of an explainee. This formalization decomposes any XAI method into four components: (1) the inference to be explained, (2) the explanatory medium, (3) the explainee model, and (4) the explainer model. The abstraction afforded by Bayesian Teaching to decompose any XAI method elucidates the invariances among them. The decomposition of XAI systems enables modular validation, as each of the first three components listed can be tested semi-independently. This decomposition also promotes generalization through recombination of components from different XAI systems, which facilitates the generation of novel variants. These new variants need not be evaluated one by one provided that each component has been validated, leading to an exponential decrease in development time. Finally, by making the goal of explanation explicit, Bayesian Teaching helps developers to assess how suitable an XAI system is for its intended real-world use case. Thus, Bayesian Teaching provides a theoretical framework that encourages systematic, scientific investigation of XAI."}}
{"id": "D9Tg8Hlefw", "cdate": 1609459200000, "mdate": 1683898773816, "content": {"title": "Explainable AI for Natural Adversarial Images", "abstract": "Adversarial images highlight how vulnerable modern image classifiers are to perturbations outside of their training set. Human oversight might mitigate this weakness, but depends on humans understanding the AI well enough to predict when it is likely to make a mistake. In previous work we have found that humans tend to assume that the AI's decision process mirrors their own. Here we evaluate if methods from explainable AI can disrupt this assumption to help participants predict AI classifications for adversarial and standard images. We find that both saliency maps and examples facilitate catching AI errors, but their effects are not additive, and saliency maps are more effective than examples."}}
{"id": "7f7UrPAvVyL", "cdate": 1609459200000, "mdate": 1683898773759, "content": {"title": "Explainable AI for medical imaging: Explaining pneumothorax diagnoses with Bayesian Teaching", "abstract": "Limited expert time is a key bottleneck in medical imaging. Due to advances in image classification, AI can now serve as decision-support for medical experts, with the potential for great gains in radiologist productivity and, by extension, public health. However, these gains are contingent on building and maintaining experts' trust in the AI agents. Explainable AI may build such trust by helping medical experts to understand the AI decision processes behind diagnostic judgements. Here we introduce and evaluate explanations based on Bayesian Teaching, a formal account of explanation rooted in the cognitive science of human learning. We find that medical experts exposed to explanations generated by Bayesian Teaching successfully predict the AI's diagnostic decisions and are more likely to certify the AI for cases when the AI is correct than when it is wrong, indicating appropriate trust. These results show that Explainable AI can be used to support human-AI collaboration in medical imaging."}}
{"id": "Til2OYaHEHK", "cdate": 1577836800000, "mdate": 1659959605140, "content": {"title": "Interpretable Deep Gaussian Processes with Moments", "abstract": "Deep Gaussian Processes (DGPs) combine the the expressiveness of Deep Neural Networks (DNNs) with quantified uncertainty of Gaussian Processes (GPs). Expressive power and intractable inference both..."}}
{"id": "1m-i5VPz4z4", "cdate": 1577836800000, "mdate": 1683898773846, "content": {"title": "Replicating L2 learning in a Computational Model", "abstract": "Hundreds of millions of people learn a second language (L2).1When learning a specific L2, there are common errors for native speakers of a given L1 language, suggesting specific effects of L1 on L2 learning. Nevertheless, language instruction materials are designed based only on L2. We develop a computational model that mimics the behavior of a non-native speaker of a specific language to provide a deeper understanding of the problem of learning a second language. We use a Naive Bayes to model prepositional choices in English (L2) by native Mandarin (L1) speakers. Our results show that both correct and incorrect responses can be explained by the learner's L1 information. Moreover, our model predicts incorrect choices with no explicit training data of non-native mistakes. Our results thus provide a new medium to analyze and develop tools for L2 teaching."}}
{"id": "XN8cjVz3b0", "cdate": 1546300800000, "mdate": 1683898774320, "content": {"title": "A Unifying Computational Framework for Teaching and Active Learning", "abstract": "According to rational pedagogy models, learners take into account the way in which teachers generate evidence, and teachers take into account the way in which learners assimilate that evidence. The a..."}}
