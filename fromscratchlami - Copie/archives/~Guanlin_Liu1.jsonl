{"id": "jdIR6KF-uFW", "cdate": 1621629890270, "mdate": null, "content": {"title": "Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning", "abstract": "Due to the broad range of applications of reinforcement learning (RL), understanding the effects of adversarial attacks against RL model is essential for the safe applications of this model. Prior theoretical works on adversarial attacks against RL mainly focus on either reward poisoning attacks or environment poisoning attacks. In this paper, we introduce a new class of attacks named action poisoning attacks, where an adversary can change the action signal selected by the agent. Compared with existing attack models, the attacker\u2019s ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-box and black-box settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the black-box setting. We prove that LCB-H attack can force any efficient RL agent, whose dynamic regret scales sublinearly with the total number of steps taken, to choose actions according to a policy selected by the attacker very frequently, with only sublinear cost. In addition, we apply LCB-H attack against a very popular model-free RL algorithm: UCB-H. We show that, even in black-box setting, by spending only logarithm cost, the proposed LCB-H attack scheme can force the UCB-H agent to choose actions according to the policy selected by the attacker very frequently."}}
{"id": "mQ0TDi-4-0x", "cdate": 1609459200000, "mdate": 1682326028868, "content": {"title": "Efficient Action Poisoning Attacks on Linear Contextual Bandits", "abstract": "Contextual bandit algorithms have many applicants in a variety of scenarios. In order to develop trustworthy contextual bandit systems, understanding the impacts of various adversarial attacks on contextual bandit algorithms is essential. In this paper, we propose a new class of attacks: action poisoning attacks, where an adversary can change the action signal selected by the agent. We design action poisoning attack schemes against linear contextual bandit algorithms in both white-box and black-box settings. We further analyze the cost of the proposed attack strategies for a very popular and widely used bandit algorithm: LinUCB. We show that, in both white-box and black-box settings, the proposed attack schemes can force the LinUCB agent to pull a target arm very frequently by spending only logarithm cost."}}
{"id": "eqtY_zuTI9", "cdate": 1609459200000, "mdate": 1682326028868, "content": {"title": "Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning", "abstract": "Due to the broad range of applications of reinforcement learning (RL), understanding the effects of adversarial attacks against RL model is essential for the safe applications of this model. Prior theoretical works on adversarial attacks against RL mainly focus on either observation poisoning attacks or environment poisoning attacks. In this paper, we introduce a new class of attacks named action poisoning attacks, where an adversary can change the action signal selected by the agent. Compared with existing attack models, the attacker's ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-box and black-box settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the black-box setting. We prove that the LCB-H attack can force any efficient RL agent, whose dynamic regret scales sublinearly with the total number of steps taken, to choose actions according to a policy selected by the attacker very frequently, with only sublinear cost. In addition, we apply LCB-H attack against a popular model-free RL algorithm: UCB-H. We show that, even in the black-box setting, by spending only logarithm cost, the proposed LCB-H attack scheme can force the UCB-H agent to choose actions according to the policy selected by the attacker very frequently."}}
{"id": "QHkCsO2b-TS", "cdate": 1609459200000, "mdate": 1682326028888, "content": {"title": "Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning", "abstract": "Due to the broad range of applications of reinforcement learning (RL), understanding the effects of adversarial attacks against RL model is essential for the safe applications of this model. Prior theoretical works on adversarial attacks against RL mainly focus on either reward poisoning attacks or environment poisoning attacks. In this paper, we introduce a new class of attacks named action poisoning attacks, where an adversary can change the action signal selected by the agent. Compared with existing attack models, the attacker\u2019s ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-box and black-box settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the black-box setting. We prove that LCB-H attack can force any efficient RL agent, whose dynamic regret scales sublinearly with the total number of steps taken, to choose actions according to a policy selected by the attacker very frequently, with only sublinear cost. In addition, we apply LCB-H attack against a very popular model-free RL algorithm: UCB-H. We show that, even in black-box setting, by spending only logarithm cost, the proposed LCB-H attack scheme can force the UCB-H agent to choose actions according to the policy selected by the attacker very frequently."}}
{"id": "jCEOkoyxBIg", "cdate": 1577836800000, "mdate": 1682326028868, "content": {"title": "Action-Manipulation Attacks Against Stochastic Bandits: Attacks and Defense", "abstract": "Due to the broad range of applications of stochastic multi-armed bandit model, understanding the effects of adversarial attacks and designing bandit algorithms robust to attacks are essential for the safe applications of this model. In this paper, we introduce a new class of attack named action-manipulation attack. In this attack, an adversary can change the action signal selected by the user. We show that without knowledge of mean rewards of arms, our proposed attack can manipulate Upper Confidence Bound (UCB) algorithm, a widely used bandit algorithm, into pulling a target arm very frequently by spending only logarithmic cost. To defend against this class of attacks, we introduce a novel algorithm that is robust to action-manipulation attacks when an upper bound for the total attack cost is given. We prove that our algorithm has a pseudo-regret upper bounded by $\\mathcal{O}(\\max\\{\\log T,A\\})$, where $T$ is the total number of rounds and $A$ is the upper bound of the total attack cost."}}
{"id": "g5CRksxTsX", "cdate": 1577836800000, "mdate": 1682326028868, "content": {"title": "Action-Manipulation Attacks on Stochastic Bandits", "abstract": "As stochastic multi-armed bandit model has many important applications, understanding the impact of adversarial attacks on this model is essential for the safe applications of this model. In this paper, we propose a new class of attack named action-manipulation attack, where an adversary can change the action signal selected by the user. We investigate the attack against a very popular and widely used bandit algorithm: Upper Confidence Bound (UCB) algorithm. Without knowledge of mean rewards of arms, our proposed attack scheme can force the user to pull a target arm very frequently by spending only logarithm cost."}}
{"id": "R4mIe9BGJK", "cdate": 1577836800000, "mdate": 1682326028880, "content": {"title": "Action-Manipulation Attacks Against Stochastic Bandits: Attacks and Defense", "abstract": "Due to the broad range of applications of stochastic multi-armed bandit model, understanding the effects of adversarial attacks and designing bandit algorithms robust to attacks are essential for the safe applications of this model. In this paper, we introduce a new class of attacks named action-manipulation attacks. In this class of attacks, an adversary can change the action signal selected by the user. We show that without knowledge of mean rewards of arms, our proposed attack can manipulate Upper Confidence Bound (UCB) algorithm, a widely used bandit algorithm, into pulling a target arm very frequently by spending only logarithmic cost. To defend against this class of attacks, we introduce a novel algorithm that is robust to action-manipulation attacks when an upper bound for the total attack cost is given. We prove that our algorithm has a pseudo-regret upper bounded by O(max{log T, A}) with a high probability, where T is the total number of rounds and A is the upper bound of the total attack cost."}}
