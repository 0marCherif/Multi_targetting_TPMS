{"id": "76fkZiP_J4U", "cdate": 1685198070352, "mdate": 1685198070352, "content": {"title": "Problems in the deployment of machine-learned models in health care", "abstract": "KEY POINTS\n+ Decision-support systems or clinical prediction tools based on machine learning (including the special case of deep learning) are similar to clinical support tools developed using classical statistical models and, as such, have similar limitations.\n+ If a machine-learned model is trained using data that do not match the data it will encounter when deployed, its performance may be lower than expected.\n+ When training, machine learning algorithms take the \u201cpath of least resistance,\u201d leading them to learn features from the data that are spuriously correlated with target outputs instead of the correct features; this can impair the effective generalization of the resulting learned model.\n+ Avoiding errors related to these problems involves careful evaluation of machine-learned models using new data from the performance distribution, including data samples that are expected to \u201ctrick\u201d the model, such as those with different population demographics, difficult conditions or bad-quality inputs.\n\nIn a companion article, Verma and colleagues discuss how machine-learned solutions can be developed and implemented to support medical decision-making.1 Both decision-support systems and clinical prediction tools developed using machine learning (including the special case of deep learning) are similar to clinical support tools developed using classical statistical models and, as such, have similar limitations.2,3 A model that makes incorrect predictions can lead its users to make errors they otherwise would not have made when caring for patients, and therefore it is important to understand how these models can fail.4 We discuss these limitations \u2014 focusing on 2 issues in particular: out-of-distribution (or out-of-sample) generalization and incorrect feature attribution \u2014 to underscore the need to consider potential caveats when using machine-learned solutions."}}
{"id": "s5aYvDauYG", "cdate": 1664725482076, "mdate": null, "content": {"title": "How many trained neural networks are needed for influence estimation in modern deep learning?", "abstract": "Influence estimation attempts to estimate the effect of removing a training example on downstream predictions. Prior work has shown that a first-order approximation to estimate influence does not agree with the ground-truth of re-training or fine-tuning without a training example. Recently, Feldman and Zhang [2020] created an influence estimator that provides meaningful influence estimates but requires training thousands of models on large subsets of a dataset. In this work, we explore how the method in Feldman and Zhang [2020] scales with the number of trained models. We also show empirical and analytical results in the standard influence estimation setting that provide intuitions about the role of nondeterminism in neural network training and how the accuracy of test predictions affects the number of models needed to detect an influential training example. We ultimately find that a large amount of models are needed for influence estimation, though the exact number is hard to quantify due to training nondeterminism and depends on test example difficulty, which varies between tasks.\n\n\n"}}
{"id": "pX21pH4CsNB", "cdate": 1663850476461, "mdate": null, "content": {"title": "Differentially Private Diffusion Models", "abstract": "While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. However, training DP generative models is highly challenging due to the noise injected into training to enforce DP. We propose to leverage diffusion models (DMs), an emerging class of deep generative models, and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We motivate why DP-SGD is well suited for training DPDMs, and thoroughly investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs. Furthermore, we propose noise multiplicity, a simple yet powerful modification of the DM training objective tailored to the DP setting to boost performance. We validate our novel DPDMs on widely-used image generation benchmarks and achieve state-of-the-art (SOTA) performance by large margins. For example, on MNIST we improve the SOTA FID from 48.4 to 5.01 and downstream classification accuracy from 83.2% to 98.1% for the privacy setting DP-$(\\varepsilon=10, \\delta=10^{-5})$. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models."}}
{"id": "oUg5rC_95OM", "cdate": 1623124977255, "mdate": null, "content": {"title": "A Benchmark of Medical Out of Distribution Detection", "abstract": "Motivation: Deep learning models deployed on medical tasks can be equipped with Out-of-Distribution Detection (OoDD) methods in order to avoid erroneous predictions. However it is unclear which OoDD methods are effective in practice. \n\nSpecific Problem: Systems trained for one particular domain of images cannot be expected to perform accurately on images of a different domain.  These images should be flagged by an OoDD method prior to prediction. \n\nOur approach: This paper defines 3 categories of OoD examples and benchmarks popular OoDD methods in three domains of medical imaging: chest X-ray, fundus imaging, and histology slides. \n\nResults: Our experiments show that despite methods yielding good results on some categories of out-of-distribution samples, they fail to recognize images close to the training distribution.\n\nConclusion: We find a simple binary classifier on the feature representation has the best accuracy and AUPRC on average. Users of diagnostic tools which employ these OoDD methods should still remain vigilant that images very close to the training distribution yet not in it could yield unexpected results."}}
{"id": "NEQYGJr1qL3", "cdate": 1621630084903, "mdate": null, "content": {"title": "Scalable Neural Data Server: A Data Recommender for Transfer Learning", "abstract": "Absence of large-scale labeled data in the practitioner's target domain can be a bottleneck to applying machine learning algorithms in practice. Transfer learning is a popular strategy for leveraging additional data to improve the downstream performance, but finding the most relevant data to transfer from can be challenging. Neural Data Server (NDS), a search engine that recommends relevant data for a given downstream task, has been previously proposed to address this problem (Yan et al., 2020). NDS uses a mixture of experts trained on data sources to estimate similarity between each source and the downstream task. Thus, the computational cost to each user grows with the number of sources and requires an expensive training step for each data provider.\n\nTo address these issues, we propose Scalable Neural Data Server (SNDS), a large-scale search engine that can theoretically index thousands of datasets to serve relevant ML data to end users. SNDS trains the mixture of experts on intermediary datasets during initialization, and represents both data sources and downstream tasks by their proximity to the intermediary datasets. As such, computational cost incurred by users of SNDS remains fixed as new datasets are added to the server, without pre-training for the data providers.\n\nWe validate SNDS on a plethora of real world tasks and find that data recommended by SNDS improves downstream task performance over baselines. We also demonstrate the scalability of our system by demonstrating its ability to select relevant data for transfer outside of the natural image setting."}}
{"id": "waWmZSw0mn", "cdate": 1621630042916, "mdate": null, "content": {"title": "Don\u2019t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence", "abstract": "Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for controlling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images. "}}
{"id": "zgMPc_48Zb", "cdate": 1601308266428, "mdate": null, "content": {"title": "Differentially Private Generative Models Through Optimal Transport", "abstract": "Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge and provide indirect access to the private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn relies on minimizing the Sinkhorn divergence---a computationally efficient approximation to the exact optimal transport distance---between the model and the data in a differentially private manner and also uses a novel technique for conditional generation in the Sinkhorn framework. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by the privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, despite our method's simplicity we improve upon the state-of-the-art on multiple image modeling benchmarks. We also show differentially private synthesis of informative RGB images, which has not been demonstrated before by differentially private generative models without the use of auxiliary public data."}}
{"id": "O1jyld3pLrQ", "cdate": 1598644826260, "mdate": null, "content": {"title": "BabyAI++: Towards Grounded-Language Learning beyond Memorization", "abstract": "Despite success in many real-world tasks (e.g., robotics), reinforcement learning\n(RL) agents still learn from tabula rasa when facing new and dynamic scenarios.\nBy contrast, humans can offload this burden through textual descriptions. Although\nrecent works have shown the benefits of instructive texts in goal-conditioned RL,\nfew have studied whether descriptive texts help agents to generalize across dynamic\nenvironments. To promote research in this direction, we introduce a new platform,\nBabyAI++, to generate various dynamic environments along with corresponding\ndescriptive texts. Moreover, we benchmark several baselines inherited from the instruction following setting and develop a novel approach towards visually-grounded\nlanguage learning on our platform. Extensive experiments show strong evidence\nthat using descriptive texts improves the generalization of RL agents across environments with varied dynamics. Code for BabyAI++ platform and baselines are\navailable online: https://github.com/caotians1/BabyAIPlusPlus"}}
{"id": "1ABDN92t49", "cdate": 1579955645638, "mdate": null, "content": {"title": "Which MOoD Methods work? A Benchmark of Medical Out of Distribution Detection", "abstract": "There is a rise in the use of deep learning for automated medical diagnosis, most notably in  medical  imaging.   Such  an  automated  system  uses  a  set  of  images  from  a  patient  to diagnose whether they have a disease.  However, systems trained for one particular domain of images cannot be expected to perform accurately on images of a different domain.  These images should be filtered out by an Out-of-Distribution Detection (OoDD) method prior to diagnosis.  This paper benchmarks popular OoDD methods in three domains of medical imaging:  chest x-rays, fundus images and histology slides.  Despite methods yielding good results on all three domains, they fail to recognize images close to the training distribution."}}
{"id": "HkgB2TNYPS", "cdate": 1569439149001, "mdate": null, "content": {"title": "A Theoretical Analysis of the Number of Shots in Few-Shot Learning", "abstract": "Few-shot classification is the task of predicting the category of an example from a set of few labeled examples. The number of labeled examples per category is called the number of shots (or shot number). Recent works tackle this task through meta-learning, where a meta-learner extracts information from observed tasks during meta-training to quickly adapt to new tasks during meta-testing. In this formulation, the number of shots exploited during meta-training has an impact on the recognition performance at meta-test time. Generally, the shot number used in meta-training should match the one used in meta-testing to obtain the best performance. We introduce a theoretical analysis of the impact of the shot number on Prototypical Networks, a state-of-the-art few-shot classification method. From our analysis, we propose a simple method that is robust to the choice of shot number used during meta-training, which is a crucial hyperparameter. The performance of our model trained for an arbitrary meta-training shot number shows great performance for different values of meta-testing shot numbers. We experimentally demonstrate our approach on different few-shot classification benchmarks."}}
