{"id": "u1IgPNkEs1", "cdate": 1698655768568, "mdate": 1698655768568, "content": {"title": "CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow", "abstract": "Despite impressive performance for high-level downstream tasks, self-supervised pre-training methods have not yet fully delivered on dense geometric vision tasks such as stereo matching or optical flow. The application of self-supervised concepts, such as instance discrimination or masked image modeling, to geometric tasks is an active area of research. In this work, we build on the recent cross-view completion framework, a variation of masked image modeling that leverages a second view from the same scene which makes it well suited for binocular downstream tasks. The applicability of this concept has so far been limited in at least two ways: (a) by the difficulty of collecting real-world image pairs -- in practice only synthetic data have been used -- and (b) by the lack of generalization of vanilla transformers to dense downstream tasks for which relative position is more meaningful than absolute position. We explore three avenues of improvement. First, we introduce a method to collect suitable real-world image pairs at large scale. Second, we experiment with relative positional embeddings and show that they enable vision transformers to perform substantially better. Third, we scale up vision transformer based cross-completion architectures, which is made possible by the use of large amounts of data. With these improvements, we show for the first time that state-of-the-art results on stereo matching and optical flow can be reached without using any classical task-specific techniques like correlation volume, iterative estimation, image warping or multi-scale reasoning, thus paving the way towards universal vision models."}}
{"id": "1CdZBzRb9X", "cdate": 1698655590316, "mdate": 1698655590316, "content": {"title": "SACReg: Scene-Agnostic Coordinate Regression for Visual Localization", "abstract": "Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets. In this paper, we propose a new paradigm where a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations. The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input. It is trained on a few diverse datasets and significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization. In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches."}}
{"id": "P0czwrookpb", "cdate": 1698655511619, "mdate": 1698655511619, "content": {"title": "MFOS: Model-Free & One-Shot Object Pose Estimation", "abstract": "Existing learning-based methods for object pose estimation in RGB images are mostly model-specific or category based. They lack the capability to generalize to new object categories at test time, hence severely hindering their practicability and scalability. Notably, recent attempts have been made to solve this issue, but they still require accurate 3D data of the object surface at both train and test time. In this paper, we introduce a novel approach that can estimate in a single forward pass the pose of objects never seen during training, given minimum input. In contrast to existing state-of-the-art approaches, which rely on task-specific modules, our proposed model is entirely based on a transformer architecture, which can benefit from recently proposed 3D-geometry general pretraining. We conduct extensive experiments and report state-of-the-art one-shot performance on the challenging LINEMOD benchmark. Finally, extensive ablations allow us to determine good practices with this relatively new type of architecture in the field."}}
{"id": "Hq9sMoGRykm", "cdate": 1667380394884, "mdate": 1667380394884, "content": {"title": "Robust Automatic Monocular Vehicle Speed Estimation for Traffic Surveillance", "abstract": "Even though CCTV cameras are widely deployed for\ntraffic surveillance and have therefore the potential of becoming cheap automated sensors for traffic speed analysis,\ntheir large-scale usage toward this goal has not been reported yet. A key difficulty lies in fact in the camera calibration phase. Existing state-of-the-art methods perform the\ncalibration using image processing or keypoint detection\ntechniques that require high-quality video streams, yet typical CCTV footage is low-resolution and noisy. As a result,\nthese methods largely fail in real-world conditions. In contrast, we propose two novel calibration techniques whose\nonly inputs come from an off-the-shelf object detector. Both\nmethods consider multiple detections jointly, leveraging the\nfact that cars have similar and well-known 3D shapes with\nnormalized dimensions. The first one is based on minimizing an energy function corresponding to a 3D reprojection\nerror, the second one instead learns from synthetic training\ndata to predict the scene geometry directly. Noticing the\nlack of speed estimation benchmarks faithfully reflecting the\nactual quality of surveillance cameras, we introduce a novel\ndataset collected from public CCTV streams. Experimental results conducted on three diverse benchmarks demonstrate excellent speed estimation accuracy that could enable\nthe wide use of CCTV cameras for traffic analysis, even in\nchallenging conditions where state-of-the-art methods completely fail. Additional information can be found on our\nproject web page: https://rebrand.ly/nle-cctv"}}
{"id": "skXJ84YqMue", "cdate": 1667380331963, "mdate": 1667380331963, "content": {"title": "PUMP: Pyramidal and Uniqueness Matching Priors for Unsupervised Learning of Local Descriptors", "abstract": "Existing approaches for learning local image descriptors have shown remarkable achievements in a wide range\nof geometric tasks. However, most of them require per-pixel\ncorrespondence-level supervision, which is difficult to acquire at scale and in high quality. In this paper, we propose\nto explicitly integrate two matching priors in a single loss in\norder to learn local descriptors without supervision. Given\ntwo images depicting the same scene, we extract pixel descriptors and build a correlation volume. The first prior\nenforces the local consistency of matches in this volume via\na pyramidal structure iteratively constructed using a nonparametric module. The second prior exploits the fact that\neach descriptor should match with at most one descriptor from the other image. We combine our unsupervised\nloss with a standard self-supervised loss trained from synthetic image augmentations. Feature descriptors learned\nby the proposed approach outperform their fully- and selfsupervised counterparts on various geometric benchmarks\nsuch as visual localization and image matching, achieving state-of-the-art performance. Project webpage: https:\n//europe.naverlabs.com/research/3d-vision/pump."}}
{"id": "wZEfHUM5ri", "cdate": 1652737581232, "mdate": null, "content": {"title": "CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion", "abstract": "Masked Image Modeling (MIM) has recently been established as a potent pre-training paradigm. A pretext task is constructed by masking patches in an input image, and this masked content is then predicted by a neural network using visible patches as sole input. This pre-training leads to state-of-the-art performance when finetuned for high-level semantic tasks, e.g. image classification and object detection. In this paper we instead seek to learn representations that transfer well to a wide variety of 3D vision and lower-level geometric downstream tasks, such as depth prediction or optical flow estimation. Inspired by MIM, we propose an unsupervised representation learning task trained from pairs of images showing the same scene from different viewpoints. More precisely, we propose the pretext task of cross-view completion where the first input image is partially masked, and this masked content has to be reconstructed from the visible content and the second image. In single-view MIM, the masked content often cannot be inferred precisely from the visible portion only, so the model learns to act as a prior influenced by high-level semantics. In contrast, this ambiguity can be resolved with cross-view completion from the second unmasked image, on the condition that the model is able to understand the spatial relationship between the two images. Our experiments show that our pretext task leads to significantly improved performance for monocular 3D vision downstream tasks such as depth estimation. In addition, our model can be directly applied to binocular downstream tasks like optical flow or relative camera pose estimation, for which we obtain competitive results without bells and whistles, i.e., using a generic architecture without any task-specific design."}}
{"id": "HkvYtImh8D", "cdate": 1580467318331, "mdate": null, "content": {"title": "Learning with Average Precision: Training Image Retrieval with a Listwise Loss", "abstract": "Image retrieval can be formulated as a ranking problem where the goal is to order database images by decreasing similarity to the query. Recent deep models for image retrieval have outperformed traditional methods by leveraging ranking-tailored loss functions, but important theoretical and practical problems remain. First, rather than directly optimizing the global ranking, they minimize an upper-bound on the essential loss, which does not necessarily result in an optimal mean average precision (mAP). Second, these methods require significant engineering efforts to work well, e.g., special pre-training and hard-negative mining. In this paper we propose instead to directly optimize the global mAP by leveraging recent advances in listwise loss formulations. Using a histogram binning approximation, the AP can be differentiated and thus employed to end-to-end learning. Compared to existing losses, the proposed method considers thousands of images simultaneously at each iteration and eliminates the need for ad hoc tricks. It also establishes a new state of the art on many standard retrieval benchmarks. Models and evaluation scripts have been made available at: https://europe.naverlabs.com/Deep-Image-Retrieval/. "}}
{"id": "3wL0UEo6Q9", "cdate": 1580223614774, "mdate": null, "content": {"title": "R2D2: Repeatable and Reliable Detector and Descriptor", "abstract": "Interest point detection and local feature description are fundamental steps in many\ncomputer vision applications. Classical approaches are based on a detect-thendescribe paradigm where separate handcrafted methods are used to first identify\nrepeatable keypoints and then represent them with a local descriptor. Neural\nnetworks trained with metric learning losses have recently caught up with these\ntechniques, focusing on learning repeatable saliency maps for keypoint detection\nor learning descriptors at the detected keypoint locations. In this work, we argue\nthat repeatable regions are not necessarily discriminative and can therefore lead\nto select suboptimal keypoints. Furthermore, we claim that descriptors should be\nlearned only in regions for which matching can be performed with high confidence.\nWe thus propose to jointly learn keypoint detection and description together with\na predictor of the local descriptor discriminativeness. This allows to avoid ambiguous areas, thus leading to reliable keypoint detection and description. Our\ndetection-and-description approach simultaneously outputs sparse, repeatable and\nreliable keypoints that outperforms state-of-the-art detectors and descriptors on the\nHPatches dataset and on the recent Aachen Day-Night localization benchmark."}}
{"id": "HyeN9SrgIS", "cdate": 1567802763970, "mdate": null, "content": {"title": "R2D2: Reliable and Repeatable Detector and Descriptor", "abstract": "Interest point detection and local feature description are fundamental steps in many computer vision applications. Classical methods for these tasks are based on a detect-then-describe paradigm where separate handcrafted methods are used to first identify repeatable keypoints and then represent them with a local descriptor. Neural networks trained with metric learning losses have recently caught up with these techniques, focusing on learning repeatable saliency maps for keypoint detection and learning descriptors at the detected keypoint locations.  In this work, we argue that salient regions are not necessarily discriminative, and therefore can harm the performance of the description. Furthermore, we claim that descriptors should be learned only in regions for which matching can be performed with high confidence. We thus propose to jointly learn keypoint detection and description together with a predictor of the local descriptor discriminativeness. This allows us to avoid ambiguous areas and lead to reliable keypoint detections and descriptions. Our detection-and-description approach, trained with self-supervision, can simultaneously output sparse, repeatable and reliable keypoints that outperforms state-of-the-art detectors and descriptors on the HPatches dataset."}}
