{"id": "rkVz4j-_bS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity", "abstract": "Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a monotone submodular f..."}}
{"id": "rJNVHjbuZB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity", "abstract": "Submodular maximization is a general optimization problem with a wide range of applications in machine learning (e.g., active learning, clustering, and feature selection). In large-scale optimizati..."}}
{"id": "rk-8HnbObB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints", "abstract": "Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an in..."}}
{"id": "HyWHwnZuWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Data Summarization at Scale: A Two-Stage Submodular Approach", "abstract": "The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summarizat..."}}
{"id": "B1ZnDiZd-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Proportional Allocation: Simple, Distributed, and Diverse Matching with High Entropy", "abstract": "Inspired by many applications of bipartite matching in online advertising and machine learning, we study a simple and natural iterative proportional allocation algorithm: Maintain a priority score ..."}}
{"id": "rk4_UhZ_WS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Probabilistic Submodular Maximization in Sub-Linear Time", "abstract": "In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items m..."}}
{"id": "Sk-_KfZd-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "Submodular Optimization Over Sliding Windows", "abstract": "Maximizing submodular functions under cardinality constraints lies at the core of numerous data mining and machine learning applications, including data diversification, data summarization, and coverage problems. In this work, we study this question in the context of data streams, where elements arrive one at a time, and we want to design low-memory and fast update-time algorithms that maintain a good solution. Specifically, we focus on the sliding window model, where we are asked to maintain a solution that considers only the last W items. In this context, we provide the first non-trivial algorithm that maintains a provable approximation of the optimum using space sublinear in the size of the window. In particular we give a 1/3 - \u03b5 approximation algorithm that uses space polylogarithmic in the spread of the values of the elements, \u03b4, and linear in the solution size k for any constant \u03b5 > 0. At the same time, processing each element only requires a polylogarithmic number of evaluations of the function itself. When a better approximation is desired, we show a different algorithm that, at the cost of using more memory, provides a 1/2 - \u03b5 approximation, and allows a tunable trade-off between average update time and space. This algorithm matches the best known approximation guarantees for submodular optimization in insertion-only streams, a less general formulation of the problem. We demonstrate the efficacy of the algorithms on a number of real world datasets, showing that their practical performance far exceeds the theoretical bounds. The algorithms preserve high quality solutions in streams with millions of items, while storing a negligible fraction of them."}}
{"id": "H1Zn4RxuZS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Scalable Feature Selection via Distributed Diversity Maximization", "abstract": "Feature selection is a fundamental problem in machine learning and data mining. The majority of feature selection algorithms are designed for running on a single machine (centralized setting) and they are less applicable to very large datasets. Although there are some distributed methods to tackle this problem, most of them are distributing the data horizontally which are not suitable for datasets with a large number of features and few number of instances. Thus, in this paper, we introduce a novel vertically distributable feature selection method in order to speed up this process and be able to handle very large datasets in a scalable manner. In general, feature selection methods aim at selecting relevant and non-redundant features (Minimum Redundancy and Maximum Relevance). It is much harder to consider redundancy in a vertically distributed setting than a centralized setting since there is no global access to the whole data. To the best of our knowledge, this is the first attempt toward solving the feature selection problem with a vertically distributed filter method which handles the redundancy with consistently comparable results with centralized methods. In this paper, we formalize the feature selection problem as a diversity maximization problem by introducing a mutual-information-based metric distance on the features. We show the effectiveness of our method by performing an extensive empirical study. In particular, we show that our distributed method outperforms state-of-the-art centralized feature selection algorithms on a variety of datasets. From a theoretical point of view, we have proved that the used greedy algorithm in our method achieves an approximation factor of 1/4 for the diversity maximization problem in a distributed setting with high probability. Furthermore, we improve this to 8/25 expected approximation using multiplicity in our distribution."}}
{"id": "SkW4sUWuZB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Fast Distributed Submodular Cover: Public-Private Data Summarization", "abstract": "In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy. The goal is to provide a succinct summary of massive dataset, ideally as small as possible, from which customized summaries can be built for each user, i.e. it can contain elements from the public data (for diversity) and users' private data (for personalization). To formalize the above challenge, we assume that the scoring function according to which a user evaluates the utility of her summary satisfies submodularity, a widely used notion in data summarization applications. Thus, we model the data summarization targeted to each user as an instance of a submodular cover problem. However, when the data is massive it is infeasible to use the centralized greedy algorithm to find a customized summary even for a single user. Moreover, for a large pool of users, it is too time consuming to find such summaries separately. Instead, we develop a fast distributed algorithm for submodular cover, FASTCOVER, that provides a succinct summary in one shot and for all users. We show that the solution provided by FASTCOVER is competitive with that of the centralized algorithm with the number of rounds that is exponentially smaller than state of the art results. Moreover, we have implemented FASTCOVER with Spark to demonstrate its practical performance on a number of concrete applications, including personalized location recommendation, personalized movie recommendation, and dominating set on tens of millions of data points and varying number of users."}}
{"id": "By-sYsZ_-S", "cdate": 1451606400000, "mdate": null, "content": {"title": "Horizontally Scalable Submodular Maximization", "abstract": "A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawb..."}}
