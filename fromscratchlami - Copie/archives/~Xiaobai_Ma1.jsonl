{"id": "Vi3eTI48FBP", "cdate": 1577836800000, "mdate": null, "content": {"title": "Normalizing Flow Policies for Multi-agent Systems", "abstract": "Stochastic policy gradient methods using neural representations have had considerable success in single-agent domains with continuous action spaces. These methods typically use networks that output the parameters of a diagonal Gaussian distribution from which the resulting action is sampled. In multi-agent contexts, however, better policies may require complex multimodal action distributions. Based on recent progress in density modeling, we propose an alternative for policy representation in the form of conditional normalizing flows. This approach allows for greater flexibility in action distribution representation beyond mixture models. We demonstrate their advantage over standard methods on a set of tasks including human behavior modeling and reinforcement learning in multi-agent settings."}}
{"id": "MYcML5HeTrZ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Handling Missing Data with Graph Representation Learning", "abstract": "Machine learning with missing data has been approached in two different ways, including feature imputation where missing feature values are estimated based on observed values, and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label prediction often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a graph-based framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using a graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph Neural Networks. Experimental results on nine benchmark datasets show that GRAPE yields 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks, compared with existing state-of-the-art methods."}}
{"id": "CzLxfflyKv5", "cdate": 1577836800000, "mdate": null, "content": {"title": "Normalizing Flow Model for Policy Representation in Continuous Action Multi-agent Systems", "abstract": "Neural networks that output the parameters of a diagonal Gaussian distribution are widely used in reinforcement learning tasks with continuous action spaces. They have had considerable success in single-agent domains and even in some multi-agent tasks. However, general multi-agent tasks often require mixed strategies whose distributions cannot be well approximated by Gaussians or their mixtures. This paper proposes an alternative for policy representation based on normalizing flows. This approach allows for greater flexibility in action distribution representation beyond mixture models. We demonstrate their advantage over standard methods on a set of imitation learning tasks modeling human driving behaviors in the presence of other drivers."}}
{"id": "A4Qjsfi4iAZ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reinforcement Learning for Autonomous Driving with Latent State Inference and Spatial-Temporal Relationships", "abstract": "Deep reinforcement learning (DRL) provides a promising way for learning navigation in complex autonomous driving scenarios. However, identifying the subtle cues that can indicate drastically different outcomes remains an open problem with designing autonomous systems that operate in human environments. In this work, we show that explicitly inferring the latent state and encoding spatial-temporal relationships in a reinforcement learning framework can help address this difficulty. We encode prior knowledge on the latent states of other drivers through a framework that combines the reinforcement learner with a supervised learner. In addition, we model the influence passing between different vehicles through graph neural networks (GNNs). The proposed framework significantly improves performance in the context of navigating T-intersections compared with state-of-the-art baseline approaches."}}
{"id": "_iZgKvp49A5", "cdate": 1546300800000, "mdate": null, "content": {"title": "Monte-Carlo Tree Search for Policy Optimization", "abstract": "Gradient-based methods are often used for policy optimization in deep reinforcement learning, despite being vulnerable to local optima and saddle points. Although gradient-free methods (e.g., genetic algorithms or evolution strategies) help mitigate these issues, poor initialization and local optima are still concerns in highly nonconvex spaces. This paper presents a method for policy optimization based on Monte-Carlo tree search and gradient-free optimization. Our method, called Monte-Carlo tree search for policy optimization (MCTSPO), provides a better exploration-exploitation trade-off through the use of the upper confidence bound heuristic. We demonstrate improved performance on reinforcement learning tasks with deceptive or sparse reward functions compared to popular gradient-based and deep genetic algorithm baselines."}}
{"id": "Dn-jCPxWVuo", "cdate": 1546300800000, "mdate": null, "content": {"title": "Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning", "abstract": "To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods."}}
{"id": "97fDIGFepqp", "cdate": 1546300800000, "mdate": null, "content": {"title": "Monte Carlo Tree Search for Policy Optimization", "abstract": "Gradient-based methods are often used for policy optimization in deep reinforcement learning, despite being vulnerable to local optima and saddle points. Although gradient-free methods (e.g., genetic algorithms or evolution strategies) help mitigate these issues, poor initialization and local optima are still concerns in highly nonconvex spaces. This paper presents a method for policy optimization based on Monte-Carlo tree search and gradient-free optimization. Our method, called Monte-Carlo tree search for policy optimization (MCTSPO), provides a better exploration-exploitation trade-off through the use of the upper confidence bound heuristic. We demonstrate improved performance on reinforcement learning tasks with deceptive or sparse reward functions compared to popular gradient-based and deep genetic algorithm baselines."}}
{"id": "nwzeW6PE_c-", "cdate": 1514764800000, "mdate": null, "content": {"title": "3-D Decentralized Prioritized Motion Planning and Coordination for High-Density Operations of Micro Aerial Vehicles", "abstract": "This paper presents a decentralized motion planning method for multiple aerial vehicles moving among 3-D polygonal obstacles resembling an urbanlike environment. The algorithm combines a prioritized A* algorithm for high-level planning, along with a coordination method based on barrier functions for low-level trajectory generation and vehicle control. To this end, we extend the barrier functions method developed in our earlier work so that it treats 2-D and 3-D polygonal obstacles, and generates collision-free trajectories for the multiagent system. We furthermore augment the low-level trajectory generation and control with a prioritized A* path planning algorithm, in order to compute waypoints and paths that force the agents of lower priority to avoid the paths of the agents of higher priority, reducing thus congestion. This feature enhances further the performance of the barrier-based coordination, and results in shorter paths and time to the goal destinations. We finally extend the proposed control design to the agents of constrained double-integrator dynamics, compared with the single-integrator case in our earlier work. We assume that the obstacles are known to the agents, and that each agent knows the state of other agents lying in its sensing area. Simulation results in 2-D and 3-D polygonal environments, as well as experimental results with micro aerial vehicles (quadrotors) in an indoor lab environment demonstrate the efficacy of the proposed approach."}}
{"id": "WbcCIGgqhLr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning", "abstract": "To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods."}}
