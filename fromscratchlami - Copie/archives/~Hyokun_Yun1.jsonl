{"id": "VezcnWFSd2d", "cdate": 1663850254591, "mdate": null, "content": {"title": "On Threshold Functions in Learning to Generate Feasible Solutions of Mixed Integer Programs", "abstract": "Finding a high-quality feasible solution to a combinatorial optimization problem in a given time budget is a challenging task due to its discrete nature. Neural diving is a learning-based approach to generating partial assignments for the discrete variables in MIP. We find that there usually is a small range of selection rates which lead to feasible and optimal solutions; when too many parameters are selected, the solution space is too restricted to find a feasible solution; when too few parameters are selected, the solution space is too wide to efficiently find a feasible solution. Therefore, the choice of selection rate is the critical determinant of the Neural diving performance. In this context, we present theoretical insights that there exist threshold functions in feasibility and feasible optimality over the selection rate. Based on the theoretical foundations, we introduce a post-hoc method, and a learning-based approach to optimize the selection rate for partial discrete variable assignments in MIP more efficiently. A key idea is to jointly learn to restrict the selection rate search space, and to predict the selection rate in the learned search space that results in a high-quality feasible solution. MIP solver is integrated into the end-to-end learning framework. We suggest that learning a deep neural network to generate a threshold-aware selection rate is effective in finding high-quality feasible solutions more quickly. Experimental results demonstrate that our method achieves state-of-the-art performance in NeurIPS ML4CO datasets. In the workload apportionment dataset, our method achieves the optimality gap of 0.45%, which is around 10\u00d7 better than SCIP, at the one-minute time limit."}}
{"id": "b4jq1xzirPS", "cdate": 1632875512889, "mdate": null, "content": {"title": "An Attention-LSTM Hybrid Model for the Coordinated Routing of Multiple Vehicles", "abstract": "Reinforcement learning has recently shown promise in learning quality solutions in a number of combinatorial optimization problems. In particular, the attention-based encoder-decoder models show high effectiveness on various routing problems, including the Traveling Salesman Problem (TSP).  Unfortunately, they perform poorly for the TSP with Drones (TSP-D), requiring routing a heterogeneous fleet of vehicles in coordination. In TSP-D, two different types of vehicles are moving in tandem and may need to wait at a node for the other vehicle to join. State-less attention-based decoder fails to make such coordination between vehicles. We propose an attention encoder-LSTM decoder hybrid model, in which the decoder's hidden state can represent the sequence of actions made. We empirically demonstrate that such a hybrid model improves upon a purely attention-based model for both solution quality and computational efficiency. Our experiments on the min-max Capacitated Vehicle Routing Problem (mmCVRP) also confirm that the hybrid model is more suitable for coordinated routing of multiple vehicles than the attention-based model."}}
{"id": "ry018WZAZ", "cdate": 1518730168877, "mdate": null, "content": {"title": "Deep Active Learning for Named Entity Recognition", "abstract": "Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data."}}
{"id": "r1E5VzM_-S", "cdate": 1451606400000, "mdate": null, "content": {"title": "WordRank: Learning Word Embeddings via Robust Ranking", "abstract": "Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage."}}
{"id": "rJ-Kae-_bS", "cdate": 1420070400000, "mdate": null, "content": {"title": "A Scalable Asynchronous Distributed Algorithm for Topic Modeling", "abstract": "Learning meaningful topic models with massive document collections which contain millions of documents and billions of tokens is challenging because of two reasons. First, one needs to deal with a large number of topics (typically on the order of thousands). Second, one needs a scalable and efficient way of distributing the computation across multiple machines. In this paper, we present a novel algorithm F+Nomad LDA which simultaneously tackles both these problems. In order to handle large number of topics we use an appropriately modified Fenwick tree. This data structure allows us to sample from a multinomial distribution over T items in O(log T) time. Moreover, when topic counts change the data structure can be updated in O(log T) time. In order to distribute the computation across multiple processors, we present a novel asynchronous framework inspired by the Nomad algorithm of Yun et al, 2014. We show that F+Nomad LDA significantly outperforms recent state-of-the-art topic modeling approaches on massive problems which involve millions of documents, billions of words, and thousands of topics."}}
{"id": "rkW4ePbubB", "cdate": 1388534400000, "mdate": null, "content": {"title": "Ranking via Robust Binary Classification", "abstract": "We propose RoBiRank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. The algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature. Further, in large scale problems where explicit feature vectors and scores are not given, our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 x 49,824,519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation."}}
