{"id": "2nRUDRP0qbM", "cdate": 1640995200000, "mdate": 1648692960853, "content": {"title": "Point-Based Multilevel Domain Adaptation for Point Cloud Segmentation", "abstract": "Although good performance has been recently achieved in point cloud semantic segmentation based on deep learning, it has not been promoted due to differences in actual scenes and the time-consuming production of labeled data sets. Unsupervised domain adaptation (UDA) aims to solve the problem of how to adapt the classifier from one scene (source domain) to another unlabeled scene (target domain), which can reduce the performance drop caused by the domain shift. Since spatial information is important for light detection and ranging (LiDAR) point and the causes of domain gap in image and point cloud tasks are different, projecting point cloud into image for processing is not suitable and how to apply the image-oriented domain adaptation (DA) methods to point cloud is not trivial. In this letter, we propose a 3D point-based UDA method for point cloud semantic segmentation. This model introduces point- and set-level domain adaptive modules to achieve feature alignment between the domains. We evaluate the proposed method with two experiments, including cross terrain adaptation and airborne to mobile adaptation. Compared with the results without using DA, the mean intersection over union (mIoU) increased by 10.45% and 24.69%, respectively, indicating the effectiveness of our method."}}
{"id": "DnYRG7tTPw", "cdate": 1609459200000, "mdate": 1648692960856, "content": {"title": "An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation", "abstract": "Most semi-supervised learning models are consistency-based, which leverage unlabeled images by maximizing the similarity between different augmentations of an image. But when we apply them to human pose estimation that has extremely imbalanced class distribution, they often collapse and predict every pixel in unlabeled images as background. We find this is because the decision boundary passes the high-density areas of the minor class so more and more pixels are gradually misclassified as background. In this work, we present a surprisingly simple approach to drive the model to learn in the correct direction. For each image, it composes a pair of easy-hard augmentations and uses the more accurate predictions on the easy image to teach the network to learn pose information of the hard one. The accuracy superiority of teaching signals allows the network to be \"monotonically\" improved which effectively avoids collapsing. We apply our method to the state-of-the-art pose estimators and it further improves their performance on three public datasets."}}
{"id": "YVCqceecR3", "cdate": 1577836800000, "mdate": 1632150761756, "content": {"title": "Humble Teacher and Eager Student: Dual Network Learning for Semi-supervised 2D Human Pose Estimation", "abstract": "Semi-supervised learning aims to boost the accuracy of a model by exploring unlabeled images. The state-of-the-art methods are consistency-based which learn about unlabeled images by encouraging the model to give consistent predictions for images under different augmentations. However, when applied to pose estimation, the methods degenerate and predict every pixel in unlabeled images as background. This is because contradictory predictions are gradually pushed to the background class due to highly imbalanced class distribution. But this is not an issue in supervised learning because it has accurate labels. This inspires us to stabilize the training by obtaining reliable pseudo labels. Specifically, we learn two networks to mutually teach each other. In particular, for each image, we compose an easy-hard pair by applying different augmentations and feed them to both networks. The more reliable predictions on easy images in each network are used to teach the other network to learn about the corresponding hard images. The approach successfully avoids degeneration and achieves promising results on public datasets. The source code and pretrained models have been released at https://github.com/xierc/Semi_Human_Pose."}}
{"id": "0tMr9nwg3Oh", "cdate": 1577836800000, "mdate": 1632150761701, "content": {"title": "MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation", "abstract": "Cross view feature fusion is the key to address the occlusion problem in human pose estimation. The current fusion methods need to train a separate model for every pair of cameras making them difficult to scale. In this work, we introduce MetaFuse, a pre-trained fusion model learned from a large number of cameras in the Panoptic dataset. The model can be efficiently adapted or finetuned for a new pair of cameras using a small number of labeled images. The strong adaptation power of MetaFuse is due in large part to the proposed factorization of the original fusion model into two parts--(1) a generic fusion model shared by all cameras, and (2) lightweight camera-dependent transformations. Furthermore, the generic model is learned from many cameras by a meta-learning style algorithm to maximize its adaptation capability to various camera poses. We observe in experiments that MetaFuse finetuned on the public datasets outperforms the state-of-the-arts by a large margin which validates its value in practice."}}
{"id": "ImLEIwsKhJ1", "cdate": 1546300800000, "mdate": 1632150761715, "content": {"title": "Multi-Level Domain Adaptive Learning for Cross-Domain Detection", "abstract": "In recent years, object detection has shown impressive results using supervised deep learning, but it remains challenging in a cross-domain environment. The variations of illumination, style, scale, and appearance in different domains can seriously affect the performance of detection models. Previous works use adversarial training to align global features across the domain shift and to achieve image information transfer. However, such methods do not effectively match the distribution of local features, resulting in limited improvement in cross-domain object detection. To solve this problem, we propose a multi-level domain adaptive model to simultaneously align the distributions of local-level features and global-level features. We evaluate our method with multiple experiments, including adverse weather adaptation, synthetic data adaptation, and cross camera adaptation. In most object categories, the proposed method achieves superior performance against state-of-the-art techniques, which demonstrates the effectiveness and robustness of our method."}}
{"id": "Ckff0EFSQTF", "cdate": 1546300800000, "mdate": 1632150761714, "content": {"title": "Combined Lane Mapping Using a Mobile Mapping System", "abstract": "High-definition mapping of 3D lane lines has been widely needed for the highway documentation and intelligent navigation of autonomous systems. A mobile mapping system (MMS) captures both accurate 3D LiDAR point clouds and high-resolution images of lane markings at highway driving speeds, providing an abundant data source for combined lane mapping. This paper aims to map lanes with an MMS. The main contributions of this paper include the following: (1) an intensity correction method was introduced to eliminate the reflectivity inconsistency of road-surface LiDAR points; (2) a self-adaptive thresholding method was developed to extract lane markings from their complicated surroundings; and (3) a LiDAR-guided textural saliency analysis of MMS images was proposed to improve the robustness of lane mapping. The proposed method was tested with a dataset acquired in Wuhan, Hubei, China, which contained straight roads, curved roads, and a roundabout with various pavement markings and a complex roadside environment. The experimental results achieved a recall of 96.4%, a precision of 97.6%, and an F-score of 97.0%, demonstrating that the proposed method has strong mapping ability for various urban roads."}}
