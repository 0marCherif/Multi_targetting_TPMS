{"id": "CohKbinev2", "cdate": 1632328762785, "mdate": null, "content": {"title": "Causal Inference, is just Inference: A beautifully simple idea that not everyone accepts", "abstract": "It is often argued that causal inference is a step that follows probabilistic estimation in a two step procedure, with a separate statistical estimation and causal inference step and each step is governed by its own principles.  We have argued to the contrary that  Bayesian decision theory is perfectly adequate to do causal inference in a single step using nothing more than Bayesian conditioning.  If true this formulation greatly simplifies causal inference.  We outline this beautifully simple idea and discuss why some object to it."}}
{"id": "BkegKynEKH", "cdate": 1571237751742, "mdate": null, "content": {"title": "Latent Variable Session-Based Recommendation", "abstract": "  We present a probabilistic framework for session based recommendation.  A latent variable for the user state is updated as the user views more items and we learn more about their interests.  We provide computational solutions using both the re-parameterization trick and  using the Bouchard bound for the softmax function, we further explore employing a variational auto-encoder and a variational Expectation-Maximization algorithm for tightening the variational bound.  Finally we show that the Bouchard bound causes the denominator of the softmax to decompose into a sum enabling fast noisy gradients of the bound giving a fully probabilistic algorithm reminiscent of word2vec and a fast online EM algorithm.\n"}}
