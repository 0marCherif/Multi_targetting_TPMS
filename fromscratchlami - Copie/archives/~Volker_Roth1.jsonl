{"id": "253DOGs6EF", "cdate": 1663850332891, "mdate": null, "content": {"title": "Mesh-free Eulerian Physics-Informed Neural Networks", "abstract": "Physics-informed Neural Networks (PINNs) have recently emerged as a principled way to include prior physical knowledge in form of partial differential equations (PDEs) into neural networks. Although PINNs are generally viewed as mesh-free, current approaches still rely on collocation points within a bounded region, even in settings with spatially sparse signals. Furthermore, if the boundaries are not known, the selection of such a region is difficult and often results in a large proportion of collocation points being selected in areas of low relevance. To resolve this severe drawback of current methods, we present a mesh-free and adaptive approach termed particle-density PINN (pdPINN), which is inspired by the microscopic viewpoint of fluid dynamics. The method is based on the Eulerian formulation and, different from classical mesh-free method, does not require the introduction of Lagrangian updates. We propose to sample directly from the distribution over the particle positions, eliminating the need to introduce boundaries while adaptively focusing on the most relevant regions. This is achieved by interpreting a non-negative physical quantity (such as the density or temperature) as an unnormalized probability distribution from which we sample with dynamic Monte Carlo methods. The proposed method leads to higher sample efficiency and improved performance of PINNs. These advantages are demonstrated on various experiments based on the continuity equations, Fokker-Planck equations, and the heat equation."}}
{"id": "ScIEZdIiqe5", "cdate": 1646077545349, "mdate": null, "content": {"title": "Feature Learning and Random Features in Standard Finite-Width Convolutional Neural Networks: An Empirical Study", "abstract": "The Neural Tangent Kernel is an important milestone in the ongoing effort to build a theory for deep learning. Its prediction that sufficiently wide neural networks behave as kernel methods, or equivalently as random feature models arising from linearized networks, has been confirmed empirically for certain wide architectures. In this paper, we compare the performance of two common finite-width convolutional neural networks, LeNet and AlexNet, to their linearizations on common benchmark datasets like MNIST and modified versions of it, CIFAR-10 and an ImageNet subset. We demonstrate empirically that finite-width neural networks, generally, greatly outperform the finite-width linearization of these architectures. When increasing the problem difficulty of the classification task, we observe a larger gap which is in line with common intuition that finite-width neural networks perform feature learning which finite-width linearizations cannot. At the same time, finite-width linearizations improve dramatically with width, approaching the behavior of the wider standard networks which in turn perform slightly better than their standard width counterparts. Therefore, it appears that feature learning for non-wide standard networks is important but becomes less significant with increasing width. We furthermore identify cases where both standard and linearized networks match in performance, in agreement with NTK theory, and a case where a wide linearization outperforms its standard width counterpart."}}
{"id": "iZf6KwjbsN", "cdate": 1640995200000, "mdate": 1682324035639, "content": {"title": "Truly Mesh-free Physics-Informed Neural Networks", "abstract": "Physics-informed Neural Networks (PINNs) have recently emerged as a principled way to include prior physical knowledge in form of partial differential equations (PDEs) into neural networks. Although PINNs are generally viewed as mesh-free, current approaches still rely on collocation points within a bounded region, even in settings with spatially sparse signals. Furthermore, if the boundaries are not known, the selection of such a region is difficult and often results in a large proportion of collocation points being selected in areas of low relevance. To resolve this severe drawback of current methods, we present a mesh-free and adaptive approach termed particle-density PINN (pdPINN), which is inspired by the microscopic viewpoint of fluid dynamics. The method is based on the Eulerian formulation and, different from classical mesh-free method, does not require the introduction of Lagrangian updates. We propose to sample directly from the distribution over the particle positions, eliminating the need to introduce boundaries while adaptively focusing on the most relevant regions. This is achieved by interpreting a non-negative physical quantity (such as the density or temperature) as an unnormalized probability distribution from which we sample with dynamic Monte Carlo methods. The proposed method leads to higher sample efficiency and improved performance of PINNs. These advantages are demonstrated on various experiments based on the continuity equations, Fokker-Planck equations, and the heat equation."}}
{"id": "UGNcFBfolY", "cdate": 1640995200000, "mdate": 1682324035636, "content": {"title": "Learning Invariances with Generalised Input-Convex Neural Networks", "abstract": "Considering smooth mappings from input vectors to continuous targets, our goal is to characterise subspaces of the input domain, which are invariant under such mappings. Thus, we want to characterise manifolds implicitly defined by level sets. Specifically, this characterisation should be of a global parametric form, which is especially useful for different informed data exploration tasks, such as building grid-based approximations, sampling points along the level curves, or finding trajectories on the manifold. However, global parameterisations can only exist if the level sets are connected. For this purpose, we introduce a novel and flexible class of neural networks that generalise input-convex networks. These networks represent functions that are guaranteed to have connected level sets forming smooth manifolds on the input space. We further show that global parameterisations of these level sets can be always found efficiently. Lastly, we demonstrate that our novel technique for characterising invariances is a powerful generative data exploration tool in real-world applications, such as computational chemistry."}}
{"id": "HV37BmIAZBT", "cdate": 1640995200000, "mdate": 1682324035639, "content": {"title": "Feature learning and random features in standard finite-width convolutional neural networks: An empirical study", "abstract": "The Neural Tangent Kernel is an important milestone in the ongoing effort to build a theory for deep learning. Its prediction that sufficiently wide neural networks behave as kernel methods, or equ..."}}
{"id": "PEamM6ANue9", "cdate": 1621604534791, "mdate": null, "content": {"title": "On the Empirical Neural Tangent Kernel of Standard Finite-Width Convolutional Neural Network Architectures", "abstract": "The Neural Tangent Kernel (NTK) is an important milestone in the ongoing effort to build a theory for deep learning. Its prediction that sufficiently wide neural networks behave as kernel methods, or equivalently as random feature models, has been confirmed empirically for certain wide architectures. It remains an open question how well NTK theory models standard neural network architectures of widths common in practice, trained on complex datasets such as ImageNet. We study this question empirically for two well-known convolutional neural network architectures, namely AlexNet and LeNet, and find that their behavior deviates significantly from their finite-width NTK counterparts. For wider versions of these networks, where the number of channels and widths of fully-connected layers are increased, the deviation decreases. "}}
{"id": "g0uZPA27Ke", "cdate": 1609459200000, "mdate": 1682324035651, "content": {"title": "Learning Extremal Representations with Deep Archetypal Analysis", "abstract": ""}}
{"id": "WpDSrSkjC-", "cdate": 1609459200000, "mdate": 1682324035657, "content": {"title": "Learning Conditional Invariance Through Cycle Consistency", "abstract": "Identifying meaningful and independent factors of variation in a dataset is a challenging learning task frequently addressed by means of deep latent variable models. This task can be viewed as learning symmetry transformations preserving the value of a chosen property along latent dimensions. However, existing approaches exhibit severe drawbacks in enforcing the invariance property in the latent space. We address these shortcomings with a novel approach to cycle consistency. Our method involves two separate latent subspaces for the target property and the remaining input information, respectively. In order to enforce invariance as well as sparsity in the latent space, we incorporate semantic knowledge by using cycle consistency constraints relying on property side information. The proposed method is based on the deep information bottleneck and, in contrast to other approaches, allows using continuous target properties and provides inherent model selection capabilities. We demonstrate on synthetic and molecular data that our approach identifies more meaningful factors which lead to sparser and more interpretable models with improved invariance properties."}}
{"id": "EpLgmn_Dc8m", "cdate": 1609459200000, "mdate": 1682324035650, "content": {"title": "Optimizing for Interpretability in Deep Neural Networks with Tree Regularization", "abstract": "Deep models have advanced prediction in many domains, but their lack of interpretability&nbsp; remains a key barrier to the adoption in many real world applications. There exists a large&nbsp; body of work aiming to help humans understand these black box functions to varying levels&nbsp; of granularity \u2013 for example, through distillation, gradients, or adversarial examples. These&nbsp; methods however, all tackle interpretability as a separate process after training. In this&nbsp; work, we take a different approach and explicitly regularize deep models so that they are&nbsp; well-approximated by processes that humans can step through in little time. Specifically,&nbsp; we train several families of deep neural networks to resemble compact, axis-aligned decision&nbsp; trees without significant compromises in accuracy. The resulting axis-aligned decision&nbsp; functions uniquely make tree regularized models easy for humans to interpret. Moreover,&nbsp; for situations in which a single, global tree is a poor estimator, we introduce a regional tree regularizer that encourages the deep model to resemble a compact, axis-aligned decision&nbsp; tree in predefined, human-interpretable contexts. Using intuitive toy examples, benchmark&nbsp; image datasets, and medical tasks for patients in critical care and with HIV, we demonstrate&nbsp; that this new family of tree regularizers yield models that are easier for humans to simulate&nbsp; than L1 or L2 penalties without sacrificing predictive power.&nbsp;"}}
{"id": "DEz6g_SG0hx", "cdate": 1609459200000, "mdate": 1682324035658, "content": {"title": "Learning Conditional Invariance through Cycle Consistency", "abstract": "Identifying meaningful and independent factors of variation in a dataset is a challenging learning task frequently addressed by means of deep latent variable models. This task can be viewed as learning symmetry transformations preserving the value of a chosen property along latent dimensions. However, existing approaches exhibit severe drawbacks in enforcing the invariance property in the latent space. We address these shortcomings with a novel approach to cycle consistency. Our method involves two separate latent subspaces for the target property and the remaining input information, respectively. In order to enforce invariance as well as sparsity in the latent space, we incorporate semantic knowledge by using cycle consistency constraints relying on property side information. The proposed method is based on the deep information bottleneck and, in contrast to other approaches, allows using continuous target properties and provides inherent model selection capabilities. We demonstrate on synthetic and molecular data that our approach identifies more meaningful factors which lead to sparser and more interpretable models with improved invariance properties."}}
