{"id": "IWJsdu_dfBx", "cdate": 1672531200000, "mdate": 1695952611542, "content": {"title": "Cell Instance Segmentation VIA Multi-Scale Non-Local Correlation", "abstract": "For cell instance segmentation on Electron Microscopy (EM) images, state-of-the-art methods either conduct pixel-wise classification or follow a detection and segmentation manner. However, both approaches suffer from the enormous cell instances of EM images where cells are tightly close to each other and show inconsistent morphological properties and/or homogeneous appearances. This fact can easily lead to over-segmentation and under-segmentation problems for model prediction, i.e., falsely splitting and merging adjacent instances. In this paper, we propose a novel approach incorporating non-local correlation in the embedding space to make pixel features distinct or similar to their neighbors and thus address the over- and under-segmentation problems. We perform experiments on five different EM datasets where our proposed method yields better results than several strong baselines. More importantly, by using non-local correlation, we observe fewer false separations within one cell and fewer false fusions between cells."}}
{"id": "GU5DC5hMcOu", "cdate": 1672531200000, "mdate": 1695952611580, "content": {"title": "Towards Saner Deep Image Registration", "abstract": "With recent advances in computing hardware and surges of deep-learning architectures, learning-based deep image registration methods have surpassed their traditional counterparts, in terms of metric performance and inference time. However, these methods focus on improving performance measurements such as Dice, resulting in less attention given to model behaviors that are equally desirable for registrations, especially for medical imaging. This paper investigates these behaviors for popular learning-based deep registrations under a sanity-checking microscope. We find that most existing registrations suffer from low inverse consistency and nondiscrimination of identical pairs due to overly optimized image similarities. To rectify these behaviors, we propose a novel regularization-based sanity-enforcer method that imposes two sanity checks on the deep model to reduce its inverse consistency errors and increase its discriminative power simultaneously. Moreover, we derive a set of theoretical guarantees for our sanity-checked image registration method, with experimental results supporting our theoretical findings and their effectiveness in increasing the sanity of models without sacrificing any performance. Our code and models are available at https://github.com/tuffr5/Saner-deep-registration."}}
{"id": "8zTvuUDGWsF", "cdate": 1672531200000, "mdate": 1695952611537, "content": {"title": "Optical Flow Estimation in 360\u00b0 Videos: Dataset, Model and Application", "abstract": "Optical flow estimation has been a long-lasting and fundamental problem in the computer vision community. However, despite the advances of optical flow estimation in perspective videos, the 360$^\\circ$ videos counterpart remains in its infancy, primarily due to the shortage of benchmark datasets and the failure to accommodate the omnidirectional nature of 360$^\\circ$ videos. We propose the first perceptually realistic 360$^\\circ$ filed-of-view video benchmark dataset, namely FLOW360, with 40 different videos and 4,000 video frames. We then conduct comprehensive characteristic analysis and extensive comparisons with existing datasets, manifesting FLOW360's perceptual realism, uniqueness, and diversity. Moreover, we present a novel Siamese representation Learning framework for Omnidirectional Flow (SLOF) estimation, which is trained in a contrastive manner via a hybrid loss that combines siamese contrastive and optical flow losses. By training the model on random rotations of the input omnidirectional frames, our proposed contrastive scheme accommodates the omnidirectional nature of optical flow estimation in 360$^\\circ$ videos, resulting in significantly reduced prediction errors. The learning scheme is further proven to be efficient by expanding our siamese learning scheme and omnidirectional optical flow estimation to the egocentric activity recognition task, where the classification accuracy is boosted up to $\\sim$26%. To summarize, we study the optical flow estimation in 360$^\\circ$ videos problem from perspectives of the benchmark dataset, learning model, and also practical application. The FLOW360 dataset and code are available at https://siamlof.github.io."}}
{"id": "xdKN9PFx-ck", "cdate": 1640995200000, "mdate": 1668717333221, "content": {"title": "Lipschitz Continuity Retained Binary Neural Network", "abstract": "Relying on the premise that the performance of a binary neural network can be largely restored with eliminated quantization error between full-precision weight vectors and their corresponding binary vectors, existing works of network binarization frequently adopt the idea of model robustness to reach the aforementioned objective. However, robustness remains to be an ill-defined concept without solid theoretical support. In this work, we introduce the Lipschitz continuity, a well-defined functional property, as the rigorous criteria to define the model robustness for BNN. We then propose to retain the Lipschitz continuity as a regularization term to improve the model robustness. Particularly, while the popular Lipschitz-involved regularization methods often collapse in BNN due to its extreme sparsity, we design the Retention Matrices to approximate spectral norms of the targeted weight matrices, which can be deployed as the approximation for the Lipschitz constant of BNNs without the exact Lipschitz constant computation (NP-hard). Our experiments prove that our BNN-specific regularization method can effectively enhance the robustness of BNN (testified on ImageNet-C), achieving SoTA on CIFAR10 and ImageNet. Our code is available at https://github.com/42Shawn/LCR_BNN ."}}
{"id": "l28b4kJtAml", "cdate": 1640995200000, "mdate": 1695952611580, "content": {"title": "Learning Omnidirectional Flow in 360$^\\circ $ Video via Siamese Representation", "abstract": "Optical flow estimation in omnidirectional videos faces two significant issues: the lack of benchmark datasets and the challenge of adapting perspective video-based methods to accommodate the omnidirectional nature. This paper proposes the first perceptually natural-synthetic omnidirectional benchmark dataset with a 360 $$^\\circ $$ field of view, FLOW360, with 40 different videos and 4,000 video frames. We conduct comprehensive characteristic analysis and comparisons between our dataset and existing optical flow datasets, which manifest perceptual realism, uniqueness, and diversity. To accommodate the omnidirectional nature, we present a novel Siamese representation Learning framework for Omnidirectional Flow (SLOF). We train our network in a contrastive manner with a hybrid loss function that combines contrastive loss and optical flow loss. Extensive experiments verify the proposed framework\u2019s effectiveness and show up to 40% performance improvement over the state-of-the-art approaches. Our FLOW360 dataset and code are available at https://siamlof.github.io/ ."}}
{"id": "VeLnc_53ut", "cdate": 1640995200000, "mdate": 1668717333355, "content": {"title": "Win The Lottery Ticket Via Fourier Analysis: Frequencies Guided Network Pruning", "abstract": "With the remarkable success of deep learning recently, efficient network compression algorithms are urgently demanded for releasing the potential computational power of edge devices, such as smartphones or tablets. However, optimal network pruning is a non-trivial task which mathematically is an NP-hard problem. Previous researchers explain training a pruned network as buying a lottery ticket. In this paper, we investigate the Magnitude-Based Pruning (MBP) scheme and analyze it from a novel perspective through Fourier analysis on the deep learning model to guide model designation. Besides explaining the generalization ability of MBP using Fourier transform, we also propose a novel two-stage pruning approach, where one stage is to obtain the topological structure of the pruned network and the other stage is to retrain the pruned network to recover the capacity using knowledge distillation from lower to higher on the frequency domain. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate the superiority of our novel Fourier analysis based MBP compared to other traditional MBP algorithms."}}
{"id": "7LfJaSiNZp", "cdate": 1609459200000, "mdate": 1668717333192, "content": {"title": "Lipschitz Continuity Guided Knowledge Distillation", "abstract": "Knowledge distillation has become one of the most important model compression techniques by distilling knowledge from larger teacher networks to smaller student ones. Although great success has been achieved by prior distillation methods via delicately designing various types of knowledge, they overlook the functional properties of neural networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To alleviate such problem, in this paper, we initially leverage Lipschitz continuity to better represent the functional characteristic of neural networks and guide the knowledge distillation process. In particular, we propose a novel Lipschitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networks\u2019 Lipschitz constants, which enables teacher networks to better regularize student networks and improve the corresponding performance. We derive an explainable approximation algorithm with an explicit theoretical derivation to address the NP-hard problem of calculating the Lipschitz constant. Experimental results have shown that our method outperforms other benchmarks over several knowledge distillation tasks (e.g., classification, segmentation and object detection) on CIFAR-100, ImageNet, and PASCAL VOC datasets. Our code is available at https://github.com/42Shawn/LONDON/tree/master."}}
{"id": "5C4k5nrqGk", "cdate": 1609459200000, "mdate": 1668671474274, "content": {"title": "Audio-Visual Event Localization via Recursive Fusion by Joint Co-Attention", "abstract": "The major challenge in audio-visual event localization task lies in how to fuse information from multiple modalities effectively. Recent works have shown that the attention mechanism is beneficial to the fusion process. In this paper, we propose a novel joint attention mechanism with multi-modal fusion methods for audio-visual event localization. Particularly, we present a concise yet valid architecture that effectively learns representations from multiple modalities in a joint manner. Initially, visual features are combined with auditory features and then turned into joint representations. Next, we make use of the joint representations to attend to visual features and auditory features, respectively. With the help of this joint co-attention, new visual and auditory features are produced, and thus both features can enjoy the mutually improved benefits from each other. It is worth noting that the joint co-attention unit is recursive meaning that it can be performed multiple times for obtaining better joint representations progressively. Extensive experiments on the public AVE dataset have shown that the proposed method achieves significantly better results than the state-of-the-art methods."}}
{"id": "1teMoMW_4y_", "cdate": 1609459200000, "mdate": 1695952611628, "content": {"title": "Unsupervised Neural Tracing In Densely Labeled Multispectral Brainbow Images", "abstract": "Recent advances in imaging technologies for generating large quantities of high-resolution 3D images, especially multispectral labeling technology such as Brainbow, permits unambiguous differentiation of neighboring neurons in a densely labeled brain. This enables, for the first time, the possibility of studying the connectivity between many neurons from a light microscopy image. The lack of reliable automated neuron morphology reconstruction, however, makes data analysis the bottleneck of extracting rich informatics in neuroscience. Supervoxel-based neuron segmentation methods have been proposed to solve this problem, however, previous approaches have been impeded by the large numbers of errors which arise in the final segmentation. In this paper, we present a novel unsupervised approach to trace neurons from multispectral Brainbow images, which prevents segmentation errors and tracing continuity errors using two innovations: First, we formulate a Gaussian mixture model-based clustering strategy to improve the separation of segmented color channels that provides accurate skeletons for the next steps. Then, a skeleton graph approach is proposed to allow the identification and correction of discontinuities in the neuron tree topology. We find that these innovations allow better performance over current state-of-the-art approaches, which results in more accurate neuron tracing results close to human expert annotation."}}
{"id": "yDrvo-vIJ4q", "cdate": 1577836800000, "mdate": 1668671474502, "content": {"title": "Cascade Attention Guided Residue Learning GAN for Cross-Modal Translation", "abstract": "Since we were babies, we intuitively develop the ability to correlate the input from different cognitive sensors such as vision, audio, and text. However, in machine learning, this cross-modal learning is a nontrivial task because different modalities have no homogeneous properties. Previous works discover that there should be bridges among different modalities. From a neurology and psychology perspective, humans have the capacity to link one modality with another one, e.g., associating a picture of a bird with the only hearing of its singing and vice versa. Is it possible for machine learning algorithms to recover the scene given the audio signal? In this paper, we propose a novel Cascade Attention-Guided Residue GAN (CAR-GAN), aiming at reconstructing the scenes given the corresponding audio signals. Particularly, we present a residue module to mitigate the gap between different modalities progressively. Moreover, a cascade attention guided network with a novel classification loss function is designed to tackle the cross-modal learning task. Our model keeps consistency in the high-level semantic label domain and is able to balance two different modalities. The experimental results demonstrate that our model achieves the state-of-the-art cross-modal audio-visual generation on the challenging Sub-URMP dataset."}}
