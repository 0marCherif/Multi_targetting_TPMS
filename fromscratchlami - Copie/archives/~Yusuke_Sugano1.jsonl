{"id": "xkVIRDAcaK", "cdate": 1672531200000, "mdate": 1699231707305, "content": {"title": "Image-to-Text Translation for Interactive Image Recognition: A Comparative User Study with Non-Expert Users", "abstract": "Interactive machine learning (IML) allows users to build their custom machine learning models without expert knowledge. While most existing IML systems are designed with classification algorithms, they sometimes oversimplify the capabilities of machine learning algorithms and restrict the user's task definition. On the other hand, as recent large-scale language models have shown, natural language representation has the potential to enable more flexible and generic task descriptions. Models that take images as input and output text have the potential to represent a variety of tasks by providing appropriate text labels for training. However, the effect of introducing text labels to IML system design has never been investigated. In this work, we aim to investigate the difference between image-to-text translation and image classification for IML systems. Using our prototype systems, we conducted a comparative user study with non-expert users, where participants solved various tasks. Our results demonstrate the underlying difficulty for users in properly defining image recognition tasks while highlighting the potential and challenges of interactive image-to-text translation systems."}}
{"id": "d-4MVcJ2wci", "cdate": 1672531200000, "mdate": 1699231707144, "content": {"title": "Rotation-Constrained Cross-View Feature Fusion for Multi-View Appearance-based Gaze Estimation", "abstract": "Appearance-based gaze estimation has been actively studied in recent years. However, its generalization performance for unseen head poses is still a significant limitation for existing methods. This work proposes a generalizable multi-view gaze estimation task and a cross-view feature fusion method to address this issue. In addition to paired images, our method takes the relative rotation matrix between two cameras as additional input. The proposed network learns to extract rotatable feature representation by using relative rotation as a constraint and adaptively fuses the rotatable features via stacked fusion modules. This simple yet efficient approach significantly improves generalization performance under unseen head poses without significantly increasing computational cost. The model can be trained with random combinations of cameras without fixing the positioning and can generalize to unseen camera pairs during inference. Through experiments using multiple datasets, we demonstrate the advantage of the proposed method over baseline methods, including state-of-the-art domain generalization approaches."}}
{"id": "TJVY3x8ymfwR", "cdate": 1672531200000, "mdate": 1699156514048, "content": {"title": "Utilizing Human Social Norms for Multimodal Trajectory Forecasting via Group-Based Forecasting Module", "abstract": "Trajectory forecasting to generate plausible pedestrian trajectories in crowded scenes requires an understanding of human-human social interactions. Groups of pedestrians with the social norm move along similar trajectories, while groups of pedestrians with different norms make changes to their trajectories to avoid a collision. This paper introduces a group-based forecasting module for modeling inter- and intra-group interactions to enable an understanding of the social norm of humans for trajectory forecasting. In addition, group-based forecasting module takes the trajectory predicted by another prospection module as input to consider potential interactions with other groups in the future. In this way, our method models the complex group-level social interactions in crowded scenes through the attention mechanism and predicts socially plausible trajectories in accordance with each social norm. Comparisons we conducted with state-of-the-art forecasting methods show the effectiveness of our approach on three publicly available crowd datasets (ETH, UCY, and SDD). From experimental results, our network enables to predict plausible social trajectories by introducing two forecasting modules."}}
{"id": "IXySFN0lSKX", "cdate": 1672531200000, "mdate": 1699231707027, "content": {"title": "Technical Understanding from IML Hands-on Experience: A Study through a Public Event for Science Museum Visitors", "abstract": "While AI technology is becoming increasingly prevalent in our daily lives, the comprehension of machine learning (ML) among non-experts remains limited. Interactive machine learning (IML) has the potential to serve as a tool for end users, but many existing IML systems are designed for users with a certain level of expertise. Consequently, it remains unclear whether IML experiences can enhance the comprehension of ordinary users. In this study, we conducted a public event using an IML system to assess whether participants could gain technical comprehension through hands-on IML experiences. We implemented an interactive sound classification system featuring visualization of internal feature representation and invited visitors at a science museum to freely interact with it. By analyzing user behavior and questionnaire responses, we discuss the potential and limitations of IML systems as a tool for promoting technical comprehension among non-experts."}}
{"id": "1mLsXw8xKE", "cdate": 1672531200000, "mdate": 1699231707224, "content": {"title": "Domain-Adaptive Full-Face Gaze Estimation via Novel-View-Synthesis and Feature Disentanglement", "abstract": "Along with the recent development of deep neural networks, appearance-based gaze estimation has succeeded considerably when training and testing within the same domain. Compared to the within-domain task, the variance of different domains makes the cross-domain performance drop severely, preventing gaze estimation deployment in real-world applications. Among all the factors, ranges of head pose and gaze are believed to play a significant role in the final performance of gaze estimation, while collecting large ranges of data is expensive. This work proposes an effective model training pipeline consisting of a training data synthesis and a gaze estimation model for unsupervised domain adaptation. The proposed data synthesis leverages the single-image 3D reconstruction to expand the range of the head poses from the source domain without requiring a 3D facial shape dataset. To bridge the inevitable gap between synthetic and real images, we further propose an unsupervised domain adaptation method suitable for synthetic full-face data. We propose a disentangling autoencoder network to separate gaze-related features and introduce background augmentation consistency loss to utilize the characteristics of the synthetic source domain. Through comprehensive experiments, we show that the model only using monocular-reconstructed synthetic training data can perform comparably to real data with a large label range. Our proposed domain adaptation approach further improves the performance on multiple target domains. The code and data will be available at \\url{https://github.com/ut-vision/AdaptiveGaze}."}}
{"id": "JQuyGQWRocN", "cdate": 1668087838385, "mdate": 1668087838385, "content": {"title": "Improving action segmentation via graph-based temporal reasoning", "abstract": "emporal relations among multiple action segments play an important role in action segmentation especially when observations are limited (eg, actions are occluded by other objects or happen outside a field of view). In this paper, we propose a network module called Graph-based Temporal Reasoning Module (GTRM) that can be built on top of existing action segmentation models to learn the relation of multiple action segments in various time spans. We model the relations by using two Graph Convolution Networks (GCNs) where each node represents an action segment. The two graphs have different edge properties to account for boundary regression and classification tasks, respectively. By applying graph convolution, we can update each node's representation based on its relation with neighboring nodes. The updated representation is then used for improved action segmentation. We evaluate our model on the challenging egocentric datasets namely EGTEA and EPIC-Kitchens, where actions may be partially observed due to the viewpoint restriction. The results show that our proposed GTRM outperforms state-of-the-art action segmentation models by a large margin. We also demonstrate the effectiveness of our model on two third-person video datasets, the 50Salads dataset and the Breakfast dataset."}}
{"id": "zK-1M-Qww4", "cdate": 1640995200000, "mdate": 1667347369073, "content": {"title": "Ego4D: Around the World in 3, 000 Hours of Egocentric Video", "abstract": "We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of dailylife activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/"}}
{"id": "vkClRjX3Ll", "cdate": 1640995200000, "mdate": 1667347369058, "content": {"title": "Interactive Machine Learning on Edge Devices With User-in-the-Loop Sample Recommendation", "abstract": "Interactive machine learning (IML) aims to make machine learning an easy-to-use tool for novice users to solve personalized tasks. However, despite the recent popularity of edge AI, research into interactive machine learning on edge devices has not been conducted actively. Existing IML designs cannot be directly applied to small edge devices due to interface and computational resource limitations. In this paper, we propose a method for efficient model personalization on a small interactive object recognition camera device by combining sample recommendations with an IML workflow. The proposed method recommends training data candidates from unlabeled samples in addition to the usual annotation operations. Our method interactively trains a noise filter to handle a noisy sample pool obtained while using the device. The user can indicate whether the recommended sample corresponds to 1) the recommended class; 2) other classes; or 3) noise unrelated to the recognition task by providing ternary feedback. Our system is designed to gradually update both the target classifier and the noise filtering recommendation modules on the basis of feedback. We show that our feedback design achieves more efficient model training while improving system usability through a systematic evaluation and user study using a prototype device."}}
{"id": "oWqajURzFI", "cdate": 1640995200000, "mdate": 1667347368931, "content": {"title": "Self-Supervised Learning for Audio-Visual Relationships of Videos With Stereo Sounds", "abstract": "Learning cross-modal features is an essential task for many multimedia applications such as sound localization, audio-visual alignment, and image/audio retrieval. Most existing methods mainly focus on the semantic correspondence between videos and monaural sounds, and spatial information of sound sources has not been considered. However, sound locations are critical for understanding the sound environment. To this end, it is necessary to acquire cross-modal features that reflect the semantic and spatial relationship between videos and sounds. A video with stereo sound, which has become commonly used, provides the direction of arrival of each sound source in addition to the category information. This indicates its potential to acquire a desired cross-modal feature space. In this paper, we propose a novel self-supervised approach to learn a cross-modal feature representation that captures both the category and location of each sound source using stereo sound as input. For a set of unlabeled videos, the proposed method generates three kinds of audio-visual pairs: 1) perfectly matched pairs from the same video, 2) pairs from the same video but with the flipped stereo sound, and 3) pairs from a different video. The cross-modal feature encoder of the proposed method is trained on triplet loss to reflect the relationship between these three pairs ( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$1&gt;2&gt;3$ </tex-math></inline-formula> ). We apply this method to cross-modal image/audio retrieval. Compared with previous audio-visual pretext tasks, the proposed method shows significant improvement in both real and synthetic datasets."}}
{"id": "luguTfC8aBt", "cdate": 1640995200000, "mdate": 1667347368992, "content": {"title": "Learning Video-independent Eye Contact Segmentation from In-the-Wild Videos", "abstract": "Human eye contact is a form of non-verbal communication and can have a great influence on social behavior. Since the location and size of the eye contact targets vary across different videos, learning a generic video-independent eye contact detector is still a challenging task. In this work, we address the task of one-way eye contact detection for videos in the wild. Our goal is to build a unified model that can identify when a person is looking at his gaze targets in an arbitrary input video. Considering that this requires time-series relative eye movement information, we propose to formulate the task as a temporal segmentation. Due to the scarcity of labeled training data, we further propose a gaze target discovery method to generate pseudo-labels for unlabeled videos, which allows us to train a generic eye contact segmentation model in an unsupervised way using in-the-wild videos. To evaluate our proposed approach, we manually annotated a test dataset consisting of 52 videos of human conversations. Experimental results show that our eye contact segmentation model outperforms the previous video-dependent eye contact detector and can achieve 71.88% framewise accuracy on our annotated test set. Our code and evaluation dataset are available at https://github.com/ut-vision/Video-Independent-ECS."}}
