{"id": "DcfsR89KUa", "cdate": 1654533738571, "mdate": null, "content": {"title": "Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world", "abstract": "We introduce \\textit{Nocturne}, a new 2D driving simulator for investigating multi-agent coordination under partial observability. The focus of Nocturne is to enable research into inference and theory of mind in real-world multi-agent settings without the computational overhead of computer vision and feature extraction from images. Agents in this simulator only observe an obstructed view of the scene, mimicking human visual sensing constraints. Unlike existing benchmarks that are bottlenecked by rendering human-like observations directly using a camera input, Nocturne uses efficient intersection methods to compute a vectorized set of visible features in a C++ back-end, allowing the simulator to run at $2000+$ steps-per-second. Using open-source trajectory and map data, we construct a simulator to load and replay arbitrary trajectories and scenes from real-world driving data. Using this environment, we benchmark reinforcement-learning and imitation-learning agents and demonstrate that the agents are quite far from human-level coordination ability and deviate significantly from the expert trajectories."}}
{"id": "YVXaxB6L2Pl", "cdate": 1654322959583, "mdate": null, "content": {"title": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games", "abstract": "Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, the Hanabi challenge, and Google Research Football, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods are a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at https://github.com/marlbenchmark/on-policy."}}
{"id": "qa7L43sDNj", "cdate": 1640995200000, "mdate": 1681491442289, "content": {"title": "On the approximability of Time Disjoint Walks", "abstract": ""}}
{"id": "q8CuiBx6QtE", "cdate": 1640995200000, "mdate": 1681491442303, "content": {"title": "Deploying Traffic Smoothing Cruise Controllers Learned from Trajectory Data", "abstract": ""}}
{"id": "5rkJ-qcvDls", "cdate": 1640995200000, "mdate": 1681491442306, "content": {"title": "Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world", "abstract": ""}}
{"id": "5PsOma0Rs-a", "cdate": 1640995200000, "mdate": 1681491442292, "content": {"title": "Flow: A Modular Learning Framework for Mixed Autonomy Traffic", "abstract": ""}}
{"id": "33xiiWeSIff", "cdate": 1640995200000, "mdate": 1681491442310, "content": {"title": "Unified Automatic Control of Vehicular Systems with Reinforcement Learning", "abstract": ""}}
{"id": "Xe5MFhFvYGX", "cdate": 1634067443282, "mdate": null, "content": {"title": "Imitation Learning from Pixel Observations for Continuous Control", "abstract": "We study imitation learning from visual observations only for controlling dynamical systems with continuous states and actions. This setting is attractive due to the large amount of video data available from which agents could learn from. However, it is challenging due to $i)$ not observing the actions and $ii)$ the high-dimensional visual space. In this setting, we explore recipes for imitation learning based on adversarial learning and optimal transport. These recipes enable us to scale these methods to attain expert-level performance on visual continuous control tasks in the DeepMind control suite. We investigate the tradeoffs of these approaches and present a comprehensive evaluation of the key design choices. To encourage reproducible research in this area, we provide an easy-to-use implementation for benchmarking visual imitation learning, including our methods and expert demonstrations."}}
{"id": "JLbXkHkLCG6", "cdate": 1632875421900, "mdate": null, "content": {"title": "Imitation Learning from Pixel Observations for Continuous Control", "abstract": "We study imitation learning using only visual observations for controlling dynamical systems with continuous states and actions. This setting is attractive due to the large amount of video data available from which agents could learn from. However, it is challenging due to $i)$ not observing the actions and $ii)$ the high-dimensional visual space. In this setting, we explore recipes for imitation learning based on adversarial learning and optimal transport. A key feature of our methods is to use representations from the RL encoder to compute imitation rewards. These recipes enable us to scale these methods to attain expert-level performance on visual continuous control tasks in the DeepMind control suite. We investigate the tradeoffs of these approaches and present a comprehensive evaluation of the key design choices. To encourage reproducible research in this area, we provide an easy-to-use implementation for benchmarking visual imitation learning, including our methods."}}
{"id": "kgNG3Wgb-FB", "cdate": 1620503383244, "mdate": null, "content": {"title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design", "abstract": "A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments."}}
