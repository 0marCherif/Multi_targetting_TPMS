{"id": "172fQhPZ2y", "cdate": 1696004038753, "mdate": 1696004038753, "content": {"title": "Bures-Wasserstein Barycenters and Low-Rank Matrix Recovery", "abstract": "We revisit the problem of recovering a low-rank positive semidefinite matrix from rank-one projections using tools from optimal transport. More specifically, we show that a variational formulation of this problem is equivalent to computing a Wasserstein barycenter. In turn, this new perspective enables the development of new geometric first-order methods with strong convergence guarantees in Bures-Wasserstein distance. Experiments on simulated data demonstrate the advantages of our new methodology over existing methods."}}
{"id": "hyE0oNLJejj", "cdate": 1680698082362, "mdate": null, "content": {"title": "First-Order Algorithms for Optimization over Graph Laplacians", "abstract": "When solving an optimization problem over the set of graph Laplacian matrices, one must deal with the large number of constraints as well as the large objective variable size. In this paper we explore first order methods for optimization over graph Laplacian matrices. These methods include two popular methods for constrained optimization: the mirror descent algorithm and the Frank-Wolfe (conditional gradient) algorithm. We derive efficiently implementable formulations of these algorithms over graph Laplacians, and use existing theory to show their iteration complexity in various regimes. Experiments demonstrate the efficiency of these methods over alternatives like interior point methods."}}
{"id": "3LwxJwUTgV", "cdate": 1680307200000, "mdate": 1682499005201, "content": {"title": "Depth Descent Synchronization in ${{\\,\\mathrm{\\text {SO}}\\,}}(D)$", "abstract": "We give robust recovery results for synchronization on the rotation group, $${{\\,\\mathrm{\\text {SO}}\\,}}(D)$$ SO ( D ) . In particular, we consider an adversarial corruption setting, where a limited percentage of the observations are arbitrarily corrupted. We develop a novel algorithm that exploits Tukey depth in the tangent space of $${{\\,\\mathrm{\\text {SO}}\\,}}(D)$$ SO ( D ) . This algorithm, called Depth Descent Synchronization, exactly recovers the underlying rotations up to an outlier percentage of $$1/(D(D-1)+2)$$ 1 / ( D ( D - 1 ) + 2 ) , which corresponds to 1/4 for $${{\\,\\mathrm{\\text {SO}}\\,}}(2)$$ SO ( 2 ) and 1/8 for $${{\\,\\mathrm{\\text {SO}}\\,}}(3)$$ SO ( 3 ) . In the case of $${{\\,\\mathrm{\\text {SO}}\\,}}(2)$$ SO ( 2 ) , we demonstrate that a variant of this algorithm converges linearly to the ground truth rotations. We implement this algorithm for the case of $${{\\,\\mathrm{\\text {SO}}\\,}}(3)$$ SO ( 3 ) and demonstrate that it performs competitively on baseline synthetic data."}}
{"id": "HzT-_xyLOS", "cdate": 1640995200000, "mdate": 1668800223158, "content": {"title": "Scalable Cluster-Consistency Statistics for Robust Multi-Object Matching", "abstract": "We develop new statistics for robustly filtering corrupted keypoint matches in the structure from motion pipeline. The statistics are based on consistency constraints that arise within the clustered structure of the graph of keypoint matches. The statistics are designed to give smaller values to corrupted matches and than uncorrupted matches. These new statistics are combined with an iterative reweighting scheme to filter keypoints, which can then be fed into any standard structure from motion pipeline. This filtering method can be efficiently implemented and scaled to massive datasets as it only requires sparse matrix multiplication. We demonstrate the efficacy of this method on synthetic and real structure from motion datasets and show that it achieves state-of-the-art accuracy and speed in these tasks."}}
{"id": "GCYy2chsTMc", "cdate": 1640995200000, "mdate": 1682499005201, "content": {"title": "Stochastic and Private Nonconvex Outlier-Robust PCA", "abstract": "We develop theoretically guaranteed stochastic methods for outlier-robust PCA. Outlier-robust PCA seeks an underlying low-dimensional linear subspace from a dataset that is corrupted with outliers. We are able to show that our methods, which involve stochastic geodesic gradient descent over the Grassmannian manifold, converge and recover an underlying subspace in various regimes through the development of a novel convergence analysis. The main application of this method is an effective differentially private algorithm for outlier-robust PCA that uses a Gaussian noise mechanism within the stochastic gradient method. Our results emphasize the advantages of the nonconvex methods over another convex approach to solving this problem in the differentially private setting. Experiments on synthetic and stylized data verify these results."}}
{"id": "PPzV1H4atM4", "cdate": 1621629932695, "mdate": null, "content": {"title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport", "abstract": "We consider the fundamental problem of sampling the optimal transport coupling between given source and target distributions. In certain cases, the optimal transport plan takes the form of a one-to-one mapping from the source support to the target support, but learning or even approximating such a map is computationally challenging for large and high-dimensional datasets due to the high cost of linear programming routines and an intrinsic curse of dimensionality. We study instead the Sinkhorn problem, a regularized form of optimal transport whose solutions are couplings between the source and the target distribution. We introduce a novel framework for learning the Sinkhorn coupling between two distributions in the form of a score-based generative model. Conditioned on source data, our procedure iterates Langevin Dynamics to sample target data according to the regularized optimal coupling. Key to this approach is a neural network parametrization of the Sinkhorn problem, and we prove convergence of gradient descent with respect to network parameters in this formulation. We demonstrate its empirical success on a variety of large scale optimal transport tasks. "}}
{"id": "h1iLHko5M6V", "cdate": 1609459200000, "mdate": 1681527583981, "content": {"title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport", "abstract": ""}}
{"id": "VOS52F6aM-L", "cdate": 1609459200000, "mdate": 1668800223252, "content": {"title": "Scalable Cluster-Consistency Statistics for Robust Multi-Object Matching", "abstract": "We develop new statistics for robustly filtering corrupted keypoint matches in the structure from motion pipeline. The statistics are based on consistency constraints that arise within the clustered structure of the graph of keypoint matches. The statistics are designed to give smaller values to corrupted matches and than uncorrupted matches. These new statistics are combined with an iterative reweighting scheme to filter keypoints, which can then be fed into any standard structure from motion pipeline. This filtering method can be efficiently implemented and scaled to massive datasets as it only requires sparse matrix multiplication. We demonstrate the efficacy of this method on synthetic and real structure from motion datasets and show that it achieves state-of-the-art accuracy and speed in these tasks."}}
{"id": "K9huvRnBm5X", "cdate": 1609459200000, "mdate": 1681527583978, "content": {"title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport", "abstract": ""}}
{"id": "w67F0iy9jto", "cdate": 1577836800000, "mdate": null, "content": {"title": "Gradient descent algorithms for Bures-Wasserstein barycenters", "abstract": "We study first order methods to compute the barycenter of a probability distribution $P$ over the space of probability measures with finite second moment. We develop a framework to derive global rates of convergence for both gradient descent and stochastic gradient descent despite the fact that the barycenter functional is not geodesically convex. Our analysis overcomes this technical hurdle by employing a Polyak-\u0141{}ojasiewicz (PL) inequality and relies on tools from optimal transport and metric geometry. In turn, we establish a PL inequality when $P$ is supported on the Bures-Wasserstein manifold of Gaussian probability measures. It leads to the first global rates of convergence for first order methods in this context."}}
