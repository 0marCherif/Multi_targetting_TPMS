{"id": "2LykhBxggh", "cdate": 1684247686611, "mdate": 1684247686611, "content": {"title": "Person identification using deep neural networks on physiological biomarkers during exercise", "abstract": "Much progress has been made in wearable sensors that provide real-time continuous physiological data from noninvasive measurements including heart rate and biofluids such as sweat. This information can potentially be used to identify the health condition of a person by applying machine learning algorithms on the physiological measurements. We present a person identification task that uses machine learning algorithms on a set of biomarkers collected from 30 subjects carrying out a cycling experiment. We compared an SVM and a gated recurrent neural network (RNN) for real-time accuracy using different window sizes of the measured data. Results show that using all biomarkers gave the best results from any of the models. With all biomarkers, the gated RNN model achieved ~ 90% accuracy even in a 30 s time window; and ~92.3% accuracy in a 150 s time window. Excluding any of the biomarkers leads to at least 7.4% absolute accuracy drop for the RNN model. The RNN implementation on the Jetson Nano incurs a low latency of ~ 45 ms per inference."}}
{"id": "z8Oeh_wC-k", "cdate": 1672531200000, "mdate": 1699193124408, "content": {"title": "Live Demo: E2P-Events to Polarization Reconstruction from PDAVIS Events", "abstract": "This demonstration shows live operation of of PDAVIS polarization event camera reconstruction by the E2P DNN reported in the main CVPR conference paper Deep Polarization Reconstruction with PDAVIS Events (paper 9149 [7]). Demo code: github.com/SensorsINI/e2p"}}
{"id": "G-Cz5thNGz", "cdate": 1672531200000, "mdate": 1699193124409, "content": {"title": "3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network", "abstract": "This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\\times$ without losing accuracy when tested on a \\texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \\url{https://github.com/qinche106/cb-convlstm-eyetracking}."}}
{"id": "fV8CfTjnXgH", "cdate": 1664294261755, "mdate": null, "content": {"title": "Fast temporal decoding from large-scale neural recordings in monkey visual cortex", "abstract": "With new developments in electrode and nanoscale technology, a large-scale multi-electrode cortical neural prosthesis with thousands of stimulation and recording electrodes is becoming viable. Such a system will be useful as both a neuroscience tool and a  neuroprosthesis.\nIn the context of a visual neuroprosthesis, a rudimentary form of vision can be presented to the visually impaired by stimulating the electrodes to induce phosphene patterns. Additional feedback in a closed-loop system can be provided by rapid decoding of recorded responses from relevant brain areas. This work looks at temporal decoding results from a dataset of 1024 electrode recordings collected from the V1 and V4 areas of a primate performing a visual discrimination task. By applying deep learning models, the peak decoding accuracy from the V1 data can be obtained by a moving time window of 150 ms across the 800 ms phase of stimulus presentation. The peak accuracy from the V4 data is achieved at a larger latency and by using a larger moving time window of 300 ms. Decoding using a running window of 30 ms on the V1 data showed only a 4\\% drop in peak accuracy. We also determined the robustness of the decoder to electrode failure by choosing a subset of important electrodes using a previously reported algorithm for scaling the importance of inputs to a network. Results show that the accuracy of 91.1\\% from a network trained on the selected subset of 256 electrodes is close to the accuracy of 91.7\\% from using all 1024 electrodes."}}
{"id": "tYa5suErxu", "cdate": 1640995200000, "mdate": 1683879194873, "content": {"title": "Exploiting Spatial Sparsity for Event Cameras with Visual Transformers", "abstract": "Event cameras report local changes of brightness through an asynchronous stream of output events. Events are spatially sparse at pixel locations with little brightness variation. We propose using a visual transformer (ViT) architecture to leverage its ability to process a variable-length input. The input to the ViT consists of events that are accumulated into time bins and spatially separated into non-overlapping sub-regions called patches. Patches are selected when the number of nonzero pixel locations within a sub-region is above a threshold. We show that by fine-tuning a ViT model on these selected active patches, we can reduce the average number of patches fed into the backbone during the inference by at least 50% with only a minor drop (0.34%) of the classification accuracy on the N-Caltech101 dataset. This reduction translates into a decrease of 51% in Multiply-Accumulate (MAC) operations and an increase of 46% in the inference speed using a server CPU."}}
{"id": "1d-JLPEJRbW", "cdate": 1640995200000, "mdate": 1683879194840, "content": {"title": "Person identification using deep neural networks on physiological biomarkers during exercise", "abstract": "Much progress has been made in wearable sensors that provide real-time continuous physiological data from noninvasive measurements including heart rate and biofluids such as sweat. This information can potentially be used to identify the health condition of a person by applying machine learning algorithms on the physiological measurements. We present a person identification task that uses machine learning algorithms on a set of biomarkers collected from 30 subjects carrying out a cycling experiment. We compared an SVM and a gated recurrent neural network (RNN) for real-time accuracy using different window sizes of the measured data. Results show that using all biomarkers gave the best results from any of the models. With all biomarkers, the gated RNN model achieved ~ 90% accuracy even in a 30 s time window; and ~92.3% accuracy in a 150 s time window. Excluding any of the biomarkers leads to at least 7.4% absolute accuracy drop for the RNN model. The RNN implementation on the Jetson Nano incurs a low latency of ~ 45 ms per inference."}}
{"id": "dFu7pLQcDDE", "cdate": 1609459200000, "mdate": 1683879194841, "content": {"title": "LiteEdge: Lightweight Semantic Edge Detection Network", "abstract": "Scene parsing is a critical component for understanding complex scenes in applications such as autonomous driving. Semantic segmentation networks are typically reported for scene parsing but semantic edge networks have also become of interest because of the sparseness of the segmented maps. This work presents an end-to-end trained lightweight deep semantic edge detection architecture called LiteEdge suitable for edge deployment. By utilizing hierarchical supervision and a new weighted multi-label loss function to balance different edge classes during training, LiteEdge predicts with high accuracy category-wise binary edges. Our LiteEdge network with only \u2248 3M parameters, has a semantic edge prediction accuracy of 52.9% mean maximum F (MF) score on the Cityscapes dataset. This accuracy was evaluated on the network trained to produce a low resolution edge map. The network can be quantized to 6-bit weights and 8-bit activations and shows only a 2% drop in the mean MF score. This quantization leads to a memory footprint savings of 6X for an edge device."}}
{"id": "W82KRsochJ-", "cdate": 1577836800000, "mdate": 1683879194850, "content": {"title": "GRUBERT: A GRU-Based Method to Fuse BERT Hidden Layers for Twitter Sentiment Analysis", "abstract": "Leo Horne, Matthias Matti, Pouya Pourjafar, Zuowen Wang. Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop. 2020."}}
{"id": "2EPzNPCUWuS", "cdate": 1577836800000, "mdate": 1683879194841, "content": {"title": "Understanding (Non-)Robust Feature Disentanglement and the Relationship Between Low- and High-Dimensional Adversarial Attacks", "abstract": "Recent work has put forth the hypothesis that adversarial vulnerabilities in neural networks are due to them overusing \"non-robust features\" inherent in the training data. We show empirically that for PGD-attacks, there is a training stage where neural networks start heavily relying on non-robust features to boost natural accuracy. We also propose a mechanism reducing vulnerability to PGD-style attacks consisting of mixing in a certain amount of images contain-ing mostly \"robust features\" into each training batch, and then show that robust accuracy is improved, while natural accuracy is not substantially hurt. We show that training on \"robust features\" provides boosts in robust accuracy across various architectures and for different attacks. Finally, we demonstrate empirically that these \"robust features\" do not induce spatial invariance."}}
{"id": "B1e6oy39aE", "cdate": 1559048100767, "mdate": null, "content": {"title": "Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness", "abstract": "This work provides theoretical and empirical evidence that invariance-inducing regularizers can increase predictive accuracy for worst-case spatial transformations (spatial robustness).  Evaluated on these adversarially transformed examples, we demonstrate that adding regularization on top of standard or adversarial training reduces the relative error by 20% for CIFAR10 without increasing the computational cost.  This outperforms handcrafted networks that were explicitly designed to be spatial-equivariant. Furthermore, we observe for SVHN, known to have inherent variance in orientation, that robust training also improves standard accuracy on the test set. "}}
