{"id": "olhhqrp1sCA", "cdate": 1663849907402, "mdate": null, "content": {"title": "Fast Test-Time Adaptation Using Hints", "abstract": "We propose a framework for adapting neural networks to distribution shifts at test-time. The primary idea is to leverage proper adaptation objectives based on known general properties of the target task, e.g. multi-view geometry for 3D tasks, or hierarchical structure for semantic tasks. These properties can be instantiated as adaptation signals at test-time, which we refer to as \"hints\". These hints are robust to distribution shifts, thus, they make adaptation more reliable compared to existing test-time adaptation methods, e.g. entropy minimization. Next, we show that this optimization during test-time can be amortized using a side-network, thus, making the adaptation orders of magnitude faster. We call this variant of test-time adaption Rapid Network Adaptation (RNA). We demonstrate consistent improvements over the baselines on diverse tasks (depth, optical flow, semantic segmentation, classification), datasets (Taskonomy, Replica, ScanNet, COCO, ImageNet) and distribution shifts (Common Corruptions, 3D Common Corruptions, cross-datasets)."}}
{"id": "Evar7nqAQtL", "cdate": 1654348671779, "mdate": null, "content": {"title": "3D Common Corruptions for Object Recognition", "abstract": "We introduce a set of image transformations that can be used as corruptions to evaluate the robustness of models. The primary distinction of the proposed transformations is that, unlike existing approaches such as Common Corruptions, the geometry of the scene is incorporated in the transformations \u2013 thus leading to corruptions that are more likely to occur in the real world. We apply these corruptions to the ImageNet validation set to create 3D Common Corruptions (ImageNet-3DCC) benchmark. The evaluations on recent ImageNet models with robustness mechanisms show that ImageNet-3DCC is a challenging benchmark for object recognition task. Furthermore, it exposes vulnerabilities that are not captured by Common Corruptions, which can be informative during model development. "}}
{"id": "MPE-L3Kc12", "cdate": 1640995200000, "mdate": 1666113679909, "content": {"title": "3D Common Corruptions and Data Augmentation", "abstract": "We introduce a set of image transformations that can be used as corruptions to evaluate the robustness of models as well as data augmentation mechanisms for training neural networks. The primary distinction of the proposed transformations is that, unlike existing approaches such as Common Corruptions [27], the geometry of the scene is incorporated in the transformations - thus leading to corruptions that are more likely to occur in the real world. We also introduce a set of semantic corruptions (e.g. natural object occlusions. See Fig. 1). We show these transformations are \u2018efficient\u2019 (can be computed on-the-fly), \u2018extendable\u2019 (can be applied on most image datasets), expose vulnerability of existing models, and can effectively make models more robust when employed as \u20183D data augmentation\u2019 mechanisms. The evaluations on several tasks and datasets suggest incorporating 3D information into benchmarking and training opens up a promising direction for robustness research."}}
{"id": "-brOVDOvJ8A", "cdate": 1640995200000, "mdate": 1666113680287, "content": {"title": "3D Common Corruptions and Data Augmentation", "abstract": "We introduce a set of image transformations that can be used as corruptions to evaluate the robustness of models as well as data augmentation mechanisms for training neural networks. The primary distinction of the proposed transformations is that, unlike existing approaches such as Common Corruptions, the geometry of the scene is incorporated in the transformations -- thus leading to corruptions that are more likely to occur in the real world. We also introduce a set of semantic corruptions (e.g. natural object occlusions). We show these transformations are `efficient' (can be computed on-the-fly), `extendable' (can be applied on most image datasets), expose vulnerability of existing models, and can effectively make models more robust when employed as `3D data augmentation' mechanisms. The evaluations on several tasks and datasets suggest incorporating 3D information into benchmarking and training opens up a promising direction for robustness research."}}
{"id": "uGHafIiCuV", "cdate": 1609459200000, "mdate": 1666113679902, "content": {"title": "Robustness via Cross-Domain Ensembles", "abstract": "We present a method for making neural network predictions robust to shifts from the training data distribution. The proposed method is based on making predictions via a diverse set of cues (called 'middle domains') and ensembling them into one strong prediction. The premise of the idea is that predictions made via different cues respond differently to a distribution shift, hence one should be able to merge them into one robust final prediction. We perform the merging in a straightforward but principled manner based on the uncertainty associated with each prediction. The evaluations are performed using multiple tasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of adversarial and non-adversarial distribution shifts which demonstrate the proposed method is considerably more robust than its standard learning counterpart, conventional deep ensembles, and several other baselines."}}
{"id": "kfmVk3YV-Y", "cdate": 1609459200000, "mdate": 1666113681076, "content": {"title": "High-Resolution Multi-Spectral Imaging With Diffractive Lenses and Learned Reconstruction", "abstract": "Spectral imaging is a fundamental diagnostic technique with widespread application. Conventional spectral imaging approaches have intrinsic limitations on spatial and spectral resolutions due to the physical components they rely on. To overcome these physical limitations, in this paper, we develop a novel multi-spectral imaging modality that enables higher spatial and spectral resolutions. In the developed computational imaging modality, we exploit a diffractive lens, such as a photon sieve, for both dispersing and focusing the optical field, and achieve measurement diversity by changing the focusing behavior of this lens. Because the focal length of a diffractive lens is wavelength-dependent, each measurement is a superposition of differently blurred spectral components. To reconstruct the individual spectral images from these superimposed and blurred measurements, model-based fast reconstruction algorithms are developed with deep and analytical priors using alternating minimization and unrolling. Finally, the effectiveness and performance of the developed technique is illustrated for an application in astrophysical imaging under various observation scenarios in the extreme ultraviolet (EUV) regime. The results demonstrate that the technique provides not only diffraction-limited high spatial resolution, as enabled by diffractive lenses, but also the capability of resolving close-by spectral sources that would not otherwise be possible with the existing techniques. This work enables high resolution multi-spectral imaging with low cost designs for a variety of applications and spectral regimes."}}
{"id": "FIv0tT5FU3", "cdate": 1609459200000, "mdate": 1666113680772, "content": {"title": "Robustness via Cross-Domain Ensembles", "abstract": "We present a method for making neural network predictions robust to shifts from the training data distribution. The proposed method is based on making predictions via a diverse set of cues (called \u2018middle domains\u2019) and ensembling them into one strong prediction. The premise of the idea is that predictions made via different cues respond differently to a distribution shift, hence one should be able to merge them into one robust final prediction. We perform the merging in a straightforward but principled manner based on the uncertainty associated with each prediction. The evaluations are performed using multiple tasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of adversarial and non-adversarial distribution shifts which demonstrate the proposed method is considerably more robust than its standard learning counterpart, conventional deep ensembles, and several other baselines."}}
{"id": "tLRxBLoTAM", "cdate": 1601308076011, "mdate": null, "content": {"title": "Robustness via Probabilistic Cross-Task Ensembles", "abstract": "We present a method for making predictions using neural networks that, at the test time, is robust against shifts from the training data distribution. The proposed method is based on making \\emph{one prediction via different cues} (called middle domains) and ensembling their outputs into one strong prediction. The premise of the idea is that predictions via different cues respond differently to distribution shifts, hence one can merge them into one robust final prediction, if ensembling can be done successfully. We perform the ensembling in a straightforward but principled probabilistic manner. The evaluations are performed using multiple vision dataset under a range of natural and synthetic distribution shifts which demonstrate the proposed method is considerably more robust compared to its standard learning counterpart, conventional ensembles, and several other baselines."}}
{"id": "SaXJw-V0QM", "cdate": 1577836800000, "mdate": 1666113680652, "content": {"title": "Robust Learning Through Cross-Task Consistency", "abstract": "Visual perception entails solving a wide set of tasks, e.g., object detection, depth estimation, etc. The predictions made for multiple tasks from the same image are not independent, and therefore, are expected to be consistent. We propose a broadly applicable and fully computational method for augmenting learning with Cross-Task Consistency. The proposed formulation is based on inference-path invariance over a graph of arbitrary tasks. We observe that learning with cross-task consistency leads to more accurate predictions and better generalization to out-of-distribution inputs. This framework also leads to an informative unsupervised quantity, called Consistency Energy, based on measuring the intrinsic consistency of the system. Consistency Energy correlates well with the supervised error (r=0.67), thus it can be employed as an unsupervised confidence metric as well as for detection of out-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on multiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape, and they benchmark cross-task consistency versus various baselines including conventional multi-task learning, cycle consistency, and analytical consistency."}}
{"id": "rxlh2Pfg05t", "cdate": 1546300800000, "mdate": 1666113680651, "content": {"title": "A Transform Learning Based Deconvolution Technique with Super-Resolution and Microscanning Applications", "abstract": "We deal with reconstruction of convolved images with known point spread functions. We adopt a feature enhanced deconvolution method. Instead of using a pre-designed sparsifying transform, we use an online transform learning based method, and reconstruct images along with a sparsifying transform. To avoid circular effects, we implement non-circular convolution operator using FFT based convolution and dead pixels. We use a coordinate descent type algorithm and derive the associated update steps for both circular and non-circular deconvolution. Moreover, we show single image super-resolution extension for non-circular deconvolution. We compare the proposed method to other feature enhanced deconvolution alternatives, as well as conventional methods such as Lucy-Richardson method. Finally, we demonstrate the effectiveness of the algorithm for circular deconvolution, non-circular deconvolution, and single-image super-resolution applications."}}
