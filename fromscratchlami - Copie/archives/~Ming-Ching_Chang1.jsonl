{"id": "17-EGnXDW15", "cdate": 1667954723018, "mdate": 1667954723018, "content": {"title": "FedTrust: Towards Building Secure Robust and Trustworthy Moderators for Federated Learning", "abstract": "Most Federated Learning (FL) systems are built upon a strong assumption of trust-clients fully trust the cen-tralized moderator, which might not be feasible in practice. This work aims to mitigate the assumption by using appropriate cryptographic tools. Particularly, we examine various scenarios with different trust demands in FL, and then design the corresponding practical protocols with lightweight cryptographic tools. Three solutions for secure and trustworthy aggregation are proposed with increasing sophistication: (1) a single verifiable moderator, (2) a single secure and verifiable moderator, and (3) multiple secure and verifiable moderators, which can handle adversarial behaviors with different levels. We evaluate the performances of all our proposed protocols on the test accuracy and the training time, showing that our protocols maintain the accuracy with time overhead from 30% to 156% depending on the secure and trustworthy levels. The protocols can be deployed in many practical FL settings with appropriate optimizations."}}
{"id": "qgQypKNssN", "cdate": 1667954594273, "mdate": 1667954594273, "content": {"title": "A Robust Collaborative Learning Framework Using Data Digests and Synonyms to Represent Absent Clients", "abstract": "We propose Collaborative Learning with Synonyms (CLSyn), a robust and versatile collaborative machine learning framework that can tolerate unexpected client absence during training while maintaining high model accuracy. Client absence during collaborative training can seriously degrade model performances, particularly for unbalanced and non-IID client data. We address this issue by introducing the notion of data digests of the training samples from the clients. The expansion of digests called synonyms can represent the original samples on the server and thus maintain overall model accuracy, even after the clients become unavailable. We compare our CLSyn implementations against three centralized Federated Learning algorithms, namely FedAvg, FedProx, and FedNova as baselines. Results on CIFAR-10, CIFAR-100, and EMNIST show that CLSyn consistently outperforms these baselines by significant margins in various client absence scenarios."}}
{"id": "BOO4w83Vz79", "cdate": 1640995200000, "mdate": 1648671823337, "content": {"title": "PseudoProp: Robust Pseudo-Label Generation for Semi-Supervised Object Detection in Autonomous Driving Systems", "abstract": "Semi-supervised object detection methods are widely used in autonomous driving systems, where only a fraction of objects are labeled. To propagate information from the labeled objects to the unlabeled ones, pseudo-labels for unlabeled objects must be generated. Although pseudo-labels have proven to improve the performance of semi-supervised object detection significantly, the applications of image-based methods to video frames result in numerous miss or false detections using such generated pseudo-labels. In this paper, we propose a new approach, PseudoProp, to generate robust pseudo-labels by leveraging motion continuity in video frames. Specifically, PseudoProp uses a novel bidirectional pseudo-label propagation approach to compensate for misdetection. A feature-based fusion technique is also used to suppress inference noise. Extensive experiments on the large-scale Cityscapes dataset demonstrate that our method outperforms the state-of-the-art semi-supervised object detection methods by 7.4% on mAP75."}}
{"id": "B8qZw82Vzmc", "cdate": 1640995200000, "mdate": 1648671823143, "content": {"title": "Simultaneous multi-person tracking and activity recognition based on cohesive cluster search", "abstract": "Highlights \u2022 Simultaneous multi-person tracking and activity recognition using a bootstrapping framework. \u2022 High-order correlation formulations among social dynamics and activities using a hypergraph representation. \u2022 Cohesive cluster search applied to solve the hypergraph optimization. Abstract We present a bootstrapping framework to simultaneously improve multi-person tracking and activity recognition at individual, interaction and social group activity levels. The inference consists of identifying trajectories of all pedestrian actors, individual activities, pairwise interactions, and collective activities, given the observed pedestrian detections. Our method uses a graphical model to represent and solve the joint tracking and recognition problems via three stages: (i) activity-aware tracking, (ii) joint interaction recognition and occlusion recovery, and (iii) collective activity recognition. This full-stack problem induces great complexity in learning the representations for the sub-problems at each stage, and the complexity increases as with more stages in the system. Our solution is to make use of symbolic cues for inference at higher stages, inspired by the observations of cohesive clusters at different stages. This also avoids learning more ambiguous representations in the higher stages. High-order correlations among the visible and occluded individuals, pairwise interactions, groups, and activities are then solved using the cohesive cluster search within a Bayesian framework. Experiments on several benchmarks show the advantages of our approach over the existing methods."}}
{"id": "rnMVDU2VfQq", "cdate": 1609459200000, "mdate": 1648671823385, "content": {"title": "Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate Single-Shot Object Detection", "abstract": "This paper proposes the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) for fast and accurate single-shot object detection. Feature Pyramid (FP) is widely used in recent visual detection, however the top-down pathway of FP cannot preserve accurate localization due to pooling shifting. The advantage of FP is weakened as deeper backbones with more layers are used. In addition, it cannot keep up accurate detection of both small and large objects at the same time. To address these issues, we propose a new parallel FP structure with bi-directional (top-down and bottom-up) fusion and associated improvements to retain high-quality features for accurate localization. We provide the following design improvements: 1) parallel bifusion FP structure with a bottom-up fusion module (BFM) to detect both small and large objects at once with high accuracy; 2) concatenation and re-organization (CORE) module provides a bottom-up pathway for feature fusion, which leads to the bi-directional fusion FP that can recover lost information from lower-layer feature maps; 3) CORE feature is further purified to retain richer contextual information. Such CORE purification in both top-down and bottom-up pathways can be finished in only a few iterations; 4) adding of a residual design to CORE leads to a new Re-CORE module that enables easy training and integration with a wide range of deeper or lighter backbones. The proposed network achieves state-of-the-art performance on the UAVDT17 and MS COCO datasets."}}
{"id": "rffEw8nEMQc", "cdate": 1609459200000, "mdate": 1648671823318, "content": {"title": "A Video Analytic In-Class Student Concentration Monitoring System", "abstract": "Automatic learning feedback monitoring and analysis are becoming essential in modern education. We present a video analytic system capable of monitoring in-class student\u2019s learning behaviors and providing feedback to the instructor. It is a common practice nowadays for students to take electronic notes or browse online using laptops and cellphones in class. However the use of technology can also impact student concentration and affect learning behaviors, which can seriously hinder their learning progress if not controlled properly. In this pioneering study, we propose a non-intrusive deep-learning based computer vision system to monitor student concentration by extracting and inferring high-level visual behavior cues, including their facial expressions, gestures and activities. Our system can automatically assist instructors with situational awareness in real time. We assume only RGB color images as input and runable system on edge devices for easy deployment. We propose two video analytic components for student behavior analysis: (1) The facial analysis component operates based on Dlib face detection and facial landmark tracking to localize each student and analyze their face orientations, eye blinking, gazes, and facial expressions. (2) The activity detection and recognition component operates based on OpenPose and COCO object detection can identify eight types of in-class gestures and behaviors including raising-hand, typing, phone-answering, crooked-head, desk napping, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">etc.</i> Experiments are performed on a newly collected real-world In-Class Student Activity Dataset (ICSAD), where we achieved nearly 80% activity detection rate. Our system is view-independent in handling facial and pose orientations with average angular error < 10\u00b0. The source code of this work is at: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/YiZengHsieh/ICSAD</uri> ."}}
{"id": "recdU3NM75", "cdate": 1609459200000, "mdate": 1648671823542, "content": {"title": "VocBench: A Neural Vocoder Benchmark for Speech Synthesis", "abstract": "Neural vocoders, used for converting the spectral representations of an audio signal to the waveforms, are a commonly used component in speech synthesis pipelines. It focuses on synthesizing waveforms from low-dimensional representation, such as Mel-Spectrograms. In recent years, different approaches have been introduced to develop such vocoders. However, it becomes more challenging to assess these new vocoders and compare their performance to previous ones. To address this problem, we present VocBench, a framework that benchmark the performance of state-of-the art neural vocoders. VocBench uses a systematic study to evaluate different neural vocoders in a shared environment that enables a fair comparison between them. In our experiments, we use the same setup for datasets, training pipeline, and evaluation metrics for all neural vocoders. We perform a subjective and objective evaluation to compare the performance of each vocoder along a different axis. Our results demonstrate that the framework is capable of showing the competitive efficacy and the quality of the synthesized samples for each vocoder. VocBench framework is available at https://github.com/facebookresearch/vocoder-benchmark."}}
{"id": "r__PvInVMX9", "cdate": 1609459200000, "mdate": 1648671823373, "content": {"title": "Robust Attentive Deep Neural Network for Exposing GAN-generated Faces", "abstract": "GAN-based techniques that generate and synthesize realistic faces have caused severe social concerns and security problems. Existing methods for detecting GAN-generated faces can perform well on limited public datasets. However, images from existing public datasets do not represent real-world scenarios well enough in terms of view variations and data distributions (where real faces largely outnumber synthetic faces). The state-of-the-art methods do not generalize well in real-world problems and lack the interpretability of detection results. Performance of existing GAN-face detection models degrades significantly when facing imbalanced data distributions. To address these shortcomings, we propose a robust, attentive, end-to-end network that can spot GAN-generated faces by analyzing their eye inconsistencies. Specifically, our model learns to identify inconsistent eye components by localizing and comparing the iris artifacts between the two eyes automatically. Our deep network addresses the imbalance learning issues by considering the AUC loss and the traditional cross-entropy loss jointly. Comprehensive evaluations of the FFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the superiority of the proposed method."}}
{"id": "rMVwUnNMX9", "cdate": 1609459200000, "mdate": 1648671823353, "content": {"title": "The 2020 Low-Power Computer Vision Challenge", "abstract": "AI computer vision has advanced significantly in recent years. IoT and edge computing devices such as mobile phones have become the primary computing platform for many end users. Mobile devices such as robots and drones that rely on batteries demand for energy efficient computation. Since 2015, the IEEE Annual International Low-Power Computer Vision Challenge (LPCVC) was held to identify energy-efficient AI and computer vision solutions. The 2020 LPCVC includes three challenge tracks: (1) PyTorch UAV Video Track, (2) FPGA Image Track, and (3) On-device Visual Intelligence Competition (OVIC) Tenforflow Track. This paper summarizes the 2020 winning solutions from the three tracks of LPCVC competitions. Methods and future directions for energy-efficient AI and computer vision research are discussed."}}
{"id": "StLpD824f7q", "cdate": 1609459200000, "mdate": 1648671823486, "content": {"title": "Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces", "abstract": "Generative adversary network (GAN) generated high-realistic human faces have been used as profile images for fake social media accounts and are visually challenging to discern from real ones. In this work, we show that GAN-generated faces can be exposed via irregular pupil shapes. This phenomenon is caused by the lack of physiological constraints in the GAN models. We demonstrate that such artifacts exist widely in high-quality GAN-generated faces and further describe an automatic method to extract the pupils from two eyes and analysis their shapes for exposing the GAN-generated faces. Qualitative and quantitative evaluations of our method suggest its simplicity and effectiveness in distinguishing GAN-generated faces."}}
