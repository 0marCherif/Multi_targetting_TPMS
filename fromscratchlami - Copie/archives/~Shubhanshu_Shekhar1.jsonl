{"id": "zD2OpTeErm", "cdate": 1676827101098, "mdate": null, "content": {"title": "Risk-limiting Financial Audits via Weighted Sampling without Replacement", "abstract": "We introduce the notion of risk-limiting financial audits~(RLFA): procedures that manually evaluate a subset of $N$ financial transactions to  check the validity of a claimed assertion $\\mathcal{A}$ about the transactions. More specifically, RLFA satisfy two properties: (i) if $\\mathcal{A}$ is false, they correctly disprove it with probability at least $1-\\delta$, and (ii) they validate the correctness of $\\mathcal{A}$ with probability $1$, if it is true. We propose a general RLFA strategy, by  constructing new confidence sequences~(CSs)  for the weighted average of $N$ unknown values,  based on samples drawn without replacement  from a (randomized) weighted sampling scheme.  Next, we develop methods to improve the quality of CSs by incorporating side information about  the unknown values. We show that when the side information is sufficiently accurate, it can directly drive the sampling. For the case where the accuracy is unknown \\emph{a priori}, we introduce an alternative approach using control variates. Crucially, our construction adapts to the quality of side information by strongly leveraging the side information if it is highly predictive, and learning to ignore it if it is uninformative.  Our methods also recover the state-of-the-art bounds for the special case of uniformly sampled observations with no side information, which has already found applications in election auditing. The harder weighted case with general side information solves the more challenging problem of AI-assisted financial auditing.\n"}}
{"id": "IB4QCOgYf5n", "cdate": 1672531200000, "mdate": 1681927752449, "content": {"title": "Sequential change detection via backward confidence sequences", "abstract": "We present a simple reduction from sequential estimation to sequential changepoint detection (SCD). In short, suppose we are interested in detecting changepoints in some parameter or functional $\\theta$ of the underlying distribution. We demonstrate that if we can construct a confidence sequence (CS) for $\\theta$, then we can also successfully perform SCD for $\\theta$. This is accomplished by checking if two CSs -- one forwards and the other backwards -- ever fail to intersect. Since the literature on CSs has been rapidly evolving recently, the reduction provided in this paper immediately solves several old and new change detection problems. Further, our \"backward CS\", constructed by reversing time, is new and potentially of independent interest. We provide strong nonasymptotic guarantees on the frequency of false alarms and detection delay, and demonstrate numerical effectiveness on several problems."}}
{"id": "PbKa0yApPq5", "cdate": 1652737583333, "mdate": null, "content": {"title": "A permutation-free kernel two-sample test", "abstract": "The kernel Maximum Mean Discrepancy~(MMD) is a popular multivariate distance metric between distributions. The usual kernel-MMD test statistic (for two-sample testing) is a degenerate U-statistic under the null, and thus it has an intractable limiting null distribution. Hence, the standard approach for designing a level-$(1-\\alpha)$ two-sample test using this statistic involves selecting the rejection threshold as the $(1-\\alpha)$-quantile of the permutation distribution. The resulting nonparametric test has finite-sample validity but suffers from large computational cost, since the test statistic must be recomputed for every permutation. \n\nWe propose the cross-MMD, a new quadratic time MMD test statistic based on sample-splitting and studentization. We prove that under mild assumptions, it has a standard normal limiting distribution under the null. Importantly, we also show that the resulting test is consistent against any fixed alternative, and when using the Gaussian kernel, it has minimax rate-optimal power against local alternatives.  For large sample-sizes, our new cross-MMD provides a significant speedup over the MMD, for only a slight loss in power."}}
{"id": "d091ToOP6D", "cdate": 1640995200000, "mdate": 1681927752461, "content": {"title": "A Permutation-free Kernel Two-Sample Test", "abstract": "The kernel Maximum Mean Discrepancy~(MMD) is a popular multivariate distance metric between distributions that has found utility in two-sample testing. The usual kernel-MMD test statistic is a degenerate U-statistic under the null, and thus it has an intractable limiting distribution. Hence, to design a level-$\\alpha$ test, one usually selects the rejection threshold as the $(1-\\alpha)$-quantile of the permutation distribution. The resulting nonparametric test has finite-sample validity but suffers from large computational cost, since every permutation takes quadratic time. We propose the cross-MMD, a new quadratic-time MMD test statistic based on sample-splitting and studentization. We prove that under mild assumptions, the cross-MMD has a limiting standard Gaussian distribution under the null. Importantly, we also show that the resulting test is consistent against any fixed alternative, and when using the Gaussian kernel, it has minimax rate-optimal power against local alternatives. For large sample sizes, our new cross-MMD provides a significant speedup over the MMD, for only a slight loss in power."}}
{"id": "UMWbJyU58vN", "cdate": 1640995200000, "mdate": 1681927752442, "content": {"title": "Instance Dependent Regret Analysis of Kernelized Bandits", "abstract": "We study the problem of designing an adaptive strategy for querying a noisy zeroth-order-oracle to efficiently learn about the optimizer of an unknown function $f$. To make the problem tractable, w..."}}
{"id": "HN5nwre7NpY", "cdate": 1640995200000, "mdate": 1681927752472, "content": {"title": "A Permutation-Free Kernel Independence Test", "abstract": "In nonparametric independence testing, we observe i.i.d.\\ data $\\{(X_i,Y_i)\\}_{i=1}^n$, where $X \\in \\mathcal{X}, Y \\in \\mathcal{Y}$ lie in any general spaces, and we wish to test the null that $X$ is independent of $Y$. Modern test statistics such as the kernel Hilbert-Schmidt Independence Criterion (HSIC) and Distance Covariance (dCov) have intractable null distributions due to the degeneracy of the underlying U-statistics. Thus, in practice, one often resorts to using permutation testing, which provides a nonasymptotic guarantee at the expense of recalculating the quadratic-time statistics (say) a few hundred times. This paper provides a simple but nontrivial modification of HSIC and dCov (called xHSIC and xdCov, pronounced ``cross'' HSIC/dCov) so that they have a limiting Gaussian distribution under the null, and thus do not require permutations. This requires building on the newly developed theory of cross U-statistics by Kim and Ramdas (2020), and in particular developing several nontrivial extensions of the theory in Shekhar et al. (2022), which developed an analogous permutation-free kernel two-sample test. We show that our new tests, like the originals, are consistent against fixed alternatives, and minimax rate optimal against smooth local alternatives. Numerical simulations demonstrate that compared to the full dCov or HSIC, our variants have the same power up to a $\\sqrt 2$ factor, giving practitioners a new option for large problems or data-analysis pipelines where computation, not sample size, could be the bottleneck."}}
{"id": "-YN5sj88v0I", "cdate": 1640995200000, "mdate": 1681927752456, "content": {"title": "Multi-Scale Zero-Order Optimization of Smooth Functions in an RKHS", "abstract": "Consider the problem of optimizing a black-box function under the assumption that the function is Holder smooth and has bounded norm in the reproducing kernel Hilbert space associated with a given kernel. We propose the LP-GP-UCB algorithm which augments a Gaussian process surrogate model with local polynomial estimators of the function to construct a multi-scale upper confidence bound to guide the search for the optimizer. We provide high probability bounds on the cumulative regret in terms of the maximum information gain and smoothness parameters for the kernel. We then show that the Holder smoothness assumption is satisfied for several commonly used and practically relevant kernels\u2014the Matern, rational- quadratic, \u03b3-exponential, and piecewise-polynomial kernels\u2014and obtain explicit regret bounds for them as a result. These regret bounds establish the near-optimality of LP-GP-UCB for these kernels and are also the first explicit bounds for many of them. Finally, we demonstrate the practical benefits experimentally."}}
{"id": "ZDMqRGSksHs", "cdate": 1621630095116, "mdate": null, "content": {"title": "Adaptive Sampling for Minimax Fair Classification", "abstract": "Machine learning models trained on uncurated datasets can often end up adversely affecting inputs belonging to underrepresented groups. To address this issue, we consider the problem of adaptively constructing training sets which allow us to learn classifiers that are fair in a {\\em minimax} sense. We first propose an adaptive sampling algorithm based on the principle of \\emph{optimism}, and derive theoretical bounds on its performance. We also propose heuristic extensions of this algorithm suitable for application to large scale, practical problems. Next, by deriving algorithm independent lower-bounds for a specific class of problems, we show that the performance achieved by our adaptive scheme cannot be improved in general. We then validate the benefits of adaptively constructing training sets via experiments on synthetic tasks with logistic regression classifiers, as well as on several real-world tasks using convolutional neural networks (CNNs)."}}
{"id": "xbOSRnwRcJ", "cdate": 1609459200000, "mdate": 1681927752454, "content": {"title": "Uncertain-aware Safe Exploratory Planning using Gaussian Process and Neural Control Contraction Metric", "abstract": "Robots operating in unstructured, complex, and changing real-world environments should navigate and maintain safety while collecting data about its environment and updating its model dynamics. In t..."}}
{"id": "u_RV9nWm0zu", "cdate": 1609459200000, "mdate": 1664053377442, "content": {"title": "Active Learning for Classification With Abstention", "abstract": "We construct and analyze active learning algorithms for the problem of binary classification with abstention, in which the learner has an additional option to withhold its decision on certain points in the input space. We consider this problem in the fixed-cost setting, where the learner incurs a cost \u03bb \u2208 (0, 1/2) every time the abstain option is invoked. Our proposed algorithm can work with the three most commonly used active learning query models, namely, membership-query, pool-based, and stream-based models. We obtain a high probability upper-bound on the excess risk of our algorithm, and establish its minimax near-optimality by deriving matching lower-bound (modulo polylogarithmic factors). Since our algorithm relies on the knowledge of the smoothness parameters of the regression function, we also describe a new strategy to adapt to these unknown parameters in a data-driven manner under an additional quality assumption. We show that using this strategy our algorithm achieves the same performance in terms of excess risk as their counterparts with the knowledge of the smoothness parameters. We end the paper with a discussion about the extension of our results to the setting of bounded rate of abstention."}}
