{"id": "dIRiBmOblUv", "cdate": 1609459200000, "mdate": 1632898381086, "content": {"title": "An A*-algorithm for the Unordered Tree Edit Distance with Custom Costs", "abstract": "The unordered tree edit distance is a natural metric to compute distances between trees without intrinsic child order, such as representations of chemical molecules. While the unordered tree edit distance is MAX SNP-hard in principle, it is feasible for small cases, e.g. via an A* algorithm. Unfortunately, current heuristics for the A* algorithm assume unit costs for deletions, insertions, and replacements, which limits our ability to inject domain knowledge. In this paper, we present three novel heuristics for the A* algorithm that work with custom cost functions. In experiments on two chemical data sets, we show that custom costs make the A* computation faster and improve the error of a 5-nearest neighbor regressor, predicting chemical properties. We also show that, on these data, polynomial edit distances can achieve similar results as the unordered tree edit distance."}}
{"id": "ZVTcElBt8aI", "cdate": 1609459200000, "mdate": 1632898381070, "content": {"title": "ast2vec: Utilizing Recursive Neural Encodings of Python Programs", "abstract": "Educational datamining involves the application of datamining techniques to student activity. However, in the context of computer programming, many datamining techniques can not be applied because they expect vector-shaped input whereas computer programs have the form of syntax trees. In this paper, we present ast2vec, a neural network that maps Python syntax trees to vectors and back, thereby facilitating datamining on computer programs as well as the interpretation of datamining results. Ast2vec has been trained on almost half a million programs of novice programmers and is designed to be applied across learning tasks without re-training, meaning that users can apply it without any need for (additional) deep learning. We demonstrate the generality of ast2vec in three settings: First, we provide example analyses using ast2vec on a classroom-sized dataset, involving visualization, student motion analysis, clustering, and outlier detection, including two novel analyses, namely a progress-variance-projection and a dynamical systems analysis. Second, we consider the ability of ast2vec to recover the original syntax tree from its vector representation on the training data and two further large-scale programming datasets. Finally, we evaluate the predictive capability of a simple linear regression on top of ast2vec, obtaining similar results to techniques that work directly on syntax trees. We hope ast2vec can augment the educational datamining toolbelt by making analyses of computer programs easier, richer, and more efficient."}}
{"id": "UebJ8dyglHH", "cdate": 1609459200000, "mdate": 1632898381085, "content": {"title": "Graph Edit Networks", "abstract": "While graph neural networks have made impressive progress in classification and regression, few approaches to date perform time series prediction on graphs, and those that do are mostly limited to edge changes. We suggest that graph edits are a more natural interface for graph-to-graph learning. In particular, graph edits are general enough to describe any graph-to-graph change, not only edge changes; they are sparse, making them easier to understand for humans and more efficient computationally; and they are local, avoiding the need for pooling layers in graph neural networks. In this paper, we propose a novel output layer - the graph edit network - which takes node embeddings as input and generates a sequence of graph edits that transform the input graph to the output graph. We prove that a mapping between the node sets of two graphs is sufficient to construct training data for a graph edit network and that an optimal mapping yields edit scripts that are almost as short as the graph edit distance between the graphs. We further provide a proof-of-concept empirical evaluation on several graph dynamical systems, which are difficult to learn for baselines from the literature."}}
{"id": "Q5Tq3vRLlQh", "cdate": 1609459200000, "mdate": 1632898381093, "content": {"title": "Progress Networks as a Tool for Analysing Student Programming Difficulties", "abstract": "The behavior of students during completion of a learning task can give crucial insights into typical misconceptions as well as issues with the task design. However, analysing the detailed trace of every individual student is time-consuming and infeasible for large-scale classes. In this paper, we propose progress networks as an analytical tool to make sense of student data and demonstrate the technique in large-scale online learning environments for computer programming. These networks, which are easily interpreted by teachers, summarise the progression of a student population through a learning task in a single diagram and, importantly, highlight locations where students fail to make progress. Using data from three different programming courses (N > 4000), we provide instructive examples of how to apply progress networks, including how to zoom in on areas of interest to identify reasons for student difficulty. In addition, we propose a simple technique for comparing progress networks across different cohorts of interest, for instance to analyse learning differences between older and younger students, and to investigate learning retention across tasks on the same programming concept. Finally, we discuss options to improve instructional design based on the insights from progress networks, and show that progress networks can also apply to smaller cohorts."}}
{"id": "OMVmjJIwzpC", "cdate": 1609459200000, "mdate": 1632898381042, "content": {"title": "Reservoir Stack Machines", "abstract": "Memory-augmented neural networks equip a recurrent neural network with an explicit memory to support tasks that require information storage without interference over long times. A key motivation for such research is to perform classic computation tasks, such as parsing. However, memory-augmented neural networks are notoriously hard to train, requiring many backpropagation epochs and a lot of data. In this paper, we introduce the reservoir stack machine, a model which can provably recognize all deterministic context-free languages and circumvents the training problem by training only the output layer of a recurrent net and employing auxiliary information during training about the desired interaction with a stack. In our experiments, we validate the reservoir stack machine against deep and shallow networks from the literature on three benchmark tasks for Neural Turing machines and six deterministic context-free languages. Our results show that the reservoir stack machine achieves zero error, even on test sequences longer than the training data, requiring only a few seconds of training time and 100 training sequences."}}
{"id": "dlEJsyHGeaL", "cdate": 1601308052897, "mdate": null, "content": {"title": "Graph Edit Networks", "abstract": "While graph neural networks have made impressive progress in classification and regression, few approaches to date perform time series prediction on graphs, and those that do are mostly limited to edge changes. We suggest that graph edits are a more natural interface for graph-to-graph learning. In particular,  graph edits are general enough to describe any graph-to-graph change, not only edge changes; they are sparse, making them easier to understand for humans and more efficient computationally; and they are local, avoiding the need for pooling layers in graph neural networks. In this paper, we propose a novel output layer - the graph edit network - which takes node embeddings as input and generates a sequence of graph edits that transform the input graph to the output graph. We prove that a mapping between the node sets of two graphs is sufficient to construct training data for a graph edit network and that an optimal mapping yields edit scripts that are almost as short as the graph edit distance between the graphs. We further provide a proof-of-concept empirical evaluation on several graph dynamical systems, which are difficult to learn for baselines from the literature."}}
{"id": "y1rO8rDI-eK", "cdate": 1577836800000, "mdate": 1632898381049, "content": {"title": "Reservoir memory machines", "abstract": ""}}
{"id": "lEnJl72JW4", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reservoir memory machines", "abstract": "In recent years, Neural Turing Machines have gathered attention by joining the flexibility of neural networks with the computational capabilities of Turing machines. However, Neural Turing Machines are notoriously hard to train, which limits their applicability. We propose reservoir memory machines, which are still able to solve some of the benchmark tests for Neural Turing Machines, but are much faster to train, requiring only an alignment algorithm and linear regression. Our model can also be seen as an extension of echo state networks with an external memory, enabling arbitrarily long storage without interference."}}
{"id": "VfD_IDTaleC", "cdate": 1577836800000, "mdate": 1632898381036, "content": {"title": "The Gendered Nature and Malleability of Gamer Stereotypes", "abstract": "Video gaming is seen as a male space. Female gamers are seen as atypical, have their competence challenged, and face more harassment than male gamers do. This precarious position is increasingly problematic as video gaming is now one of the most prevalent leisure activities, providing an opportunity to both forge and maintain friendships, and to achieve social status and career opportunities. We argue that the marginalization of female gamers is driven by masculine gamer stereotypes. We investigate the content and gendered nature of gamer stereotypes as well as their malleability in response to exposure to female gamers across two studies (NStudy 1\u2009=\u2009287; NStudy 2\u2009=\u2009176). We explore the content of gamer stereotypes and find that they contain both negative aspects, such as lacking social skills, and positive aspects, such as being competent and agentic. Both studies demonstrate that gamer stereotypes are more similar to stereotypes of men and boys than those of women and girls. In Study 2 we test whether exposure to a female gamer can change the negative association between female stereotypes and gamer stereotypes, finding support for this prediction. We conclude that gamer stereotypes are highly gendered but may be malleable: increasing the visibility of female gamers could potentially reduce the incompatibility between femininity and gaming."}}
{"id": "Q-v52fUqviL", "cdate": 1577836800000, "mdate": 1632898417675, "content": {"title": "Reservoir Memory Machines as Neural Computers", "abstract": "Differentiable neural computers extend artificial neural networks with an explicit memory without interference, thus enabling the model to perform classic computation tasks such as graph traversal. However, such models are difficult to train, requiring long training times and large datasets. In this work, we achieve some of the computational capabilities of differentiable neural computers with a model that can be trained very efficiently, namely an echo state network with an explicit memory without interference. This extension enables echo state networks to recognize all regular languages, including those that contractive echo state networks provably can not recognize. Further, we demonstrate experimentally that our model performs comparably to its fully-trained deep version on several typical benchmark tasks for differentiable neural computers."}}
