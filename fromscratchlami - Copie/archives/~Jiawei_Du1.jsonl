{"id": "xK6wRfL2mv7", "cdate": 1652737435614, "mdate": null, "content": {"title": "Sharpness-Aware Training for Free", "abstract": "Modern deep neural networks (DNNs) have achieved state-of-the-art performances but are typically over-parameterized. The over-parameterization may result in undesirably large generalization error in the absence of other customized training strategies. Recently, a line of research under the name of Sharpness-Aware Minimization (SAM) has shown that minimizing a sharpness measure, which reflects the geometry of the loss landscape, can significantly reduce the generalization error. However, SAM-like methods incur a two-fold computational overhead of the given base optimizer (e.g. SGD) for approximating the sharpness measure. In this paper, we propose Sharpness-Aware Training for Free, or SAF, which mitigates the sharp landscape at almost zero additional computational cost over the base optimizer. Intuitively, SAF achieves this by avoiding sudden drops in the loss in the sharp local minima throughout the trajectory of the updates of the weights. Specifically, we suggest a novel trajectory loss, based on the KL-divergence between the outputs of DNNs with the current weights and past weights, as a replacement of the SAM's sharpness measure. This loss captures the rate of change of the training loss along the model's update trajectory. By minimizing it, SAF ensures the convergence to a flat minimum with improved generalization capabilities. Extensive empirical results show that SAF minimizes the sharpness in the same way that SAM does, yielding better results on the ImageNet dataset with essentially the same computational cost as the base optimizer."}}
{"id": "uwfBD_T6bO", "cdate": 1640995200000, "mdate": 1673689922441, "content": {"title": "Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation", "abstract": ""}}
{"id": "rVxZsCEiHe5", "cdate": 1640995200000, "mdate": 1645749459280, "content": {"title": "Memory-Assistant Collaborative Language Understanding for Artificial Intelligence of Things", "abstract": "Artificial intelligence shows promising efforts in collaborating the language models with the artificial intelligence of things (AIoT), promoting the edging intelligence on natural language understanding. To adapt to the limited computational resources in AIoT, the large language models (e.g., transformer) are compressed into light-weight models, which always results in poor feature representation and unsatisfactory performance on downstream tasks, especially on those low-resource language understanding tasks. To address the above issues, we propose a method named memory-assistant multi-task learning (MAMT), where an auxiliary memory module is introduced to promote multitask learning (MT), which serves as a surrogate of target domain representation and performs instance-level weighted MT. More importantly, our MAMT module is in a plug-and-play fashion. Thus, researchers can plug in it to conduct collaborative training and plug it out for AIoT model inference without extra computation burdens. Experiments demonstrate that MAMT significantly improves the performance of light-weight transformer models and show its superiority over the state-of-the-arts on eight GLUE subtasks."}}
{"id": "PSc7gcjlnxO", "cdate": 1640995200000, "mdate": 1673689922421, "content": {"title": "Sharpness-Aware Training for Free", "abstract": ""}}
{"id": "PNVBEsM2Ccz", "cdate": 1640995200000, "mdate": 1673689922391, "content": {"title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks", "abstract": ""}}
{"id": "BjT5yrNwi0q", "cdate": 1640995200000, "mdate": 1667569406774, "content": {"title": "Locality-Aware Crowd Counting", "abstract": "Imbalanced data distribution in crowd counting datasets leads to severe under-estimation and over-estimation problems, which has been less investigated in existing works. In this paper, we tackle this challenging problem by proposing a simple but effective locality-based learning paradigm to produce generalizable features by alleviating sample bias. Our proposed method is locality-aware in two aspects. First, we introduce a locality-aware data partition (LADP) approach to group the training data into different bins via locality-sensitive hashing. As a result, a more balanced data batch is then constructed by LADP. To further reduce the training bias and enhance the collaboration with LADP, a new data augmentation method called locality-aware data augmentation (LADA) is proposed where the image patches are adaptively augmented based on the loss. The proposed method is independent of the backbone network architectures, and thus could be smoothly integrated with most existing deep crowd counting approaches in an end-to-end paradigm to boost their performance. We also demonstrate the versatility of the proposed method by applying it for adversarial defense. Extensive experiments verify the superiority of the proposed method over the state of the arts."}}
{"id": "n0OeTdNRG0Q", "cdate": 1632875526484, "mdate": null, "content": {"title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks", "abstract": "Overparametrized Deep Neural Networks (DNNs) often achieve astounding performances, but may potentially result in severe generalization error. Recently, the relation between the sharpness of the loss landscape and the generalization error has been established by Foret et al. (2020), in which the Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the generalization. Unfortunately, SAM\u2019s computational cost is roughly double that of base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus proposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM\u2019s efficiency at no cost to its generalization performance. ESAM includes two novel and efficient training strategies\u2014StochasticWeight Perturbation and Sharpness-Sensitive Data Selection. In the former, the sharpness measure is approximated by perturbing a stochastically chosen set of weights in each iteration; in the latter, the SAM loss is optimized using only a judiciously selected subset of data that is sensitive to the sharpness. We provide theoretical explanations as to why these strategies perform well. We also show, via extensive experiments on the CIFAR and ImageNet\ndatasets, that ESAM enhances the efficiency over SAM from requiring 100% extra computations to 40% vis-`a-vis base optimizers, while test accuracies are preserved or even improved."}}
{"id": "gHxWHagABV1", "cdate": 1609459200000, "mdate": 1673689922465, "content": {"title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks", "abstract": ""}}
{"id": "qnu5Jh2n97", "cdate": 1577836800000, "mdate": 1673689922657, "content": {"title": "RAIN: Robust and Accurate Classification Networks with Randomization and Enhancement", "abstract": ""}}
{"id": "q49iwYBI2t", "cdate": 1577836800000, "mdate": 1673689922616, "content": {"title": "Query-efficient Meta Attack to Deep Neural Networks", "abstract": ""}}
