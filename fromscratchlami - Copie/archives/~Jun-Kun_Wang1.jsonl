{"id": "FJXf1FXN8C", "cdate": 1663850417411, "mdate": null, "content": {"title": "Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation", "abstract": "We consider a setting that a model needs to adapt to a new domain under distribution shifts, given that only unlabeled test samples from the new domain are accessible at test time. A common idea in most of the related works is constructing pseudo-labels for the unlabeled test samples and applying gradient descent (GD) to a loss function with the pseudo-labels. Recently, Goyal et al. (2022) propose conjugate labels, which is a new kind of pseudo-labels for self-training at test time. They empirically show that the conjugate label outperforms other ways of pseudo-labeling on many domain adaptation benchmarks. However, provably showing that GD with conjugate labels learns a good classifier for test-time adaptation remains open. In this work, we aim at theoretically understanding GD with hard and conjugate labels for a binary classification problem. We show that for square loss, GD with conjugate labels converges to an $\\epsilon$-optimal predictor under a Gaussian model for any arbitrarily small $\\epsilon$, while GD with hard pseudo-labels fails in this task. We also analyze them under different loss functions for the update. Our results shed lights on understanding when and why GD with hard labels or conjugate labels works in test-time adaptation."}}
{"id": "FbRY1XVfwK", "cdate": 1663850363019, "mdate": null, "content": {"title": "Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time", "abstract": "Hamiltonian Monte Carlo (HMC) is a popular method in sampling. While there are quite a few works of studying this method on various aspects, an interesting question is how to choose its integration time to achieve acceleration. In this work, we consider accelerating the process of sampling from a distribution $\\pi(x) \\propto \\exp(-f(x))$ via HMC via time-varying integration time. When the potential $f$ is $L$-smooth and $m$-strongly convex, i.e. for sampling from a log-smooth and strongly log-concave target distribution $\\pi$, it is known that under a constant integration time, the number of iterations that ideal HMC takes to get an $\\epsilon$ Wasserstein-2 distance to the target $\\pi$ is $O( \\kappa \\log \\frac{1}{\\epsilon} )$, where $\\kappa := \\frac{L}{m}$ is the condition number. We propose a scheme of time-varying integration time based on the roots of Chebyshev polynomials. We show that in the case of quadratic potential $f$, i.e. when the target $\\pi$ is a Gaussian distribution, ideal HMC with this choice of integration time only takes $O( \\sqrt{\\kappa} \\log \\frac{1}{\\epsilon} )$ number of iterations to reach Wasserstein-2 distance less than $\\epsilon$; this improvement on the dependence on condition number is akin to acceleration in optimization. The design and analysis of HMC with the proposed integration time is built on the tools of Chebyshev polynomials. Experiments find the advantage of adopting our scheme of time-varying integration time even for sampling from distributions with smooth strongly convex potentials that are not quadratic. \n"}}
{"id": "yYbhKqdi7Hz", "cdate": 1663850037073, "mdate": null, "content": {"title": "Continuized Acceleration for Quasar Convex Functions  in Non-Convex Optimization", "abstract": "Quasar convexity is a condition that allows some first-order methods to efficiently minimize a function even when the optimization landscape is non-convex. Previous works develop near-optimal accelerated algorithms for minimizing this class of functions, however, they require a subroutine of binary search which results in multiple calls to gradient evaluations in each iteration, and consequently the total number of gradient evaluations does not match a known lower bound. In this work, we show that a recently proposed continuized Nesterov acceleration can be applied to minimizing quasar convex functions and achieves the optimal bound with a high probability. Furthermore, we find that the objective functions of training generalized linear models (GLMs) satisfy quasar convexity, which broadens the applicability of the relevant algorithms, while known practical examples of quasar convexity in non-convex learning are sparse in the literature. We also show that if a smooth and one-point strongly convex, Polyak-Lojasiewicz, or quadratic-growth function satisfies quasar convexity, then attaining an accelerated linear rate for minimizing the function is possible under certain conditions, while acceleration is not known in general for these classes of functions.\n"}}
{"id": "pEPxxVmnIWl", "cdate": 1620761305397, "mdate": null, "content": {"title": "Faster rates for convex-concave games", "abstract": "We consider the use of no-regret algorithms to compute equilibria for particular classes of convex-concave games. While standard regret bounds would lead to convergence rates on the order of $O(T^{-1/2})$, recent work (Rakhlin and Sridharan, 2013b; Syrgkanis et al., 2015) has established $O(1/T)$ rates by taking advantage of a particular class of optimistic prediction algorithms. In this work we go further, showing that for a particular class of games one achieves a $O(1/T^2)$ rate, and we show how this applies to the Frank-Wolfe method and recovers a similar bound (Garber and Hazan, 2015). We also show that such no-regret techniques can even achieve a linear rate, $O(\\exp(-T))$, for equilibrium computation under additional curvature assumptions.\n"}}
{"id": "IVk_yq6gtho", "cdate": 1620760874619, "mdate": null, "content": {"title": "A Modular Analysis of Provable Acceleration via Polyak's momentum: Training a Wide ReLU Network and a Deep Linear Network", "abstract": "Incorporating a so-called ``momentum'' dynamic in gradient descent methods is widely used in neural net training as it has been broadly observed that, at least empirically, it often leads to significantly faster convergence. At the same time, there are very few theoretical guarantees in the literature to explain this apparent acceleration effect. Even for the classical strongly convex quadratic problems, several existing results only show Polyak's momentum has an accelerated linear rate asymptotically. In this paper, we first revisit the quadratic problems and show a non-asymptotic accelerated linear rate of Polyak's momentum. Then, we provably show that Polyak's momentum achieves acceleration for training a one-layer wide ReLU network and a deep linear network,\nwhich are perhaps the two most popular canonical models for studying optimization and deep learning in the literature. Prior work Du et al. and Wu et al. showed that using vanilla gradient descent, and with an use of over-parameterization, the error decays as $(1- \\Theta(\\frac{1}{ \\kappa'}))^t$ after $t$ iterations, where $\\kappa'$ is the condition number of a Gram Matrix. Our result shows that with the appropriate choice of parameters Polyak's momentum has a rate of $(1-\\Theta(\\frac{1}{\\sqrt{\\kappa'}}))^t$. For the deep linear network, prior work Hu et al. showed that vanilla gradient descent has a rate of $(1-\\Theta(\\frac{1}{\\kappa}))^t$, where $\\kappa$ is the condition number of a data matrix.  Our result shows an acceleration rate $(1- \\Theta(\\frac{1}{\\sqrt{\\kappa}}))^t$ is achievable by Polyak's momentum. All the results in this work are obtained from a modular analysis, which can be of independent interest. This work establishes that momentum does indeed speed up neural net training.\n"}}
{"id": "E3SWxn0cDBG", "cdate": 1601308262016, "mdate": null, "content": {"title": "Provable Acceleration of Wide Neural Net Training via Polyak's Momentum", "abstract": "Incorporating a so-called momentum dynamic in gradient descent methods is widely used in neural net training as it has been broadly observed that, at least empirically, it often leads to significantly faster convergence. At the same time, there are very few theoretical guarantees in the literature to explain this apparent acceleration effect. In this paper we show that Polyak's momentum, in combination with over-parameterization of the model,  helps achieve faster convergence in training a one-layer ReLU network on $n$ examples.  We show specifically that gradient descent with Polyak's momentum decreases the initial training error at a rate much faster than that of vanilla gradient descent. We provide a bound for a fixed sample size $n$, and we show that gradient descent with Polyak's momentum converges at an accelerated rate to a small error that is controllable by the number of neurons $m$. Prior work (Du et al. 2019) showed that using vanilla gradient descent, and with a similar method of over-parameterization,  the error decays as $(1-\\kappa_n)^t$ after $t$ iterations, where $\\kappa_n$ is a problem-specific parameter. Our result shows that with the appropriate choice of momentum parameter one has a rate of $(1-\\sqrt{\\kappa_n})^t$. This work establishes that momentum does indeed speed up neural net training.\n"}}
{"id": "DUbd4PNhlg", "cdate": 1601308197314, "mdate": null, "content": {"title": "Understanding How Over-Parametrization Leads to Acceleration: A case of learning a single teacher neuron", "abstract": "Over-parametrization has become a popular technique in deep learning.  It is observed that by over-parametrization, a larger neural network needs a fewer training iterations than a smaller one to achieve a certain level of performance --- namely, over-parametrization leads to acceleration in optimization. However, despite that over-parametrization is widely used nowadays, little theory is available to explain the acceleration due to over-parametrization. In this paper, we propose understanding it by studying a simple problem first. Specifically, we consider the setting that there is a single teacher neuron with quadratic activation, where over-parametrization is realized by having multiple student neurons learn the data generated from the teacher neuron. We provably show that over-parametrization helps the iterate generated by gradient descent to enter the neighborhood of a global optimal solution that achieves zero testing error faster. On the other hand, we also point out an issue regarding the necessity of over-parametrization and study how the scaling of the output neurons affects the convergence time.\n\n"}}
{"id": "IZQm8mMRVqW", "cdate": 1601308135499, "mdate": null, "content": {"title": "Quickly Finding a Benign Region via Heavy Ball Momentum in Non-Convex Optimization", "abstract": "The Heavy Ball Method, proposed by Polyak over five decades ago, is a first-order method for optimizing continuous functions. While its stochastic counterpart has proven extremely popular in training deep networks, there are almost no known functions where deterministic Heavy Ball is provably faster than the simple and classical gradient descent algorithm in non-convex optimization. The success of Heavy Ball has thus far eluded theoretical understanding.  Our goal is to address this gap, and in the present work we identify two non-convex problems where we provably show that the Heavy Ball momentum helps the iterate to enter a benign region that contains a global optimal point faster. We show that Heavy Ball exhibits simple dynamics that clearly reveal the benefit of using a larger value of momentum parameter for the problems. The first of these optimization problems is the phase retrieval problem, which has useful applications in physical science. The second of these optimization problems is the cubic-regularized minimization, a critical subroutine required by Nesterov-Polyak cubic-regularized method to find second-order stationary points in general smooth non-convex problems."}}
{"id": "SklE_CNFPr", "cdate": 1569439339775, "mdate": null, "content": {"title": "Zeroth Order Optimization by a Mixture of Evolution Strategies", "abstract": "Evolution strategies or zeroth-order optimization algorithms have become popular in some areas of optimization and machine learning where only the oracle of function value evaluations is available. The central idea in the design of the algorithms is by querying function values of some perturbed points in the neighborhood of the current update and constructing a pseudo-gradient using the function values. In recent years, there is a growing interest in developing new ways of perturbation. Though the new perturbation methods are well motivating, most of them are criticized for lack of convergence guarantees even when the underlying function is convex. Perhaps the only methods that enjoy convergence guarantees are the ones that sample the perturbed points uniformly from a unit sphere or from a multivariate Gaussian distribution with an isotropic covariance. In this work, we tackle the non-convergence issue and propose sampling perturbed points from a mixture of distributions. Experiments show that our proposed method can identify the best perturbation scheme for the convergence and might also help to leverage the complementariness of different perturbation schemes.\n"}}
{"id": "rklw4AVtDH", "cdate": 1569439279350, "mdate": null, "content": {"title": "Optimistic Adaptive Acceleration for Optimization", "abstract": "This paper considers a new variant of AMSGrad called Optimistic-AMSGrad. AMSGrad is a popular adaptive gradient based optimization algorithm that is widely used in training deep neural networks. The new variant assumes that mini-batch gradients in consecutive iterations have some underlying structure, which makes the gradients sequentially predictable. By exploiting the predictability and some ideas from Optimistic Online learning, the proposed algorithm can accelerate the convergence and also enjoys a tighter regret bound. We evaluate Optimistic-AMSGrad and AMSGrad in terms of various performance measures (i.e., training loss, testing loss, and classification accuracy on training/testing data), which demonstrate that Optimistic-AMSGrad improves AMSGrad."}}
