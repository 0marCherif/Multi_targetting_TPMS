{"id": "woZOk2uHRE", "cdate": 1694673774536, "mdate": 1694673774536, "content": {"title": "Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning", "abstract": "Federated Learning (FL) aims to learn a global model from distributed users while protecting their privacy. However, when data are distributed heterogeneously the learning process becomes noisy, unstable, and biased towards the last seen clients\u2019 data, slowing down convergence. To address these issues and improve the robustness and generalization capabilities of the global model, we propose\nWIMA (Window-based Model Averaging). WIMA aggregates global models from different rounds using a window-based approach, effectively capturing knowledge from multiple users and reducing the bias from the last ones. By adopting a windowed view on the rounds, WIMA can be applied from the initial stages of training. Importantly, our method introduces no additional communication or client-\nside computation overhead. Our experiments demonstrate the robustness of WIMA against distribution shifts and bad client sampling, resulting in smoother and more stable learning trends. Additionally, WIMA can be easily integrated with state-of-the-art algorithms. We extensively evaluate our approach on standard FL benchmarks, such as CIFAR10/100 and FEMNIST, demonstrating its effectiveness."}}
{"id": "FbJ-H2fLWRh", "cdate": 1672531200000, "mdate": 1676838730366, "content": {"title": "Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning", "abstract": ""}}
{"id": "sub-ZwPxbIc", "cdate": 1640995200000, "mdate": 1679908269898, "content": {"title": "FedDrive: Generalizing Federated Learning to Semantic Segmentation in Autonomous Driving", "abstract": ""}}
{"id": "n7xR-pRSlmG", "cdate": 1640995200000, "mdate": 1679908270004, "content": {"title": "Improving Generalization in Federated Learning by Seeking Flat Minima", "abstract": ""}}
{"id": "eB5tVS7xYtE", "cdate": 1640995200000, "mdate": 1679908269892, "content": {"title": "Speeding up Heterogeneous Federated Learning with Sequentially Trained Superclients", "abstract": ""}}
{"id": "pBL9xEExWZA", "cdate": 1609459200000, "mdate": 1663657551755, "content": {"title": "Cluster-Driven Graph Federated Learning Over Multiple Domains", "abstract": "Federated Learning (FL) deals with learning a central model (i.e. the server) in privacy-constrained scenarios, where data are stored on multiple devices (i.e. the clients). The central model has no direct access to the data, but only to the updates of the parameters computed locally by each client. This raises a problem, known as statistical heterogeneity, because the clients may have different data distributions (i.e. domains). This is only partly alleviated by clustering the clients. Clustering may reduce heterogeneity by identifying the domains, but it deprives each cluster model of the data and supervision of others. Here we propose a novel Cluster-driven Graph Federated Learning (FedCG). In FedCG, clustering serves to address statistical heterogeneity, while Graph Convolutional Networks (GCNs) enable sharing knowledge across them. FedCG: i) identifies the domains via an FL-compliant clustering and instantiates domain-specific modules (residual branches) for each domain; ii) connects the domain-specific modules through a GCN at training to learn the interactions among domains and share knowledge; and iii) learns to cluster unsupervised via teacher-student classifier-training iterations and to address novel unseen test domains via their domain soft-assignment scores. Thanks to the unique interplay of GCN over clusters, FedCG achieves the state-of-the-art on multiple FL benchmarks."}}
