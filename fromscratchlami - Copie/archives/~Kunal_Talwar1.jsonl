{"id": "vHiCXW2HKwS", "cdate": 1680530728213, "mdate": 1680530728213, "content": {"title": "When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?", "abstract": "Modern machine learning models are complex and frequently encode surprising amounts of information about individual inputs. In extreme cases, complex models appear to memorize entire input examples, including seemingly irrelevant information (social security numbers from text, for example). In this paper, we aim to understand whether this sort of memorization is necessary for accurate learning. We describe natural prediction problems in which every sufficiently accurate training algorithm must encode, in the prediction model, essentially all the information about a large subset of its training examples. This remains true even when the examples are high-dimensional and have entropy much higher than the sample size, and even when most of that information is ultimately irrelevant to the task at hand. Further, our results do not depend on the training algorithm or the class of models used for learning.\nOur problems are simple and fairly natural variants of the next-symbol prediction and the cluster labeling tasks. These tasks can be seen as abstractions of text- and image-related prediction problems. To establish our results, we reduce from a family of one-way communication problems for which we prove new information complexity lower bounds. Additionally, we present synthetic-data experiments demonstrating successful attacks on logistic regression and neural network classifiers. "}}
{"id": "OCsSqikQso", "cdate": 1672531200000, "mdate": 1681491645461, "content": {"title": "Near-Optimal Algorithms for Private Online Optimization in the Realizable Regime", "abstract": ""}}
{"id": "BQtVH_vRa9i", "cdate": 1672531200000, "mdate": 1681491645461, "content": {"title": "Stronger Privacy Amplification by Shuffling for Renyi and Approximate Differential Privacy", "abstract": ""}}
{"id": "1kIZiRelqFt", "cdate": 1654539973041, "mdate": null, "content": {"title": "FLAIR: Federated Learning Annotated Image Repository", "abstract": "Cross-device federated learning is an emerging machine learning (ML) paradigm where a large population of devices collectively train an ML model while the data remains on the devices.\nThis research field has a unique set of practical challenges, and to systematically make advances, new datasets curated to be compatible with this paradigm are needed.\nExisting federated learning benchmarks in the image domain do not accurately capture the scale and heterogeneity of many real-world use cases. \nWe introduce FLAIR, a challenging large-scale annotated image dataset for multi-label classification suitable for federated learning.\nFLAIR has 429,078 images from  51,414  Flickr users and captures many of the intricacies typically encountered in federated learning, such as heterogeneous user data and a long-tailed label distribution.\nWe implement multiple baselines in different learning setups for different tasks on this dataset. \nWe believe FLAIR can serve as a challenging benchmark for advancing the state-of-the art in federated learning.\nDataset access and the code for the benchmark are available at https://github.com/apple/ml-flair.\n"}}
{"id": "dYhB_alLyCO", "cdate": 1652737828718, "mdate": null, "content": {"title": "Mean Estimation with User-level Privacy under Data Heterogeneity", "abstract": "A key challenge in many modern data analysis tasks is that user data is heterogeneous. Different users may possess vastly different numbers of data points. More importantly, it cannot be assumed that all users sample from the same underlying distribution.  This is true, for example in language data, where different speech styles result in data heterogeneity. In this work we propose a simple model of heterogeneous user data that differs in both distribution and quantity of data, and we provide a method for estimating the population-level mean while preserving user-level differential privacy. We demonstrate asymptotic optimality of our estimator and also prove general lower bounds on the error achievable in our problem."}}
{"id": "mUeMOdJ2IJp", "cdate": 1652737579689, "mdate": null, "content": {"title": "Subspace Recovery from Heterogeneous Data with Non-isotropic Noise", "abstract": "Recovering linear subspaces from data is a fundamental and important task in statistics and machine learning. Motivated by heterogeneity in Federated Learning settings, we study a basic formulation of this problem: the principal component analysis (PCA),  with a focus on dealing with irregular noise. Our data come from $n$ users with user $i$ contributing data samples from a $d$-dimensional distribution with mean $\\mu_i$. Our goal is to recover the linear subspace shared by $\\mu_1,\\ldots,\\mu_n$ using the data points from all users, where every data point from user $i$ is formed by adding an independent mean-zero noise vector to $\\mu_i$. If we only have one data point from every user, subspace recovery is information-theoretically impossible when the covariance matrices of the noise vectors can be non-spherical, necessitating additional restrictive assumptions in previous work. We avoid these assumptions by leveraging at least two data points from each user, which allows us to design an efficiently-computable estimator under non-spherical and user-dependent noise. We prove an upper bound for the estimation error of our estimator in general scenarios where the number of data points and amount of noise can vary across users, and prove an information-theoretic error lower bound that not only matches the upper bound up to a constant factor, but also holds even for spherical Gaussian noise. This implies that our estimator does not introduce additional estimation error (up to a constant factor) due to irregularity in the noise. We show additional results for a linear regression problem in a similar setup."}}
{"id": "pDUYkwrx__w", "cdate": 1652737472088, "mdate": null, "content": {"title": "Privacy of Noisy Stochastic Gradient Descent: More Iterations without More Privacy Loss", "abstract": "A central issue in machine learning is how to train models on sensitive user data. Industry has widely adopted a simple algorithm: Stochastic Gradient Descent with noise (a.k.a. Stochastic Gradient Langevin Dynamics). However, foundational theoretical questions about this algorithm's privacy loss remain open---even in the seemingly simple setting of smooth convex losses over a bounded domain. Our main result resolves these questions: for a large range of parameters, we characterize the differential privacy up to a constant. This result reveals that all previous analyses for this setting have the wrong qualitative behavior. Specifically, while previous privacy analyses increase ad infinitum in the number of iterations, we show that after a small burn-in period, running SGD longer leaks no further privacy. Our analysis departs from previous approaches based on fast mixing, instead using techniques based on optimal transport (namely, Privacy Amplification by Iteration) and the Sampled Gaussian Mechanism (namely, Privacy Amplification by Sampling). Our techniques readily extend to other settings."}}
{"id": "vw0q8lPwt7", "cdate": 1640995200000, "mdate": 1681491645460, "content": {"title": "Differential Secrecy for Distributed Data and Applications to Robust Differentially Secure Vector Summation", "abstract": ""}}
{"id": "ryU5sLk24AT", "cdate": 1640995200000, "mdate": 1681491645475, "content": {"title": "Private Online Prediction from Experts: Separations and Faster Rates", "abstract": ""}}
{"id": "qJDv23OTgbI", "cdate": 1640995200000, "mdate": 1681491645477, "content": {"title": "Differential Secrecy for Distributed Data and Applications to Robust Differentially Secure Vector Summation", "abstract": ""}}
