{"id": "lmmKGi7zXn", "cdate": 1652737662553, "mdate": null, "content": {"title": "Infinite Recommendation Networks: A Data-Centric Approach", "abstract": "We leverage the Neural Tangent Kernel and its equivalence to training infinitely-wide neural networks to devise $\\infty$-AE: an autoencoder with infinitely-wide bottleneck layers. The outcome is a highly expressive yet simplistic recommendation model with a single hyper-parameter and a closed-form solution. Leveraging $\\infty$-AE's simplicity, we also develop Distill-CF for synthesizing tiny, high-fidelity data summaries which distill the most important knowledge from the extremely large and sparse user-item interaction matrix for efficient and accurate subsequent data-usage like model training, inference, architecture search, etc. This takes a data-centric approach to recommendation, where we aim to improve the quality of logged user-feedback data for subsequent modeling, independent of the learning algorithm. We particularly utilize the concept of differentiable Gumbel-sampling to handle the inherent data heterogeneity, sparsity, and semi-structuredness, while being scalable to datasets with hundreds of millions of user-item interactions. Both of our proposed approaches significantly outperform their respective state-of-the-art and when used together, we observe $96-105$% of $\\infty$-AE's performance on the full dataset with as little as $0.1$% of the original dataset size, leading us to explore the counter-intuitive question: Is more data what you need for better recommendation?"}}
{"id": "xRqh5Zpx936", "cdate": 1640995200000, "mdate": 1696307819752, "content": {"title": "Infinite Recommendation Networks: A Data-Centric Approach", "abstract": "We leverage the Neural Tangent Kernel and its equivalence to training infinitely-wide neural networks to devise $\\infty$-AE: an autoencoder with infinitely-wide bottleneck layers. The outcome is a highly expressive yet simplistic recommendation model with a single hyper-parameter and a closed-form solution. Leveraging $\\infty$-AE's simplicity, we also develop Distill-CF for synthesizing tiny, high-fidelity data summaries which distill the most important knowledge from the extremely large and sparse user-item interaction matrix for efficient and accurate subsequent data-usage like model training, inference, architecture search, etc. This takes a data-centric approach to recommendation, where we aim to improve the quality of logged user-feedback data for subsequent modeling, independent of the learning algorithm. We particularly utilize the concept of differentiable Gumbel-sampling to handle the inherent data heterogeneity, sparsity, and semi-structuredness, while being scalable to datasets with hundreds of millions of user-item interactions. Both of our proposed approaches significantly outperform their respective state-of-the-art and when used together, we observe $96-105$% of $\\infty$-AE's performance on the full dataset with as little as $0.1$% of the original dataset size, leading us to explore the counter-intuitive question: Is more data what you need for better recommendation?"}}
{"id": "NplU28cljB1", "cdate": 1640995200000, "mdate": 1682357739377, "content": {"title": "Infinite Recommendation Networks: A Data-Centric Approach", "abstract": "We leverage the Neural Tangent Kernel and its equivalence to training infinitely-wide neural networks to devise $\\infty$-AE: an autoencoder with infinitely-wide bottleneck layers. The outcome is a highly expressive yet simplistic recommendation model with a single hyper-parameter and a closed-form solution. Leveraging $\\infty$-AE's simplicity, we also develop Distill-CF for synthesizing tiny, high-fidelity data summaries which distill the most important knowledge from the extremely large and sparse user-item interaction matrix for efficient and accurate subsequent data-usage like model training, inference, architecture search, etc. This takes a data-centric approach to recommendation, where we aim to improve the quality of logged user-feedback data for subsequent modeling, independent of the learning algorithm. We particularly utilize the concept of differentiable Gumbel-sampling to handle the inherent data heterogeneity, sparsity, and semi-structuredness, while being scalable to datasets with hundreds of millions of user-item interactions. Both of our proposed approaches significantly outperform their respective state-of-the-art and when used together, we observe 96-105% of $\\infty$-AE's performance on the full dataset with as little as 0.1% of the original dataset size, leading us to explore the counter-intuitive question: Is more data what you need for better recommendation?"}}
{"id": "yGxvSZ-cT5j", "cdate": 1609459200000, "mdate": 1696307819761, "content": {"title": "Automatic Creation of a Domain Specific Thesaurus Using Siamese Networks", "abstract": "Recent trends have increasingly indicated a shift in search technologies across all applications from syntactic and lexical matching approaches to semantic methods, aiming to understand the intent and contextual meaning of search queries, in order to yield more relevant and accurate results. Such methods often rely on semantic ontologies to map query words to concepts and aid in expansion. However, most applications require a domain specific language definition in order to overcome issues of ambiguity and misinterpretation of meaning. General purpose ontologies are often lacking in this purpose and fail to yield appropriate results in specific applications. In this paper, we propose a novel method of building a domain specific thesaurus for aiding semantic search through automatically creating a refined general thesaurus, followed by training a Siamese Network in two phases to classify candidate synonyms as relevant or non-relevant to the particular domain. We focus on the application of tag-based gallery image retrieval and extract and utilise information from Google's Conceptual Captions dataset in order to improve our model's performance. In order to investigate and justify our training method and architecture, we conduct an ablation study and compare results with our model. We further analytically and empirically demonstrate the advantage of representing terms in a domain-specific environment through semantic vectors fine-tuned on corpora related to the domain. Although our experiments are focused on building a word ontology specific to image retrieval, our method is generic and can be generalised to any field requiring a domain specific semantic language."}}
{"id": "o1r5CJqIoS", "cdate": 1609459200000, "mdate": 1696307819748, "content": {"title": "Fairness and diversity in the recommendation and ranking of participatory media content", "abstract": "Online participatory media platforms that enable one-to-many communication among users, see a significant amount of user generated content and consequently face a problem of being able to recommend a subset of this content to its users. We address the problem of recommending and ranking this content such that different viewpoints about a topic get exposure in a fair and diverse manner. We build our model in the context of a voice-based participatory media platform running in rural central India, for low-income and less-literate communities, that plays audio messages in a ranked list to users over a phone call and allows them to contribute their own messages. In this paper, we describe our model and evaluate it using call-logs from the platform, to compare the fairness and diversity performance of our model with the manual editorial processes currently being followed. Our models are generic and can be adapted and applied to other participatory media platforms as well."}}
{"id": "VinwYpSU8w", "cdate": 1609459200000, "mdate": 1687987400065, "content": {"title": "On-Device Extractive Text Summarization", "abstract": "With increasing connectivity, there has been an exponential surge in the creation and availability of textual content in the form of news articles, blogs, social media posts and product reviews. A large portion of this data is consumed on mobile devices, and more recently, through wearables and smart speakers. Text summarization involves generating a brief description of a text, which captures the overall intention and the vital information being conveyed in its content. Common techniques for automatic text summarization follow extractive or abstractive approaches and involve large scale models with millions of parameters. While such models can be utilized in web or cloud-based applications, they are impractical for deployment on devices with limited storage and computational capabilities. In this paper, we propose a novel character-level neural architecture for extractive text summarization, with the model size reduced by 99.64% to 97.98% from existing methods, thus making it suitable for deployment on-device such as in mobiles, tabs and smart speakers. We tested the performance of our model on various benchmark datasets and compared it with several strong baselines and models. Despite using only a fraction of the space, our model outperformed the baselines and several state-of-the-art models, while coming close in performance with others. On-device text summarization remains largely an unexplored area, and our model's results show a promising approach towards building summarization models suitable for a constrained environment."}}
{"id": "gv-khbyK50", "cdate": 1577836800000, "mdate": 1687987400069, "content": {"title": "TransKP: Transformer based Key-Phrase Extraction", "abstract": "Increased connectivity has led to a sharp rise in the creation and availability of structured and unstructured text content, with millions of new documents being generated every minute. Key-phrase extraction is the process of finding the most important words and phrases which best capture the overall meaning and topics of a text document. Common techniques follow supervised or unsupervised methods for extractive or abstractive key-phrase extraction, but struggle to perform well and generalize to different datasets. In this paper, we follow a supervised, extractive approach and model the key-phrase extraction problem as a sequence labeling task. We utilize the power of transformers on sequential tasks and explore the effect of initializing the embedding layer of the model with pre-trained weights. We test our model on different standard key-phrase extraction datasets and our results significantly outperform all baselines as well as state-of-the-art scores on all the datasets."}}
{"id": "47H201Tf8g", "cdate": 1577836800000, "mdate": 1687987400069, "content": {"title": "Two-Phase Multimodal Neural Network for App Categorization using APK Resources", "abstract": "Following an exponential increase in the number of applications created every year, there are currently over 2.5 million apps in the Google Play Store. Consequently, there has been a sharp rise in the number of apps downloaded by users on their devices. However, limited research has been done on navigability, grouping, and searching of applications on these devices. Current methods of app classification require manual labelling or extracting information from the app store. Such methods are not only resource-intensive and time-consuming but are also not scalable. To overcome these issues, the authors propose a novel architecture for classification of applications into categories, utilizing only the information available in their application packages (APKs) - consequently removing any external dependency and making the entire process completely on-device. A multimodal deep learning approach is followed in a 2-phase training scheme by independently training neural models on distinct sets of information extracted from the APKs and assimilating and fine-tuning the learned weights to incorporate combined knowledge. Our experiments show significant improvement in the evaluation metrics for app classification and clustering over the set benchmarks. The proposed architecture enables a fully on-device solution for app categorization."}}
{"id": "7Davo6exmBJ", "cdate": 1546300800000, "mdate": 1696307819790, "content": {"title": "Fairness and Diversity in the Recommendation and Ranking of Participatory Media Content", "abstract": "Online participatory media platforms that enable one-to-many communication among users, see a significant amount of user generated content and consequently face a problem of being able to recommend a subset of this content to its users. We address the problem of recommending and ranking this content such that different viewpoints about a topic get exposure in a fair and diverse manner. We build our model in the context of a voice-based participatory media platform running in rural central India, for low-income and less-literate communities, that plays audio messages in a ranked list to users over a phone call and allows them to contribute their own messages. In this paper, we describe our model and evaluate it using call-logs from the platform, to compare the fairness and diversity performance of our model with the manual editorial processes currently being followed. Our models are generic and can be adapted and applied to other participatory media platforms as well."}}
