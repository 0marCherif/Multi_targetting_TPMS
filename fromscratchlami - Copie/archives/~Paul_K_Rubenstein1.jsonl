{"id": "NJwDXDVWgTP", "cdate": 1641552909646, "mdate": 1641552909646, "content": {"title": "Causal Consistency of Structural Equation Models", "abstract": "Complex systems can be modelled at various levels of detail. Ideally, causal models of the same system should be consistent with one another in the sense that they agree in their predictions of the effects of interventions. We formalise this notion of consistency in the case of Structural Equation Models (SEMs) by introducing exact transformations between SEMs. This provides a general language to consider, for instance, the different levels of description in the following three scenarios: (a) models with large numbers of variables versus models in which the `irrelevant' or unobservable variables have been marginalised out; (b) micro-level models versus macro-level models in which the macro-variables are aggregate features of the micro-variables; (c) dynamical time series models versus models of their stationary behaviour. Our analysis stresses the importance of well specified interventions in the causal modelling process and sheds light on the interpretation of cyclic SEMs."}}
{"id": "rkxoh24FPH", "cdate": 1569438899021, "mdate": null, "content": {"title": "On Mutual Information Maximization for Representation Learning", "abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods."}}
{"id": "BkeIdNBgLB", "cdate": 1567802477673, "mdate": null, "content": {"title": "Practical and Consistent Estimation of f-Divergences", "abstract": "The estimation of an f-divergence between two probability distributions based on samples is a fundamental problem in statistics and machine learning. Most works study this problem under very weak assumptions, in which case it is provably hard. We consider the case of stronger structural assumptions that are commonly satisfied in modern machine learning, including  representation learning and generative modelling with autoencoder architectures. Under these assumptions we propose and study an estimator that can be easily implemented, works well in high dimensions, and enjoys faster rates of convergence. We verify the behavior of our estimator empirically in both synthetic and real-data experiments, and discuss its direct implications for total correlation, entropy, and mutual information estimation."}}
{"id": "r157GIJvz", "cdate": 1518457377716, "mdate": null, "content": {"title": "Wasserstein Auto-Encoders: Latent Dimensionality and Random Encoders", "abstract": "We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders."}}
{"id": "Hy79-UJPM", "cdate": 1518457226735, "mdate": null, "content": {"title": "Learning Disentangled Representations with Wasserstein Auto-Encoders", "abstract": "We apply Wasserstein auto-encoders (WAEs) to the problem of disentangled representation learning. We highlight the potential of WAEs with promising results on a benchmark disentanglement task."}}
