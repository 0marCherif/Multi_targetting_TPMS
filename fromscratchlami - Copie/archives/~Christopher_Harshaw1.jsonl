{"id": "GM16YCLkEmz", "cdate": 1664815578604, "mdate": null, "content": {"title": "A Design-Based Riesz Representation Framework For Randomized Experiments", "abstract": "We describe a new design-based framework for drawing causal inference in randomized experiments. Estimands in the framework are defined as arbitrary linear functionals of the potential outcome functions, which are posited to live in an experimenter-specified function class. This makes the framework expressive, allowing experimenters to formulate and investigate a wide range of causal questions. We describe a class of estimators for estimands defined using the framework and investigate their properties. The construction of the estimators is based on insights from the Riesz representation theorem. We provide necessary and sufficient conditions for unbiasedness and consistency. Finally, we provide conditions under which the estimators are asymptotically normal, and describe a conservative variance estimator to facilitate inference about the estimands."}}
{"id": "rWUymQp0HA", "cdate": 1640995200000, "mdate": 1681676798285, "content": {"title": "Design and Analysis of Bipartite Experiments Under a Linear Exposure-response Model", "abstract": "A bipartite experiment consists of one set of units being assigned treatments and another set of units for which we measure outcomes. The two sets of units are connected by a bipartite graph, governing how the treated units can affect the outcome units. The bipartite framework naturally arises in marketplace experiments where, for example, experimenters may seek to investigate the effect of discounting goods on buyer behavior. In this paper, we consider estimation of the average total treatment effect in the bipartite experimental framework under a linear exposure-response model. We introduce the Exposure Reweighted Linear (ERL) estimator, and show that the estimator is unbiased, consistent and asymptotically normal, provided that the bipartite graph is sufficiently sparse. To facilitate inference, we introduce an unbiased and consistent estimator of the variance of the ERL point estimator. In addition, we introduce a cluster-based design, Exposure-Design, that uses heuristics to increase the precision of the ERL estimator by realizing a desirable exposure distribution. Finally, we demonstrate the application of the described methodology to marketplace experiments using a publicly available Amazon user-item review dataset. The full version of the paper is available at: https://arxiv.org/abs/2103.06392."}}
{"id": "NJNBzWpy3TS", "cdate": 1640995200000, "mdate": 1681676798259, "content": {"title": "The Power of Subsampling in Submodular Maximization", "abstract": "We propose subsampling as a unified algorithmic technique for submodular maximization in centralized and online settings. The idea is simple: independently sample elements from the ground set and u..."}}
{"id": "WIj0fOu23h4", "cdate": 1609459200000, "mdate": 1681677082467, "content": {"title": "The Power of Subsampling in Submodular Maximization", "abstract": "We propose subsampling as a unified algorithmic technique for submodular maximization in centralized and online settings. The idea is simple: independently sample elements from the ground set, and use simple combinatorial techniques (such as greedy or local search) on these sampled elements. We show that this approach leads to optimal/state-of-the-art results despite being much simpler than existing methods. In the usual offline setting, we present SampleGreedy, which obtains a $(p + 2 + o(1))$-approximation for maximizing a submodular function subject to a $p$-extendible system using $O(n + nk/p)$ evaluation and feasibility queries, where $k$ is the size of the largest feasible set. The approximation ratio improves to $p+1$ and $p$ for monotone submodular and linear objectives, respectively. In the streaming setting, we present SampleStreaming, which obtains a $(4p +2 - o(1))$-approximation for maximizing a submodular function subject to a $p$-matchoid using $O(k)$ memory and $O(km/p)$ evaluation and feasibility queries per element, where $m$ is the number of matroids defining the $p$-matchoid. The approximation ratio improves to $4p$ for monotone submodular objectives. We empirically demonstrate the effectiveness of our algorithms on video summarization, location summarization, and movie recommendation tasks."}}
{"id": "v060XAymnbQ", "cdate": 1594389849042, "mdate": null, "content": {"title": "Greed is Good: Near-Optimal Submodular Maximization via Greedy Optimization", "abstract": "It is known that greedy methods perform well for maximizing monotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity. In this paper, we show - arguably, surprisingly - that invoking the classical greedy algorithm O(k\u221a)-times leads to the (currently) fastest deterministic algorithm, called Repeated Greedy, for maximizing a general submodular function subject to k-independent system constraints. Repeated Greedy achieves (1+O(1/k\u221a))k approximation using O(nrk\u221a) function evaluations (here, n and r denote the size of the ground set and the maximum size of a feasible solution, respectively). We then show that by a careful sampling procedure, we can run the greedy algorithm only once and obtain the (currently) fastest randomized algorithm, called Sample Greedy, for maximizing a submodular function subject to k-extendible system constraints (a subclass of k-independent system constrains). Sample Greedy achieves (k+3)-approximation with only O(nr/k) function evaluations. Finally, we derive an almost matching lower bound, and show that no polynomial time algorithm can have an approximation ratio smaller than k+1/2\u2212\u03b5. To further support our theoretical results, we compare the performance of Repeated Greedy and Sample Greedy with prior art in a concrete application (movie recommendation). We consistently observe that while Sample Greedy achieves practically the same utility as the best baseline, it performs at least two orders of magnitude faster."}}
{"id": "VjWM9VqCQ1j", "cdate": 1594389776339, "mdate": null, "content": {"title": "Balancing covariates in randomized experiments using the Gram-Schmidt Walk", "abstract": "The design of experiments involves a compromise between covariate balance and robustness. This paper introduces an experimental design that admits precise control over this trade-off. The design is specified by a parameter that bounds the worst-case mean square error of an estimator of the average treatment effect. Subject to the experimenter's desired level of robustness, the design aims to simultaneously balance all linear functions of the targeted covariates. The achieved level of balance is considerably better than what a fully random assignment would produce, and it is close to optimal given the desired level of robustness. We show that the mean square error of the estimator is bounded by the minimum of the loss function of a ridge regression of the potential outcomes on the covariates. One may thus interpret the approach as regression adjustment by design. Finally, we provide non-asymptotic tail bounds for the estimator, which facilitate the construction of conservative confidence intervals."}}
{"id": "wDeMZrmsYR", "cdate": 1577836800000, "mdate": 1681676798295, "content": {"title": "Simultaneous Greedys: A Swiss Army Knife for Constrained Submodular Maximization", "abstract": "We present SimultaneousGreedys, a deterministic algorithm for constrained submodular maximization. At a high level, the algorithm maintains $\\ell$ solutions and greedily updates them in a simultaneous fashion. SimultaneousGreedys achieves the tightest known approximation guarantees for both $k$-extendible systems and the more general $k$-systems, which are $(k+1)^2/k = k + \\mathcal{O}(1)$ and $(1 + \\sqrt{k+2})^2 = k + \\mathcal{O}(\\sqrt{k})$, respectively. This is in contrast to previous algorithms, which are designed to provide tight approximation guarantees in one setting, but not both. We also improve the analysis of RepeatedGreedy, showing that it achieves an approximation ratio of $k + \\mathcal{O}(\\sqrt{k})$ for $k$-systems when allowed to run for $\\mathcal{O}(\\sqrt{k})$ iterations, an improvement in both the runtime and approximation over previous analyses. We demonstrate that both algorithms may be modified to run in nearly linear time with an arbitrarily small loss in the approximation.   Both SimultaneousGreedys and RepeatedGreedy are flexible enough to incorporate the intersection of $m$ additional knapsack constraints, while retaining similar approximation guarantees: both algorithms yield an approximation guarantee of roughly $k + 2m + \\mathcal{O}(\\sqrt{k+m})$ for $k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee of $k+2m + \\mathcal{O}(\\sqrt{m})$ for $k$-extendible systems. To complement our algorithmic contributions, we provide a hardness result which states that no algorithm making polynomially many oracle queries can achieve an approximation better than $k + 1/2 + \\varepsilon$. We also present SubmodularGreedy.jl, a Julia package which implements these algorithms and may be downloaded at https://github.com/crharshaw/SubmodularGreedy.jl . Finally, we test the effectiveness of these algorithms on real datasets."}}
{"id": "p3REiu-yPu5", "cdate": 1546300800000, "mdate": 1681677082464, "content": {"title": "Risk based planning of network changes in evolving data centers", "abstract": "Data center networks evolve as they serve customer traffic. When applying network changes, operators risk impacting customer traffic because the network operates at reduced capacity and is more vulnerable to failures and traffic variations. The impact on customer traffic ultimately translates to operator cost (e.g., refunds to customers). However, planning a network change while minimizing the risks is challenging as we need to adapt to a variety of traffic dynamics and cost functions while scaling to large networks and large changes. Today, operators often use plans that maximize the residual capacity (MRC), which often incurs a high cost under different traffic dynamics. Instead, we propose Janus, which searches the large planning space by leveraging the high degree of symmetry in data center networks. Our evaluation on large Clos networks and Facebook traffic traces shows that Janus generates plans in real-time only needing 33~71% of the cost of MRC planners while adapting to a variety of settings."}}
{"id": "YVGGLc7W_R", "cdate": 1546300800000, "mdate": 1681677082466, "content": {"title": "Balancing covariates in randomized experiments using the Gram-Schmidt walk", "abstract": "The design of experiments involves a compromise between covariate balance and robustness. This paper provides a formalization of this trade-off and describes an experimental design that allows experimenters to navigate it. The design is specified by a robustness parameter that bounds the worst-case mean squared error of an estimator of the average treatment effect. Subject to the experimenter's desired level of robustness, the design aims to simultaneously balance all linear functions of potentially many covariates. Less robustness allows for more balance. We show that the mean squared error of the estimator is bounded in finite samples by the minimum of the loss function of an implicit ridge regression of the potential outcomes on the covariates. Asymptotically, the design perfectly balances all linear functions of a growing number of covariates with a diminishing reduction in robustness, effectively allowing experimenters to escape the compromise between balance and robustness in large samples. Finally, we describe conditions that ensure asymptotic normality and provide a conservative variance estimator, which facilitate the construction of asymptotically valid confidence intervals."}}
{"id": "Sy4Cx3WdZr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications", "abstract": "It is generally believed that submodular functions\u2013and the more general class of $\\gamma$-weakly submodular functions\u2013may only be optimized under the non-negativity assumption $f(S) \\geq 0$. In thi..."}}
