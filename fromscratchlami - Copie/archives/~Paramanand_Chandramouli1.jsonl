{"id": "1zeYCAHHeqU", "cdate": 1698592730516, "mdate": 1698592730516, "content": {"title": "On the unreasonable vulnerability of transformers for image restoration - and an easy fix", "abstract": "Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the \"Baseline network\" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to find an easy fix and improve their robustness through adversarial training. While this yields a significant increase in robustness for Restormer, results on other networks are less promising. Interestingly, we find that the design choices in NAFNet and Baselines, which were based on i.i.d. performance, and not on robust generalization, seem to be at odds with the model robustness."}}
{"id": "Gwblt6gAPL", "cdate": 1698591500983, "mdate": 1698591500983, "content": {"title": "Exploring Open Domain Image Super-Resolution through Text", "abstract": "In this work, we propose for the first time a zero-shot approach for flexible open domain extreme super-resolution of images which allows users to interactively explore plausible solutions by using language prompts. Our approach exploits a recent diffusion based text-to-image (T2I) generative model. We modify the generative process of the T2I diffusion model to analytically enforce data consistency of the solution and explore diverse contents of null-space using text guidance. Our approach results in diverse solutions which are simultaneously consistent with input text and the low resolution images."}}
{"id": "L-N1uAxfQk1", "cdate": 1673287854288, "mdate": null, "content": {"title": "Evaluating Adversarial Robustness of Low dose CT Recovery", "abstract": "Low dose computer tomography (CT) acquisition using reduced radiation or sparse angle measurements is recommended to decrease the harmful effects of X-ray radiation. Recent works successfully apply deep networks to the problem of low dose CT recovery\non benchmark datasets. However, their robustness needs a thorough evaluation before use in clinical settings. In this work, we evaluate the robustness of different deep learning  approaches and classical methods for CT recovery.We show that deep networks, including model based networks encouraging data consistency are more susceptible to untargeted attacks. Surprisingly, we observe that data consistency is not heavily affected  even for these poor quality reconstructions, motivating the need for better regularization for the networks. We demonstrate the feasibility of  universal attacks and study attack transferability across different methods.  We analyze robustness to attacks causing localized changes in clinically relevant regions. Both classical approaches and deep networks are affected by such attacks leading to change in  visual appearance of localized lesions, for extremely small perturbations. As the resulting reconstructions have high\ndata consistency with original measurements, these localized attacks can be used to explore the solution space of CT recovery problem. "}}
{"id": "heK0llEQmH", "cdate": 1640995200000, "mdate": 1667246258681, "content": {"title": "On Adversarial Robustness of Deep Image Deblurring", "abstract": "Recent approaches employ deep learning-based solutions for the recovery of a sharp image from its blurry observation. This paper introduces adversarial attacks against deep learning-based image deblurring methods and evaluates the robustness of these neural networks to untargeted and targeted attacks. We demonstrate that imperceptible distortion can significantly degrade the performance of state-of-the-art deblurring networks, even producing drastically different content in the output, indicating the strong need to include adversarially robust training not only in classification but also for image recovery."}}
{"id": "9E1hBiTdvH", "cdate": 1640995200000, "mdate": 1667246258684, "content": {"title": "A Generative Model for Generic Light Field Reconstruction", "abstract": "Recently deep generative models have achieved impressive progress in modeling the distribution of training data. In this work, we present for the first time a generative model for 4D light field patches using variational autoencoders to capture the data distribution of light field patches. We develop a generative model conditioned on the central view of the light field and incorporate this as a prior in an energy minimization framework to address diverse light field reconstruction tasks. While pure learning-based approaches do achieve excellent results on each instance of such a problem, their applicability is limited to the specific observation model they have been trained on. On the contrary, our trained light field generative model can be incorporated as a prior into any model-based optimization approach and therefore extend to diverse reconstruction tasks including light field view synthesis, spatial-angular super resolution and reconstruction from coded projections. Our proposed method demonstrates good reconstruction, with performance approaching end-to-end trained networks, while outperforming traditional model-based approaches on both synthetic and real scenes. Furthermore, we show that our approach enables reliable light field recovery despite distortions in the input."}}
{"id": "1B2ktMLe7R", "cdate": 1640995200000, "mdate": 1667246258680, "content": {"title": "LDEdit: Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models", "abstract": "Research in vision-language models has seen rapid developments off-late, enabling natural language-based interfaces for image generation and manipulation. Many existing text guided manipulation techniques are restricted to specific classes of images, and often require fine-tuning to transfer to a different style or domain. Nevertheless, generic image manipulation using a single model with flexible text inputs is highly desirable. Recent work addresses this task by guiding generative models trained on the generic image datasets using pretrained vision-language encoders. While promising, this approach requires expensive optimization for each input. In this work, we propose an optimization-free method for the task of generic image manipulation from text prompts. Our approach exploits recent Latent Diffusion Models (LDM) for text to image generation to achieve zero-shot text guided manipulation. We employ a deterministic forward diffusion in a lower dimensional latent space, and the desired manipulation is achieved by simply providing the target text to condition the reverse diffusion process. We refer to our approach as LDEdit. We demonstrate the applicability of our method on semantic image manipulation and artistic style transfer. Our method can accomplish image manipulation on diverse domains and enables editing multiple attributes in a straightforward fashion. Extensive experiments demonstrate the benefit of our approach over competing baselines."}}
{"id": "KPGXAsWvPxQ", "cdate": 1609459200000, "mdate": 1667246258671, "content": {"title": "Light Field Implicit Representation for Flexible Resolution Reconstruction", "abstract": "Inspired by the recent advances in implicitly representing signals with trained neural networks, we aim to learn a continuous representation for narrow-baseline 4D light fields. We propose an implicit representation model for 4D light fields which is conditioned on a sparse set of input views. Our model is trained to output the light field values for a continuous range of query spatio-angular coordinates. Given a sparse set of input views, our scheme can super-resolve the input in both spatial and angular domains by flexible factors. consists of a feature extractor and a decoder which are trained on a dataset of light field patches. The feature extractor captures per-pixel features from the input views. These features can be resized to a desired spatial resolution and fed to the decoder along with the query coordinates. This formulation enables us to reconstruct light field views at any desired spatial and angular resolution. Additionally, our network can handle scenarios in which input views are either of low-resolution or with missing pixels. Experiments show that our method achieves state-of-the-art performance for the task of view synthesis while being computationally fast."}}
{"id": "0xsVAk4X1Z", "cdate": 1577836800000, "mdate": 1667246258698, "content": {"title": "Generative Models for Generic Light Field Reconstruction", "abstract": "Recently deep generative models have achieved impressive progress in modeling the distribution of training data. In this work, we present for the first time a generative model for 4D light field patches using variational autoencoders to capture the data distribution of light field patches. We develop a generative model conditioned on the central view of the light field and incorporate this as a prior in an energy minimization framework to address diverse light field reconstruction tasks. While pure learning-based approaches do achieve excellent results on each instance of such a problem, their applicability is limited to the specific observation model they have been trained on. On the contrary, our trained light field generative model can be incorporated as a prior into any model-based optimization approach and therefore extend to diverse reconstruction tasks including light field view synthesis, spatial-angular super resolution and reconstruction from coded projections. Our proposed method demonstrates good reconstruction, with performance approaching end-to-end trained networks, while outperforming traditional model-based approaches on both synthetic and real scenes. Furthermore, we show that our approach enables reliable light field recovery despite distortions in the input."}}
{"id": "wZlRFdsIk_I", "cdate": 1546300800000, "mdate": 1667246258801, "content": {"title": "A Bit Too Much? High Speed Imaging from Sparse Photon Counts", "abstract": "Recent advances in photographic sensing technologies have made it possible to achieve light detection in terms of a single photon. Photon counting sensors are being increasingly used in many diverse applications. We address the problem of jointly recovering spatial and temporal scene radiance from very few photon counts. Our ConvNet-based scheme effectively combines spatial and temporal information present in measurements to reduce noise. We demonstrate that using our method one can acquire videos at a high frame rate and still achieve good quality signal-to-noise ratio. Experiments show that the proposed scheme performs quite well in different challenging scenarios while the existing approaches are unable to handle them."}}
{"id": "EsL98ja_BQx", "cdate": 1546300800000, "mdate": 1667246258829, "content": {"title": "Blind Single Image Reflection Suppression for Face Images using Deep Generative Priors", "abstract": "The goal of single image reflection removal is to suppress unwanted merging of radiances from different surfaces in the scene. This is an inherently ill-posed and challenging problem. Conventional approaches use different assumptions and constraints on the background and reflected layers to solve this problem. Recently, deep learning-based approaches have been applied to this task. These methods require extensive amount of realistic data for training. In this paper, we propose to incorporate class-specific prior models for reducing the ill-posedness of the reflection separation task. Specifically, we use a pre-trained deep face-generative model for reflection supression from face images. We design an optimization scheme that effectively leverages the deep generative model and leads to a constrained solution space. Our method does not require training data corresponding to reflection separation task. We evaluate our proposed approach using both synthetic and real world facial images containing reflections and compare with existing state-of-the-art techniques. The results demonstrate advantages of our approach over the current state-of-the-art in single image reflection separation from faces."}}
