{"id": "X_AJqHfE1H", "cdate": 1673287852742, "mdate": null, "content": {"title": "Vesselformer: Towards Complete 3D Vessel Graph Generation from Images", "abstract": "The reconstruction of graph representations from Images (Image-to-graph) is a frequent task, especially vessel graph extraction from biomedical images. Traditionally, this problem is tackled by a two-stage process: segmentation followed by skeletonization. However, the ambiguity in the heuristic-based pruning of the centerline graph from the skeleta makes it hard to achieve a compact yet faithful graph representation. Recently, \\textit{Relationformer} proposed an end-to-end solution to extract graphs directly from images. However, it does not consider edge features, particularly radius information, which is crucial in many applications such as flow simulation. Further, Relationformer predicts only patch-based graphs. In this work, we address these two shortcomings. We propose a task-specific token, namely radius-token, which explicitly focuses on capturing radius information between two nodes. Second, we propose an efficient algorithm to infer a large 3D graph from patch inference. Finally, we show experimental results on a synthetic vessel dataset and achieve the first 3D complete graph prediction. Code is available at \\url{https://github.com/chinmay5/vesselformer}.\n"}}
{"id": "2Aoi0VKPOWT", "cdate": 1673287845647, "mdate": null, "content": {"title": "ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations", "abstract": "Self-supervised learning has attracted increasing attention as it learns data-driven representation from data without annotations. Vision transformer-based autoencoder (ViT-AE) by He et al. (2021) is a recent self-supervised learning technique that employs a patch-masking strategy to learn a meaningful latent space. In this paper, we focus on improving ViT-AE (nicknamed ViT-AE++) for a more effective representation of both 2D and 3D medical images. We propose two new loss functions to enhance the representation during the training stage. The first loss term aims to improve self-reconstruction by considering the structured dependencies and hence indirectly improving the representation. The second loss term leverages contrastive loss to directly optimize the representation from two randomly masked views.\nAs an independent contribution, we extended ViT-AE++ to a 3D fashion for volumetric medical images. \nWe extensively evaluate ViT-AE++ on both natural images and medical images, demonstrating consistent improvement over vanilla ViT-AE and its superiority over other contrastive learning approaches.  Our code is available at https://github.com/chinmay5/vit_ae_plus_plus.git"}}
{"id": "tkx7uSRhe3D", "cdate": 1672531200000, "mdate": 1693850383596, "content": {"title": "Multi-contrast MRI Super-resolution via Implicit Neural Representations", "abstract": "Clinical routine and retrospective cohorts commonly include multi-parametric Magnetic Resonance Imaging; however, they are mostly acquired in different anisotropic 2D views due to signal-to-noise-ratio and scan-time constraints. Thus acquired views suffer from poor out-of-plane resolution and affect downstream volumetric image analysis that typically requires isotropic 3D scans. Combining different views of multi-contrast scans into high-resolution isotropic 3D scans is challenging due to the lack of a large training cohort, which calls for a subject-specific framework.This work proposes a novel solution to this problem leveraging Implicit Neural Representations (INR). Our proposed INR jointly learns two different contrasts of complementary views in a continuous spatial function and benefits from exchanging anatomical information between them. Trained within minutes on a single commodity GPU, our model provides realistic super-resolution across different pairs of contrasts in our experiments with three datasets. Using Mutual Information (MI) as a metric, we find that our model converges to an optimum MI amongst sequences, achieving anatomically faithful reconstruction. Code is available at: https://github.com/jqmcginnis/multi_contrast_inr."}}
{"id": "nUXR8sXljRD", "cdate": 1672531200000, "mdate": 1693850383638, "content": {"title": "Topologically Faithful Image Segmentation via Induced Matching of Persistence Barcodes", "abstract": "Segmentation models predominantly optimize pixel-overlap-based loss, an objective that is actually inadequate for many segmentation tasks. In recent years, their limitations fueled a growing intere..."}}
{"id": "kGo-E-SNEQS", "cdate": 1672531200000, "mdate": 1693850383629, "content": {"title": "GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance Segmentation", "abstract": "Recent trends in Video Instance Segmentation (VIS) have seen a growing reliance on online methods to model complex and lengthy video sequences. However, the degradation of representation and noise accumulation of the online methods, especially during occlusion and abrupt changes, pose substantial challenges. Transformer-based query propagation provides promising directions at the cost of quadratic memory attention. However, they are susceptible to the degradation of instance features due to the above-mentioned challenges and suffer from cascading effects. The detection and rectification of such errors remain largely underexplored. To this end, we introduce \\textbf{GRAtt-VIS}, \\textbf{G}ated \\textbf{R}esidual \\textbf{Att}ention for \\textbf{V}ideo \\textbf{I}nstance \\textbf{S}egmentation. Firstly, we leverage a Gumbel-Softmax-based gate to detect possible errors in the current frame. Next, based on the gate activation, we rectify degraded features from its past representation. Such a residual configuration alleviates the need for dedicated memory and provides a continuous stream of relevant instance features. Secondly, we propose a novel inter-instance interaction using gate activation as a mask for self-attention. This masking strategy dynamically restricts the unrepresentative instance queries in the self-attention and preserves vital information for long-term tracking. We refer to this novel combination of Gated Residual Connection and Masked Self-Attention as \\textbf{GRAtt} block, which can easily be integrated into the existing propagation-based framework. Further, GRAtt blocks significantly reduce the attention overhead and simplify dynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance on YouTube-VIS and the highly challenging OVIS dataset, significantly improving over previous methods. Code is available at \\url{https://github.com/Tanveer81/GRAttVIS}."}}
{"id": "jStYqNQ0-n", "cdate": 1672531200000, "mdate": 1693850383522, "content": {"title": "Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling", "abstract": "In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data."}}
{"id": "ZJ_fCArfxfG", "cdate": 1672531200000, "mdate": 1693850383598, "content": {"title": "InstanceFormer: An Online Video Instance Segmentation Framework", "abstract": "Recent transformer-based offline video instance segmentation (VIS) approaches achieve encouraging results and significantly outperform online approaches. However, their reliance on the whole video and the immense computational complexity caused by full Spatio-temporal attention limit them in real-life applications such as processing lengthy videos. In this paper, we propose a single-stage transformer-based efficient online VIS framework named InstanceFormer, which is especially suitable for long and challenging videos. We propose three novel components to model short-term and long-term dependency and temporal coherence. First, we propagate the representation, location, and semantic information of prior instances to model short-term changes. Second, we propose a novel memory cross-attention in the decoder, which allows the network to look into earlier instances within a certain temporal window. Finally, we employ a temporal contrastive loss to impose coherence in the representation of an instance across all frames. Memory attention and temporal coherence are particularly beneficial to long-range dependency modeling, including challenging scenarios like occlusion. The proposed InstanceFormer outperforms previous online benchmark methods by a large margin across multiple datasets. Most importantly, InstanceFormer surpasses offline approaches for challenging and long datasets such as YouTube-VIS-2021 and OVIS. Code is available at https://github.com/rajatkoner08/InstanceFormer."}}
{"id": "2rekyGx9osHn", "cdate": 1672531200000, "mdate": 1677745230845, "content": {"title": "Learn-Morph-Infer: A new way of solving the inverse problem for brain tumor modeling", "abstract": ""}}
{"id": "8W1ar7zATN", "cdate": 1664090931394, "mdate": null, "content": {"title": "Graflow: Neural Blood Flow Solver for Vascular Graph", "abstract": "Simulating blood flow is paramount in identifying flow-based biomarkers for vascular-related diseases. A segmented vessel graph is used as a domain for the simulation. Traditionally, partial differential equations are solved with numerical methods. Here, we propose an alternative solver for the simulation of blood flow on a vascular graph leveraging geometric deep learning. Specifically, we reformulate the problem as an implicit function on the graph and learn the simulation by imposing the physics in the loss through a message passing layer. The resultant flow is accurate, fast, and applicable to various tasks."}}
{"id": "a3-QYAgcDBl", "cdate": 1663849858054, "mdate": null, "content": {"title": " Topologically faithful image segmentation via induced matching of persistence barcodes", "abstract": "Image segmentation is a largely researched field where neural networks find vast applications in many facets of technology.\nSome of the most popular approaches to train segmentation networks employ loss functions optimizing pixel-overlap, an objective that is \ninsufficient for many segmentation tasks. In recent years, their limitations fueled a growing interest in topology-aware methods, which aim to recover the correct topology of the segmented structures. However, so far, none of the existing approaches achieve a spatially correct matching between the topological features (persistence barcodes) of label (ground truth) and prediction (output of the neural network). \n\nIn this work, we propose the first topologically and feature-wise accurate metric and loss function for supervised image segmentation, which we term TopoMatch. We show how induced matchings guarantee the spatially correct matching between barcodes in a segmentation setting. Furthermore, we propose an efficient algorithm to compute TopoMatch for images. We show that TopoMatch is an interpretable metric to evaluate the topological correctness of segmentations. Moreover, we demonstrate how induced matchings can be used to train segmentation networks and improve the topological correctness of the segmentations across all 6 baseline datasets while preserving volumetric segmentation performance. "}}
