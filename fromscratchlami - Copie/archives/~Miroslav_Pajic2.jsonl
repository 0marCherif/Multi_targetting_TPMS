{"id": "HO1j9FzQc6N", "cdate": 1683920177747, "mdate": 1683920177747, "content": {"title": "Learning Optimal Strategies for Temporal Tasks in Stochastic Games", "abstract": "Synthesis from linear temporal logic (LTL) specifications provides assured controllers for autonomous systems operating in stochastic and potentially adversarial environments. Automatic synthesis tools, however, require a model of the environment to construct controllers. In this work, we introduce a model-free reinforcement learning (RL) approach that derives controllers from given LTL specifications even when the environment is completely unknown. We model the problem of satisfying the LTL specifications as a stochastic game (SG) between the controller and the adversarial environment; we then learn optimal controller strategies that maximize the probability of satisfying the LTL specifications against the worst-case environment behavior. We first construct a product game using the deterministic parity automaton (DPA) translated from the given LTL specification. By deriving distinct rewards and discount factors from the acceptance condition of the DPA, we reduce the maximization of the worst-case probability of satisfying the LTL specification into the maximization of a discounted reward objective in the product game; this allows for the use of model-free RL algorithms to learn an optimal controller strategy. To deal with the common scalability problems when the number of colors defining the acceptance condition of the DPA is large, we propose a lazy color generation method where distinct rewards and discount factors are utilized only when needed, and an approximate method where the controller eventually focuses on only one color. In several case studies, we show that our approach is scalable to a wide range of LTL formulas, significantly outperforming existing methods for learning controllers from LTL specifications in SGs."}}
{"id": "OurOdAvLoj", "cdate": 1683919596465, "mdate": 1683919596465, "content": {"title": "Formal Verification of Stochastic Systems with ReLU Neural Network Controllers", "abstract": "In this work, we address the problem of formal safety verification for stochastic cyber-physical systems (CPS) equipped with ReLU neural network (NN) controllers. Our goal is to find the set of initial states from where, with a predetermined confidence, the system will not reach an unsafe configuration within a specified time horizon. Specifically, we consider discrete-time LTI systems with Gaussian noise, which we abstract by a suitable graph. Then, we formulate a Satisfiability Modulo Convex (SMC) problem to estimate upper bounds on the transition probabilities between nodes in the graph. Using this abstraction, we propose a method to compute tight bounds on the safety probabilities of nodes in this graph, despite possible over-approximations of the transition probabilities between these nodes. Additionally, using the proposed SMC formula, we devise a heuristic method to refine the abstraction of the system in order to further improve the estimated safety bounds. Finally, we corroborate the efficacy of the proposed method with simulation results considering a robot navigation example and comparison against a state-of-the-art verification scheme."}}
{"id": "5tGQX8WikZ", "cdate": 1669864296859, "mdate": null, "content": {"title": "Concentration Phenomenon for Random Dynamical Systems:  An Operator Theoretic Approach", "abstract": "Via operator theoretic methods, we formalize the concentration phenomenon for a given observable `$r$' of a discrete time Markov chain with `$\\mu_{\\pi}$' as invariant ergodic measure, possibly having support on an unbounded state space. The main contribution of this paper is circumventing tedious probabilistic methods with a study of a composition of the Markov transition operator $P$ followed by a multiplication operator defined by $e^{r}$. It turns out that even if the observable/ reward function  is unbounded, but for some for some $q>2$, $\\|e^{r}\\|_{q \\rightarrow 2} \\propto \\exp\\big(\\mu_{\\pi}(r) +\\frac{2q}{q-2}\\big) $ and $P$ is hyperbounded with norm control $\\|P\\|_{2 \\rightarrow q }< e^{\\frac{1}{2}[\\frac{1}{2}-\\frac{1}{q}]}$, sharp non-asymptotic concentration bounds follow. \\emph{Transport-entropy} inequality ensures the aforementioned upper bound on multiplication operator for all $q>2$.  The role of \\emph{reversibility} in concentration phenomenon is demystified. These results are particularly useful for the reinforcement learning and controls communities as they allow for concentration inequalities w.r.t standard unbounded obersvables/reward functions where exact knowledge of the system is not available, let alone the reversibility of stationary measure. "}}
{"id": "Mv0nGiYER_j", "cdate": 1669852567188, "mdate": null, "content": {"title": "Transportation-Inequalities, Lyapunov Stability and Sampling for Dynamical Systems on Continuous State Space", "abstract": "We study the concentration phenomenon for discrete-time random dynamical systems with an un-\nbounded state space. We develop a heuristic approach towards obtaining exponential concentration\ninequalities for dynamical systems using an entirely functional analytic framework. We also show\nthat existence of exponential-type Lyapunov function, compared to the purely deterministic setting,\nnot only implies stability but also exponential concentration inequalities for sampling from the sta-\ntionary distribution, via transport-entropy inequality (T-E). These results have significant impact\nin reinforcement learning (RL) and controls, leading to exponential concentration inequalities even\nfor unbounded observables (i.e., rewards), while neither assuming reversibility nor exact knowledge\nof the considered random dynamical system (assumptions at heart of concentration inequalities in\nstatistical mechanics and Markov diffusion processes)."}}
{"id": "3VFQfAG3vwi", "cdate": 1663850176118, "mdate": null, "content": {"title": "Variational Latent Branching Model for Off-Policy Evaluation", "abstract": "Model-based methods have recently shown great potential for off-policy evaluation (OPE); offline trajectories induced by behavioral policies are fitted to transitions of Markov decision processes (MDPs), which are used to rollout simulated trajectories and estimate the performance of policies. Model-based OPE methods face two key challenges. First, as offline trajectories are usually fixed, they tend to cover limited state and action space. Second, the performance of model-based methods can be sensitive to the initialization of their parameters. In this work, we propose the variational latent branching model (VLBM) to learn the transition function of MDPs by formulating the environmental dynamics as a compact latent space, from which the next states and rewards are then sampled. Specifically, VLBM leverages and extends the variational inference framework with the recurrent state alignment (RSA), which is designed to capture as much information underlying the limited training data, by smoothing out the information flow between the variational (encoding) and generative (decoding) part of VLBM. Moreover, we also introduce the branching architecture to improve the model\u2019s robustness against randomly initialized model weights. The effectiveness of the VLBM is evaluated on the deep OPE (DOPE) benchmark, from which the training trajectories are designed to result in varied coverage of the state-action space. We show that the VLBM outperforms existing state-of-the-art OPE methods in general."}}
{"id": "fXHl76nO2AZ", "cdate": 1632875732906, "mdate": null, "content": {"title": "Gradient Importance Learning for Incomplete Observations", "abstract": "Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks such as classification. This is particularly true if the data have large missingness rates or a small sample size. More importantly, the imputation error could be propagated into the prediction step that follows, which may constrain the capabilities of the prediction model. In this work, we introduce the gradient importance learning (GIL) method to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Specifically, we employ reinforcement learning (RL) to adjust the gradients used to train these models via back-propagation. This allows the model to exploit the underlying information behind missingness patterns. We test the approach on real-world time-series (i.e., MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset (i.e., MNIST), where our imputation-free predictions outperform the traditional two-step imputation-based predictions using state-of-the-art imputation methods."}}
