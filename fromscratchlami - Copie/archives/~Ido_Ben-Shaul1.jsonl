{"id": "GM_5E9MnSX", "cdate": 1672531200000, "mdate": 1682321547411, "content": {"title": "Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions", "abstract": "Multiplication layers are a key component in various influential neural network modules, including self-attention and hypernetwork layers. In this paper, we investigate the approximation capabilities of deep neural networks with intermediate neurons connected by simple multiplication operations. We consider two classes of target functions: generalized bandlimited functions, which are frequently used to model real-world signals with finite bandwidth, and Sobolev-Type balls, which are embedded in the Sobolev Space $\\mathcal{W}^{r,2}$. Our results demonstrate that multiplicative neural networks can approximate these functions with significantly fewer layers and neurons compared to standard ReLU neural networks, with respect to both input dimension and approximation error. These findings suggest that multiplicative gates can outperform standard feed-forward layers and have potential for improving neural network design."}}
{"id": "MJSIkA72S4k", "cdate": 1663850194958, "mdate": null, "content": {"title": "On the Implicit Bias Towards Depth Minimization in Deep Neural Networks", "abstract": "Recent results in the literature suggest that the penultimate (second-to-last) layer representations of neural networks that are trained for classification exhibit a clustering property called neural collapse (NC). We study the implicit bias of stochastic gradient descent (SGD) in favor of low-depth solutions when training deep neural networks. We characterize a notion of effective depth that measures the first layer for which sample embeddings are separable using the nearest-class center classifier. Furthermore, we hypothesize and empirically show that SGD implicitly selects neural networks of small effective depths. \n\nSecondly, while neural collapse emerges even when generalization should be impossible - we argue that the \\emph{degree of separability} in the intermediate layers is related to generalization. We derive a generalization bound based on comparing the effective depth of the network with the minimal depth required to fit the same dataset with partially corrupted labels. Remarkably, this bound provides non-trivial estimations of the test performance. Finally, we empirically show that the effective depth of a trained neural network monotonically increases when increasing the number of random labels in data."}}
{"id": "rrlxYRmNfXc", "cdate": 1648669649147, "mdate": 1648669649147, "content": {"title": "Sparsity-Probe: Analysis tool for Deep Learning Models", "abstract": "We propose a probe for the analysis of deep learning architectures that is based on machine learning and approximation theoretical principles. Given a deep learning architecture and a training set, during or after training, the Sparsity Probe allows to analyze the performance of intermediate layers by quantifying the geometrical features of representations of the training set. We show how the Sparsity Probe enables measuring the contribution of adding depth to a given architecture, to detect under-performing layers, etc., all this without any auxiliary test data set."}}
{"id": "STfgstm4zmq", "cdate": 1648669570775, "mdate": 1648669570775, "content": {"title": "Nearest Class-Center Simplification through Intermediate Layers", "abstract": "Recent advances in theoretical Deep Learning have introduced geometric properties that occur during training, past the Interpolation Threshold -- where the training error reaches zero. We inquire into the phenomena coined Neural Collapse in the intermediate layers of the networks, and emphasize the innerworkings of Nearest Class-Center Mismatch inside the deepnet. We further show that these processes occur both in vision and language model architectures. Lastly, we propose a Stochastic Variability-Simplification Loss (SVSL) that encourages better geometrical features in intermediate layers, and improves both train metrics and generalization."}}
{"id": "w8YGyp92JwU", "cdate": 1640995200000, "mdate": 1682321547413, "content": {"title": "Is it out yet? Automatic Future Product Releases Extraction from Web Data", "abstract": ""}}
{"id": "IJRZREWTrk", "cdate": 1640995200000, "mdate": 1682321547457, "content": {"title": "Nearest Class-Center Simplification through Intermediate Layers", "abstract": "Recent advances in theoretical Deep Learning have introduced geometric properties that occur during training, past the Interpolation Threshold -- where the training error reaches zero. We inquire into the phenomena coined Neural Collapse in the intermediate layers of the networks, and emphasize the innerworkings of Nearest Class-Center Mismatch inside the deepnet. We further show that these processes occur both in vision and language model architectures. Lastly, we propose a Stochastic Variability-Simplification Loss (SVSL) that encourages better geometrical features in intermediate layers, and improves both train metrics and generalization."}}
{"id": "PS3XqDJQnJ", "cdate": 1609459200000, "mdate": 1682321547172, "content": {"title": "Sparsity-Probe: Analysis tool for Deep Learning Models", "abstract": "We propose a probe for the analysis of deep learning architectures that is based on machine learning and approximation theoretical principles. Given a deep learning architecture and a training set, during or after training, the Sparsity Probe allows to analyze the performance of intermediate layers by quantifying the geometrical features of representations of the training set. We show how the Sparsity Probe enables measuring the contribution of adding depth to a given architecture, to detect under-performing layers, etc., all this without any auxiliary test data set."}}
{"id": "m4baHw5LZ7M", "cdate": 1601308164334, "mdate": null, "content": {"title": "Deep Learning Solution of the Eigenvalue Problem for Differential Operators", "abstract": "Solving the eigenvalue problem for differential operators is a common problem in many scientific fields. Classical numerical methods rely on intricate domain discretization, and yield non-analytic or non-smooth approximations. We introduce a novel Neural Network (NN)-based solver for the eigenvalue problem of differential self-adjoint operators where the eigenpairs are learned in an unsupervised end-to-end fashion. We propose three different training procedures, for solving increasingly challenging tasks towards the general eigenvalue problem. \n\nThe proposed solver is able to find the M smallest eigenpairs for a general differential operator. We demonstrate the method on the Laplacian operator which is of particular interest in image processing, computer vision, shape analysis among many other applications. \nUnlike other numerical methods such as finite differences, the partial derivatives of the network approximation of the eigenfunction can be analytically calculated to any order. Therefore, the proposed framework enables the solution of higher order operators and on free shape domain or even on a manifold. Non-linear operators can be investigated by this approach as well."}}
{"id": "i4pG7ZKr8e", "cdate": 1577836800000, "mdate": 1682321547476, "content": {"title": "Certainty Pooling for Multiple Instance Learning", "abstract": ""}}
{"id": "YLQ66L2F8x0", "cdate": 1577836800000, "mdate": 1682321547269, "content": {"title": "Certainty Pooling for Multiple Instance Learning", "abstract": "Multiple Instance Learning is a form of weakly supervised learning in which the data is arranged in sets of instances called bags with one label assigned per bag. The bag level class prediction is derived from the multiple instances through application of a permutation invariant pooling operator on instance predictions or embeddings. We present a novel pooling operator called \\textbf{Certainty Pooling} which incorporates the model certainty into bag predictions resulting in a more robust and explainable model. We compare our proposed method with other pooling operators in controlled experiments with low evidence ratio bags based on MNIST, as well as on a real life histopathology dataset - Camelyon16. Our method outperforms other methods in both bag level and instance level prediction, especially when only small training sets are available. We discuss the rationale behind our approach and the reasons for its superiority for these types of datasets."}}
