{"id": "BCm6ToGCyhA", "cdate": 1693526400000, "mdate": 1695950048991, "content": {"title": "ATNAS: Automatic Termination for Neural Architecture Search", "abstract": ""}}
{"id": "mIPL2opTUT", "cdate": 1672531200000, "mdate": 1681653348418, "content": {"title": "Language Agnostic Gesture Generation Model: A Case Study of Japanese Speakers' Gesture Generation Using English Text-to-Gesture Model", "abstract": ""}}
{"id": "dISbisBjKR", "cdate": 1672531200000, "mdate": 1695950048999, "content": {"title": "Surrogate-Assisted (1+1)-CMA-ES with Switching Mechanism of Utility Functions", "abstract": "The invariance to any monotonically increasing transformation of the objective function is an essential property of the covariance matrix adaptation evolution strategy (CMA-ES). However, the surrogate-assisted CMA-ES often loses this invariance because the performance of the surrogate model is influenced by such transformation. In this paper, we propose a surrogate-assisted $$(1+1)$$ -CMA-ES with the Gaussian process regression (GPR) possessing the robustness for such transformation. We introduce two utility functions based on the ranking and Lebesgue measure of candidate solutions, which are invariant to such transformation. We train GPR with the utility function values instead of the objective function values to estimate the ranking of the candidate solutions. Moreover, we propose switching Gaussian process CMA-ES (SGP-CMA-ES), which contains a mechanism selecting the suitable target variable of GPR from utility function values in addition to the objective function values to improve the performance on the objective functions that GPR can estimate easily. The experimental results show that SGP-CMA-ES is superior to existing methods on the objective function with several transformations, while maintaining the performance comparable to that of existing methods on the objective function without transformation."}}
{"id": "M9NNMBGlTG", "cdate": 1672531200000, "mdate": 1695950048986, "content": {"title": "Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization", "abstract": "In real-world applications, a machine learning model is required to handle an open-set recognition (OSR), where unknown classes appear during the inference, in addition to a domain shift, where the distribution of data differs between the training and inference phases. Domain generalization (DG) aims to handle the domain shift situation where the target domain of the inference phase is inaccessible during model training. Open domain generalization (ODG) takes into account both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a method targeting ODG but has a complicated learning process. On the other hand, although various DG methods have been proposed, they have not been evaluated in ODG situations. This work comprehensively evaluates existing DG methods in ODG and shows that two simple DG methods, CORrelation ALignment (CORAL) and Maximum Mean Discrepancy (MMD), are competitive with DAML in several cases. In addition, we propose simple extensions of CORAL and MMD by introducing the techniques used in DAML, such as ensemble learning and Dirichlet mixup data augmentation. The experimental evaluation demonstrates that the extended CORAL and MMD can perform comparably to DAML with lower computational costs. This suggests that the simple DG methods and their simple extensions are strong baselines for ODG. The code used in the experiments is available at https://github.com/shiralab/OpenDG-Eval."}}
{"id": "0dvV1fZ9eyl", "cdate": 1672531200000, "mdate": 1695950049028, "content": {"title": "(1+1)-CMA-ES with Margin for Discrete and Mixed-Integer Problems", "abstract": "The covariance matrix adaptation evolution strategy (CMA-ES) is an efficient continuous black-box optimization method. The CMA-ES possesses many attractive features, including invariance properties and a well-tuned default hyperparameter setting. Moreover, several components to specialize the CMA-ES have been proposed, such as noise handling and constraint handling. To utilize these advantages in mixed-integer optimization problems, the CMA-ES with margin has been proposed. The CMA-ES with margin prevents the premature convergence of discrete variables by the margin correction, in which the distribution parameters are modified to leave the generation probability for changing the discrete variable. The margin correction has been applied to (\u03bc/\u03bcw,\u039b)-CMA-ES, while this paper introduces the margin correction into (1+1)-CMA-ES, an elitist version of CMA-ES. The (1+1)-CMA-ES is often advantageous for unimodal functions and can be computationally less expensive. To tackle the performance deterioration on mixed-integer optimization, we use the discretized elitist solution as the mean of the sampling distribution and modify the margin correction not to move the elitist solution. The numerical simulation using benchmark functions on mixed-integer, integer, and binary domains shows that (1+1)-CMA-ES with margin outperforms the CMA-ES with margin and is better than or comparable with several specialized methods to a particular search domain."}}
{"id": "wewZ5DgeNJ", "cdate": 1640995200000, "mdate": 1664151476412, "content": {"title": "Efficient Search of Multiple Neural Architectures with Different Complexities via Importance Sampling", "abstract": "Neural architecture search (NAS) aims to automate architecture design processes and improve the performance of deep neural networks. Platform-aware NAS methods consider both performance and complexity and can find well-performing architectures with low computational resources. Although ordinary NAS methods result in tremendous computational costs owing to the repetition of model training, one-shot NAS, which trains the weights of a supernetwork containing all candidate architectures only once during the search process, has been reported to result in a lower search cost. This study focuses on the architecture complexity-aware one-shot NAS that optimizes the objective function composed of the weighted sum of two metrics, such as the predictive performance and number of parameters. In existing methods, the architecture search process must be run multiple times with different coefficients of the weighted sum to obtain multiple architectures with different complexities. This study aims at reducing the search cost associated with finding multiple architectures. The proposed method uses multiple distributions to generate architectures with different complexities and updates each distribution using the samples obtained from multiple distributions based on importance sampling. The proposed method allows us to obtain multiple architectures with different complexities in a single architecture search, resulting in reducing the search cost. The proposed method is applied to the architecture search of convolutional neural networks on the CIAFR-10 and ImageNet datasets. Consequently, compared with baseline methods, the proposed method finds multiple architectures with varying complexities while requiring less computational effort."}}
{"id": "gm7L3ilr1AX", "cdate": 1640995200000, "mdate": 1664151476419, "content": {"title": "CMA-ES with margin: lower-bounding marginal probability for mixed-integer black-box optimization", "abstract": "This study targets the mixed-integer black-box optimization (MI-BBO) problem where continuous and integer variables should be optimized simultaneously. The CMA-ES, our focus in this study, is a population-based stochastic search method that samples solution candidates from a multivariate Gaussian distribution (MGD), which shows excellent performance in continuous BBO. The parameters of MGD, mean and (co)variance, are updated based on the evaluation value of candidate solutions in the CMA-ES. If the CMA-ES is applied to the MI-BBO with straightforward discretization, however, the variance corresponding to the integer variables becomes much smaller than the granularity of the discretization before reaching the optimal solution, which leads to the stagnation of the optimization. In particular, when binary variables are included in the problem, this stagnation more likely occurs because the granularity of the discretization becomes wider, and the existing modification to the CMA-ES does not address this stagnation. To overcome these limitations, we propose a simple modification of the CMA-ES based on lower-bounding the marginal probabilities associated with the generation of integer variables in the MGD. The numerical experiments on the MI-BBO benchmark problems demonstrate the efficiency and robustness of the proposed method."}}
{"id": "fbs3S4pg7-", "cdate": 1640995200000, "mdate": 1664151476418, "content": {"title": "A two-phase framework with a b\u00e9zier simplex-based interpolation method for computationally expensive multi-objective optimization", "abstract": "This paper proposes a two-phase framework with a B\u00e9zier simplex-based interpolation method (TPB) for computationally expensive multi-objective optimization. The first phase in TPB aims to approximate a few Pareto optimal solutions by optimizing a sequence of single-objective scalar problems. The first phase in TPB can fully exploit a state-of-the-art single-objective derivative-free optimizer. The second phase in TPB utilizes a B\u00e9zier simplex model to interpolate the solutions obtained in the first phase. The second phase in TPB fully exploits the fact that a B\u00e9zier simplex model can approximate the Pareto optimal solution set by exploiting its simplex structure when a given problem is simplicial. We investigate the performance of TPB on the 55 bi-objective BBOB problems. The results show that TPB performs significantly better than HMO-CMA-ES and some state-of-the-art meta-model-based optimizers."}}
{"id": "cpj-TuYqav", "cdate": 1640995200000, "mdate": 1674959929493, "content": {"title": "Neural Architecture Search for Improving Latency-Accuracy Trade-off in Split Computing", "abstract": "This paper proposes a neural architecture search (NAS) method for split computing. Split computing is an emerging machine-learning inference technique that addresses the privacy and latency challenges of deploying deep learning in IoT systems. In split computing, neural network models are separated and cooperatively processed using edge servers and IoT devices via networks. Thus, the architecture of the neural network model significantly impacts the communication payload size, model accuracy, and computational load. In this paper, we address the challenge of optimizing neural network architecture for split computing. To this end, we proposed NASC (NAS for Split Computing), which jointly explores optimal model architecture and a split point to achieve higher accuracy while meeting latency requirements (i.e., smaller total latency of computation and communication than a certain threshold). NASC employs a one-shot NAS that does not require repeating model training for a computationally efficient architecture search. Our performance evaluation using hardware (HW)-NAS-Bench of benchmark data demonstrates that the proposed NASC can improve the \u201ccommunication latency and model accuracy\u201d trade-off, i.e., reduce the latency by approximately 40\u201360% from the baseline, with slight accuracy degradation."}}
{"id": "WAXVJ08lDi", "cdate": 1640995200000, "mdate": 1681653348546, "content": {"title": "Improvement of sep-CMA-ES for Optimization of High-Dimensional Functions with Low Effective Dimensionality", "abstract": ""}}
