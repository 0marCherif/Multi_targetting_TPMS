{"id": "G_5nrlUutJt", "cdate": 1683880164253, "mdate": 1683880164253, "content": {"title": "On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology", "abstract": "Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute (access) time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under `graph rewiring'."}}
{"id": "LIDvgVjpkZr", "cdate": 1664194172076, "mdate": null, "content": {"title": "Sheaf Attention Networks", "abstract": "Attention has become a central inductive bias for deep learning models irrespective of domain. However, increasing theoretical and empirical evidence suggests that Graph Attention Networks (GATs) suffer from the same pathological issues affecting many other Graph Neural Networks (GNNs). First, GAT's features tend to become progressively smoother as more layers are stacked, and second, the model performs poorly in heterophilic graphs. Sheaf Neural Networks (SNNs), a new class of models inspired by algebraic topology and geometry, have shown much promise in tackling these two issues. Building upon the recent success of SNNs and the wide adoption of attention-based architectures, we propose Sheaf Attention Networks (SheafANs). By making use of a novel and more expressive attention mechanism equipped with geometric inductive biases, we show that this type of construction generalizes popular attention-based GNN models to cellular sheaves. We demonstrate that these models help tackle the oversmoothing and heterophily problems and show that, in practice, SheafANs consistently outperform GAT on synthetic and real-world benchmarks."}}
{"id": "JLR_B7n_Wqr", "cdate": 1663850315118, "mdate": null, "content": {"title": "Latent Graph Inference using Product Manifolds", "abstract": "Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer similarity measures that are leveraged by the latent graph learning model to obtain optimized latent graphs. Moreover, the curvature of the product manifold is learned during training alongside the rest of the network parameters and based on the downstream task, rather than it being a static embedding space. Our novel approach is tested on a wide range of datasets, and outperforms the original dDGM model."}}
{"id": "pOVaPJkP0E", "cdate": 1662812639376, "mdate": null, "content": {"title": "Graph Neural Network Expressivity and Meta-Learning for Molecular Property Regression", "abstract": "We demonstrate the applicability of model-agnostic algorithms for meta-learning, specifically Reptile, to GNN models in molecular regression tasks. Using meta-learning we are able to learn new chemical prediction tasks with only a few model updates, as compared to using randomly initialized GNNs which require learning each regression task from scratch. We experimentally show that GNN layer expressivity is correlated to improved meta-learning. Additionally, we also experiment with GNN emsembles which yield best performance and rapid convergence for k-shot learning."}}
{"id": "iywwhvrtSrc", "cdate": 1658771414074, "mdate": 1658771414074, "content": {"title": "Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift", "abstract": "Machine learning for malware classification shows\nencouraging results, but real deployments suffer from performance degradation as malware authors adapt their techniques to\nevade detection. This phenomenon, known as concept drift, occurs\nas new malware examples evolve and become less and less like the\noriginal training examples. One promising method to cope with\nconcept drift is classification with rejection in which examples\nthat are likely to be misclassified are instead quarantined until\nthey can be expertly analyzed.\nWe propose TRANSCENDENT, a rejection framework built on\nTranscend, a recently proposed strategy based on conformal\nprediction theory. In particular, we provide a formal treatment of\nTranscend, enabling us to refine conformal evaluation theory\u2014its\nunderlying statistical engine\u2014and gain a better understanding\nof the theoretical reasons for its effectiveness. In the process,\nwe develop two additional conformal evaluators that match\nor surpass the performance of the original while significantly\ndecreasing the computational overhead. We evaluate TRANSCENDENT on a malware dataset spanning 5 years that removes\nsources of experimental bias present in the original evaluation.\nTRANSCENDENT outperforms state-of-the-art approaches while\ngeneralizing across different malware domains and classifiers.\nTo further assist practitioners, we showcase optimal operational settings for a TRANSCENDENT deployment and show how\nit can be applied to many popular learning algorithms. These\ninsights support both old and new empirical findings, making\nTranscend a sound and practical solution for the first time. To\nthis end, we release TRANSCENDENT as open source, to aid the\nadoption of rejection strategies by the security community."}}
