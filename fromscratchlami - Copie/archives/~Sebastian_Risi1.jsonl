{"id": "g5iNkzYpPAl", "cdate": 1677628800000, "mdate": 1695972428763, "content": {"title": "Hybrid Encoding for Generating Large Scale Game Level Patterns With Local Variations", "abstract": "Generative adversarial networks (GANs) are a powerful indirect genotype-to-phenotype mapping for evolutionary search. Much previous work applying GANs to level generation focuses on fixed-size segments combined into a whole level, but individual segments may not fit together cohesively. In contrast, segments in human designed levels are often repeated, directly or with variation, and organized into patterns (the symmetric eagle in Level 1 of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">The Legend of Zelda</i> , or repeated pipe motifs in <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Super Mario Bros.</i> ). Such patterns can be produced with compositional pattern producing networks (CPPNs). CPPNs define latent vector GAN inputs as a function of geometry, organizing segments output by a GAN into complete levels. However, collections of latent vectors can also be evolved directly, producing more chaotic levels. We propose a hybrid approach that evolves CPPNs first, but allows latent vectors to evolve later, combining the benefits of both approaches. These approaches are evaluated in <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Super Mario Bros</i> . and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">The Legend of Zelda</i> . We previously demonstrated via a quality-diversity algorithm that CPPNs better cover the space of possible levels than directly evolved levels. Here, we show that the hybrid approach first, covers areas that neither of the other methods can, and second, achieves comparable or superior quality diversity (QD) scores."}}
{"id": "sExO5aOc1W", "cdate": 1672531200000, "mdate": 1695972428766, "content": {"title": "Skill Decision Transformer", "abstract": "Recent work has shown that Large Language Models (LLMs) can be incredibly effective for offline reinforcement learning (RL) by representing the traditional RL problem as a sequence modelling problem (Chen et al., 2021; Janner et al., 2021). However many of these methods only optimize for high returns, and may not extract much information from a diverse dataset of trajectories. Generalized Decision Transformers (GDTs) (Furuta et al., 2021) have shown that utilizing future trajectory information, in the form of information statistics, can help extract more information from offline trajectory data. Building upon this, we propose Skill Decision Transformer (Skill DT). Skill DT draws inspiration from hindsight relabelling (Andrychowicz et al., 2017) and skill discovery methods to discover a diverse set of primitive behaviors, or skills. We show that Skill DT can not only perform offline state-marginal matching (SMM), but can discovery descriptive behaviors that can be easily sampled. Furthermore, we show that through purely reward-free optimization, Skill DT is still competitive with supervised offline RL approaches on the D4RL benchmark. The code and videos can be found on our project page: https://github.com/shyamsn97/skill-dt"}}
{"id": "dcA1jWmVX5", "cdate": 1672531200000, "mdate": 1683891583946, "content": {"title": "MarioGPT: Open-Ended Text2Level Generation through Large Language Models", "abstract": "Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We also combine MarioGPT with novelty search, enabling it to generate diverse levels with varying play-style dynamics (i.e. player paths). This combination allows for the open-ended generation of an increasingly diverse range of content."}}
{"id": "_5O1Z_h4KQ", "cdate": 1672531200000, "mdate": 1695972428737, "content": {"title": "Learning to Act through Evolution of Neural Diversity in Random Neural Networks", "abstract": "Biological nervous systems consist of networks of diverse, sophisticated information processors in the form of neurons of different classes. In most artificial neural networks (ANNs), neural computation is abstracted to an activation function that is usually shared between all neurons within a layer or even the whole network; training of ANNs focuses on synaptic optimization. In this paper, we propose the optimization of neuro-centric parameters to attain a set of diverse neurons that can perform complex computations. Demonstrating the promise of the approach, we show that evolving neural parameters alone allows agents to solve various reinforcement learning tasks without optimizing any synaptic weights. While not aiming to be an accurate biological model, parameterizing neurons to a larger degree than the current common practice, allows us to ask questions about the computational abilities afforded by neural diversity in random neural networks. The presented results open up interesting future research directions, such as combining evolved neural diversity with activity-dependent plasticity."}}
{"id": "WuTJ3-amvB", "cdate": 1672531200000, "mdate": 1695972428767, "content": {"title": "Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs", "abstract": "Biological nervous systems are created in a fundamentally different way than current artificial neural networks. Despite its impressive results in a variety of different domains, deep learning often requires considerable engineering effort to design high-performing neural architectures. By contrast, biological nervous systems are grown through a dynamic self-organizing process. In this paper, we take initial steps toward neural networks that grow through a developmental process that mirrors key properties of embryonic development in biological organisms. The growth process is guided by another neural network, which we call a Neural Developmental Program (NDP) and which operates through local communication alone. We investigate the role of neural growth on different machine learning benchmarks and different optimization methods (evolutionary training, online RL, offline RL, and supervised learning). Additionally, we highlight future research directions and opportunities enabled by having self-organization driving the growth of neural networks."}}
{"id": "S2euoDubvHD", "cdate": 1672531200000, "mdate": 1695972428746, "content": {"title": "Learning to Act through Evolution of Neural Diversity in Random Neural Networks", "abstract": ""}}
{"id": "M_oDPhdWM-W", "cdate": 1672531200000, "mdate": 1695972428780, "content": {"title": "A Fully-distributed Shape-aware Neural Controller for Modular Robots", "abstract": "Modular robots are promising for their versatility and large design freedom. Modularity can also enable automatic assembly and reconfiguration, be it autonomous or via external machinery. However, these procedures are error-prone and often result in misassemblings. This, in turn, can cause catastrophic effects on the robot functionality, as the controller deployed in each module is optimized for a different robot shape than the actual one. In this work, we address such shortcoming by proposing a shape-aware modular controller, operating with (1) a self-discovery phase, in which each module controller identifies the shape it is assembled in, followed by (2) a parameter selection phase, where the controller selects its parameters according to the inferred shape. We deploy a self-classifying neural cellular automaton for phase (1), and we leverage evolutionary optimization for implementing a library of controller parameters for phase (2). We test the validity of the proposed method considering voxel-based soft robots, a class of modular soft robots, and the task of locomotion. Our findings confirm the effectiveness of such a controller paradigm, and also show that it can be used to partially overcome unforeseen damages or assembly mistakes."}}
{"id": "LIMo4eE-7E", "cdate": 1672531200000, "mdate": 1695972428749, "content": {"title": "Evolution of an Internal Reward Function for Reinforcement Learning", "abstract": "Artificial neural networks (ANNs) can be trained with reinforcement learning (RL) in simulation to control robots. However, changes in the environment resulting in out-of-distribution situations put learned policies at risk of failure. Since the world can change in unpredictable ways, it might be desirable to be able to continue to update the parameters of the ANNs even after deployment, to prevent failures stemming from a distributional shift. However, in order to optimize with RL, a reward signal is needed. This is usually provided in the simulated environment, but might not necessarily always be available after training. We propose a solution to this problem that involves evolving a function that provides a reward signal to an RL algorithm based only on the inputs and outputs of the policy. We call this approach Evolved Internal Reward Reinforcement Learning (EIR-RL) and test it on various control tasks that have different reward structures and difficulty levels. Our method shows improved training stability and speed of the RL agent under standard circumstances, as well as the ability to train the RL agent under circumstances unseen during the initial optimization phase. We discuss how our results could inform future studies on autonomous, adapting agents."}}
{"id": "AK9wy_-Sm9", "cdate": 1672531200000, "mdate": 1692190677680, "content": {"title": "CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution", "abstract": "Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained multi-modal networks. From the perspective of vulnerability to off-manifold attacks, we therefore argue for the mitigation of modality gaps in CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints are available at https://github.com/matfrei/CLIPMasterPrints."}}
{"id": "4f8HTwAZijl", "cdate": 1672531200000, "mdate": 1692190677787, "content": {"title": "Prompt-Guided Level Generation", "abstract": "Automated generation of complex and diverse environments can be achieved through the use of Procedural Content Generation (PCG) algorithms. However, generating content that is both meaningful and reflective of specific intentions and constraints remains a challenge. Recent advances in Large Language Models (LLMs) have demonstrated their effectiveness in various domains. These models can be fine-tuned and information can be reused to accelerate training for new tasks. Our study presents MarioGPT, a fine-tuned GPT2 model that has been trained to generate tile-based game levels for Super Mario Bros. The results demonstrate that MarioGPT can generate diverse levels and can be text-prompted for controllable level generation, addressing a critical challenge in current PCG techniques."}}
