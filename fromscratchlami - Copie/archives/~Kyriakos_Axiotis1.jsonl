{"id": "2mKhSxNy9h", "cdate": 1676827091462, "mdate": null, "content": {"title": "SubMix: Learning to Mix Graph Sampling Heuristics", "abstract": "Sampling subgraphs for training Graph Neural Networks (GNNs) is receiving much attention from the GNN community. While a variety of methods have been proposed, each method samples the graph according to its own heuristic. However, there has been little work in mixing these heuristics in an end-to-end trainable manner. In this work, we design a generative framework for graph sampling. Our method, SubMix, parameterizes graph sampling as a convex combination of heuristics. We show that a continuous relaxation of the discrete sampling process allows us to efficiently obtain analytical gradients for training the sampling parameters. Our experimental results illustrate the usefulness of learning graph sampling in three scenarios: (1) robust training of GNNs by automatically learning to discard noisy edge sources; (2) improving model performance by trainable and online edge subset selection; and (3) by integrating our framework into state-of-the-art (SOTA) decoupled GNN models, for homogeneous OGBN datasets. Our method raises the SOTA on challenging ogbn-arxiv and ogbn-products, respectively, by over 4 and 0.5 percentage points."}}
{"id": "CKATCkQFcdJ", "cdate": 1663850468481, "mdate": null, "content": {"title": "Gradient Descent Converges Linearly for Logistic Regression on Separable Data", "abstract": "We show that running gradient descent on the logistic regression objective guarantees loss $f(x) \\leq 1.1 \\cdot f(x^*) + \\epsilon$, where the error $\\epsilon$ decays exponentially with the number of iterations. This is in contrast to the common intuition that the absence of strong convexity precludes linear convergence of first-order methods, and highlights the importance of variable learning rates for gradient descent. For separable data, our analysis proves that the error between the predictor returned by gradient descent and the hard SVM predictor decays as $\\mathrm{poly}(1/t)$, exponentially faster than the previously known bound of $O(\\log\\log t / \\log t)$. Our key observation is a property of the logistic loss that we call multiplicative smoothness and is (surprisingly) little-explored: As the loss decreases, the objective becomes (locally) smoother and therefore the learning rate can increase. Our results also extend to sparse logistic regression, where they lead to an exponential improvement of the sparsity-error tradeoff.\n"}}
{"id": "qpuF10UDH6o", "cdate": 1609459200000, "mdate": null, "content": {"title": "Decomposable Submodular Function Minimization via Maximum Flow", "abstract": "This paper bridges discrete and continuous optimization approaches for decomposable submodular function minimization, in both the standard and parametric settings. We provide improved running times for this problem by reducing it to a number of calls to a maximum flow oracle. When each function in the decomposition acts on $O(1)$ elements of the ground set $V$ and is polynomially bounded, our running time is up to polylogarithmic factors equal to that of solving maximum flow in a sparse graph with $O(\\vert V \\vert)$ vertices and polynomial integral capacities. We achieve this by providing a simple iterative method which can optimize to high precision any convex function defined on the submodular base polytope, provided we can efficiently minimize it on the base polytope corresponding to the cut function of a certain graph that we construct. We solve this minimization problem by lifting the solutions of a parametric cut problem, which we obtain via a new efficient combinatorial reduction to maximum flow. This reduction is of independent interest and implies some previously unknown bounds for the parametric minimum $s,t$-cut problem in multiple settings."}}
{"id": "bE74o-WmDpl", "cdate": 1609459200000, "mdate": null, "content": {"title": "Fast and Simple Modular Subset Sum", "abstract": "We revisit the Subset Sum problem over the finite cyclic group \u2124m for some given integer m. A series of recent works has provided near-optimal algorithms for this problem under the Strong Exponential Time Hypothesis. Koiliaris and Xu (SODA'17, TALG'19) gave a deterministic algorithm running in time , which was later improved to O(m log7 m) randomized time by Axiotis et al. (SODA'19). In this work, we present two simple algorithms for the Modular Subset Sum problem running in near-linear time in m, both efficiently implementing Bellman's iteration over \u2124m. The first one is a randomized algorithm running in time O(m log2 m), that is based solely on rolling hash and an elementary data-structure for prefix sums; to illustrate its simplicity we provide a short and efficient implementation of the algorithm in Python. Our second solution is a deterministic algorithm running in time \u2124(m polylog m), that uses dynamic data structures for string manipulation. We further show that the techniques developed in this work can also lead to simple algorithms for the All Pairs Non-Decreasing Paths Problem (APNP) on undirected graphs, matching the near-optimal running time of provided in the recent work of Duan et al. (ICALP'19)."}}
{"id": "StZlGZDrNbq", "cdate": 1609459200000, "mdate": 1646708473424, "content": {"title": "Local Search Algorithms for Rank-Constrained Convex Optimization", "abstract": "We propose greedy and local search algorithms for rank-constrained convex optimization, namely solving $\\underset{\\mathrm{rank}(A)\\leq r^*}{\\min}\\, R(A)$ given a convex function $R:\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}$ and a parameter $r^*$. These algorithms consist of repeating two steps: (a) adding a new rank-1 matrix to $A$ and (b) enforcing the rank constraint on $A$. We refine and improve the theoretical analysis of Shalev-Shwartz et al. (2011), and show that if the rank-restricted condition number of $R$ is $\\kappa$, a solution $A$ with rank $O(r^*\\cdot \\min\\{\\kappa \\log \\frac{R(\\mathbf{0})-R(A^*)}{\\epsilon}, \\kappa^2\\})$ and $R(A) \\leq R(A^*) + \\epsilon$ can be recovered, where $A^*$ is the optimal solution. This significantly generalizes associated results on sparse convex optimization, as well as rank-constrained convex optimization for smooth functions. We then introduce new practical variants of these algorithms that have superior runtime and recover better solutions in practice. We demonstrate the versatility of these methods on a wide range of applications involving matrix completion and robust principal component analysis."}}
{"id": "SeZbWwH4b5", "cdate": 1609459200000, "mdate": 1646708473425, "content": {"title": "Sparse Convex Optimization via Adaptively Regularized Hard Thresholding", "abstract": "The goal of Sparse Convex Optimization is to optimize a convex function f under a sparsity constraint s <= s* \u03b3, where s* is the target number of non-zero entries in a feasible solution (sparsity) and \u03b3 >= 1 is an approximation factor. There has been a lot of work to analyze the sparsity guarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP), Iterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number \u03ba. The best known algorithms guarantee to find an approximate solution of value f(x*)+\u03b5 with the sparsity bound of \u03b3 = O(\u03ba min{log ((f(x0)-f(x*)) / \u03b5), \u03ba}), where x* is the target solution. We present a new Adaptively Regularized Hard Thresholding (ARHT) algorithm that makes significant progress on this problem by bringing the bound down to \u03b3=O(\u03ba), which has been shown to be tight for a general class of algorithms including LASSO, OMP, and IHT. This is achieved without significant sacrifice in the runtime efficiency compared to the fastest known algorithms. We also provide a new analysis of OMP with Replacement (OMPR) for general f, under the condition s > s* \u03ba^2 / 4, which yields compressed sensing bounds under the Restricted Isometry Property (RIP). When compared to other compressed sensing approaches, it has the advantage of providing a strong tradeoff between the RIP condition and the solution sparsity, while working for any general function f that meets the RIP condition."}}
{"id": "SM---wSVbc", "cdate": 1609459200000, "mdate": 1646708473420, "content": {"title": "Decomposable Submodular Function Minimization via Maximum Flow", "abstract": "This paper bridges discrete and continuous optimization approaches for decomposable submodular function minimization, in both the standard and parametric settings. We provide improved running times..."}}
{"id": "HnV-WbvBNWc", "cdate": 1609459200000, "mdate": 1646708473479, "content": {"title": "Local Search Algorithms for Rank-Constrained Convex Optimization", "abstract": "We propose greedy and local search algorithms for rank-constrained convex optimization, namely solving $\\underset{\\mathrm{rank}(A)\\leq r^*}{\\min}\\, R(A)$ given a convex function $R:\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}$ and a parameter $r^*$. These algorithms consist of repeating two steps: (a) adding a new rank-1 matrix to $A$ and (b) enforcing the rank constraint on $A$. We refine and improve the theoretical analysis of Shalev-Shwartz et al. (2011), and show that if the rank-restricted condition number of $R$ is $\\kappa$, a solution $A$ with rank $O(r^*\\cdot \\min\\{\\kappa \\log \\frac{R(\\mathbf{0})-R(A^*)}{\\epsilon}, \\kappa^2\\})$ and $R(A) \\leq R(A^*) + \\epsilon$ can be recovered, where $A^*$ is the optimal solution. This significantly generalizes associated results on sparse convex optimization, as well as rank-constrained convex optimization for smooth functions. We then introduce new practical variants of these algorithms that have superior runtime and recover better solutions in practice. We demonstrate the versatility of these methods on a wide range of applications involving matrix completion and robust principal component analysis."}}
{"id": "BAMZ-bDSVZ5", "cdate": 1609459200000, "mdate": 1646708473424, "content": {"title": "Faster Sparse Minimum Cost Flow by Electrical Flow Localization", "abstract": "We give an $\\widetilde{O}({m^{3/2 - 1/762} \\log (U+W))}$ time algorithm for minimum cost flow with capacities bounded by $U$ and costs bounded by $W$. For sparse graphs with general capacities, this is the first algorithm to improve over the $\\widetilde{O}({m^{3/2} \\log^{O(1)} (U+W)})$ running time obtained by an appropriate instantiation of an interior point method [Daitch-Spielman, 2008].   Our approach is extending the framework put forth in [Gao-Liu-Peng, 2021] for computing the maximum flow in graphs with large capacities and, in particular, demonstrates how to reduce the problem of computing an electrical flow with general demands to the same problem on a sublinear-sized set of vertices -- even if the demand is supported on the entire graph. Along the way, we develop new machinery to assess the importance of the graph's edges at each phase of the interior point method optimization process. This capability relies on establishing a new connections between the electrical flows arising inside that optimization process and vertex distances in the corresponding effective resistance metric."}}
{"id": "tH6_VWZjoq", "cdate": 1601308291898, "mdate": null, "content": {"title": "Local Search Algorithms for Rank-Constrained Convex Optimization", "abstract": "We propose greedy and local search algorithms for rank-constrained convex optimization, namely solving $\\underset{\\mathrm{rank}(A)\\leq r^*}{\\min}\\, R(A)$ given a convex function $R:\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}$ and a parameter $r^*$. These algorithms consist of repeating two steps: (a) adding a new rank-1 matrix to $A$ and (b) enforcing the rank constraint on $A$. We refine and improve the theoretical analysis of Shalev-Shwartz et al. (2011), and show that if the rank-restricted condition number of $R$ is $\\kappa$, a solution $A$ with rank $O(r^*\\cdot \\min\\{\\kappa \\log \\frac{R(\\mathbf{0})-R(A^*)}{\\epsilon}, \\kappa^2\\})$ and $R(A) \\leq R(A^*) + \\epsilon$ can be recovered, where $A^*$ is the optimal solution. This significantly generalizes associated results on sparse convex optimization, as well as rank-constrained convex optimization for smooth functions. We then introduce new practical variants of these algorithms that have superior runtime and recover better solutions in practice. We demonstrate the versatility of these methods on a wide range of applications involving matrix completion and robust principal component analysis.\n"}}
