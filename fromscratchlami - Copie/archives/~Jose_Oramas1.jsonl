{"id": "aHwehiwz6YW", "cdate": 1663850000448, "mdate": null, "content": {"title": "Considering Layerwise Importance in the Lottery Ticket Hypothesis", "abstract": "The recently-introduced Lottery Ticket Hypothesis (LTH) posits that it is possible to extract a sparse trainable subnetwork from a dense network using iterative magnitude pruning.\nBy iteratively training the model, removing the connections with the lowest global weight magnitude and rewinding the remaining connections, sparse networks can be extracted that, when fully trained, reach a similar or better performance than their dense counterpart.\n\nIntuitively, this approach of comparing connection weights globally removes a lot of context about the connection weights and their relations to other connections in their layer as the weight distributions in layers throughout the network often differ significantly.\n\nIn this paper we study a number of different approaches that try to recover some of this layer distributional context by computing an importance value for each connection that is dependent on the weights of the other connections in the same layer. We then generalise the LTH to use weight importances rather than weight magnitudes.\n\nExperiments using these importance metrics on several architectures and datasets,  reveal interesting aspects on the structure and emergence of Lottery tickets. \n\nWe find that given a repeatable training procedure, applying different importance metrics lead to distinct performant lottery tickets with little overlapping connections."}}
{"id": "hDQ-dYA8vB4", "cdate": 1632875459775, "mdate": null, "content": {"title": "Towards Human-Understandable Visual Explanations: Human Imperceptible Cues Can Better Be Removed", "abstract": "Explainable AI (XAI) methods focus on explaining what a neural network has learned - in other words, identifying the features that are the most influential to the prediction. In this paper, we call them \"distinguishing features\". \nHowever, whether a human can make sense of the generated explanation also depends on the perceptibility of these features to humans.\nTo make sure an explanation is human-understandable, we\nargue that the capabilities of humans, constrained by the Human Visual System (HVS) and psychophysics, need to be taken into account. \nWe propose the human perceptibility principle for XAI, stating that, to generate human-understandable explanations, neural networks should be steered towards focusing on human-understandable cues during training.\nWe conduct a case study \nregarding the classification of real vs. fake face images, where many of the distinguishing features picked up by standard neural networks turn out not to be perceptible to humans. By applying the proposed principle, a neural network with human-understandable explanations is trained which, in a survey, is shown to better align with human intuition. This is likely to make the AI more trustworthy and open the door to humans learning from machines. \nIn the case study, we specifically investigate and analyze the behaviour of the human-imperceptible high spatial frequency features in neural networks and XAI methods.\n"}}
{"id": "zlpeOaZHhEK", "cdate": 1609459200000, "mdate": 1630153105720, "content": {"title": "Towards Human-Understandable Visual Explanations: Imperceptible High-frequency Cues Can Better Be Removed", "abstract": "Explainable AI (XAI) methods focus on explaining what a neural network has learned - in other words, identifying the features that are the most influential to the prediction. In this paper, we call them \"distinguishing features\". However, whether a human can make sense of the generated explanation also depends on the perceptibility of these features to humans. To make sure an explanation is human-understandable, we argue that the capabilities of humans, constrained by the Human Visual System (HVS) and psychophysics, need to be taken into account. We propose the {\\em human perceptibility principle for XAI}, stating that, to generate human-understandable explanations, neural networks should be steered towards focusing on human-understandable cues during training. We conduct a case study regarding the classification of real vs. fake face images, where many of the distinguishing features picked up by standard neural networks turn out not to be perceptible to humans. By applying the proposed principle, a neural network with human-understandable explanations is trained which, in a user study, is shown to better align with human intuition. This is likely to make the AI more trustworthy and opens the door to humans learning from machines. In the case study, we specifically investigate and analyze the behaviour of the human-imperceptible high spatial frequency features in neural networks and XAI methods."}}
{"id": "Wc4rgba7eB3", "cdate": 1609459200000, "mdate": 1630153105642, "content": {"title": "MinMaxCAM: Improving object coverage for CAM-basedWeakly Supervised Object Localization", "abstract": "One of the most common problems of weakly supervised object localization is that of inaccurate object coverage. In the context of state-of-the-art methods based on Class Activation Mapping, this is caused either by localization maps which focus, exclusively, on the most discriminative region of the objects of interest or by activations occurring in background regions. To address these two problems, we propose two representation regularization mechanisms: Full Region Regularizationwhich tries to maximize the coverage of the localization map inside the object region, and Common Region Regularization which minimizes the activations occurring in background regions. We evaluate the two regularizations on the ImageNet, CUB-200-2011 and OpenImages-segmentation datasets, and show that the proposed regularizations tackle both problems, outperforming the state-of-the-art by a significant margin."}}
{"id": "GeOIKynj_V", "cdate": 1601308383788, "mdate": null, "content": {"title": "Playing Atari with Capsule Networks: A systematic comparison of CNN and CapsNets-based agents.", "abstract": "In recent years, Capsule Networks (CapsNets) have achieved promising results in tasks in the object recognition task thanks to their invariance characteristics towards pose and lighting.\nThey have been proposed as an alternative to relational insensitive and translation invariant Convolutional Neural Networks (CNN). \nIt has been empirically proven that CapsNets are capable of achieving competitive performance while requiring significantly fewer parameters.\nThis is a desirable characteristic for Deep reinforcement learning which is known to be sample-inefficient during training.\nIn this paper, we conduct a systematic analysis to explore the potential of CapsNets-based agents in the deep reinforcement learning setting.\nMore specifically, we compare the performance of a CNN-based agent with a CapsNets-based agent in a deep Q-network using the Atari suite as the testbed of our analysis. \nTo the best of our knowledge, this work constitutes the first CapsNets based deep reinforcement learning model to learn state-action value functions without the need of task-specific adaptation.\nOur results show that, in this setting, CapsNets-based architectures require 92% fewer parameters compared to their CNN-based counterparts.\nMoreover, despite their smaller size, the CapsNets-based agents provide significant boosts in performance (score), ranging between 10% - 77%.\nThis is supported by our empirical results which shows that CapsNets-based agents outperform the CNN-based agent, in a Double-DQN with Prioritized experience replay setting, in eight out of the nine selected environments."}}
{"id": "_Fgzx3wKz6", "cdate": 1577836800000, "mdate": 1630153105724, "content": {"title": "Unpaired Image-To-Image Shape Translation Across Fashion Data", "abstract": "We address the problem of unpaired geometric image-to-image translation. Rather than transferring the style of an image as a whole, our goal is to translate the geometry of an object while preserving its appearance. Our model is trained without the need for paired images. It performs all steps of the shape transfer within a single model and without additional post-processing stages. Experiments on clothing-based datasets show the effectiveness of the proposed method."}}
{"id": "NjUTGv9-zAK", "cdate": 1577836800000, "mdate": 1630153105720, "content": {"title": "Can the state of relevant neurons in a deep neural networks serve as indicators for detecting adversarial attacks?", "abstract": "We present a method for adversarial attack detection based on the inspection of a sparse set of neurons. We follow the hypothesis that adversarial attacks introduce imperceptible perturbations in the input and that these perturbations change the state of neurons relevant for the concepts modelled by the attacked model. Therefore, monitoring the status of these neurons would enable the detection of adversarial attacks. Focusing on the image classification task, our method identifies neurons that are relevant for the classes predicted by the model. A deeper qualitative inspection of these sparse set of neurons indicates that their state changes in the presence of adversarial samples. Moreover, quantitative results from our empirical evaluation indicate that our method is capable of recognizing adversarial samples, produced by state-of-the-art attack methods, with comparable accuracy to that of state-of-the-art detectors."}}
{"id": "LFB7DE096lX", "cdate": 1577836800000, "mdate": 1630153105637, "content": {"title": "Multiple Exemplars-Based Hallucination for Face Super-Resolution and Editing", "abstract": "Given a really low resolution input image of a face (say $$16\\,{\\times }\\,16$$ or $$8\\,{\\times }\\,8$$ pixels), the goal of this paper is to reconstruct a high-resolution version thereof. This, by itself, is an ill-posed problem, as the high-frequency information is missing in the low-resolution input and needs to be hallucinated, based on prior knowledge about the image content. Rather than relying on a generic face prior, in this paper we explore the use of a set of exemplars, i.e. other high-resolution images of the same person. These guide the neural network as we condition the output on them. Multiple exemplars work better than a single one. To combine the information from multiple exemplars effectively, we introduce a pixel-wise weight generation module. Besides standard face super-resolution, our method allows to perform subtle face editing simply by replacing the exemplars with another set with different facial features. A user study is conducted and shows the super-resolved images can hardly be distinguished from real images on the CelebA dataset. A qualitative comparison indicates our model outperforms methods proposed in the literature on the CelebA and WebFace datasets."}}
{"id": "8LsO6TrKf_4", "cdate": 1577836800000, "mdate": 1630153105607, "content": {"title": "In Defense of LSTMs for Addressing Multiple Instance Learning Problems", "abstract": "LSTMs have a proven track record in analyzing sequential data. But what about unordered instance bags, as found under a Multiple Instance Learning (MIL) setting? While not often used for this, we show LSTMs excell under this setting too. In addition, we show that LSTMs are capable of indirectly capturing instance-level information using only bag-level annotations. Thus, they can be used to learn instance-level models in a weakly supervised manner. Our empirical evaluation on both simplified (MNIST) and realistic (Lookbook and Histopathology) datasets shows that LSTMs are competitive with or even surpass state-of-the-art methods specially designed for handling specific MIL problems. Moreover, we show that their performance on instance-level prediction is close to that of fully-supervised methods."}}
{"id": "0hRIGKW1TvJ", "cdate": 1577836800000, "mdate": 1630153105721, "content": {"title": "Multiple Exemplars-based Hallucinationfor Face Super-resolution and Editing", "abstract": "Given a really low-resolution input image of a face (say 16x16 or 8x8 pixels), the goal of this paper is to reconstruct a high-resolution version thereof. This, by itself, is an ill-posed problem, as the high-frequency information is missing in the low-resolution input and needs to be hallucinated, based on prior knowledge about the image content. Rather than relying on a generic face prior, in this paper, we explore the use of a set of exemplars, i.e. other high-resolution images of the same person. These guide the neural network as we condition the output on them. Multiple exemplars work better than a single one. To combine the information from multiple exemplars effectively, we introduce a pixel-wise weight generation module. Besides standard face super-resolution, our method allows to perform subtle face editing simply by replacing the exemplars with another set with different facial features. A user study is conducted and shows the super-resolved images can hardly be distinguished from real images on the CelebA dataset. A qualitative comparison indicates our model outperforms methods proposed in the literature on the CelebA and WebFace dataset."}}
