{"id": "DWDPhB6Hi7k", "cdate": 1663850061566, "mdate": null, "content": {"title": "A Representation Bottleneck of Bayesian Neural Networks", "abstract": "Unlike standard deep neural networks (DNNs), Bayesian neural networks (BNNs) formulate network weights as probability distributions, which results in distinctive representation capacities from standard DNNs. In this paper, we explore the representation bottleneck of BNNs from the perspective of conceptual representations. It is proven that the logic of a neural network can be faithfully mimicked by a specific sparse causal graph, where each causal pattern can be considered as a concept encoded by the neural network. Then, we formally define the complexity of concepts, and prove that compared to standard DNNs, it is more difficult for BNNs to encode complex concepts. Extensive experiments verify our theoretical proofs. The code will be released when the paper is accepted."}}
{"id": "-0jbdOhFn4g", "cdate": 1663849990857, "mdate": null, "content": {"title": "Is the Deep Model Representation Sparse and Symbolic with Causal Patterns?", "abstract": "This paper aims to show that the inference logic of a deep model can be faithfully approximated as a sparse, symbolic causal graph. Such a causal graph potentially bridges the gap between connectionism and symbolism. To this end, the faithfulness of the causal graph is theoretically guaranteed, because we show that the causal graph can well mimic the model's output on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), which explains the logical relationship between interactive concepts encoded by the deep model, without losing much explanation accuracy. The code will be released when the paper is accepted."}}
{"id": "WevBjPK4V3j", "cdate": 1663849951573, "mdate": null, "content": {"title": "Evaluation of Attribution Explanations without Ground Truth", "abstract": "This paper proposes a metric to evaluate the objectiveness of explanation methods of neural networks, i.e., the accuracy of the estimated importance/attribution/saliency values of input variables. This is crucial for the development of explainable AI, but it also presents significant challenges. The core challenge is that people usually cannot obtain the ground-truth value of the attribution of each input variable. Thus, we design a metric to evaluate the objectiveness of the attribution map without ground truth. Our metric is used to evaluate eight benchmark methods of attribution explanations, which provides new insights into attribution methods. We will release the code when the paper is accepted."}}
{"id": "0cm8HroIxJV", "cdate": 1663849947281, "mdate": null, "content": {"title": "Explaining Representation Bottlenecks of Convolutional Decoder Networks", "abstract": "In this paper, we prove representation bottlenecks of a cascaded convolutional decoder network, considering the capacity of representing different frequency components of an input sample. We conduct the discrete Fourier transform on each channel of the feature map in an intermediate layer of the decoder network. Then, we introduce the rule of the forward propagation of such intermediate-layer spectrum maps, which is equivalent to the forward propagation of feature maps through a convolutional layer. Based on this, we find that each frequency component in the spectrum map is forward propagated independently with other frequency components. Furthermore, we prove two bottlenecks in representing feature spectrums. First, we prove that the convolution operation, the zero-padding operation, and a set of other settings all make a convolutional decoder network more likely to weaken high-frequency components. Second, we prove that the upsampling operation generates a feature spectrum, in which strong signals repetitively appears at certain frequencies. We will release all codes when this paper is accepted."}}
{"id": "lMPJP3nRGtJ", "cdate": 1663849947161, "mdate": null, "content": {"title": "Batch Normalization Is Blind to the First and Second Derivatives of the Loss w.r.t. Features", "abstract": "We prove that when we do the Taylor series expansion of the loss function, the BN operation will block the influence of the first-order term and most influence of the second-order term of the loss. This is a potential defect of the BN operation. We also find that such a problem is caused by the standardization phase of the BN operation. We believe that the proof of the blindness of a deep model is of significant value to avoiding systemic collapses of a deep model, although such a blindness does not always makes significant damages in all applications. Experiments show that the BN operation significantly affects feature representations in specific tasks."}}
{"id": "Vsh8gspKmuu", "cdate": 1663849944659, "mdate": null, "content": {"title": "Why Adversarial Training of ReLU Networks Is Difficult?", "abstract": "This paper mathematically derives an analytic solution of the adversarial perturbation on a ReLU network, and theoretically explains the difficulty of adversarial training. Specifically, we formulate the dynamics of the adversarial perturbation generated by the multi-step attack, which shows that the adversarial perturbation tends to strengthen eigenvectors corresponding to a few top-ranked eigenvalues of the Hessian matrix of the loss w.r.t. the input. We also prove that adversarial training tends to strengthen the influence of unconfident input samples with large gradient norms in an exponential manner. Besides, we find that adversarial training strengthens the influence of the Hessian matrix of the loss w.r.t. network parameters, which makes the adversarial training more likely to oscillate along directions of a few samples, and boosts the difficulty of adversarial training. Crucially, our proofs provide a unified explanation for previous findings in understanding adversarial training."}}
{"id": "YV8tP7bW6Kt", "cdate": 1663849944064, "mdate": null, "content": {"title": "Can We Faithfully Represent Absence States to Compute Shapley Values on a DNN?", "abstract": "Masking some input variables of a deep neural network (DNN) and computing output changes on the masked input sample represent a typical way to compute attributions of input variables in the sample. People usually mask an input variable using its baseline value. However, there is no theory to examine whether baseline value faithfully represents the absence of an input variable, i.e., removing all signals from the input variable. Fortunately, recent studies (Ren et al., 2023a; Deng et al., 2022a) show that the inference score of a DNN can be strictly disentangled into a set of causal patterns (or concepts) encoded by the DNN. Therefore, we propose to use causal patterns to examine the faithfulness of baseline values. More crucially, it is proven that causal patterns can be explained as the elementary rationale of the Shapley value. Furthermore, we propose a method to learn optimal baseline values, and experimental results have demonstrated its effectiveness."}}
{"id": "FLMvYXMucWk", "cdate": 1663849860563, "mdate": null, "content": {"title": "Temporary feature collapse phenomenon in early learning of MLPs", "abstract": "In this paper, we focus on a typical two-phase phenomenon in the learning of multi-layer perceptrons (MLPs). We discover and explain the reason for the feature collapse phenomenon in the first phase, i.e., the diversity of features over different samples keeps decreasing in the first phase, until samples of different categories share almost the same feature, which hurts the optimization of MLPs. We explain such a phenomenon in terms of the learning dynamics of MLPs. Furthermore, we theoretically analyze the reason why four typical operations can alleviate the feature collapse. The code has been attached with the submission."}}
{"id": "pG9RSmBrY3", "cdate": 1663849853084, "mdate": null, "content": {"title": "Formulating and Proving the Trend of DNNs Learning Simple Concepts", "abstract": "This paper theoretically explains the intuition that simple concepts are more likely to be learned by deep neural networks (DNNs) than complex concepts. Beyond empirical studies, our research first specifies an exact definition of the complexity of the concept that boosts the learning difficulty. Specifically, it is proven that the inference logic of a neural network can be represented as a causal graph. In this way, causal patterns in the causal graph can be used to formulate interactive concepts learned by the neural network. Based on such formulation, we explain the reason why simple interactive concepts in the data are more likely to be learned than complex interactive concepts. More crucially, we discover that our research provides a new perspective to explain previous understandings of the conceptual complexity. The code will be released when the paper is accepted."}}
{"id": "ZV3PZXrRDQ", "cdate": 1632875564167, "mdate": null, "content": {"title": "Towards a Game-Theoretic View of Baseline Values in the Shapley Value", "abstract": "This paper aims to formulate the problem of estimating optimal baseline values, which are used to compute the Shapley value in game theory. In the computation of Shapley values, people usually set an input variable to its baseline value to represent the absence of this variable. However, there are no studies on how to ensure that baseline values represent the absence states of variables without bringing in additional information, which ensures the trustworthiness of the Shapley value. To this end, previous studies usually determine baseline values in an empirical manner, which are not reliable. Therefore, we revisit the feature representation of a deep model in game theory, and formulate the absence state of an input variable. From the perspective of game-theoretic interaction, we learn the optimal baseline value of each input variable. Experimental results have demonstrated the effectiveness of our method. The code will be released when the paper is accepted."}}
