{"id": "thirVlDJ2IL", "cdate": 1652737565991, "mdate": null, "content": {"title": "A Fourier Approach to Mixture Learning", "abstract": "We revisit the problem of learning mixtures of spherical Gaussians. Given samples from a mixture $\\frac{1}{k}\\sum_{j=1}^{k}\\mathcal{N}(\\mu_j, I_d)$, the goal is to estimate the means $\\mu_1, \\mu_2, \\ldots, \\mu_k \\in \\mathbb{R}^d$ up to a small error. The hardness of this learning problem can be measured by the \\emph{separation} $\\Delta$ defined as the minimum distance between all pairs of means. Regev and Vijayaraghavan (2017) showed that with $\\Delta = \\Omega(\\sqrt{\\log k})$ separation, the means can be learned using $\\mathrm{poly}(k, d)$ samples, whereas super-polynomially many samples are required if $\\Delta = o(\\sqrt{\\log k})$ and $d = \\Omega(\\log k)$. This leaves open the low-dimensional regime where $d = o(\\log k)$.\n    \nIn this work, we give an algorithm that efficiently learns the means in $d = O(\\log k/\\log\\log k)$ dimensions under separation $d/\\sqrt{\\log k}$ (modulo doubly logarithmic factors). This separation is strictly smaller than $\\sqrt{\\log k}$, and is also shown to be necessary. Along with the results of Regev and Vijayaraghavan (2017), our work almost pins down the critical separation threshold at which efficient parameter learning becomes possible for spherical Gaussian mixtures. More generally, our algorithm runs in time $\\mathrm{poly}(k)\\cdot f(d, \\Delta, \\epsilon)$, and is thus fixed-parameter tractable in parameters $d$, $\\Delta$ and $\\epsilon$.\n    \nOur approach is based on estimating the Fourier transform of the mixture at carefully chosen frequencies, and both the algorithm and its analysis are simple and elementary. Our positive results can be easily extended to learning mixtures of non-Gaussian distributions, under a mild condition on the Fourier spectrum of the distribution."}}
{"id": "H4ph4BG-Hfs", "cdate": 1635535329337, "mdate": null, "content": {"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is the\nquadratic dependency (mainly in terms of memory) on the sequence length due to\ntheir full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse\nattention mechanism that reduces this quadratic dependency to linear. We show\nthat BIGBIRD is a universal approximator of sequence functions and is Turing\ncomplete, thereby preserving these properties of the quadratic, full attention model.\nAlong the way, our theoretical analysis reveals some of the benefits of having\nO(1) global tokens (such as CLS), that attend to the entire sequence as part of the\nsparse attention mechanism. The proposed sparse attention can handle sequences\nof length up to 8x of what was previously possible using similar hardware. As\na consequence of the capability to handle longer context, BIGBIRD drastically\nimproves performance on various NLP tasks such as question answering and\nsummarization. We also propose novel applications to genomics data."}}
{"id": "hVnM-ni5o5nVQ", "cdate": 1621630328932, "mdate": null, "content": {"title": "Contextual Recommendations and Low-Regret Cutting-Plane Algorithms", "abstract": "We consider the following variant of contextual linear bandits motivated by routing applications in navigational engines  and recommendation systems. We wish to learn a hidden $d$-dimensional value $w^*$. Every round, we are presented with a subset $\\mathcal{X}_t \\subseteq \\mathbb{R}^d$ of possible actions. If we choose (i.e. recommend to the user) action $x_t$, we obtain utility $\\langle x_t, w^* \\rangle$ but only learn the identity of the best action $\\arg\\max_{x \\in \\X_t} \\langle x, w^* \\rangle$.\n\nWe design algorithms for this problem which achieve regret $O(d\\log T)$ and $\\exp(O(d \\log d))$. To accomplish this, we design novel cutting-plane algorithms with low \u201cregret\u201d -- the total distance between the true point $w^*$ and the hyperplanes the separation oracle returns. \n\nWe also consider the variant where we are allowed to provide a list of several recommendations. In this variant, we give an algorithm with $O(d^2 \\log d)$ regret and list size $\\poly(d)$. Finally, we construct nearly tight algorithms for a weaker variant of this problem where the learner only learns the identity of an action that is better than the recommendation. Our results rely on new algorithmic techniques in convex geometry (including a variant of Steiner\u2019s formula for the centroid of a convex set) which may be of independent interest. "}}
{"id": "45GfBQYtYlp", "cdate": 1621630328932, "mdate": null, "content": {"title": "Contextual Recommendations and Low-Regret Cutting-Plane Algorithms", "abstract": "We consider the following variant of contextual linear bandits motivated by routing applications in navigational engines  and recommendation systems. We wish to learn a hidden $d$-dimensional value $w^*$. Every round, we are presented with a subset $\\mathcal{X}_t \\subseteq \\mathbb{R}^d$ of possible actions. If we choose (i.e. recommend to the user) action $x_t$, we obtain utility $\\langle x_t, w^* \\rangle$ but only learn the identity of the best action $\\arg\\max_{x \\in \\X_t} \\langle x, w^* \\rangle$.\n\nWe design algorithms for this problem which achieve regret $O(d\\log T)$ and $\\exp(O(d \\log d))$. To accomplish this, we design novel cutting-plane algorithms with low \u201cregret\u201d -- the total distance between the true point $w^*$ and the hyperplanes the separation oracle returns. \n\nWe also consider the variant where we are allowed to provide a list of several recommendations. In this variant, we give an algorithm with $O(d^2 \\log d)$ regret and list size $\\poly(d)$. Finally, we construct nearly tight algorithms for a weaker variant of this problem where the learner only learns the identity of an action that is better than the recommendation. Our results rely on new algorithmic techniques in convex geometry (including a variant of Steiner\u2019s formula for the centroid of a convex set) which may be of independent interest. "}}
{"id": "XK4eVsG2LKw", "cdate": 1621630272561, "mdate": null, "content": {"title": "Margin-Independent Online Multiclass Learning via Convex Geometry", "abstract": "We consider the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, we minimize the total distance from each query to the region corresponding to its assigned label. When the true labels are determined via a nearest neighbor partition -- i.e. the label of a point is given by which of $k$ centers it is closest to in Euclidean distance -- we show that one can achieve a loss that is independent of the total number of queries. We complement this result by showing that learning general convex sets requires an almost linear loss per query. Our results build off of regret guarantees for the problem of contextual search. In addition, we develop a novel reduction technique from multiclass classification to binary classification which may be of independent interest. "}}
{"id": "zdP3h_BcZ1C", "cdate": 1600184832558, "mdate": null, "content": {"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."}}
{"id": "dPF5kB1cjYT", "cdate": 1577836800000, "mdate": null, "content": {"title": "Chasing Convex Bodies with Linear Competitive Ratio", "abstract": "We study the problem of chasing convex bodies online: given a sequence of convex bodies Kt \u2286 \u211dd the algorithm must respond with points xt \u03f5 Kt in an on-line fashion (i.e., xt is chosen before Kt+1 is revealed). The objective is to minimize the total distance between successive points in this sequence. Recently, Bubeck et al. (STOC 2019) gave a 2O(d)-competitive algorithm for this problem. We give an algorithm that is -competitive for any sequence of length T."}}
{"id": "95kNOn3s29", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sticky Brownian Rounding and its Applications to Constraint Satisfaction Problems", "abstract": "Semi-definite programming is a powerful tool in the design and analysis of approximation algorithms for combinatorial optimization problems. In particular, the random hyperplane rounding method of Goemans and Williamson [23] has been extensively studied for more than two decades, resulting in various extensions to the original technique and beautiful algorithms for a wide range of applications. Despite the fact that this approach yields tight approximation guarantees for some problems, e.g., Max-Cut, for many others, e.g., Max-SAT and Max-DiCut, the tight approximation ratio is still unknown. One of the main reasons for this is the fact that very few techniques for rounding semi-definite relaxations are known. In this work, we present a new general and simple method for rounding semi-definite programs, based on Brownian motion. Our approach is inspired by recent results in algorithmic discrepancy theory. We develop and present tools for analyzing our new rounding algorithms, utilizing mathematical machinery from the theory of Brownian motion, complex analysis, and partial differential equations. Focusing on constraint satisfaction problems, we apply our method to several classical problems, including Max-Cut, Max-2SAT, and Max-DiCut, and derive new algorithms that are competitive with the best known results. To illustrate the versatility and general applicability of our approach, we give new approximation algorithms for the Max-Cut problem with side constraints that crucially utilizes measure concentration results for the Sticky Brownian Motion, a feature missing from hyperplane rounding and its generalizations."}}
{"id": "RMnZrib0FKL", "cdate": 1546300800000, "mdate": null, "content": {"title": "Chasing Convex Bodies with Linear Competitive Ratio", "abstract": "We study the problem of chasing convex bodies online: given a sequence of convex bodies $K_t\\subseteq \\mathbb{R}^d$ the algorithm must respond with points $x_t\\in K_t$ in an online fashion (i.e., $x_t$ is chosen before $K_{t+1}$ is revealed). The objective is to minimize the sum of distances between successive points in this sequence. Bubeck et al. (STOC 2019) gave a $2^{O(d)}$-competitive algorithm for this problem. We give an algorithm that is $O(\\min(d, \\sqrt{d \\log T}))$-competitive for any sequence of length $T$."}}
{"id": "JlAjczYMz42", "cdate": 1546300800000, "mdate": null, "content": {"title": "Stochastic Online Metric Matching", "abstract": "We study the minimum-cost metric perfect matching problem under online i.i.d arrivals. We are given a fixed metric with a server at each of the points, and then requests arrive online, each drawn independently from a known probability distribution over the points. Each request has to be matched to a free server, with cost equal to the distance. The goal is to minimize the expected total cost of the matching. Such stochastic arrival models have been widely studied for the maximization variants of the online matching problem; however, the only known result for the minimization problem is a tight O(log n)-competitiveness for the random-order arrival model. This is in contrast with the adversarial model, where an optimal competitive ratio of O(log n) has long been conjectured and remains a tantalizing open question. In this paper, we show that the i.i.d model admits substantially better algorithms: our main result is an O((log log log n)^2)-competitive algorithm in this model, implying a strict separation between the i.i.d model and the adversarial and random order models. Along the way we give a 9-competitive algorithm for the line and tree metrics - the first O(1)-competitive algorithm for any non-trivial arrival model for these much-studied metrics."}}
