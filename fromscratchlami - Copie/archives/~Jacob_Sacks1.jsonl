{"id": "8ktEdb5NHEh", "cdate": 1655376342588, "mdate": null, "content": {"title": "Learning Sampling Distributions for Model Predictive Control", "abstract": "Sampling-based methods have become a cornerstone of contemporary approaches to Model Predictive Control (MPC), as they make no restrictions on the differentiability of the dynamics or cost function and are straightforward to parallelize. However, their efficacy is highly dependent on the quality of the sampling distribution itself, which is often assumed to be simple, like a Gaussian. This restriction can result in samples which are far from optimal, leading to poor performance. Recent work has explored improving the performance of MPC by sampling in a learned latent space of controls. However, these methods ultimately perform all MPC parameter updates and warm-starting between time steps in the control space. This requires us to rely on a number of heuristics for generating samples and updating the distribution and may lead to sub-optimal performance. Instead, we propose to carry out all operations in the latent space, allowing us to take full advantage of the learned distribution. Specifically, we frame the learning problem as bi-level optimization and show how to train the controller with backpropagation-through-time. By using a normalizing flow parameterization of the distribution, we can leverage its tractable density to avoid requiring differentiability of the dynamics and cost function. Finally, we evaluate the proposed approach on simulated robotics tasks and demonstrate its ability to surpass the performance of prior methods and scale better with a reduced number of samples."}}
{"id": "Q6SO0-YJ3xr", "cdate": 1640995200000, "mdate": 1672864817165, "content": {"title": "Learning to Optimize in Model Predictive Control", "abstract": ""}}
{"id": "1bpGSdbpMW", "cdate": 1640995200000, "mdate": 1672864817147, "content": {"title": "Learning Sampling Distributions for Model Predictive Control", "abstract": ""}}
{"id": "WexgA3M5OY", "cdate": 1546300800000, "mdate": 1672864817147, "content": {"title": "An Online Learning Approach to Model Predictive Control", "abstract": ""}}
{"id": "S1-A3IWdWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Differentiable MPC for End-to-end Planning and Control", "abstract": "We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable."}}
