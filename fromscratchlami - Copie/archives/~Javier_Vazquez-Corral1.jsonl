{"id": "AQ-KflDHiRv", "cdate": 1672531200000, "mdate": 1676592028013, "content": {"title": "Perceptual Image Enhancement for Smartphone Real-Time Applications", "abstract": "Recent advances in camera designs and imaging pipelines allow us to capture high-quality images using smartphones. However, due to the small size and lens limitations of the smartphone cameras, we commonly find artifacts or degradation in the processed images. The most common unpleasant effects are noise artifacts, diffraction artifacts, blur, and HDR overexposure. Deep learning methods for image restoration can successfully remove these artifacts. However, most approaches are not suitable for real-time applications on mobile devices due to their heavy computation and memory requirements.In this paper, we propose LPIENet, a lightweight network for perceptual image enhancement, with the focus on deploying it on smartphones. Our experiments show that, with much fewer parameters and operations, our model can deal with the mentioned artifacts and achieve competitive performance compared with state-of-the-art methods on standard benchmarks. Moreover, to prove the efficiency and reliability of our approach, we deployed the model directly on commercial smartphones and evaluated its performance. Our model can process 2K resolution images under 1 second in mid-level commercial smartphones."}}
{"id": "a8iVRUcXaWF", "cdate": 1640995200000, "mdate": 1668179216836, "content": {"title": "Perceptual Image Enhancement for Smartphone Real-Time Applications", "abstract": ""}}
{"id": "JrJRqTgSgS", "cdate": 1640995200000, "mdate": 1668179217034, "content": {"title": "Image Quality Evaluation in Professional HDR/WCG Production Questions the Need for HDR Metrics", "abstract": ""}}
{"id": "iprplqdEEQ", "cdate": 1609459200000, "mdate": 1668179216818, "content": {"title": "Vision Models for Wide Color Gamut Imaging in Cinema", "abstract": ""}}
{"id": "vKiU5xqHLq", "cdate": 1582551108860, "mdate": null, "content": {"title": "Color Constancy by Category Correlation", "abstract": "Finding color representations which are stable to illuminant changes is still an open problem in computer vision.\nUntil now most approaches have been based on physical constraints or statistical assumptions derived from the scene, while very little attention has been paid to the effects that selected illuminants have on the final color image representation.\nThe novelty of this work is to propose perceptual constraints that are computed on the corrected images. We define the category hypothesis, which weights the set of feasible illuminants according to their ability to map the corrected image onto specific colors. \nHere we choose these colors as the universal color categories related to basic linguistic terms which have been psychophysically measured. These color categories encode natural color statistics, and their relevance across different cultures is indicated by the fact that they have received a common color name. From this category hypothesis we propose a fast implementation that allows the sampling of a large set of illuminants. \nExperiments prove that our method rivals current state-of-art performance without the need for training algorithmic parameters. Additionally, the method can be used as a framework to insert top-down information from other sources, thus opening further research directions in solving for color constancy.\n"}}
{"id": "mSau3NTZHn", "cdate": 1582550992040, "mdate": null, "content": {"title": "Color Stabilization Along Time and Across Shots of the Same Scene, for One or Several Cameras of Unknown Specifications", "abstract": "We propose a method for color stabilization of shots\nof the same scene, taken under the same illumination, where one\nimage is chosen as reference and one or several other images\nare modified so that their colors match those of the reference.\nWe make use of two crucial but often overlooked observations:\nfirstly, that the core of the color correction chain in a digital\ncamera is simply a multiplication by a 3x3 matrix; secondly,\nthat to color-match a source image to a reference image we don\u2019t\nneed to compute their two color correction matrices, it\u2019s enough\nto compute the operation that transforms one matrix into the\nother. This operation is a 3x3 matrix as well, which we call H.\nOnce we have H, we just multiply by it each pixel value of the\nsource and obtain an image which matches in color the reference.\nTo compute H we only require a set of pixel correspondences,\nwe don\u2019t need any information about the cameras used, neither\nmodels nor specifications or parameter values. We propose an\nimplementation of our framework which is very simple and fast,\nand show how it can be successfully employed in a number\nof situations, comparing favourably with the state of the art.\nThere is a wide range of applications of our technique, both for\namateur and professional photography and video: color matching\nfor multi-camera TV broadcasts, color matching for 3D cinema,\ncolor stabilization for amateur video, etc."}}
{"id": "3xoNau0xcC", "cdate": 1582550815783, "mdate": null, "content": {"title": "Enhancing spatio-chromatic representation with more-than-three color coding for image description", "abstract": "Extraction of spatio-chromatic features from color images is usually performed independently on each\ncolor channel. Usual 3D color spaces, such as RGB, present a high inter-channel correlation for natural\nimages. This correlation can be reduced using color-opponent representations, but the spatial structure\nof regions with small color differences is not fully captured in two generic Red-Green and Blue-Yellow\nchannels. To overcome these problems, we propose a new color coding that is adapted to the specific content of each image. Our proposal is based on two steps: (a) setting the number of channels to the number\nof distinctive colors we find in each image (avoiding the problem of channel correlation), and (b) building a channel representation that maximizes contrast differences within each color channel (avoiding the\nproblem of low local contrast). We call this approach more-than-three color coding (MTT) to enhance the\nfact that the number of channels is adapted to the image content. The higher color complexity an image\nhas, the more channels can be used to represent it. Here we select distinctive colors as the most predominant in the image, which we call color pivots, and we build the new color coding using these color pivots\nas a basis. To evaluate the proposed approach we measure its efficiency in an image categorization task.\nWe show how a generic descriptor improves its performance at the description level when applied on the\nMTT coding"}}
{"id": "sB9XAHZWo", "cdate": 1582550639829, "mdate": null, "content": {"title": "Angular-based pre-processing for image denoising", "abstract": "There is not a large research on how to use color information for improving results in image denoising. Currently,\nmost of the methods modify the color space from sRGB to an opponent-like one as better results are obtained, but out of\nthis conversion, color is mostly ignored in the image denoising pipelines. In this work, we propose a color decomposition to\npre-process an image before applying a typical denoising. Our decomposition consists in obtaining a set of images in the\nspherical coordinate system, each of them with the origin of the spherical transformation in a different color value. These color values, that we call color centers, are defined so as to be far away from the dominant colors of the image. Once in the spherical coordinate system, we perform a mild denoising operation with some state-of-the-art method in the angular components. Then, we convert these images back to sRGB, and we merge them depending on the distance between the color of each pixel and the color centers. Finally, we denoise the pre-processed image with the same state-of-the-art method used in our pre-processing. Experiments show that our method outperforms the results of directly applying the denoising method on the input image for different state-of-the-art denoising methods."}}
{"id": "OXOO21IDER", "cdate": 1582550547928, "mdate": null, "content": {"title": "Spatial gamut mapping among non-inclusive gamuts", "abstract": "Gamut mapping transforms the color gamut of an image to that of a target device.\nTwo cases are usually considered: gamut reduction (target gamut smaller than\nsource gamut), and gamut extension (target gamut larger than the source gamut).\nLess attention is devoted to the more general case, when neither gamut is fully\nincluded in the other. In this work we unify and expand two recent methods\nfor gamut extension and reduction, so as to simultaneously perform both forms\nof gamut mapping in different regions of the same image without introducing\ncolor artifacts or halos. We demonstrate the usefulness of this approach for the\ntraditional gamut mapping problem, and also how the proposed method can be\nused to adapt the color palette of an image so that it is closer to that of a given\nreference image. Results are compared with the state-of-the-art and validated\nthrough user tests and objective metrics"}}
{"id": "ucrTvGG6AM", "cdate": 1582545983230, "mdate": null, "content": {"title": "A fast image dehazing method that does not introduce color artifacts", "abstract": "We propose a method for color dehazing with four main characteristics: it does not introduce color artifacts, it does not depend on inverting any physical equation, it is based on models of visual perception, and it is fast, potentially real-time. Our method converts the original input image to the HSV color space and works in the saturation and value domains by: i) reducing the value component via a global constrained histogram flattening, ii) modifying the saturation component in consistency with the previous reduced value, and iii) performing a local contrast enhancement in the value component. Results show that our method competes with the state-of-the-art when dealing with standard hazy images, and outperforms it when dealing with challenging haze cases. Furthermore, our method is able to dehaze a FullHD image on a GPU in 90 milliseconds."}}
