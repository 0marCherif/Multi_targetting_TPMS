{"id": "YTMwcJ5Qa26", "cdate": 1674940179787, "mdate": 1674940179787, "content": {"title": "Rethinking Transfer Learning for Medical Image Classification", "abstract": "Transfer learning (TL) from pretrained deep models is a standard practice in modern medical image classification (MIC). However, what levels of features to be reused are problem-dependent, and uniformly finetuning all layers of pretrained models may be suboptimal. This insight has partly motivated the recent differential TL strategies, such as TransFusion (TF) and layer-wise finetuning (LWFT), which treat the layers in the pretrained models differentially. In this paper, we add one more strategy into this family, called TruncatedTL, which reuses and finetunes appropriate bottom layers and directly discards the remaining layers. This yields not only superior MIC performance but also compact models for efficient inference, compared to other differential TL methods. We validate the performance and model efficiency of TruncatedTL on three MIC tasks covering both 2D and 3D images. For example, on the BIMCV COVID-19 classification dataset, we obtain improved performance with around 1/4 model size and 2/3 inference time compared to the standard full TL model. "}}
{"id": "D5OD6nBAwvh", "cdate": 1674939951956, "mdate": 1674939951956, "content": {"title": "Performance of a Chest Radiograph AI Diagnostic Tool for COVID-19: A Prospective Observational Study", "abstract": "To conduct a prospective observational study across 12 U.S. hospitals to evaluate real-time performance of an interpretable\nartificial intelligence (AI) model to detect COVID-19 on chest radiographs."}}
{"id": "cZO6QddzUce", "cdate": 1674939826017, "mdate": 1674939826017, "content": {"title": "Early Stopping for Deep Image Prior", "abstract": "Deep image prior (DIP) and its variants have showed remarkable potential for solving inverse problems in computer vision, without any extra training data. Practical DIP models are often substantially overparameterized. During the fitting process, these models learn mostly the desired visual content first, and then pick up the potential modeling and observational noise, i.e., overfitting. Thus, the practicality of DIP hinges on good early stopping (ES) that captures the transition period. In this regard, the majority of DIP works for vision tasks only demonstrates the potential of the models\u2014reporting the peak performance against the ground truth, but provides no clue about how\nto operationally obtain near-peak performance without access to the groundtruth. In this paper, we set to break this practicality barrier of DIP, and propose an efficient ES strategy which consistently detects near-peak performance across several vision tasks and DIP variants. Based on a simple measure of dispersion of consecutive DIP reconstructions, our ES method not only outpaces the existing ones\u2014which only work in very narrow domains, but also remains effective when combined with a number of methods that try to mitigate the overfitting."}}
{"id": "khjMKZv6Kue", "cdate": 1674939631785, "mdate": 1674939631785, "content": {"title": "Blind image deblurring with unknown kernel size and substantial noise", "abstract": "Blind image deblurring (BID) has been extensively studied in computer vision and adjacent fields. Modern methods for BID can be grouped into two categories: single-instance methods that deal with individual instances using statistical inference and numerical optimization, and data-driven methods that train deep-learning models to deblur future instances directly. Data-driven methods can be free from the difficulty in deriving accurate blur models, but are fundamentally limited by the diversity and quality of the training data\u2014collecting sufficiently expressive and realistic training data is a standing challenge. In this paper, we focus on single-instance methods that remain competitive and indispensable. However, most such methods do not prescribe how to deal with unknown kernel size and substantial noise, precluding practical deployment. Indeed, we show that several state-of-the-art (SOTA) single-instance methods are unstable when the kernel size is overspecified, and/or the noise level is high. On the positive side, we propose a practical BID method that is stable against both, the\nfirst of its kind. Our method builds on the recent ideas of solving inverse problems by integrating the physical models and structured deep neural networks, without extra training data. We introduce several crucial modifications to achieve the desired stability. Extensive empirical tests on standard synthetic datasets, as well as real-world NTIRE2020 and RealBlur datasets, show the superior effectiveness and practicality of our BID method compared to SOTA single-instance as well as data-driven methods. "}}
{"id": "4OCNf_mpXOl", "cdate": 1674939443807, "mdate": 1674939443807, "content": {"title": "Rethink Autoencoders: Robust Manifold Learning", "abstract": "PCA can be made robust to data corruption, i.e., robust PCA. What about the deep autoencoder, as a nonlinear generalization of PCA? This further motivates us to \u201creinvent\u201d a factorization-based PCA as well as its nonlinear generalization. Focusing on sparse corruption, we model the sparsity structure explicitly using the L1 norm to obtain various robust formulations. For linear data, robust factorization performs comparably to the seminal convex formulation of robust PCA, whereas robust autoencoders provably fail. For nonlinear data, we perform a careful experimental evaluation of robust deep autoencoders and robust nonlinear factorization for corruption removal on natural\nimages. Both schemes can remove a considerable level of sparse corruption and effectively reconstruct the clean images."}}
{"id": "ziEEIcgZ6i", "cdate": 1674939279172, "mdate": 1674939279172, "content": {"title": "Self-Validation: Early Stopping for Single-Instance Deep Generative Priors", "abstract": "Recent works have shown the surprising effectiveness of deep generative models in solving numerous image reconstruction (IR) tasks, even without training data. We call these models, such as deep image prior and deep decoder, collectively as single-instance deep generative priors (SIDGPs). The successes, however, often hinge on appropriate early stopping (ES), which by far has largely been handled in an ad-hoc manner. In this paper, we propose the first principled method for ES when applying SIDGPs to IR, taking advantage of the typical bell trend of the reconstruction quality. In particular, our method is based on collaborative training and self-validation: the primal reconstruction\nprocess is monitored by a deep autoencoder, which is trained online with the historic reconstructed images and used to validate the reconstruction quality constantly. Experimentally, on several IR problems and different SIDGPs, our self-validation method is able to reliably detect near-peak performance and signal good ES points. Our code is available at https://sun-umn.github.io/Self-Validation/."}}
{"id": "JIl_kij_aov", "cdate": 1663850034708, "mdate": null, "content": {"title": "Early Stopping for Deep Image Prior", "abstract": "Deep image prior (DIP) and its variants have shown remarkable potential for solving inverse problems in computational imaging (CI), needing no separate training data. Practical DIP models are often substantially overparameterized. During the learning process, these models learn the desired visual content first and then pick up the potential modeling and observational noise, i.e., overfitting. Thus, the practicality of DIP hinges on early stopping (ES) that captures the transition period. In this regard, the majority of prior DIP works for CI tasks only demonstrate the potential of the models---reporting the peak performance against the groundtruth but providing no clue about how to operationally obtain near-peak performance without access to the groundtruth. In this paper, we set to break this practicality barrier of DIP, and propose an efficient ES strategy that consistently detects near-peak performance across several CI tasks and DIP variants. Simply based on the running variance of DIP intermediate reconstructions, our ES method not only outpaces the existing ones---which only work in very narrow regimes, but also remains effective when combined with methods that try to mitigate overfitting."}}
{"id": "AlJ06ubQDHF", "cdate": 1620333059224, "mdate": null, "content": {"title": "Detection of Isocitrate Dehydrogenase Mutated Glioblastomas Through Anomaly Detection Analytics", "abstract": " We employed an anomaly detection strategy in the detection of IDH\nmutation in glioblastoma using preoperative T1 postcontrast imaging. We show these\nmethods outperform traditional two-class classification in the setting of dataset imbal-\nances inherent to IDH mutation prevalence in glioblastoma. We validate our results using\nan external dataset and highlight new possible avenues for radiogenomic rare event\nprediction in glioblastoma and beyond.\n"}}
{"id": "aY20ERN5Js", "cdate": 1620332713283, "mdate": null, "content": {"title": "Lung Cancer Risk Prediction Method Based on Feature Selection and Artificial Neural Network", "abstract": "A method to predict the risk of lung cancer is proposed, based on two feature selection algorithms: Fisher and ReliefF, and BP Neural Networks. An appropriate quantity of risk factors was chosen for lung cancer risk prediction. The process featured two steps, firstly choosing the risk factors by combining two feature selection algorithms, then providing the predictive value by neural network. Based on the method framework, an algorithm LCRP (lung cancer risk prediction) is presented, to reduce the amount of risk factors collected in practical applications. The proposed method is suitable for health monitoring and self-testing. Experiments showed it can actually provide satisfactory accuracy under low dimensions of risk factors."}}
{"id": "w2-ir2qDkl", "cdate": 1620332643347, "mdate": null, "content": {"title": "False positive elimination in intrusion detection based on clustering", "abstract": "In order to solve the problem of high false positive in network intrusion detection systems, we adopted clustering algorithms, the K-means algorithm and the Fuzzy C Mean (FCM) algorithm, to identify false alerts, to reduce invalid alerts and to purify alerts for a better analysis. In this paper, we first introduced typical clustering algorithms, including the partition clustering, the hierarchical clustering, the density and grid clustering, and the fuzzy clustering, and then analyzed their feasibilities in security data processing. Furthermore, we introduced an intrusion detection framework, and tested the validity and feasibility of false positive elimination in intrusion detection. The process steps of false positive elimination were clearly described, and additionally, two typical clustering algorithms, the K-means algorithm and the FCM algorithm, were implemented for false alerts identification and filtration. Also, we defined three evaluation indexes: the elimination rate, the false elimination rate and the miss elimination rate. Accordingly, we used DARPA 2000 LLDOS1.0 dataset for our experiments, and adopted Snort as our intrusion detection system. Eventually, the results showed that the method proposed by us has a satisfactory validity and feasibility in false positive elimination, and the clustering algorithms we adopted can achieve a high elimination rate."}}
