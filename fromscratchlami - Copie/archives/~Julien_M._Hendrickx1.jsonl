{"id": "_wRhG_QlpL0", "cdate": 1675079867785, "mdate": 1675079867785, "content": {"title": "Minimax rate for learning from pairwise comparisons in the BTL model", "abstract": "We consider the problem of learning the qualities w1, . . . , wn of a collection of items by performing noisy comparisons among them. A standard assumption is that there is a fixed \u201ccomparison graph\u201d and every neighboring pair of items is compared k times. We will study the popular Bradley-Terry-Luce model, where the probability that item i wins a comparison against j equals wi/(wi + wj ). The goal is to understand how the expected error in estimating the vector w = (w1, . . . , wn) behaves in the regime when the number of comparisons k is large. Our contribution is the determination of the minimax rate up to a constant factor. We show that this rate is achieved by a simple algorithm based on weighted least squares, with weights determined from the empirical outcomes of the comparisons. This algorithm can be implemented in nearly linear time in the total number of comparisons"}}
{"id": "gUntOLY6yGR", "cdate": 1650586869251, "mdate": 1650586869251, "content": {"title": "PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python", "abstract": "PEPit is a Python package aiming at simplifying the access to worst-case analyses of a large family of first-order optimization methods possibly involving gradient, projection, proximal, or linear optimization oracles, along with their approximate, or Bregman variants. In short, PEPit is a package enabling computer-assisted worst-case analyses of first-order optimization methods. The key underlying idea is to cast the problem of performing a worst-case analysis, often referred to as a performance estimation problem (PEP), as a semidefinite program (SDP) which can be solved numerically. For doing that, the package users are only required to write first-order methods nearly as they would have implemented them. The package then takes care of the SDP modelling parts, and the worst-case analysis is performed numerically via a standard solver."}}
{"id": "sfqXvUeD7t", "cdate": 1640995200000, "mdate": 1675079984799, "content": {"title": "Guest Editorial Special Issue on Dynamics and Behaviors in Social Networks", "abstract": ""}}
{"id": "jDFx6tieggV", "cdate": 1640995200000, "mdate": 1675079984797, "content": {"title": "Automatic Performance Estimation for Decentralized Optimization", "abstract": "We present a methodology to automatically compute worst-case performance bounds for a large class of first-order decentralized optimization algorithms. These algorithms aim at minimizing the average of local functions that are distributed across a network of agents. They typically combine local computations and consensus steps. Our methodology is based on the approach of Performance Estimation Problem (PEP), which allows computing the worst-case performance and a worst-case instance of first-order optimization algorithms by solving an SDP. We propose two ways of representing consensus steps in PEPs, which allow writing and solving PEPs for decentralized optimization. The first formulation is exact but specific to a given averaging matrix. The second formulation is a relaxation but provides guarantees valid over an entire class of averaging matrices, characterized by their spectral range. This formulation often allows recovering a posteriori the worst possible averaging matrix for the given algorithm. We apply our methodology to three different decentralized methods. For each of them, we obtain numerically tight worst-case performance bounds that significantly improve on the existing ones, as well as insights about the parameters tuning and the worst communication networks."}}
{"id": "fJ1CFYF-Y13", "cdate": 1640995200000, "mdate": 1675079984798, "content": {"title": "Automated Performance Estimation for Decentralized Optimization via Network Size Independent Problems", "abstract": "We develop a novel formulation of the Performance Estimation Problem (PEP) for decentralized optimization whose size is independent of the number of agents in the network. The PEP approach allows computing automatically the worst-case performance and worst-case instance of first-order optimization methods by solving an SDP. Unlike previous work, the size of our new PEP formulation is independent of the network size. For this purpose, we take a global view of the decentralized problem and we also decouple the consensus subspace and its orthogonal complement. We apply our methodology to different decentralized methods such as DGD, DIGing and EXTRA and obtain numerically tight performance guarantees that are valid for any network size."}}
{"id": "aJgD3xAOZ1", "cdate": 1640995200000, "mdate": 1675079984814, "content": {"title": "Convergence, Consensus and Dissensus in the Weighted-Median Opinion Dynamics", "abstract": "Mechanistic and tractable mathematical models play a key role in understanding how social influence shapes public opinions. Recently, a weighted-median mechanism has been proposed as a new micro-foundation of opinion dynamics and validated via experimental data. Numerical studies also indicate that this new mechanism recreates some non-trivial real-world features of opinion evolution. In this paper, we conduct a thorough theoretical analysis of the weighted-median opinion dynamics. We fully characterize the set of all equilibria, and we establish the almost-sure finite-time convergence for any initial condition. Moreover, we prove a necessary and sufficient graph-theoretic condition for the almost-sure convergence to consensus, as well as a sufficient graph-theoretic condition for almost-sure persistent dissensus. It turns out that the weighted-median opinion dynamics, despite its simplicity in form, exhibit rich dynamical behavior that depends on some delicate network structures. To complement our sufficient conditions for almost-sure dissensus, we further prove that, given the influence network, determining whether the system almost surely achieves persistent dissensus is NP-hard, which reflects the complexity the network topology contributes to opinion evolution."}}
{"id": "WINSMElwLH", "cdate": 1640995200000, "mdate": 1675079984798, "content": {"title": "Automated Performance Estimation for Decentralized Optimization via Network Size Independent Problems", "abstract": "We develop a novel formulation of the Performance Estimation Problem (PEP) for decentralized optimization whose size is independent of the number of agents in the network. The PEP approach allows computing automatically the worst-case performance and worst-case instance of first-order optimization methods by solving an SDP. Unlike previous work, the size of our new PEP formulation is independent of the network size. For this purpose, we take a global view of the decentralized problem and we also decouple the consensus subspace and its orthogonal complement. We apply our methodology to different decentralized methods such as DGD, DIGing and EXTRA and obtain numerically tight performance guarantees that are valid for any network size."}}
{"id": "R89SXMGCVY6", "cdate": 1640995200000, "mdate": 1675079984805, "content": {"title": "Information Bounds and Convergence Rates for Side-Channel Security Evaluators", "abstract": ""}}
{"id": "P_z1oRO-5a", "cdate": 1640995200000, "mdate": 1675079984621, "content": {"title": "Multiagent Persistent Monitoring of Targets With Uncertain States", "abstract": "We address the problem of persistent monitoring, where a finite set of mobile agents has to persistently visit a finite set of targets. Each of these targets has an internal state that evolves with linear stochastic dynamics. The agents can observe these states, and the observation quality is a function of the distance between the agent and a given target. The goal is then to minimize the mean squared estimation error of these target states. We approach the problem from an infinite horizon perspective, where we prove that, under some natural assumptions, the covariance matrix of each target converges to a limit cycle. The goal, therefore, becomes to minimize the steady-state uncertainty. Assuming that the trajectory is parameterized, we provide tools for computing the steady-state cost gradient. We show that, in 1-D (one dimensional) environments with bounded control and nonoverlapping targets, when an optimal control exists it can be represented using a finite number of parameters. We also propose an efficient parameterization of the agent trajectories for multidimensional settings using Fourier curves. Simulation results show the efficacy of the proposed technique in 1-D, 2-D, and 3-D scenarios."}}
{"id": "OnmltIrc0f", "cdate": 1640995200000, "mdate": 1675079984797, "content": {"title": "Minimax Multi-Agent Persistent Monitoring of a Network System", "abstract": "We investigate the problem of optimally observing a finite set of targets using a mobile agent over an infinite time horizon. The agent is tasked to move in a network-constrained structure to gather information so as to minimize the worst-case uncertainty about the internal states of the targets. To do this, the agent has to decide its sequence of target-visits and the corresponding dwell-times at each visited target. For a given visiting sequence, we prove that in an optimal dwelling time allocation the peak uncertainty is the same among all the targets. This allows us to formulate the optimization of dwelling times as a resource allocation problem and to solve it using a novel efficient algorithm. Next, we optimize the visiting sequence using a greedy exploration process, using heuristics inspired by others developed in the context of the traveling salesman problem. Numerical results are included to illustrate the contributions."}}
