{"id": "PXVGer7hmJ", "cdate": 1663850170217, "mdate": null, "content": {"title": "Offline Congestion Games: How Feedback Type Affects Data Coverage Requirement", "abstract": "This paper investigates when one can efficiently recover an approximate Nash Equilibrium (NE) in offline congestion games. The existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. We consider three different types of feedback with decreasing revealed information. Starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and show a pessimism-type algorithm that can recover an approximate NE. For the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. On the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate NE. Lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. Again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate NE. Together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback."}}
{"id": "p3w4l4nf_Rr", "cdate": 1652737570712, "mdate": null, "content": {"title": "Learning in Congestion Games with Bandit Feedback", "abstract": "In this paper, we investigate Nash-regret minimization in congestion games, a class of games with benign theoretical structure and broad real-world applications. We first propose a centralized algorithm based on the optimism in the face of uncertainty principle for congestion games with (semi-)bandit feedback, and obtain finite-sample guarantees. Then we propose a decentralized algorithm via a novel combination of the Frank-Wolfe method and G-optimal design. By exploiting the structure of the congestion game, we show the sample complexity of both algorithms depends only polynomially on the number of players and the number of facilities, but not the size of the action set, which can be exponentially large in terms of the number of facilities. We further define a new problem class, Markov congestion games, which allows us to model the non-stationarity in congestion games. We propose a centralized algorithm for Markov congestion games, whose sample complexity again has only polynomial dependence on all relevant problem parameters, but not the size of the action set."}}
{"id": "q2nJyb3cvR9", "cdate": 1652737465073, "mdate": null, "content": {"title": "Near-Optimal Randomized Exploration for Tabular Markov Decision Processes", "abstract": "We study algorithms using randomized value functions for exploration in reinforcement learning. This type of algorithms enjoys appealing empirical performance. We show that when we use 1) a single random seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a worst-case $\\widetilde{O}\\left(H\\sqrt{SAT}\\right)$ regret bound for episodic time-inhomogeneous Markov Decision Process where $S$ is the size of state space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is the number of interactions. This bound polynomially improves all existing bounds for algorithms based on randomized value functions, and for the first time, matches the $\\Omega\\left(H\\sqrt{SAT}\\right)$ lower bound up to logarithmic factors. Our result highlights that randomized exploration can be near-optimal, which was previously achieved only by optimistic algorithms. To achieve the desired result, we develop 1) a new clipping operation to ensure both the probability of being optimistic and the probability of being pessimistic are lower bounded by a constant, and 2) a new recursive  formula for the absolute value of estimation errors to analyze the regret."}}
{"id": "uDwnFislkpN", "cdate": 1640995200000, "mdate": 1682318525093, "content": {"title": "Fourier Learning with Cyclical Data", "abstract": "Many machine learning models for online applications, such as recommender systems, are often trained on data with cyclical properties. These data sequentially arrive from a time-varying distributio..."}}
{"id": "lixSWNR3X19", "cdate": 1640995200000, "mdate": 1682318525092, "content": {"title": "Learning in Congestion Games with Bandit Feedback", "abstract": "In this paper, we investigate Nash-regret minimization in congestion games, a class of games with benign theoretical structure and broad real-world applications. We first propose a centralized algorithm based on the optimism in the face of uncertainty principle for congestion games with (semi-)bandit feedback, and obtain finite-sample guarantees. Then we propose a decentralized algorithm via a novel combination of the Frank-Wolfe method and G-optimal design. By exploiting the structure of the congestion game, we show the sample complexity of both algorithms depends only polynomially on the number of players and the number of facilities, but not the size of the action set, which can be exponentially large in terms of the number of facilities. We further define a new problem class, Markov congestion games, which allows us to model the non-stationarity in congestion games. We propose a centralized algorithm for Markov congestion games, whose sample complexity again has only polynomial dependence on all relevant problem parameters, but not the size of the action set."}}
{"id": "NC2ZLFvGIv", "cdate": 1640995200000, "mdate": 1681676707930, "content": {"title": "Offline congestion games: How feedback type affects data coverage requirement", "abstract": "This paper investigates when one can efficiently recover an approximate Nash Equilibrium (NE) in offline congestion games.The existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. We consider three different types of feedback with decreasing revealed information. Starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and give a pessimism-type algorithm that can recover an approximate NE. For the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. On the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate NE. Lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. Again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate NE. Together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback."}}
{"id": "aoXERVeC7cC", "cdate": 1621630098570, "mdate": null, "content": {"title": "Selective Sampling for Online Best-arm Identification", "abstract": "This work considers the problem of selective-sampling for best-arm identification. Given a set of potential options $\\mathcal{Z}\\subset\\mathbb{R}^d$, a learner aims to compute with probability greater than $1-\\delta$, $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$ where $\\theta_{\\ast}$ is unknown. At each time step, a potential measurement $x_t\\in \\mathcal{X}\\subset\\mathbb{R}^d$ is drawn IID and the learner can either choose to take the measurement, in which case they observe a noisy measurement of $x^{\\top}\\theta_{\\ast}$, or to abstain from taking the measurement and wait for a potentially more informative point to arrive in the stream. Hence the learner faces a fundamental trade-off between the number of labeled samples they take and when they have collected enough evidence to declare the best arm and stop sampling. The main results of this work precisely characterize this trade-off between labeled samples and stopping time and provide an algorithm that nearly-optimally achieves the minimal label complexity given a desired stopping time. In addition, we show that the optimal decision rule has a simple geometric form based on deciding whether a point is in an ellipse or not. Finally, our framework is general enough to capture binary classification improving upon previous works. "}}
{"id": "q-fVQmW2ry", "cdate": 1609459200000, "mdate": 1681830630254, "content": {"title": "Selective Sampling for Online Best-arm Identification", "abstract": "This work considers the problem of selective-sampling for best-arm identification. Given a set of potential options $\\mathcal{Z}\\subset\\mathbb{R}^d$, a learner aims to compute with probability greater than $1-\\delta$, $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$ where $\\theta_{\\ast}$ is unknown. At each time step, a potential measurement $x_t\\in \\mathcal{X}\\subset\\mathbb{R}^d$ is drawn IID and the learner can either choose to take the measurement, in which case they observe a noisy measurement of $x^{\\top}\\theta_{\\ast}$, or to abstain from taking the measurement and wait for a potentially more informative point to arrive in the stream. Hence the learner faces a fundamental trade-off between the number of labeled samples they take and when they have collected enough evidence to declare the best arm and stop sampling. The main results of this work precisely characterize this trade-off between labeled samples and stopping time and provide an algorithm that nearly-optimally achieves the minimal label complexity given a desired stopping time. In addition, we show that the optimal decision rule has a simple geometric form based on deciding whether a point is in an ellipse or not. Finally, our framework is general enough to capture binary classification improving upon previous works."}}
{"id": "MLzLHyW2GZl", "cdate": 1609459200000, "mdate": null, "content": {"title": "Randomized Exploration is Near-Optimal for Tabular MDP", "abstract": "We study algorithms using randomized value functions for exploration in reinforcement learning. This type of algorithms enjoys appealing empirical performance. We show that when we use 1) a single random seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a worst-case $\\widetilde{O}\\left(H\\sqrt{SAT}\\right)$ regret bound for episodic time-inhomogeneous Markov Decision Process where $S$ is the size of state space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is the number of interactions. This bound polynomially improves all existing bounds for algorithms based on randomized value functions, and for the first time, matches the $\\Omega\\left(H\\sqrt{SAT}\\right)$ lower bound up to logarithmic factors. Our result highlights that randomized exploration can be near-optimal, which was previously achieved only by optimistic algorithms. To achieve the desired result, we develop 1) a new clipping operation to ensure both the probability of being optimistic and the probability of being pessimistic are lower bounded by a constant, and 2) a new recursive formula for the absolute value of estimation errors to analyze the regret."}}
{"id": "EA0ZCm8We0v", "cdate": 1609459200000, "mdate": 1681830630271, "content": {"title": "Selective Sampling for Online Best-arm Identification", "abstract": "This work considers the problem of selective-sampling for best-arm identification. Given a set of potential options $\\mathcal{Z}\\subset\\mathbb{R}^d$, a learner aims to compute with probability greater than $1-\\delta$, $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$ where $\\theta_{\\ast}$ is unknown. At each time step, a potential measurement $x_t\\in \\mathcal{X}\\subset\\mathbb{R}^d$ is drawn IID and the learner can either choose to take the measurement, in which case they observe a noisy measurement of $x^{\\top}\\theta_{\\ast}$, or to abstain from taking the measurement and wait for a potentially more informative point to arrive in the stream. Hence the learner faces a fundamental trade-off between the number of labeled samples they take and when they have collected enough evidence to declare the best arm and stop sampling. The main results of this work precisely characterize this trade-off between labeled samples and stopping time and provide an algorithm that nearly-optimally achieves the minimal label complexity given a desired stopping time. In addition, we show that the optimal decision rule has a simple geometric form based on deciding whether a point is in an ellipse or not. Finally, our framework is general enough to capture binary classification improving upon previous works."}}
