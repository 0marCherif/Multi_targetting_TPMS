{"id": "sbabGqakIc", "cdate": 1668734781415, "mdate": null, "content": {"title": "Certified defences hurt generalisation", "abstract": "In recent years, much work has been devoted to designing certified\ndefences for neural networks, i.e., methods for learning neural\nnetworks that are provably robust to certain adversarial\nperturbations. Due to the non-convexity of the problem, dominant\napproaches in this area rely on convex approximations, which are\ninherently loose. In this paper, we question the effectiveness of such\napproaches for realistic computer vision tasks. First, we provide\nextensive empirical evidence to show that certified defences suffer\nnot only worse accuracy but also worse robustness and fairness than\nempirical defences. We hypothesise that the reason for why certified\ndefences suffer in generalisation is (i) the large number of\nrelaxed non-convex constraints and (ii) strong alignment between the\nadversarial perturbations and the \"signal\" direction. We provide a\ncombination of theoretical and experimental evidence to support these\nhypotheses."}}
{"id": "h1j5I0WVxoI", "cdate": 1664725483013, "mdate": null, "content": {"title": "Certified defences hurt generalisation", "abstract": "In recent years, much work has been devoted to designing certified defences for neural networks, i.e., methods for learning neural networks that are provably robust to certain adversarial perturbations. Due to the non-convexity of the problem, dominant approaches in this area rely on convex approximations, which are inherently loose. In this paper, we question the effectiveness of such approaches for realistic computer vision tasks. First, we provide extensive empirical evidence to show that certified defences suffer not only worse accuracy but also worse robustness and fairness than empirical defences. We hypothesise that the reason for why certified defences suffer in generalisation is (i) the large number of relaxed non-convex constraints and (ii) strong alignment between the adversarial perturbations and the \"signal\" direction. We provide a combination of theoretical and experimental evidence to support these hypotheses."}}
{"id": "-CA8yFkPc7O", "cdate": 1663850307431, "mdate": null, "content": {"title": "Why adversarial training can hurt robust accuracy", "abstract": "Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite can be true for a natural class of perceptible perturbations --- even though adversarial training helps when enough data is  available, it may in fact hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Using intuitive insights from the proof, we could surprisingly find perturbations on standard image datasets for which this behavior persists. Specifically, it occurs for perceptible attacks that effectively reduce class information such as object occlusions or corruptions. "}}
