{"id": "AI40VMBIe5L", "cdate": 1631877065089, "mdate": null, "content": {"title": "VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human Mesh Recovery from Partial Point Clouds", "abstract": "3D human mesh recovery from point clouds is essential for vari- ous tasks, including AR/VR and human behavior understanding. Previous works in this field either require high-quality 3D human scans or sequential point clouds, which cannot be easily applied to low-quality 3D scans captured by consumer-level depth sensors. In this paper, we make the first attempt to reconstruct reliable 3D human shapes from single-frame partial point clouds. To achieve this, we propose an end-to-end learnable method, named VoteHMR. The core of VoteHMR is a novel occlusion-aware voting network that can first reliably produce visible joint-level features from the input partial point clouds, and then complete the joint-level features through the kinematic tree of the human skeleton. Compared with holistic features used by previous works, the joint-level features can not only effectively encode the human geometry information but also be robust to noisy inputs with self-occlusions and miss- ing areas. By exploiting the rich complementary clues from the joint-level features and global features from the input point clouds, the proposed method encourages reliable and disentangled param- eter predictions for statistical 3D human models, such as SMPL. The proposed method achieves state-of-the-art performances on two large-scale datasets, namely SURREAL and DFAUST. Further- more, VoteHMR also demonstrates superior generalization ability on real-world datasets, such as Berkeley MHAD."}}
{"id": "BHNUjA1e-id", "cdate": 1631877025887, "mdate": null, "content": {"title": "FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration", "abstract": "Most existing monocular 3D pose estimation approaches only focus on a single body part, neglecting the fact that the essential nuance of human motion is conveyed through a concert of subtle movements of face, hands, and body. In this paper, we present FrankMocap, a fast and accurate whole-body 3D pose estimation system that can produce 3D face, hands, and body simultaneously from in-the-wild monocular images. The idea of FrankMocap is its modular design: We first run 3D pose regression methods for face, hands, and body independently, followed by composing the regression outputs via an integration module. The sepa- rate regression modules allow us to take full advantage of their state-of-the-art performances without compromising the original accuracy and reliability in practice. We develop three different integration modules that trade off between latency and accuracy. All of them are capable of provid- ing simple yet effective solutions to unify the separate out- puts into seamless whole-body pose estimation results. We quantitatively and qualitatively demonstrate that our modu- larized system outperforms both the optimization-based and end-to-end methods of estimating whole-body pose. Code and models and demo videos are available at https:// github.com/facebookresearch/frankmocap."}}
{"id": "mgqsb6xeT1k", "cdate": 1631876969730, "mdate": null, "content": {"title": "Delving Deep Into Hybrid Annotations for 3D Human Recovery in the Wild", "abstract": "Though much progress has been achieved in single- image 3D human recovery, estimating 3D model for in-the- wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that con- sumes enormous amount of resources and manpower. To tackle this problem, previous methods adopt a hybrid train- ing strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the effi- cacy of each annotation not thoroughly investigated. In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations. Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D an- notations are not fully available. Through extensive exper- iments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less compe- tent in guiding 3D human recovery. 2) Dense Correspon- dence such as DensePose [1] is effective. When there are no paired in-the-wild 3D annotations available, the model ex- ploiting dense correspondence can achieve 92% of the per- formance compared to a model trained with paired 3D data. We show that incorporating dense correspondence into in- the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research."}}
