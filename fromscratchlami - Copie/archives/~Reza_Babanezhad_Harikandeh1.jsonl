{"id": "22tWlk_NycL", "cdate": 1664731452397, "mdate": null, "content": {"title": "Target-based Surrogates for Stochastic Optimization", "abstract": "We consider minimizing functions for which it is expensive to compute the gradient. Such functions are prevalent in reinforcement learning, imitation learning and bilevel optimization. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a \\emph{target space} (e.g. the logits output by a linear model for classification) that can be minimized efficiently. This allows for multiple parameter updates to the model, amortizing the cost of gradient computation. In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the $SSO$ algorithm that can be viewed as projected stochastic gradient descent in the target space. This connection enables us to use standard stochastic optimization algorithms to construct surrogates which can be minimized using deterministic optimization. Our experiments on supervised learning and imitation learning exhibit the benefits of target optimization, even in stochastic settings. "}}
{"id": "HgWfewLsqxc", "cdate": 1646077527723, "mdate": null, "content": {"title": "Towards Painless Policy Optimization for Constrained MDPs", "abstract": "We study policy optimization in an infinite horizon, $\\gamma$-discounted constrained Markov decision process (CMDP). Our objective is to return a policy that achieves large expected reward with a small constraint violation. We consider the online setting with linear function approximation and assume global access to the corresponding features. We propose a generic primal-dual framework that allows us to bound the reward sub-optimality and constraint violation for arbitrary algorithms in terms of their primal and dual regret on online linear optimization problems. We instantiate this framework to use coin-betting algorithms and propose the Coin Betting Politex (CBP) algorithm. Assuming that the action-value functions are $\\epsilon_{\\text{\\tiny{b}}}$-close to the span of the $d$-dimensional state-action features and no sampling errors, we prove that $T$ iterations of CBP result in an $O\\left(\\frac{1}{(1 - \\gamma)^3 \\sqrt{T}} + \\frac{\\epsilon_{\\text{\\tiny{b}}} \\sqrt{d}}{(1 - \\gamma)^2} \\right)$ reward sub-optimality and an $O\\left(\\frac{1}{(1 - \\gamma)^2 \\sqrt{T}} + \\frac{\\epsilon_{\\text{\\tiny{b}}} \\sqrt{d}}{1 - \\gamma} \\right)$ constraint violation. Importantly, unlike gradient descent-ascent and other recent methods, CBP does not require extensive hyperparameter tuning. Via experiments on synthetic and Cartpole environments, we demonstrate the effectiveness and robustness of CBP."}}
{"id": "SJx4O34YvS", "cdate": 1569438827794, "mdate": null, "content": {"title": "Semantics Preserving Adversarial Attacks", "abstract": "While progress has been made in crafting visually imperceptible adversarial examples, constructing semantically meaningful ones remains a challenge. In this paper, we propose a framework to generate semantics preserving adversarial examples. First, we present a manifold learning method to capture the semantics of the inputs. The motivating principle is to learn the low-dimensional geometric summaries of the inputs via statistical inference. Then, we perturb the elements of the learned manifold using the Gram-Schmidt process to induce the perturbed elements to remain in the manifold. To produce adversarial examples, we propose an efficient algorithm whereby we leverage the semantics of the inputs as a source of knowledge upon which we impose adversarial constraints. We apply our approach on toy data, images and text, and show its effectiveness in producing semantics preserving adversarial examples which evade existing defenses against adversarial attacks."}}
{"id": "r1qKBtJvG", "cdate": 1518470530367, "mdate": null, "content": {"title": "Online variance-reducing optimization", "abstract": "We emphasize the importance of variance reduction in stochastic methods and propose a probabilistic interpretation as a way to store information about past gradients. The resulting algorithm is very similar to the momentum method, with the difference that the weight over past gradients depends on the distance moved in parameter space rather than the number of steps."}}
