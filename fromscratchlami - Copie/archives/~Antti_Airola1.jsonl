{"id": "xRmYJL4K0o", "cdate": 1640995200000, "mdate": 1681654256912, "content": {"title": "Generalized vec trick for fast learning of pairwise kernel models", "abstract": ""}}
{"id": "rreMObH5ci", "cdate": 1640995200000, "mdate": 1681654256854, "content": {"title": "Striving for platform independence in the e-learning landscape: a study on a flexible exercise creation system", "abstract": ""}}
{"id": "deJL5ENrM67", "cdate": 1609459200000, "mdate": 1681654256954, "content": {"title": "Adaptive risk prediction system with incremental and transfer learning", "abstract": ""}}
{"id": "0vB82Ijn75B", "cdate": 1609459200000, "mdate": 1681654256882, "content": {"title": "Detecting Aortic Stenosis Using Seismocardiography and Gryocardiography Combined with Convolutional Neural Networks", "abstract": ""}}
{"id": "xVe2S4GiOe9", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust ECG R-peak detection using LSTM.", "abstract": "Detecting QRS complexes or R-peaks from the electrocardiogram (ECG) is the basis for heart rate determination and heart rate variability analysis. Over the years, multiple different methods have been proposed as solutions to this problem. Vast majority of the proposed methods are traditional rule based algorithms that are vulnerable to noise. We propose a new R-peak detection method that is based on the Long Short-Term Memory (LSTM) network. LSTM networks excel at temporal modelling tasks that include long-term dependencies, making it suitable for ECG analysis. Additionally, we propose data generator for creating noisy ECG data that is used to train the robust R-peak detector. Our initial testing shows that the proposed method outperforms traditional algorithms while the greatest competitive edge is achieved with the noisy ECG signals."}}
{"id": "Y5AcKj-cde", "cdate": 1577836800000, "mdate": null, "content": {"title": "Measuring Player Retention and Monetization Using the Mean Cumulative Function.", "abstract": "Game analytics supports game development by providing direct quantitative feedback about player experience. Player retention and monetization have become central business statistics in free-to-play game development. Total playtime and lifetime value in particular are central benchmarks, but many metrics have been used for this purpose. However, game developers often want to perform analytics in a timely manner before all users have churned from the game. This causes data censoring, which makes many metrics biased. In this article, we introduce how the mean cumulative function (MCF) can be used to measure metrics from censored data. Statistical tools based on the MCF allow game developers to determine whether a given change improves a game or whether a game is good enough for public release. The MCF is a general tool that estimates the expected value of a metric for any data set and does not rely on a model for the data. We demonstrate the advantages of this approach on a real in-development free-to-play mobile game Hipster Sheep."}}
{"id": "Bb3gk9TtuNyc", "cdate": 1577836800000, "mdate": null, "content": {"title": "Cost-effective survival prediction for patients with advanced prostate cancer using clinical trial and real-world hospital registry datasets.", "abstract": "Highlights \u2022 Two variable selection methods are implemented: LASSO and greedy selection. \u2022 The methods are tested in four cohorts of prostate cancer patients. \u2022 The cost of survival prediction for patients with prostate cancer can be significantly reduced. \u2022 LASSO tends to give better peak accuracy but with low budgets the greedy method is better. Abstract Introduction Predictive survival modeling offers systematic tools for clinical decision-making and individualized tailoring of treatment strategies to improve patient outcomes while reducing overall healthcare costs. In 2015, a number of machine learning and statistical models were benchmarked in the DREAM 9.5 Prostate Cancer Challenge, based on open clinical trial data for metastatic castration resistant prostate cancer (mCRPC). However, applying these models into clinical practice poses a practical challenge due to the inclusion of a large number of model variables, some of which are not routinely monitored or are expensive to measure. Objectives To develop cost-specified variable selection algorithms for constructing cost-effective prognostic models of overall survival that still preserve sufficient model performance for clinical decision making. Methods Penalized Cox regression models were used for the survival prediction. For the variable selection, we implemented two algorithms: (i) LASSO regularization approach; and (ii) a greedy cost-specified variable selection algorithm. The models were compared in three cohorts of mCRPC patients from randomized clinical trials (RCT), as well as in a real-world cohort (RWC) of advanced prostate cancer patients treated at the Turku University Hospital. Hospital laboratory expenses were utilized as a reference for computing the costs of introducing new variables into the models. Results Compared to measuring the full set of clinical variables, economic costs could be reduced by half without a significant loss of model performance. The greedy algorithm outperformed the LASSO-based variable selection with the lowest tested budgets. The overall top performance was higher with the LASSO algorithm. Conclusion The cost-specified variable selection offers significant budget optimization capability for the real-world survival prediction without compromising the predictive power of the model. Previous article in issue Next article in issue"}}
{"id": "15jx5hKkjAoW", "cdate": 1577836800000, "mdate": null, "content": {"title": "Algebraic shortcuts for leave-one-out cross-validation in supervised network inference.", "abstract": "Supervised machine learning techniques have traditionally been very successful at reconstructing biological networks, such as protein\u2013ligand interaction, protein\u2013protein interaction and gene regulatory networks. Many supervised techniques for network prediction use linear models on a possibly nonlinear pairwise feature representation of edges. Recently, much emphasis has been placed on the correct evaluation of such supervised models. It is vital to distinguish between using a model to either predict new interactions in a given network or to predict interactions for a new vertex not present in the original network. This distinction matters because (i) the performance might dramatically differ between the prediction settings and (ii) tuning the model hyperparameters to obtain the best possible model depends on the setting of interest. Specific cross-validation schemes need to be used to assess the performance in such different prediction settings."}}
{"id": "IPH-C4Zo0hd", "cdate": 1546300800000, "mdate": null, "content": {"title": "The spatial leave-pair-out cross-validation method for reliable AUC estimation of spatial classifiers.", "abstract": "Machine learning based classification methods are widely used in geoscience applications, including mineral prospectivity mapping. Typical characteristics of the data, such as small number of positive instances, imbalanced class distributions and lack of verified negative instances make ROC analysis and cross-validation natural choices for classifier evaluation. However, recent literature has identified two sources of bias, that can affect reliability of area under ROC curve estimation via cross-validation on spatial data. The pooling procedure performed by methods such as leave-one-out can introduce a substantial negative bias to results. At the same time, spatial dependencies leading to spatial autocorrelation can result in overoptimistic results, if not corrected for. In this work, we introduce the spatial leave-pair-out cross-validation method, that corrects for both of these biases simultaneously. The methodology is used to benchmark a number of classification methods on mineral prospectivity mapping data from the Central Lapland greenstone belt. The evaluation highlights the dangers of obtaining misleading results on spatial data and demonstrates how these problems can be avoided. Further, the results show the advantages of simple linear models for this classification task."}}
{"id": "pylp1n3XLHm", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning with multiple pairwise kernels for drug bioactivity prediction.", "abstract": "Many inference problems in bioinformatics, including drug bioactivity prediction, can be formulated as pairwise learning problems, in which one is interested in making predictions for pairs of objects, e.g. drugs and their targets. Kernel-based approaches have emerged as powerful tools for solving problems of that kind, and especially multiple kernel learning (MKL) offers promising benefits as it enables integrating various types of complex biomedical information sources in the form of kernels, along with learning their importance for the prediction task. However, the immense size of pairwise kernel spaces remains a major bottleneck, making the existing MKL algorithms computationally infeasible even for small number of input pairs."}}
