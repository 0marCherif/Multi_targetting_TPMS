{"id": "vDaCDegnGU", "cdate": 1672531200000, "mdate": 1683738693794, "content": {"title": "Understanding Self-Distillation in the Presence of Label Noise", "abstract": "Self-distillation (SD) is the process of first training a \\enquote{teacher} model and then using its predictions to train a \\enquote{student} model with the \\textit{same} architecture. Specifically, the student's objective function is $\\big(\\xi*\\ell(\\text{teacher's predictions}, \\text{ student's predictions}) + (1-\\xi)*\\ell(\\text{given labels}, \\text{ student's predictions})\\big)$, where $\\ell$ is some loss function and $\\xi$ is some parameter $\\in [0,1]$. Empirically, SD has been observed to provide performance gains in several settings. In this paper, we theoretically characterize the effect of SD in two supervised learning problems with \\textit{noisy labels}. We first analyze SD for regularized linear regression and show that in the high label noise regime, the optimal value of $\\xi$ that minimizes the expected error in estimating the ground truth parameter is surprisingly greater than 1. Empirically, we show that $\\xi > 1$ works better than $\\xi \\leq 1$ even with the cross-entropy loss for several classification datasets when 50\\% or 30\\% of the labels are corrupted. Further, we quantify when optimal SD is better than optimal regularization. Next, we analyze SD in the case of logistic regression for binary classification with random label corruption and quantify the range of label corruption in which the student outperforms the teacher in terms of accuracy. To our knowledge, this is the first result of its kind for the cross-entropy loss."}}
{"id": "0FllaTqjor7", "cdate": 1664731446527, "mdate": null, "content": {"title": "Differentially Private Federated Learning with Normalized Updates", "abstract": "The customary approach for client-level differentially private federated learning (FL) is to add Gaussian noise to the average of the clipped client updates. Clipping is associated with the following issue: as the client updates fall below the clipping threshold, they get drowned out by the added noise, inhibiting convergence. To mitigate this issue, we propose replacing clipping with normalization, where we use only a scaled version of the unit vector along the client updates. Normalization ensures that the noise does not drown out the client updates even when the original updates are small. We theoretically show that the resulting normalization-based private FL algorithm attains better convergence than its clipping-based counterpart on convex objectives in over-parameterized settings."}}
{"id": "I7-KGoBeRMk", "cdate": 1664567041977, "mdate": 1664567041977, "content": {"title": "On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Federated Learning", "abstract": "Federated learning (FL) is an emerging collaborative machine learning (ML) framework that enables training of predictive models in a distributed fashion where the communication among the participating nodes are facilitated by a central server. To deal with the communication bottleneck at the server, decentralized FL (DFL) methods advocate rely on local communication of nodes with their neighbors according to a specific communication network. In DFL, it is common algorithmic practice to have nodes interleave (local) gradient descent iterations with gossip (i.e. averaging over the network) steps. As the size of the ML models grows, the limited communication bandwidth among the nodes does not permit communication of full-precision messages; hence, it is becoming increasingly common to require that messages be {\\em lossy, compressed} versions of the local parameters. The requirement of communicating compressed messages gives rise to the important question: {\\em given a fixed communication budget, what should be our communication strategy to minimize the (training) loss as much as possible?} In this paper, we explore this direction, and show that in such compressed DFL settings, there are benefits to having {\\em multiple} gossip steps between subsequent gradient iterations, even when the cost of doing so is appropriately accounted for, e.g. by means of reducing the precision of compressed information. In particular, we show that having $\\O(\\log\\frac{1}{\\epsilon})$ gradient iterations {with constant step size} - and $\\O(\\log\\frac{1}{\\epsilon})$ gossip steps between every pair of these iterations - enables convergence to within $\\epsilon$ of the optimal value for a class of non-convex problems that arise in the training of deep learning models, namely, smooth non-convex objectives satisfying Polyak-\\L{}ojasiewicz condition. Empirically, we show that our proposed scheme bridges the gap between centralized gradient descent and DFL on various machine learning tasks across different network topologies and compression operators."}}
{"id": "SSlLRUIs9e9", "cdate": 1646077526332, "mdate": null, "content": {"title": "Faster Non-Convex Federated Learning via Global and Local Momentum", "abstract": "We propose \\texttt{FedGLOMO}, a novel federated learning (FL) algorithm with an iteration complexity of $\\mathcal{O}(\\epsilon^{-1.5})$ to converge to an $\\epsilon$-stationary point (i.e., $\\mathbb{E}[\\|\\nabla f(x)\\|^2] \\leq \\epsilon$) for smooth non-convex functions -- under arbitrary client heterogeneity and compressed communication -- compared to the $\\mathcal{O}(\\epsilon^{-2})$ complexity of most prior works. Our key algorithmic idea that enables achieving this improved complexity is based on the observation that the convergence in FL is hampered by two sources of high variance: (i) the global server aggregation step with multiple local updates, exacerbated by client heterogeneity, and (ii) the noise of the local client-level stochastic gradients. The first issue is particularly detrimental to FL algorithms that perform plain averaging at the server. By modeling the server aggregation step as a generalized gradient-type update, we propose a variance-reducing momentum-based global update at the server, which when applied in conjunction with variance-reduced local updates at the clients, enables \\texttt{FedGLOMO} to enjoy an improved convergence rate. Our experiments illustrate the intrinsic variance reduction effect of \\texttt{FedGLOMO}, which implicitly suppresses client-drift in heterogeneous data distribution settings and promotes communication efficiency."}}
{"id": "FzCsiMBe6X", "cdate": 1640995200000, "mdate": 1682366334358, "content": {"title": "Beyond Uniform Lipschitz Condition in Differentially Private Optimization", "abstract": "Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded; specifically, we recommend tuning the clip norm only till values up to the minimum per-sample Lipschitz constant. This finds application in the private training of a softmax layer on top of a deep network pre-trained on public data. We verify the efficacy of our recommendation via experiments on 8 datasets. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex functions when the Lipschitz constants are unbounded but have bounded moments, i.e., they are heavy-tailed."}}
