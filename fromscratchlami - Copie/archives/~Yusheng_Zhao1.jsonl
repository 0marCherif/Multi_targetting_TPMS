{"id": "a9WVZyzqgn", "cdate": 1696336476493, "mdate": 1696336476493, "content": {"title": "Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting", "abstract": "This paper studies the problem of traffic flow\nforecasting, which aims to predict future traffic conditions on\nthe basis of road networks and traffic conditions in the past.\nThe problem is typically solved by modeling complex spatiotemporal correlations in traffic data using spatio-temporal graph\nneural networks (GNNs). However, the performance of these\nmethods is still far from satisfactory since GNNs usually have\nlimited representation capacity when it comes to complex traffic\nnetworks. Graphs, by nature, fall short in capturing non-pairwise\nrelations. Even worse, existing methods follow the paradigm\nof message passing that aggregates neighborhood information\nlinearly, which fails to capture complicated spatio-temporal highorder interactions. To tackle these issues, in this paper, we\npropose a novel model named Dynamic Hypergraph Structure\nLearning (DyHSL) for traffic flow prediction. To learn nonpairwise relationships, our DyHSL extracts hypergraph structural information to model dynamics in the traffic networks, and\nupdates each node representation by aggregating messages from\nits associated hyperedges. Additionally, to capture high-order\nspatio-temporal relations in the road network, we introduce an\ninteractive graph convolution block, which further models the\nneighborhood interaction for each node. Finally, we integrate\nthese two views into a holistic multi-scale correlation extraction\nmodule, which conducts temporal pooling with different scales\nto model different temporal patterns. Extensive experiments\non four popular traffic benchmark datasets demonstrate the\neffectiveness of our proposed DyHSL compared with a broad\nrange of competing baselines."}}
{"id": "tuR-216_6ud", "cdate": 1640995200000, "mdate": 1668745596116, "content": {"title": "Target-Driven Structured Transformer Planner for Vision-Language Navigation", "abstract": "Vision-language navigation is the task of directing an embodied agent to navigate in 3D scenes with natural language instructions. For the agent, inferring the long-term navigation target from visual-linguistic clues is crucial for reliable path planning, which, however, has rarely been studied before in literature. In this article, we propose a Target-Driven Structured Transformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware navigation. Specifically, we devise an Imaginary Scene Tokenization mechanism for explicit estimation of the long-term target (even located in unexplored environments). In addition, we design a Structured Transformer Planner which elegantly incorporates the explored room layout into a neural attention architecture for structured and global planning. Experimental results demonstrate that our TD-STP substantially improves previous best methods' success rate by 2% and 5% on the test set of R2R and REVERIE benchmarks, respectively. Our code is available at https://github.com/YushengZhao/TD-STP."}}
{"id": "S-IQWknbfaD", "cdate": 1640995200000, "mdate": 1666359043262, "content": {"title": "Target-Driven Structured Transformer Planner for Vision-Language Navigation", "abstract": "Vision-language navigation is the task of directing an embodied agent to navigate in 3D scenes with natural language instructions. For the agent, inferring the long-term navigation target from visual-linguistic clues is crucial for reliable path planning, which, however, has rarely been studied before in literature. In this article, we propose a Target-Driven Structured Transformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware navigation. Specifically, we devise an Imaginary Scene Tokenization mechanism for explicit estimation of the long-term target (even located in unexplored environments). In addition, we design a Structured Transformer Planner which elegantly incorporates the explored room layout into a neural attention architecture for structured and global planning. Experimental results demonstrate that our TD-STP substantially improves previous best methods' success rate by 2% and 5% on the test set of R2R and REVERIE benchmarks, respectively. Our code is available at https://github.com/YushengZhao/TD-STP ."}}
{"id": "W0ge5ou8t9", "cdate": 1609459200000, "mdate": 1668566199740, "content": {"title": "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding", "abstract": "Recently proposed fine-grained 3D visual grounding is an essential and challenging task, whose goal is to identify the 3D object referred by a natural language sentence from other distractive objects of the same category. Existing works usually adopt dynamic graph networks to indirectly model the intra/inter-modal interactions, making the model difficult to distinguish the referred object from distractors due to the monolithic representations of visual and linguistic contents. In this work, we exploit Transformer for its natural suitability on permutation-invariant 3D point clouds data and propose a TransRefer3D network to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. Concretely, we devise an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module to conduct fine-grained cross-modal feature matching. Facilitated by co-attention operation, our EA module matches visual entity features with linguistic entity features while RA module matches pair-wise visual relation features with linguistic relation features, respectively. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Extensive experiments on both Nr3D and Sr3D datasets demonstrate that our proposed model significantly outperforms existing approaches by up to 10.6% and claims the new state-of-the-art. To the best of our knowledge, this is the first work investigating Transformer architecture for fine-grained 3D visual grounding task."}}
{"id": "DCinAK54v79", "cdate": 1609459200000, "mdate": 1668681601674, "content": {"title": "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding", "abstract": "Recently proposed fine-grained 3D visual grounding is an essential and challenging task, whose goal is to identify the 3D object referred by a natural language sentence from other distractive objects of the same category. Existing works usually adopt dynamic graph networks to indirectly model the intra/inter-modal interactions, making the model difficult to distinguish the referred object from distractors due to the monolithic representations of visual and linguistic contents. In this work, we exploit Transformer for its natural suitability on permutation-invariant 3D point clouds data and propose a TransRefer3D network to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. Concretely, we devise an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module to conduct fine-grained cross-modal feature matching. Facilitated by co-attention operation, our EA module matches visual entity features with linguistic entity features while RA module matches pair-wise visual relation features with linguistic relation features, respectively. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Extensive experiments on both Nr3D and Sr3D datasets demonstrate that our proposed model significantly outperforms existing approaches by up to 10.6% and claims the new state-of-the-art performance. To the best of our knowledge, this is the first work investigating Transformer architecture for fine-grained 3D visual grounding task."}}
{"id": "MmisJpcI0H", "cdate": 1577836800000, "mdate": 1675578908307, "content": {"title": "Object Hider: Adversarial Patch Attack Against Object Detectors", "abstract": "Deep neural networks have been widely used in many computer vision tasks. However, it is proved that they are susceptible to small, imperceptible perturbations added to the input. Inputs with elaborately designed perturbations that can fool deep learning models are called adversarial examples, and they have drawn great concerns about the safety of deep neural networks. Object detection algorithms are designed to locate and classify objects in images or videos and they are the core of many computer vision tasks, which have great research value and wide applications. In this paper, we focus on adversarial attack on some state-of-the-art object detection models. As a practical alternative, we use adversarial patches for the attack. Two adversarial patch generation algorithms have been proposed: the heatmap-based algorithm and the consensus-based algorithm. The experiment results have shown that the proposed methods are highly effective, transferable and generic. Additionally, we have applied the proposed methods to competition \"Adversarial Challenge on Object Detection\" that is organized by Alibaba on the Tianchi platform and won top 7 in 1701 teams. Code is available at: https://github.com/FenHua/DetDak"}}
