{"id": "o4O6dzAEL9N", "cdate": 1609459200000, "mdate": 1668063408575, "content": {"title": "Fast Analytic Soft Shadows from Area Lights", "abstract": ""}}
{"id": "mvwG9lW1uF", "cdate": 1609459200000, "mdate": 1668063408570, "content": {"title": "Appearance Editing with Free-viewpoint Neural Rendering", "abstract": "We present a neural rendering framework for simultaneous view synthesis and appearance editing of a scene from multi-view images captured under known environment illumination. Existing approaches either achieve view synthesis alone or view synthesis along with relighting, without direct control over the scene's appearance. Our approach explicitly disentangles the appearance and learns a lighting representation that is independent of it. Specifically, we independently estimate the BRDF and use it to learn a lighting-only representation of the scene. Such disentanglement allows our approach to generalize to arbitrary changes in appearance while performing view synthesis. We show results of editing the appearance of a real scene, demonstrating that our approach produces plausible appearance editing. The performance of our view synthesis approach is demonstrated to be at par with state-of-the-art approaches on both real and synthetic data."}}
{"id": "wybJKnf1hxs", "cdate": 1577836800000, "mdate": 1623769762555, "content": {"title": "Geometric Scene Refocusing", "abstract": "An image captured with a wide-aperture camera exhibits a finite depth-of-field, with focused and defocused pixels. A compact and robust representation of focus and defocus helps analyze and manipulate such images. In this work, we study the fine characteristics of images with a shallow depth-of-field in the context of focal stacks. We present a composite measure for focus that is a combination of existing measures. We identify in-focus pixels, dual-focus pixels, pixels that exhibit bokeh and spatially-varying blur kernels between focal slices. We use these to build a novel representation that facilitates easy manipulation of focal stacks. We present a comprehensive algorithm for post-capture refocusing in a geometrically correct manner. Our approach can refocus the scene at high fidelity while preserving fine aspects of focus and defocus blur."}}
{"id": "EZ1eh8UWdNb", "cdate": 1546300800000, "mdate": 1623769762708, "content": {"title": "A Flexible Neural Renderer for Material Visualization", "abstract": "Photo realism in computer generated imagery is crucially dependent on how well an artist is able to recreate real-world materials in the scene. The workflow for material modeling and editing typically involves manual tweaking of material parameters and uses a standard path tracing engine for visual feedback. A lot of time may be spent in iterative selection and rendering of materials at an appropriate quality. In this work, we propose a convolutional neural network based workflow which quickly generates high-quality ray traced material visualizations on a shaderball. Our novel architecture allows for control over environment lighting and assists material selection along with the ability to render spatially-varying materials. Additionally, our network enables control over environment lighting which gives an artist more freedom and provides better visualization of the rendered material. Comparison with state-of-the-art denoising and neural rendering techniques suggests that our neural renderer performs faster and better. We provide a interactive visualization tool and release our training dataset to foster further research in this area."}}
{"id": "8O0XtgAs1h", "cdate": 1546300800000, "mdate": 1623769762708, "content": {"title": "Defocus Magnification Using Conditional Adversarial Networks", "abstract": "Defocus magnification is the process of rendering a shallow depth-of-field in an image captured using a camera with a narrow aperture. Defocus magnification is a useful tool in photography for emphasis on the subject and for highlighting background bokeh. Estimating the per-pixel blur kernel or the depth-map of the scene followed by spatially-varying re-blurring is the standard approach to defocus magnification. We propose a single-step approach that directly converts a narrow-aperture image to a wide-aperture image. We use a conditional adversarial network trained on multi-aperture images created from light-fields. We use a novel loss term based on a composite focus measure to improve generalization and show high quality defocus magnification."}}
{"id": "67TxojiKGGd", "cdate": 1546300800000, "mdate": 1623769762561, "content": {"title": "A Flexible Neural Renderer for Material Visualization", "abstract": "Photo realism in computer generated imagery is crucially dependent on how well an artist is able to recreate real-world materials in the scene. The workflow for material modeling and editing typically involves manual tweaking of material parameters and uses a standard path tracing engine for visual feedback. A lot of time may be spent in iterative selection and rendering of materials at an appropriate quality. In this work, we propose a convolutional neural network that quickly generates high-quality ray traced material visualizations on a shaderball. Our novel architecture allows for control over environment lighting which assists in material selection and also provides the ability to render spatially-varying materials. Comparison with state-of-the-art denoising and neural rendering techniques suggests that our neural renderer performs faster and better. We provide an interactive visualization tool and an extensive dataset to foster further research in this area."}}
{"id": "zbOtK0sVTOb", "cdate": 1514764800000, "mdate": 1623769762561, "content": {"title": "Structured Adversarial Training for Unsupervised Monocular Depth Estimation", "abstract": "The problem of estimating scene-depth from a single image has seen great progress lately. Recent unsupervised methods are based on view-synthesis and learn depth by minimizing photometric reconstruction error. In this paper, we introduce Structured Adversarial Training (StrAT) to this problem. We generate multiple novel views using depth (or disparity), with the stereo-baseline changing in an increasing order. Adversarial training that goes from easy examples to harder ones produces richer losses and better models. The impact of StrAT is shown to exceed traditional data augmentation using random new views. The combination of an adversarial framework, multiview learning, and structured adversarial training produces state-of-the-art performance on unsupervised depth estimation for monocular images. The StrAT framework can benefit several problems that use adversarial training."}}
{"id": "BH-twACDjP4", "cdate": 1514764800000, "mdate": 1623769762556, "content": {"title": "SLFT: A physically accurate framework for Tracing Synthetic Light Fields", "abstract": "A light field is a 4D function which captures all the radiance information of a scene. Image-based creation of light fields reconstructs the 4D space using pre-captured imagery from various views and employs refocusing to generate output images. Handheld cameras can also capture light fields using a microlens array between the sensor and the main lens, but physical constraints limit their spatial and angular resolutions. In this paper, we present a GPU based synthetic light field rendering framework that is robust and physically accurate. We demonstrate the equivalence of the standard light field camera representation with light slab representation for synthetic light fields and exhibit the capability of our framework to trace light fields of resolutions much higher than available in commercial plenoptic cameras. The light slab is rich in quality but bulky to store. Our system provides parameters to balance the quality and storage requirements. We also present a compact representation of the 4D light slabs using a video compression codec and demonstrate different quality-size combinations using these representations."}}
{"id": "skLXP0C4yiZ", "cdate": 1483228800000, "mdate": 1623769762708, "content": {"title": "SynCam: Capturing sub-frame synchronous media using smartphones", "abstract": "Smartphones have become the de-facto capture devices for everyday photography. Unlike traditional digital cameras, smartphones are versatile devices with auxiliary sensors, processing power, and networking capabilities. In this work, we harness the communication capabilities of smartphones and present a synchronous/co-ordinated multi-camera capture system. Synchronous capture is important for many image/video fusion and 3D reconstruction applications. The proposed system provides an inexpensive and effective means to capture multi-camera media for such applications. Our coordinated capture system is based on a wireless protocol that uses NTP based synchronization and device specific lag compensation. It achieves sub-frame synchronization across all participating smartphones of even heterogeneous make and model. We propose a new method based on fiducial markers displayed on an LCD screen to temporally calibrate smart-phone cameras. We demonstrate the utility and versatility of this system to enhance traditional videography and to create novel visual representations such as panoramic videos, HDR videos, multi-view 3D reconstruction, multi-flash imaging, and multi-camera social media."}}
{"id": "cSpjwkQwauR", "cdate": 1483228800000, "mdate": null, "content": {"title": "Beyond OCRs for Document Blur Estimation", "abstract": "The current document blur/quality estimation algorithms rely on the OCR accuracy to measure their success. A sharp document image, however, at times may yield lower OCR accuracy owing to factors independent of blur or quality of capture. The necessity to rely on OCR is mainly due to the difficulty in quantifying the quality otherwise. In this work, we overcome this limitation by proposing a novel dataset for document blur estimation, for which we physically quantify the blur using a capture set-up which computationally varies the focal distance of the camera. We also present a selective search mechanism to improve upon the recently successful patch-based learning approaches (using codebooks or convolutional neural networks). We present a thorough analysis of the improved blur estimation pipeline using correlation with OCR accuracy as well as the actual amount of blur. Our experiments demonstrate that our method outperforms the current state-of-the-art by a significant margin."}}
