{"id": "7uIGl1AB_M_", "cdate": 1652737781890, "mdate": null, "content": {"title": "Overparameterization from Computational Constraints", "abstract": "Overparameterized models with millions of parameters have been hugely successful. In this work, we ask:  can the need for large models be, at least in part, due to the \\emph{computational} limitations of the learner? Additionally, we ask, is this situation exacerbated for \\emph{robust} learning? We show that this indeed could be the case. We show learning tasks for which computationally bounded learners need \\emph{significantly more} model parameters than what information-theoretic learners need. Furthermore, we show that even more model parameters could be necessary for robust learning. In particular, for computationally bounded learners, we extend the recent result of Bubeck and Sellke [NeurIPS'2021] which shows that robust models might need more parameters, to the computational regime and show that bounded learners could provably need an even larger number of parameters. Then, we address the following related question: can we hope to remedy the situation for robust computationally bounded learning by restricting \\emph{adversaries} to also be computationally bounded for sake of obtaining models with fewer parameters? Here again, we show that this could be possible. Specifically, building on the work of Garg, Jha, Mahloujifar, and Mahmoody [ALT'2020], we demonstrate a learning task that can be learned efficiently and robustly against a computationally bounded attacker, while to be robust against an information-theoretic attacker requires the learner to utilize significantly more parameters."}}
{"id": "A1Y8cGB9w72", "cdate": 1621630055777, "mdate": null, "content": {"title": "A Separation Result Between Data-oblivious and Data-aware Poisoning Attacks", "abstract": "Poisoning attacks have emerged as a significant security threat to machine learning algorithms. It has been demonstrated that adversaries who make small changes to the training set, such as adding specially crafted data points, can hurt the performance of the output model. Most of these attacks require the full knowledge of training data. This leaves open the possibility of achieving the same attack results using poisoning attacks that do not have the full knowledge of the clean training set.\nIn this work, we initiate a theoretical study of the problem above. Specifically, for the case of feature selection with LASSO, we show that \\emph{full information} adversaries (that craft poisoning examples based on the rest of the training data) are provably much more devastating compared to the optimal attacker that is \\emph{oblivious} to the training set yet has access to the distribution of the data.  Our separation result shows that the two settings of data-aware and data-oblivious are fundamentally different and we cannot hope to achieve the same attack or defense results in these scenarios. "}}
{"id": "nIoxaeLG0je", "cdate": 1609459200000, "mdate": null, "content": {"title": "Reusable Two-Round MPC from LPN", "abstract": ""}}
{"id": "z_DefwrQuf6", "cdate": 1577836800000, "mdate": null, "content": {"title": "An Attack on InstaHide: Is Private Learning Possible with Instance Encoding?", "abstract": "A private machine learning algorithm hides as much as possible about its training data while still preserving accuracy. In this work, we study whether a non-private learning algorithm can be made private by relying on an instance-encoding mechanism that modifies the training inputs before feeding them to a normal learner. We formalize both the notion of instance encoding and its privacy by providing two attack models. We first prove impossibility results for achieving a (stronger) model. Next, we demonstrate practical attacks in the second (weaker) attack model on InstaHide, a recent proposal by Huang, Song, Li and Arora [ICML'20] that aims to use instance encoding for privacy."}}
{"id": "yxRyORqGV2y", "cdate": 1577836800000, "mdate": null, "content": {"title": "Formalizing Data Deletion in the Context of the Right to be Forgotten", "abstract": ""}}
{"id": "wzbxkEU7GVF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Two-Round Oblivious Transfer from CDH or LPN", "abstract": "We show a new general approach for constructing maliciously-secure two-round oblivious transfer (OT). Specifically, we provide a generic sequence of transformations to upgrade a very basic notion of two-round OT, which we call elementary OT, to UC-secure OT. We then give simple constructions of elementary OT under the Computational Diffie-Hellman (CDH) assumption or the Learning Parity with Noise (LPN) assumption, yielding the first constructions of malicious (UC-secure) two-round OT under these assumptions. Since two-round OT is complete for two-round 2-party and multi-party computation in the malicious setting, we also achieve the first constructions of the latter under these assumptions."}}
{"id": "wmxxOGs1yrp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Formalizing Data Deletion in the Context of the Right to be Forgotten", "abstract": "The right of an individual to request the deletion of their personal data by an entity that might be storing it -- referred to as the right to be forgotten -- has been explicitly recognized, legislated, and exercised in several jurisdictions across the world, including the European Union, Argentina, and California. However, much of the discussion surrounding this right offers only an intuitive notion of what it means for it to be fulfilled -- of what it means for such personal data to be deleted. In this work, we provide a formal definitional framework for the right to be forgotten using tools and paradigms from cryptography. In particular, we provide a precise definition of what could be (or should be) expected from an entity that collects individuals' data when a request is made of it to delete some of this data. Our framework captures several, though not all, relevant aspects of typical systems involved in data processing. While it cannot be viewed as expressing the statements of current laws (especially since these are rather vague in this respect), our work offers technically precise definitions that represent possibilities for what the law could reasonably expect, and alternatives for what future versions of the law could explicitly require. Finally, with the goal of demonstrating the applicability of our framework and definitions, we consider various natural and simple scenarios where the right to be forgotten comes up. For each of these scenarios, we highlight the pitfalls that arise even in genuine attempts at implementing systems offering deletion guarantees, and also describe technological solutions that provably satisfy our definitions. These solutions bring together techniques built by various communities."}}
{"id": "vNmzrZg1x-", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reusable Two-Round MPC from DDH", "abstract": ""}}
{"id": "lTdDtWp7pZT", "cdate": 1577836800000, "mdate": null, "content": {"title": "Black-Box Constructions of Bounded-Concurrent Secure Computation", "abstract": ""}}
{"id": "k3hhJ0X687a", "cdate": 1577836800000, "mdate": null, "content": {"title": "FHE-Based Bootstrapping of Designated-Prover NIZK", "abstract": ""}}
