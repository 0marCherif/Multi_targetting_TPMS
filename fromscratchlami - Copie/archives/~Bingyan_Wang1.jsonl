{"id": "J2YvvXDp7H", "cdate": 1621630345849, "mdate": null, "content": {"title": "Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model", "abstract": "The curse of dimensionality is a widely known issue in reinforcement learning (RL). In the tabular setting where the state space $\\mathcal{S}$ and the action space $\\mathcal{A}$ are both finite, to obtain a near optimal policy with sampling access to a generative model, the minimax optimal sample complexity scales linearly with $|\\mathcal{S}|\\times|\\mathcal{A}|$, which can be prohibitively large when $\\mathcal{S}$ or $\\mathcal{A}$ is large. This paper considers a Markov decision process (MDP) that admits a set of state-action features, which can linearly express (or approximate) its probability transition kernel. We show that a model-based approach (resp.$~$Q-learning) provably learns an $\\varepsilon$-optimal policy (resp.$~$Q-function) with high probability as soon as the sample size exceeds the order of $\\frac{K}{(1-\\gamma)^{3}\\varepsilon^{2}}$ (resp.$~$$\\frac{K}{(1-\\gamma)^{4}\\varepsilon^{2}}$), up to some logarithmic factor. Here $K$ is the feature dimension and $\\gamma\\in(0,1)$ is the discount factor of the MDP. Both sample complexity bounds are provably tight, and our result for the model-based approach matches the minimax lower bound. Our results show that for arbitrarily large-scale MDP, both the model-based approach and Q-learning are sample-efficient when $K$ is relatively small, and hence the title of this paper."}}
{"id": "uEutO2BBbSs", "cdate": 1577836800000, "mdate": null, "content": {"title": "Convex and Nonconvex Optimization Are Both Minimax-Optimal for Noisy Blind Deconvolution", "abstract": "We investigate the effectiveness of convex relaxation and nonconvex optimization in solving bilinear systems of equations under two different designs (i.e.$~$a sort of random Fourier design and Gaussian design). Despite the wide applicability, the theoretical understanding about these two paradigms remains largely inadequate in the presence of random noise. The current paper makes two contributions by demonstrating that: (1) a two-stage nonconvex algorithm attains minimax-optimal accuracy within a logarithmic number of iterations. (2) convex relaxation also achieves minimax-optimal statistical accuracy vis-\\`a-vis random noise. Both results significantly improve upon the state-of-the-art theoretical guarantees."}}
