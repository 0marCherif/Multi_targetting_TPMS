{"id": "g_BjLtjtCwT", "cdate": 1668734779564, "mdate": null, "content": {"title": "Probabilistically Robust PAC Learning", "abstract": "Recently, Robey et al. propose a notion of probabilistic robustness, which, at a high-level, requires a classifier to be robust to most but not all perturbations. They show that for certain hypothesis classes where proper learning under worst-case robustness is \\textit{not} possible, proper learning under probabilistic robustness \\textit{is} possible with sample complexity exponentially smaller than in the worst-case robustness setting. This motivates the question of whether proper learning under probabilistic robustness is always possible. In this paper, we show that this is \\textit{not} the case. We exhibit examples of hypothesis classes $\\mathcal{H}$ with finite VC dimension that are \\textit{not} probabilistically robustly PAC learnable with \\textit{any} proper learning rule."}}
