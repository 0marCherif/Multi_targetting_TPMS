{"id": "97533cC788", "cdate": 1685577600000, "mdate": 1683882342212, "content": {"title": "Extreme Low-Resolution Action Recognition with Confident Spatial-Temporal Attention Transfer", "abstract": "Action recognition on extreme low-resolution videos, e.g., a resolution of $$12 \\times 16$$ 12 \u00d7 16 pixels, plays a vital role in far-view surveillance and privacy-preserving multimedia analysis. As low-resolution videos often only contain limited information, it is difficult for us to perform action recognition in them. Given the fact that one same action may be represented by videos in both high resolution (HR) and extreme low resolution (eLR), it is worth studying to utilize the relevant HR data to improve the eLR action recognition. In this work, we propose a novel Confident Spatial-Temporal Attention Transfer (CSTAT) for eLR action recognition. CSTAT acquires information from HR data by reducing the attention differences with a transfer-learning strategy. Besides, the confidence of the supervisory signal is also taken into consideration for a more reliable transferring process. Experimental results demonstrate that, the proposed method can effectively improve the accuracy of eLR action recognition and achieve state-of-the-art performances on $$12\\times 16$$ 12 \u00d7 16 HMDB51, $$12\\times 16$$ 12 \u00d7 16 Kinects-400, and $$12\\times 16$$ 12 \u00d7 16 Something-Something v2."}}
{"id": "N-EnKp42ZR", "cdate": 1680307200000, "mdate": 1683983544021, "content": {"title": "RadarVerses in Metaverses: A CPSI-Based Architecture for 6S Radar Systems in CPSS", "abstract": "Metaverses have caused significant changes in the industry and their academic foundation can be traced back to the term cyber\u2013physical\u2013social systems (CPSS), which was proposed in 2010. Radar is an important sensor in sensing systems that are widely applied in many fields, especially, in autonomous driving. To deal with the complex environment, smart radars with real-time information processing capabilities are required. Human factors play a critical role in the operation and management of radar systems, thus, digital twins\u2019 radars in cyber\u2013physical systems (CPS) are unable to achieve intelligence in CPSS due to an incomplete consideration of human involvement. For this consideration, we propose a novel framework of RadarVerses for smart radars in metaverses based on ACP-based parallel intelligence, which is also known as cyber\u2013physical\u2013social intelligence (CPSI). RadarVerses consist of five main parts which are physical radars, descriptive radars, predictive radars, prescriptive radars, and deep radars. To construct RadarVerses at the technical level, we introduce four main technical foundations: 1) communication technology; 2) scenarios engineering; 3) foundation models; and 4) digital workers. In addition, we also provide a case study about LiDARs\u2019 predictive maintenance of accumulated snow in RadarVerses."}}
{"id": "JPQAcUMPK_1", "cdate": 1677628800000, "mdate": 1683983544059, "content": {"title": "MSFANet: A Light Weight Object Detector Based on Context Aggregation and Attention Mechanism for Autonomous Mining Truck", "abstract": "Accurate and reliable object detection is a fundamental component of perception system for autonomous driving. Specially, in some circumstances like autonomous driving in surface mine, there is a fact that the particularity of scene brings tremendous challenges for object detection with a series of problems caused by the multi-scale and camouflaged objects. In this paper, a multi-scale feature fusion and attention based multi-branches framework was proposed to improve the performance of object detection for above problems called MSFANet. In the proposed MSFANet, a multi-scale feature fusion module, which was used to capture the rich context features for multi-scale high level feature maps, and a multi-scale attention module, which was used to enhance the feature saliency of objects with different scales, were designed. What's more, to improve the performance of multi-scale object detection, we build 4 different prediction branches for large, medium small and smaller scale objects respectively. At last, we built our own dataset for automatic driving in surface mine called SurMine and test the model at our own datasets and KITTI benchmark. It achieved 82.7 mAP(%) and 92.57 mAP(%) in 32 36 ms on a TITAN RTX, compared to 80.2 mAP(%) and 87.83 mAP(%) in 28 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">${\\sim }$</tex-math></inline-formula> 34 ms by YOLOv7 on SurMine and KITTI benchmarks."}}
{"id": "GamwuaqmNIv", "cdate": 1677628800000, "mdate": 1683983544186, "content": {"title": "Refined Crack Detection via LECSFormer for Autonomous Road Inspection Vehicles", "abstract": "Due to the rising cost of human resources in road maintenance and the pursuit of efficiency, autonomous road inspection vehicles are developed for intelligent detection of road disease to prevent severe traffic disasters in the early stages. Nevertheless, as a prevalent road disease, road cracks are diverse and susceptible to shadows, weather changes, and noise in data acquisition. Moreover, they usually appear with thin shapes that are hard to detect correctly by existing methods. To handle this problem, more details of the road cracks need to be better analyzed. In this article, we propose a refined road crack detection method named locally enhanced cross-shaped windows transformer (LECSFormer), which adopts a delicate design of the encoder-decoder structure. The encoder employs window-based transformer blocks to model long-range dependencies. Each transformer block ensembles the locally enhanced module to enrich the local contextual information, and token shuffle operation is applied to build cross-windows connections. The decoder uses dense connections to fuse multi-scale information, and the feature fusion module fuses hierarchical features and reweights them by the channel attention mechanism. The proposed method outperforms other state-of-the-art methods with ODS of 0.963, 0.917, 0.952, and 0.953 on four challenging datasets, CrackTree260, CrackLS315, Stone331, and CRKWH100. It can accurately detect cracks in road surfaces and support intelligent preventive maintenance of roads."}}
{"id": "u_V5ECc9Bo", "cdate": 1675209600000, "mdate": 1683983544257, "content": {"title": "The Use of Intelligent Vehicles and Artificial Intelligence in Mining Operations: Ethics, Responsibility, and Sustainability", "abstract": "This letter is resulted from IEEE TIV's Decentralized and Hybrid Workshops (DHW) on Autonomous Mining (AM). We have already conducted 2 distributed/decentralized and hybrid symposia (DHS), 5 DHWs, and more than 10 seminars on AM in the past year. The following is a brief summary on the key components from our DHS, DHWs, and Seminars about ethics, responsibility, and sustainability in intelligent vehicles (IVs) and artificial intelligence (AI) for AM (Wang, 2022), (Cao et al., 2022)."}}
{"id": "NHwbZjdNJcm", "cdate": 1675209600000, "mdate": 1683983544122, "content": {"title": "MetaScenario: A Framework for Driving Scenario Data Description, Storage and Indexing", "abstract": "Autonomous driving related researches require the analysis and usage of massive amounts of driving scenario data. Compared to raw data collected by sensors, scenario data provide a preliminary abstraction of driving tasks and processes, explicitly integrate information about the road environment and the dynamic and static attributes of traffic participants, making it easier to conduct task understanding and decision making. However, many existing driving scenario datasets have the following two problems. First, it is not clear which data fields need to be recorded for driving scenarios. The data storage formats and organization standards are inconsistent. Second, the datasets cannot establish driving scenario indexing effectively. Existing datasets are sparsely annotated and difficult to index, which is detrimental to data sampling and extraction for machine learning process, thus hindering efficient fusion and reuse. In this paper, we propose MetaScenario, a framework for driving scenario data. We describe driving scenarios and design the centralized and unified data framework for the storage, processing, and indexing of scenario data based on relational database. The concept of atom scenario is proposed and characterized using semantic graphs. We also annotate and classify behaviors and interactions of traffic participants in atom scenarios by extracting the spatiotemporal evolution of semantic information. The annotation facilitates the indexing and extraction of data. The scenario datasets are further evaluated via the data distribution and annotation statistics. MetaScenario can provide researchers with convenient tools for scenario data extraction and important analytical references."}}
{"id": "IlspGGHDy0", "cdate": 1675209600000, "mdate": 1683983544038, "content": {"title": "Milestones in Autonomous Driving and Intelligent Vehicles: Survey of Surveys", "abstract": "Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing at a rapid pace due to the convenience, safety, and economic benefits. Although a number of surveys have reviewed research achievements in this field, they are still limited in specific tasks, lack of systematic summary and research directions in the future. Here we propose a Survey of Surveys (SoS) for total technologies of AD and IVs that reviews the history, summarizes the milestones, and provides the perspectives, ethics, and future research directions. To our knowledge, this article is the first SoS with milestones in AD and IVs, which constitutes our complete research work together with two other technical surveys. We anticipate that this article will bring novel and diverse insights to researchers and abecedarians, and serve as a bridge between past and future."}}
{"id": "BJhIrDt-iT6", "cdate": 1675209600000, "mdate": 1683983544179, "content": {"title": "Gaze Control for Active Visual SLAM via Panoramic Cost Map", "abstract": "In this work, we aim to improve the positioning accuracy of the visual simultaneous localization and mapping (VSLAM) through actively controlling the gaze of the positioning camera mounted on an autonomous guided vehicle (AGV). A panoramic cost map (PCM)-based gaze control method (PGC) is proposed for the active VSLAM. Different from traditional method, a panoramic camera is added beside the positioning camera to aid the gaze control of the positioning camera. The panoramic camera is used to perceive the environment and evaluate the potential performance of each available orientation of the positioning camera. The evaluation of all the available orientations will make up a panoramic cost map. The cost map is then used to help the gaze control method to select an optimal target gaze for the positioning camera. In the calculation of the panoramic cost map, the effective factors of the VSLAM, such as feature points and moving objects, are taken into consideration. In the gaze control method, we also take into consideration of errors of the system, the time delay of the proposed method, and the velocity of the AGV. The test results in different scenes with different VSLAM algorithms show that the proposed method can improve the positioning accuracy of all the tested VSLAM algorithms compared to fixed camera gaze."}}
{"id": "xohKQKnJUhN", "cdate": 1672531200000, "mdate": 1683983544039, "content": {"title": "Hierarchical Interpretable Imitation Learning for End-to-End Autonomous Driving", "abstract": "End-to-end autonomous driving provides a simple and efficient framework for autonomous driving systems, which can directly obtain control commands from raw perception data. However, it fails to address stability and interpretability problems in complex urban scenarios. In this paper, we construct a two-stage end-to-end autonomous driving model for complex urban scenarios, named HIIL (Hierarchical Interpretable Imitation Learning), which integrates interpretable BEV mask and steering angle to solve the problems shown above. In Stage One, we propose a pretrained Bird's Eye View (BEV) model which leverages a BEV mask to present an interpretation of the surrounding environment. In Stage Two, we construct an Interpretable Imitation Learning (IIL) model that fuses BEV latent feature from Stage One with an additional steering angle from Pure-Pursuit (PP) algorithm. In the HIIL model, visual information is converted to semantic images by the semantic segmentation network, and the semantic images are encoded to extract the BEV latent feature, which are decoded to predict BEV masks and fed to the IIL as perception data. In this way, the BEV latent feature bridges the BEV and IIL models. Visual information can be supplemented by the calculated steering angle for PP algorithm, speed vector, and location information, thus it could have better performance in complex and terrible scenarios. Our HIIL model meets an urgent requirement for interpretability and robustness of autonomous driving. We validate the proposed model in the CARLA simulator with extensive experiments which show remarkable interpretability, generalization, and robustness capability in unknown scenarios for navigation tasks."}}
{"id": "k-GGqq3lqSz", "cdate": 1672531200000, "mdate": 1683983544037, "content": {"title": "Sim2real and Digital Twins in Autonomous Driving: A Survey", "abstract": "Safety and cost are two important concerns for the development of autonomous driving technologies. From the academic research to commercial applications of autonomous driving vehicles, sufficient simulation and real world testing are required. In general, a large scale of testing in simulation environment is conducted and then the learned driving knowledge is transferred to the real world, so how to adapt driving knowledge learned in simulation to reality becomes a critical issue. However, the virtual simulation world differs from the real world in many aspects such as lighting, textures, vehicle dynamics, and agents' behaviors, etc., which makes it difficult to bridge the gap between the virtual and real worlds. This gap is commonly referred to as the reality gap (RG). In recent years, researchers have explored various approaches to address the reality gap issue, which can be broadly classified into three categories: transferring knowledge from simulation to reality (sim2real), learning in digital twins (DTs), and learning by parallel intelligence (PI) technologies. In this paper, we consider the solutions through the sim2real, DTs, and PI technologies, and review important applications and innovations in the field of autonomous driving. Meanwhile, we show the state-of-the-arts from the views of algorithms, models, and simulators, and elaborate the development process from sim2real to DTs and PI. The presentation also illustrates the far-reaching effects and challenges in the development of sim2real, DTs, and PI in autonomous driving."}}
