{"id": "191LgBmXYr", "cdate": 1640995200000, "mdate": 1665915344288, "content": {"title": "C2L: Causally Contrastive Learning for Robust Text Classification", "abstract": "Despite the super-human accuracy of recent deep models in NLP tasks, their robustness is reportedly limited due to their reliance on spurious patterns. We thus aim to leverage contrastive learning and counterfactual augmentation for robustness. For augmentation, existing work either requires humans to add counterfactuals to the dataset or machines to automatically matches near-counterfactuals already in the dataset. Unlike existing augmentation is affected by spurious correlations, ours, by synthesizing \u201ca set\u201d of counterfactuals, and making a collective decision on the distribution of predictions on this set, can robustly supervise the causality of each term. Our empirical results show that our approach, by collective decisions, is less sensitive to task model bias of attribution-based synthesis, and thus achieves significant improvements, in diverse dimensions: 1) counterfactual robustness, 2) cross-domain generalization, and 3) generalization from scarce data."}}
{"id": "lnwcBjiNBeG", "cdate": 1609459200000, "mdate": 1665915344296, "content": {"title": "Counterfactual Generative Smoothing for Imbalanced Natural Language Classification", "abstract": "Classification datasets are often biased in observations, leaving onlya few observations for minority classes. Our key contribution is de-tecting and reducing Under-represented (U-) and Over-represented(O-) artifacts from dataset imbalance, by proposing a Counterfac-tual Generative Smoothing approach on both feature-space anddata-space, namely CGS_f and CGS_d. Our technical contribution issmoothing majority and minority observations, by sampling a ma-jority seed and transferring to minority. Our proposed approachesnot only outperform state-of-the-arts in both synthetic and real-lifedatasets, they effectively reduce both artifact types."}}
{"id": "QSXxwsaGI9", "cdate": 1609459200000, "mdate": 1665915344286, "content": {"title": "Label and Context Augmentation for Response Selection at DSTC8", "abstract": "This paper studies the dialogue response selection task. As state-of-the-arts are neural models requiring a large training set, data augmentation has been considered as a means to overcome the sparsity of observational annotation, where only one observed response is annotated as gold. In this paper, we first consider label augmentation, of selecting, among unobserved utterances, that would \u201ccounterfactually\u201d replace the labeled response, for the given context, and augmenting labels only if that is the case. The key advantage of this model is not incurring human annotation overhead, thus not increasing the training cost, i.e., for low-resource scenarios. In addition, we consider context augmentation scenarios where the given dialogue context is not sufficient for label augmentation. In this case, inspired by open-domain question answering, we \u201cdecontextualize\u201d by retrieving missing contexts, such as related persona. We empirically show that our pipeline improves BERT-based models in two different response selection tasks without incurring annotation overheads."}}
{"id": "Dgl8dQUTiQG", "cdate": 1609459200000, "mdate": 1665915344287, "content": {"title": "Structure-Augmented Keyphrase Generation", "abstract": ""}}
{"id": "V6lHSAolpN", "cdate": 1577836800000, "mdate": 1665915344289, "content": {"title": "Conditional Response Augmentation for Dialogue Using Knowledge Distillation", "abstract": "This paper studies dialogue response selection task. As state-of-the-arts are neural models requiring a large training set, data augmentation is essential to overcome the sparsity of observational annotation, where one observed response is annotated as gold. In this paper, we propose counterfactual augmentation, of considering whether unobserved utterances would \u201ccounterfactually\u201d replace the labelled response, for the given context, and augment only if that is the case. We empirically show that our pipeline improves BERT-based models in two different response selection tasks without incurring annotation overheads."}}
{"id": "Q65vLcZlavA", "cdate": 1577836800000, "mdate": 1665915344286, "content": {"title": "Label-Efficient Training for Next Response Selection", "abstract": ""}}
