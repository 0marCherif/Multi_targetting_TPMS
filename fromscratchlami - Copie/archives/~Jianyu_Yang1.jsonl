{"id": "tlDQSM1pVyL", "cdate": 1668765313656, "mdate": null, "content": {"title": "MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video", "abstract": "Recent transformer-based solutions have been introduced to estimate 3D human pose from 2D keypoint sequence by considering body joints among all frames globally to learn spatio-temporal correlation. We observe that the motions of different joints differ significantly. However, the previous methods cannot efficiently model the solid inter-frame correspondence of each joint, leading to insufficient learning of spatial-temporal correlation. We propose MixSTE (Mixed Spatio-Temporal Encoder), which has a temporal transformer block to separately model the temporal motion of each joint and a spatial transformer block to learn inter-joint spatial correlation. These two blocks are utilized alternately to obtain better spatio-temporal feature encoding. In addition, the network output is extended from the central frame to entire frames of the input video, thereby improving the coherence between the input and output sequences. Extensive experiments are conducted on three benchmarks (Human3.6M, MPI-INF-3DHP, and HumanEva). The results show that our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and 7.6% MPJPE. The code is available at https://github.com/JinluZhang1126/MixSTE."}}
{"id": "rUHGgW-3SNQ", "cdate": 1609459200000, "mdate": 1631878534571, "content": {"title": "Vision Based Hand Gesture Recognition Using 3D Shape Context", "abstract": "Hand gesture recognition is a popular topic in computer vision and makes human-computer interaction more flexible and convenient. The representation of hand gestures is critical for recognition. In this paper, we propose a new method to measure the similarity between hand gestures and exploit it for hand gesture recognition. The depth maps of hand gestures captured via the Kinect sensors are used in our method, where the 3D hand shapes can be segmented from the cluttered backgrounds. To extract the pattern of salient 3D shape features, we propose a new descriptor-3D Shape Context, for 3D hand gesture representation. The 3D Shape Context information of each 3D point is obtained in multiple scales because both local shape context and global shape distribution are necessary for recognition. The description of all the 3D points constructs the hand gesture representation, and hand gesture recognition is explored via dynamic time warping algorithm. Extensive experiments are conducted on multiple benchmark datasets. The experimental results verify that the proposed method is robust to noise, articulated variations, and rigid transformations. Our method outperforms state-of-the-art methods in the comparisons of accuracy and efficiency."}}
{"id": "hpNNozxfRrP", "cdate": 1609459200000, "mdate": 1631878534868, "content": {"title": "A multi-scale descriptor for real time RGB-D hand gesture recognition", "abstract": "Highlights \u2022 A new RGB-D shape-based hand gesture recognition method is proposed. \u2022 A new hand shape descriptor is proposed with emphasized finger feature. \u2022 Different recognition engines are explored and compared for different applications. \u2022 Sota accuracies are achieved on several benchmarks, as well as excellent efficiency. \u2022 A demo video is given for a real life application of human-computer interaction. Abstract The development of depth cameras, e.g., the Kinect sensor, provides new opportunities for human computer interaction (HCI). Although the Kinect sensor has been extensively applied for human tracking, human action recognition and hand gesture recognition, real time hand gesture recognition is still a challenging problem. In this paper, a new real time hand gesture recognition method is proposed. Since fingers are the most important clue for hand gesture classification, a finger-emphasized multi-scale descriptor is proposed. The proposed descriptor incorporates three types of parameters of multiple scales to make a discriminative representation of the hand shape. Furthermore, the features of fingers are emphasized for hand gesture analysis. Three solutions to hand gesture recognition are then investigated with DTW, SVM, and neural network. Extensive experiments are conducted and the results show that the proposed method is robust to noise, articulations and rigid transformations. The comparison with state-of-the-art methods verifies the accuracy and efficiency of our method."}}
{"id": "fV9Q4zIY9B0", "cdate": 1609459200000, "mdate": 1631878534871, "content": {"title": "FastHand: Fast Hand Pose Estimation From A Monocular Camera", "abstract": "Hand pose estimation is a fundamental task in many human-robot interaction-related applications. However, previous approaches suffer from unsatisfying hand landmark predictions in real-world scenes and high computation burden. This paper proposes a fast and accurate framework for hand pose estimation, dubbed as \"FastHand\". Using a lightweight encoder-decoder network architecture, FastHand fulfills the requirements of practical applications running on embedded devices. The encoder consists of deep layers with a small number of parameters, while the decoder makes use of spatial location information to obtain more accurate results. The evaluation took place on two publicly available datasets demonstrating the improved performance of the proposed pipeline compared to other state-of-the-art approaches. FastHand offers high accuracy scores while reaching a speed of 25 frames per second on an NVIDIA Jetson TX2 graphics processing unit."}}
{"id": "H8aSjRBh1GU", "cdate": 1609459200000, "mdate": 1631878534865, "content": {"title": "Hierarchical Soft Quantization for Skeleton-Based Human Action Recognition", "abstract": "In daily life, human beings rely on hands and body parts to complete particular actions cooperatively. These selected body parts and their cooperative relationships are essential cues to distinguish these actions. However, most existing action recognition methods, which try to model the body appearance or spatial relations in skeleton sequences, often ignore the essential cooperation relationship among joints. Differently, in this paper, we propose a spatio-temporal hierarchical soft quantization method to extract the congenerous motion features, which reflect the cooperation relations among joints and body parts. Specifically, we design a hierarchical network with multiple soft quantization layers to extract congenerous features. The hierarchical network not only models the spatial hierarchy of skeleton structure for joint, part, and body, but also extracts the temporal hierarchy with sliding windows for frame, fragment, and sequence. Moreover, the features in each layer are visually explainable, which reflect the cooperation among body parts. The trainable parameters in the network are also significantly reduced, which reduces computational cost. Extensive experiments conducted on four benchmarks demonstrate that our method can provide competitive results compared with state-of-the-arts. The visualized congenerous features also validate that our approach can effectively perceive the essential cooperation relations."}}
{"id": "lnc95EXDGK", "cdate": 1577836800000, "mdate": 1631878534576, "content": {"title": "Discriminative Pose Analysis for Human Action Recognition", "abstract": "Analysis of pose-level information is critical for human action recognition. For fast visual perception, we propose to encode human pose with discriminative information in the pose feature space, which needs low computational cost. The main idea is that the common poses that exist in different actions have low discrimination while the discriminative poses exist in few actions. Therefore, we employ the Gaussian Mixture Model to cluster the feature vectors of all poses in all classes of actions, and then quantify the discrimination of each pose as well as its contribution to each action class. Meanwhile, we calculate the correlation between the clusters and action classes as a rating matrix. The query action is inferred by simply calculating the Gaussion weights of poses and multiply the rating matrix. The framework runs without deep neural networks, which is low cost for application on low power devices. The proposed method is validated on three benchmark human action datasets, and the experimental results show good performances of the proposed method. The inferring time of each query is only 24ms on a 2.5 GHz CPU that indicats the efficiency of our method."}}
{"id": "kz5fvWcm2SZ", "cdate": 1577836800000, "mdate": 1631878534619, "content": {"title": "Video Summarization with a Dual Attention Capsule Network", "abstract": "In this paper, we address the problem of video summarization, which aims at selecting a subset of video frames as a summary to represent the original video contents compactly and completely. We propose a simple but effective supervised approach with a dual attention capsule network towards this end. Unlike existing LSTM based methods, it pays attention to short- and long-term dependencies among video frames through an elaborate dual self-attention architecture, which can handle longer-term dependencies and admit parallel computing. To reconcile the outputs of dual self-attention, we rely on a two-stream capsule network to learn the underlying frame selection criteria. Experiments on real-world datasets show the advantages of the proposed approach compared with state-of-the-art methods."}}
{"id": "eRBVS-hKc5P", "cdate": 1577836800000, "mdate": 1631878534642, "content": {"title": "Selective Complementary Features For Multi-Person Pose Estimation", "abstract": "Multi-person pose estimation is a fundamental yet challenging research topic for many computer vision applications. It is difficult to achieve accurate localization results due to occlusion and complex background. In this paper, we propose a novel multi-person pose estimation approach with information complement and attention refinement residual module. To recover occlusion, the complementary features with multi-scale semantics information are extracted by our proposed Information Complement Module (ICM). To effectively discover the channel relationship and selectively highlight task-related regions in the feature maps, we design an Attention Refinement Residual Bottleneck (ARRB) module, which is an extension of residual unit with attention mechanism. We conduct ablation studies to investigate the efficacy of our method and compare it with the state-of-the-art methods on the COCO keypoint benchmark. Experimental results demonstrate that the selective complementary features are effective for multi-person pose estimation."}}
{"id": "-Iusjx2eNn", "cdate": 1577836800000, "mdate": null, "content": {"title": "SIRI: Spatial Relation Induced Network For Spatial Description Resolution", "abstract": "Spatial Description Resolution, as a language-guided localization task, is proposed for target location in a panoramic street view, given corresponding language descriptions. Explicitly characterizing an object-level relationship while distilling spatial relationships are currently absent but crucial to this task. Mimicking humans, who sequentially traverse spatial relationship words and objects with a first-person view to locate their target, we propose a novel spatial relationship induced (SIRI) network. Specifically, visual features are firstly correlated at an implicit object-level in a projected latent space; then they are distilled by each spatial relationship word, resulting in each differently activated feature representing each spatial relationship. Further, we introduce global position priors to fix the absence of positional information, which may result in global positional reasoning ambiguities. Both the linguistic and visual features are concatenated to finalize the target localization. Experimental results on the Touchdown show that our method is around 24\\% better than the state-of-the-art method in terms of accuracy, measured by an 80-pixel radius. Our method also generalizes well on our proposed extended dataset collected using the same settings as Touchdown. The code for this project is publicly available at https://github.com/wong-puiyiu/siri-sdr."}}
{"id": "SHJW5FJ9yXq", "cdate": 1546300800000, "mdate": 1631878534569, "content": {"title": "Spatio-Temporal Multi-scale Soft Quantization Learning for Skeleton-Based Human Action Recognition", "abstract": "Effective feature representation is important for action recognition. In this paper, a novel soft quantization learning method is proposed to represent visual features for action recognition. Specifically, we propose a dual multi-scale soft-quantization network, which is a trainable quantizer using RBF neurons. The RBF layer includes dual multi-scale structure, namely a three-level hierarchical skeleton structure in space, and a temporal-pyramid based multi-scale time structure. Different spatial levels in the RBF layer have respective RBF neurons for hierarchical spatial information, while the temporal scales share them to reduce the number of parameters in the network. An accumulation layer following the RBF layer summarizes the RBF output as a histogram representation for classification task. The proposed method is end-to-end differentiable that can be trained using regular back-propagation. The conducted experiments on benchmark datasets verify that the proposed method outperforms state-of-the-art methods."}}
