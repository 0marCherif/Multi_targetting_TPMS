{"id": "xTVSGS1Mc2", "cdate": 1649257561916, "mdate": 1649257561916, "content": {"title": "Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline", "abstract": "Processing point cloud data is an important component of many real-world systems. As such, a\nwide variety of point-based approaches have been\nproposed, reporting steady benchmark improvements over time. We study the key ingredients\nof this progress and uncover two critical results.\nFirst, we find that auxiliary factors like different evaluation schemes, data augmentation strategies, and loss functions, which are independent\nof the model architecture, make a large difference in performance. The differences are large\nenough that they obscure the effect of architecture. When these factors are controlled for, PointNet++, a relatively older network, performs competitively with recent methods. Second, a very\nsimple projection-based method, which we refer to as SimpleView, performs surprisingly well.\nIt achieves on par or better results than sophisticated state-of-the-art methods on ModelNet40\nwhile being half the size of PointNet++. It also\noutperforms state-of-the-art methods on ScanObjectNN, a real-world point cloud benchmark, and\ndemonstrates better cross-dataset generalization.\nCode is available at https://github.com/\nprinceton-vl/SimpleView."}}
{"id": "XwATtbX3oCz", "cdate": 1601308015742, "mdate": null, "content": {"title": "Revisiting Point Cloud Classification with a Simple and Effective Baseline", "abstract": "Processing point cloud data is an important component of many real-world systems. As such, a wide variety of point-based approaches have been proposed, reporting steady benchmark improvements over time. We study the key ingredients of this progress and uncover two critical results. First, we find that auxiliary factors like different evaluation schemes, data augmentation strategies, and loss functions, which are independent of the model architecture, make a large difference in performance. The differences are large enough that they obscure the effect of architecture. When these factors are controlled for, PointNet++, a relatively older network, performs competitively with recent methods. Second, a very simple projection-based method, which we refer to as SimpleView, performs surprisingly well. It achieves on par or better results than sophisticated state-of-the-art methods on ModelNet40, while being half the size of PointNet++. It also outperforms state-of-the-art methods on ScanObjectNN, a real-world point cloud benchmark, and demonstrates better cross-dataset generalization.\n"}}
{"id": "QYwRxgcB2Fi", "cdate": 1577836800000, "mdate": null, "content": {"title": "How Useful is Self-Supervised Pretraining for Visual Tasks?", "abstract": "Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at \\href{https://www.github.com/princeton-vl/selfstudy}{github.com/princeton-vl/selfstudy}."}}
{"id": "B1eoyAVFwH", "cdate": 1569439203127, "mdate": null, "content": {"title": "Feature Partitioning for Efficient Multi-Task Architectures", "abstract": "Multi-task learning promises to use less data, parameters, and time than training separate single-task models. But realizing these benefits in practice is challenging. In particular, it is difficult to define a suitable architecture that has enough capacity to support many tasks while not requiring excessive compute for each individual task. There are difficult trade-offs when deciding how to allocate parameters and layers across a large set of tasks. To address this, we propose a method for automatically searching over multi-task architectures that accounts for resource constraints. We define a parameterization of feature sharing strategies for effective coverage and sampling of architectures. We also present a method for quick evaluation of such architectures with feature distillation. Together these contributions allow us to quickly optimize for parameter-efficient multi-task models. We benchmark on Visual Decathlon, demonstrating that we can automatically search for and identify architectures that effectively make trade-offs between task resource requirements while maintaining a high level of final performance."}}
{"id": "rybcaIWdbH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Associative Embedding: End-to-End Learning for Joint Detection and Grouping", "abstract": "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets."}}
{"id": "Sy4nxwbd-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "Pixels to Graphs by Associative Embedding", "abstract": "Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph definition. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and demonstrate state-of-the-art performance on the challenging task of scene graph generation."}}
{"id": "HkV3O5bdWH", "cdate": 1451606400000, "mdate": null, "content": {"title": "Stacked Hourglass Networks for Human Pose Estimation", "abstract": "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a \u201cstacked hourglass\u201d network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods."}}
