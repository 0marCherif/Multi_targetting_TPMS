{"id": "_ZWURLc900", "cdate": 1640995200000, "mdate": 1672034161102, "content": {"title": "On Coresets for Fair Regression and Individually Fair Clustering", "abstract": "In this paper we present coresets for Fair Regression with Statistical Parity (SP) constraints and for Individually Fair Clustering. Due to the fairness constraints, the classical coreset definition is not enough for these problems. We first define coresets for both the problems. We show that to obtain such coresets, it is sufficient to sample points based on the probabilities dependent on combination of sensitivity score and a carefully chosen term according to the fairness constraints. We give provable guarantees with relative error in preserving the cost and a small additive error in preserving fairness constraints for both problems. Since our coresets are much smaller in size as compared to $n$, the number of points, they can give huge benefits in computational costs (from polynomial to polylogarithmic in $n$), especially when $n \\gg d$, where $d$ is the input dimension. We support our theoretical claims with experimental evaluations."}}
{"id": "PBgnAGNCD9f", "cdate": 1640995200000, "mdate": 1674806427483, "content": {"title": "Efficient NTK using Dimensionality Reduction", "abstract": "Recently, neural tangent kernel (NTK) has been used to explain the dynamics of learning parameters of neural networks, at the large width limit. Quantitative analyses of NTK give rise to network widths that are often impractical and incur high costs in time and energy in both training and deployment. Using a matrix factorization technique, we show how to obtain similar guarantees to those obtained by a prior analysis while reducing training and inference resource costs. The importance of our result further increases when the input points' data dimension is in the same order as the number of input points. More generally, our work suggests how to analyze large width networks in which dense linear layers are replaced with a low complexity factorization, thus reducing the heavy dependence on the large width."}}
{"id": "NL60t4u1R2T", "cdate": 1609459200000, "mdate": null, "content": {"title": "\u2113p Subspace Embedding in Input Sparsity Time", "abstract": "We study the distribution of matrices that can be used to preserve \u2113p subspace embedding in input sparsity time, for integer p \u2208 [2, \u221e). We use the notion of power of two choice (Mitzenmacher, 2001) to design a distribution such matrices. For p = 2 case, we empirically compare our algorithm\u2019s performance with an existing method such as CountSketch\u00a0(Clarkson and Woodruff, 2017)."}}
{"id": "W7RIxV6NBlM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Streaming Coresets for Symmetric Tensor Factorization", "abstract": "Factorizing tensors has recently become an important optimization module in a number of machine learning pipelines, especially in latent variable models. We show how to do this efficiently in the s..."}}
{"id": "FzWNh5SQLxD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Online Coresets for Clustering with Bregman Divergences", "abstract": "We present algorithms that create coresets in an online setting for clustering problems according to a wide subset of Bregman divergences. Notably, our coresets have a small additive error, similar in magnitude to the lightweight coresets Bachem et. al. 2018, and take update time $O(d)$ for every incoming point where $d$ is dimension of the point. Our first algorithm gives online coresets of size $\\tilde{O}(\\mbox{poly}(k,d,\\epsilon,\\mu))$ for $k$-clusterings according to any $\\mu$-similar Bregman divergence. We further extend this algorithm to show existence of a non-parametric coresets, where the coreset size is independent of $k$, the number of clusters, for the same subclass of Bregman divergences. Our non-parametric coresets are larger by a factor of $O(\\log n)$ ($n$ is number of points) and have similar (small) additive guarantee. At the same time our coresets also function as lightweight coresets for non-parametric versions of the Bregman clustering like DP-Means. While these coresets provide additive error guarantees, they are also significantly smaller (scaling with $O(\\log n)$ as opposed to $O(d^d)$ for points in $\\~R^d$) than the (relative-error) coresets obtained in Bachem et. al. 2015 for DP-Means. While our non-parametric coresets are existential, we give an algorithmic version under certain assumptions."}}
{"id": "7HCMqIYWDBQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Coresets for Regularized Regression", "abstract": "We study the effect of norm based regularization on the size of coresets for regression problems. Specifically, given a matrix $ \\mathbf{A} \\in {\\mathbb{R}}^{n \\times d}$ with $n\\gg d$ and a vector..."}}
