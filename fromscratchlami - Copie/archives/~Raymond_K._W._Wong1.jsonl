{"id": "RTemQ-x0SI", "cdate": 1672531200000, "mdate": 1679982060686, "content": {"title": "Implicit Regularization for Group Sparsity", "abstract": ""}}
{"id": "d7Q0vVfJ0wO", "cdate": 1663850472952, "mdate": null, "content": {"title": "Implicit Regularization for Group Sparsity", "abstract": "We study the implicit regularization of gradient descent towards structured sparsity via a novel neural reparameterization, which we call a diagonally grouped linear neural network. We show the following intriguing property of our reparameterization: gradient descent over the squared regression loss, without any explicit regularization, biases towards solutions with a group sparsity structure. In contrast to many existing works in understanding implicit regularization, we prove that our training trajectory cannot be simulated by mirror descent. We analyze the gradient dynamics of the corresponding regression problem in the general noise setting and obtain minimax-optimal error rates. Compared to existing bounds for implicit sparse regularization using diagonal linear networks, our analysis with the new reparameterization shows improved sample complexity. In the degenerate case of size-one groups, our approach gives rise to a new algorithm for sparse linear regression. Finally, we demonstrate the efficacy of our approach with several numerical experiments."}}
{"id": "LRPQhhpONFj", "cdate": 1640995200000, "mdate": 1679982060622, "content": {"title": "Extending the Use of MDL for High-Dimensional Problems: Variable Selection, Robust Fitting, and Additive Modeling", "abstract": ""}}
{"id": "QM8oG0bz1o", "cdate": 1621629885696, "mdate": null, "content": {"title": "Implicit Sparse Regularization: The Impact of Depth and Early Stopping", "abstract": "In this paper, we study the implicit bias of gradient descent for sparse regression. We extend results on regression with quadratic parametrization, which amounts to depth-2 diagonal linear networks, to more general depth-$N$ networks, under more realistic settings of noise and correlated designs. We show that early stopping is crucial for gradient descent to converge to a sparse model, a phenomenon that we call \\emph{implicit sparse regularization}. This result is in sharp contrast to known results for noiseless and uncorrelated-design cases. \n  \nWe characterize the impact of depth and early stopping and show that for a general depth parameter $N$, gradient descent with early stopping achieves minimax optimal sparse recovery with sufficiently small initialization $w_0$ and step size $\\eta$. In particular, we show that increasing depth enlarges the scale of working initialization and the early-stopping window so that this implicit sparse regularization effect is more likely to take place."}}
{"id": "uZCPcsuYgc", "cdate": 1609459200000, "mdate": 1679982060344, "content": {"title": "Benefits of Jointly Training Autoencoders: An Improved Neural Tangent Kernel Analysis", "abstract": ""}}
{"id": "nlt9jveMSo", "cdate": 1609459200000, "mdate": 1679982060418, "content": {"title": "Tensor Linear Regression: Degeneracy and Solution", "abstract": ""}}
{"id": "GqhBRygime", "cdate": 1609459200000, "mdate": 1679982152462, "content": {"title": "Projected State-action Balancing Weights for Offline Reinforcement Learning", "abstract": ""}}
{"id": "BZNWZKuC1yc", "cdate": 1609459200000, "mdate": 1644320888745, "content": {"title": "Matrix Completion with Model-free Weighting", "abstract": "In this paper, we propose a novel method for matrix completion under general non-uniform missing structures. By controlling an upper bound of a novel balancing error, we construct weights that can ..."}}
{"id": "0X2OjqgNHtq", "cdate": 1609459200000, "mdate": 1679982152445, "content": {"title": "Implicit Sparse Regularization: The Impact of Depth and Early Stopping", "abstract": ""}}
{"id": "qW2Uw7PRzMX", "cdate": 1595772834285, "mdate": null, "content": {"title": "Benefits of Jointly Training Autoencoders: An Improved Neural Tangent Kernel Analysis", "abstract": "A remarkable recent discovery in machine learning has been that deep neural networks can achieve impressive performance (in terms of both lower training error and higher generalization capacity) in the regime where they are massively over-parameterized. Consequently, over the past year, the community has devoted growing interest in analyzing optimization and generalization properties of over-parameterized networks, and several breakthrough works have led to important theoretical progress. However, the majority of existing work only applies to supervised learning scenarios and hence are limited to settings such as classification and regression. In contrast, the role of over-parameterization in the unsupervised setting has gained far less attention. In this paper, we study the gradient dynamics of two-layer over-parameterized autoencoders with ReLU activation. We make very few assumptions about the given training dataset (other than mild non-degeneracy conditions). Starting from a randomly initialized autoencoder network, we rigorously prove the linear convergence of gradient descent in two learning regimes, namely: (i) the weakly-trained regime where only the encoder is trained, and (ii) the jointly-trained regime where both the encoder and the decoder are trained. Our results indicate the considerable benefits of joint training over weak training for finding global optima, achieving a dramatic decrease in the required level of over-parameterization. We also analyze the case of weight-tied autoencoders (which is a commonly used architectural choice in practical settings) and prove that in the over-parameterized setting, training such networks from randomly initialized points leads to certain unexpected degeneracies."}}
