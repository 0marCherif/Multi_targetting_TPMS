{"id": "0kAhIGQNHF", "cdate": 1667371106663, "mdate": 1667371106663, "content": {"title": "Deep Space-Time Video Upsampling Networks", "abstract": "Video super-resolution (VSR) and frame interpolation (FI) are traditional computer vision problems, and the performance have been improving by incorporating deep learning recently. In this paper, we investigate the problem of jointly upsampling videos both in space and time, which is becoming more important with advances in display systems. One solution for this is to run VSR and FI, one by one, independently. This is highly ine\u000ecient as heavy deep neural networks (DNN) are involved in each solution. To this end, we propose an end- to-end DNN framework for the space-time video upsampling by effciently merging VSR and FI into a joint framework. In our framework, a novel weighting scheme is proposed to fuse all input frames effectively without explicit motion compensation for efficient processing of videos. The results show better results both quantitatively and qualitatively, while reducing the computation time (x7 faster) and the number of parameters (30%) compared to baselines. Our source code is available at https://github.com/JaeYeonKang/STVUN-Pytorch."}}
{"id": "j2d7SiPm1h", "cdate": 1609459200000, "mdate": 1667352749223, "content": {"title": "SRFlow-DA: Super-Resolution Using Normalizing Flow With Deep Convolutional Block", "abstract": "Multiple high-resolution (HR) images can be generated from a single low-resolution (LR) image, as super-resolution (SR) is an underdetermined problem. Recently, the conditional normalizing flow-based model, SRFlow, shows remarkable performance by learning an exact mapping from HR image manifold to a latent space. The flow-based SR model allows sampling multiple output images from a learned SR space with a given LR image. In this work, we propose SRFlow-DA which has a more suitable architecture for the SR task based on the original SRFlow model. Specifically, our approach enlarges the receptive field by stacking more convolutional layers in the affine couplings, and so our model can get more expressive power. At the same time, we reduce the total number of model parameters for efficiency. Compared to SRFlow, our SRFlow-DA achieves better or comparable PSNR and LPIPS for x4 and x8 SR tasks, while having a reduced number of parameters. In addition, our method generates visually clear results without excessive sharpness artifacts."}}
{"id": "POqFSK1e-Nm", "cdate": 1609459200000, "mdate": 1667352749229, "content": {"title": "Practical Single-Image Super-Resolution Using Look-Up Table", "abstract": ""}}
{"id": "1aHXaQGYmMF", "cdate": 1609459200000, "mdate": 1667352749219, "content": {"title": "Tackling the Ill-Posedness of Super-Resolution Through Adaptive Target Generation", "abstract": "By the one-to-many nature of the super-resolution (SR) problem, a single low-resolution (LR) image can be mapped to many high-resolution (HR) images. However, learning based SR algorithms are trained to map an LR image to the corresponding ground truth (GT) HR image in the training dataset. The training loss will increase and penalize the algorithm when the output does not exactly match the GT target, even when the outputs are mathematically valid candidates according to the SR framework. This becomes more problematic for the blind SR, as diverse unknown blur kernels exacerbate the ill-posedness of the problem. To this end, we propose a fundamentally different approach for the SR by introducing the concept of the adaptive target. The adaptive target is generated from the original GT target by a transformation to match the output of the SR network. The adaptive target provides an effective way for the SR algorithm to deal with the ill-posed nature of the SR, by providing the algorithm with the flexibility of accepting a variety of valid solutions. Experimental results show the effectiveness of our algorithm, especially for improving the perceptual quality of HR outputs."}}
{"id": "TVklhIHpk2r", "cdate": 1577836800000, "mdate": 1667352749512, "content": {"title": "Deep Space-Time Video Upsampling Networks", "abstract": "Video super-resolution (VSR) and frame interpolation (FI) are traditional computer vision problems, and the performance have been improving by incorporating deep learning recently. In this paper, we investigate the problem of jointly upsampling videos both in space and time, which is becoming more important with advances in display systems. One solution for this is to run VSR and FI, one by one, independently. This is highly inefficient as heavy deep neural networks (DNN) are involved in each solution. To this end, we propose an end-to-end DNN framework for the space-time video upsampling by efficiently merging VSR and FI into a joint framework. In our framework, a novel weighting scheme is proposed to fuse all input frames effectively without explicit motion compensation for efficient processing of videos. The results show better results both quantitatively and qualitatively, while reducing the computation time ( $$\\times $$ 7 faster) and the number of parameters (30%) compared to baselines. Our source code is available at https://github.com/JaeYeonKang/STVUN-Pytorch ."}}
{"id": "NJFcjg3V-i-", "cdate": 1577836800000, "mdate": 1667352749487, "content": {"title": "Investigating Loss Functions for Extreme Super-Resolution", "abstract": "The performance of image super-resolution (SR) has been greatly improved by using convolutional neural networks. Most of the previous SR methods have been studied up to x4 upsampling, and few were studied for x16 upsampling. The general approach for perceptual x4 SR is using GAN with VGG based perceptual loss, however, we found that it creates inconsistent details for perceptual x16 SR. To this end, we have investigated loss functions and we propose to use GAN with LPIPS loss for perceptual extreme SR. In addition, we use U-net structure discriminator together to consider both the global and local context of an input image. Experimental results show that our method outperforms the conventional perceptual loss, and we achieved second place in preliminary results of NTIRE 2020 perceptual extreme SR challenge."}}
{"id": "KEcHde6MjZ4", "cdate": 1577836800000, "mdate": 1667352749524, "content": {"title": "Learning the Loss Functions in a Discriminative Space for Video Restoration", "abstract": "With more advanced deep network architectures and learning schemes such as GANs, the performance of video restoration algorithms has greatly improved recently. Meanwhile, the loss functions for optimizing deep neural networks remain relatively unchanged. To this end, we propose a new framework for building effective loss functions by learning a discriminative space specific to a video restoration task. Our framework is similar to GANs in that we iteratively train two networks - a generator and a loss network. The generator learns to restore videos in a supervised fashion, by following ground truth features through the feature matching in the discriminative space learned by the loss network. In addition, we also introduce a new relation loss in order to maintain the temporal consistency in output videos. Experiments on video superresolution and deblurring show that our method generates visually more pleasing videos with better quantitative perceptual metric values than the other state-of-the-art methods."}}
{"id": "-BnYNRgY4E", "cdate": 1577836800000, "mdate": 1667347842011, "content": {"title": "NTIRE 2020 Challenge on Perceptual Extreme Super-Resolution: Methods and Results", "abstract": "This paper reviews the NTIRE 2020 challenge on perceptual extreme super-resolution with focus on proposed solutions and results. The challenge task was to super-resolve an input image with a magnification factor 16 based on a set of prior examples of low and corresponding high resolution images. The goal is to obtain a network design capable to produce high resolution results with the best perceptual quality and similar to the ground truth. The track had 280 registered participants, and 19 teams submitted the final results. They gauge the state-of-the-art in single image superresolution."}}
{"id": "H1WWX1fdWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation", "abstract": "Video super-resolution (VSR) has become even more important recently to provide high resolution (HR) contents for ultra high definition displays. While many deep learning based VSR methods have been proposed, most of them rely heavily on the accuracy of motion estimation and compensation. We introduce a fundamentally different framework for VSR in this paper. We propose a novel end-to-end deep neural network that generates dynamic upsampling filters and a residual image, which are computed depending on the local spatio-temporal neighborhood of each pixel to avoid explicit motion compensation. With our approach, an HR image is reconstructed directly from the input image using the dynamic upsampling filters, and the fine details are added through the computed residual. Our network with the help of a new data augmentation technique can generate much sharper HR videos with temporal consistency, compared with the previous methods. We also provide analysis of our network through extensive experiments to show how the network deals with motions implicitly."}}
