{"id": "5jnV7NsKb3", "cdate": 1672531200000, "mdate": 1681653004715, "content": {"title": "CLIPood: Generalizing CLIP to Out-of-Distributions", "abstract": ""}}
{"id": "L8ESR8IQ7Gb", "cdate": 1652737398637, "mdate": null, "content": {"title": "Hub-Pathway: Transfer Learning from A Hub of Pre-trained Models", "abstract": "Transfer learning aims to leverage knowledge from pre-trained models to benefit the target task. Prior transfer learning work mainly transfers from a single model. However, with the emergence of deep models pre-trained from different resources, model hubs consisting of diverse models with various architectures, pre-trained datasets and learning paradigms are available. Directly applying single-model transfer learning methods to each model wastes the abundant knowledge of the model hub and suffers from high computational cost. In this paper, we propose a Hub-Pathway framework to enable knowledge transfer from a model hub. The framework generates data-dependent pathway weights, based on which we assign the pathway routes at the input level to decide which pre-trained models are activated and passed through, and then set the pathway aggregation at the output level to aggregate the knowledge from different models to make predictions. The proposed framework can be trained end-to-end with the target task-specific loss, where it learns to explore better pathway configurations and exploit the knowledge in pre-trained models for each target datum. We utilize a noisy pathway generator and design an exploration loss to further explore different pathways throughout the model hub. To fully exploit the knowledge in pre-trained models, each model is further trained by specific data that activate it, which ensures its performance and enhances knowledge transfer. Experiment results on computer vision and reinforcement learning tasks demonstrate that the proposed Hub-Pathway framework achieves the state-of-the-art performance for model hub transfer learning."}}
{"id": "ewhDK1yZVHj", "cdate": 1640995200000, "mdate": 1681653004754, "content": {"title": "Transferability in Deep Learning: A Survey", "abstract": ""}}
{"id": "rz-5IMlk1i", "cdate": 1609459200000, "mdate": 1681653004754, "content": {"title": "Omni-Training for Data-Efficient Deep Learning", "abstract": ""}}
{"id": "W0QMcCg3LQV", "cdate": 1609459200000, "mdate": 1648704880086, "content": {"title": "Zoo-Tuning: Adaptive Transfer from A Zoo of Models", "abstract": "With the development of deep networks on various large-scale datasets, a large zoo of pretrained models are available. When transferring from a model zoo, applying classic single-model-based transf..."}}
{"id": "DWnellIheZh", "cdate": 1609459200000, "mdate": 1648704880085, "content": {"title": "Open Domain Generalization with Domain-Augmented Meta-Learning", "abstract": "Leveraging datasets available to learn a model with high generalization ability to unseen domains is important for computer vision, especially when the unseen domain's annotated data are unavailable. We study the problem of learning from different source domains to achieve high performance on an unknown target domain, where the distributions and label sets of each individual source domain and the target domain are different. The problem can be generally applied to diverse source domains and widely applicable to real-world applications. We propose a Domain-Augmented Meta-Learning framework to learn open-domain generalizable representations. We augment domains on both feature-level by a new Dirichlet mixup and label-level by distilled soft-labeling, which complements each domain with missing classes and other domain knowledge. We conduct meta-learning over domains by designing new meta-learning tasks and losses to preserve domain unique knowledge and generalize knowledge across domains simultaneously. Experiment results on various multi-domain datasets demonstrate that the proposed Domain-Augmented Meta-Learning outperforms previous methods for unseen target classification."}}
{"id": "BioEfZzldTr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Transferable Curriculum for Weakly-Supervised Domain Adaptation.", "abstract": "Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that \u201csource-domain engineering\u201d becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weaklysupervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks."}}
