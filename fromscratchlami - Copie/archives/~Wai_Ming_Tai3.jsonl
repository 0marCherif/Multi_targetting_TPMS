{"id": "REPEIS4BQuy", "cdate": 1640995200000, "mdate": 1649823140283, "content": {"title": "Optimal estimation of Gaussian DAG models", "abstract": "We study the optimal sample complexity of learning a Gaussian directed acyclic graph (DAG) from observational data. Our main results establish the minimax optimal sample complexity for learning the structure of a linear Gaussian DAG model in two settings of interest: 1) Under equal variances without knowledge of the true ordering, and 2) For general linear models given knowledge of the ordering. In both cases the sample complexity is $n\\asymp q\\log(d/q)$, where $q$ is the maximum number of parents and $d$ is the number of nodes. We further make comparisons with the classical problem of learning (undirected) Gaussian graphical models, showing that under the equal variance assumption, these two problems share the same optimal sample complexity. In other words, at least for Gaussian models with equal error variances, learning a directed graphical model is statistically no more difficult than learning an undirected graphical model. Our results also extend to more general identification assumptions as well as subgaussian errors."}}
{"id": "IS-kldKVlwx", "cdate": 1640995200000, "mdate": 1649823140289, "content": {"title": "A super-polynomial lower bound for learning nonparametric mixtures", "abstract": "We study the problem of learning nonparametric distributions in a finite mixture, and establish tight bounds on the sample complexity for learning the component distributions in such models. Namely, we are given i.i.d. samples from a pdf $f$ where $$ f=\\sum_{i=1}^k w_i f_i, \\quad\\sum_{i=1}^k w_i=1, \\quad w_i>0 $$ and we are interested in learning each component $f_i$. Without any assumptions on $f_i$, this problem is ill-posed. In order to identify the components $f_i$, we assume that each $f_i$ can be written as a convolution of a Gaussian and a compactly supported density $\\nu_i$ with $\\text{supp}(\\nu_i)\\cap \\text{supp}(\\nu_j)=\\emptyset$. Our main result shows that $(\\frac{1}{\\varepsilon})^{\\Omega(\\log\\log \\frac{1}{\\varepsilon})}$ samples are required for estimating each $f_i$. Unlike parametric mixtures, the difficulty does not arise from the order $k$ or small weights $w_i$, and unlike nonparametric density estimation it does not arise from the curse of dimensionality, irregularity, or inhomogeneity. The proof relies on a fast rate for approximation with Gaussians, which may be of independent interest. To show this is tight, we also propose an algorithm that uses $(\\frac{1}{\\varepsilon})^{O(\\log\\log \\frac{1}{\\varepsilon})}$ samples to estimate each $f_i$. Unlike existing approaches to learning latent variable models based on moment-matching and tensor methods, our proof instead involves a delicate analysis of an ill-conditioned linear system via orthogonal functions. Combining these bounds, we conclude that the optimal sample complexity of this problem properly lies in between polynomial and exponential, which is not common in learning theory."}}
{"id": "7GfP8JJssiN", "cdate": 1609459200000, "mdate": 1649823140284, "content": {"title": "Finding an Approximate Mode of a Kernel Density Estimate", "abstract": "Given points P = {p\u2081,...,p_n} subset of \u211d^d, how do we find a point x which approximately maximizes the function 1/n \u2211_{p_i \u2208 P} e^{-\u2016p_i-x\u2016\u00b2}? In other words, how do we find an approximate mode of a Gaussian kernel density estimate (KDE) of P? Given the power of KDEs in representing probability distributions and other continuous functions, the basic mode finding problem is widely applicable. However, it is poorly understood algorithmically. We provide fast and provably accurate approximation algorithms for mode finding in both the low and high dimensional settings. For low (constant) dimension, our main contribution is a reduction to solving systems of polynomial inequalities. For high dimension, we prove the first dimensionality reduction result for KDE mode finding. The latter result leverages Johnson-Lindenstrauss projection, Kirszbraun\u2019s classic extension theorem, and perhaps surprisingly, the mean-shift heuristic for mode finding. For constant approximation factor these algorithms run in O(n (log n)^{O(d)}) and O(nd + (log n)^{O(log\u00b3 n)}), respectively; these are proven more precisely as a (1+\u03b5)-approximation guarantee. Furthermore, for the special case of d = 2, we give a combinatorial algorithm running in O(n log\u00b2 n) time. We empirically demonstrate that the random projection approach and the 2-dimensional algorithm improves over the state-of-the-art mode-finding heuristics."}}
{"id": "BWIqGmIwEsf", "cdate": 1577836800000, "mdate": 1649823140285, "content": {"title": "The GaussianSketch for Almost Relative Error Kernel Distance", "abstract": "We introduce two versions of a new sketch for approximately embedding the Gaussian kernel into Euclidean inner product space. These work by truncating infinite expansions of the Gaussian kernel, and carefully invoking the RecursiveTensorSketch [Ahle et al. SODA 2020]. After providing concentration and approximation properties of these sketches, we use them to approximate the kernel distance between points sets. These sketches yield almost (1+\u03b5)-relative error, but with a small additive \u03b1 term. In the first variants the dependence on 1/\u03b1 is poly-logarithmic, but has higher degree of polynomial dependence on the original dimension d. In the second variant, the dependence on 1/\u03b1 is still poly-logarithmic, but the dependence on d is linear."}}
{"id": "4vkz6fYWNh2", "cdate": 1577836800000, "mdate": 1649823140311, "content": {"title": "New Nearly-Optimal Coreset for Kernel Density Estimation", "abstract": "Given a point set $P\\subset \\mathbb{R}^d$, the kernel density estimate of $P$ is defined as \\[ \\overline{\\mathcal{G}}_P(x) = \\frac{1}{\\left|P\\right|}\\sum_{p\\in P}e^{-\\left\\lVert x-p \\right\\rVert^2} \\] for any $x\\in\\mathbb{R}^d$. We study how to construct a small subset $Q$ of $P$ such that the kernel density estimate of $P$ is approximated by the kernel density estimate of $Q$. This subset $Q$ is called a coreset. The main technique in this work is constructing a $\\pm 1$ coloring on the point set $P$ by discrepancy theory and we leverage Banaszczyk's Theorem. When $d>1$ is a constant, our construction gives a coreset of size $O\\left(\\frac{1}{\\varepsilon}\\right)$ as opposed to the best-known result of $O\\left(\\frac{1}{\\varepsilon}\\sqrt{\\log\\frac{1}{\\varepsilon}}\\right)$. It is the first result to give a breakthrough on the barrier of $\\sqrt{\\log}$ factor even when $d=2$."}}
{"id": "2yOTTOgq7L9", "cdate": 1577836800000, "mdate": 1649823140299, "content": {"title": "Near-Optimal Coresets of Kernel Density Estimates", "abstract": "We construct near-optimal coresets for kernel density estimates for points in $${\\mathbb {R}}^d$$ R d when the kernel is positive definite. Specifically we provide a polynomial time construction for a coreset of size $$O(\\sqrt{d}/\\varepsilon \\cdot \\sqrt{\\log 1/\\varepsilon } )$$ O ( d / \u03b5 \u00b7 log 1 / \u03b5 ) , and we show a near-matching lower bound of size $$\\Omega (\\min \\{\\sqrt{d}/\\varepsilon , 1/\\varepsilon ^2\\})$$ \u03a9 ( min { d / \u03b5 , 1 / \u03b5 2 } ) . When $$d\\ge 1/\\varepsilon ^2$$ d \u2265 1 / \u03b5 2 , it is known that the size of coreset can be $$O(1/\\varepsilon ^2)$$ O ( 1 / \u03b5 2 ) . The upper bound is a polynomial-in- $$(1/\\varepsilon )$$ ( 1 / \u03b5 ) improvement when $$d \\in [3,1/\\varepsilon ^2)$$ d \u2208 [ 3 , 1 / \u03b5 2 ) and the lower bound is the first known lower bound to depend on d for this problem. Moreover, the upper bound restriction that the kernel is positive definite is significant in that it applies to a wide variety of kernels, specifically those most important for machine learning. This includes kernels for information distances and the sinc kernel which can be negative."}}
{"id": "y_Hf2BXKOh6", "cdate": 1546300800000, "mdate": 1649823140284, "content": {"title": "Finding the Mode of a Kernel Density Estimate", "abstract": "Given points $p_1, \\dots, p_n$ in $\\mathbb{R}^d$, how do we find a point $x$ which maximizes $\\frac{1}{n} \\sum_{i=1}^n e^{-\\|p_i - x\\|^2}$? In other words, how do we find the maximizing point, or mode of a Gaussian kernel density estimation (KDE) centered at $p_1, \\dots, p_n$? Given the power of KDEs in representing probability distributions and other continuous functions, the basic mode finding problem is widely applicable. However, it is poorly understood algorithmically. Few provable algorithms are known, so practitioners rely on heuristics like the \"mean-shift\" algorithm, which are not guaranteed to find a global optimum. We address this challenge by providing fast and provably accurate approximation algorithms for mode finding in both the low and high dimensional settings. For low dimension $d$, our main contribution is to reduce the mode finding problem to a solving a small number of systems of polynomial inequalities. For high dimension $d$, we prove the first dimensionality reduction result for KDE mode finding, which allows for reduction to the low dimensional case. Our result leverages Johnson-Lindenstrauss random projection, Kirszbraun's classic extension theorem, and perhaps surprisingly, the mean-shift heuristic for mode finding."}}
{"id": "nrXv-_swW1x", "cdate": 1546300800000, "mdate": 1649823140285, "content": {"title": "Approximate Guarantees for Dictionary Learning", "abstract": "In the dictionary learning (or sparse coding) problem, we are given a collection of signals (vectors in $\\mathbb{R}^d$), and the goal is to find a \"basis\" in which the signals have a sparse (approximate) representation. The problem has received a lot of attention in signal processing, learning, and theoretical computer science. The problem is formalized as factorizing a matrix $X (d \\times n)$ (whose columns are the signals) as $X = AY$, where $A$ has a prescribed number $m$ of columns (typically $m \\ll n$), and $Y$ has columns that are $k$-sparse (typically $k \\ll d$). Most of the known theoretical results involve assuming that the columns of the unknown $A$ have certain incoherence properties, and that the coefficient matrix $Y$ has random (or partly random) structure. The goal of our work is to understand what can be said in the absence of such assumptions. Can we still find $A$ and $Y$ such that $X \\approx AY$? We show that this is possible, if we allow violating the bounds on $m$ and $k$ by appropriate factors that depend on $k$ and the desired approximation. Our results rely on an algorithm for what we call the threshold correlation problem, which turns out to be related to hypercontractive norms of matrices. We also show that our algorithmic ideas apply to a setting in which some of the columns of $X$ are outliers, thus giving similar guarantees even in this challenging setting."}}
{"id": "T1GcA_PmpV", "cdate": 1546300800000, "mdate": 1649823140311, "content": {"title": "Approximate Guarantees for Dictionary Learning", "abstract": "In the dictionary learning (or sparse coding) problem, we are given a collection of signals (vectors in $\\mathbb{R}^d$), and the goal is to find a \"basis\" in which the signals have a sparse (approximate) representation. The problem has received a lot of attention in signal processing, learning, and theoretical computer science. The problem is formalized as factorizing a matrix $X (d \\times n)$ (whose columns are the signals) as $X = AY$, where $A$ has a prescribed number $m$ of columns (typically $m \\ll n$), and $Y$ has columns that are $k$-sparse (typically $k \\ll d$). Most of the known theoretical results involve assuming that the columns of the unknown $A$ have certain incoherence properties, and that the coefficient matrix $Y$ has random (or partly random) structure. The goal of our work is to understand what can be said in the absence of such assumptions. Can we still find $A$ and $Y$ such that $X \\approx AY$? We show that this is possible, if we allow violating the bounds on $m$ and $k$ by appropriate factors that depend on $k$ and the desired approximation. Our results rely on an algorithm for what we call the threshold correlation problem, which turns out to be related to hypercontractive norms of matrices. We also show that our algorithmic ideas apply to a setting in which some of the columns of $X$ are outliers, thus giving similar guarantees even in this challenging setting."}}
{"id": "-nKPCQEWtxO", "cdate": 1546300800000, "mdate": 1649823140284, "content": {"title": "Learning In Practice: Reasoning About Quantization", "abstract": "There is a mismatch between the standard theoretical analyses of statistical machine learning and how learning is used in practice. The foundational assumption supporting the theory is that we can represent features and models using real-valued parameters. In practice, however, we do not use real numbers at any point during training or deployment. Instead, we rely on discrete and finite quantizations of the reals, typically floating points. In this paper, we propose a framework for reasoning about learning under arbitrary quantizations. Using this formalization, we prove the convergence of quantization-aware versions of the Perceptron and Frank-Wolfe algorithms. Finally, we report the results of an extensive empirical study of the impact of quantization using a broad spectrum of datasets."}}
