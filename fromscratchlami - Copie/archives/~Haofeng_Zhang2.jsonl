{"id": "ulDSTH10MDP", "cdate": 1640995200000, "mdate": 1667378524107, "content": {"title": "Zero-Shot Logit Adjustment", "abstract": "Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses challenges in recognizing novel classes in the test phase. The development of generative models enables current GZSL techniques to probe further into the semantic-visual link, culminating in a two-stage form that includes a generator and a classifier. However, existing generation-based methods focus on enhancing the generator's effect while neglecting the improvement of the classifier. In this paper, we first analyze of two properties of the generated pseudo unseen samples: bias and homogeneity. Then, we perform variational Bayesian inference to back-derive the evaluation metrics, which reflects the balance of the seen and unseen classes. As a consequence of our derivation, the aforementioned two properties are incorporated into the classifier training as seen-unseen priors via logit adjustment. The Zero-Shot Logit Adjustment further puts semantic-based classifiers into effect in generation-based GZSL. Our experiments demonstrate that the proposed technique achieves state-of-the-art when combined with the basic generator, and it can improve various generative Zero-Shot Learning frameworks. Our codes are available on https://github.com/cdb342/IJCAI-2022-ZLA."}}
{"id": "oZva8MxVVEK", "cdate": 1640995200000, "mdate": 1667378526467, "content": {"title": "Towards the Semantic Weak Generalization Problem in Generative Zero-Shot Learning: Ante-hoc and Post-hoc", "abstract": "Generation-based methods have captured most of the recent attention in Zero-Shot Learning research. In this paper, we attempt to deconstruct the generator-classifier framework to guide its improvement and extension. We begin by analyzing the generator-learned instance-level distribution by alternating it with a Gaussian distribution. Then we reveal the roles of the class-level distribution and the instance-level distribution learned by the generator in classifier training by decomposing the classifier gradients. We finally conclude with the guidelines for improving the generator-classifier framework from the deconstruction of the generator and the classifier, i.e., (i) The key for the ZSL generator is attribute generalization; and (ii) classifier learning emphasizes mitigating the impact of pseudo unseen samples on decision boundaries between seen classes during training, and reducing the seen-unseen bias. We propose a simple method based on the guidelines. Without complex designs, the proposed method outperforms the state of the art on four public ZSL datasets, which demonstrates the validity of the proposed guidelines. The proposed method is still effective when replacing the generative model with an attribute-to-visual center single mapping model, demonstrating its strong transferability. Codes will be public upon acceptance."}}
{"id": "erzbKpDRI0b", "cdate": 1640995200000, "mdate": 1667378522778, "content": {"title": "Learning discriminative and representative feature with cascade GAN for generalized zero-shot learning", "abstract": ""}}
{"id": "dP8oj9IxxAv", "cdate": 1640995200000, "mdate": 1667378523600, "content": {"title": "Depth-embedded instance segmentation network for urban scene parsing", "abstract": "The ubiquitous availability of cost-effective cameras has rendered large scale collection of street view data a straightforward endeavour. Yet, the effective use of these data to assist autonomous driving remains a challenge, especially lack of exploration and exploitation of stereo images with abundant perceptible depth. In this paper, we propose a novel Depth-embedded Instance Segmentation Network (DISNet) which can effectively improve the performance of instance segmentation by incorporating the depth information of stereo images. The proposed network takes binocular images as input to observe the displacement of the object and estimate the corresponding depth perception without additional supervisions. Furthermore, we introduce a new module for computing the depth cost-volume, which can be integrated with the colour cost-volume to jointly capture useful disparities of stereo images. The shared-weights structure of Siamese Network is applied to learn the intrinsic information of stereo images while reducing the computational burden. Extensive experiments have been carried out on publicly available datasets (i.e., Cityscapes and KITTI), and the obtained results clearly demonstrate the superiority in segmenting instances with different depths."}}
{"id": "cwcOsK0ZMLP", "cdate": 1640995200000, "mdate": 1667378523599, "content": {"title": "Semi-supervised cross-modal hashing with multi-view graph representation", "abstract": ""}}
{"id": "bbTEMGzBIQ", "cdate": 1640995200000, "mdate": 1667378526977, "content": {"title": "Learning to Hash Naturally Sorts", "abstract": "Learning to hash pictures a list-wise sorting problem. Its testing metrics, e.g., mean-average precision, count on a sorted candidate list ordered by pair-wise code similarity. However, scarcely does one train a deep hashing model with the sorted results end-to-end because of the non-differentiable nature of the sorting operation. This inconsistency in the objectives of training and test may lead to sub-optimal performance since the training loss often fails to reflect the actual retrieval metric. In this paper, we tackle this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming distances of samples' hash codes and accordingly gather their latent representations for self-supervised training. Thanks to the recent advances in differentiable sorting approximations, the hash head receives gradients from the sorter so that the hash encoder can be optimized along with the training procedure. Additionally, we describe a novel Sorted Noise-Contrastive Estimation (SortedNCE) loss that selectively picks positive and negative samples for contrastive learning, which allows NSH to mine data semantic relations during training in an unsupervised manner. Our extensive experiments show the proposed NSH model significantly outperforms the existing unsupervised hashing methods on three benchmarked datasets."}}
{"id": "X90SXqdAC_", "cdate": 1640995200000, "mdate": 1667378525700, "content": {"title": "Zero-Shot Logit Adjustment", "abstract": "Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses challenges in recognizing novel classes in the test phase. The development of generative models enables current GZSL techniques to probe further into the semantic-visual link, culminating in a two-stage form that includes a generator and a classifier. However, existing generation-based methods focus on enhancing the generator's effect while neglecting the improvement of the classifier. In this paper, we first analyze of two properties of the generated pseudo unseen samples: bias and homogeneity. Then, we perform variational Bayesian inference to back-derive the evaluation metrics, which reflects the balance of the seen and unseen classes. As a consequence of our derivation, the aforementioned two properties are incorporated into the classifier training as seen-unseen priors via logit adjustment. The Zero-Shot Logit Adjustment further puts semantic-based classifiers into effect in generation-based GZSL. Our experiments demonstrate that the proposed technique achieves state-of-the-art when combined with the basic generator, and it can improve various generative Zero-Shot Learning frameworks. Our codes are available on https://github.com/cdb342/IJCAI-2022-ZLA."}}
{"id": "W8niTjO7-w2", "cdate": 1640995200000, "mdate": 1667378523599, "content": {"title": "Learning to Hash Naturally Sorts", "abstract": "Learning to hash pictures a list-wise sorting problem. Its testing metrics, e.g., mean-average precision, count on a sorted candidate list ordered by pair-wise code similarity. However, scarcely does one train a deep hashing model with the sorted results end-to-end because of the non-differentiable nature of the sorting operation. This inconsistency in the objectives of training and test may lead to sub-optimal performance since the training loss often fails to reflect the actual retrieval metric. In this paper, we tackle this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming distances of samples' hash codes and accordingly gather their latent representations for self-supervised training. Thanks to the recent advances in differentiable sorting approximations, the hash head receives gradients from the sorter so that the hash encoder can be optimized along with the training procedure. Additionally, we describe a novel Sorted Noise-Contrastive Estimation (SortedNCE) loss that selectively picks positive and negative samples for contrastive learning, which allows NSH to mine data semantic relations during training in an unsupervised manner. Our extensive experiments show the proposed NSH model significantly outperforms the existing unsupervised hashing methods on three benchmarked datasets."}}
{"id": "Uaoz6I5Ni6d", "cdate": 1640995200000, "mdate": 1667378522776, "content": {"title": "Entropy-weighted reconstruction adversary and curriculum pseudo labeling for domain adaptation in semantic segmentation", "abstract": ""}}
{"id": "G2cwbAGup5", "cdate": 1640995200000, "mdate": 1667378526212, "content": {"title": "From Less to More: Progressive Generalized Zero-Shot Detection With Curriculum Learning", "abstract": "Object detection, as one of the most important environment perception tasks for traffic safety in intelligent transportation systems, has been widely investigated recently. However, most of the researches focus on the fully supervised scenario, and inevitably lead to model failure. With the continuous development of Zero-Shot Learning (ZSL) models, Generalized Zero-Shot Detection (GZSD) has attracted great attention due to its ability of detecting unseen objects. Many researchers tend to map the detected visual features to semantic attributes and then separate seen and unseen domains during inference. But they have ignore that the generative methods generally have higher performance than these visual-semantic mapping methods, and they have been confirmed from previous GZSL methods. In order to make up for the vacancy of GZSD in the generative methods, we propose an idea of using curriculum learning to generate more precise unseen visual features. And with the excellent performance of WGAN-based method in sample synthesis, we realize the function of using semantics to generate visual features for unseen domains. In addition, we also adopt part of the idea of meta-learning to progressively correct the capability of the generator for better mitigating domain shift problem during the generation process. Through the above ideas, we can detect both seen and unseen bounding boxes and classify them accurately, by combining with the excellent detection ability of Faster-RCNN. Extensive experimental results on two popular datasets, i.e., MSCOCO and KITTI, show that our proposed method can outperform the state-of-the-art methods."}}
