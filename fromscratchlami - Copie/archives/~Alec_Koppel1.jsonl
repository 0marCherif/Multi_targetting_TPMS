{"id": "VJgxHKjj7e", "cdate": 1679929747077, "mdate": 1679929747077, "content": {"title": "Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic", "abstract": "Many existing reinforcement learning (RL) methods\nemploy stochastic gradient iteration on the\nback end, whose stability hinges upon a hypothesis\nthat the data-generating process mixes exponentially\nfast with a rate parameter that appears\nin the step-size selection. Unfortunately, this assumption\nis violated for large state spaces or settings\nwith sparse rewards, and the mixing time is\nunknown, making the step size inoperable. In this\nwork, we propose an RL methodology attuned\nto the mixing time by employing a multi-level\nMonte Carlo estimator for the critic, the actor,\nand the average reward embedded within an actor critic\n(AC) algorithm. This method, which we\ncall Multi-level Actor-Critic (MAC), is developed\nespecially for infinite-horizon average-reward settings\nand neither relies on oracle knowledge of\nthe mixing time in its parameter selection nor assumes\nits exponential decay; it, therefore, is readily\napplicable to applications with slower mixing\ntimes. Nonetheless, it achieves a convergence rate\ncomparable to the state-of-the-art AC algorithms.\nWe experimentally show that these alleviated restrictions\non the technical conditions required for\nstability translate to superior performance in practice\nfor RL problems with sparse rewards."}}
{"id": "g1D4cN70ee", "cdate": 1672531200000, "mdate": 1681490236641, "content": {"title": "Scalable Multi-Agent Reinforcement Learning with General Utilities", "abstract": ""}}
{"id": "TH8GUX0TMwP", "cdate": 1672531200000, "mdate": 1681490236590, "content": {"title": "STEERING: Stein Information Directed Exploration for Model-Based Reinforcement Learning", "abstract": ""}}
{"id": "SsIeg-ikW3", "cdate": 1672531200000, "mdate": 1681490236587, "content": {"title": "Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic", "abstract": ""}}
{"id": "UjEqQnS5fLc", "cdate": 1667440633295, "mdate": null, "content": {"title": "Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search", "abstract": "We develop a new measure of the exploration/exploitation trade-off in infinite-horizon reinforcement learning (RL) problems called the occupancy information ratio (OIR), which is comprised of a ratio between the infinite-horizon average cost of a policy and the entropy of its induced long-term state occupancy measure. Modifying the classic RL objective in this way yields policies that strike an optimal balance between exploitation and exploration, providing a new tool for addressing the exploration/exploitation trade-off in RL. The paper develops for the first time policy gradient and actor-critic algorithms for OIR optimization based upon a new entropy gradient theorem, and establishes both asymptotic and non-asymptotic convergence results with global optimality guarantees. In experiments, these methodologies outperform several deep RL baselines in problems with sparse rewards, where many trajectories may be uninformative and skepticism about the environment is crucial to success."}}
{"id": "GdGpM3VWWXD", "cdate": 1664310940589, "mdate": null, "content": {"title": "Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) exhibits favorable performance in practice, but its theoretical guarantees are mostly restricted to the setting when the transition model is Gaussian or Lipschitz and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a \\emph{Bayesian coreset} of only statistically significant past state-action pairs; and (iii) {exhibits a Bayesian regret of $\\mathcal{O}(dH^{1+({\\alpha}/{2})}T^{1-({\\alpha}/{2})})$ with coreset size of $\\Omega(\\sqrt{T^{1+\\alpha}})$, where $d$ is the aggregate dimension of state action space, $H$ is the episode length, $T$ is the total number of time steps experienced, and $\\alpha\\in (0,1]$ is the tuning parameter which is a novel introduction into the analysis of MBRL in this work}. To achieve these results, we adopt an approach based upon Stein's method, which allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). Experimentally, we observe that this approach is competitive with several state-of-the-art RL methodologies, and can achieve up to $50\\%$ reduction in wall clock time in some continuous control environments."}}
{"id": "z7fxFOS1pi", "cdate": 1640995200000, "mdate": 1681490236668, "content": {"title": "On Submodular Set Cover Problems for Near-Optimal Online Kernel Basis Selection", "abstract": ""}}
{"id": "wzC8QX2z1Z", "cdate": 1640995200000, "mdate": 1681490236650, "content": {"title": "Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach", "abstract": ""}}
{"id": "odMd3RoFRx", "cdate": 1640995200000, "mdate": 1681490236648, "content": {"title": "Distributed Riemannian Optimization with Lazy Communication for Collaborative Geometric Estimation", "abstract": ""}}
{"id": "havF79YWq3", "cdate": 1640995200000, "mdate": 1681490236645, "content": {"title": "Convergence Rates of Average-Reward Multi-agent Reinforcement Learning via Randomized Linear Programming", "abstract": ""}}
