{"id": "wUMWK0cdrKW", "cdate": 1672531200000, "mdate": 1695969413229, "content": {"title": "Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization", "abstract": ""}}
{"id": "rI8qMD5Pux0", "cdate": 1672531200000, "mdate": 1695969413206, "content": {"title": "Measuring Progress in Fine-grained Vision-and-Language Understanding", "abstract": "Emanuele Bugliarello, Laurent Sartran, Aishwarya Agrawal, Lisa Anne Hendricks, Aida Nematzadeh. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023."}}
{"id": "iqXmKZec3ep", "cdate": 1672531200000, "mdate": 1695969413204, "content": {"title": "Language Modelling with Pixels", "abstract": ""}}
{"id": "a-g5pXsrNR", "cdate": 1672531200000, "mdate": 1681651538302, "content": {"title": "A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision", "abstract": ""}}
{"id": "Tx4Z1z0gZTu", "cdate": 1672531200000, "mdate": 1695969413209, "content": {"title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization", "abstract": "Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions. Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation. StoryBench aims at encouraging future research efforts in this exciting new area."}}
{"id": "QSzAK2Pcwy", "cdate": 1672531200000, "mdate": 1695969413216, "content": {"title": "Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining", "abstract": "Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we add supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional views of images. With masked relation prediction, we further encourage relating entities from visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data."}}
{"id": "FkSp8VW8RjH", "cdate": 1663850101431, "mdate": null, "content": {"title": "Language Modelling with Pixels", "abstract": "Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels."}}
{"id": "upnjRhsVTl", "cdate": 1640995200000, "mdate": 1681727449510, "content": {"title": "Mostra: A Flexible Balancing Framework to Trade-off User, Artist and Platform Objectives for Music Sequencing", "abstract": "We consider the task of sequencing tracks on music streaming platforms where the goal is to maximise not only user satisfaction, but also artist- and platform-centric objectives, needed to ensure long-term health and sustainability of the platform. Grounding the work across four objectives: Sat, Discovery, Exposure and Boost, we highlight the need and the potential to trade-off performance across these objectives, and propose Mostra, a Set Transformer-based encoder-decoder architecture equipped with submodular multi-objective beam search decoding. The proposed model affords system designers the power to balance multiple goals, and dynamically control the impact on one objective to satisfy other objectives. Through extensive experiments on data from a large-scale music streaming platform, we present insights on the trade-offs that exist across different objectives, and demonstrate that the proposed framework leads to a superior, just-in-time balancing across the various metrics of interest."}}
{"id": "fCV5NL7UZd", "cdate": 1640995200000, "mdate": 1681727449360, "content": {"title": "Multilingual Multimodal Learning with Machine Translated Text", "abstract": ""}}
{"id": "HVPvDr5ZHR", "cdate": 1640995200000, "mdate": 1681727449362, "content": {"title": "Ancestor-to-Creole Transfer is Not a Walk in the Park", "abstract": ""}}
