{"id": "6Um8P8Fvyhl", "cdate": 1686324886113, "mdate": null, "content": {"title": "MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning", "abstract": "We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing methods for high-dimensional model-based offline RL are not suitable for offline-to-online fine-tuning due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty. We find that our approach successfully solves tasks from the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation environment completely from images. To our knowledge, MOTO is the first and only method to solve this environment from pixels."}}
{"id": "QPajRB7ISyB", "cdate": 1676591080338, "mdate": null, "content": {"title": "Model-Based Adversarial Imitation Learning As Online Fine-Tuning", "abstract": "In many real world applications of sequential decision-making problems, such as robotics or autonomous driving,  expert-level data is available (or easily obtainable) with methods such as tele-operation. However, directly learning to copy these expert behaviours can result in poor performance due to distribution shift at deployment time. Adversarial imitation learning algorithms alleviate this issue by learning to match the expert state-action distribution through additional environment interactions. Such methods are built around standard reinforcement-learning algorithms with both model-based and model-free approaches. In this work we focus on the model-based approach and argue that algorithms developed for online RL are sub-optimal for the distribution matching problem. We theoretically justify utilizing conservative algorithms developed for the offline learning paradigm in online adversarial imitation learning and empirically demonstrate improved performance and safety on a complex long-range robot manipulation task, directly from images. "}}
{"id": "cH8XVu9hUV", "cdate": 1676591080278, "mdate": null, "content": {"title": "MOTO: Offline to Online Fine-tuning for Model-Based Reinforcement Learning", "abstract": "We study the problem of offline-to-online reinforcement learning from high-dimensional pixel observations. While recent model-free approaches successfully use offline pre-training with online fine-tuning to either improve the performance of the data-collection policy or adapt to novel tasks, model-based approaches still remain underutilized in this setting. In this work, we argue that existing methods for high-dimensional model-based offline RL are not suitable for offline-to-online fine-tuning due to issues with representation learning shifts, off-dynamics data, and non-stationary rewards. We propose a simple on-policy model-based method with adaptive behavior regularization. In our simulation experiments, we find that our approach successfully solves long-horizon robot manipulation tasks completely from images by using a combination of offline data and online interactions. "}}
{"id": "qxuIaeDlemv", "cdate": 1676170032392, "mdate": null, "content": {"title": "Behavioral Cloning for Crystal Design", "abstract": "Solid-state materials, which are made up of periodic 3D crystal structures, are particularly useful for a variety of real-world applications such as batteries, fuel cells and catalytic materials. Designing solid-state materials, especially in a robust and automated fashion, remains an ongoing challenge. To further the automated design of crystalline materials, we propose a method to learn to design valid crystal structures given a crystal skeleton. By incorporating Euclidean equivariance into a policy network, we portray the problem of designing new crystals as a sequential prediction task suited for imitation learning. At each step, given an incomplete graph of a crystal skeleton, an agent assigns an element to a specific node. We adopt a behavioral cloning strategy to train the policy network on data consisting of curated trajectories generated from known crystals."}}
{"id": "hfE9u5d3_dw", "cdate": 1664994278308, "mdate": null, "content": {"title": "Offline Policy Comparison with Confidence: Benchmarks and Baselines", "abstract": "Decision makers often wish to use offline historical data to compare sequential-action policies at various world states. Importantly, computational tools should produce confidence values for such offline policy comparison (OPC) to account for statistical variance and limited data coverage. Nevertheless, there is little work that directly evaluates the quality of confidence values for OPC. In this work, we address this issue by creating benchmarks for OPC with Confidence (OPCC), derived by adding sets of policy comparison queries to datasets from offline reinforcement learning. In addition, we present an empirical evaluation of the \\emph{risk versus coverage} trade-off for a class of model-based baselines. In particular, the baselines learn ensembles of dynamics models, which are used in various ways to produce simulations for answering queries with confidence values. While our results suggest advantages for certain baseline variations, there appears to be significant room for improvement in future work."}}
{"id": "n8hGHUfZ3Sy", "cdate": 1664314532416, "mdate": null, "content": {"title": "Group SELFIES: A Robust Fragment-Based Molecular String Representation", "abstract": "We introduce Group SELFIES, a molecular string representation that leverages group tokens to represent functional groups or entire substructures while maintaining chemical robustness guarantees. Molecular string representations, such as SMILES and SELFIES, serve as the basis for molecular generation and optimization in chemical language models, deep generative models, and evolutionary methods. While SMILES and SELFIES leverage atomic representations, Group SELFIES builds on top of the chemical robustness guarantees of SELFIES by enabling group tokens, thereby creating additional flexibility to the representation. Moreover, the group tokens in Group SELFIES can take advantage of inductive biases of molecular fragments that capture meaningful chemical motifs. The advantages of capturing chemical motifs and flexibility are demonstrated in our experiments, which show that Group SELFIES improves distribution learning of common molecular datasets. Further experiments also show that random sampling of Group SELFIES strings improves the quality of generated molecules compared to regular SELFIES strings. Our open-source implementation of Group SELFIES is available at \\url{https://github.com/aspuru-guzik-group/group-selfies}, which we hope will aid future research in molecular generation and optimization."}}
{"id": "b832-LQhwYP", "cdate": 1664314531724, "mdate": null, "content": {"title": "Conformer Search Using SE3-Transformers and Imitation Learning", "abstract": " We introduce a novel approach to conformer search, the discovery of three-dimensional structures for two-dimensional molecular formulas. We focus on organic molecules using deep imitation learning and equivariant graph neural networks, with the prospect of using reinforcement learning algorithms for fine tuning. To that end, we present our interactive environment that describes the molecule in a ridig-rotor approximation and leverage a behavioral cloning torsion policy to autoregressively determine the dihedral angles of the molecule ultimately yielding a three-dimensional molecular structure. For our policy architecture, we leverage an SE(3) equivariant neural network, which enables us to exploit inherent molecular symmetries and to respect the topology of the angle distribution using a Mixture of Projected Normals action distribution. Our preliminary results for a policy trained on a behavioral cloning objective using the QM9 dataset for expert trajectories shows that the policy can accurately predict torsion angles for various molecules. We believe this to be a promising starting point for future work pertaining to performing conformer search using deep reinforcement learning."}}
{"id": "AsOLzq1S-p", "cdate": 1663850150803, "mdate": null, "content": {"title": "Offline Policy Comparison with Confidence: Benchmarks and Baselines", "abstract": "Decision makers often wish to use offline historical data to compare sequential-action policies at various world states. Importantly, computational tools should produce confidence values for such offline policy comparison (OPC) to account for statistical variance and limited data coverage. Nevertheless, there is little work that directly evaluates the quality of confidence values for OPC. In this work, we address this issue by creating benchmarks for OPC with Confidence (OPCC), derived by adding sets of policy comparison queries to datasets from offline reinforcement learning. In addition, we present an empirical evaluation of the \"risk versus coverage\" trade-off for a class of model-based baselines. In particular, the baselines learn ensembles of dynamics models, which are used in various ways to produce simulations for answering queries with confidence values. While our results suggest advantages for certain baseline variations, there appears to be significant room for improvement in future work."}}
{"id": "uCaNr6_dQB0", "cdate": 1655376342149, "mdate": null, "content": {"title": "Modularity through Attention: Efficient Training and Transfer of Language-Conditioned Policies for Robot Manipulation", "abstract": "Language-conditioned policies allow robots to interpret and execute human instructions. Learning such policies requires a substantial investment with regards to time and compute resources. Still, the resulting controllers are highly device-specific and cannot easily be transferred to a robot with different morphology, capability, appearance or dynamics. In this paper, we propose a sample-efficient approach for training language-conditioned manipulation policies that allows for rapid transfer across different types of robots. By introducing a novel method, namely Hierarchical Modularity, and adopting supervised attention across multiple sub-modules, we bridge the divide between modular and end-to-end learning and enable the reuse of functional building blocks. In both simulated and real world robot manipulation experiments, we demonstrate that our method outperforms the current state-of-the-art methods and can transfer policies across 4 different robots in a sample-efficient manner. Finally, we show that the functionality of learned sub-modules is maintained beyond the training process and can be used to introspect the robot decision-making process."}}
{"id": "SUI7tllPw7q", "cdate": 1653438350552, "mdate": 1653438350552, "content": {"title": "DNS: Determinantal Point Process Based Neural Network Sampler for Ensemble Reinforcement Learning", "abstract": "The application of an ensemble of neural networks is becoming an imminent tool for advancing state-of-the-art deep reinforcement learning\nalgorithms. However, training these large numbers of neural networks in the ensemble has an\nexceedingly high computation cost which may become a hindrance in training large-scale systems.\nIn this paper, we propose DNS: a Determinantal\nPoint Process based Neural Network Sampler that\nspecifically uses k-DPP to sample a subset of neural networks for backpropagation at every training step thus significantly reducing the training\ntime and computation cost. We integrated DNS in\nREDQ for continuous control tasks and evaluated\non MuJoCo environments. Our experiments show\nthat DNS augmented REDQ matches the baseline\nREDQ in terms of average cumulative reward and\nachieves this using less than 50% computation\nwhen measured in FLOPS."}}
