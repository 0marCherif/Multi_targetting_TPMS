{"id": "q2d12xIUSMP", "cdate": 1698611581378, "mdate": 1698611581378, "content": {"title": "LightSpeed: Light and Fast Neural Light Fields on Mobile Devices", "abstract": "Real-time novel-view image synthesis on mobile devices is prohibitive due to the limited computational power and storage. Using volumetric rendering methods, such as NeRF and its derivatives, on mobile devices is not suitable due to the high computational cost of volumetric rendering. On the other hand, recent advances in neural light field representations have shown promising real-time view synthesis results on mobile devices. Neural light field methods learn a direct mapping from a ray representation to the pixel color. The current choice of ray representation is either stratified ray sampling or Pl\u00fccker coordinates, overlooking the classic light slab (two-plane) representation, the preferred representation to interpolate between light field views. In this work, we find that using the light slab representation is an efficient representation for learning a neural light field. More importantly, it is a lower-dimensional ray representation enabling us to learn the 4D ray space using feature grids which are significantly faster to train and render. Although mostly designed for frontal views, we show that the light-slab representation can be further extended to non-frontal scenes using a divide-and-conquer strategy. Our method provides better rendering quality than prior light field methods and a significantly better trade-off between rendering quality and speed than prior light field methods."}}
{"id": "rise4b1zp8", "cdate": 1672531200000, "mdate": 1681651498804, "content": {"title": "DyLiN: Making Light Field Networks Dynamic", "abstract": ""}}
{"id": "nxFFi92Ton", "cdate": 1672531200000, "mdate": 1681651498805, "content": {"title": "Flow supervision for Deformable NeRF", "abstract": ""}}
{"id": "cNqGPfq5AJE", "cdate": 1672531200000, "mdate": 1681651498741, "content": {"title": "CoNFies: Controllable Neural Face Avatars", "abstract": ""}}
{"id": "DCj-HmLiqvh", "cdate": 1672531200000, "mdate": 1681651498747, "content": {"title": "Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring", "abstract": ""}}
{"id": "21wIfbopNfc", "cdate": 1669015561161, "mdate": null, "content": {"title": "Synthetic Expressions are Better Than Real for Learning to Detect Facial Actions", "abstract": "Critical obstacles in training classifiers to detect facial actions are the limited sizes of annotated video databases and the relatively low frequencies of occurrence of many actions. To address these problems, we propose an approach that makes use of facial expression generation. Our approach reconstructs the 3D shape of the face from each video frame, aligns the 3D mesh to a canonical view, and then trains a GAN-based network to synthesize novel images with facial action units of interest. To evaluate this approach, a deep neural network was trained on two separate datasets: One network was trained on video of synthesized facial expressions generated from FERA17; the other network was trained on unaltered video from the same database. Both networks used the same train and validation partitions and were tested on the test partition of actual video from FERA17. The network trained on synthesized facial expressions outperformed the one trained on actual facial expressions and surpassed current state-of-the-art approaches."}}
{"id": "R7I5dYogmS", "cdate": 1669015313689, "mdate": 1669015313689, "content": {"title": "Cross-domain au detection: Domains, learning approaches, and measures", "abstract": "Facial action unit (AU) detectors have performed well when trained and tested within the same domain. Do AU detectors transfer to new domains in which they have not been trained? To answer this question, we review literature on cross-domain transfer and conduct experiments to address limitations of prior research. We evaluate both deep and shallow approaches to AU detection (CNN and SVM, respectively) in two large, well-annotated, publicly available databases, Expanded BP4D+ and GFT. The databases differ in observational scenarios, participant characteristics, range of head pose, video resolution, and AU base rates. For both approaches and databases, performance decreased with change in domain, often to below the threshold needed for behavioral research. Decreases were not uniform, however. They were more pronounced for GFT than for Expanded BP4D+ and for shallow relative to deep learning. These findings suggest that more varied domains and deep learning approaches may be better suited for promoting generalizability. Until further improvement is realized, caution is warranted when applying AU classifiers from one domain to another."}}
{"id": "dzrhWeo_CJq", "cdate": 1669014730634, "mdate": null, "content": {"title": "D-pattnet: Dynamic patch-attentive deep network for action unit detection", "abstract": "Facial action units (AUs) relate to specific local facial regions. Recent efforts in automated AU detection have focused on learning the facial patch representations to detect specific AUs. These efforts have encountered three hurdles. First, they implicitly assume that facial patches are robust to head rotation; yet non-frontal rotation is common. Second, mappings between AUs and patches are defined a priori, which ignores co-occurrences among AUs. And third, the dynamics of AUs are either ignored or modeled sequentially rather than simultaneously as in human perception. Inspired by recent advances in human perception, we propose a dynamic patch-attentive deep network, called D-PAttNet, for AU detection that i) controls for 3D head and face rotation, ii) learns mappings of patches to AUs, and iii) models spatiotemporal dynamics. D-PAttNet approach significantly improves upon existing state of the art."}}
{"id": "i1bFPSw42W0", "cdate": 1654111652236, "mdate": null, "content": {"title": "MBW: Multi-view Bootstrapping in the Wild", "abstract": "Labeling articulated objects in unconstrained settings has a wide variety of applications including entertainment, neuroscience, psychology, ethology, and many fields of medicine. Large offline labeled datasets do not exist for all but the most common articulated object categories (e.g., humans). Hand labeling these landmarks within a video sequence is a laborious task. Learned landmark detectors can help, but can be error-prone when trained from only a few examples. Multi-camera systems that train fine-grained detectors have shown significant promise in detecting such errors, allowing for self-supervised solutions that only need a small percentage of the video sequence to be hand-labeled. The approach, however, is based on calibrated cameras and rigid geometry, making it expensive, difficult to manage, and impractical in real-world scenarios. In this paper, we address these bottlenecks by combining a non-rigid 3D neural prior with deep flow to obtain high-fidelity landmark estimates from videos with only two or three uncalibrated, handheld cameras. With just a few annotations (representing $1-2\\%$ of the frames), we are able to produce 2D results comparable to state-of-the-art fully supervised methods, along with 3D reconstructions that are impossible with other existing approaches. Our Multi-view Bootstrapping in the Wild (MBW) approach demonstrates impressive results on standard human datasets, as well as tigers, cheetahs, fish, colobus monkeys, chimpanzees, and flamingos from videos captured casually in a zoo. We release the codebase for MBW as well as this challenging zoo dataset consisting of image frames of tail-end distribution categories with their corresponding 2D and 3D labels generated from minimal human intervention."}}
{"id": "n7-UcWfR7R", "cdate": 1640995200000, "mdate": 1668206029131, "content": {"title": "3D Human Pose, Shape and Texture From Low-Resolution Images and Videos", "abstract": ""}}
