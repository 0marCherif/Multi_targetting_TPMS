{"id": "DH9IKM_hP4", "cdate": 1685577600000, "mdate": 1683878872242, "content": {"title": "Graph Self-Supervised Learning: A Survey", "abstract": "Deep learning on graphs has attracted significant interests recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">graph self-supervised learning</i> , we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We construct a unified framework that mathematically formalizes the paradigm of graph SSL. According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We further describe the applications of graph SSL across various research fields and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph SSL. Finally, we discuss the remaining challenges and potential future directions in this research field."}}
{"id": "9vI9ulkUDf", "cdate": 1680307200000, "mdate": 1682319812955, "content": {"title": "Compact network embedding for fast node classification", "abstract": ""}}
{"id": "3sj4KndQDsl", "cdate": 1680307200000, "mdate": 1683773854029, "content": {"title": "Hierarchical attention neural network for information cascade prediction", "abstract": ""}}
{"id": "JtSlA972EHP", "cdate": 1676827070864, "mdate": null, "content": {"title": "Fast Heterogeneous Federated Learning with Hybrid Client Selection", "abstract": "Client selection schemes are widely adopted to handle the communication-efficient problems in recent studies of Federated Learning (FL). However, the large variance of the model updates aggregated from the randomly-selected unrepresentative subsets directly slows the FL convergence. We present a novel clustering-based client selection scheme to accelerate the FL convergence by variance reduction. Simple yet effective schemes are designed to improve the clustering effect and control the effect fluctuation, therefore, generating the client subset with certain representativeness of sampling. Theoretically, we demonstrate the improvement of the proposed scheme in variance reduction. We also present the tighter convergence guarantee of the proposed method thanks to the variance reduction. Experimental results confirm the exceed efficiency of our scheme compared to alternatives."}}
{"id": "MjQwQS7ag-0", "cdate": 1675332330737, "mdate": 1675332330737, "content": {"title": " A Dynamic Variational Framework for Open-World Node Classification in Structured Sequences", "abstract": "Structured sequences are a popular data representation, used to model complex data such as traffic networks. A key machine learning task for structured sequences is node classification, that is predicting the class labels of unlabeled nodes. Though many node classification models were proposed, they assume a closed world setting, that all class labels appear in the training data. But in the real-world, the presence of  never-before-seen class labels in testing data can considerably degrade a classifier\u2019s accuracy. A promising solution to this issue is to build classifiers for an open-world setting, where samples with unknown class labels are continuously observed such that\ntraining and testing data may have different class label spaces.  Several approaches have been proposed for open-world learning\nproblems in computer vision and natural language processing, but they cannot be applied directly to structured sequences due\nto the complexity of their non-Euclidean properties and their dynamic nature. This paper addresses this important research\ngap by proposing a novel Open-world Structured Sequence node Classification (OSSC) model, to learn from structured sequences\nin an open-world setting. OSSC captures the structural and temporal information via a GCN-based dynamic variational\nframework. A latent distribution sequence is learned for each node using both stochastic states and deterministic states, to\ncapture the evolution of node attributes and topology, followed by a sampling process to generate node representations. An\nopen-world classification loss is further adopted to ensure that node representations are sensitive to unknown classes. And a\ncombination of Openmax and Softmax is utilized to recognize nodes from unknown classes and to classify others to one of the\nknown classes. Experiments on real-world datasets show that the proposed OSSC method is capable of learning accurate openworld node classifiers from structured sequence data."}}
{"id": "v4HLGCpZVA", "cdate": 1675209600000, "mdate": 1684181688560, "content": {"title": "Learning Graph Representations With Maximal Cliques", "abstract": "Non-Euclidean property of graph structures has faced interesting challenges when deep learning methods are applied. Graph convolutional networks (GCNs) can be regarded as one of the successful approaches to classification tasks on graph data, although the structure of this approach limits its performance. In this work, a novel representation learning approach is introduced based on spectral convolutions on graph-structured data in a semisupervised learning setting. Our proposed method, COnvOlving cLiques (COOL), is constructed as a neighborhood aggregation approach for learning node representations using established GCN architectures. This approach relies on aggregating local information by finding maximal cliques. Unlike the existing graph neural networks which follow a traditional neighborhood averaging scheme, COOL allows for aggregation of densely connected neighboring nodes of potentially differing locality. This leads to substantial improvements on multiple transductive node classification tasks."}}
{"id": "Do7vet7YIbh", "cdate": 1674013696341, "mdate": 1674013696341, "content": {"title": "Neighbor Contrastive Learning on Learnable Graph Augmentation", "abstract": "Recent years, graph contrastive learning (GCL), which aims to learn representations from unlabeled graphs, has made great progress. However, the existing GCL methods mostly adopt human-designed graph augmentations, which are sensitive to various graph datasets. In addition, the contrastive losses originally developed in computer vision have been directly applied to graph data, where the neighboring nodes are regarded as negatives and consequently pushed far apart from the anchor. However, this is contradictory with the homophily assumption of net-works that connected nodes often belong to the same class and should be close to each other. In this work, we propose an end-to-end automatic GCL method, named NCLA to apply neighbor contrastive learning on learnable graph augmentation. Several graph augmented views with adaptive topology are automatically learned by the multi-head graph attention mechanism, which can be compatible with various graph datasets without prior domain knowledge. In addition, a neighbor contrastive loss is devised to allow multiple positives per anchor by taking network topology as the supervised signals. Both augmentations and embeddings are learned end-to-end in the pro-posed NCLA. Extensive experiments on the benchmark datasets demonstrate that NCLA yields the state-of-the-art node classification performance on self-supervised GCL and even exceeds the supervised ones, when the labels are extremely limited. Our code is released at https://github.com/shenxiaocam/NCLA."}}
{"id": "tMhA4OCZs6O", "cdate": 1672531200000, "mdate": 1683880899321, "content": {"title": "CurvDrop: A Ricci Curvature Based Approach to Prevent Graph Neural Networks from Over-Smoothing and Over-Squashing", "abstract": "Graph neural networks (GNNs) are powerful models to handle graph data and can achieve state-of-the-art in many critical tasks including node classification and link prediction. However, existing graph neural networks still face both challenges of over-smoothing and over-squashing based on previous literature. To this end, we propose a new Curvature-based topology-aware Dropout sampling technique named CurvDrop, in which we integrate the Discrete Ricci Curvature into graph neural networks to enable more expressive graph models. Also, this work can improve graph neural networks by quantifying connections in graphs and using structural information such as community structures in graphs. As a result, our method can tackle the both challenges of over-smoothing and over-squashing with theoretical justification. Also, numerous experiments on public datasets show the effectiveness and robustness of our proposed method. The code and data are released in https://github.com/liu-yang-maker/Curvature-based-Dropout."}}
{"id": "pF5OYApIMy3", "cdate": 1672531200000, "mdate": 1684308790203, "content": {"title": "Heterogeneous deep graph convolutional network with citation relational BERT for COVID-19 inline citation recommendation", "abstract": ""}}
{"id": "nNvdEEr3BDW", "cdate": 1672531200000, "mdate": 1684308791239, "content": {"title": "Neighbor Contrastive Learning on Learnable Graph Augmentation", "abstract": "Recent years, graph contrastive learning (GCL), which aims to learn representations from unlabeled graphs, has made great progress. However, the existing GCL methods mostly adopt human-designed graph augmentations, which are sensitive to various graph datasets. In addition, the contrastive losses originally developed in computer vision have been directly applied to graph data, where the neighboring nodes are regarded as negatives and consequently pushed far apart from the anchor. However, this is contradictory with the homophily assumption of networks that connected nodes often belong to the same class and should be close to each other. In this work, we propose an end-to-end automatic GCL method, named NCLA to apply neighbor contrastive learning on learnable graph augmentation. Several graph augmented views with adaptive topology are automatically learned by the multi-head graph attention mechanism, which can be compatible with various graph datasets without prior domain knowledge. In addition, a neighbor contrastive loss is devised to allow multiple positives per anchor by taking network topology as the supervised signals. Both augmentations and embeddings are learned end-to-end in the proposed NCLA. Extensive experiments on the benchmark datasets demonstrate that NCLA yields the state-of-the-art node classification performance on self-supervised GCL and even exceeds the supervised ones, when the labels are extremely limited. Our code is released at https://github.com/shenxiaocam/NCLA."}}
