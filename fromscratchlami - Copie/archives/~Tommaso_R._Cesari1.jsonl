{"id": "euUgX7XM9j", "cdate": 1621630010848, "mdate": null, "content": {"title": "ROI Maximization in Stochastic Online Decision-Making", "abstract": "We introduce a novel theoretical framework for Return On Investment (ROI) maximization in repeated decision-making. Our setting is motivated by the use case of companies that regularly receive proposals for technological innovations and want to quickly decide whether they are worth implementing. We design an algorithm for learning ROI-maximizing decision-making policies over a sequence of innovation proposals. Our algorithm provably converges to an optimal policy in class $\\Pi$ at a rate of order $\\min\\big\\{1/(N\\Delta^2),N^{-1/3}\\}$, where $N$ is the number of innovations and $\\Delta$ is the suboptimality gap in $\\Pi$. A significant hurdle of our formulation, which sets it aside from other online learning problems such as bandits, is that running a policy does not provide an unbiased estimate of its performance."}}
{"id": "yrqn9rQO2YT", "cdate": 1621629990627, "mdate": null, "content": {"title": "Instance-Dependent Bounds for Zeroth-order Lipschitz Optimization with Error Certificates", "abstract": "We study the problem of zeroth-order (black-box) optimization of a Lipschitz function $f$ defined on a compact subset $\\mathcal{X}$ of $\\mathbb{R}^d$, with the additional constraint that algorithms must certify the accuracy of their recommendations. We characterize the optimal number of evaluations of any Lipschitz function $f$ to find and certify an approximate maximizer of $f$ at accuracy $\\varepsilon$. Under a weak assumption on $\\mathcal{X}$, this optimal sample complexity is shown to be nearly proportional to the integral $\\int_{\\mathcal{X}} \\mathrm{d}\\boldsymbol{x}/( \\max(f) - f(\\boldsymbol{x}) + \\varepsilon )^d$. This result, which was only (and partially) known in dimension $d=1$, solves an open problem dating back to 1991. In terms of techniques, our upper bound relies on a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that we link to the above integral. We also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds. Our instance-dependent lower bound differs from traditional worst-case lower bounds in the Lipschitz setting and relies on a local worst-case analysis that could likely prove useful for other learning tasks."}}
{"id": "ZDqkpag3WVy", "cdate": 1609459200000, "mdate": null, "content": {"title": "An Efficient Algorithm for Cooperative Semi-Bandits", "abstract": "We consider the problem of asynchronous online combinatorial optimization on a network of communicating agents. At each time step, some of the agents are stochastically activated, requested to make..."}}
{"id": "IxaIQfDYE7", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Regret Analysis of Bilateral Trade", "abstract": "Bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. Despite the simplicity of this problem, a classical result by Myerson and Satterthwaite (1983) affirms the impossibility of designing a mechanism which is simultaneously efficient, incentive compatible, individually rational, and budget balanced. This impossibility result fostered an intense investigation of meaningful trade-offs between these desired properties. Much work has focused on approximately efficient fixed-price mechanisms, i.e., Blumrosen and Dobzinski (2014; 2016), Colini-Baldeschi et al. (2016), which have been shown to fully characterize strong budget balanced and ex-post individually rational direct revelation mechanisms. All these results, however, either assume some knowledge on the priors of the seller/buyer valuations, or a black box access to some samples of the distributions, as in D{\\\"u}tting et al. (2021). In this paper, we cast for the first time the bilateral trade problem in a regret minimization framework over rounds of seller/buyer interactions, with no prior knowledge on the private seller/buyer valuations. Our main contribution is a complete characterization of the regret regimes for fixed-price mechanisms with different models of feedback and private valuations, using as benchmark the best fixed price in hindsight. More precisely, we prove the following bounds on the regret: $\\bullet$ $\\widetilde{\\Theta}(\\sqrt{T})$ for full-feedback (i.e., direct revelation mechanisms); $\\bullet$ $\\widetilde{\\Theta}(T^{2/3})$ for realistic feedback (i.e., posted-price mechanisms) and independent seller/buyer valuations with bounded densities; $\\bullet$ $\\Theta(T)$ for realistic feedback and seller/buyer valuations with bounded densities; $\\bullet$ $\\Theta(T)$ for realistic feedback and independent seller/buyer valuations; $\\bullet$ $\\Theta(T)$ for the adversarial setting."}}
{"id": "6rQQ2My50cn", "cdate": 1609459200000, "mdate": null, "content": {"title": "The Sample Complexity of Level Set Approximation", "abstract": "We study the problem of approximating the level set of an unknown function by sequentially querying its values. We introduce a family of algorithms called Bisect and Approximate through which we reduce the level set approximation problem to a local function approximation problem. We then show how this approach leads to rate-optimal sample complexity guarantees for H\u00f6lder functions, and we investigate how such rates improve when additional smoothness or other structural assumptions hold true."}}
{"id": "t5GjUlHF7q6", "cdate": 1577836800000, "mdate": null, "content": {"title": "Regret analysis of the Piyavskii-Shubert algorithm for global Lipschitz optimization", "abstract": "We consider the problem of maximizing a non-concave Lipschitz multivariate function over a compact domain by sequentially querying its (possibly perturbed) values. We study a natural algorithm designed originally by Piyavskii and Shubert in 1972, for which we prove new bounds on the number of evaluations of the function needed to reach or certify a given optimization accuracy. Our analysis uses a bandit-optimization viewpoint and solves an open problem from Hansen et al.\\ (1991) by bounding the number of evaluations to certify a given accuracy with a near-optimal sum of packing numbers."}}
{"id": "-R55hoaor2", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Nearest Neighbor Characterization of Lebesgue Points in Metric Measure Spaces", "abstract": "The property of almost every point being a Lebesgue point has proven to be crucial for the consistency of several classification algorithms based on nearest neighbors. We characterize Lebesgue points in terms of a 1-Nearest Neighbor regression algorithm for pointwise estimation, fleshing out the role played by tie-breaking rules in the corresponding convergence problem. We then give an application of our results, proving the convergence of the risk of a large class of 1-Nearest Neighbor classification algorithms in general metric spaces where almost every point is a Lebesgue point."}}
{"id": "m7N4hrEnI8", "cdate": 1546300800000, "mdate": null, "content": {"title": "Cooperative Online Learning: Keeping your Neighbors Updated", "abstract": "We study an asynchronous online learning setting with a network of agents. At each time step, some of the agents are activated, requested to make a prediction, and pay the corresponding loss. The loss function is then revealed to these agents and also to their neighbors in the network. Our results characterize how much knowing the network structure affects the regret as a function of the model of agent activations. When activations are stochastic, the optimal regret (up to constant factors) is shown to be of order $\\sqrt{\\alpha T}$, where $T$ is the horizon and $\\alpha$ is the independence number of the network. We prove that the upper bound is achieved even when agents have no information about the network structure. When activations are adversarial the situation changes dramatically: if agents ignore the network structure, a $\\Omega(T)$ lower bound on the regret can be proven, showing that learning is impossible. However, when agents can choose to ignore some of their neighbors based on the knowledge of the network structure, we prove a $O(\\sqrt{\\overline{\\chi} T})$ sublinear regret bound, where $\\overline{\\chi} \\ge \\alpha$ is the clique-covering number of the network."}}
{"id": "CcbngYtLlv", "cdate": 1546300800000, "mdate": null, "content": {"title": "Dynamic Pricing with Finitely Many Unknown Valuations", "abstract": "Motivated by posted price auctions where buyers are grouped in an unknown number of latent types characterized by their private values for the good on sale, we investigate regret minimization in st..."}}
