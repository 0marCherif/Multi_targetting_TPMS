{"id": "e0heI1h3tZ", "cdate": 1680014240170, "mdate": 1680014240170, "content": {"title": "Optimal Algorithms for Differentially Private Stochastic Monotone Variational Inequalities and Saddle-Point Problems", "abstract": "In this work, we conduct the first systematic study of stochastic variational inequality (SVI) and stochastic saddle point (SSP) problems under the constraint of differential privacy (DP). We propose two algorithms: Noisy Stochastic Extragradient (NSEG) and Noisy Inexact Stochastic Proximal Point (NISPP). We show that a stochastic approximation variant of these algorithms attains risk bounds vanishing as a function of the dataset size, with respect to the strong gap function; and a sampling with replacement variant achieves optimal risk bounds with respect to a weak gap function. We also show lower bounds of the same order on weak gap function. Hence, our algorithms are optimal. Key to our analysis is the investigation of algorithmic stability bounds, both of which are new even in the nonprivate case. The dependence of the running time of the sampling with replacement algorithms, with respect to the dataset size n, is n2 for NSEG and \u00d5(n3/2) for NISPP."}}
{"id": "agihaAKJ89X", "cdate": 1652737834114, "mdate": null, "content": {"title": "Differentially Private Generalized Linear Models Revisited", "abstract": "We study the problem of $(\\epsilon,\\delta)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an  upper bound on the excess population risk of $\\tilde{O}\\left(\\frac{\\Vert w^*\\Vert}{\\sqrt{n}} + \\min\\left\\{\\frac{\\Vert w^* \\Vert^2}{(n\\epsilon)^{2/3}},\\frac{\\sqrt{d}\\Vert w^*\\Vert^2}{n\\epsilon}\\right\\}\\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\\Vert w^\\ast\\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\\tilde{\\Omega}\\left(\\frac{1}{\\sqrt{n}} + {\\min\\left\\{\\frac{\\Vert w^*\\Vert^{4/3}}{(n\\epsilon)^{2/3}}, \\frac{\\sqrt{d}\\Vert w^*\\Vert}{n\\epsilon}\\right\\}}\\right)$. We also revisit the previously studied case of Lipschitz losses \\cite{SSTT21}.  For this case, we close the gap in the existing work and show that the optimal rate is (up to log factors) $\\Theta\\left(\\frac{\\Vert w^*\\Vert}{\\sqrt{n}} + \\min\\left\\{\\frac{\\Vert w^*\\Vert}{\\sqrt{n\\epsilon}},\\frac{\\sqrt{\\text{rank}}\\Vert w^*\\Vert}{n\\epsilon}\\right\\}\\right)$, where $\\text{rank}$ is the rank of the design matrix. This improves over existing work in the high privacy regime. Finally, our algorithms involve a private model selection approach that we develop to enable attaining the stated rates without a-priori knowledge of $\\Vert w^*\\Vert$. "}}
{"id": "BRZos-8TpCf", "cdate": 1652737767846, "mdate": null, "content": {"title": "Stochastic Halpern Iteration with Variance Reduction for Stochastic Monotone Inclusions", "abstract": "We study stochastic monotone inclusion problems, which widely appear in machine learning applications, including robust regression and adversarial learning. We propose novel variants of stochastic Halpern iteration with recursive variance reduction. In the cocoercive---and more generally Lipschitz-monotone---setup, our algorithm attains $\\epsilon$ norm of the operator with $\\mathcal{O}(\\frac{1}{\\epsilon^3})$ stochastic operator evaluations, which significantly improves over state of the art $\\mathcal{O}(\\frac{1}{\\epsilon^4})$ stochastic operator evaluations required for existing monotone inclusion solvers applied to the same problem classes. We further show how to couple one of the proposed variants of stochastic Halpern iteration with a scheduled restart scheme to solve stochastic monotone inclusion problems with ${\\mathcal{O}}(\\frac{\\log(1/\\epsilon)}{\\epsilon^2})$ stochastic operator evaluations under additional sharpness or strong monotonicity assumptions. "}}
{"id": "_gA20SUfd4a", "cdate": 1652737527116, "mdate": null, "content": {"title": "Between Stochastic and Adversarial Online Convex Optimization: Improved Regret Bounds via Smoothness", "abstract": "Stochastic and adversarial data are two widely studied settings in online learning. But many optimization\ntasks are neither i.i.d. nor fully adversarial, which makes it of  fundamental interest to get a better theoretical\n understanding of the world between these extremes.\n In this work we establish novel regret bounds for online convex\n optimization in a setting that interpolates between stochastic\n i.i.d. and fully adversarial losses. By exploiting smoothness of\n the expected losses, these bounds replace a dependence on the maximum\n gradient length by the variance of the gradients, which was previously\n known only for linear losses. In addition, they weaken the i.i.d.\n assumption by allowing, for example, adversarially poisoned rounds,\n which were previously considered in the expert and bandit setting. Our results extend this to the online convex \noptimization framework.  In the fully i.i.d. case, our bounds match the rates one would expect\n from results in stochastic acceleration, and in the fully adversarial\n case they gracefully deteriorate to match the minimax regret.  \nWe further provide lower bounds showing that our regret upper bounds are\ntight for all intermediate regimes in terms of the stochastic variance and the\nadversarial variation of the loss gradients."}}
{"id": "uZJJFpFl60W", "cdate": 1621630150419, "mdate": null, "content": {"title": "Best-case lower bounds in online learning", "abstract": "Much of the work in online learning focuses on the study of sublinear upper bounds on the regret. In this work, we initiate the study of best-case lower bounds in online convex optimization, wherein we bound the largest \\emph{improvement} an algorithm can obtain relative to the single best action in hindsight. This problem is motivated by the goal of better understanding the adaptivity of a learning algorithm. Another motivation comes from fairness: it is known that best-case lower bounds are instrumental in obtaining algorithms for decision-theoretic online learning (DTOL) that satisfy a notion of group fairness. Our contributions are a general method to provide best-case lower bounds in Follow The Regularized Leader (FTRL) algorithms with time-varying regularizers, which we use to show that best-case lower bounds are of the same order as existing upper regret bounds: this includes situations with a fixed learning rate, decreasing learning rates, timeless methods, and adaptive gradient methods. In stark contrast, we show that the linearized version of FTRL can attain negative linear regret. Finally, in DTOL with two experts and binary losses, we fully characterize the best-case sequences, which provides a finer understanding of the best-case lower bounds."}}
{"id": "wn79e85F42W", "cdate": 1621630057341, "mdate": null, "content": {"title": "Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings", "abstract": "  We study differentially private stochastic optimization in convex and non-convex settings. For the convex case, we focus on the family of non-smooth generalized linear losses (GLLs). Our algorithm for the $\\ell_2$ setting achieves optimal excess population risk in near-linear time, while the best known differentially private algorithms for general convex losses run in super-linear time. Our algorithm for the $\\ell_1$ setting has nearly-optimal excess population risk $\\tilde{O}\\big(\\sqrt{\\frac{\\log{d}}{n}}\\big)$, and circumvents the dimension dependent lower bound of \\cite{Asi:2021} for general non-smooth convex losses. In the differentially private non-convex setting, we provide several new algorithms for approximating stationary points of the population risk. For the $\\ell_1$-case with smooth losses and polyhedral constraint, we provide the first nearly dimension independent rate, $\\tilde O\\big(\\frac{\\log^{2/3}{d}}{{n^{1/3}}}\\big)$ in linear time. For the constrained $\\ell_2$-case, with smooth losses, we obtain a linear-time algorithm with rate $\\tilde O\\big(\\frac{1}{n^{3/10}d^{1/10}}+\\big(\\frac{d}{n^2}\\big)^{1/5}\\big)$. \n  Finally, for the $\\ell_2$-case we provide the first method  for {\\em non-smooth weakly convex} stochastic optimization with rate $\\tilde O\\big(\\frac{1}{n^{1/4}}+\\big(\\frac{d}{n^2}\\big)^{1/6}\\big)$ which matches the best existing non-private algorithm when $d= O(\\sqrt{n})$. We also extend all our results above for the non-convex $\\ell_2$ setting to the $\\ell_p$ setting, where $1 < p \\leq 2$, with only polylogarithmic (in the dimension) overhead in the rates."}}
{"id": "Ra-2OvXr7UU", "cdate": 1621630057341, "mdate": null, "content": {"title": "Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings", "abstract": "  We study differentially private stochastic optimization in convex and non-convex settings. For the convex case, we focus on the family of non-smooth generalized linear losses (GLLs). Our algorithm for the $\\ell_2$ setting achieves optimal excess population risk in near-linear time, while the best known differentially private algorithms for general convex losses run in super-linear time. Our algorithm for the $\\ell_1$ setting has nearly-optimal excess population risk $\\tilde{O}\\big(\\sqrt{\\frac{\\log{d}}{n}}\\big)$, and circumvents the dimension dependent lower bound of \\cite{Asi:2021} for general non-smooth convex losses. In the differentially private non-convex setting, we provide several new algorithms for approximating stationary points of the population risk. For the $\\ell_1$-case with smooth losses and polyhedral constraint, we provide the first nearly dimension independent rate, $\\tilde O\\big(\\frac{\\log^{2/3}{d}}{{n^{1/3}}}\\big)$ in linear time. For the constrained $\\ell_2$-case, with smooth losses, we obtain a linear-time algorithm with rate $\\tilde O\\big(\\frac{1}{n^{3/10}d^{1/10}}+\\big(\\frac{d}{n^2}\\big)^{1/5}\\big)$. \n  Finally, for the $\\ell_2$-case we provide the first method  for {\\em non-smooth weakly convex} stochastic optimization with rate $\\tilde O\\big(\\frac{1}{n^{1/4}}+\\big(\\frac{d}{n^2}\\big)^{1/6}\\big)$ which matches the best existing non-private algorithm when $d= O(\\sqrt{n})$. We also extend all our results above for the non-convex $\\ell_2$ setting to the $\\ell_p$ setting, where $1 < p \\leq 2$, with only polylogarithmic (in the dimension) overhead in the rates."}}
{"id": "kg0YSz63Vpm", "cdate": 1609459200000, "mdate": null, "content": {"title": "Non-Euclidean Differentially Private Stochastic Convex Optimization", "abstract": "Differentially private (DP) stochastic convex optimization (SCO) is a fundamental problem, where the goal is to approximately minimize the population risk with respect to a convex loss function, given a dataset of $n$ i.i.d. samples from a distribution, while satisfying differential privacy with respect to the dataset. Most of the existing works in the literature of private convex optimization focus on the Euclidean (i.e., $\\ell_2$) setting, where the loss is assumed to be Lipschitz (and possibly smooth) w.r.t. the $\\ell_2$ norm over a constraint set with bounded $\\ell_2$ diameter. Algorithms based on noisy stochastic gradient descent (SGD) are known to attain the optimal excess risk in this setting. In this work, we conduct a systematic study of DP-SCO for $\\ell_p$-setups under a standard smoothness assumption on the loss. For $1< p\\leq 2$, under a standard smoothness assumption, we give a new, linear-time DP-SCO algorithm with optimal excess risk. Previously known constructions with optimal excess risk for $1< p <2$ run in super-linear time in $n$. For $p=1$, we give an algorithm with nearly optimal excess risk. Our result for the $\\ell_1$-setup also extends to general polyhedral norms and feasible sets. Moreover, we show that the excess risk bounds resulting from our algorithms for $1\\leq p \\leq 2$ are attained with high probability. For $2 < p \\leq \\infty$, we show that existing linear-time constructions for the Euclidean setup attain a nearly optimal excess risk in the low-dimensional regime. As a consequence, we show that such constructions attain a nearly optimal excess risk for $p=\\infty$. Our work draws upon concepts from the geometry of normed spaces, such as the notions of regularity, uniform convexity, and uniform smoothness."}}
{"id": "c9Y6-YmyonW", "cdate": 1609459200000, "mdate": null, "content": {"title": "The Complexity of Nonconvex-Strongly-Concave Minimax Optimization", "abstract": "This paper studies the complexity for finding approximate stationary points of nonconvex-strongly-concave (NC-SC) smooth minimax problems, in both general and averaged smooth finite-sum settings. We establish nontrivial lower complexity bounds of $\\Omega(\\sqrt{\\kappa}\\Delta L\\epsilon^{-2})$ and $\\Omega(n+\\sqrt{n\\kappa}\\Delta L\\epsilon^{-2})$ for the two settings, respectively, where $\\kappa$ is the condition number, $L$ is the smoothness constant, and $\\Delta$ is the initial gap. Our result reveals substantial gaps between these limits and best-known upper bounds in the literature. To close these gaps, we introduce a generic acceleration scheme that deploys existing gradient-based methods to solve a sequence of crafted strongly-convex-strongly-concave subproblems. In the general setting, the complexity of our proposed algorithm nearly matches the lower bound; in particular, it removes an additional poly-logarithmic dependence on accuracy present in previous works. In the averaged smooth finite-sum setting, our proposed algorithm improves over previous algorithms by providing a nearly-tight dependence on the condition number."}}
{"id": "IVAc8928Dxu", "cdate": 1609459200000, "mdate": null, "content": {"title": "Optimal Algorithms for Differentially Private Stochastic Monotone Variational Inequalities and Saddle-Point Problems", "abstract": "In this work, we conduct the first systematic study of stochastic variational inequality (SVI) and stochastic saddle point (SSP) problems under the constraint of differential privacy (DP). We propose two algorithms: Noisy Stochastic Extragradient (NSEG) and Noisy Inexact Stochastic Proximal Point (NISPP). We show that a stochastic approximation variant of these algorithms attains risk bounds vanishing as a function of the dataset size, with respect to the strong gap function; and a sampling with replacement variant achieves optimal risk bounds with respect to a weak gap function. We also show lower bounds of the same order on weak gap function. Hence, our algorithms are optimal. Key to our analysis is the investigation of algorithmic stability bounds, both of which are new even in the nonprivate case. The dependence of the running time of the sampling with replacement algorithms, with respect to the dataset size $n$, is $n^2$ for NSEG and $\\tilde{O}(n^{3/2})$ for NISPP."}}
