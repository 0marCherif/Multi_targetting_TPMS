{"id": "WL7wIMHp_F", "cdate": 1684332717451, "mdate": 1684332717451, "content": {"title": "Cold Posteriors through PAC-Bayes", "abstract": "We investigate the cold posterior effect through the lens of PAC-Bayes generalization bounds. We argue that in the non-asymptotic setting, when the number of training samples is (relatively) small, discussions of the cold posterior effect should take into account that approximate Bayesian inference does not readily provide guarantees of performance on out-of-sample data. Instead, out-of-sample error is better described through a generalization bound. In this context, we explore the connections of the ELBO objective from variational inference and the PAC-Bayes objectives. We note that, while the ELBO and PAC-Bayes objectives are similar, the latter objectives naturally contain a temperature parameter  which is not restricted to be . For realistic classification tasks, in the case of Laplace approximations to the posterior,  we show how this PAC-Bayesian interpretation of the temperature parameter captures important aspects of the cold posterior effect."}}
{"id": "r9CvCsfkfPW", "cdate": 1681833045876, "mdate": null, "content": {"title": "Clustering inconsistency for Pitman--Yor mixture models with a prior on the precision but fixed discount parameter", "abstract": "Bayesian nonparametric (BNP) mixture models such as Dirichlet process (DP) and Pitman--Yor process (PY) mixture models are popular to model complex data. Their posterior distributions exhibit nice theoretical properties, converging at the optimal minimax rate to the true data-generating distribution, and extensive research has been devoted to developing this theory.  However, consistency of the posterior distribution does not imply consistency of the number of clusters, and asymptotic guarantees for the posterior number of clusters of these BNP mixture models have been lacking until recently. \nRecent research has shown that these models can be inconsistent for the number of clusters. In the case of DP mixture models, this problem can be avoided when a prior is put on the model's concentration hyperparameter $\\alpha$, as is common practice. In this work, we prove that PY mixture models remain inconsistent for the number of clusters when a prior is put on $\\alpha$, in the special case where the true number of components in the data generating mechanism is equal to 1 and the discount parameter $\\sigma \\in (0,1)$ is a fixed constant."}}
{"id": "jMCNK40Khgx", "cdate": 1668734784080, "mdate": null, "content": {"title": "Cold Posteriors through PAC-Bayes", "abstract": "We investigate the cold posterior effect through the lens of PAC-Bayes generalization bounds. We argue that in the non-asymptotic setting, when the number of training samples is (relatively) small, discussions of the cold posterior effect should take into account that approximate Bayesian inference does not readily provide guarantees of performance on out-of-sample data. Instead, out-of-sample error is better described through a generalization bound. In this context, we explore the connections of the ELBO objective from variational inference and the PAC-Bayes objectives. We note that, while the ELBO and PAC-Bayes objectives are similar, the latter objectives naturally contain a temperature parameter $\\lambda$ which is not restricted to be $\\lambda=1$. For realistic classification tasks, in the case of Laplace approximations to the posterior, we show how this PAC-Bayesian interpretation of the temperature parameter captures important aspects of the cold posterior effect."}}
{"id": "L3BrimSnJ4A", "cdate": 1665069633650, "mdate": null, "content": {"title": "Cold Posteriors through PAC-Bayes", "abstract": "We investigate the cold posterior effect through the lens of PAC-Bayes generalization bounds. We argue that in the non-asymptotic setting, when the number of training samples is (relatively) small, discussions of the cold posterior effect should take into account that approximate Bayesian inference does not readily provide guarantees of performance on out-of-sample data. Instead, out-of-sample error is better described through a generalization bound. In this context, we explore the connections of the ELBO objective from variational inference and the PAC-Bayes objectives. We note that, while the ELBO and PAC-Bayes objectives are similar, the latter objectives naturally contain a temperature parameter $\\lambda$ which is not restricted to be $\\lambda=1$. For realistic classification tasks, in the case of Laplace approximations to the posterior,  we show how this PAC-Bayesian interpretation of the temperature parameter captures important aspects of the cold posterior effect."}}
{"id": "HwcEuhLtCJr", "cdate": 1663850289529, "mdate": null, "content": {"title": "Cold Posteriors through PAC-Bayes", "abstract": "We investigate the cold posterior effect through the lens of PAC-Bayes generalization bounds. We argue that in the non-asymptotic setting, when the number of training samples is (relatively) small, discussions of the cold posterior effect should take into account that approximate Bayesian inference does not readily provide guarantees of performance on out-of-sample data. Instead, out-of-sample error is better described through a generalization bound. In this context, we explore the connections of the ELBO objective from variational inference and the PAC-Bayes objectives. We note that, while the ELBO and PAC-Bayes objectives are similar, the latter objectives naturally contain a temperature parameter $\\lambda$ which is not restricted to be $\\lambda=1$. For both simplified regression and realistic classification tasks, in the case of Laplace approximations to the posterior,  we show how this PAC-Bayesian interpretation of the temperature parameter captures important aspects of the cold posterior effect."}}
{"id": "DU1TC5G7iPI", "cdate": 1637317987237, "mdate": 1637317987237, "content": {"title": "Bayesian block-diagonal graphical models via the Fiedler prior", "abstract": "We study the problem of inferring the conditional independence structure\nbetween the entries of a Gaussian random vector. Our focus is on finding groups of\nindependent variables. This can be translated into the estimation of a precision matrix\n(inverse of the covariance matrix) with a block-diagonal structure. We borrow ideas from\nspectral graph theory and spectral clustering and propose a novel prior called Fiedler prior\nshowing shrinkage properties towards block-diagonal precision matrices. We compare the\nshrinkage induced by our prior and the popular Graphical Lasso prior, and compare their\nperformance on a simulated dataset."}}
{"id": "J0SSW5XeWUY", "cdate": 1606146135444, "mdate": null, "content": {"title": "Approximating the clusters' prior distribution in Bayesian nonparametric models", "abstract": "In Bayesian nonparametrics, knowledge of the prior distribution induced on the number of clusters is key for prior specification and calibration. However, evaluating this prior is infamously difficult even for moderate sample size.\nWe evaluate several statistical approximations to the prior distribution on the number of clusters for Gibbs-type processes, a class including the Pitman--Yor process and the normalized generalized gamma process.\nWe introduce a new approximation based on the predictive distribution of Gibbs-type process, which compares favourably with the existing methods.\nWe thoroughly discuss the limitations of these various approximations by comparing them against an exact implementation of the prior distribution of the number of clusters. "}}
{"id": "r1xLV3jDDr", "cdate": 1569336366509, "mdate": null, "content": {"title": "Bayesian nonparametric priors for hidden Markov random fields", "abstract": "One of the central issues in statistics and machine learning is how to select an adequate model that can automatically adapt its complexity to the observed data. In the present paper, we focus on the issue of determining the structure of clustered data, both in terms of finding the appropriate number of clusters and of modelling the right dependence structure between the observations. Bayesian nonparametric (BNP) models, which do not impose an upper limit on the number of clusters, are appropriate to avoid the required guess on the number of clusters but have been mainly developed for independent data. In contrast, Markov random fields (MRF) have been extensively used to model dependencies in a tractable manner but usually reduce to finite cluster numbers when clustering tasks are addressed. Our main contribution is to propose a general scheme to design tractable BNP-MRF priors that combine both features: no commitment to an arbitrary number of clusters and a dependence modelling. A key ingredient in this construction is the availability of a stick-breaking representation which has the threefold advantage to allowing us to extend standard discrete MRFs to infinite state space, to design a tractable estimation algorithm using variational approximation and to derive theoretical properties on the predictive distribution and the number of clusters of the proposed model. This approach is illustrated on a challenging natural image segmentation task for which it shows good performance with respect to the literature."}}
{"id": "H1xPSqovwr", "cdate": 1569335870773, "mdate": null, "content": {"title": "Approximate Bayesian computation via the energy statistic", "abstract": "Approximate Bayesian computation (ABC) has become an essential part of the Bayesian toolbox for addressing problems in which the likelihood is prohibitively expensive or entirely unknown, making it intractable. ABC defines a quasi-posterior by comparing observed data with simulated data, traditionally based on some summary statistics, the elicitation of which is regarded as a key difficulty. In recent years, a number of data discrepancy measures bypassing the construction of summary statistics have been proposed, including the Kullback--Leibler divergence, the Wasserstein distance and maximum mean discrepancies. Here we propose a novel importance-sampling (IS) ABC algorithm relying on the so-called two-sample energy statistic. We establish a new asymptotic result for the case where both the observed sample size and the simulated data sample size increase to infinity, which highlights to what extent the data discrepancy measure impacts the asymptotic pseudo-posterior. The result holds in the broad setting of IS-ABC methodologies, thus generalizing previous results that have been established only for rejection ABC algorithms. Furthermore, we propose a consistent V-statistic estimator of the energy statistic, under which we show that the large sample result holds. Our proposed energy statistic based ABC algorithm is demonstrated on a variety of models, including a Gaussian mixture, a moving-average model of order two, a bivariate beta and a multivariate g-and-k distribution. We find that our proposed method compares well with alternative discrepancy measures."}}
{"id": "r1-0OnbObH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Understanding Priors in Bayesian Neural Networks at the Unit Level", "abstract": "We investigate deep Bayesian neural networks with Gaussian priors on the weights and a class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian priors are well known to induce an L..."}}
