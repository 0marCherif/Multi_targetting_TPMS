{"id": "V9d9vyGAaTo", "cdate": 1672531200000, "mdate": 1682323182148, "content": {"title": "QAID: Question Answering Inspired Few-shot Intent Detection", "abstract": "Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance."}}
{"id": "gNI4_85Cyve", "cdate": 1663850578092, "mdate": null, "content": {"title": "QAID: Question Answering Inspired Few-shot Intent Detection", "abstract": "Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance."}}
{"id": "tgHrlAC7Ra0", "cdate": 1640995200000, "mdate": 1682323182140, "content": {"title": "A New Data Augmentation Method for Intent Classification Enhancement and its Application on Spoken Conversation Datasets", "abstract": "Intent classifiers are vital to the successful operation of virtual agent systems. This is especially so in voice activated systems where the data can be noisy with many ambiguous directions for user intents. Before operation begins, these classifiers are generally lacking in real-world training data. Active learning is a common approach used to help label large amounts of collected user input. However, this approach requires many hours of manual labeling work. We present the Nearest Neighbors Scores Improvement (NNSI) algorithm for automatic data selection and labeling. The NNSI reduces the need for manual labeling by automatically selecting highly-ambiguous samples and labeling them with high accuracy. This is done by integrating the classifier\u2019s output from a semantically similar group of text samples. The labeled samples can then be added to the training set to improve the accuracy of the classifier. We demonstrated the use of NNSI on two large-scale, real-life voice conversation systems. Evaluation of our results showed that our method was able to select and label useful samples with high accuracy. Adding these new samples to the training data significantly improved the classifiers and reduced error rates by up to 10%."}}
{"id": "jQH6x3LKdF", "cdate": 1640995200000, "mdate": 1682323182164, "content": {"title": "A new data augmentation method for intent classification enhancement and its application on spoken conversation datasets", "abstract": "Intent classifiers are vital to the successful operation of virtual agent systems. This is especially so in voice activated systems where the data can be noisy with many ambiguous directions for user intents. Before operation begins, these classifiers are generally lacking in real-world training data. Active learning is a common approach used to help label large amounts of collected user input. However, this approach requires many hours of manual labeling work. We present the Nearest Neighbors Scores Improvement (NNSI) algorithm for automatic data selection and labeling. The NNSI reduces the need for manual labeling by automatically selecting highly-ambiguous samples and labeling them with high accuracy. This is done by integrating the classifier's output from a semantically similar group of text samples. The labeled samples can then be added to the training set to improve the accuracy of the classifier. We demonstrated the use of NNSI on two large-scale, real-life voice conversation systems. Evaluation of our results showed that our method was able to select and label useful samples with high accuracy. Adding these new samples to the training data significantly improved the classifiers and reduced error rates by up to 10%."}}
{"id": "08EHRPW2Qv", "cdate": 1640995200000, "mdate": 1682323182165, "content": {"title": "Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms", "abstract": ""}}
{"id": "bRyI_2zRm3", "cdate": 1609459200000, "mdate": 1682323182126, "content": {"title": "Improved Goal Oriented Dialogue via Utterance Generation and Look Ahead", "abstract": "Goal oriented dialogue systems have become a prominent customer-care interaction channel for most businesses. However, not all interactions are smooth, and customer intent misunderstanding is a major cause of dialogue failure. We show that intent prediction can be improved by training a deep text-to-text neural model to generate successive user utterances from unlabeled dialogue data. For that, we define a multi-task training regime that utilizes successive user-utterance generation to improve the intent prediction. Our approach achieves the reported improvement due to two complementary factors: First, it uses a large amount of unlabeled dialogue data for an auxiliary generation task. Second, it uses the generated user utterance as an additional signal for the intent prediction model. Lastly, we present a novel look-ahead approach that uses user utterance generation to improve intent prediction in inference time. Specifically, we generate counterfactual successive user utterances for conversations with ambiguous predicted intents, and disambiguate the prediction by reassessing the concatenated sequence of available and generated utterances."}}
{"id": "uDMvas-ZdDW", "cdate": 1577836800000, "mdate": 1682323182134, "content": {"title": "Do Not Have Enough Data? Deep Learning to the Rescue!", "abstract": "Based on recent advances in natural language modeling and those in text generation capabilities, we propose a novel data augmentation method for text classification tasks. We use a powerful pre-trained neural network model to artificially synthesize new labeled data for supervised learning. We mainly focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAMBADA), involves fine-tuning a state-of-the-art language generator to a specific task through an initial training phase on the existing (usually small) labeled data. Using the fine-tuned model and given a class label, new sentences for the class are generated. Our process then filters these new sentences by using a classifier trained on the original data. In a series of experiments, we show that LAMBADA improves classifiers' performance on a variety of datasets. Moreover, LAMBADA significantly improves upon the state-of-the-art techniques for data augmentation, specifically those applicable to text classification tasks with little data."}}
{"id": "bwJ34WHEyR", "cdate": 1577836800000, "mdate": 1682323182174, "content": {"title": "Balancing via Generation for Multi-Class Text Classification Improvement", "abstract": ""}}
{"id": "UDMNqa7DAY", "cdate": 1577836800000, "mdate": 1682323182140, "content": {"title": "Unsupervised FAQ Retrieval with Question Generation and BERT", "abstract": ""}}
{"id": "X7802qdz0rv", "cdate": 1546300800000, "mdate": 1681532153008, "content": {"title": "Neural network gradient-based learning of black-box function interfaces", "abstract": ""}}
