{"id": "FWohKbMhlo", "cdate": 1677713814870, "mdate": null, "content": {"title": "Iterative weakly supervised learning for novel class object detection", "abstract": "Training object detectors for new classes usually requires collecting and labeling large amounts of data. Our paper introduces a new approach to address this issue - training novel-class object detectors using a combination of a few labeled images and weakly labeled data, that is easy to obtain. We propose an iterative fine-tuning framework that cycles through predicting pseudo-labels, filtering them using weak labels, and fine-tuning the model on this data. By repeating the process, we can mostly close the gap to a model trained on 40x more data, thereby offering a new approach to improving the trade-off between labeling effort and performance."}}
{"id": "MmhGK8YkUKO", "cdate": 1675827741396, "mdate": null, "content": {"title": "The Independent Compositional Subspace Hypothesis for the Structure of CLIP's Last Layer", "abstract": "In this paper, we propose a hypothesis which posits that CLIP disentangles compositional visual attributes into orthogonal, independent subspaces which CLIP uses to build compositional representations of images. Our hypothesis suggests that CLIP learns compositional techniques that are similar to humans'. We find five core compositional attributes predicted by the hypothesis: color, size, counting, camera view, and pattern. We empirically test their properties and find that they code for their respective compositional attribute type and are essentially orthogonal to one another, as well as the subject of the image."}}
{"id": "TsXe-CyYJqx", "cdate": 1654886254337, "mdate": null, "content": {"title": "Multivariable Causal Discovery with General Nonlinear Relationships", "abstract": "Today's methods for uncovering the causal relationship(s) from observational data either constrain the function class (linearity/additive noise) or the data. We make assumptions on the data to develop a framework for Causal Discovery (CD) that works for general non-linear dependencies. Similar to previous work, we use nonlinear Independent Component Analysis (ICA) to infer the underlying sources from the observed variables. Instead of using conditional independence tests to determine the causal directions, we rely on the Jacobian of the inference function; thus, generalizing LiNGAM's approach to the nonlinear case. We show that causal models resolve the permutation indeterminacy of ICA and prove that under strong identifiability, the inference function's Jacobian captures the sparsity structure of the causal graph. We demonstrate that our method can infer the causal graph on multiple synthetic data sets."}}
{"id": "0wlAmXOfGc", "cdate": 1654841141596, "mdate": null, "content": {"title": "Embrace the Gap: VAEs Perform Independent Mechanism Analysis", "abstract": "Despite the widespread use of variational autoencoders (VAEs), the consequences of optimizing the evidence lower bound (ELBO) opposed to the exact log-likelihood remain poorly understood. We shed light on this matter by studying nonlinear VAEs in the limit of near-deterministic decoders. We first prove that, in this regime, the optimal encoder approximately inverts the decoder---a commonly used but unproven conjecture---which we call self-consistency. Leveraging self-consistency, we show that the ELBO converges to a regularized log-likelihood rather than to the exact one. The regularization term allows VAEs to perform what has been termed independent mechanism analysis (IMA): it adds an inductive bias towards decoders with column-orthogonal Jacobians. This connection to IMA allows us to precisely characterize the gap w.r.t. the log-likelihood in near-deterministic VAEs. Furthermore, it elucidates an unanticipated benefit of ELBO optimization for nonlinear representation learning as, unlike the unregularized log-likelihood, the IMA-regularized objective promotes identification of the ground-truth latent factors. "}}
{"id": "LiC2vmzbpMO", "cdate": 1654348672602, "mdate": null, "content": {"title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation", "abstract": "We propose a new challenging dataset to benchmark robustness of ImageNet-trained models with respect to domain shifts: ImageNet-D. ImageNet- D has six different domains (\u201cReal\u201d, \u201cPainting\u201d, \u201cClipart\u201d, \u201cSketch\u201d, \u201cInfograph\u201d and \u201cQuickdraw\u201d). We show that even state-of-the-art models struggle on this dataset and find that they make well-interpretable errors. For example, our best EfficientNet-L2 model experiences a large performance drop even on the \u201cReal\u201d domain from 11.6% on ImageNet clean to 29.2% on the \u201cReal\u201d domain."}}
{"id": "G4GpqX4bKAH", "cdate": 1652737793020, "mdate": null, "content": {"title": "Embrace the Gap: VAEs Perform Independent Mechanism Analysis", "abstract": "Variational autoencoders (VAEs) are a popular framework for modeling complex data distributions; they can be efficiently trained via variational inference by maximizing the evidence lower bound (ELBO), at the expense of a gap to the exact (log-)marginal likelihood. While VAEs are commonly used for representation learning, it is unclear why ELBO maximization would yield useful representations, since unregularized maximum likelihood estimation cannot invert the data-generating process. Yet, VAEs often succeed at this task. We seek to elucidate this apparent paradox by studying nonlinear VAEs in the limit of near-deterministic decoders. We first prove that, in this regime, the optimal encoder approximately inverts the decoder---a commonly used but unproven conjecture---which we refer to as self-consistency. Leveraging self-consistency, we show that the ELBO converges to a regularized log-likelihood. This allows VAEs to perform what has recently been termed independent mechanism analysis (IMA): it adds an inductive bias towards decoders with column-orthogonal Jacobians, which helps recovering the true latent factors. The gap between ELBO and log-likelihood is therefore welcome, since it bears unanticipated benefits for nonlinear representation learning. In experiments on synthetic and image data, we show that VAEs uncover the true latent factors when the data generating process satisfies the IMA assumption."}}
{"id": "NkK4i91VWp", "cdate": 1652737340943, "mdate": null, "content": {"title": "Increasing Confidence in Adversarial Robustness Evaluations", "abstract": "Hundreds of defenses have been proposed to make deep neural networks robust against minimal (adversarial) input perturbations. However, only a handful of these defenses held up their claims because correctly evaluating robustness is extremely challenging: Weak attacks often fail to find adversarial examples even if they unknowingly exist, thereby making a vulnerable network look robust. In this paper, we propose a test to identify weak attacks and, thus, weak defense evaluations. Our test slightly modifies a neural network to guarantee the existence of an adversarial example for every sample. Consequentially, any correct attack must succeed in breaking this modified network. For eleven out of thirteen previously-published defenses, the original evaluation of the defense fails our test, while stronger attacks that break these defenses pass it. We hope that attack unit tests - such as ours - will be a major component in future robustness evaluations and increase confidence in an empirical field that is currently riddled with skepticism."}}
{"id": "9RUHPlladgh", "cdate": 1632875715371, "mdate": null, "content": {"title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain", "abstract": "An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world.\nIn this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. \nIn contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark.\nIn total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets.\nDespite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization."}}
{"id": "1oEvY1a67c1", "cdate": 1632875685070, "mdate": null, "content": {"title": "If your data distribution shifts, use self-learning", "abstract": "In this paper, we demonstrate that self-learning techniques like entropy minimization or pseudo-labeling are simple, yet effective techniques for increasing test performance under domain shifts. Our results show that self-learning consistently increases performance under distribution shifts, irrespective of the model architecture, the pre-training technique or the type of distribution shift. At the same time, self-learning is simple to use in practice because it does not require knowledge or access to the original training data or scheme, is robust to hyperparameter choices, is straight-forward to implement and requires only a few training epochs. This makes self-learning techniques highly attractive for any practitioner who applies machine learning algorithms in the real world. We present state-of-the art adaptation results on CIFAR10-C (8.5% error),  ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error), theoretically study the dynamics of self-supervised adaptation methods and propose a new classification dataset (ImageNet-D) which is challenging even with adaptation."}}
{"id": "ahGBBZ7bHKr", "cdate": 1624022580168, "mdate": null, "content": {"title": "Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints", "abstract": "Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. \nThe inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model.\nIn this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different $\\ell_p$-norm perturbation models ($p=0, 1, 2, \\infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. \nIt works by iteratively finding the sample misclassified with maximum confidence within an $\\ell_p$-norm constraint of size $\\epsilon$, while adapting $\\epsilon$ to minimize the distance of the current sample to the decision boundary.\nExtensive experiments show that FMN significantly outperforms existing attacks in terms of convergence speed and computation time, while reporting comparable or even smaller perturbation sizes."}}
