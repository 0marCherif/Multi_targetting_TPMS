{"id": "zK0IScK4Kg", "cdate": 1672531200000, "mdate": 1706632085894, "content": {"title": "Complexity of Single Loop Algorithms for Nonlinear Programming with Stochastic Objective and Constraints", "abstract": "We analyze the complexity of single-loop quadratic penalty and augmented Lagrangian algorithms for solving nonconvex optimization problems with functional equality constraints. We consider three cases, in all of which the objective is stochastic and smooth, that is, an expectation over an unknown distribution that is accessed by sampling. The nature of the equality constraints differs among the three cases: deterministic and linear in the first case, deterministic, smooth and nonlinear in the second case, and stochastic, smooth and nonlinear in the third case. Variance reduction techniques are used to improve the complexity. To find a point that satisfies $\\varepsilon$-approximate first-order conditions, we require $\\widetilde{O}(\\varepsilon^{-3})$ complexity in the first case, $\\widetilde{O}(\\varepsilon^{-4})$ in the second case, and $\\widetilde{O}(\\varepsilon^{-5})$ in the third case. For the first and third cases, they are the first algorithms of \"single loop\" type (that also use $O(1)$ samples at each iteration) that still achieve the best-known complexity guarantees."}}
{"id": "RMpDxzeQ9O", "cdate": 1672531200000, "mdate": 1692766152860, "content": {"title": "Beyond the Golden Ratio for Variational Inequality Algorithms", "abstract": "We improve the understanding of the golden ratio algorithm, which solves monotone variational inequalities (VI) and convex-concave min-max problems via the distinctive feature of adapting the step sizes to the local Lipschitz constants. Adaptive step sizes not only eliminate the need to pick hyperparameters, but they also remove the necessity of global Lipschitz continuity and can increase from one iteration to the next. We first establish the equivalence of this algorithm with popular VI methods such as reflected gradient, Popov or optimistic gradient descent-ascent (OGDA) in the unconstrained case with constant step sizes. We then move on to the constrained setting and introduce a new analysis that allows to use larger step sizes, to complete the bridge between the golden ratio algorithm and the existing algorithms in the literature. Doing so, we actually eliminate the link between the golden ratio {$\\frac{1+\\sqrt{5}}{2}$} and the algorithm. Moreover, we improve the adaptive version of the algorithm, first by removing the maximum step size hyperparameter (an artifact from the analysis), and secondly, by adjusting it to nonmonotone problems with weak Minty solutions, with superior empirical performance."}}
{"id": "N3Ynesq8db", "cdate": 1672531200000, "mdate": 1692766152953, "content": {"title": "Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data", "abstract": "We focus on analyzing the classical stochastic projected gradient methods under a general dependent data sampling scheme for constrained smooth nonconvex optimization. We show the worst-case rate o..."}}
{"id": "gRWbKhvzSQK", "cdate": 1640995200000, "mdate": 1657298982832, "content": {"title": "A Natural Actor-Critic Framework for Zero-Sum Markov Games", "abstract": "We introduce algorithms based on natural actor-critic and analyze their sample complexity for solving two player zero-sum Markov games in the tabular case. Our results improve the best-known sample..."}}
{"id": "gDM7vidjpP", "cdate": 1640995200000, "mdate": 1657298982805, "content": {"title": "On the Convergence of Stochastic Primal-Dual Hybrid Gradient", "abstract": "In this paper, we analyze the recently proposed stochastic primal-dual hybrid gradient (SPDHG) algorithm and provide new theoretical results. In particular, we prove almost sure convergence of the iterates to a solution with convexity and linear convergence with further structure, using standard step sizes independent of strong convexity or other regularity constants. In the general convex case, we also prove the $\\mathcal{O}(1/k)$ convergence rate for the ergodic sequence, on expected primal-dual gap function. Our assumption for linear convergence is metric subregularity, which is satisfied for strongly convex-strongly concave problems in addition to many nonsmooth and/or nonstrongly convex problems, such as linear programs, Lasso, and support vector machines. We also provide numerical evidence showing that SPDHG with standard step sizes shows a competitive practical performance against its specialized strongly convex variant SPDHG-$\\mu$ and other state-of-the-art algorithms, including variance reduction methods."}}
{"id": "OMVvibK1l_W", "cdate": 1640995200000, "mdate": 1657298982826, "content": {"title": "Stochastic Variance Reduction for Variational Inequality Methods", "abstract": "We propose stochastic variance reduced algorithms for solving convex-concave saddle point problems, monotone variational inequalities, and monotone inclusions. Our framework applies to extragradien..."}}
{"id": "NP9mR8Wbr0s", "cdate": 1640995200000, "mdate": 1648836211971, "content": {"title": "On the Complexity of a Practical Primal-Dual Coordinate Method", "abstract": "We prove complexity bounds for the primal-dual algorithm with random extrapolation and coordinate descent (PURE-CD), which has been shown to obtain good practical performance for solving convex-concave min-max problems with bilinear coupling. Our complexity bounds either match or improve the best-known results in the literature for both dense and sparse (strongly)-convex-(strongly)-concave problems."}}
{"id": "mniwiEAuzL", "cdate": 1632875670573, "mdate": null, "content": {"title": "Sample-efficient actor-critic algorithms with an etiquette for zero-sum Markov games", "abstract": "We introduce algorithms based on natural policy gradient and two time-scale natural actor-critic, and analyze their sample complexity for solving two player zero-sum Markov games in the tabular case. Our results improve the best-known sample complexities of policy gradient/actor-critic methods for convergence to Nash equilibrium in the multi-agent setting. We use the error propagation scheme in approximate dynamic programming, recent advances for global convergence of policy gradient methods, temporal difference learning, and techniques from stochastic primal-dual optimization literature. Our algorithms feature two stages, requiring agents to agree on an etiquette before starting their interactions, which is feasible for instance in self-play. On the other hand, the agents only access to joint reward and joint next state and not to each other's actions or policies. Our sample complexities also match the best-known results for global convergence of policy gradient and two time-scale actor-critic algorithms in the single agent setting. We provide numerical verification of our method for a two-player bandit environment and a two player game, Alesia. We observe improved empirical performance as compared to the recently proposed optimistic gradient descent ascent variant for Markov games."}}
{"id": "GNFcszMtYvV", "cdate": 1621630277838, "mdate": null, "content": {"title": "Convergence of adaptive algorithms for constrained weakly convex optimization", "abstract": "We analyze the adaptive first order algorithm AMSGrad, for solving a constrained stochastic optimization problem with a weakly convex objective. We prove the $\\mathcal{\\tilde O}(t^{-1/2})$ rate of convergence for the squared norm of the gradient of Moreau envelope, which is the standard stationarity measure for this class of problems. It matches the known rates that adaptive algorithms enjoy for the specific case of unconstrained smooth nonconvex stochastic optimization. Our analysis works with mini-batch size of $1$, constant first and second order moment parameters, and possibly unbounded optimization domains. Finally, we illustrate the applications and extensions of our results to specific problems and algorithms."}}
{"id": "wHc3CHmPkKQ", "cdate": 1609459200000, "mdate": 1648836211966, "content": {"title": "Forward-reflected-backward method with variance reduction", "abstract": "We propose a variance reduced algorithm for solving monotone variational inequalities. Without assuming strong monotonicity, cocoercivity, or boundedness of the domain, we prove almost sure convergence of the iterates generated by the algorithm to a solution. In the monotone case, the ergodic average converges with the optimal O(1/k) rate of convergence. When strong monotonicity is assumed, the algorithm converges linearly, without requiring the knowledge of strong monotonicity constant. We finalize with extensions and applications of our results to monotone inclusions, a class of non-monotone variational inequalities and Bregman projections."}}
