{"id": "p6wiThIOS5m", "cdate": 1663850239735, "mdate": null, "content": {"title": "Provably efficient multi-task Reinforcement Learning in large state spaces", "abstract": "We study multi-task Reinforcement Learning where shared knowledge among different environments is distilled to enable scalable generalization to a variety of problem instances. In the context of general function approximation, Markov Decision Process (MDP) with low Bilinear rank encapsulates a wide range of structural conditions that permit polynomial sample complexity in large state spaces, where the Bellman errors are related to bilinear forms of features with low intrinsic dimensions. To achieve multi-task learning in MDPs, we propose online representation learning algorithms to capture the shared features in the different task-specific bilinear forms. We show that in the presence of low-rank structures in the features of the bilinear forms, the algorithms benefit from sample complexity improvements compared to single-task learning. Therefore, we achieve the first sample efficient multi-task reinforcement learning algorithm with general function approximation."}}
{"id": "DD8ZJNdTPtO", "cdate": 1663850233536, "mdate": null, "content": {"title": "Stochastic Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity", "abstract": "Optimization of convex functions under stochastic zeroth-order feedback has been a major and challenging question in online learning. In this work we consider the problem of optimizing second-order smooth and strongly convex functions where the algorithm is only accessible to noisy evaluations of the objective function it queries. We provide the first tight characterization for the rate of the minimax simple regret by developing matching upper and lower bounds. We propose an algorithm that features a combination of a bootstrapping stage and a mirror-descent stage. The main innovation of our approach is the usage of a gradient estimation scheme that exploits the local geometry of the objective function, and we provide sharp analysis for the corresponding estimation bounds. "}}
{"id": "jZRMrvsjND", "cdate": 1640995200000, "mdate": 1682353024737, "content": {"title": "Offline Reinforcement Learning with Realizability and Single-policy Concentrability", "abstract": "Sample-efficiency guarantees for offline reinforcement learning (RL) often rely on strong assumptions on both the function classes (e.g., Bellman-completeness) and the data coverage (e.g., all-policy concentrability). Despite the recent efforts on relaxing these assumptions, existing works are only able to relax one of the two factors, leaving the strong assumption on the other factor intact. As an important open problem, can we achieve sample-efficient offline RL with weak assumptions on both factors? In this paper we answer the question in the positive. We analyze a simple algorithm based on the primal-dual formulation of MDPs, where the dual variables (discounted occupancy) are modeled using a density-ratio function against offline data. With proper regularization, we show that the algorithm enjoys polynomial sample complexity, under only realizability and single-policy concentrability. We also provide alternative analyses based on different assumptions to shed light on the nature of primal-dual algorithms for offline RL."}}
{"id": "fh5UbenMTCX", "cdate": 1640995200000, "mdate": 1682370851710, "content": {"title": "Towards General Function Approximation in Zero-Sum Markov Games", "abstract": "This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value function or the model is parameterized by..."}}
{"id": "dakpQ7B6TSz", "cdate": 1640995200000, "mdate": 1682353024712, "content": {"title": "Offline Reinforcement Learning with Realizability and Single-policy Concentrability", "abstract": "Sample-efficiency guarantees for offline reinforcement learning (RL) often rely on strong assumptions on both the function classes (e.g., Bellman-completeness) and the data coverage (e.g., all-poli..."}}
{"id": "_9ueuUOzil", "cdate": 1640995200000, "mdate": 1682318410439, "content": {"title": "Solving SDP Faster: A Robust IPM Framework and Efficient Implementation", "abstract": "This paper introduces a new robust interior point method analysis for semidefinite programming (SDP). This new robust analysis can be combined with either logarithmic barrier or hybrid barrier.Under this new framework, we can improve the running time of semidefinite programming (SDP) with variable size $n\\times n$ and m constraints up to $\\epsilon$ accuracy.We show that for the case $m=\\Omega(n^{2})$, we can solve SDPs in $m^{\\omega}$ time. This suggests solving SDP is nearly as fast as solving the linear system with equal number of variables and constraints. This is the first result that tall dense SDP can be solved in the nearly-optimal running time, and it also improves the stateof-the-art SDP solver [Jiang, Kathuria, Lee, Padmanabhan and Song, FOCS 2020].In addition to our new IPM analysis, we also propose a number of techniques that might be of further interest, such as, maintaining the inverse of a Kronecker product using lazy updates, a general amortization scheme for positive semi-definite matrices."}}
{"id": "8i4_pig-mn", "cdate": 1640995200000, "mdate": 1682318410045, "content": {"title": "A Dynamic Fast Gaussian Transform", "abstract": "The Fast Gaussian Transform (FGT) enables subquadratic-time multiplication of an $n\\times n$ Gaussian kernel matrix $\\mathsf{K}_{i,j}= \\exp ( - \\| x_i - x_j \\|_2^2 ) $ with an arbitrary vector $h \\in \\mathbb{R}^n$, where $x_1,\\dots, x_n \\in \\mathbb{R}^d$ are a set of fixed source points. This kernel plays a central role in machine learning and random feature maps. Nevertheless, in most modern ML and data analysis applications, datasets are dynamically changing, and recomputing the FGT from scratch in (kernel-based) algorithms, incurs a major computational overhead ($\\gtrsim n$ time for a single source update $\\in \\mathbb{R}^d$). These applications motivate the development of a dynamic FGT algorithm, which maintains a dynamic set of sources under kernel-density estimation (KDE) queries in sublinear time, while retaining Mat-Vec multiplication accuracy and speed. Our main result is an efficient dynamic FGT algorithm, supporting the following operations in $\\log^{O(d)}(n/\\epsilon)$ time: (1) Adding or deleting a source point, and (2) Estimating the \"kernel-density\" of a query point with respect to sources with $\\epsilon$ additive accuracy. The core of the algorithm is a dynamic data structure for maintaining the \"interaction rank\" between source and target boxes, which we decouple into finite truncation of Taylor series and Hermite expansions."}}
{"id": "sA4qIu3zv6v", "cdate": 1632875739358, "mdate": null, "content": {"title": "Towards General Function Approximation in Zero-Sum Markov Games", "abstract": "This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value\nfunction or the model is parameterized by general function classes. Provably efficient\nalgorithms for both decoupled and coordinated settings are developed. In the decoupled setting where the agent controls a single player and plays against an arbitrary opponent, we propose a new model-free algorithm. The sample complexity is governed by the Minimax Eluder dimension\u2014a new dimension of the function class in Markov games. As a special case, this method improves the state-of-the-art algorithm\nby a $\\sqrt{d}$ factor in the regret when the reward function and transition kernel are parameterized with d-dimensional linear features. In the coordinated setting where both\nplayers are controlled by the agent, we propose a model-based algorithm and a model-free algorithm. In the model-based algorithm, we prove that sample complexity can\nbe bounded by a generalization of Witness rank to Markov games. The model-free\nalgorithm enjoys a  $\\sqrt{K}$-regret upper bound where $K$ is the number of episodes. Our\nalgorithms are based on new techniques of alternate optimism"}}
{"id": "QEBHPRodWYE", "cdate": 1632875716116, "mdate": null, "content": {"title": "InstaHide\u2019s Sample Complexity When Mixing Two Private Images ", "abstract": "Inspired by InstaHide challenge [Huang, Song, Li and Arora'20], [Chen, Song and Zhuo'20] recently provides one mathematical formulation of InstaHide attack problem under Gaussian images distribution. They show that it suffices to use $O(n_{\\mathsf{priv}}^{k_{\\mathsf{priv}} - 2/(k_{\\mathsf{priv}} + 1)})$ samples to recover one private image in $n_{\\mathsf{priv}}^{O(k_{\\mathsf{priv}})} + \\mathrm{poly}(n_{\\mathsf{pub}})$ time for any integer $k_{\\mathsf{priv}}$, where $n_{\\mathsf{priv}}$ and $n_{\\mathsf{pub}}$ denote the number of images used in the private and the public dataset to generate a mixed image sample. Under the current setup for the InstaHide challenge of mixing two private images ($k_{\\mathsf{priv}} = 2$), this means $n_{\\mathsf{priv}}^{4/3}$ samples are sufficient to recover a private image. In this work, we show that $n_{\\mathsf{priv}} \\log ( n_{\\mathsf{priv}} )$ samples are sufficient (information-theoretically) for recovering all the private images. \n\n"}}
{"id": "7SGgWl2uVG-", "cdate": 1621629787387, "mdate": null, "content": {"title": "Optimal Gradient-based Algorithms for Non-concave Bandit Optimization", "abstract": "Bandit problems with linear or concave reward have been extensively studied, but relatively few works have studied bandits with non-concave reward. This work considers a large family of bandit problems where the unknown underlying reward function is non-concave, including the low-rank generalized linear bandit problems and two-layer neural network with polynomial activation bandit problem.\nFor the low-rank generalized linear bandit problem, we provide a minimax-optimal algorithm in the dimension, refuting both conjectures in \\cite{lu2021low,jun2019bilinear}. Our algorithms are based on a unified zeroth-order optimization paradigm that applies in great generality and attains optimal rates in several structured polynomial settings (in the dimension). We further demonstrate the applicability of our algorithms in RL in the generative model setting, resulting in improved sample complexity over prior approaches.\nFinally, we show that the standard optimistic algorithms (e.g., UCB) are sub-optimal by dimension factors. In the neural net setting (with polynomial activation functions) with noiseless reward, we provide a bandit algorithm with sample complexity equal to the intrinsic algebraic dimension. Again, we show that optimistic approaches have worse sample complexity, polynomial in the extrinsic dimension (which could be exponentially worse in the polynomial degree)."}}
