{"id": "x26Mpsf45P3", "cdate": 1652737594153, "mdate": null, "content": {"title": "Bellman Residual Orthogonalization for Offline Reinforcement Learning", "abstract": "We propose and analyze a reinforcement learning principle that\napproximates the Bellman equations by enforcing their validity only\nalong a user-defined space of test functions.  Focusing on\napplications to model-free offline RL with function approximation, we\nexploit this principle to derive confidence intervals for off-policy\nevaluation, as well as to optimize over policies within a prescribed\npolicy class.  We prove an oracle inequality on our policy\noptimization procedure in terms of a trade-off between the value and\nuncertainty of an arbitrary comparator policy.  Different choices of\ntest function spaces allow us to tackle different problems within a\ncommon framework.  We characterize the loss of efficiency in moving\nfrom on-policy to off-policy data using our procedures, and establish\nconnections to concentrability coefficients studied in past work.  We\nexamine in depth the implementation of our methods with linear\nfunction approximation, and provide theoretical guarantees with\npolynomial-time implementations even when Bellman closure does not\nhold."}}
{"id": "EnmG3G5SYR", "cdate": 1621630040450, "mdate": null, "content": {"title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning", "abstract": "Actor-critic methods are widely used in offline reinforcement learning\npractice, but are not so well-understood theoretically. We propose a new\noffline actor-critic algorithm that naturally incorporates the pessimism principle, leading to several key advantages compared to the state of the art. \nThe algorithm can operate when the Bellman evaluation operator is closed with respect to the action value function of the actor's policies; this is a more general setting than the low-rank MDP model. Despite the added generality, the procedure is computationally tractable as it involves the solution of a sequence of second-order programs.\nWe prove an upper bound on the suboptimality gap of the policy returned by the procedure that depends on the data coverage of any arbitrary, possibly data dependent comparator policy.\nThe achievable guarantee is complemented with a minimax lower bound that is matching up to logarithmic factors."}}
{"id": "KsfuvGB3vco", "cdate": 1621630034388, "mdate": null, "content": {"title": "Design of Experiments for Stochastic Contextual Linear Bandits", "abstract": "In the stochastic linear contextual bandit setting there exist several minimax procedures for exploration with policies that are reactive to the data being acquired. In practice, there can be a significant engineering overhead to deploy these algorithms, especially when the dataset is collected in a distributed fashion or when a human in the loop is needed to implement a different policy. Exploring with a single non-reactive policy is beneficial in such cases. Assuming some batch contexts are available, we design a single stochastic policy to collect a good dataset from which a near-optimal policy can be extracted. We present a theoretical analysis as well as numerical experiments on both synthetic and real-world datasets."}}
{"id": "a3KrkTjoDPg", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Near Optimal Policies with Low Inherent Bellman Error.", "abstract": "We study the exploration problem with approximate linear action-value functions in episodic reinforcement learning under the notion of low inherent Bellman error, a condition normally employed to show convergence of approximate value iteration. First we relate this condition to other common frameworks and show that it is strictly more general than the low rank (or linear) MDP assumption of prior work. Second we provide an algorithm with a high probability regret bound $\\widetilde O(\\sum_{t=1}^H d_t \\sqrt{K} + \\sum_{t=1}^H \\sqrt{d_t} \\IBE K)$ where $H$ is the horizon, $K$ is the number of episodes, $\\IBE$ is the value if the inherent Bellman error and $d_t$ is the feature dimension at timestep $t$. In addition, we show that the result is unimprovable beyond constants and logs by showing a matching lower bound. This has two important consequences: 1) it shows that exploration is possible using only \\emph{batch assumptions} with an algorithm that achieves the optimal statistical rate for the setting we consider, which is more general than prior work on low-rank MDPs 2) the lack of closedness (measured by the inherent Bellman error) is only amplified by $\\sqrt{d_t}$ despite working in the online setting. Finally, the algorithm reduces to the celebrated \\textsc{LinUCB} when $H=1$ but with a different choice of the exploration parameter that allows handling misspecified contextual linear bandits. While computational tractability questions remain open for the MDP setting, this enriches the class of MDPs with a linear representation for the action-value function where statistically efficient reinforcement learning is possible."}}
{"id": "rkg6vwe2Pr", "cdate": 1569617764874, "mdate": null, "content": {"title": "Robust Super-Level Set Estimation using Gaussian Processes", "abstract": "This paper focuses on the problem of determining as large a region as possible where a function exceeds a given threshold with high probability. We assume that we only have access to a noise-corrupted version of the function and that function evaluations are costly. To select the next query point, we propose maximizing the expected volume of the domain identified as above the threshold as predicted by a Gaussian process, robustified by a variance term. We also give asymptotic guarantees on the exploration effect of the algorithm, regardless of the prior misspecification. We show by various numerical examples that our approach also outperforms existing techniques in the literature in practice."}}
{"id": "ryMXc4SgLr", "cdate": 1567802507497, "mdate": null, "content": {"title": "Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model", "abstract": "This paper focuses on the problem of computing an $\\epsilon$-optimal policy in a discounted Markov Decision Process (MDP) provided that we can access the reward and transition function through a generative model. We propose an algorithm that is initially agnostic to the MDP but that can leverage the specific MDP structure, expressed in terms of variances of the rewards and next-state value function, and gaps in the optimal action-value function to reduce the sample complexity needed to find a good policy, precisely highlighting the contribution of each state-action pair to the final sample complexity. A key feature of our analysis is that it removes all horizon dependencies in the sample complexity of suboptimal actions except for the intrinsic scaling of the value function and a constant additive term."}}
{"id": "HkzQ9VBlIS", "cdate": 1567802507289, "mdate": null, "content": {"title": "Limiting Extrapolation in Linear Approximate Value Iteration", "abstract": "We study linear approximate value iteration (LAVI) with a generative model. While linear models may accurately represent the optimal value function using a few parameters, several empirical and theoretical studies show the combination of least-squares projection with the Bellman operator may be expansive, thus leading LAVI to amplify errors over iterations and eventually diverge. We introduce an algorithm that approximates value functions by combining Q-values estimated at a set of \\textit{anchor} states. Our algorithm tries to balance the generalization and compactness of linear methods with the small amplification of errors typical of interpolation methods. We prove that if the features at any state can be represented as a convex combination of features at the anchor points, then errors are propagated linearly over iterations (instead of exponentially) and our method achieves a polynomial sample complexity bound in the horizon and the number of anchor points. These findings are confirmed in preliminary simulations in a number of simple problems where a traditional least-square LAVI method diverges."}}
{"id": "MT7kflwRFyg", "cdate": 1546300800000, "mdate": null, "content": {"title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration.", "abstract": "We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\\widetilde O(d^2 H^2 \\sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation."}}
{"id": "H14uMi-_WS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds", "abstract": "Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory th..."}}
{"id": "r1Z0msZOZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs", "abstract": "In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In t..."}}
