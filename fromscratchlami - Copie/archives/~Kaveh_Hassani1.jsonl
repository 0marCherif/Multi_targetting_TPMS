{"id": "e65KZ0ixi0", "cdate": 1652737666856, "mdate": null, "content": {"title": "Evaluating Graph Generative Models with Contrastively Learned Features", "abstract": "A wide range of models have been proposed for Graph Generative Models, necessitating effective methods to evaluate their quality. So far, most techniques use either traditional metrics based on subgraph counting, or the representations of randomly initialized Graph Neural Networks (GNNs). We propose using representations from constrastively trained GNNs, rather than random GNNs, and show this gives more reliable evaluation metrics. Neither traditional approaches nor GNN-based approaches dominate the other, however: we give examples of graphs that each approach is unable to distinguish. We demonstrate that Graph Substructure Networks (GSNs), which in a way combine both approaches, are better at distinguishing the distances between graph datasets."}}
{"id": "hNgDQPe8Uj", "cdate": 1632875473352, "mdate": null, "content": {"title": "Learning Graph Augmentations to Learn Graph Representations", "abstract": "Devising augmentations for graph contrastive learning is challenging due to their irregular structure, and drastic distribution shifts and nonequivalent feature spaces across datasets. To address this, we propose LG2AR, Learning Graph Augmentations to Learn Graph Representations, which is an end-to-end automatic graph augmentation framework that helps encoders learn generalizable representations on both node and graph levels. LG2AR consists of a probabilistic policy that learns a distribution over augmentations and a set of probabilistic augmentation heads that learn distributions over augmentation parameters. Under linear evaluation protocol, LG2AR achieves state-of-the-art results on 8 out of 8 graph classification tasks and 6 out of 7 node classification benchmarks."}}
{"id": "tHHzVxqV5sY", "cdate": 1577836800000, "mdate": null, "content": {"title": "Contrastive Multi-View Representation Learning on Graphs", "abstract": "We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing t..."}}
{"id": "IJpycb-1VyG", "cdate": 1577836800000, "mdate": null, "content": {"title": "PointMask: Towards Interpretable and Bias-Resilient Point Cloud Processing", "abstract": "Deep classifiers tend to associate a few discriminative input variables with their objective function, which in turn, may hurt their generalization capabilities. To address this, one can design systematic experiments and/or inspect the models via interpretability methods. In this paper, we investigate both of these strategies on deep models operating on point clouds. We propose PointMask, a model-agnostic interpretable information-bottleneck approach for attribution in point cloud models. PointMask encourages exploring the majority of variation factors in the input space while gradually converging to a general solution. More specifically, PointMask introduces a regularization term that minimizes the mutual information between the input and the latent features used to masks out irrelevant variables. We show that coupling a PointMask layer with an arbitrary model can discern the points in the input space which contribute the most to the prediction score, thereby leading to interpretability. Through designed bias experiments, we also show that thanks to its gradual masking feature, our proposed method is effective in handling data bias."}}
{"id": "9tZQDzSV6rc", "cdate": 1577836800000, "mdate": null, "content": {"title": "Contrastive Multi-View Representation Learning on Graphs", "abstract": "We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: https://github.com/kavehhassani/mvgrl"}}
{"id": "r1laNeBYPB", "cdate": 1569439797443, "mdate": null, "content": {"title": "Memory-Based Graph Networks", "abstract": "Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. We introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. We also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results shows that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data.\n"}}
{"id": "IMx1MAhl96n", "cdate": 1546300800000, "mdate": null, "content": {"title": "Relational Graph Representation Learning for Open-Domain Question Answering", "abstract": "We introduce a relational graph neural network with bi-directional attention mechanism and hierarchical representation learning for open-domain question answering task. Our model can learn contextual representation by jointly learning and updating the query, knowledge graph, and document representations. The experiments suggest that our model achieves state-of-the-art on the WebQuestionsSP benchmark."}}
{"id": "Fz1cR624wMP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Unsupervised Multi-Task Feature Learning on Point Clouds", "abstract": "We introduce an unsupervised multi-task model to jointly learn point and shape features on point clouds. We define three unsupervised tasks including clustering, reconstruction, and self-supervised classification to train a multi-scale graph-based encoder. We evaluate our model on shape classification and segmentation benchmarks. The results suggest that it outperforms prior state-of-the-art unsupervised models: In the ModelNet40 classification task, it achieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an mIoU of 68.2 and accuracy of 88.6%."}}
{"id": "DjT-0zaQzSa", "cdate": 1546300800000, "mdate": null, "content": {"title": "Unsupervised Multi-Task Feature Learning on Point Clouds", "abstract": "We introduce an unsupervised multi-task model to jointly learn point and shape features on point clouds. We define three unsupervised tasks including clustering, reconstruction, and self-supervised classification to train a multi-scale graph-based encoder. We evaluate our model on shape classification and segmentation benchmarks. The results suggest that it outperforms prior state-of-the-art unsupervised models: In the ModelNet40 classification task, it achieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an mIoU of 68.2 and accuracy of 88.6%."}}
{"id": "TbgU_XQ7oUY", "cdate": 1483228800000, "mdate": null, "content": {"title": "Learning Physical Properties of Objects Using Gaussian Mixture Models", "abstract": "Common-sense knowledge of physical properties of objects such as size and weight is required in a vast variety of AI applications. Yet, available common-sense knowledge-bases cannot answer simple questions regarding these properties such as \u201cis a microwave oven bigger than a spoon?\u201d or \u201cis a feather heavier than a king size mattress?\u201d. To bridge this gap, we harvest semi-structured data associated with physical properties of objects from the web. We then use an unsupervised taxonomy merging scheme to map a set of extracted objects to WordNet hierarchy. We also train a classifier to extend WordNet taxonomy to address both fine-grained and missing concepts. Finally, we use an ensemble of Gaussian mixture models to learn the distribution parameters of these properties. We also propose a Monte Carlo inference mechanism to answer comparative questions. Results suggest that the proposed approach can answer 94.6% of such questions, correctly."}}
