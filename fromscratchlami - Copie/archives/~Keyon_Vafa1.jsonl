{"id": "CBhqzCyxNU", "cdate": 1706757407859, "mdate": 1706757407859, "content": {"title": "Use large language models to promote equity", "abstract": "Advances in large language models (LLMs) have driven an explosion of interest about their societal impacts. Much of the discourse around how they will impact social equity has been cautionary or negative, focusing on questions like \"how might LLMs be biased and how would we mitigate those biases?\" This is a vital discussion: the ways in which AI generally, and LLMs specifically, can entrench biases have been well-documented. But equally vital, and much less discussed, is the more opportunity-focused counterpoint: \"what promising applications do LLMs enable that could promote equity?\" If LLMs are to enable a more equitable world, it is not enough just to play defense against their biases and failure modes. We must also go on offense, applying them positively to equity-enhancing use cases to increase opportunities for underserved groups and reduce societal discrimination. There are many choices which determine the impact of AI, and a fundamental choice very early in the pipeline is the problems we choose to apply it to. If we focus only later in the pipeline -- making LLMs marginally more fair as they facilitate use cases which intrinsically entrench power -- we will miss an important opportunity to guide them to equitable impacts. Here, we highlight the emerging potential of LLMs to promote equity by presenting four newly possible, promising research directions, while keeping risks and cautionary points in clear view."}}
{"id": "zETSaPIQOM", "cdate": 1664928782881, "mdate": null, "content": {"title": "CAREER: Economic Prediction of Labor Sequence Data Under Distribution Shift", "abstract": "Labor economists regularly analyze employment data by fitting predictive models to small, carefully constructed longitudinal survey datasets. Although modern machine learning methods offer promise for such problems, these survey datasets are too small to take advantage of them. In recent years large datasets of online resumes have also become available, providing data about the career trajectories of millions of individuals. However, the distribution of these large resume datasets differ in meaningful ways from the survey datasets used for economic estimation; standard econometric models cannot take advantage of their scale or make predictions under distribution shift. To this end we develop CAREER, a transformer-based model that uses transfer learning to learn representations of job sequences. CAREER is first fit to large, passively-collected resume data and then fine-tuned on samples of the downstream data distribution of interest. We find that CAREER forms accurate predictions of job sequences, achieving state-of-the-art predictive performance on three widely-used economics datasets. We also find that CAREER is adept at making predictions under distribution shifts in time. \n\nFull paper available at: https://arxiv.org/abs/2202.08370"}}
{"id": "SudRJjtGRz", "cdate": 1664928780874, "mdate": null, "content": {"title": "An Invariant Learning Characterization of Controlled Text Generation", "abstract": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. \nMany approaches reduce this problem to building a predictor of the desired attribute.\nFor example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. \nIn this paper, we show that the performance of controlled generation may be poor if the target distribution of text differs from the distribution the predictor was trained on. \nInstead, we take inspiration from causal representation learning and cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. Experiments demonstrate the promise and difficulty of adapting invariant learning methods, which have been primarily developed for vision, to text."}}
{"id": "zInaytkuzX", "cdate": 1664833378623, "mdate": null, "content": {"title": "An Invariant Learning Characterization of Controlled Text Generation", "abstract": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. \nMany approaches reduce this problem to building a predictor of the desired attribute.\nFor example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. \nIn this paper, we show that the performance of controlled generation may be poor if the target distribution of text differs from the distribution the predictor was trained on. \nInstead, we take inspiration from causal representation learning and cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. Experiments demonstrate the promise and difficulty of adapting invariant learning methods, which have been primarily developed for vision, to text."}}
{"id": "lyjMArzIxH6", "cdate": 1663850203191, "mdate": null, "content": {"title": "CAREER: Transfer Learning for Economic Prediction of Labor Data", "abstract": "Labor economists regularly analyze employment data by fitting predictive models to small, carefully constructed longitudinal survey datasets. Although modern machine learning methods offer promise for such problems, these survey datasets are too small to take advantage of them. In recent years large datasets of online resumes have also become available, providing data about the career trajectories of millions of individuals. However, standard econometric models cannot take advantage of their scale or incorporate them into the analysis of survey data. To this end we develop CAREER, a transformer-based model that uses transfer learning to learn representations of job sequences. CAREER is first fit to large, passively-collected resume data and then fine-tuned to smaller, better-curated datasets for economic inferences.  We fit CAREER to a dataset of 24 million job sequences from resumes, and fine-tune its representations on longitudinal survey datasets. We find that CAREER forms accurate predictions of job sequences, achieving state-of-the-art predictive performance on three widely-used economics datasets.  We further find that CAREER can be used to form good predictions of other downstream variables; incorporating CAREER into a wage model provides better predictions than the econometric models currently in use."}}
{"id": "7DgOz2mQD1U", "cdate": 1634227738466, "mdate": 1634227738466, "content": {"title": "Rationales for Sequential Predictions", "abstract": "Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the combinatorial objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales."}}
{"id": "rJlo4UIt_E", "cdate": 1553716786549, "mdate": null, "content": {"title": "Discrete Flows: Invertible Generative Models of Discrete Data", "abstract": "While normalizing flows have led to significant advances in modeling high-dimensional continuous distributions, their applicability to discrete distributions remains unknown. In this paper, we show that flows can in fact be extended to discrete events---and under a simple change-of-variables formula not requiring log-determinant-Jacobian computations. Discrete flows have numerous applications. We display proofs of concept under 2 flow architectures: discrete autoregressive flows enable bidirectionality, allowing for example tokens in text to depend on both left-to-right and right-to-left contexts in an exact language model; and discrete bipartite flows (i.e., with layer structure from RealNVP) enable parallel generation such as exact nonautoregressive text modeling."}}
