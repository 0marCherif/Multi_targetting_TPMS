{"id": "qeFqIMwJETc", "cdate": 1325376000000, "mdate": 1624238090535, "content": {"title": "Fast planar object detection and tracking via edgel templates", "abstract": "We describe an efficient method to detect and track planar objects using a template of edge segments. Such segments are selected at multiple scales based on gradient magnitude; their positions and orientations are used to determine a canonical reference frame where the descriptor is computed based on quantized orientation. The resulting descriptors are efficiently matched using logical operations, and tracked between frames. The method yields pose estimates that are robust to scale changes, foreshortening, partial occlusions, and is suitable for use in augmented reality and human-computer interaction."}}
{"id": "pXkO-h6zJur", "cdate": 1293840000000, "mdate": 1624238090682, "content": {"title": "Video-based descriptors for object recognition", "abstract": "We describe a visual recognition system operating on a hand-held device, based on a video-based feature descriptor, and characterize its invariance and discriminative properties. Feature selection and tracking are performed in real-time, and used to train a template-based classifier during a capture phase prompted by the user. During normal operation, the system recognizes objects in the field of view based on their ranking. Severe resource constraints have prompted a re-evaluation of existing algorithms improving their performance (accuracy and robustness) as well as computational efficiency. We motivate the design choices in the implementation with a characterization of the stability properties of local invariant detectors, and of the conditions under which a template-based descriptor is optimal. The analysis also highlights the role of time as \u201cweak supervisor\u201d during training, which we exploit in our implementation."}}
{"id": "HaxdNZ0P3AL", "cdate": 1293840000000, "mdate": 1624238090679, "content": {"title": "Edgel templates for fast planar object detection and pose estimation", "abstract": "We describe a method to select edgels and to calculate gradient orientation-based template descriptors for edgel features. An edgel is selected within a grid block based on gradient magnitude; its position and orientation are used to determine a canonical frame where the descriptor is computed based on quantized orientation. The resulting descriptor is efficiently matched using logical operations. We demonstrate the use of the resulting edgel detection and description method for planar object detection and pose estimation."}}
{"id": "B1V_N1MO-r", "cdate": 1293840000000, "mdate": null, "content": {"title": "Learning and matching multiscale template descriptors for real-time detection, localization and tracking", "abstract": "We describe a system to learn an object template from a video stream, and localize and track the corresponding object in live video. The template is decomposed into a number of local descriptors, thus enabling detection and tracking in spite of partial occlusion. Each local descriptor aggregates contrast invariant statistics (normalized intensity and gradient orientation) across scales, in a way that enables matching under significant scale variations. Low-level tracking during the training video sequence enables capturing object-specific variability due to the shape of the object, which is encapsulated in the descriptor. Salient locations on both the template and the target image are used as hypotheses to expedite matching."}}
{"id": "KTJ1MDINuA7", "cdate": 1262304000000, "mdate": 1624238090698, "content": {"title": "Feature tracking and object recognition on a hand-held", "abstract": "We demonstrate a visual recognition system operating on a hand-held device, with the help of an efficient and robust feature tracking and an object recognition mechanism that can be used for interactive mobile applications. In our recognition system, corner features are detected from captured video frames in a multi-scale image pyramid, and are tracked between consecutive frames efficiently. In order to perform object recognition, local descriptors are calculated on the tracked features, and quantized using a vocabulary tree. For each object, a bag-of-words model is learned from multiple views. The learned objects are recognized by computing the ranking score for the set of features in a single video frame. Our feature tracking algorithm and local descriptors are different than the Lucas-Kanade algorithm in image pyramid or the SIFT descriptor, however improving the efficiency and accuracy. For our implementation on a mobile phone, we used an iPhone 3GS with a 600MHz ARM chip CPU. The video frame is captured from a camera preview screen at a rate of 15 frames per second using the public API. The task of object recognition on a mobile phone runs at around 7 frames per second, including the feature tracking and descriptor calculation."}}
{"id": "N60gHQ1Ip-4", "cdate": 1230768000000, "mdate": 1624238090986, "content": {"title": "Multithreaded Hybrid Feature Tracking for Markerless Augmented Reality", "abstract": "We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking. Distinctive image features of the scene are detected and tracked frame-to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a synchronized multi-threaded manner: capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce user interaction methodology for establishing a global coordinate system and for placing virtual objects in the AR environment by tracking a user's outstretched hand and estimating a camera pose relative to it. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling AR in unprepared tabletop environments, using bare hands for interaction."}}
{"id": "5v-PN9uAS-r", "cdate": 1230768000000, "mdate": 1624238090986, "content": {"title": "Robust 3D street-view reconstruction using sky motion estimation", "abstract": "We introduce a robust 3D reconstruction system that uses a combination of the structure-from-motion (SfM) filter and the bundle adjustment. The local bundle adjustment provides an initial depth of a newly introduced feature to the SfM filter, and the filter enables to predict the motion of the camera while performing the reconstruction process. In addition, we increase the robustness of the rotation estimation by estimating the motion of the sky from cylindrical panoramas of street views. The sky region is segmented by a robust estimating algorithm based on a translational motion model in the cylindrical panoramas. We show that the combination of the SfM filter and the bundle adjustment with sky motion estimation algorithms produces a robust 3D reconstruction from the street view images, compared to running each method separately."}}
{"id": "LtwMnQp9aF7", "cdate": 1199145600000, "mdate": 1624238090546, "content": {"title": "Hybrid Feature Tracking and User Interaction for Markerless Augmented Reality", "abstract": "We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking methods. Distinctive image features of the scene are detected and tracked frame- to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a multi-threaded manner for capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce a user interaction for establishing a global coordinate system and for locating virtual objects in the AR environment. A user's bare hand is used for the user interface by estimating a camera pose relative to the user's outstretched hand. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling AR in unprepared tabletop environments using hands for interaction."}}
{"id": "MiUXfc15Fhz", "cdate": 1167609600000, "mdate": 1624238090846, "content": {"title": "Handy AR: Markerless Inspection of Augmented Reality Objects Using Fingertip Tracking", "abstract": "We present markerless camera tracking and user interface methodology for readily inspecting augmented reality (AR) objects in wearable computing applications. Instead of a marker, we use the human hand as a distinctive pattern that almost all wearable computer users have readily available. We present a robust real-time algorithm that recognizes fingertips to reconstruct the six-degree-of-freedom camera pose relative to the user's outstretched hand. A hand pose model is constructed in a one-time calibration step by measuring the fingertip positions in presence of ground-truth scale information. Through frame-by-frame reconstruction of the camera pose relative to the hand, we can stabilize 3D graphics annotations on top of the hand, allowing the user to inspect such virtual objects conveniently from different viewing angles in AR. We evaluate our approach with regard to speed and accuracy, and compare it to state-of-the-art marker-based AR systems. We demonstrate the robustness and usefulness of our approach in an example AR application for selecting and inspecting world-stabilized virtual objects."}}
{"id": "IJzgifnrqWG", "cdate": 1167609600000, "mdate": 1624238090669, "content": {"title": "Initializing Markerless Tracking Using a Simple Hand Gesture", "abstract": "We introduce a technique to establish a coordinate system for augmented reality (AR) on tabletop environments. A user's hand is tracked and the fingertips on the outstretched hand are detected, providing a camera pose estimation relative to the hand. As a user places the hand on the surface of a tabletop environment, the hand's coordinate system is propagated to the environment, detecting distinctive image features in the scene. The features are tracked fast and robustly using optical flow. In this way, a new tabletop AR environment is set up without having to carry a marker or a sophisticated tracking system to the environment itself. We also demonstrate a proof-of-concept application for establishing a tabletop AR environment and recognizing a scene when detecting its features."}}
