{"id": "pIW2m4DKiIf", "cdate": 1696635904910, "mdate": 1696635904910, "content": {"title": "Climbing the WOL: Training for Cheaper Inference", "abstract": "Efficient inference for wide output layers (WOLs) is an essential yet challenging task in large scale machine learning. Most approaches reduce this problem to approximate maximum inner product search (MIPS), which relies heavily on the observation that for a given model, ground truth labels correspond to logits of highest value during full model inference. However, such an assumption is restrictive in practice. In this paper, we argue that approximate MIPS subroutines, despite having sub-linear computation time, are sub-optimal because they are tailored for retrieving large inner products with high recall instead of retrieving the correct labels. With WOL, the labels often have moderate inner products, which makes approximate MIPS more challenging. We propose an alternative problem formulation, called Label Superior Sampling (LSS), where the objective is to tailor the system to ensure retrieval of the correct label. Accordingly, we propose a novel learned hash approach, which is significantly more efficient and sufficient for high inference accuracy than MIPS baselines. Our extensive evaluation indicates that LSS can match or even outperform full inference accuracy with around 5x speed up and 87% energy reduction."}}
{"id": "lgtqFR1rOj", "cdate": 1696635750185, "mdate": null, "content": {"title": "HALOS: Hashing Large Output Space for Cheap Inference", "abstract": "Efficient inference in large output space is an essential yet challenging task in large scale machine learning.\nPrevious approaches reduce this problem to Approximate Maximum Inner Product Search (AMIPS), which is\nbased on the observation that the prediction of a given model corresponds to the logit with the largest value.\nHowever, models are not perfect in accuracy, and the successful retrievals of the largest logit may not lead to\nthe correct predictions. We argue that approximate MIPS approaches are sub-optimal because they are tailored\nfor retrieving largest inner products class instead of retrieving the correct class. Moreover, the logits generated\nfrom neural networks with large output space lead to extra challenges for the AMIPS method to achieve a high\nrecall rate within the computation budget of efficient inference. In this paper, we propose HALOS, which reduces\ninference into sub-linear computation by selectively activating a small set of output layer neurons that are likely to\ncorrespond to the correct classes rather than to yield the largest logit. Our extensive evaluations show that HALOS\nmatches or even outperforms the accuracy of given models with 21\u00d7 speed up and 87% energy reduction."}}
{"id": "wcNk7H-uZYU", "cdate": 1681930066177, "mdate": 1681930066177, "content": {"title": "High-throughput Generative Inference of Large Language Models with a Single GPU", "abstract": "The high computational and memory requirements of large language model (LLM) inference traditionally make it feasible only with multiple high-end accelerators. In this paper, we study how to lower the requirements of LLM inference down to one commodity GPU and achieve practical performance. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. Through a linear programming optimizer, it searches for the best pattern to store and access the tensors, including weights, activations, and attention key/value cache. FlexGen further compresses both weights and KV cache to 4 bits with negligible accuracy loss. Compared with state-of-the-art offloading systems, FlexGen runs OPT-175B up to 100 faster on a single 16GB GPU and achieves a practical generation throughput of 1 token/s for the first time. FlexGen also comes with a pipeline parallelism runtime to allow super-linear scaling on decoding if more distributed GPUs are given."}}
{"id": "hEp_SFmxM7", "cdate": 1672531200000, "mdate": 1681499269366, "content": {"title": "Modeling Scattering Coefficients using Self-Attentive Complex Polynomials with Image-based Representation", "abstract": ""}}
{"id": "0dMmuFaW_v", "cdate": 1672531200000, "mdate": 1681483695219, "content": {"title": "High-throughput Generative Inference of Large Language Models with a Single GPU", "abstract": ""}}
{"id": "UHoGOaGjEq", "cdate": 1652737734274, "mdate": null, "content": {"title": "Decentralized Training of Foundation Models in Heterogeneous Environments", "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron and Deepspeed, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational \u201ctasklets\u201d in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8\u00d7 faster than prior state-of-the-art training systems."}}
{"id": "QDPonrGtl1", "cdate": 1652737731293, "mdate": null, "content": {"title": "Fine-tuning Language Models over Slow Networks using Activation Quantization with Guarantees", "abstract": "Communication compression is a crucial technique for modern distributed learning systems to alleviate their communication bottlenecks over slower networks. Despite recent intensive studies of gradient compression for data parallel-style training, compressing the activations for models trained with pipeline parallelism is still an open problem. In this paper, we propose AQ-SGD, a novel activation compression algorithm for communication-efficient pipeline parallelism training over slow networks. Different from previous efforts in activation compression, instead of compressing activation values directly, AQ-SGD compresses the changes of the activations. This allows us to show, to the best of our knowledge for the first time, that one can still achieve $O(1/\\sqrt{T})$ convergence rate for non-convex objectives under activation compression, without making assumptions on gradient unbiasedness that do not hold for deep learning models with non-linear activation functions. We then show that AQ-SGD can be optimized and implemented efficiently, without additional end-to-end runtime overhead. We evaluated AQ-SGD to fine-tune language models with up to 1.5 billion parameters, compressing activation to 2-4 bits. AQ-SGD provides up to $4.3\\times$ end-to-end speed-up in slower networks, without sacrificing model quality. Moreover, we also show that AQ-SGD can be combined with state-of-the-art gradient compression algorithms to enable end-to-end communication compression: All communications between machines, including model gradients, forward activations, and backward gradients are compressed into lower precision. This provides up to $4.9\\times$ end-to-end speed-up, without sacrificing model quality."}}
{"id": "gCEWkCeuLAl", "cdate": 1640995200000, "mdate": 1683756634452, "content": {"title": "Decentralized Training of Foundation Models in Heterogeneous Environments", "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron and Deepspeed, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational \u201ctasklets\u201d in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8\u00d7 faster than prior state-of-the-art training systems."}}
{"id": "cG-BTRD4km", "cdate": 1640995200000, "mdate": 1681679386105, "content": {"title": "Decentralized Training of Foundation Models in Heterogeneous Environments", "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational \"tasklets\" in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron)."}}
{"id": "YVqXI-ra7w", "cdate": 1640995200000, "mdate": 1668459978320, "content": {"title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models", "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is $3\\times$ faster than Butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.3$\\times$ faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 small with no drop in accuracy."}}
