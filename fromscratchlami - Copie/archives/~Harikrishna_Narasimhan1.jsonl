{"id": "o_sYD-1T2ZZ", "cdate": 1676827101524, "mdate": null, "content": {"title": "Robust Distillation for Worst-class Performance: On the Interplay Between Teacher and Student Objectives", "abstract": "Knowledge distillation is a popular technique that has been shown to produce remarkable gains in average accuracy. However, recent work has shown that these gains are not uniform across subgroups in the data, and can often come at the cost of accuracy on rare subgroups and classes. Robust optimization is a common remedy to improve worst-class accuracy in standard learning settings, but in distillation it is unknown whether it is best to apply robust objectives when training the teacher, the student, or both. This work studies the interplay between robust objectives for the teacher and student. Empirically, we show that that jointly modifying the teacher and student objectives can lead to better worst-class student performance and even Pareto improvement in the tradeoff between worst-class and overall performance. Theoretically, we show that the *per-class calibration* of teacher scores is key when training a robust student. Both the theory and experiments support the surprising finding that applying a robust teacher training objective does not always yield a more robust student."}}
{"id": "TbT8OaJow8", "cdate": 1674767584760, "mdate": 1674767584760, "content": {"title": "Distilling Double Descent", "abstract": "Distillation is the technique of training a \"student\" model based on examples that are labeled by a separate \"teacher\" model, which itself is trained on a labeled dataset. The most common explanations for why distillation \"works\" are predicated on the assumption that student is provided with \\emph{soft} labels, \\eg probabilities or confidences, from the teacher model. In this work, we show, that, even when the teacher model is highly overparameterized, and provides \\emph{hard} labels, using a very large held-out unlabeled dataset to train the student model can result in a model that outperforms more \"traditional\" approaches.\nOur explanation for this phenomenon is based on recent work on \"double descent\". It has been observed that, once a model's complexity roughly exceeds the amount required to memorize the training data, increasing the complexity \\emph{further} can, counterintuitively, result in \\emph{better} generalization. Researchers have identified several settings in which it takes place, while others have made various attempts to explain it (thus far, with only partial success). In contrast, we avoid these questions, and instead seek to \\emph{exploit} this phenomenon by demonstrating that a highly-overparameterized teacher can avoid overfitting via double descent, while a student trained on a larger independent dataset labeled by this teacher will avoid overfitting due to the size of its training set."}}
{"id": "LtoKJjkEnN", "cdate": 1674767450766, "mdate": 1674767450766, "content": {"title": "Consistent Multiclass Algorithms for Complex Metrics and Constraints", "abstract": "We present consistent algorithms for multiclass learning with complex performance metrics and constraints, where the objective and constraints are defined by arbitrary functions of the confusion matrix. This setting includes many common performance metrics such as the multiclass G-mean and micro F1-measure, and constraints such as those on the classifier\u2019s precision and recall and more recent measures of fairness discrepancy. We give a general framework for designing consistent algorithms for such complex design goals by viewing the learning problem as an optimization problem over the set of feasible confusion matrices. We provide multiple instantiations of our framework under different assumptions on the performance metrics and constraints, and in each case show rates of convergence to the optimal (feasible) classifier (and thus asymptotic consistency). Experiments on a variety of multiclass classification tasks and fairness constrained problems show that our algorithms compare favorably to the state-of-the-art baselines."}}
{"id": "4lZZjsC5UG", "cdate": 1674767276505, "mdate": 1674767276505, "content": {"title": "Robust Distillation for Wost-class Performance", "abstract": "Knowledge distillation has proven to be an effective technique in improving the\nperformance a student model using predictions from a teacher model. However,\nrecent work has shown that gains in average efficiency are not uniform across\nsubgroups in the data, and in particular can often come at the cost of accuracy\non rare subgroups and classes. To preserve strong performance across classes\nthat may follow a long-tailed distribution, we develop distillation techniques that\nare tailored to improve the student\u2019s worst-class performance. Specifically, we\nintroduce robust optimization objectives in different combinations for the teacher\nand student, and further allow for training with any tradeoff between the overall\naccuracy and the robust worst-class objective. We show empirically that our robust\ndistillation techniques not only achieve better worst-class performance, but also\nlead to Pareto improvement in the tradeoff between overall performance and worst-class \nperformance compared to other baseline methods. Theoretically, we provide\ninsights into what makes a good teacher when the goal is to train a robust student."}}
{"id": "ZfqZlVgSEA", "cdate": 1672531200000, "mdate": 1681497890160, "content": {"title": "Learning to reject meets OOD detection: Are all abstentions created equal?", "abstract": ""}}
{"id": "3KUfbI9_DQE", "cdate": 1663850022458, "mdate": null, "content": {"title": "Distributionally Robust Post-hoc Classifiers under Prior Shifts", "abstract": "The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization objective is inspired from a natural notion of robustness to controlled distribution shifts. Our method comes with provable guarantees and empirically makes a strong case for distributional robust post-hoc classifiers. An empirical implementation is available at https://github.com/weijiaheng/Drops.\n"}}
{"id": "_jg6Sf6tuF7", "cdate": 1652737741771, "mdate": null, "content": {"title": "Post-hoc estimators for learning to defer to an expert", "abstract": "Many practical settings allow a learner to defer predictions to one or more costly experts. For example, the learning to defer paradigm allows a learner to defer to a human expert, at some monetary cost. Similarly, the adaptive inference paradigm allows a base model to defer to one or more large models, at some computational cost. The goal in these settings is to learn classification and deferral mechanisms to optimise a suitable accuracy-cost tradeoff. To achieve this, a central issue studied in prior work is the design of a coherent loss function for both mechanisms. In this work, we demonstrate that existing losses have two subtle limitations: they can encourage underfitting when there is a high cost of deferring, and the deferral function can have a weak dependence on the base model predictions. To resolve these issues, we propose a post-hoc training scheme: we train a deferral function on top of a base model, with the objective of predicting to defer when the base model's error probability exceeds the cost of the expert model. This may be viewed as applying a partial surrogate to the ideal deferral loss, which can lead to a tighter approximation and thus better performance. Empirically, we verify the efficacy of post-hoc training on benchmarks for learning to defer and adaptive inference."}}
{"id": "HcUdtvUoqx5", "cdate": 1646077537270, "mdate": null, "content": {"title": "Quadratic Metric Elicitation for Fairness and Beyond", "abstract": "Metric elicitation is a recent framework for eliciting classification performance metrics that best reflect implicit user preferences based on the task and context. However, available elicitation strategies have been limited to linear (or quasi-linear) functions of predictive rates, which can be practically restrictive for many applications including fairness. This paper develops a strategy for eliciting more flexible multiclass metrics defined by quadratic functions of rates, designed to reflect human preferences better. We show its application in eliciting quadratic violation-based group-fair metrics. Our strategy requires only relative preference feedback, is robust to noise, and achieves near-optimal query complexity. We further extend this strategy to eliciting polynomial metrics -- thus broadening the use cases for metric elicitation."}}
{"id": "uv6nVpdRt5", "cdate": 1640995200000, "mdate": 1683657144914, "content": {"title": "Consistent Multiclass Algorithms for Complex Metrics and Constraints", "abstract": "We present consistent algorithms for multiclass learning with complex performance metrics and constraints, where the objective and constraints are defined by arbitrary functions of the confusion matrix. This setting includes many common performance metrics such as the multiclass G-mean and micro F1-measure, and constraints such as those on the classifier's precision and recall and more recent measures of fairness discrepancy. We give a general framework for designing consistent algorithms for such complex design goals by viewing the learning problem as an optimization problem over the set of feasible confusion matrices. We provide multiple instantiations of our framework under different assumptions on the performance metrics and constraints, and in each case show rates of convergence to the optimal (feasible) classifier (and thus asymptotic consistency). Experiments on a variety of multiclass classification tasks and fairness-constrained problems show that our algorithms compare favorably to the state-of-the-art baselines."}}
{"id": "_zQJ1lA3Qbp", "cdate": 1640995200000, "mdate": 1683657144338, "content": {"title": "Post-hoc estimators for learning to defer to an expert", "abstract": "Many practical settings allow a learner to defer predictions to one or more costly experts. For example, the learning to defer paradigm allows a learner to defer to a human expert, at some monetary cost. Similarly, the adaptive inference paradigm allows a base model to defer to one or more large models, at some computational cost. The goal in these settings is to learn classification and deferral mechanisms to optimise a suitable accuracy-cost tradeoff. To achieve this, a central issue studied in prior work is the design of a coherent loss function for both mechanisms. In this work, we demonstrate that existing losses have two subtle limitations: they can encourage underfitting when there is a high cost of deferring, and the deferral function can have a weak dependence on the base model predictions. To resolve these issues, we propose a post-hoc training scheme: we train a deferral function on top of a base model, with the objective of predicting to defer when the base model's error probability exceeds the cost of the expert model. This may be viewed as applying a partial surrogate to the ideal deferral loss, which can lead to a tighter approximation and thus better performance. Empirically, we verify the efficacy of post-hoc training on benchmarks for learning to defer and adaptive inference."}}
