{"id": "DA4Ay4rJfCL", "cdate": 1671498480303, "mdate": 1671498480303, "content": {"title": "CMG: A Class-Mixed Generation Approach to Out-of-Distribution Detection", "abstract": "Recently, contrastive learning with data and class augmentations has been shown to produce markedly better results for out-of-distribution (OOD) detection than previous approaches. However, a major shortcoming of this approach is that it is extremely slow due to the significant increase in data size and in the number of classes and the quadratic pairwise similarity computation. This paper shows that this heavy machinery is unnecessary. A novel approach, called CMG (Class-Mixed Generation), is proposed, which generates pseudo-OOD data by mixing class embeddings as abnormal conditions to CVAE (conditional variational Auto-Encoder) and then uses the data to fine-tune a classifier built using the given in-distribution (IND) data. To our surprise, the obvious approach of using the IND data and the pseudo-OOD data to directly train an OOD model is a very poor choice. The fine-tuning based approach turns out to be markedly better. Empirical evaluation shows that CMG not only produces new state-of-the-art results but also is much more efficient than contrastive learning, at least 10 times faster."}}
{"id": "-u5L5CItlKV", "cdate": 1649649181496, "mdate": null, "content": {"title": "HRN: A Holistic Approach to One Class Learning", "abstract": "Existing neural network based one-class learning methods mainly use various forms\nof auto-encoders or GAN style adversarial training to learn a latent representation\nof the given one class of data. This paper proposes an entirely different approach\nbased on a novel regularization, called holistic regularization (or H-regularization),\nwhich enables the system to consider the data holistically, not to produce a model\nthat biases towards some features. Combined with a proposed 2-norm instance \nlevel data normalization, we obtain an effective one-class learning method, called\nHRN. To our knowledge, the proposed regularization and the normalization method\nhave not been reported before. Experimental evaluation using both benchmark\nimage classification and traditional anomaly detection datasets show that HRN\nmarkedly outperforms the state-of-the-art existing deep/non-deep learning models.\nThe code of HRN can be found here.\n."}}
{"id": "DBczWhxch7", "cdate": 1649648629801, "mdate": null, "content": {"title": "BNS: Building Network Structures Dynamically for Continual Learning", "abstract": "Continual learning (CL) of a sequence of tasks is often accompanied with the\ncatastrophic forgetting (CF) problem. Existing research has achieved remarkable\nresults in overcoming CF, especially for task continual learning. However, limited\nwork has been done to achieve another important goal of CL, knowledge transfer.\nIn this paper, we propose a technique (called BNS) to do both. The novelty of\nBNS is that it dynamically builds a network to learn each new task to overcome\nCF and to transfer knowledge across tasks at the same time. Experimental results\nshow that when the tasks are different (with little shared knowledge), BNS can\nalready outperform the state-of-the-art baselines. When the tasks are similar and\nhave shared knowledge, BNS outperforms the baselines substantially by a large\nmargin due to its knowledge transfer capability"}}
{"id": "JvPopr9skL0", "cdate": 1632875604052, "mdate": null, "content": {"title": "Efficient Out-of-Distribution Detection via CVAE data Generation", "abstract": "Recently, contrastive loss with data augmentation and pseudo class creation has been shown to produce markedly better results for out-of-distribution (OOD) detection than previous methods. However, a major shortcoming of this approach is that it is extremely slow due to significant increase in the data size and the number of classes and the quadratic complexity of pairwise similarity computation. This paper proposes a novel and simple method that can build an effective data generator using Conditional Variational Auto-Encoder (CVAE) to generate pseudo OOD samples. Based on the generated pseudo OOD data, a flexible and efficient OOD detection method is proposed through fine-tuning, which achieves results comparable to the state-of-the-art OOD detection techniques, but the execution speed is at least 10 times faster. Also importantly, the proposed approach is in fact a general framework that can be applied to many existing OOD methods and improve them via the proposed fine-tuning. We have combined it with the best baseline OOD models in our experiments to produce new state-of-the-art results."}}
{"id": "2ybxtABV2Og", "cdate": 1621630095344, "mdate": null, "content": {"title": "BNS: Building Network Structures Dynamically for Continual Learning", "abstract": "Continual learning (CL) of a sequence of tasks is often accompanied with the catastrophic forgetting(CF) problem. Existing research has achieved remarkable results in overcoming CF, especially for task continual learning. However, limited work has been done to achieve another important goal of CL,knowledge transfer.In this paper, we propose a technique (called BNS) to do both.  The novelty of BNS is that it dynamically builds a network to learn each new task to overcome CF and to transfer knowledge across tasks at the same time. Experimental results show that when the tasks are different (with little shared knowledge), BNS can already outperform the state-of-the-art baselines. When the tasks are similar and have shared knowledge, BNS outperforms the baselines substantially by a large margin due to its knowledge transfer capability."}}
{"id": "HygPjlrYvB", "cdate": 1569439902643, "mdate": null, "content": {"title": "Learning from Positive and Unlabeled Data  with Adversarial Training", "abstract": "Positive-unlabeled (PU) learning learns a binary classifier using only positive and unlabeled examples without labeled negative examples. This paper shows that the GAN (Generative Adversarial Networks) style of adversarial training is quite suitable for PU learning. GAN learns a generator to generate data (e.g., images) to fool a discriminator which tries to determine whether the generated data belong to a (positive) training class. PU learning is similar and can be naturally casted as trying to identify (not generate) likely positive data from the unlabeled set also to fool a discriminator that determines whether the identified likely positive data from the unlabeled set (U) are indeed positive (P). A direct adaptation of GAN for PU learning does not produce a strong classifier. This paper proposes a more effective method called Predictive Adversarial Networks (PAN) using a new objective function based on KL-divergence, which performs much better.~Empirical evaluation using both image and text data shows the effectiveness of PAN.  "}}
{"id": "B1eO9oA5Km", "cdate": 1538087823942, "mdate": null, "content": {"title": "A Guider Network for Multi-Dual Learning", "abstract": "A large amount of parallel data is needed to train a strong neural machine translation (NMT) system. This is a major challenge for low-resource languages. Building on recent work on unsupervised and semi-supervised methods, we propose a multi-dual learning framework to improve the performance of NMT by using an almost infinite amount of available monolingual data and some parallel data of other languages. Since our framework involves multiple languages and components, we further propose a timing optimization method that uses reinforcement learning (RL) to optimally schedule the different components in order to avoid imbalanced training. Experimental results  demonstrate the validity of our model, and confirm its superiority to existing dual learning methods."}}
{"id": "ryGvcoA5YX", "cdate": 1538087823049, "mdate": null, "content": {"title": "Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation", "abstract": "Learning multiple tasks sequentially is important for the development of AI and lifelong learning systems. However, standard neural network architectures suffer from catastrophic forgetting which makes it difficult for them to learn a sequence of tasks. Several continual learning methods have been proposed to address the problem. In this paper, we propose a very different approach, called Parameter Generation and Model Adaptation (PGMA), to dealing with the problem. The proposed approach learns to build a model, called the solver, with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the solver to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach."}}
{"id": "rk3pnae0b", "cdate": 1518730178179, "mdate": null, "content": {"title": "Topic-Based Question Generation", "abstract": "Asking questions is an important ability for a chatbot. This paper focuses on question generation. Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem. In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text. The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics. One almost never asks a random question in a conversation. Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use. To solve the problem, we propose a novel neural network that is able to generate topic-specific questions. One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers. Experimental results show that our model outperforms the state-of-the-art baseline."}}
{"id": "rkRR1ynIf", "cdate": 1518231510316, "mdate": null, "content": {"title": "Aspect-based Question Generation", "abstract": "Asking questions is an important ability for a chatbot.  Although there are existing works on question generation with a piece of descriptive text, it remains to be a very challenging problem. In this paper, we consider a new question generation problem  which also requires the input of a target aspect in addition to a piece of descriptive text. The key reason for this new problem is that it has been found from practical applications that useful questions need to be targeted toward some relevant aspects. One almost never asks a random question in a conversation. Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use. in order to solve this problem, we propose a novel neural network which is able to generate aspect-based questions. One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating aspects in the questions or answers. Experimental results show that our proposed model outperforms the state-of-the-art question generation methods."}}
