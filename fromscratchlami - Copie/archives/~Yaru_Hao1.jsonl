{"id": "fzbHRjAd8U", "cdate": 1675827732797, "mdate": null, "content": {"title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers", "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict labels for unseen inputs without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand ICL as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We compare the behaviors of ICL and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. "}}
{"id": "OOQK1CmdN3", "cdate": 1672531200000, "mdate": 1681636855487, "content": {"title": "Language Is Not All You Need: Aligning Perception with Language Models", "abstract": ""}}
{"id": "nUsP9lFADUF", "cdate": 1663850055592, "mdate": null, "content": {"title": "Prototypical Calibration for Few-shot Learning of Language Models", "abstract": "In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations. In this work, we propose prototypical calibration to adaptively learn a more robust decision boundary for zero- and few-shot classification, instead of greedy decoding. Concretely, our method first adopts Gaussian mixture distribution to estimate the prototypical clusters for all categories. Then we assign each cluster to the corresponding label by solving a weighted bipartite matching problem. Given an example, its prediction is calibrated by the likelihood of prototypical clusters. Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks. Extensive analysis across different scales also indicates that our method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance."}}
{"id": "oSFvruV37O", "cdate": 1640995200000, "mdate": 1673187915558, "content": {"title": "Prototypical Calibration for Few-shot Learning of Language Models", "abstract": ""}}
{"id": "nbP55HeaRD", "cdate": 1640995200000, "mdate": 1673187915530, "content": {"title": "Language Models are General-Purpose Interfaces", "abstract": ""}}
{"id": "eJJewnoXT92", "cdate": 1640995200000, "mdate": 1673187916271, "content": {"title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers", "abstract": ""}}
{"id": "-V5UU6-xkUE", "cdate": 1640995200000, "mdate": 1681553193772, "content": {"title": "Prototypical Fine-tuning: Towards Robust Performance Under Varying Data Sizes", "abstract": ""}}
{"id": "vcjEsStdQXh", "cdate": 1609459200000, "mdate": 1638251335847, "content": {"title": "Learning to Sample Replacements for ELECTRA Pre-Training", "abstract": "Yaru Hao, Li Dong, Hangbo Bao, Ke Xu, Furu Wei. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021."}}
{"id": "W_uuU8uc55s", "cdate": 1609459200000, "mdate": 1638251335674, "content": {"title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer", "abstract": "The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT."}}
{"id": "rPBH9flMTsB", "cdate": 1577836800000, "mdate": null, "content": {"title": "Investigating Learning Dynamics of BERT Fine-Tuning", "abstract": "Yaru Hao, Li Dong, Furu Wei, Ke Xu. Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing. 2020."}}
