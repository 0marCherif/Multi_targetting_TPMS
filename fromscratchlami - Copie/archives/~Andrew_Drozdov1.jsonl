{"id": "oaaFoaBwQa", "cdate": 1672531200000, "mdate": 1702356787583, "content": {"title": "kNN-LM Does Not Improve Open-ended Text Generation", "abstract": ""}}
{"id": "S1U6Pty3uc", "cdate": 1672531200000, "mdate": 1701998484593, "content": {"title": "PaRaDe: Passage Ranking using Demonstrations with Large Language Models", "abstract": "Recent studies show that large language models (LLMs) can be instructed to effectively perform zero-shot passage re-ranking, in which the results of a first stage retrieval method, such as BM25, are rated and reordered to improve relevance. In this work, we improve LLM-based re-ranking by algorithmically selecting few-shot demonstrations to include in the prompt. Our analysis investigates the conditions where demonstrations are most helpful, and shows that adding even one demonstration is significantly beneficial. We propose a novel demonstration selection strategy based on difficulty rather than the commonly used semantic similarity. Furthermore, we find that demonstrations helpful for ranking are also effective at question generation. We hope our work will spur more principled research into question generation and passage ranking."}}
{"id": "GQ3omMGQS4R", "cdate": 1672531200000, "mdate": 1702356787550, "content": {"title": "Compositional Semantic Parsing with Large Language Models", "abstract": ""}}
{"id": "AWD6Uzv1a1T", "cdate": 1672531200000, "mdate": 1702356787555, "content": {"title": "Multistage Collaborative Knowledge Distillation from Large Language Models", "abstract": "We study semi-supervised sequence prediction tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from a prompted LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we propose a new distillation method, multistage collaborative knowledge distillation from an LLM (MCKD), for such tasks. MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively."}}
{"id": "BeQQEV84jg-", "cdate": 1665180798935, "mdate": null, "content": {"title": "You can\u2019t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM", "abstract": "Retrieval-enhanced language models (LMs), which condition their predictions on text retrieved from large external datastores, have recently shown significant perplexity improvements compared to standard LMs. One such approach, the kNN-LM, interpolates any existing LM's predictions with the output of a k-nearest neighbors model and requires no additional training. In this paper, we explore the importance of lexical and semantic matching in the context of items retrieved by kNN-LM. We find two trends: (1) the presence of large overlapping n-grams between the datastore and evaluation set plays an important factor in strong performance, even when the datastore is derived from the training data; and (2) the kNN-LM is most beneficial when retrieved items have high semantic similarity with the query. Based on our analysis, we define a new formulation of the kNN-LM that uses retrieval quality to assign the interpolation coefficient. We empirically measure the effectiveness of our approach on two English language modeling datasets, Wikitext-103 and PG-19. Our re-formulation of the kNN-LM is beneficial in both cases, and leads to nearly 4% improvement in perplexity on the Wikitext-103 test set."}}
{"id": "gJW8hSGBys8", "cdate": 1663850135938, "mdate": null, "content": {"title": "Compositional Semantic Parsing with Large Language Models", "abstract": "Humans can reason compositionally when presented with new tasks.  Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications."}}
{"id": "HcyBuhJXXV3", "cdate": 1640995200000, "mdate": 1665180444673, "content": {"title": "Inducing and Using Alignments for Transition-based AMR Parsing", "abstract": "Andrew Drozdov, Jiawei Zhou, Radu Florian, Andrew McCallum, Tahira Naseem, Yoon Kim, Ram\u00f3n Astudillo. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "67H3WwRfbRt", "cdate": 1609459200000, "mdate": 1637086756908, "content": {"title": "Improved Latent Tree Induction with Distant Supervision via Span Constraints", "abstract": "Zhiyang Xu, Andrew Drozdov, Jay Yoon Lee, Tim O\u2019Gorman, Subendhu Rongali, Dylan Finkbeiner, Shilpa Suresh, Mohit Iyyer, Andrew McCallum. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "e95S-F977R0", "cdate": 1577836800000, "mdate": 1634313951556, "content": {"title": "Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders", "abstract": "Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim O\u2019Gorman, Mohit Iyyer, Andrew McCallum. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "Ibqp_q5tMd-", "cdate": 1577836800000, "mdate": 1637086757071, "content": {"title": "The impact of preprint servers in the formation of novel ideas", "abstract": "Swarup Satish, Zonghai Yao, Andrew Drozdov, Boris Veytsman. Proceedings of the First Workshop on Scholarly Document Processing. 2020."}}
