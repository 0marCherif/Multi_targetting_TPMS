{"id": "_kLoXNwyws7", "cdate": 1640995200000, "mdate": 1682623326247, "content": {"title": "Continuous Forecasting via Neural Eigen Decomposition of Stochastic Dynamics", "abstract": "Neural differential equations predict the derivative of a stochastic process. This allows irregular forecasting with arbitrary time-steps. However, the expressive temporal flexibility often comes with a high sensitivity to noise. In addition, current methods model measurements and control together, limiting generalization to different control policies. These properties severely limit applicability to medical treatment problems, which require reliable forecasting given high noise, limited data and changing treatment policies. We introduce the Neural Eigen-SDE algorithm (NESDE), which relies on piecewise linear dynamics modeling with spectral representation. NESDE provides control over the expressiveness level; decoupling of control from measurements; and closed-form continuous prediction in inference. NESDE is demonstrated to provide robust forecasting in both synthetic and real high-noise medical problems. Finally, we use the learned dynamics models to publish simulated medical gym environments."}}
{"id": "S1gqraNKwB", "cdate": 1569439041576, "mdate": null, "content": {"title": "Contextual Inverse Reinforcement Learning", "abstract": "We consider the Inverse Reinforcement Learning problem in Contextual Markov\nDecision Processes. In this setting, the reward, which is unknown to the agent, is a\nfunction of a static parameter referred to as the context. There is also an \u201cexpert\u201d\nwho knows this mapping and acts according to the optimal policy for each context.\nThe goal of the agent is to learn the expert\u2019s mapping by observing demonstrations.\nWe define an optimization problem for finding this mapping and show that when\nit is linear, the problem is convex. We present and analyze the sample complexity\nof three algorithms for solving this problem: the mirrored descent algorithm,\nevolution strategies, and the ellipsoid method. We also extend the first two methods\nto work with general reward functions, e.g., deep neural networks, but without the\ntheoretical guarantees. Finally, we compare the different techniques empirically in\ndriving simulation and a medical treatment regime."}}
