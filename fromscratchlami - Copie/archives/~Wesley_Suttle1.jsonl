{"id": "VJgxHKjj7e", "cdate": 1679929747077, "mdate": 1679929747077, "content": {"title": "Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic", "abstract": "Many existing reinforcement learning (RL) methods\nemploy stochastic gradient iteration on the\nback end, whose stability hinges upon a hypothesis\nthat the data-generating process mixes exponentially\nfast with a rate parameter that appears\nin the step-size selection. Unfortunately, this assumption\nis violated for large state spaces or settings\nwith sparse rewards, and the mixing time is\nunknown, making the step size inoperable. In this\nwork, we propose an RL methodology attuned\nto the mixing time by employing a multi-level\nMonte Carlo estimator for the critic, the actor,\nand the average reward embedded within an actor critic\n(AC) algorithm. This method, which we\ncall Multi-level Actor-Critic (MAC), is developed\nespecially for infinite-horizon average-reward settings\nand neither relies on oracle knowledge of\nthe mixing time in its parameter selection nor assumes\nits exponential decay; it, therefore, is readily\napplicable to applications with slower mixing\ntimes. Nonetheless, it achieves a convergence rate\ncomparable to the state-of-the-art AC algorithms.\nWe experimentally show that these alleviated restrictions\non the technical conditions required for\nstability translate to superior performance in practice\nfor RL problems with sparse rewards."}}
{"id": "UjEqQnS5fLc", "cdate": 1667440633295, "mdate": null, "content": {"title": "Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search", "abstract": "We develop a new measure of the exploration/exploitation trade-off in infinite-horizon reinforcement learning (RL) problems called the occupancy information ratio (OIR), which is comprised of a ratio between the infinite-horizon average cost of a policy and the entropy of its induced long-term state occupancy measure. Modifying the classic RL objective in this way yields policies that strike an optimal balance between exploitation and exploration, providing a new tool for addressing the exploration/exploitation trade-off in RL. The paper develops for the first time policy gradient and actor-critic algorithms for OIR optimization based upon a new entropy gradient theorem, and establishes both asymptotic and non-asymptotic convergence results with global optimality guarantees. In experiments, these methodologies outperform several deep RL baselines in problems with sparse rewards, where many trajectories may be uninformative and skepticism about the environment is crucial to success."}}
{"id": "8DP1sfCJl8", "cdate": 1640995200000, "mdate": 1681832962515, "content": {"title": "Reinforcement Learning Based Distributed Control of Dissipative Networked Systems", "abstract": "We consider the problem of designing distributed controllers to stabilize a class of networked systems, where each subsystem is dissipative and designs a reinforcement learning based local controller to maximize an individual cumulative reward function. We develop an approach that enforces dissipativity conditions on these local controllers at each subsystem to guarantee stability of the entire networked system. The proposed approach is illustrated on a dc microgrid example, where the objective is to maintain voltage stability of the network using locally distributed controllers at each generation unit."}}
{"id": "o9kPQdPmfn", "cdate": 1609459200000, "mdate": 1681832962498, "content": {"title": "Reinforcement Learning for Cost-Aware Markov Decision Processes", "abstract": "Ratio maximization has applications in areas as diverse as finance, reward shaping for reinforcement learning (RL), and the development of safe artificial intelligence, yet there has been very litt..."}}
{"id": "YDwisu4gRmt", "cdate": 1577836800000, "mdate": 1681832962494, "content": {"title": "Reinforcement Learning based Distributed Control of Dissipative Networked Systems", "abstract": "We consider the problem of designing distributed controllers to stabilize a class of networked systems, where each subsystem is dissipative and designs a reinforcement learning based local controller to maximize an individual cumulative reward function. We develop an approach that enforces dissipativity conditions on these local controllers at each subsystem to guarantee stability of the entire networked system. The proposed approach is illustrated on a DC microgrid example, where the objective is maintain voltage stability of the network using local distributed controllers at each generation unit."}}
{"id": "H27hXKd0P0", "cdate": 1546300800000, "mdate": 1681832962496, "content": {"title": "A Multi-Agent Off-Policy Actor-Critic Algorithm for Distributed Reinforcement Learning", "abstract": "This paper extends off-policy reinforcement learning to the multi-agent case in which a set of networked agents communicating with their neighbors according to a time-varying graph collaboratively evaluates and improves a target policy while following a distinct behavior policy. To this end, the paper develops a multi-agent version of emphatic temporal difference learning for off-policy policy evaluation, and proves convergence under linear function approximation. The paper then leverages this result, in conjunction with a novel multi-agent off-policy policy gradient theorem and recent work in both multi-agent on-policy and single-agent off-policy actor-critic methods, to develop and give convergence guarantees for a new multi-agent off-policy actor-critic algorithm."}}
