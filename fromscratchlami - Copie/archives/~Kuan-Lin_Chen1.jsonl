{"id": "DcscYUolZy", "cdate": 1683881521257, "mdate": 1683881521257, "content": {"title": "ResNEsts and DenseNEsts: Block-based DNN Models with Improved Representation Guarantees", "abstract": "Models recently used in the literature proving residual networks (ResNets) are better than linear predictors are actually different from standard ResNets that have been widely used in computer vision. In addition to the assumptions such as scalar-valued output or single residual block, the models fundamentally considered in the literature have no nonlinearities at the final residual representation that feeds into the final affine layer. To codify such a difference in nonlinearities and reveal a linear estimation property, we define ResNEsts, i.e., Residual Nonlinear Estimators, by simply dropping nonlinearities at the last residual representation from standard ResNets. We show that wide ResNEsts with bottleneck blocks can always guarantee a very desirable training property that standard ResNets aim to achieve, i.e., adding more blocks does not decrease performance given the same set of basis elements. To prove that, we first recognize ResNEsts are basis function models that are limited by a coupling problem in basis learning and linear prediction. Then, to decouple prediction weights from basis learning, we construct a special architecture termed augmented ResNEst (A-ResNEst) that always guarantees no worse performance with the addition of a block. As a result, such an A-ResNEst establishes empirical risk lower bounds for a ResNEst using corresponding bases. Our results demonstrate ResNEsts indeed have a problem of diminishing feature reuse; however, it can be avoided by sufficiently expanding or widening the input space, leading to the above-mentioned desirable property. Inspired by the densely connected networks (DenseNets) that have been shown to outperform ResNets, we also propose a corresponding new model called Densely connected Nonlinear Estimator (DenseNEst). We show that any DenseNEst can be represented as a wide ResNEst with bottleneck blocks. Unlike ResNEsts, DenseNEsts exhibit the desirable property without any special architectural re-design."}}
{"id": "J9bWIdibqC", "cdate": 1683880869919, "mdate": null, "content": {"title": "A DNN Based Normalized Time-Frequency Weighted Criterion for Robust Wideband DoA Estimation", "abstract": "Deep neural networks (DNNs) have greatly benefited direction of arrival (DoA) estimation methods for speech source localization in noisy environments. However, their localization accuracy is still far from satisfactory due to the vulnerability to nonspeech interference. To improve the robustness against interference, we propose a DNN based normalized time-frequency (T-F) weighted criterion which minimizes the distance between the candidate steering vectors and the filtered snapshots in the T-F domain. Our method requires no eigendecomposition and uses a simple normalization to prevent the optimization objective from being misled by noisy filtered snapshots. We also study different designs of T-F weights guided by a DNN. We find that duplicating the Hadamard product of speech ratio masks is highly effective and better than other techniques such as direct masking and taking the mean in the proposed approach. However, the best-performing design of T-F weights is criterion-dependent in general. Experiments show that the proposed method outperforms popular DNN based DoA estimation methods including widely used subspace methods in noisy and reverberant environments."}}
{"id": "sQiEJLPt1Qh", "cdate": 1652737438527, "mdate": null, "content": {"title": "Improved Bounds on Neural Complexity for Representing Piecewise Linear Functions", "abstract": "A deep neural network using rectified linear units represents a continuous piecewise linear (CPWL) function and vice versa. Recent results in the literature estimated that the number of neurons needed to exactly represent any CPWL function grows exponentially with the number of pieces or exponentially in terms of the factorial of the number of distinct linear components. Moreover, such growth is amplified linearly with the input dimension. These existing results seem to indicate that the cost of representing a CPWL function is expensive. In this paper, we propose much tighter bounds and establish a polynomial time algorithm to find a network satisfying these bounds for any given CPWL function. We prove that the number of hidden neurons required to exactly represent any CPWL function is at most a quadratic function of the number of pieces. In contrast to all previous results, this upper bound is invariant to the input dimension. Besides the number of pieces, we also study the number of distinct linear components in CPWL functions. When such a number is also given, we prove that the quadratic complexity turns into bilinear, which implies a lower neural complexity because the number of distinct linear components is always not greater than the minimum number of pieces in a CPWL function. When the number of pieces is unknown, we prove that, in terms of the number of distinct linear components, the neural complexities of any CPWL function are at most polynomial growth for low-dimensional inputs and factorial growth for the worst-case scenario, which are significantly better than existing results in the literature."}}
{"id": "hjo-yo4JGX4", "cdate": 1640995200000, "mdate": 1682548763090, "content": {"title": "Leveraging Heteroscedastic Uncertainty in Learning Complex Spectral Mapping for Single-channel Speech Enhancement", "abstract": "Most speech enhancement (SE) models learn a point estimate and do not make use of uncertainty estimation in the learning process. In this paper, we show that modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian negative log-likelihood (NLL) improves SE performance at no extra cost. During training, our approach augments a model learning complex spectral mapping with a temporary submodel to predict the covariance of the enhancement error at each time-frequency bin. Due to unrestricted heteroscedastic uncertainty, the covariance introduces an undersampling effect, detrimental to SE performance. To mitigate undersampling, our approach inflates the uncertainty lower bound and weights each loss component with their uncertainty, effectively compensating severely undersampled components with more penalties. Our multivariate setting reveals common covariance assumptions such as scalar and diagonal matrices. By weakening these assumptions, we show that the NLL achieves superior performance compared to popular loss functions including the mean squared error (MSE), mean absolute error (MAE), and scale-invariant signal-to-distortion ratio (SI-SDR)."}}
{"id": "IROqhpEha8", "cdate": 1621629886671, "mdate": null, "content": {"title": "ResNEsts and DenseNEsts: Block-based DNN Models with Improved Representation Guarantees", "abstract": "Models recently used in the literature proving residual networks (ResNets) are better than linear predictors are actually different from standard ResNets that have been widely used in computer vision. In addition to the assumptions such as scalar-valued output or single residual block, the models fundamentally considered in the literature have no nonlinearities at the final residual representation that feeds into the final affine layer. To codify such a difference in nonlinearities and reveal a linear estimation property, we define ResNEsts, i.e., Residual Nonlinear Estimators, by simply dropping nonlinearities at the last residual representation from standard ResNets. We show that wide ResNEsts with bottleneck blocks can always guarantee a very desirable training property that standard ResNets aim to achieve, i.e., adding more blocks does not decrease performance given the same set of basis elements. To prove that, we first recognize ResNEsts are basis function models that are limited by a coupling problem in basis learning and linear prediction. Then, to decouple prediction weights from basis learning, we construct a special architecture termed augmented ResNEst (A-ResNEst) that always guarantees no worse performance with the addition of a block. As a result, such an A-ResNEst establishes empirical risk lower bounds for a ResNEst using corresponding bases. Our results demonstrate ResNEsts indeed have a problem of diminishing feature reuse; however, it can be avoided by sufficiently expanding or widening the input space, leading to the above-mentioned desirable property. Inspired by the densely connected networks (DenseNets) that have been shown to outperform ResNets, we also propose a corresponding new model called Densely connected Nonlinear Estimator (DenseNEst). We show that any DenseNEst can be represented as a wide ResNEst with bottleneck blocks. Unlike ResNEsts, DenseNEsts exhibit the desirable property without any special architectural re-design."}}
{"id": "Kyky0JGPzN3", "cdate": 1609459200000, "mdate": 1682548763105, "content": {"title": "Multirate Audiometric Filter Bank for Hearing Aid Devices", "abstract": "The frequency-dependent nature of hearing loss poses many challenges for hearing aid design. In order to compensate for a hearing aid user\u2019s unique hearing loss pattern, an input signal often needs to be separated into frequency bands, or channels, through a process called sub-band decomposition. In this paper, we present a real-time filter bank for hearing aids. Our filter bank features 10 channels uniformly distributed on the logarithmic scale, located at the standard audiometric frequencies used for the characterization and fitting of hearing aids. We obtained filters with very narrow passbands in the lower frequencies by employing multi-rate signal processing. Our filter bank offers a 9.1\u00d7 reduction in complexity as compared to conventional signal processing. We implemented our filter bank on Open Speech Platform, an open-source hearing aid, and confirmed real-time operation."}}
{"id": "n7hWt1tc8b", "cdate": 1577836800000, "mdate": 1682548763200, "content": {"title": "Jointly Leveraging Decorrelation and Sparsity for Improved Feedback Cancellation in Hearing Aids", "abstract": "We propose a new adaptive feedback cancellation (AFC) system in hearing aids (HAs) based on a well-posed optimization criterion that jointly considers both decorrelation of the signals and sparsity of the underlying channel. We show that the least squares criterion on subband errors regularized by a p-norm-like diversity measure can be used to simultaneously decorrelate the speech signals and exploit sparsity of the acoustic feedback path impulse response. Compared with traditional subband adaptive filters that are not appropriate for incorporating sparsity due to shorter sub-filters, our proposed framework is suitable for promoting sparse characteristics, as the update rule utilizing subband information actually operates in the fullband. Simulation results show that the normalized misalignment, added stable gain, and other objective metrics of the AFC are significantly improved by choosing a proper sparsity promoting factor and a suitable number of subbands. More importantly, the results indicate that the benefits of subband decomposition and sparsity promoting are complementary and additive for AFC in HAs."}}
{"id": "PgxuDHXiqYa", "cdate": 1546300800000, "mdate": 1682548763249, "content": {"title": "On Mitigating Acoustic Feedback in Hearing Aids with Frequency Warping by All-Pass Networks", "abstract": "Acoustic feedback control continues to be a challenging problem due to the emerging form factors in advanced hearing aids (HAs) and hearables. In this paper, we present a novel use of well-known all-pass filters in a network to perform frequency warping that we call \u201cfreping.\u201d Freping helps in breaking the Nyquist stability criterion and improves adaptive feedback cancellation (AFC). Based on informal subjective assessments, distortions due to freping are fairly benign. While common objective metrics like the perceptual evaluation of speech quality (PESQ) and the hearing-aid speech quality index (HASQI) may not adequately capture distortions due to freping and acoustic feedback artifacts from a perceptual perspective, they are still instructive in assessing the proposed method. We demonstrate quality improvements with freping for a basic AFC (PESQ: 2.56 to 3.52 and HASQI: 0.65 to 0.78) at a gain setting of 20; and an advanced AFC (PESQ: 2.75 to 3.17 and HASQI: 0.66 to 0.73) for a gain of 30. From our investigations, freping provides larger improvement for basic AFC, but still improves overall system performance for many AFC approaches."}}
{"id": "-PBKargQlbm", "cdate": 1546300800000, "mdate": 1682548763180, "content": {"title": "A Generalized Proportionate-Type Normalized Subband Adaptive Filter", "abstract": "We show that a new design criterion, i.e., the least squares on subband errors regularized by a weighted norm, can be used to generalize the proportionate-type normalized subband adaptive filtering (PtNSAF) framework. The new criterion directly penalizes subband errors and includes a sparsity penalty term which is minimized using the damped regularized Newton\u2019s method. The impact of the proposed generalized PtNSAF (GPtNSAF) is studied for the system identification problem via computer simulations. Specifically, we study the effects of using different numbers of subbands and various sparsity penalty terms for quasi-sparse, sparse, and dispersive systems. The results show that the benefit of increasing the number of subbands is larger than promoting sparsity of the estimated filter coefficients when the target system is quasi-sparse or dispersive. On the other hand, for sparse target systems, promoting sparsity becomes more important. More importantly, the two aspects provide complementary and additive benefits to the GPtNSAF for speeding up convergence."}}
{"id": "f5CAHZszsX", "cdate": 1483228800000, "mdate": 1682548763157, "content": {"title": "Brain-Computer Interfaces (BCI) Based 3D Computer-Aided Design (CAD): To Improve the Efficiency of 3D Modeling for New Users", "abstract": "In current HCI interfaces (keyboard, mice, gesture and pen), users must learn a variety of different ways to achieve the same commend in 3D modeling in different softwares (such as Maya, 3D studio Max and Rihno). For instance, to achieve the frequently used commands, such as rotating and zoom in/zoom out, users have to memorize the multiple hotkey (keyboard\u00a0+\u00a0mice) or use the graphic icon to perform these actions. Moreover, it would be even more inconvenient if you cross different 3D modeling softwares (Maya and 3D studio Max). The rotation function hotkeys are totally different even if there are designed in the same company (Autodesk). To increase the efficiency of 3D modeling, the CAD users usually need to memorize complicated hotkey combinations for different softwares. Hence, the challenge of this research is to generate a universal, intuitive and natural way to perform the \u201crotate\u201d and \u201czoom in/out\u201d command in 3D CAD modeling. Also, this alternative \u201crotate\u201d and \u201czoom in/out\u201d input can be widely applied to other 3D CAD softwares. We create a \u201cBCI embedded CAD\u201d prototype that connects EPOC+\u00a0to maxscript (3D studio Max). Through the system, the user is able to enhance the ability in 3D modeling through \u201cthinking the commends\u201d. In the future study, the BCI embedded CAD can be modified as a cross-platform 3D CAD manipulation that enables users to use imagination to control 3D modeling in different softwares (Maya, 3D studio Max, and Rihno) rather than traditional text-based commands or graphical icons."}}
