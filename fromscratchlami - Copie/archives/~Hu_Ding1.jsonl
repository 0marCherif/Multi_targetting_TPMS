{"id": "TqiuQJHJTV-", "cdate": 1672531200000, "mdate": 1681652400125, "content": {"title": "Randomized Greedy Algorithms and Composable Coreset for k-Center Clustering with Outliers", "abstract": ""}}
{"id": "JN5-nxFwP_n", "cdate": 1672531200000, "mdate": 1684348195051, "content": {"title": "Sublinear Time Algorithms for Several Geometric Optimization (With Outliers) Problems In Machine Learning", "abstract": "In this paper, we study several important geometric optimization problems arising in machine learning. First, we revisit the Minimum Enclosing Ball (MEB) problem in Euclidean space $\\mathbb{R}^d$. The problem has been extensively studied before, but real-world machine learning tasks often need to handle large-scale datasets so that we cannot even afford linear time algorithms. Motivated by the recent studies on {\\em beyond worst-case analysis}, we introduce the notion of stability for MEB, which is natural and easy to understand. Roughly speaking, an instance of MEB is stable, if the radius of the resulting ball cannot be significantly reduced by removing a small fraction of the input points. Under the stability assumption, we present two sampling algorithms for computing radius-approximate MEB with sample complexities independent of the number of input points $n$. In particular, the second algorithm has the sample complexity even independent of the dimensionality $d$. We also consider the general case without the stability assumption. We present a hybrid algorithm that can output either a radius-approximate MEB or a covering-approximate MEB. Our algorithm improves the running time and the number of passes for the previous sublinear MEB algorithms. Our method relies on two novel techniques, the Uniform-Adaptive Sampling method and Sandwich Lemma. Furthermore, we observe that these two techniques can be generalized to design sublinear time algorithms for a broader range of geometric optimization problems with outliers in high dimensions, including MEB with outliers, one-class and two-class linear SVMs with outliers, $k$-center clustering with outliers, and flat fitting with outliers. Our proposed algorithms also work fine for kernels."}}
{"id": "hlcordX4T6", "cdate": 1669852800000, "mdate": 1684348195043, "content": {"title": "Random Projection and Recovery for High Dimensional Optimization with Arbitrary Outliers", "abstract": "Robust optimization problems have attracted considerable attention in recent years. In this paper, we focus on two fundamental robust optimization problems: SVM with outliers and k-center clusterin..."}}
{"id": "kGQz0lt6Zu6", "cdate": 1652737674591, "mdate": null, "content": {"title": "Coresets for Wasserstein Distributionally Robust Optimization Problems", "abstract": "Wasserstein distributionally robust optimization (\\textsf{WDRO}) is a popular model to enhance the robustness of machine learning with ambiguous data. However, the complexity of \\textsf{WDRO} can be prohibitive in practice since solving its ``minimax'' formulation requires a great amount of computation. Recently, several fast \\textsf{WDRO} training algorithms for some specific machine learning tasks (e.g., logistic regression) have been developed. However, the research on designing efficient algorithms for general large-scale \\textsf{WDRO}s is still quite limited, to the best of our knowledge. \\textit{Coreset} is an important  tool for compressing large dataset, and thus it has been widely applied to  reduce the computational complexities for many optimization problems. In this paper, we introduce a unified framework to construct the $\\epsilon$-coreset for the general \\textsf{WDRO} problems. Though it is challenging to obtain a conventional coreset for \\textsf{WDRO}  due to the uncertainty issue of ambiguous data, we show that we can compute a ``dual coreset'' by using the strong duality property of \\textsf{WDRO}. Also, the error introduced by the dual coreset can be theoretically guaranteed for the original \\textsf{WDRO} objective. To construct the dual coreset, we propose a novel  grid sampling approach that is particularly suitable for the dual formulation of \\textsf{WDRO}. Finally, we implement our coreset approach and illustrate its effectiveness for several \\textsf{WDRO} problems in the experiments. See \\href{https://arxiv.org/abs/2210.04260}{arXiv:2210.04260} for the full version of this paper. The code is available at \\url{https://github.com/h305142/WDRO_coreset}."}}
{"id": "tPiE70y40cv", "cdate": 1652737673877, "mdate": null, "content": {"title": "Coresets for Relational Data and The Applications", "abstract": "A coreset is a small set that can approximately preserve the structure of the original input data set. Therefore we can run our algorithm on a coreset so as to reduce the total computational complexity. Conventional coreset techniques assume that the input data set is available to process explicitly. However, this assumption may not hold in real-world scenarios. In this paper, we consider the problem of coresets construction over relational data. Namely, the data is decoupled into several relational tables, and it could be very expensive to directly materialize the data matrix by joining the tables. We propose a novel approach called ``aggregation tree with pseudo-cube'' that can build a coreset from bottom to up. Moreover, our approach can neatly circumvent several troublesome issues of relational learning problems [Khamis et al., PODS 2019]. Under some mild assumptions, we show that our coreset approach can be applied for the machine learning tasks, such as clustering, logistic regression and SVM."}}
{"id": "Brx4JDUo5ec", "cdate": 1646077527206, "mdate": null, "content": {"title": "Sublinear Time Algorithms for Greedy Selection in High Dimensions", "abstract": "Greedy selection is a widely used idea for solving many machine learning problems. But greedy selection algorithms often have high complexities and thus may be prohibitive for large-scale data. In this paper, we consider two fundamental optimization problems in machine learning: -center clustering and convex hull approximation, where they both can be solved via greedy selection. We propose sublinear time algorithms for them through combining the strategies of randomization and greedy selection. Our results are similar in spirit to the linear time  stochastic greedy selection algorithms for submodular maximization [Mirzasoleiman et al., AAAI 2015, Hassidim and Singer, ICML 2017], but with several important differences. Our runtimes are independent of the number of input data items . In particular, our runtime for -center clustering significantly improves upon that of the uniform sampling approach [Huang et al, FOCS 2018], especially when the dimensionality is high. Moreover, our algorithms are particularly suitable for  the scenario that we cannot directly access the whole input data (due to the reasons like privacy preserving, data storage and transmission) and can only take a small sample via an oracle each time.  Our sublinear algorithms  yield the improvement on the efficiency for various applications, such as data selection and compression, active learning, topic modeling, {\\em etc}."}}
{"id": "r2btUV0Xcf", "cdate": 1640995200000, "mdate": 1681652400134, "content": {"title": "Coresets for Relational Data and The Applications", "abstract": ""}}
{"id": "pgDbw-THbQ5", "cdate": 1640995200000, "mdate": 1684348195245, "content": {"title": "Sublinear time algorithms for greedy selection in high dimensions", "abstract": "Greedy selection is a widely used idea for solving many machine learning problems. But greedy selection algorithms often have high complexities and thus may be prohibitive for large-scale data. In ..."}}
{"id": "ho-uYw0rOg", "cdate": 1640995200000, "mdate": 1684348195259, "content": {"title": "Coresets for Wasserstein Distributionally Robust Optimization Problems", "abstract": "Wasserstein distributionally robust optimization (\\textsf{WDRO}) is a popular model to enhance the robustness of machine learning with ambiguous data. However, the complexity of \\textsf{WDRO} can be prohibitive in practice since solving its ``minimax'' formulation requires a great amount of computation. Recently, several fast \\textsf{WDRO} training algorithms for some specific machine learning tasks (e.g., logistic regression) have been developed. However, the research on designing efficient algorithms for general large-scale \\textsf{WDRO}s is still quite limited, to the best of our knowledge. \\textit{Coreset} is an important tool for compressing large dataset, and thus it has been widely applied to reduce the computational complexities for many optimization problems. In this paper, we introduce a unified framework to construct the $\\epsilon$-coreset for the general \\textsf{WDRO} problems. Though it is challenging to obtain a conventional coreset for \\textsf{WDRO} due to the uncertainty issue of ambiguous data, we show that we can compute a ``dual coreset'' by using the strong duality property of \\textsf{WDRO}. Also, the error introduced by the dual coreset can be theoretically guaranteed for the original \\textsf{WDRO} objective. To construct the dual coreset, we propose a novel grid sampling approach that is particularly suitable for the dual formulation of \\textsf{WDRO}. Finally, we implement our coreset approach and illustrate its effectiveness for several \\textsf{WDRO} problems in the experiments. See \\href{https://arxiv.org/abs/2210.04260}{arXiv:2210.04260} for the full version of this paper. The code is available at \\url{https://github.com/h305142/WDRO_coreset}."}}
{"id": "XIZHSnTJvOs", "cdate": 1640995200000, "mdate": 1684348195055, "content": {"title": "Large-Scale Detection of the Tableland Areas and Erosion-Vulnerable Hotspots on the Chinese Loess Plateau", "abstract": "Tableland areas, featured by flat and broad landforms, provide precious land resources for agricultural production and human settlements over the Chinese Loess Plateau (CLP). However, severe gully erosion triggered by extreme rainfall and intense human activities makes tableland areas shrink continuously. Preventing the loss of tableland areas is of real urgency, in which generating its accurate distribution map is the critical prerequisite. However, a plateau-scale inventory of tableland areas is still lacking across the Loess Plateau. This study proposed a large-scale approach for tableland area mapping. The Sentinel-2 imagery was used for the initial delineation based on object-based image analysis and random forest model. Subsequently, the drainage networks extracted from AW3D30 DEM were applied for correcting commission and omission errors based on the law that rivers and streams rarely appear on the tableland areas. The automatic mapping approach performs well, with the overall accuracies over 90% in all four investigated subregions. After the strict quality control by manual inspection, a high-quality inventory of tableland areas at 10 m resolution was generated, demonstrating that the tableland areas occupied 9507.31 km2 across the CLP. Cultivated land is the dominant land-use type on the tableland areas, yet multi-temporal observations indicated that it has decreased by approximately 500 km2 during the year of 2000 to 2020. In contrast, forest and artificial surfaces increased by 57.53% and 73.10%, respectively. Additionally, we detected 455 vulnerable hotspots of the tableland with a width of less than 300 m. Particular attention should be paid to these areas to prevent the potential split of a large tableland, accompanied by damage on roads and buildings. This plateau-scale tableland inventory and erosion-vulnerable hotspots are expected to support the environmental protection policymaking for sustainable development in the CLP region severely threatened by soil erosion and land degradation."}}
