{"id": "Tz1lknIPVfp", "cdate": 1652737747751, "mdate": null, "content": {"title": "Learning Dynamical Systems via Koopman Operator Regression in Reproducing Kernel Hilbert Spaces", "abstract": "We study a class of dynamical systems modelled as stationary Markov chains that admit an invariant distribution via the corresponding transfer or Koopman operator. While data-driven algorithms to reconstruct such operators are well known, their relationship with statistical learning is largely unexplored. We formalize a framework to learn the Koopman operator from finite data trajectories of the dynamical system. We consider the restriction of this operator to a reproducing kernel Hilbert space and introduce a notion of risk, from which different estimators naturally arise. We link the risk with the estimation of the spectral decomposition of the Koopman operator. These observations motivate a reduced-rank operator regression (RRR) estimator. We derive learning bounds for the proposed estimator, holding both in i.i.d and non i.i.d. settings, the latter in terms of mixing coefficients. Our results suggest RRR might be beneficial over  other  widely used estimators as confirmed in numerical experiments  both for  forecasting and mode decomposition."}}
{"id": "Rafo6rBdep", "cdate": 1640995200000, "mdate": 1680081871577, "content": {"title": "Learning Dynamical Systems via Koopman Operator Regression in Reproducing Kernel Hilbert Spaces", "abstract": ""}}
{"id": "WJPAqX5M-2", "cdate": 1621629894950, "mdate": null, "content": {"title": "Concentration inequalities under sub-Gaussian and sub-exponential conditions", "abstract": "We prove analogues of the popular bounded difference inequality (also called McDiarmid's inequality) for functions of independent random variables under sub-gaussian and sub-exponential conditions. Applied to vector-valued concentration and the method of Rademacher complexities these inequalities allow an easy extension of uniform convergence results for PCA and linear regression to the case potentially unbounded input- and output variables."}}
{"id": "YoSQzU6HLx", "cdate": 1609459200000, "mdate": 1682616974095, "content": {"title": "Concentration inequalities under sub-Gaussian and sub-exponential conditions", "abstract": "We prove analogues of the popular bounded difference inequality (also called McDiarmid's inequality) for functions of independent random variables under sub-gaussian and sub-exponential conditions. Applied to vector-valued concentration and the method of Rademacher complexities these inequalities allow an easy extension of uniform convergence results for PCA and linear regression to the case potentially unbounded input- and output variables."}}
{"id": "SkURbkf0R2N", "cdate": 1609459200000, "mdate": 1682337678571, "content": {"title": "Robust Unsupervised Learning via L-statistic Minimization", "abstract": "Designing learning algorithms that are resistant to perturbations of the underlying data distribution is a problem of wide practical and theoretical importance. We present a general approach to thi..."}}
{"id": "AQasiXMS1g2", "cdate": 1609459200000, "mdate": null, "content": {"title": "Some Hoeffding- and Bernstein-type Concentration Inequalities", "abstract": "We prove concentration inequalities for functions of independent random variables {under} sub-gaussian and sub-exponential conditions. The utility of the inequalities is demonstrated by an extension of the now classical method of Rademacher complexities to Lipschitz function classes and unbounded sub-exponential distribution."}}
{"id": "vZxVNslbtVx", "cdate": 1577836800000, "mdate": 1682617143539, "content": {"title": "Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning", "abstract": "Developing learning methods which do not discriminate subgroups in the population is a central goal of algorithmic fairness. One way to reach this goal is by modifying the data representation in order to meet certain fairness constraints. In this work we measure fairness according to demographic parity. This requires the probability of the possible model decisions to be independent of the sensitive information. We argue that the goal of imposing demographic parity can be substantially facilitated within a multitask learning setting. We present a method for learning a shared fair representation across multiple tasks, by means of different new constraints based on MMD and Sinkhorn Divergences. We derive learning bounds establishing that the learned representation transfers well to novel tasks. We present experiments on three real world datasets, showing that the proposed method outperforms state-of-the-art approaches by a significant margin."}}
{"id": "a7UcbCbeS68", "cdate": 1577836800000, "mdate": null, "content": {"title": "Estimating weighted areas under the ROC curve", "abstract": "Exponential bounds on the estimation error are given for the plug-in estimator of weighted areas under the ROC curve. The bounds hold for single score functions and uniformly over classes of functions, whose complexity can be controlled by Gaussian or Rademacher averages. The results justify learning algorithms which select score functions to maximize the empirical partial area under the curve (pAUC). They also illustrate the use of some recent advances in the theory of nonlinear empirical processes."}}
{"id": "WaupvXkMAdi", "cdate": 1577836800000, "mdate": 1682616974113, "content": {"title": "Learning Fair and Transferable Representations with Theoretical Guarantees", "abstract": "Developing learning methods which do not discriminate subgroups in the population is the central goal of algorithmic fairness. One way to reach this goal is by modifying the data representation in order to satisfy prescribed fairness constraints. This allows to reuse the same representation in other context (tasks) without discriminate subgroups. In this work we measure fairness according to demographic parity, requiring the probability of the possible model decisions to be independent of the sensitive information. We argue that the goal of imposing demographic parity can be substantially facilitated within a multi-task learning setting. We leverage task similarities by encouraging a shared fair representation across the tasks via low rank matrix factorization. We derive learning bounds establishing that the learned representation transfers well to novel tasks both in terms of prediction performance and fairness metrics. We present experiments on three real world datasets, showing that the proposed method outperforms state-of-the-art approaches by a significant margin."}}
{"id": "41VXdJ2EkUK", "cdate": 1577836800000, "mdate": 1682337678601, "content": {"title": "A Perturbation Resilient Framework for Unsupervised Learning", "abstract": "Designing learning algorithms that are resistant to perturbations of the underlying data distribution is a problem of wide practical and theoretical importance. We present a general approach to this problem focusing on unsupervised learning. The key assumption is that the perturbing distribution is characterized by larger losses relative to a given class of admissible models. This is exploited by a general descent algorithm which minimizes an $L$-statistic criterion over the model class, weighting small losses more. Our analysis characterizes the robustness of the method in terms of bounds on the reconstruction error relative to the underlying unperturbed distribution. As a byproduct, we prove uniform convergence bounds with respect to the proposed criterion for several popular models in unsupervised learning, a result which may be of independent interest.Numerical experiments with kmeans clustering and principal subspace analysis demonstrate the effectiveness of our approach."}}
