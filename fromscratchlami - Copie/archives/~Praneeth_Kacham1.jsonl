{"id": "74A-FDAyiL", "cdate": 1663850327953, "mdate": null, "content": {"title": "Subquadratic Algorithms for Kernel Matrices via Kernel Density Estimation", "abstract": "Kernel matrices, as well as weighted graphs represented by them, are ubiquitous objects in machine learning, statistics and other related fields. The main drawback of using kernel methods (learning and inference using kernel matrices) is efficiency -- given $n$ input points, most kernel-based algorithms need to materialize the full $n \\times n$ kernel matrix before performing any subsequent computation, thus incurring $\\Omega(n^2)$ runtime. Breaking this quadratic barrier for various problems has therefore, been a subject of extensive research efforts. \n\nWe break the quadratic barrier and obtain \\emph{subquadratic} time  algorithms for several fundamental linear-algebraic and graph processing primitives, including approximating the top eigenvalue and eigenvector, spectral sparsification, solving linear systems, local clustering, low-rank approximation, arboricity estimation and counting weighted triangles. We build on the recently developed Kernel Density Estimation framework, which (after preprocessing in time subquadratic in $n$) can return estimates of row/column sums of the kernel matrix. In particular, we develop efficient reductions from \\emph{weighted vertex} and \\emph{weighted edge sampling} on kernel graphs, \\emph{simulating random walks} on kernel graphs, and \\emph{importance sampling} on matrices to Kernel Density Estimation and show that we can generate samples from these distributions in \\emph{sublinear} (in the support of the distribution) time. Our reductions are the central ingredient in each of our applications and we believe they may be of independent interest. We empirically demonstrate the efficacy of our algorithms on low-rank approximation (LRA) and spectral sparsification, where we observe a $\\textbf{9x}$ decrease in the number of kernel evaluations over baselines for LRA and a $\\textbf{41x}$ reduction in the graph size for spectral sparsification."}}
{"id": "jXEWono0VOB", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust k-means++", "abstract": "A good seeding or initialization of cluster centers for the $k$-means method is important from both theoretical and practical standpoints. The $k$-means objective is inherently non-robust and sensi..."}}
{"id": "Q_o1Q3flcj_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reduced-Rank Regression with Operator Norm Error", "abstract": "A common data analysis task is the reduced-rank regression problem: $$\\min_{\\textrm{rank-}k \\ X} \\|AX-B\\|,$$ where $A \\in \\mathbb{R}^{n \\times c}$ and $B \\in \\mathbb{R}^{n \\times d}$ are given large matrices and $\\|\\cdot\\|$ is some norm. Here the unknown matrix $X \\in \\mathbb{R}^{c \\times d}$ is constrained to be of rank $k$ as it results in a significant parameter reduction of the solution when $c$ and $d$ are large. In the case of Frobenius norm error, there is a standard closed form solution to this problem and a fast algorithm to find a $(1+\\varepsilon)$-approximate solution. However, for the important case of operator norm error, no closed form solution is known and the fastest known algorithms take singular value decomposition time. We give the first randomized algorithms for this problem running in time $$(nnz{(A)} + nnz{(B)} + c^2) \\cdot k/\\varepsilon^{1.5} + (n+d)k^2/\\epsilon + c^{\\omega},$$ up to a polylogarithmic factor involving condition numbers, matrix dimensions, and dependence on $1/\\varepsilon$. Here $nnz{(M)}$ denotes the number of non-zero entries of a matrix $M$, and $\\omega$ is the exponent of matrix multiplication. As both (1) spectral low rank approximation ($A = B$) and (2) linear system solving ($n = c$ and $d = 1$) are special cases, our time cannot be improved by more than a $1/\\varepsilon$ factor (up to polylogarithmic factors) without a major breakthrough in linear algebra. Interestingly, known techniques for low rank approximation, such as alternating minimization or sketch-and-solve, provably fail for this problem. Instead, our algorithm uses an existential characterization of a solution, together with Krylov methods, low degree polynomial approximation, and sketching-based preconditioning."}}
{"id": "B1-6e4Zr5t9", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimal Deterministic Coresets for Ridge Regression", "abstract": "We consider the ridge regression problem, for which we are given an nxd matrix A of examples and a corresponding nxd\u2019 matrix B of labels, as well as a ridge parameter $\\lambda \\geq 0$, and would li..."}}
{"id": "JxK3BbHBhtn", "cdate": 1546300800000, "mdate": null, "content": {"title": "Strong Coresets for Subspace Approximation and k-Median in Nearly Linear Time", "abstract": "We give a dimensionality reduction procedure to approximate the sum of distances of a given set of $n$ points in $R^d$ to any \"shape\" that lies in a $k$-dimensional subspace. Here, by \"shape\" we mean any set of points in $R^d$. Our algorithm takes an input in the form of an $n \\times d$ matrix $A$, where each row of $A$ denotes a data point, and outputs a subspace $P$ of dimension $O(k^{3}/\\epsilon^6)$ such that the projections of each of the $n$ points onto the subspace $P$ and the distances of each of the points to the subspace $P$ are sufficient to obtain an $\\epsilon$-approximation to the sum of distances to any arbitrary shape that lies in a $k$-dimensional subspace of $R^d$. These include important problems such as $k$-median, $k$-subspace approximation, and $(j,l)$ subspace clustering with $j \\cdot l \\leq k$. Dimensionality reduction reduces the data storage requirement to $(n+d)k^{3}/\\epsilon^6$ from nnz$(A)$. Here nnz$(A)$ could potentially be as large as $nd$. Our algorithm runs in time nnz$(A)/\\epsilon^2 + (n+d)$poly$(k/\\epsilon)$, up to logarithmic factors. For dense matrices, where nnz$(A) \\approx nd$, we give a faster algorithm, that runs in time $nd + (n+d)$poly$(k/\\epsilon)$ up to logarithmic factors. Our dimensionality reduction algorithm can also be used to obtain poly$(k/\\epsilon)$ size coresets for $k$-median and $(k,1)$-subspace approximation problems in polynomial time."}}
