{"id": "Dd18wEKZOz", "cdate": 1695621677482, "mdate": 1695621677482, "content": {"title": "Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint", "abstract": "Fair Principal Component Analysis (PCA) is a problem setting where we aim to perform PCA while making the resulting representation fair in that the projected distributions, conditional on the sensitive attributes, match one another. However, existing approaches to fair PCA have two main problems: theoretically, there has been no statistical foundation of fair PCA in terms of learnability; practically, limited memory prevents us from using existing approaches, as they explicitly rely on full access to the entire data. On the theoretical side, we rigorously formulate fair PCA using a new notion called probably approximately fair and optimal (PAFO) learnability. On the practical side, motivated by recent advances in streaming algorithms for addressing memory limitation, we propose a new setting called fair streaming PCA along with a memory-efficient algorithm, fair noisy power method (FNPM). We then provide its statistical guarantee in terms of PAFO-learnability, which is the first of its kind in fair PCA literature. We verify our algorithm in the CelebA dataset without any pre-processing; while the existing approaches are inapplicable due to memory limitations, by turning it into a streaming setting, we show that our algorithm performs fair PCA efficiently and effectively."}}
{"id": "yfd3Gi04vnF", "cdate": 1672531200000, "mdate": 1681716287225, "content": {"title": "Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond", "abstract": "We study convergence lower bounds of without-replacement stochastic gradient descent (SGD) for solving smooth (strongly-)convex finite-sum minimization problems. Unlike most existing results focusing on final iterate lower bounds in terms of the number of components $n$ and the number of epochs $K$, we seek bounds for arbitrary weighted average iterates that are tight in all factors including the condition number $\\kappa$. For SGD with Random Reshuffling, we present lower bounds that have tighter $\\kappa$ dependencies than existing bounds. Our results are the first to perfectly close the gap between lower and upper bounds for weighted average iterates in both strongly-convex and convex cases. We also prove weighted average iterate lower bounds for arbitrary permutation-based SGD, which apply to all variants that carefully choose the best permutation. Our bounds improve the existing bounds in factors of $n$ and $\\kappa$ and thereby match the upper bounds shown for a recently proposed algorithm called GraB."}}
{"id": "ezp7hGEss5", "cdate": 1672531200000, "mdate": 1681716287336, "content": {"title": "On the Training Instability of Shuffling SGD with Batch Normalization", "abstract": "We uncover how SGD interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle (SS) and Random Reshuffle (RR) -- two widely used variants of SGD -- interact surprisingly differently in the presence of batch normalization: RR leads to much more stable evolution of training loss than SS. As a concrete example, for regression using a linear network with batch normalization, we prove that SS and RR converge to distinct global optima that are \"distorted\" away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for SS and RR can, and cannot occur. We present explicit constructions to show how SS leads to distorted optima in regression and divergence for classification, whereas RR avoids both distortion and divergence. We validate our results by confirming them empirically in realistic settings, and conclude that the separation between SS and RR used with batch normalization is relevant in practice."}}
{"id": "6xXtM8bFFJ", "cdate": 1663850150557, "mdate": null, "content": {"title": "SGDA with shuffling: faster convergence for nonconvex-P\u0141 minimax optimization", "abstract": "Stochastic gradient descent-ascent (SGDA) is one of the main workhorses for solving finite-sum minimax optimization problems. Most practical implementations of SGDA randomly reshuffle components and sequentially use them (i.e., without-replacement sampling); however, there are few theoretical results on this approach for minimax algorithms, especially outside the easier-to-analyze (strongly-)monotone setups. To narrow this gap, we study the convergence bounds of SGDA with random reshuffling (SGDA-RR) for smooth nonconvex-nonconcave objectives with Polyak-{\\L}ojasiewicz (P{\\L}) geometry. We analyze both simultaneous and alternating SGDA-RR for nonconvex-P{\\L} and primal-P{\\L}-P{\\L} objectives, and obtain convergence rates faster than with-replacement SGDA. Our rates extend to mini-batch SGDA-RR, recovering known rates for full-batch gradient descent-ascent (GDA). Lastly, we present a comprehensive lower bound for GDA with an arbitrary step-size ratio, which matches the full-batch upper bound for the primal-P{\\L}-P{\\L} case."}}
{"id": "u6kRArugFrx", "cdate": 1640995200000, "mdate": 1681716287290, "content": {"title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "abstract": "In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-\u0141ojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings."}}
{"id": "LdlwbBP2mlq", "cdate": 1632875740859, "mdate": null, "content": {"title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "abstract": "In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-\u0141ojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings."}}
{"id": "smSJOkHwXOv", "cdate": 1609459200000, "mdate": 1681716287351, "content": {"title": "Provable Memorization via Deep Neural Networks using Sub-linear Parameters", "abstract": "It is known that $O(N)$ parameters are sufficient for neural networks to memorize arbitrary $N$ input-label pairs. By exploiting depth, we show that $O(N^{2/3})$ parameters suffice to memorize $N$ ..."}}
{"id": "rse7QpWEr6u", "cdate": 1609459200000, "mdate": 1681716287267, "content": {"title": "Can Single-Shuffle SGD be Better than Reshuffling SGD and GD?", "abstract": "We propose matrix norm inequalities that extend the Recht-R\\'e (2012) conjecture on a noncommutative AM-GM inequality by supplementing it with another inequality that accounts for single-shuffle, which is a widely used without-replacement sampling scheme that shuffles only once in the beginning and is overlooked in the Recht-R\\'e conjecture. Instead of general positive semidefinite matrices, we restrict our attention to positive definite matrices with small enough condition numbers, which are more relevant to matrices that arise in the analysis of SGD. For such matrices, we conjecture that the means of matrix products corresponding to with- and without-replacement variants of SGD satisfy a series of spectral norm inequalities that can be summarized as: \"single-shuffle SGD converges faster than random-reshuffle SGD, which is in turn faster than with-replacement SGD.\" We present theorems that support our conjecture by proving several special cases."}}
{"id": "rVkeShAgWs", "cdate": 1609459200000, "mdate": 1681716287382, "content": {"title": "Minimum Width for Universal Approximation", "abstract": "The universal approximation property of width-bounded networks has been studied as a dual of classical universal approximation results on depth-bounded networks. However, the critical width enabling the universal approximation has not been exactly characterized in terms of the input dimension $d_x$ and the output dimension $d_y$. In this work, we provide the first definitive result in this direction for networks using the ReLU activation functions: The minimum width required for the universal approximation of the $L^p$ functions is exactly $\\max\\{d_x+1,d_y\\}$. We also prove that the same conclusion does not hold for the uniform approximation with ReLU, but does hold with an additional threshold activation function. Our proof technique can be also used to derive a tighter upper bound on the minimum width required for the universal approximation using networks with general activation functions."}}
{"id": "jodMp7HPwL3", "cdate": 1609459200000, "mdate": 1681716287747, "content": {"title": "Open Problem: Can Single-Shuffle SGD be Better than Reshuffling SGD and GD?", "abstract": "We propose matrix norm inequalities that extend the Recht and R\u00e9 (2012) conjecture on a noncommutative AM-GM inequality, by supplementing it with another inequality that accounts for single-shuffle..."}}
