{"id": "VJaoe7Rp9tZ", "cdate": 1676827098860, "mdate": null, "content": {"title": "Neural Tangent Kernel at Initialization: Linear Width Suffices", "abstract": "In this paper we study the problem of lower bounding the minimum eigenvalue of the neural tangent kernel (NTK) at initialization, an important quantity for the theoretical analysis of training in neural networks. We consider feedforward neural networks with smooth activation functions. Without any distributional assumptions on the input, we present a novel result: we show that for suitable initialization variance, $\\widetilde{\\Omega}(n)$ width, where $n$ is the number of training samples, suffices to ensure that the NTK at initialization is positive definite, improving prior results for smooth activations under our setting. Prior to our work, the sufficiency of linear width has only been shown either for networks with ReLU activation functions, and sublinear width has been shown for smooth networks but with additional conditions on the distribution of the data. The technical challenge in the analysis stems from the layerwise inhomogeneity of smooth activation functions and we handle the challenge using generalized Hermite series expansion of such activations."}}
{"id": "Njs8LKhwV6h", "cdate": 1676827097495, "mdate": null, "content": {"title": "Finite-sample Guarantees for Nash Q-learning with Linear Function Approximation", "abstract": "Nash Q-learning may be considered one of the first and most known algorithms in multi-agent reinforcement learning (MARL) for learning policies that constitute a Nash equilibrium of an underlying general-sum Markov game. Its original proof was in the asymptotic domain and for the tabular case. Recently, finite-sample guarantees have been provided using more modern RL techniques for the tabular case. Our work analyzes Nash Q-learning using linear function approximation \u2013 a representation regime introduced when the state space is large or continuous \u2013 and provides finite-sample guarantees that indicate its sample efficiency. We find that the obtained performance nearly matches an existing efficient result for single-agent RL under the same representation and has a polynomial gap when compared to the best-known result for the tabular case."}}
{"id": "8-Dq07jcFo", "cdate": 1672531200000, "mdate": 1682626672045, "content": {"title": "Finite-sample Guarantees for Nash Q-learning with Linear Function Approximation", "abstract": "Nash Q-learning may be considered one of the first and most known algorithms in multi-agent reinforcement learning (MARL) for learning policies that constitute a Nash equilibrium of an underlying general-sum Markov game. Its original proof provided asymptotic guarantees and was for the tabular case. Recently, finite-sample guarantees have been provided using more modern RL techniques for the tabular case. Our work analyzes Nash Q-learning using linear function approximation -- a representation regime introduced when the state space is large or continuous -- and provides finite-sample guarantees that indicate its sample efficiency. We find that the obtained performance nearly matches an existing efficient result for single-agent RL under the same representation and has a polynomial gap when compared to the best-known result for the tabular case."}}
{"id": "Krk0Gnft2Zc", "cdate": 1663850524359, "mdate": null, "content": {"title": "Discrete State-Action Abstraction via the Successor Representation", "abstract": "While the difficulty of reinforcement learning problems is typically related to the complexity of their state spaces, Abstraction proposes that solutions often lie in simpler underlying latent spaces. Prior works have focused on learning either a continuous or dense abstraction, or require a human to provide one. Information-dense representations capture features irrelevant for solving tasks, and continuous spaces can struggle to represent discrete objects. In this work we automatically learn a sparse discrete abstraction of the underlying environment. We do so using a simple end-to-end trainable model based on the successor representation and max-entropy regularization. We describe an algorithm to apply our model, named Discrete State-Action Abstraction (DSAA), which computes an action abstraction in the form of temporally extended actions, i.e., Options, to transition between discrete abstract states. Empirically, we demonstrate the effects of different exploration schemes on our resulting abstraction, and show that it is efficient for solving downstream tasks."}}
{"id": "PINRbk7h01", "cdate": 1663850178020, "mdate": null, "content": {"title": "Restricted Strong Convexity of Deep Learning Models with Smooth Activations", "abstract": "We consider the problem of optimization of deep learning models with smooth activation functions. While there exist influential results on the problem from the ``near initialization'' perspective, we shed considerable new light on the problem. In particular, we make two key technical contributions for such models with $L$ layers, $m$ width, and $\\sigma_0^2$ initialization variance. First, for suitable $\\sigma_0^2$, we establish a $O(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ upper bound on the spectral norm of the Hessian of such models, considerably sharpening prior results. Second, we introduce a new analysis of optimization based on Restricted Strong Convexity (RSC) which holds as long as the squared norm of the average gradient of predictors is $\\Omega(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ for the square loss. We also present results for more general losses. The RSC based analysis does not need the ``near initialization\" perspective and guarantees geometric convergence for gradient descent (GD). To the best of our knowledge, ours is the first result on establishing geometric convergence of GD based on RSC for deep learning models, thus becoming an alternative sufficient condition for convergence that does not depend on the widely-used Neural Tangent Kernel (NTK). We share preliminary experimental results supporting our theoretical advances."}}
{"id": "wjBQwrEj_t", "cdate": 1640995200000, "mdate": 1682626672048, "content": {"title": "Weak and Semi-Contraction for Network Systems and Diffusively Coupled Oscillators", "abstract": "We develop two generalizations of contraction theory, namely, semi-contraction and weak-contraction theory. First, using the notion of seminorm, we propose a geometric framework for semi-contraction theory. We introduce matrix semimeasures and characterize their properties. We show that the spectral abscissa of a matrix is the infimum over weighted semimeasures. For dynamical systems, we use the semimeasure of their Jacobian to characterize the contractivity properties of their trajectories. Second, for weakly contracting systems, we prove a dichotomy for the asymptotic behavior of their trajectories and novel sufficient conditions for convergence to an equilibrium. Third, we show that every trajectory of a doubly contracting system, i.e., a system that is both weakly and semi-contracting, converges to an equilibrium point. Finally, we apply our results to various important network systems, including affine averaging and affine flow systems, continuous-time distributed primal-dual algorithms, and networks of diffusively coupled dynamical systems. For diffusively coupled systems, the semi-contraction theory leads to a sufficient condition for synchronization that is sharper, in general, than previously known tests."}}
{"id": "oSK7pksrz", "cdate": 1640995200000, "mdate": 1682626672049, "content": {"title": "Restricted Strong Convexity of Deep Learning Models with Smooth Activations", "abstract": "We consider the problem of optimization of deep learning models with smooth activation functions. While there exist influential results on the problem from the ``near initialization'' perspective, we shed considerable new light on the problem. In particular, we make two key technical contributions for such models with $L$ layers, $m$ width, and $\\sigma_0^2$ initialization variance. First, for suitable $\\sigma_0^2$, we establish a $O(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ upper bound on the spectral norm of the Hessian of such models, considerably sharpening prior results. Second, we introduce a new analysis of optimization based on Restricted Strong Convexity (RSC) which holds as long as the squared norm of the average gradient of predictors is $\\Omega(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ for the square loss. We also present results for more general losses. The RSC based analysis does not need the ``near initialization\" perspective and guarantees geometric convergence for gradient descent (GD). To the best of our knowledge, ours is the first result on establishing geometric convergence of GD based on RSC for deep learning models, thus becoming an alternative sufficient condition for convergence that does not depend on the widely-used Neural Tangent Kernel (NTK). We share preliminary experimental results supporting our theoretical advances."}}
{"id": "e8GYgvl3Ai", "cdate": 1640995200000, "mdate": 1682626672045, "content": {"title": "Contraction Theory for Dynamical Systems on Hilbert Spaces", "abstract": "Contraction theory for dynamical systems on Euclidean spaces is well-established. For contractive (resp. semicontractive) systems, the distance (resp. semidistance) between any two trajectories decreases exponentially fast. For partially contractive systems, each trajectory converges exponentially fast to an invariant subspace. In this article, we develop contraction theory on Hilbert spaces. First, we provide a novel integral condition for contractivity, and for time-invariant systems, we establish the existence of a unique globally exponentially stable equilibrium. Second, we introduce the notions of partial and semicontraction and we provide various sufficient conditions for time-varying and time-invariant systems. Finally, we apply the theory on a classic reaction-diffusion system."}}
{"id": "VnUMrLFzDE", "cdate": 1640995200000, "mdate": 1682626672045, "content": {"title": "A Contraction Analysis of Primal-Dual Dynamics in Distributed and Time-Varying Implementations", "abstract": "In this article, we provide an overarching analysis of primal-dual dynamics associated with linear equality-constrained optimization problems using contraction analysis. For the well-known standard version of the problem, we establish convergence under convexity and the contracting rate under strong convexity. Then, for a canonical distributed optimization problem, we use partial contractivity to establish global exponential convergence of its primal-dual dynamics. As an application, we propose a new distributed solver for the least-squares problem with the same convergence guarantees. Finally, for time-varying versions of both centralized and distributed primal-dual dynamics, we exploit their contractive nature to establish bounds on their tracking error. To support our analyses, we introduce novel results on contraction theory."}}
{"id": "VKYgXwOKua", "cdate": 1640995200000, "mdate": 1682354017822, "content": {"title": "One Policy is Enough: Parallel Exploration with a Single Policy is Minimax Optimal for Reward-Free Reinforcement Learning", "abstract": "Although parallelism has been extensively used in reinforcement learning (RL), the quantitative effects of parallel exploration are not well understood theoretically. We study the benefits of simple parallel exploration for reward-free RL in linear Markov decision processes (MDPs) and two-player zero-sum Markov games (MGs). In contrast to the existing literature, which focuses on approaches that encourage agents to explore a diverse set of policies, we show that using a single policy to guide exploration across all agents is sufficient to obtain an almost-linear speedup in all cases compared to their fully sequential counterpart. Furthermore, we demonstrate that this simple procedure is near-minimax optimal in the reward-free setting for linear MDPs. From a practical perspective, our paper shows that a single policy is sufficient and provably near-optimal for incorporating parallelism during the exploration phase."}}
