{"id": "wfhPqiSD7TN", "cdate": 1672531200000, "mdate": 1681691649569, "content": {"title": "Integrated Decision and Control: Toward Interpretable and Computationally Efficient Driving Intelligence", "abstract": "Decision and control are core functionalities of high-level automated vehicles. Current mainstream methods, such as functional decomposition and end-to-end reinforcement learning (RL), suffer high time complexity or poor interpretability and adaptability on real-world autonomous driving tasks. In this article, we present an interpretable and computationally efficient framework called integrated decision and control (IDC) for automated vehicles, which decomposes the driving task into static path planning and dynamic optimal tracking that are structured hierarchically. First, the static path planning generates several candidate paths only considering static traffic elements. Then, the dynamic optimal tracking is designed to track the optimal path while considering the dynamic obstacles. To that end, we formulate a constrained optimal control problem (OCP) for each candidate path, optimize them separately, and follow the one with the best tracking performance. To unload the heavy online computation, we propose a model-based RL algorithm that can be served as an approximate-constrained OCP solver. Specifically, the OCPs for all paths are considered together to construct a single complete RL problem and then solved offline in the form of value and policy networks for real-time online path selecting and tracking, respectively. We verify our framework in both simulations and the real world. Results show that compared with baseline methods, IDC has an order of magnitude higher online computing efficiency, as well as better driving performance, including traffic efficiency and safety. In addition, it yields great interpretability and adaptability among different driving scenarios and tasks."}}
{"id": "vgyHHfw80a", "cdate": 1640995200000, "mdate": 1681691649604, "content": {"title": "Joint Synthesis of Safety Certificate and Safe Control Policy Using Constrained Reinforcement Learning", "abstract": "Safety is the major consideration in controlling complex dynamical systems using reinforcement learning (RL), where the safety certificates can provide provable safety guarantees. A valid safety ce..."}}
{"id": "OGeouX0eUl9", "cdate": 1640995200000, "mdate": 1681691649705, "content": {"title": "Reachability Constrained Reinforcement Learning", "abstract": "Constrained reinforcement learning (CRL) has gained significant interest recently, since safety constraints satisfaction is critical for real-world problems. However, existing CRL methods constrain..."}}
{"id": "uk_7MhLWIzd", "cdate": 1609459200000, "mdate": 1681691649644, "content": {"title": "Feasibility Enhancement of Constrained Receding Horizon Control Using Generalized Control Barrier Function", "abstract": "Receding horizon control (RHC) is a popular procedure to deal with optimal control problems. Due to the existence of state constraints, optimization-based RHC often suffers the notorious issue of infeasibility, which strongly shrinks the region of controllable state. This paper proposes a generalized control barrier function (CBF) to enlarge the feasible region of constrained RHC with only a one-step constraint on the prediction horizon. This design can reduce the constrained steps by penalizing the tendency to move towards the constraint boundary. Additionally, generalized CBF is able to handle high-order equality or inequality constraints through extending the constrained step to nonadjacent nodes. We apply this technique on an automated vehicle control task. The results show that compared to multi-step pointwise constraints, generalized CBF can effectively avoid the infeasibility issue in a larger partition of the state space, and the computing efficiency is also improved by 14%-23%."}}
{"id": "u_WagXKjt8", "cdate": 1609459200000, "mdate": 1681691649594, "content": {"title": "Model-based Constrained Reinforcement Learning using Generalized Control Barrier Function", "abstract": "Model information can be used to predict future trajectories, so it has huge potential to avoid dangerous regions when applying reinforcement learning (RL) on real-world tasks, like autonomous driving. However, existing studies mostly use model-free constrained RL, which causes inevitable constraint violations. This paper proposes a model-based feasibility enhancement technique of constrained RL, which enhances the feasibility of policy using generalized control barrier function (GCBF) defined on the distance to constraint boundary. By using the model information, the policy can be optimized safely without violating actual safety constraints, and the sample efficiency is increased. The infeasibility in solving the constrained policy gradient is handled by an adaptive coefficient mechanism. We evaluate the proposed method in both simulations and real vehicle experiments in a complex autonomous driving collision avoidance task. The proposed method achieves up to four times fewer constraint violations and converges 3.36 times faster than baseline constrained RL approaches."}}
{"id": "tXUnSRtXrDv", "cdate": 1609459200000, "mdate": 1681691649707, "content": {"title": "Learn collision-free self-driving skills at urban intersections with model-based reinforcement learning", "abstract": "Intersection is one of the most complex and accident-prone urban traffic scenarios for autonomous driving wherein making safe and computationally efficient decisions with high-density traffic flow is usually non-trivial. Current rule-based methods decompose the decision-making task into several serial sub-modules, resulting in long computation time at complex scenarios for on-board computing devices. In this paper, we formulate the decision-making and control problem under intersections as a process of optimal path selection and tracking, where the former selects a path with the best safety measure from a set generated only considering static information, while the latter then considers dynamic obstacles and solve a tracking problem with safety constraints using the chosen path. To avoid the heavy computation introduced by that, we develop a reinforcement learning algorithm called generalized exterior point (GEP) to find a neural network (NN) solution offline. It first constructs a multi-task problem involving all the candidate paths and transforms it into an unconstrained problem with a penalty on safety violations. Afterward, the approximate feasible optimal control policy is obtained by alternatively performing gradient descent and enlarging the penalty. As an exterior point type method, GEP permits control policy to violate inequality constraints during the iterations. To verify the effectiveness of our method, we carried out experiments both in simulation and in a real road test. Results demonstrate that the learned policy can realize collision-free driving under different traffic conditions while reducing the computation time by a large margin."}}
{"id": "s0JQoE4PzCE", "cdate": 1609459200000, "mdate": 1681691649609, "content": {"title": "Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring Statewise Safety", "abstract": "The safety constraints commonly used by existing safe reinforcement learning (RL) methods are defined only on expectation of initial states, but allow each certain state to be unsafe, which is unsatisfying for real-world safety-critical tasks. In this paper, we introduce the feasible actor-critic (FAC) algorithm, which is the first model-free constrained RL method that considers statewise safety, e.g, safety for each initial state. We claim that some states are inherently unsafe no matter what policy we choose, while for other states there exist policies ensuring safety, where we say such states and policies are feasible. By constructing a statewise Lagrange function available on RL sampling and adopting an additional neural network to approximate the statewise Lagrange multiplier, we manage to obtain the optimal feasible policy which ensures safety for each feasible state and the safest possible policy for infeasible states. Furthermore, the trained multiplier net can indicate whether a given state is feasible or not through the statewise complementary slackness condition. We provide theoretical guarantees that FAC outperforms previous expectation-based constrained RL methods in terms of both constraint satisfaction and reward optimization. Experimental results on both robot locomotive tasks and safe exploration tasks verify the safety enhancement and feasibility interpretation of the proposed method."}}
