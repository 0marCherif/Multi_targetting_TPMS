{"id": "AEovHLrqd7", "cdate": 1640995200000, "mdate": 1681695939488, "content": {"title": "A Rigorous Study of Integrated Gradients Method and Extensions to Internal Neuron Attributions", "abstract": "As deep learning (DL) efficacy grows, concerns for poor model explainability grow also. Attribution methods address the issue of explainability by quantifying the importance of an input feature for..."}}
{"id": "PKdNRKjwL4", "cdate": 1632875439480, "mdate": null, "content": {"title": "DAIR: Data Augmented Invariant Regularization", "abstract": "While deep learning through empirical risk minimization (ERM) has succeeded at achieving human-level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We prove convergence guarantees for DAIR. We apply it to multiple real-world unsupervised and supervised learning problems involving  domain shift. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost. Furthermore, DAIR is competitive with state-of-the-art methods specifically designed for these problems."}}
{"id": "0XDbDoY1aj1", "cdate": 1609459200000, "mdate": 1684137744325, "content": {"title": "A decentralized adaptive momentum method for solving a class of min-max optimization problems", "abstract": ""}}
{"id": "-dB-Fh5S52m", "cdate": 1609459200000, "mdate": 1683883607877, "content": {"title": "Alternating Direction Method of Multipliers for Quantization", "abstract": "Quantization of the parameters of machine learning models, such as deep neural networks, requires solving constrained optimization problems, where the constraint set is formed by the Cartesian product of many simple discrete sets. For such optimization problems, we study the performance of the Alternating Direction Method of Multipliers for Quantization (ADMM-Q) algorithm, which is a variant of the widely-used ADMM method applied to our discrete optimization problem. We establish the convergence of the iterates of ADMM-Q to certain stationary points. To the best of our knowledge, this is the first analysis of an ADMM-type method for problems with discrete variables/constraints. Based on our theoretical insights, we develop a few variants of ADMM-Q that can handle inexact update rules, and have improved performance via the use of \"soft projection\" and \"injecting randomness to the algorithm\". We empirically evaluate the efficacy of our proposed approaches."}}
{"id": "l213uyc7ZrW", "cdate": 1596130066888, "mdate": null, "content": {"title": "Solving a class of non-convex min-max games using iterative first order methods", "abstract": "Recent applications that arise in machine learning have surged significant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium solution can be computed efficiently. In this paper, we study the problem in the non-convex regime and show that an \u03b5\u2013first order stationary point of the game can be computed when one of the player\u2019s objective can be optimized to global optimality efficiently. In particular, we first consider the case where the objective of one of the players satisfies the Polyak-\u0141ojasiewicz (PL) condition. For such a game, we show that a simple multi-step gradient descent-ascent algorithm finds an \u03b5\u2013first order stationary\n\udbff\udc0b \u22122\npoint of the problem in O(\u03b5 ) iterations. Then we show that our framework can\nalso be applied to the case where the objective of the \u201cmax-player\" is concave. In this case, we propose a multi-step gradient descent-ascent algorithm that finds\n\udbff\udc0b \u22123.5\nan \u03b5\u2013first order stationary point of the game in O(\u03b5 ) iterations, which is the\nbest known rate in the literature. We applied our algorithm to a fair classification problem of Fashion-MNIST dataset and observed that the proposed algorithm results in smoother training and better generalization."}}
{"id": "1OgTcFeqxQi", "cdate": 1577836800000, "mdate": 1684137744292, "content": {"title": "Nonconvex Min-Max Optimization: Applications, Challenges, and Recent Theoretical Advances", "abstract": "The min-max optimization problem, also known as the <;i>saddle point problem<;/i>, is a classical optimization problem that is also studied in the context of zero-sum games. Given a class of objective functions, the goal is to find a value for the argument that leads to a small objective value even for the worst-case function in the given class. Min-max optimization problems have recently become very popular in a wide range of signal and data processing applications, such as fair beamforming, training generative adversarial networks (GANs), and robust machine learning (ML), to just name a few."}}
{"id": "dx0-e10MZA", "cdate": 1546300800000, "mdate": null, "content": {"title": "Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods", "abstract": "Recent applications that arise in machine learning have surged significant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium solution can be computed efficiently. In this paper, we study the problem in the non-convex regime and show that an $\\varepsilon$--first order stationary point of the game can be computed when one of the player\u2019s objective can be optimized to global optimality efficiently. In particular, we first consider the case where the objective of one of the players satisfies the Polyak-{\\L}ojasiewicz (PL) condition. For such a game, we show that a simple multi-step gradient descent-ascent algorithm finds an $\\varepsilon$--first order stationary point of the problem in $\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$ iterations. Then we show that our framework can also be applied to the case where the objective of the ``max-player\" is concave. In this case, we propose a multi-step gradient descent-ascent algorithm that finds an $\\varepsilon$--first order stationary point of the game in $\\widetilde{\\cal O}(\\varepsilon^{-3.5})$ iterations, which is the best known rate in the literature. We applied our algorithm to a fair classification problem of Fashion-MNIST dataset and observed that the proposed algorithm results in smoother training and better generalization."}}
