{"id": "VCmhwF0d3gH", "cdate": 1684315534919, "mdate": 1684315534919, "content": {"title": "Multiplex Graph Representation Learning Via Dual Correlation Reduction", "abstract": "Recently, with the superior capacity for analyzing the multiplex graph data, self-supervised multiplex graph representation learning (SMGRL) has received much interest. However, existing SMGRL methods are still limited by the following issues: (i) they generally ignore the noisy information within each graph and the common information among different graphs, thus weakening the effectiveness of SMGRL, and (ii) they conduct negative sample encoding and complex pretext tasks for contrastive learning, thus weakening the efficiency of SMGRL. To solve these issues, in this work, we propose a new framework to conduct effective and efficient SMGRL. Specifically, the proposed method investigates the intra-graph and inter-graph decorrelation losses, respectively, for reducing the impact of noisy information within each graph and capturing the common information among different graphs, to achieve the effectiveness. Moreover, the proposed method does not need negative samples for the SMGRL and designs a simple pretext task, to achieve the efficiency. We further theoretically justify that our method achieves the maximal mutual information instead of directly conducting contrastive learning and theoretically justify that our method actually minimizes the multiplex graph information bottleneck, which guarantees the effectiveness. In addition, an extension for semi-supervised scenarios is proposed to fit the case that a few labels are provided in reality. Extensive experimental results verify the effectiveness and efficiency of the proposed method with respect to various downstream tasks."}}
{"id": "w5GqY3TLib", "cdate": 1640995200000, "mdate": 1667571765128, "content": {"title": "Simple Unsupervised Graph Representation Learning", "abstract": "In this paper, we propose a simple unsupervised graph representation learning method to conduct effective and efficient contrastive learning. Specifically, the proposed multiplet loss explores the complementary information between the structural information and neighbor information to enlarge the inter-class variation, as well as adds an upper bound loss to achieve the finite distance between positive embeddings and anchor embeddings for reducing the intra-class variation. As a result, both enlarging inter-class variation and reducing intra-class variation result in small generalization error, thereby obtaining an effective model. Furthermore, our method removes widely used data augmentation and discriminator from previous graph contrastive learning methods, meanwhile available to output low-dimensional embeddings, leading to an efficient model. Experimental results on various real-world datasets demonstrate the effectiveness and efficiency of our method, compared to state-of-the-art methods. The source codes are released at https://github.com/YujieMo/SUGRL."}}
{"id": "rRxSTgmTrts", "cdate": 1640995200000, "mdate": 1667571765128, "content": {"title": "Multi-view Unsupervised Graph Representation Learning", "abstract": "Both data augmentation and contrastive loss are the key components of contrastive learning. In this paper, we design a new multi-view unsupervised graph representation learning method including adaptive data augmentation and multi-view contrastive learning, to address some issues of contrastive learning ignoring the information from feature space. Specifically, the adaptive data augmentation first builds a feature graph from the feature space, and then designs a deep graph learning model on the original representation and the topology graph to update the feature graph and the new representation. As a result, the adaptive data augmentation outputs multi-view information, which is fed into two GCNs to generate multi-view embedding features. Two kinds of contrastive losses are further designed on multi-view embedding features to explore the complementary information among the topology and feature graphs. Additionally, adaptive data augmentation and contrastive learning are embedded in a unified framework to form an end-to-end model. Experimental results verify the effectiveness of our proposed method, compared to state-of-the-art methods."}}
{"id": "ZdBjebnDK7v", "cdate": 1640995200000, "mdate": 1667571765129, "content": {"title": "Simple Self-supervised Multiplex Graph Representation Learning", "abstract": "Self-supervised multiplex graph representation learning (SMGRL) aims to capture the information from the multiplex graph, and generates discriminative embedding without labels. However, previous SMGRL methods still suffer from the issues of efficiency and effectiveness due to the processes, e.g., data augmentation, negative sample encoding, complex pretext tasks, etc. In this paper, we propose a simple method to achieve efficient and effective SMGRL. Specifically, the proposed method removes the processes (i.e., data augmentation and negative sample encoding) for the SMGRL and designs a simple pretext task, for achieving the efficiency. Moreover, the proposed method also designs an intra-graph decorrelation loss and an inter-graph decorrelation loss, respectively, to capture the common information within individual graphs and the common information across graphs, for achieving the effectiveness. Extensive experimental results verify the efficiency and effectiveness of our method, compared to 11 comparison methods on 4 public benchmark datasets, on the node classification task."}}
{"id": "IWmzReEHx3", "cdate": 1640995200000, "mdate": 1667571765136, "content": {"title": "Dementia analysis from functional connectivity network with graph neural networks", "abstract": ""}}
