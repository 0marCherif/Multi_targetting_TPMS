{"id": "ju9B0jGIMNw", "cdate": 1666700418221, "mdate": 1666700418221, "content": {"title": "Towards Better Understanding Attribution Methods", "abstract": "Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models\u2019 decisions. Evaluating such methods is chal- lenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make compar- isons between them more fair, and to make visual inspec- tion more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are ap- plied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantita- tive metrics. For more systematic visualizations, we pro- pose a scheme (AggAtt) to qualitatively evaluate the meth- ods on complete datasets. We use these evaluation schemes to study strengths and shortcomings of some widely used attribution methods. Finally, we propose a post-processing smoothing step that significantly improves the performance of some attribution methods, and discuss its applicability."}}
{"id": "PeWqtulPJw", "cdate": 1666700305566, "mdate": 1666700305566, "content": {"title": "B-cos Networks: Alignment is All We Need for Interpretability", "abstract": "We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faithfully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced linear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common models such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability. Code available at this https URL"}}
{"id": "ejHUr4nfHhD", "cdate": 1663849995684, "mdate": null, "content": {"title": "Temperature Schedules for self-supervised contrastive methods on long-tail data", "abstract": "Most approaches for self-supervised learning (SSL) are optimised on curated balanced datasets, e.g. ImageNet, despite the fact that natural data usually exhibits long-tail distributions. In this paper, we analyse the behaviour of one of the most popular variants of SSL, i.e. contrastive methods, on imbalanced data. In particular, we investigate the role of the temperature parameter $\\tau$ in the contrastive loss, by analysing the loss through the lens of average distance maximisation, and find that a large $\\tau$ emphasises group-wise discrimination, whereas a small $\\tau$ leads to a higher degree of instance discrimination. While $\\tau$ has thus far been treated exclusively as a constant hyperparameter, in this work, we propose to employ a dynamic $\\tau$ and show that a simple cosine schedule can yield significant improvements in the learnt representations. Such a schedule results in a constant `task switching' between an emphasis on instance discrimination and group-wise discrimination and thereby ensures that the model learns both group-wise features, as well as instance-specific details. Since frequent classes benefit from the former, while infrequent classes require the latter, we find this method to consistently improve separation between the classes in long-tail data without any additional computational cost. "}}
{"id": "jw37FUa_Aw9", "cdate": 1663849984698, "mdate": null, "content": {"title": "Holistically Explainable Vision Transformers", "abstract": "Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model component\u2014such as the multi-layer perceptrons, attention layers, and the  tokenisation module\u2014to be dynamic linear, which allows us to faithfully summarise the entire transformer via a single linear transform. We apply our proposed design to Vision Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are highly interpretable and perform competitively to baseline ViTs on ImageNet. Code will be available at: github.com/anonymous/authors."}}
{"id": "WiRjVVuub6g", "cdate": 1609459200000, "mdate": 1631694733135, "content": {"title": "Convolutional Dynamic Alignment Networks for Interpretable Classifications", "abstract": "We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which linearly transform their input with weight vectors that dynamically align with task-relevant patterns. As a result, CoDA-Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA-Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet."}}
