{"id": "LBOPph_DS6", "cdate": 1640995200000, "mdate": 1681720913315, "content": {"title": "A Probabilistic Framework for Lifelong Test-Time Adaptation", "abstract": "Test-time adaptation (TTA) is the problem of updating a pre-trained source model at inference time given test input(s) from a different target domain. Most existing TTA approaches assume the setting in which the target domain is stationary, i.e., all the test inputs come from a single target domain. However, in many practical settings, the test input distribution might exhibit a lifelong/continual shift over time. Moreover, existing TTA approaches also lack the ability to provide reliable uncertainty estimates, which is crucial when distribution shifts occur between the source and target domain. To address these issues, we present PETAL (Probabilistic lifElong Test-time Adaptation with seLf-training prior), which solves lifelong TTA using a probabilistic approach, and naturally results in (1) a student-teacher framework, where the teacher model is an exponential moving average of the student model, and (2) regularizing the model updates at inference time using the source model as a regularizer. To prevent model drift in the lifelong/continual TTA setting, we also propose a data-driven parameter restoration technique which contributes to reducing the error accumulation and maintaining the knowledge of recent domains by restoring only the irrelevant parameters. In terms of predictive error rate as well as uncertainty based metrics such as Brier score and negative log-likelihood, our method achieves better results than the current state-of-the-art for online lifelong test-time adaptation across various benchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC datasets. The source code for our approach is accessible at https://github.com/dhanajitb/petal."}}
{"id": "ef2-wAwKu4h", "cdate": 1609459200000, "mdate": 1640929662762, "content": {"title": "Fine-Grained Emotion Prediction by Modeling Emotion Definitions", "abstract": "In this paper, we propose a new framework for fine-grained emotion prediction in the text through emotion definition modeling. Our approach involves a multi-task learning framework that models definitions of emotions as an auxiliary task while being trained on the primary task of emotion prediction. We model definitions using masked language modeling and class definition prediction tasks. Our models outperform existing state-of-the-art for fine-grained emotion dataset GoEmotions. We further show that this trained model can be used for transfer learning on other benchmark datasets in emotion prediction with varying emotion label sets, domains, and sizes. The proposed models outperform the baselines on transfer learning experiments demonstrating the generalization capability of the models."}}
{"id": "cIm8T8F6Uj", "cdate": 1609459200000, "mdate": 1671900172704, "content": {"title": "Hypernetworks for Continual Semi-Supervised Learning", "abstract": "Learning from data sequentially arriving, possibly in a non i.i.d. way, with changing task distribution over time is called continual learning. Much of the work thus far in continual learning focuses on supervised learning and some recent works on unsupervised learning. In many domains, each task contains a mix of labelled (typically very few) and unlabelled (typically plenty) training examples, which necessitates a semi-supervised learning approach. To address this in a continual learning setting, we propose a framework for semi-supervised continual learning called Meta-Consolidation for Continual Semi-Supervised Learning (MCSSL). Our framework has a hypernetwork that learns the meta-distribution that generates the weights of a semi-supervised auxiliary classifier generative adversarial network $(\\textit{Semi-ACGAN})$ as the base network. We consolidate the knowledge of sequential tasks in the hypernetwork, and the base network learns the semi-supervised learning task. Further, we present $\\textit{Semi-Split CIFAR-10}$, a new benchmark for continual semi-supervised learning, obtained by modifying the $\\textit{Split CIFAR-10}$ dataset, in which the tasks with labelled and unlabelled data arrive sequentially. Our proposed model yields significant improvements in the continual semi-supervised learning setting. We compare the performance of several existing continual learning approaches on the proposed continual semi-supervised learning benchmark of the Semi-Split CIFAR-10 dataset."}}
{"id": "G5HD70CpGXT", "cdate": 1609459200000, "mdate": 1634692407706, "content": {"title": "Fine-Grained Emotion Prediction by Modeling Emotion Definitions", "abstract": "In this paper, we propose a new framework for fine-grained emotion prediction in the text through emotion definition modeling. Our approach involves a multi-task learning framework that models definitions of emotions as an auxiliary task while being trained on the primary task of emotion prediction. We model definitions using masked language modeling and class definition prediction tasks. Our models outperform existing state-of-the-art for fine-grained emotion dataset GoEmotions. We further show that this trained model can be used for transfer learning on other benchmark datasets in emotion prediction with varying emotion label sets, domains, and sizes. The proposed models outperform the baselines on transfer learning experiments demonstrating the generalization capability of the models."}}
{"id": "EQ_KdZHbiQ2", "cdate": 1596141981832, "mdate": null, "content": {"title": "Meta-Learning for Generalized Zero-Shot Learning.", "abstract": "Learning to classify unseen class samples at test time is popularly referred to as zero-shot learning (ZSL). If test samples can be from training (seen) as well as unseen classes, it is a more challenging problem due to the existence of strong bias towards seen classes. This problem is generally known as \\emph{generalized} zero-shot learning (GZSL). Thanks to the recent advances in generative models such as VAEs and GANs, sample synthesis based approaches have gained considerable attention for solving this problem. These approaches are able to handle the problem of class bias by synthesizing unseen class samples. However, these ZSL/GZSL models suffer due to the following key limitations: (i) Their training stage learns a class-conditioned generator using only \\emph{seen} class data and the training stage does not \\emph{explicitly} learn to generate the unseen class samples; (ii) They do not learn a generic optimal parameter which can easily generalize for both seen and unseen class generation; and (iii) If we only have access to a very few samples per seen class, these models tend to perform poorly. In this paper, we propose a meta-learning based generative model that naturally handles these limitations. The proposed model is based on integrating model-agnostic meta learning with a Wasserstein GAN (WGAN) to handle (i) and (iii), and uses a novel task distribution to handle (ii). Our proposed model yields significant improvements on standard ZSL as well as more challenging GZSL setting. In ZSL setting, our model yields 4.5\\%, 6.0\\%, 9.8\\%, and 27.9\\% relative improvements over the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets, respectively."}}
{"id": "p1gQkjEvHYX", "cdate": 1577836800000, "mdate": 1671900172487, "content": {"title": "Deep Attentive Ranking Networks for Learning to Order Sentences", "abstract": "We present an attention-based ranking framework for learning to order sentences given a paragraph. Our framework is built on a bidirectional sentence encoder and a self-attention based transformer network to obtain an input order invariant representation of paragraphs. Moreover, it allows seamless training using a variety of ranking based loss functions, such as pointwise, pairwise, and listwise ranking. We apply our framework on two tasks: Sentence Ordering and Order Discrimination. Our framework outperforms various state-of-the-art methods on these tasks on a variety of evaluation metrics. We also show that it achieves better results when using pairwise and listwise ranking losses, rather than the pointwise ranking loss, which suggests that incorporating relative positions of two or more sentences in the loss function contributes to better learning."}}
{"id": "cMAoAY0kCi", "cdate": 1577836800000, "mdate": 1671900172700, "content": {"title": "Deep Attentive Ranking Networks for Learning to Order Sentences", "abstract": "We present an attention-based ranking framework for learning to order sentences given a paragraph. Our framework is built on a bidirectional sentence encoder and a self-attention based transformer network to obtain an input order invariant representation of paragraphs. Moreover, it allows seamless training using a variety of ranking based loss functions, such as pointwise, pairwise, and listwise ranking. We apply our framework on two tasks: Sentence Ordering and Order Discrimination. Our framework outperforms various state-of-the-art methods on these tasks on a variety of evaluation metrics. We also show that it achieves better results when using pairwise and listwise ranking losses, rather than the pointwise ranking loss, which suggests that incorporating relative positions of two or more sentences in the loss function contributes to better learning."}}
{"id": "4PK8vow_fw", "cdate": 1577836800000, "mdate": 1671900172495, "content": {"title": "Meta-Learning for Generalized Zero-Shot Learning", "abstract": "Learning to classify unseen class samples at test time is popularly referred to as zero-shot learning (ZSL). If test samples can be from training (seen) as well as unseen classes, it is a more challenging problem due to the existence of strong bias towards seen classes. This problem is generally known as generalized zero-shot learning (GZSL). Thanks to the recent advances in generative models such as VAEs and GANs, sample synthesis based approaches have gained considerable attention for solving this problem. These approaches are able to handle the problem of class bias by synthesizing unseen class samples. However, these ZSL/GZSL models suffer due to the following key limitations: (i) Their training stage learns a class-conditioned generator using only seen class data and the training stage does not explicitly learn to generate the unseen class samples; (ii) They do not learn a generic optimal parameter which can easily generalize for both seen and unseen class generation; and (iii) If we only have access to a very few samples per seen class, these models tend to perform poorly. In this paper, we propose a meta-learning based generative model that naturally handles these limitations. The proposed model is based on integrating model-agnostic meta learning with a Wasserstein GAN (WGAN) to handle (i) and (iii), and uses a novel task distribution to handle (ii). Our proposed model yields significant improvements on standard ZSL as well as more challenging GZSL setting. In ZSL setting, our model yields 4.5%, 6.0%, 9.8%, and 27.9% relative improvements over the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets, respectively."}}
{"id": "2tilW1Bak8d", "cdate": 1546300800000, "mdate": 1671900172701, "content": {"title": "A Meta-Learning Framework for Generalized Zero-Shot Learning", "abstract": "Learning to classify unseen class samples at test time is popularly referred to as zero-shot learning (ZSL). If test samples can be from training (seen) as well as unseen classes, it is a more challenging problem due to the existence of strong bias towards seen classes. This problem is generally known as \\emph{generalized} zero-shot learning (GZSL). Thanks to the recent advances in generative models such as VAEs and GANs, sample synthesis based approaches have gained considerable attention for solving this problem. These approaches are able to handle the problem of class bias by synthesizing unseen class samples. However, these ZSL/GZSL models suffer due to the following key limitations: $(i)$ Their training stage learns a class-conditioned generator using only \\emph{seen} class data and the training stage does not \\emph{explicitly} learn to generate the unseen class samples; $(ii)$ They do not learn a generic optimal parameter which can easily generalize for both seen and unseen class generation; and $(iii)$ If we only have access to a very few samples per seen class, these models tend to perform poorly. In this paper, we propose a meta-learning based generative model that naturally handles these limitations. The proposed model is based on integrating model-agnostic meta learning with a Wasserstein GAN (WGAN) to handle $(i)$ and $(iii)$, and uses a novel task distribution to handle $(ii)$. Our proposed model yields significant improvements on standard ZSL as well as more challenging GZSL setting. In ZSL setting, our model yields 4.5\\%, 6.0\\%, 9.8\\%, and 27.9\\% relative improvements over the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets, respectively."}}
{"id": "0j1-QqlhnZ", "cdate": 1546300800000, "mdate": 1671900172475, "content": {"title": "An Integrated Approach for Identification of Functionally Similar MicroRNAs in Colorectal Cancer", "abstract": "Colorectal cancer (CRC) is one of the most prevalent cancers around the globe. However, the molecular reasons for pathogenesis of CRC are still poorly understood. Recently, the role of microRNAs or miRNAs in the initiation and progression of CRC has been studied. MicroRNAs are small, endogenous noncoding RNAs found in plants, animals, and some viruses, which function in RNA silencing and posttranscriptional regulation of gene expression. Their role in CRC development is studied and they are found to be potential biomarkers in diagnosis and treatment of CRC. Therefore, identification of functionally similar CRC related miRNAs may help in the development of a prognostic tool. In this regard, this paper presents a new algorithm, called <inline-formula><tex-math notation=\"LaTeX\">$\\mu$</tex-math></inline-formula> Sim. It is an integrative approach for identification of functionally similar miRNAs associated with CRC. It integrates judiciously the information of miRNA expression data and miRNA-miRNA functionally synergistic network data. The functional similarity is calculated based on both miRNA expression data and miRNA-miRNA functionally synergistic network data. The effectiveness of the proposed method in comparison to other related methods is shown on four CRC miRNA data sets. The proposed method selected more significant miRNAs related to CRC as compared to other related methods."}}
