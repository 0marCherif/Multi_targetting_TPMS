{"id": "2fjrAFld3n", "cdate": 1675206792557, "mdate": null, "content": {"title": "Heavy-tailed sampling via transformed unadjusted Langevin algorithm", "abstract": "We analyze the oracle complexity of sampling from polynomially decaying heavy-tailed target densities based on running the Unadjusted Langevin Algorithm on certain transformed versions of the target density. The specific class of closed-form transformation maps that we construct are shown to be diffeomorphisms, and are particularly suited for developing efficient diffusion-based samplers. We characterize the precise class of heavy-tailed densities for which polynomial-order oracle complexities (in dimension and inverse target accuracy) could be obtained, and provide illustrative examples. We highlight the relationship between our assumptions and functional inequalities (super and weak Poincar\\'e inequalities) based on non-local Dirichlet forms defined via fractional Laplacian operators, used to characterize the heavy-tailed equilibrium densities of certain stable-driven stochastic differential equations."}}
{"id": "44a3n0yTE6", "cdate": 1675206692441, "mdate": 1675206692441, "content": {"title": "On the ergodicity, bias and asymptotic normality of randomized midpoint sampling method", "abstract": "The randomized midpoint method, proposed by (Shen and Lee, 2019), has emerged as an optimal discretization procedure for simulating the continuous time underdamped Langevin diffusion. In this paper, we analyze several probabilistic properties of the randomized midpoint discretization method, considering both overdamped and underdamped Langevin dynamics. We first characterize the stationary distribution of the discrete chain obtained with constant step-size discretization and show that it is biased away from the target distribution. Notably, the step-size needs to go to zero to obtain asymptotic unbiasedness. Next, we establish the asymptotic normality of numerical integration using the randomized midpoint method and highlight the relative advantages and disadvantages over other discretizations. Our results collectively provide several insights into the behavior of the randomized midpoint discretization method, including obtaining confidence intervals for numerical integrations."}}
{"id": "ZVS1JvOdSf", "cdate": 1672531200000, "mdate": 1682321104380, "content": {"title": "Mean-Square Analysis of Discretized It\u00f4 Diffusions for Heavy-tailed Sampling", "abstract": "We analyze the complexity of sampling from a class of heavy-tailed distributions by discretizing a natural class of It\\^o diffusions associated with weighted Poincar\\'e inequalities. Based on a mean-square analysis, we establish the iteration complexity for obtaining a sample whose distribution is $\\epsilon$ close to the target distribution in the Wasserstein-2 metric. In this paper, our results take the mean-square analysis to its limits, i.e., we invariably only require that the target density has finite variance, the minimal requirement for a mean-square analysis. To obtain explicit estimates, we compute upper bounds on certain moments associated with heavy-tailed targets under various assumptions. We also provide similar iteration complexity results for the case where only function evaluations of the unnormalized target density are available by estimating the gradients using a Gaussian smoothing technique. We provide illustrative examples based on the multivariate $t$-distribution."}}
{"id": "gtNboS8Juo2", "cdate": 1664731452992, "mdate": null, "content": {"title": "Neural Networks Efficiently Learn Low-Dimensional Representations with SGD", "abstract": "We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\\boldsymbol{x}\\in \\mathbb{R}^d$ is Gaussian and the target $y \\in \\mathbb{R}$ follows a multiple-index model, i.e., $y=g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$ with a noisy link function $g$. We prove that the first-layer weights of the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\\boldsymbol{u_1},...,\\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \\ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $\\mathcal{O}(\\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form $y=f(\\langle\\boldsymbol{u},\\boldsymbol{x}\\rangle) + \\epsilon$ by recovering the principal direction, with a sample complexity linear in $d$ (up to log factors), where $f$ is a monotonic function with at most polynomial growth, and $\\epsilon$ is the noise. This is in contrast to the known $d^{\\Omega(p)}$ sample requirement to learn any degree $p$ polynomial in the kernel regime, and it shows that NNs trained with SGD can outperform the neural tangent kernel at initialization."}}
{"id": "6taykzqcPD", "cdate": 1663850181252, "mdate": null, "content": {"title": "Neural Networks Efficiently Learn Low-Dimensional Representations with SGD", "abstract": "We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\\boldsymbol{x}\\in \\mathbb{R}^d$ is Gaussian and the target $y \\in \\mathbb{R}$ follows a multiple-index model, i.e., $y=g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$ with a noisy link function $g$. We prove that the first-layer weights in the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\\boldsymbol{u_1},...,\\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \\ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $\\mathcal{O}(\\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, by recovering the principal direction, SGD-trained ReLU NNs can learn a single-index target of the form $y=f(\\langle\\boldsymbol{u},\\boldsymbol{x}\\rangle) + \\epsilon$ with a sample complexity linear in $d$ (up to log factors), where $f$ is a monotonic function with at most polynomial growth, and $\\epsilon$ is the noise. This is in contrast to the known $d^{\\Omega(p)}$ samples required to learn any degree $p$ polynomial in the kernel regime, and shows that SGD-trained NNs can outperform the Neural Tangent Kernel at initialization. Finally, we establish compressibility guarantees for NNs using that SGD produces an approximately rank-$k$ first-layer weight matrix."}}
{"id": "akddwRG6EGi", "cdate": 1652737662976, "mdate": null, "content": {"title": "High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation", "abstract": "We study the first gradient descent step on the first-layer parameters $\\boldsymbol{W}$ in a two-layer neural network: $f(\\boldsymbol{x}) = \\frac{1}{\\sqrt{N}}\\boldsymbol{a}^\\top\\sigma(\\boldsymbol{W}^\\top\\boldsymbol{x})$, where $\\boldsymbol{W}\\in\\mathbb{R}^{d\\times N}, \\boldsymbol{a}\\in\\mathbb{R}^{N}$ are randomly initialized, and the training objective is the empirical MSE loss: $\\frac{1}{n}\\sum_{i=1}^n (f(\\boldsymbol{x}_i)-y_i)^2$. In the proportional asymptotic limit where $n,d,N\\to\\infty$ at the same rate, and an idealized student-teacher setting where the teacher $f^*$ is a single-index model, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on $\\boldsymbol{W}$ with learning rate $\\eta$. We consider two scalings of the first step learning rate $\\eta$. For small $\\eta$, we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random features model, but cannot defeat the best linear model on the input. Whereas for sufficiently large $\\eta$, we prove that for certain $f^*$, the same ridge estimator on trained features can go beyond this ``linear regime'' and outperform a wide range of (fixed) kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training.   "}}
{"id": "msBC-W9Elaa", "cdate": 1652737484245, "mdate": null, "content": {"title": "Generalization Bounds for Stochastic Gradient Descent via Localized $\\varepsilon$-Covers", "abstract": "In this paper, we propose a new covering technique localized for the trajectories of SGD. This localization provides an algorithm-specific complexity measured by the covering number, which can have dimension-independent cardinality in contrast to standard uniform covering arguments that result in exponential dimension dependency. Based on this localized construction, we show that if the objective function is a finite perturbation of a piecewise strongly convex and smooth function with $P$ pieces, i.e., non-convex and non-smooth in general, the generalization error can be upper bounded by $O(\\sqrt{(\\log n\\log(nP))/n})$, where $n$ is the number of data samples. In particular, this rate is independent of dimension and does not require early stopping and decaying step size. Finally, we employ these results in various contexts and derive generalization bounds for multi-index linear models, multi-class support vector machines, and $K$-means clustering for both hard and soft label setups, improving the previously known state-of-the-art rates."}}
{"id": "SnEez8-mrl5", "cdate": 1645715785876, "mdate": 1645715785876, "content": {"title": "Analysis of Langevin Monte Carlo from Poincar\u00e9 to Log-Sobolev", "abstract": "Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution \u03c0 under the sole assumption that \u03c0 satisfies a Poincar\u00e9 inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or R\u00e9nyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that \u03c0 satisfies either a Lata\u0142a--Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincar\u00e9 and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions."}}
{"id": "rSmQre-m39", "cdate": 1640995200000, "mdate": 1682321104343, "content": {"title": "Neural Networks Efficiently Learn Low-Dimensional Representations with SGD", "abstract": "We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\\boldsymbol{x}\\in \\mathbb{R}^d$ is Gaussian and the target $y \\in \\mathbb{R}$ follows a multiple-index model, i.e., $y=g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$ with a noisy link function $g$. We prove that the first-layer weights of the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\\boldsymbol{u_1},...,\\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \\ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $O(\\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form $y=f(\\langle\\boldsymbol{u},\\boldsymbol{x}\\rangle) + \\epsilon$ by recovering the principal direction, with a sample complexity linear in $d$ (up to log factors), where $f$ is a monotonic function with at most polynomial growth, and $\\epsilon$ is the noise. This is in contrast to the known $d^{\\Omega(p)}$ sample requirement to learn any degree $p$ polynomial in the kernel regime, and it shows that NNs trained with SGD can outperform the neural tangent kernel at initialization. Finally, we also provide compressibility guarantees for NNs using the approximate low-rank structure produced by SGD."}}
{"id": "na9JkI5OutN", "cdate": 1640995200000, "mdate": 1652396288523, "content": {"title": "High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation", "abstract": "We study the first gradient descent step on the first-layer parameters $\\boldsymbol{W}$ in a two-layer neural network: $f(\\boldsymbol{x}) = \\frac{1}{\\sqrt{N}}\\boldsymbol{a}^\\top\\sigma(\\boldsymbol{W}^\\top\\boldsymbol{x})$, where $\\boldsymbol{W}\\in\\mathbb{R}^{d\\times N}, \\boldsymbol{a}\\in\\mathbb{R}^{N}$ are randomly initialized, and the training objective is the empirical MSE loss: $\\frac{1}{n}\\sum_{i=1}^n (f(\\boldsymbol{x}_i)-y_i)^2$. In the proportional asymptotic limit where $n,d,N\\to\\infty$ at the same rate, and an idealized student-teacher setting, we show that the first gradient update contains a rank-1 \"spike\", which results in an alignment between the first-layer weights and the linear component of the teacher model $f^*$. To characterize the impact of this alignment, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on $\\boldsymbol{W}$ with learning rate $\\eta$, when $f^*$ is a single-index model. We consider two scalings of the first step learning rate $\\eta$. For small $\\eta$, we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random features model, but cannot defeat the best linear model on the input. Whereas for sufficiently large $\\eta$, we prove that for certain $f^*$, the same ridge estimator on trained features can go beyond this \"linear regime\" and outperform a wide range of random features and rotationally invariant kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training."}}
