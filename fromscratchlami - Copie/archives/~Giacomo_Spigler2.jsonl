{"id": "_nUa3G7M1xq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Flatland-RL : Multi-Agent Reinforcement Learning on Trains", "abstract": "Efficient automated scheduling of trains remains a major challenge for modern railway systems. The underlying vehicle rescheduling problem (VRSP) has been a major focus of Operations Research (OR) since decades. Traditional approaches use complex simulators to study VRSP, where experimenting with a broad range of novel ideas is time consuming and has a huge computational overhead. In this paper, we introduce a two-dimensional simplified grid environment called \"Flatland\" that allows for faster experimentation. Flatland does not only reduce the complexity of the full physical simulation, but also provides an easy-to-use interface to test novel approaches for the VRSP, such as Reinforcement Learning (RL) and Imitation Learning (IL). In order to probe the potential of Machine Learning (ML) research on Flatland, we (1) ran a first series of RL and IL experiments and (2) design and executed a public Benchmark at NeurIPS 2020 to engage a large community of researchers to work on this problem. Our own experimental results, on the one hand, demonstrate that ML has potential in solving the VRSP on Flatland. On the other hand, we identify key topics that need further research. Overall, the Flatland environment has proven to be a robust and valuable framework to investigate the VRSP for railway networks. Our experiments provide a good starting point for further research and for the participants of the NeurIPS 2020 Flatland Benchmark. All of these efforts together have the potential to have a substantial impact on shaping the mobility of the future."}}
{"id": "63vB3L_mII7", "cdate": 1577836800000, "mdate": null, "content": {"title": "Denoising Autoencoders for Overgeneralization in Neural Networks", "abstract": "Despite recent developments that allowed neural networks to achieve impressive performance on a variety of applications, these models are intrinsically affected by the problem of overgeneralization, due to their partitioning of the full input space into the fixed set of target classes used during training. Thus it is possible for novel inputs belonging to categories unknown during training or even completely unrecognizable to humans to fool the system into classifying them as one of the known classes, even with a high degree of confidence. This problem can lead to security problems in critical applications, and is closely linked to open set recognition and 1-class recognition. This paper presents a novel way to compute a confidence score using the reconstruction error of denoising autoencoders and shows how it can correctly identify the regions of the input space close to the training distribution. The proposed solution is tested on benchmarks of `fooling', open set recognition and 1-class recognition constructed from the MNIST and Fashion-MNIST datasets."}}
{"id": "YqSRZW6x2eG", "cdate": 1546300800000, "mdate": null, "content": {"title": "Convergence of regular spiking and intrinsically bursting Izhikevich neuron models as a function of discretization time with Euler method", "abstract": "This study investigates the trade-off between computational efficiency and accuracy of Izhikevich neuron models by numerically quantifying their convergence to provide design guidelines in choosing the limit time steps during a discretization procedure. This is important for bionic engineering and neuro-robotic applications where the use of embedded computational resources requires the introduction of optimality criteria. Specifically, the regular spiking (RS) and intrinsically bursting (IB) Izhikevich neuron models are evaluated with step inputs of various amplitudes. We analyze the convergence of spike sequences generated under different discretization time steps (10\u2009\u00b5s to 10\u2009ms), with respect to an ideal reference spike sequence approximated with a discretization time step of 1\u2009\u00b5s. The differences between the ideal reference and the computed spike sequences were quantified by Victor\u2013Purpura (VPd) and van Rossum (VRd) distances. For each distance, we found two limit discretization times (lower dt1 and upper dt2), as a function of the applied input and thus firing rate, beyond which the convergence is lost for each neuron model. The estimated limit time steps were found to be consistent regardless of metric used (VPd and VRd) and neuron type (RS and IB), but also to depend on the average inter-spike interval (ISI) produced by the neurons. However, in most cases, a good trade-off between the quality of the convergence of the models dynamics and the computational load required to simulate them numerically was found for values of the discretization time step between dt1\u202f\u2208\u202f(0.1\u2009ms, 1\u2009ms) and dt2\u202f\u2208\u202f(2\u2009ms, 3\u2009ms)."}}
{"id": "W6vpQZn1Q98", "cdate": 1546300800000, "mdate": null, "content": {"title": "Meta-learnt priors slow down catastrophic forgetting in neural networks", "abstract": "Current training regimes for deep learning usually involve exposure to a single task / dataset at a time. Here we start from the observation that in this context the trained model is not given any knowledge of anything outside its (single-task) training distribution, and has thus no way to learn parameters (i.e., feature detectors or policies) that could be helpful to solve other tasks, and to limit future interference with the acquired knowledge, and thus catastrophic forgetting. Here we show that catastrophic forgetting can be mitigated in a meta-learning context, by exposing a neural network to multiple tasks in a sequential manner during training. Finally, we present SeqFOMAML, a meta-learning algorithm that implements these principles, and we evaluate it on sequential learning problems composed by Omniglot and MiniImageNet classification tasks."}}
{"id": "HeyL6AiwZu", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Temporal Singularity: Time-Accelerated Simulated Civilizations and Their Implications", "abstract": "Provided significant future progress in artificial intelligence and computing, it may ultimately be possible to create multiple Artificial General Intelligences (AGIs), and possibly entire societies living within simulated environments. In that case, it should be possible to improve the problem solving capabilities of the system by increasing the speed of the simulation. If a minimal simulation with sufficient capabilities is created, it might manage to increase its own speed by accelerating progress in science and technology, in a way similar to the Technological Singularity. This may ultimately lead to large simulated civilizations unfolding at extreme temporal speedups, achieving what from the outside would look like a Temporal Singularity. Here we discuss the feasibility of the minimal simulation and the potential advantages, dangers, and connection to the Fermi paradox of the Temporal Singularity. The medium-term importance of the topic derives from the amount of computational power required to start the process, which could be available within the next decades, making the Temporal Singularity theoretically possible before the end of the century."}}
