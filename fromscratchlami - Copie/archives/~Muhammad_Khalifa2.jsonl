{"id": "9kK4R_8nAsD", "cdate": 1675827741199, "mdate": null, "content": {"title": "Exploring Demonstration Ensembling for In-context Learning", "abstract": "In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for desired tasks, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are not very relevant to the test example. Second, due to the input length limit of transformer models, it can be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. DENSE predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment on 7 different language tasks. Our experiments show max ensembling to outperform concatenation by an average of 3.8 points. "}}
{"id": "mi4Yob90MeV", "cdate": 1640995200000, "mdate": 1671488757610, "content": {"title": "LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA", "abstract": "We study few-shot reranking for multi-hop QA with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on large language models prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model. PromptRank yields strong retrieval performance on HotpotQA with only 128 training examples compared to state-of-the-art methods trained on thousands of examples -- 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval. Code available at https://github.com/mukhal/PromptRank"}}
{"id": "EEwweUSgGd4", "cdate": 1640995200000, "mdate": 1671488757621, "content": {"title": "Contrastive Training Improves Zero-Shot Classification of Semi-structured Documents", "abstract": "We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents, as positional, layout, and style information play a vital role in interpreting such documents. The standard classification setting where categories are fixed during both training and testing falls short in dynamic environments where new document categories could potentially emerge. We focus exclusively on the zero-shot setting where inference is done on new unseen classes. To address this task, we propose a matching-based approach that relies on a pairwise contrastive objective for both pretraining and fine-tuning. Our results show a significant boost in Macro F$_1$ from the proposed pretraining step in both supervised and unsupervised zero-shot settings."}}
{"id": "1M7D9UUW1t", "cdate": 1640995200000, "mdate": 1671488757610, "content": {"title": "Novel Chapter Abstractive Summarization using Spinal Tree Aware Sub-Sentential Content Selection", "abstract": "Summarizing novel chapters is a difficult task due to the input length and the fact that sentences that appear in the desired summaries draw content from multiple places throughout the chapter. We present a pipelined extractive-abstractive approach where the extractive step filters the content that is passed to the abstractive component. Extremely lengthy input also results in a highly skewed dataset towards negative instances for extractive summarization; we thus adopt a margin ranking loss for extraction to encourage separation between positive and negative examples. Our extraction component operates at the constituent level; our approach to this problem enriches the text with spinal tree information which provides syntactic context (in the form of constituents) to the extraction model. We show an improvement of 3.71 Rouge-1 points over best results reported in prior work on an existing novel chapter dataset."}}
{"id": "rzZIUX3DG9", "cdate": 1609459200000, "mdate": 1647981389679, "content": {"title": "A Bag of Tricks for Dialogue Summarization", "abstract": "Muhammad Khalifa, Miguel Ballesteros, Kathleen McKeown. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "Hff-88m3PMq", "cdate": 1609459200000, "mdate": 1647981389680, "content": {"title": "A Distributional Approach to Controlled Text Generation", "abstract": "We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. Code available at https://github.com/naver/gdc"}}
{"id": "HYWbU87nPz9", "cdate": 1609459200000, "mdate": 1647981389700, "content": {"title": "Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling", "abstract": "Muhammad Khalifa, Muhammad Abdul-Mageed, Khaled Shaalan. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."}}
{"id": "BdO-I873wz9", "cdate": 1609459200000, "mdate": 1647981389679, "content": {"title": "Extracting Synonyms from Bilingual Dictionaries", "abstract": "Mustafa Jarrar, Eman Naser, Muhammad Khalifa, Khaled Shaalan. Proceedings of the 11th Global Wordnet Conference. 2021."}}
{"id": "jWkw45-9AbL", "cdate": 1601308224936, "mdate": null, "content": {"title": "A Distributional Approach to Controlled Text Generation", "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc"}}
{"id": "S9W8UmhvMq", "cdate": 1546300800000, "mdate": 1647981389679, "content": {"title": "Character convolutions for Arabic Named Entity Recognition with Long Short-Term Memory Networks", "abstract": "Highlights \u2022 Examine an LSTM neural tagging model for Named Entity Recognition (NER). \u2022 Demonstrate the capability of the proposed model with both word and character levels and how it can handle out-of-vocabulary words. \u2022 Convolutional Neural Network can be one of the promising technologies that can successfully extract character-level representations. \u2022 Compare different word vector models for the task of Arabic NER. \u2022 Conduct experimental evaluation that shows that our model performance outperforms state-of-the-art Arabic NER systems on many standard benchmarks. Abstract Named Entity Recognition (NER) is a significant information extraction task since it is an important component of many natural language processing applications, such as Information Retrieval, Question Answering and Speech Recognition. The complexity and morphological richness of the Arabic language is the main reason why most existing Arabic NER systems rely strongly on hand-crafted feature engineering. In this paper, we propose to augment the existing LSTM neural tagging model for Arabic NER with a Convolutional Neural Network (CNN) for the extraction of relevant character-level features. By operating on the character-level, the proposed model is able to handle out-of-vocabulary words. Our results show that character CNN is able to outperform the previously used character-level Bi-directional Long Short-Term Memory Networks (BiLSTM) in many settings. Moreover, our observations indicate that CNNs tend to perform better than BiLSTM on relatively longer tokens. In addition, we conduct a comparison of four different pre-trained word vector models for Arabic NER and results show that a Skip-Gram Word2vec model, pre-trained on a subset of the Arabic Gigaword corpus, is generally sufficient to obtain acceptable Arabic NER performance."}}
