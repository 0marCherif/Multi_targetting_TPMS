{"id": "KRpM1dv_Fm", "cdate": 1672531200000, "mdate": 1683880072910, "content": {"title": "LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees", "abstract": "Graph learning from signals is a core task in Graph Signal Processing (GSP). One of the most commonly used models to learn graphs from stationary signals is SpecT. However, its practical formulation rSpecT is known to be sensitive to hyperparameter selection and, even worse, to suffer from infeasibility. In this paper, we give the first condition that guarantees the infeasibility of rSpecT and design a novel model (LogSpecT) and its practical formulation (rLogSpecT) to overcome this issue. Contrary to rSpecT, the novel practical model rLogSpecT is always feasible. Furthermore, we provide recovery guarantees of rLogSpecT, which are derived from modern optimization tools related to epi-convergence. These tools could be of independent interest and significant for various learning problems. To demonstrate the advantages of rLogSpecT in practice, a highly efficient algorithm based on the linearized alternating direction method of multipliers (L-ADMM) is proposed. The subproblems of L-ADMM admit closed-form solutions and the convergence is guaranteed. Extensive numerical results on both synthetic and real networks corroborate the stability and superiority of our proposed methods, underscoring their potential for various graph learning applications."}}
{"id": "2IQVGtuC7e", "cdate": 1664731450638, "mdate": null, "content": {"title": "Nonsmooth Composite Nonconvex-Concave Minimax Optimization", "abstract": "Nonconvex-concave minimax optimization has received intense interest in machine learning, including learning with robustness to data distribution, learning with non-decomposable loss, adversarial learning, to name a few. Nevertheless, most existing works focus on the gradient-descent-ascent (GDA) variants that can only be applied in smooth settings. In this paper, we consider a family of minimax problems whose objective function enjoys the nonsmooth composite structure in the variable of minimization and is concave in the variables of maximization. By fully exploiting the composite structure,  we propose a smoothed proximal linear descent ascent (\\textit{smoothed} PLDA) algorithm and further establish its $\\mathcal{O}(\\epsilon^{-4})$ iteration complexity, which matches that of smoothed GDA~\\cite{zhang2020single} under smooth settings.  Moreover, under the mild assumption that the objective function satisfies the one-sided Kurdyka-\\L{}ojasiewicz condition with exponent $\\theta \\in (0,1)$, we can further improve the iteration complexity to $\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$. To the best of our knowledge, this is the first provably efficient algorithm for nonsmooth nonconvex-concave problems that can achieve the optimal iteration complexity $\\mathcal{O}(\\epsilon^{-2})$ if $\\theta \\in (0,1/2]$. "}}
{"id": "p94aWFXZ2pm", "cdate": 1640995200000, "mdate": 1671847091526, "content": {"title": "Nonsmooth Composite Nonconvex-Concave Minimax Optimization", "abstract": "Nonconvex-nonconcave minimax optimization has gained widespread interest over the last decade. However, most existing works focus on variants of gradient descent-ascent (GDA) algorithms, which are only applicable to smooth nonconvex-concave settings. To address this limitation, we propose a novel algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which can effectively handle a broad range of structured nonsmooth nonconvex-nonconcave minimax problems. Specifically, we consider the setting where the primal function has a nonsmooth composite structure and the dual function possesses the Kurdyka-Lojasiewicz (KL) property with exponent $\\theta \\in [0,1)$. We introduce a novel convergence analysis framework for smoothed PLDA, the key components of which are our newly developed nonsmooth primal error bound and dual error bound. Using this framework, we show that smoothed PLDA can find both $\\epsilon$-game-stationary points and $\\epsilon$-optimization-stationary points of the problems of interest in $\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$ iterations. Furthermore, when $\\theta \\in [0,\\frac{1}{2}]$, smoothed PLDA achieves the optimal iteration complexity of $\\mathcal{O}(\\epsilon^{-2})$. To further demonstrate the effectiveness and wide applicability of our analysis framework, we show that certain max-structured problem possesses the KL property with exponent $\\theta=0$ under mild assumptions. As a by-product, we establish algorithm-independent quantitative relationships among various stationarity concepts, which may be of independent interest."}}
