{"id": "UgBYfuBt9c", "cdate": 1695933757316, "mdate": 1695933757316, "content": {"title": "The Dynamics of Functional Diversity throughout Neural Network Training", "abstract": "Deep ensembles offer reduced generalization error and improved predictive uncertainty estimates.\nThese performance gains are attributed to functional diversity among the component models that\nmake up the ensembles: ensemble performance increases with the diversity of the components. A\nstandard way to generate a diversity of components is to train multiple networks on the same data,\nusing different minibatch orders, augmentations, etc. In this work, we focus on how and when this\ntype of diversity in the learned predictor decreases throughout training.\nIn order to study the diversity of networks still accessible via SGD after t iterations, we first train a\nsingle network for t iterations, then duplicate the state of the optimizer and finish the remainder of\ntraining k times, with independent randomness (minibatches, augmentations, etc) for each duplicated\nnetwork. The result is k distinct networks whose training has been coupled for t iterations. We use\nthis methodology\u2014recently exploited for k = 2 to study linear mode connectivity\u2014to construct a\nnovel probe for studying diversity.\nWe find that coupling k for even a few epochs severely restricts the diversity of functions accessible\nby SGD, as measured by the KL divergence between the predicted label distributions as well as the\ncalibration and test error of k-ensembles. We also find that the number of forgetting events [1] drops\noff rapidly.\nThe amount of independent training time decreases with coupling time t however. To control for this\nconfounder, we study extending the number of iterations of high-learning-rate optimization for an\nadditional t iterations post-coupling. We find that this does not restore functional diversity.\nWe also study how functional diversity is affected by retraining after reinitializing the weights in some\nlayers. We find that we recover significantly more diversity by reinitializing layers closer to the input\nlayer, compared to reinitializing layers closer to the output. In this case, we see that reinitialization\nupsets linear mode connectivity. This observation agrees with the performance improvements seen by\narchitectures that share the core of a network but train multiple instantiations of the input layers [2]."}}
{"id": "7nqpV4jrKQr", "cdate": 1632796650409, "mdate": 1632796650409, "content": {"title": "Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data", "abstract": "High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap.\n"}}
{"id": "pt6AOOBqRnp", "cdate": 1632796307524, "mdate": null, "content": {"title": "BindSpace decodes transcription factor binding signals by large-scale sequence embedding", "abstract": "Decoding transcription factor (TF) binding signals in genomic DNA is a fundamental problem. Here we present a prediction model called BindSpace that learns to embed DNA sequences and TF class/family labels into the same space. By training on binding data for hundreds of TFs and embedding over 1M DNA sequences, BindSpace achieves state-of-the-art multiclass binding prediction performance, in vitro and in vivo, and can distinguish signals of closely related TFs."}}
{"id": "KWToR-Phbrz", "cdate": 1601308153489, "mdate": null, "content": {"title": "Beyond Trivial Counterfactual Generations with Diverse Valuable Explanations", "abstract": "Explainability of machine learning models has gained considerable attention within our research community given the importance of deploying more reliable machine-learning systems. Explanability can also be helpful for model debugging. In computer vision applications, most methods explain models by displaying the regions in the input image that they focus on for their prediction, but it is difficult to improve models based on these explanations since they do not indicate why the model fail. Counterfactual methods, on the other hand, indicate how to perturb the input to change the model prediction, providing details about the model's decision-making. Unfortunately, current counterfactual methods make ambiguous interpretations as they combine multiple biases of the model and the data in a single counterfactual interpretation of the model's decision. Moreover, these methods tend to generate trivial counterfactuals about the model's decision, as they often suggest to exaggerate or remove the presence of the attribute being classified. Trivial counterfactuals are usually not valuable, since the information they provide is often already known to the system's designer. In this work, we propose a counterfactual method that learns a perturbation in a disentangled latent space that is constrained using a diversity-enforcing loss to uncover multiple valuable explanations about the model's prediction. Further, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the success rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. We will make the code public."}}
