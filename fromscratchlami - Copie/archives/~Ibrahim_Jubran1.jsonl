{"id": "pgJp7rDc_hk", "cdate": 1663850578702, "mdate": null, "content": {"title": "Coreset for Rational Functions", "abstract": "We consider the problem of fitting a rational function $f:\\mathbb{R}\\to\\mathbb{R}$ to a time-series $g:\\{1,\\cdots,n\\}\\to\\mathbb{R}$. This is by minimizing the sum of distances (loss function) $\\ell(f):=\\sum_{i=1}^n |f(i)-g(i)|$, possibly with additional constraints and regularization terms that may depend on $f$. Our main motivation is to approximate such a time-series by a recursive sequence model $F_n=\\sum_{i=1}^k \\theta_i F_{n-i}$, e.g. a Fibonacci sequence, where $\\theta\\in \\mathbb{R}^k$ are the model parameters, and $k\\geq1$ is constant.\nFor $\\varepsilon\\in(0,1)$, an $\\varepsilon$-coreset for this problem is a small data structure that approximates $\\ell(g)$ up to $1\\pm\\varepsilon$ multiplicative factor, for every rational function $g$ of constant degree.\nWe prove that every signal has an $\\varepsilon$-coreset of size $O(n^{0.001}/\\varepsilon^2)$, and provide a construction algorithm that computes it in $O(n^{1.001})$ time.\nOpen source code is provided, as well as extensive experimental results, on both real and synthetic datasets, which compare our method to existing solvers from Scipy."}}
{"id": "GvU4RvMwlGo", "cdate": 1621629786921, "mdate": null, "content": {"title": "Coresets for Decision Trees of Signals", "abstract": "A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix (2D-signal) into $k\\geq 1$ block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classification loss to a given matrix $D$ of $N$ entries (labels) is the sum of squared differences over every label in $D$ and its assigned label by $t$.\nGiven an error parameter $\\varepsilon\\in(0,1)$, a $(k,\\varepsilon)$-coreset $C$ of $D$ is a small summarization that provably approximates this loss to \\emph{every} such tree, up to a multiplicative factor of $1\\pm\\varepsilon$. In particular, the optimal $k$-tree of $C$ is a $(1+\\varepsilon)$-approximation to the optimal $k$-tree of $D$.\n\nWe provide the first algorithm that outputs such a $(k,\\varepsilon)$-coreset for \\emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial in $k\\log(N)/\\varepsilon$, and its construction takes $O(Nk)$ time.\nThis is by forging a link between decision trees from machine learning -- to partition trees in computational geometry. \n\nExperimental results on \\texttt{sklearn} and \\texttt{lightGBM} show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x$10$, while keeping similar accuracy. Full open source code is provided."}}
{"id": "4JHdr4lgpVT", "cdate": 1621629786921, "mdate": null, "content": {"title": "Coresets for Decision Trees of Signals", "abstract": "A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix (2D-signal) into $k\\geq 1$ block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classification loss to a given matrix $D$ of $N$ entries (labels) is the sum of squared differences over every label in $D$ and its assigned label by $t$.\nGiven an error parameter $\\varepsilon\\in(0,1)$, a $(k,\\varepsilon)$-coreset $C$ of $D$ is a small summarization that provably approximates this loss to \\emph{every} such tree, up to a multiplicative factor of $1\\pm\\varepsilon$. In particular, the optimal $k$-tree of $C$ is a $(1+\\varepsilon)$-approximation to the optimal $k$-tree of $D$.\n\nWe provide the first algorithm that outputs such a $(k,\\varepsilon)$-coreset for \\emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial in $k\\log(N)/\\varepsilon$, and its construction takes $O(Nk)$ time.\nThis is by forging a link between decision trees from machine learning -- to partition trees in computational geometry. \n\nExperimental results on \\texttt{sklearn} and \\texttt{lightGBM} show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x$10$, while keeping similar accuracy. Full open source code is provided."}}
{"id": "VR_TZa1QJFE", "cdate": 1609459200000, "mdate": null, "content": {"title": "Provably Approximated ICP", "abstract": "The goal of the \\emph{alignment problem} is to align a (given) point cloud $P = \\{p_1,\\cdots,p_n\\}$ to another (observed) point cloud $Q = \\{q_1,\\cdots,q_n\\}$. That is, to compute a rotation matrix $R \\in \\mathbb{R}^{3 \\times 3}$ and a translation vector $t \\in \\mathbb{R}^{3}$ that minimize the sum of paired distances $\\sum_{i=1}^n D(Rp_i-t,q_i)$ for some distance function $D$. A harder version is the \\emph{registration problem}, where the correspondence is unknown, and the minimum is also over all possible correspondence functions from $P$ to $Q$. Heuristics such as the Iterative Closest Point (ICP) algorithm and its variants were suggested for these problems, but none yield a provable non-trivial approximation for the global optimum. We prove that there \\emph{always} exists a \"witness\" set of $3$ pairs in $P \\times Q$ that, via novel alignment algorithm, defines a constant factor approximation (in the worst case) to this global optimum. We then provide algorithms that recover this witness set and yield the first provable constant factor approximation for the: (i) alignment problem in $O(n)$ expected time, and (ii) registration problem in polynomial time. Such small witness sets exist for many variants including points in $d$-dimensional space, outlier-resistant cost functions, and different correspondence types. Extensive experimental results on real and synthetic datasets show that our approximation constants are, in practice, close to $1$, and up to x$10$ times smaller than state-of-the-art algorithms."}}
{"id": "uDUyFEjMa5Y", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sets Clustering", "abstract": "The input to the \\emph{sets-$k$-means} problem is an integer $k\\geq 1$ and a set $\\mathcal{P}=\\{P_1,\\cdots,P_n\\}$ of sets in $\\mathbb{R}^d$. The goal is to compute a set $C$ of $k$ centers (points) in $\\mathbb{R}^d$ that minimizes the sum $\\sum_{P\\in \\mathcal{P}} \\min_{p\\in P, c\\in C}\\left\\| p-c \\right\\|^2$ of squared distances to these sets. An \\emph{$\\varepsilon$-core-set} for this problem is a weighted subset of $\\mathcal{P}$ that approximates this sum up to $1\\pm\\varepsilon$ factor, for \\emph{every} set $C$ of $k$ centers in $\\mathbb{R}^d$. We prove that such a core-set of $O(\\log^2{n})$ sets always exists, and can be computed in $O(n\\log{n})$ time, for every input $\\mathcal{P}$ and every fixed $d,k\\geq 1$ and $\\varepsilon \\in (0,1)$. The result easily generalized for any metric space, distances to the power of $z>0$, and M-estimators that handle outliers. Applying an inefficient but optimal algorithm on this coreset allows us to obtain the first PTAS ($1+\\varepsilon$ approximation) for the sets-$k$-means problem that takes time near linear in $n$. This is the first result even for sets-mean on the plane ($k=1$, $d=2$). Open source code and experimental results for document classification and facility locations are also provided."}}
{"id": "Vk-zlpwZ1m", "cdate": 1577836800000, "mdate": null, "content": {"title": "CoBe - Coded Beacons for Localization, Object Tracking, and SLAM Augmentation", "abstract": "This paper presents a novel beacon light coding protocol, which enables fast and accurate identification of the beacons in an image. The protocol is provably robust to a predefined set of detection and decoding errors and does not require any synchronization between the beacons themselves and the optical sensor. A detailed guide is then given for developing an optical tracking and localization system, which is based on the suggested protocol and readily available hardware. Such a system operates either as a standalone system for recovering the six degrees of freedom of fast moving objects, or integrated with existing SLAM pipelines providing them with error-free and easily identifiable landmarks. Based on this guide, we implemented a low-cost positional tracking system which can run in real-time on an IoT board. We evaluate our system's accuracy and compare it to other popular methods which utilize the same optical hardware, in experiments where the ground truth is known. A companion video containing multiple real-world experiments demonstrates the accuracy, speed, and applicability of the proposed system in a wide range of environments and real-world tasks. Open source code is provided to encourage further development of low-cost localization systems integrating the suggested technology at its navigation core."}}
{"id": "C7LMTvPLbzU", "cdate": 1577836800000, "mdate": null, "content": {"title": "Autonomous Toy Drone via Coresets for Pose Estimation", "abstract": "A coreset of a dataset is a small weighted set, such that querying the coreset provably yields a ( 1 + &epsilon; )-factor approximation to the original (full) dataset, for a given family of queries. This paper suggests accurate coresets ( &epsilon; = 0 ) that are subsets of the input for fundamental optimization problems. These coresets enabled us to implement a &ldquo;Guardian Angel&rdquo; system that computes pose-estimation in a rate &gt; 20 frames per second. It tracks a toy quadcopter which guides guests in a supermarket, hospital, mall, airport, and so on. We prove that any set of n matrices in R d &times; d whose sum is a matrix S of rank r, has a coreset whose sum has the same left and right singular vectors as S, and consists of O ( d r ) = O ( d 2 ) matrices, independent of n. This implies the first (exact, weighted subset) coreset of O ( d 2 ) points to problems such as linear regression, PCA/SVD, and Wahba&rsquo;s problem, with corresponding streaming, dynamic, and distributed versions. Our main tool is a novel usage of the Caratheodory Theorem for coresets, an algorithm that computes its set in time that is linear in its cardinality. Extensive experimental results on both synthetic and real data, companion video of our system, and open code are provided."}}
{"id": "2zNyP2kAnF-", "cdate": 1577836800000, "mdate": null, "content": {"title": "Faster PAC Learning and Smaller Coresets via Smoothed Analysis", "abstract": "PAC-learning usually aims to compute a small subset ($\\varepsilon$-sample/net) from $n$ items, that provably approximates a given loss function for every query (model, classifier, hypothesis) from a given set of queries, up to an additive error $\\varepsilon\\in(0,1)$. Coresets generalize this idea to support multiplicative error $1\\pm\\varepsilon$. Inspired by smoothed analysis, we suggest a natural generalization: approximate the \\emph{average} (instead of the worst-case) error over the queries, in the hope of getting smaller subsets. The dependency between errors of different queries implies that we may no longer apply the Chernoff-Hoeffding inequality for a fixed query, and then use the VC-dimension or union bound. This paper provides deterministic and randomized algorithms for computing such coresets and $\\varepsilon$-samples of size independent of $n$, for any finite set of queries and loss function. Example applications include new and improved coreset constructions for e.g. streaming vector summarization [ICML'17] and $k$-PCA [NIPS'16]. Experimental results with open source code are provided."}}
{"id": "ggSurtCtSKk", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fast and Accurate Least-Mean-Squares Solvers", "abstract": "Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression, SVD and Elastic-Net not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as decision trees and matrix factorizations. We suggest an algorithm that gets a finite set of $n$ $d$-dimensional real vectors and returns a weighted subset of $d+1$ vectors whose sum is \\emph{exactly} the same. The proof in Caratheodory's Theorem (1907) computes such a subset in $O(n^2d^2)$ time and thus not used in practice. Our algorithm computes this subset in $O(nd)$ time, using $O(\\log n)$ calls to Caratheodory's construction on small but \"smart\" subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets. As an example application, we show how it can be used to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed (big) data is trivial. Extensive experimental results and complete open source code are also provided."}}
{"id": "Y8U8486PEnL", "cdate": 1546300800000, "mdate": null, "content": {"title": "Introduction to Coresets: Accurate Coresets", "abstract": "A coreset (or core-set) of an input set is its small summation, such that solving a problem on the coreset as its input, provably yields the same result as solving the same problem on the original (full) set, for a given family of problems (models, classifiers, loss functions). Over the past decade, coreset construction algorithms have been suggested for many fundamental problems in e.g. machine/deep learning, computer vision, graphics, databases, and theoretical computer science. This introductory paper was written following requests from (usually non-expert, but also colleagues) regarding the many inconsistent coreset definitions, lack of available source code, the required deep theoretical background from different fields, and the dense papers that make it hard for beginners to apply coresets and develop new ones. The paper provides folklore, classic and simple results including step-by-step proofs and figures, for the simplest (accurate) coresets of very basic problems, such as: sum of vectors, minimum enclosing ball, SVD/ PCA and linear regression. Nevertheless, we did not find most of their constructions in the literature. Moreover, we expect that putting them together in a retrospective context would help the reader to grasp modern results that usually extend and generalize these fundamental observations. Experts might appreciate the unified notation and comparison table that links between existing results. Open source code with example scripts are provided for all the presented algorithms, to demonstrate their practical usage, and to support the readers who are more familiar with programming than math."}}
