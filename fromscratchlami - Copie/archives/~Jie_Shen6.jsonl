{"id": "er_nz4Q9Km7", "cdate": 1663850470167, "mdate": null, "content": {"title": "Fast Yet Effective Graph Unlearning through Influence Analysis", "abstract": "Recent evolving data privacy policies and regulations have led to increasing interest in the machine unlearning problem. In this paper, we consider Graph Neural Networks (GNNs) as the target model, and study the problem of edge unlearning in GNNs, i.e., learning a new GNN model as if a specified set of edges never existed in the original training graph. Despite its practical importance, the problem remains elusive due to the non-convexity nature of GNNs. Our main technical contribution is three-fold: 1) we cast the problem of edge unlearning as estimating the influence functions of the edges to be removed; 2) we design a computationally and memory efficient algorithm named EraEdge for edge influence estimation and unlearning; 3) under standard regularity conditions, we prove that the sequence of iterates produced by our algorithm converges to the desired model. A comprehensive set of experiments on three prominent GNN models and four benchmark graph datasets demonstrate that our algorithm achieves significant speed-up gains over retraining from scratch without sacrificing the model accuracy too much. Furthermore, our algorithm outperforms the existing GNN unlearning approach in terms of both training time and accuracy of the target GNN model."}}
{"id": "PZtIiZ43E2R", "cdate": 1652737346926, "mdate": null, "content": {"title": "List-Decodable Sparse Mean Estimation", "abstract": "Robust mean estimation is one of the most important problems in statistics: given a set of samples in $\\mathbb{R}^d$ where an $\\alpha$ fraction are drawn from some distribution $D$ and the rest are adversarially corrupted, we aim to estimate the mean of $D$. A surge of recent research interest has been focusing on the list-decodable setting where $\\alpha \\in (0, \\frac12]$, and the goal is to output a finite number of estimates among which at least one approximates the target mean. In this paper, we consider that the underlying distribution $D$ is Gaussian with $k$-sparse mean. Our main contribution is the first polynomial-time algorithm that enjoys sample complexity $O\\big(\\mathrm{poly}(k, \\log d)\\big)$, i.e. poly-logarithmic in the dimension. One of our core algorithmic ingredients is using low-degree {\\em sparse polynomials} to filter outliers, which may find more applications."}}
{"id": "ZgLRxqVGvJ4", "cdate": 1620341919966, "mdate": null, "content": {"title": "On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise", "abstract": "We study {\\em online} active learning of homogeneous halfspaces in $\\Rd$ with adversarial noise where the overall probability of a noisy label is constrained to be at most $\\nu$. Our main contribution is a Perceptron-like online active learning algorithm that runs in polynomial time, and under the conditions that the marginal distribution is isotropic log-concave and $\\nu = \\Omega(\\epsilon)$, where $\\epsilon \\in (0, 1)$ is the target error rate, our algorithm PAC learns the underlying halfspace with near-optimal label complexity of $\\tilde{O}\\big(d \\cdot \\polylog(\\frac{1}{\\epsilon})\\big)$ and sample complexity of $\\tilde{O}\\big(\\frac{d}{\\epsilon} \\big)$.\\footnote{We use the notation $\\tilde{O}(f) = O(f \\cdot \\log f)$, $\\tilde{\\Omega}(f) = \\Omega(f / \\log f)$, and $\\tilde{\\Theta}(f)$ that is between $\\tilde{\\Omega}(f)$ and $\\tilde{O}(f)$.} Prior to this work, existing online algorithms designed for tolerating the adversarial noise are  subject to either label complexity polynomial in $\\frac{1}{\\epsilon}$, or suboptimal noise tolerance, or restrictive marginal distributions. With the additional prior knowledge that the underlying halfspace is $s$-sparse, we obtain attribute-efficient label complexity of $\\tilde{O}\\big( s \\cdot \\polylog(d, \\frac{1}{\\epsilon}) \\big)$ and sample complexity of $\\tilde{O}\\big(\\frac{s}{\\epsilon} \\cdot \\polylog(d) \\big)$. As an immediate corollary, we show that under the agnostic model where no assumption is made on the noise rate $\\nu$, our active learner achieves an error rate of $O(\\OPT) + \\epsilon$ with the same running time and label and sample complexity, where $\\OPT$ is the best possible error rate achievable by any homogeneous halfspace."}}
{"id": "MhZWJZcUyY", "cdate": 1620341866571, "mdate": null, "content": {"title": "Sample-Optimal PAC Learning of Halfspaces with Malicious Noise", "abstract": "We study efficient PAC learning of homogeneous halfspaces in $\\Rd$ in the presence of malicious noise of Valiant~(1985). This is a challenging noise model and only until  recently has near-optimal noise tolerance bound been established under the mild condition that the unlabeled data distribution is isotropic log-concave. However, it remains unsettled how to obtain the optimal sample complexity simultaneously. In this work, we present a new analysis for the algorithm of Awasthi et al.~(2017) and show that it essentially achieves the near-optimal sample complexity bound of $\\tilde{O}(d)$, improving the best known result of $\\tilde{O}(d^2)$. Our main ingredient is a novel incorporation of a Matrix Chernoff-type inequality to bound the spectrum of an empirical covariance matrix for well-behaved distributions, in conjunction with a careful exploration of the localization schemes of Awasthi et al.~(2017). We further extend the algorithm and analysis to the more general and stronger nasty noise model of Bshouty~et~al. (2002), showing that it is still possible to achieve near-optimal noise tolerance and sample complexity  in polynomial time."}}
{"id": "xZsypQ93zFS", "cdate": 1609459200000, "mdate": 1651545658600, "content": {"title": "Attribute-Efficient Learning of Halfspaces with Malicious Noise: Near-Optimal Label Complexity and Noise Tolerance", "abstract": "This paper is concerned with computationally efficient learning of homogeneous sparse halfspaces in $\\mathbb{R}^d$ under noise. Though recent works have established attribute-efficient learning alg..."}}
{"id": "sAmqP3HL5j1", "cdate": 1609459200000, "mdate": 1651545658600, "content": {"title": "Sample-Optimal PAC Learning of Halfspaces with Malicious Noise", "abstract": "We study efficient PAC learning of homogeneous halfspaces in $\\mathbb{R}^d$ in the presence of malicious noise of Valiant (1985). This is a challenging noise model and only until recently has near-..."}}
{"id": "_fsh5yVNxro", "cdate": 1609459200000, "mdate": 1651545658635, "content": {"title": "Sample-Optimal PAC Learning of Halfspaces with Malicious Noise", "abstract": "We study efficient PAC learning of homogeneous halfspaces in $\\mathbb{R}^d$ in the presence of malicious noise of Valiant (1985). This is a challenging noise model and only until recently has near-optimal noise tolerance bound been established under the mild condition that the unlabeled data distribution is isotropic log-concave. However, it remains unsettled how to obtain the optimal sample complexity simultaneously. In this work, we present a new analysis for the algorithm of Awasthi et al. (2017) and show that it essentially achieves the near-optimal sample complexity bound of $\\tilde{O}(d)$, improving the best known result of $\\tilde{O}(d^2)$. Our main ingredient is a novel incorporation of a matrix Chernoff-type inequality to bound the spectrum of an empirical covariance matrix for well-behaved distributions, in conjunction with a careful exploration of the localization schemes of Awasthi et al. (2017). We further extend the algorithm and analysis to the more general and stronger nasty noise model of Bshouty et al. (2002), showing that it is still possible to achieve near-optimal noise tolerance and sample complexity in polynomial time."}}
{"id": "OlT_5ZMh8il", "cdate": 1609459200000, "mdate": 1651545658599, "content": {"title": "Semi-verified Learning from the Crowd with Pairwise Comparisons", "abstract": "We study the problem of crowdsourced PAC learning of threshold functions with pairwise comparisons. This is a challenging problem and only recently have query-efficient algorithms been established in the scenario where the majority of the crowd are perfect. In this work, we investigate the significantly more challenging case that the majority are incorrect, which in general renders learning impossible. We show that under the semi-verified model of Charikar~et~al.~(2017), where we have (limited) access to a trusted oracle who always returns the correct annotation, it is possible to PAC learn the underlying hypothesis class while drastically mitigating the labeling cost via the more easily obtained comparison queries. Orthogonal to recent developments in semi-verified or list-decodable learning that crucially rely on data distributional assumptions, our PAC guarantee holds by exploring the wisdom of the crowd."}}
{"id": "7D4QXGQtcTw", "cdate": 1609459200000, "mdate": 1651545658627, "content": {"title": "On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise", "abstract": "We study {\\em online} active learning of homogeneous halfspaces in $\\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy label is constrained to be at most $\\nu$. Our main ..."}}
{"id": "s4D2nnwCcM", "cdate": 1601308076932, "mdate": null, "content": {"title": "Uncertainty-Based Adaptive Learning for Reading Comprehension", "abstract": "Recent years have witnessed a surge of successful applications of machine reading comprehension. Of central importance to the tasks is the availability of massive amount of labeled data, which facilitates the training of large-scale neural networks. However, in many real-world problems, annotated data are expensive to gather not only because of time cost and budget, but also of certain domain-specific restrictions such as privacy for healthcare data. In this regard, we propose an uncertainty-based adaptive learning algorithm for reading comprehension, which interleaves data annotation and model updating to mitigate the demand of labeling. Our key techniques are two-fold: 1) an unsupervised uncertainty-based sampling scheme that queries the labels of the most informative instances with respect to the currently learned model; and 2) an adaptive loss minimization paradigm that simultaneously fits the data and controls the degree of model updating. We demonstrate on the benchmark datasets that 25\\% less labeled samples suffice to guarantee similar, or even improved performance. Our results demonstrate a strong evidence that for label-demanding scenarios, the proposed approach offers a practical guide on data collection and model training. "}}
