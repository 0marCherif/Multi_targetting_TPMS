{"id": "BCOj8CMMHjg", "cdate": 1650547878359, "mdate": null, "content": {"title": "Adversarial Projections to Tackle Support-Query Shifts in Few-Shot Meta-Learning", "abstract": "Popular few-shot Meta-learning (ML) methods presume that a task's support and query data are drawn from a common distribution. A recent work relaxed this assumption to propose a few-shot setting where the support and query distributions differ, with disjoint yet related meta-train and meta-test support-query shifts (SQS). We relax this assumption further to a more pragmatic SQS setting (SQS+) where the meta-test SQS is unknown and need not be related to the meta-train SQS. The state-of-the-art solution to address SQS is transductive, requiring unlabelled meta-test query data to bridge the support and query distribution gap. In contrast, we propose a theoretically grounded inductive solution - Adversarial Query Projection (AQP) for addressing SQS+ and  SQS. AQP can be easily integrated into the popular ML frameworks. Exhaustive empirical investigations on benchmark datasets and their extensions, different ML approaches, and architectures establish AQP's efficacy in handling SQS+ and SQS."}}
{"id": "D380IGEhAnF", "cdate": 1633015334522, "mdate": null, "content": {"title": "Task Attended Meta-Learning for Few-Shot Learning", "abstract": "Meta-learning (ML) has emerged as a promising direction in learning models under constrained resource settings like few-shot learning. The popular approaches for ML either learn a generalizable initial model or a generic parametric optimizer through episodic training. The former approaches leverage the knowledge from a batch of tasks to learn an optimal prior. In this work, we study the importance of tasks in a batch for ML. We hypothesize that the common assumption in batch episodic training where each task in a batch has an equal contribution to learning an optimal meta-model need not be true. We propose to weight the tasks in a batch according to their ``importance\" in improving the meta-model's learning. To this end, we introduce a training curriculum, called task attended meta-training, to weight the tasks in a batch. The task attention is a standalone unit and can be integrated with any batch episodic training regimen. The comparisons of the task-attended ML models with their non-task-attended counterparts on complex datasets like miniImageNet, FC100 and tieredImageNet validate its effectiveness."}}
{"id": "abBmzKqNiEN", "cdate": 1609459200000, "mdate": null, "content": {"title": "Stress Testing of Meta-learning Approaches for Few-shot Learning", "abstract": "Meta-learning (ML) has emerged as a promising learning method under resource constraints such as few-shot learning. ML approaches typically propose a methodology to learn generalizable models. In this work-in-progress paper, we put the recent ML approaches to a stress test to discover their limitations. Precisely, we measure the performance of ML approaches for few-shot learning against increasing task complexity. Our results show a quick degradation in the performance of initialization strategies for ML (MAML, TAML, and MetaSGD), while surprisingly, approaches that use an optimization strategy (MetaLSTM) perform significantly better. We further demonstrate the effectiveness of an optimization strategy for ML (MetaLSTM++) trained in a MAML manner over a pure optimization strategy. Our experiments also show that the optimization strategies for ML achieve higher transferability from simple to complex tasks."}}
{"id": "XGcq5lezkAM", "cdate": 1609459200000, "mdate": null, "content": {"title": "Characterizing GAN Convergence Through Proximal Duality Gap", "abstract": "Despite the accomplishments of Generative Adversarial Networks (GANs) in modeling data distributions, training them remains a challenging task. A contributing factor to this difficulty is the non-intuitive nature of the GAN loss curves, which necessitates a subjective evaluation of the generated output to infer training progress. Recently, motivated by game theory, duality gap has been proposed as a domain agnostic measure to monitor GAN training. However, it is restricted to the setting when the GAN converges to a Nash equilibrium. But GANs need not always converge to a Nash equilibrium to model the data distribution. In this work, we extend the notion of duality gap to proximal duality gap that is applicable to the general context of training GANs where Nash equilibria may not exist. We show theoretically that the proximal duality gap is capable of monitoring the convergence of GANs to a wider spectrum of equilibria that subsumes Nash equilibria. We also theoretically establish the relationship between the proximal duality gap and the divergence between the real and generated data distributions for different GAN formulations. Our results provide new insights into the nature of GAN convergence. Finally, we validate experimentally the usefulness of proximal duality gap for monitoring and influencing GAN training."}}
{"id": "wED32eCeG_3", "cdate": 1577836800000, "mdate": null, "content": {"title": "MAIRE - A Model-Agnostic Interpretable Rule Extraction Procedure for Explaining Classifiers", "abstract": "The paper introduces a novel framework for extracting model-agnostic human interpretable rules to explain a classifier's output. The human interpretable rule is defined as an axis-aligned hyper-cuboid containing the instance for which the classification decision has to be explained. The proposed procedure finds the largest (high \\textit{coverage}) axis-aligned hyper-cuboid such that a high percentage of the instances in the hyper-cuboid have the same class label as the instance being explained (high \\textit{precision}). Novel approximations to the coverage and precision measures in terms of the parameters of the hyper-cuboid are defined. They are maximized using gradient-based optimizers. The quality of the approximations is rigorously analyzed theoretically and experimentally. Heuristics for simplifying the generated explanations for achieving better interpretability and a greedy selection algorithm that combines the local explanations for creating global explanations for the model covering a large part of the instance space are also proposed. The framework is model agnostic, can be applied to any arbitrary classifier, and all types of attributes (including continuous, ordered, and unordered discrete). The wide-scale applicability of the framework is validated on a variety of synthetic and real-world datasets from different domains (tabular, text, and image)."}}
{"id": "r-VYKNl3Ao", "cdate": 1577836800000, "mdate": null, "content": {"title": "Implicit Discriminator in Variational Autoencoder", "abstract": "Recently generative models have focused on combining the advantages of variational autoencoders (VAE) and generative adversarial networks (GAN) for good reconstruction and generative abilities. In this work we introduce a novel hybrid architecture, Implicit Discriminator in Variational Autoencoder (IDVAE), that combines a VAE and a GAN, which does not need an explicit discriminator network. The fundamental premise of the IDVAE architecture is that the encoder of a VAE and the discriminator of a GAN utilize common features and therefore can be trained as a shared network, while the decoder of the VAE and the generator of the GAN can be combined to learn a single network. This results in a simple two-tier architecture that has the properties of both a VAE and a GAN. The qualitative and quantitative experiments on real-world benchmark datasets demonstrate that IDVAE performs better than the state of the art hybrid approaches. We experimentally validate that IDVAE can be easily extended to work in a conditional setting and demonstrate its performance on complex datasets."}}
{"id": "qO_zSIMi8NR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi-Partition Feature Alignment Network for Unsupervised Domain Adaptation", "abstract": "In this paper, we present a novel unsupervised domain adaptation framework, Multi-Partition Feature Alignment Network, that learns a deep neural model for the target domain without the need for any supervision. Recent leading approaches for unsupervised domain adaptation are based on adversarial alignment. Aligning the global distribution of the domain representations via adversarial training does not guarantee the class-wise distribution alignment. The proposed approach is built on adversarial learning with the focus on carefully aligning class-wise domain representations. Our algorithm utilizes the pseudo-labels (the predicted labels) of the target features to stimulate class-wise alignment. As the pseudo-labels of individual target features can be erroneous, instead of iteratively aligning individual target samples, the proposed framework introduces a generic class-specific multi-partition alignment procedure that enables superior class-discriminative alignment of domain representations. The competitive performance of the proposed framework against state-of-the-art approaches over a wide variety of visual recognition tasks, namely, the digits classification task and the object recognition task, validates its effectiveness for unsupervised domain adaptation."}}
{"id": "DtTwyWz9zEc", "cdate": 1577836800000, "mdate": null, "content": {"title": "MACE: Model Agnostic Concept Extractor for Explaining Image Classification Networks", "abstract": "Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pre-trained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a Model Agnostic Concept Extractor, which can explain the working of a convolutional network through smaller concepts. The MACE framework dissects the feature maps generated by a convolution network for an image to extract concept based prototypical explanations. Further, it estimates the relevance of the extracted concepts to the pre-trained model's predictions, a critical aspect required for explaining the individual class predictions, missing in existing approaches. We validate our framework using VGG16 and ResNet50 CNN architectures, and on datasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments demonstrate that the concepts extracted by the MACE framework increase the human interpretability of the explanations, and are faithful to the underlying pre-trained black-box model."}}
{"id": "0Tgsvent0sz", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Duality Gap as a Measure for Monitoring GAN Training", "abstract": "Generative adversarial network (GAN) is among the most popular deep learning models for learning complex data distributions. However, training a GAN is known to be a challenging task. This is often attributed to the lack of correlation between the training progress and the trajectory of the generator and discriminator losses and the need for the GAN's subjective evaluation. A recently proposed measure inspired by game theory - the duality gap, aims to bridge this gap. However, as we demonstrate, the duality gap's capability remains constrained due to limitations posed by its estimation process. This paper presents a theoretical understanding of this limitation and proposes a more dependable estimation process for the duality gap. At the crux of our approach is the idea that local perturbations can help agents in a zero-sum game escape non-Nash saddle points efficiently. Through exhaustive experimentation across GAN models and datasets, we establish the efficacy of our approach in capturing the GAN training progress with minimal increase to the computational complexity. Further, we show that our estimate, with its ability to identify model convergence/divergence, is a potential performance measure that can be used to tune the hyperparameters of a GAN."}}
{"id": "pZFKrahQlSt", "cdate": 1546300800000, "mdate": null, "content": {"title": "Supervised heterogeneous feature transfer via random forests", "abstract": "Transfer learning across heterogeneous feature spaces can, in general, be a very difficult problem in practice due to the heterogeneity of features and lack of correspondence between data points of different domains. In this paper, we present a novel supervised domain adaptation algorithm (SHDA-RF) that transfers knowledge from a data-rich source domain to a target domain with only few training instances. The proposed method makes use of random forests to identify pivot features that bridge the two domains. The key idea of the proposed feature transfer approach is that every path in a decision tree leading to a partition of the data is associated with a certain label distribution and the label distributions that appear both in the source and target random forest models can be used as pivots for bridging the two domains. This information is used to generate a sparse feature transformation matrix, which maps patterns from the source feature space to the target feature space. The target model is then retrained along with the projected source data. We conduct extensive experiments on diverse datasets of varying dimensions and sparsity to verify the superiority of the proposed approach over other baseline and state of the art transfer approaches."}}
