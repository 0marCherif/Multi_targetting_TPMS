{"id": "KJFpArxWe-g", "cdate": 1664194167338, "mdate": null, "content": {"title": "Towards Architectural Optimization of Equivariant Neural Networks over Subgroups", "abstract": "Incorporating equivariance to symmetry groups in artificial neural networks (ANNs) can improve performance on tasks exhibiting those symmetries, but such symmetries are often only approximate and not explicitly known. This motivates algorithmically optimizing the architectural constraints imposed by equivariance. We propose the equivariance relaxation morphism, which preserves functionality while reparameterizing a group equivariant layer to operate with equivariance constraints on a subgroup, and the $[G]$-mixed equivariant layer, which mixes operations constrained to equivariance to different groups to enable within-layer equivariance optimization. These two architectural tools can be used within neural architecture search (NAS) algorithms for equivariance-aware architectural optimization."}}
{"id": "a6rCdfABJXg", "cdate": 1663850364078, "mdate": null, "content": {"title": "Equivariance-aware Architectural Optimization of Neural Networks", "abstract": "Incorporating equivariance to symmetry groups as a constraint during neural network training can improve performance and generalization for tasks exhibiting those symmetries, but such symmetries are often not perfectly nor explicitly present. This motivates algorithmically optimizing the architectural constraints imposed by equivariance. We propose the equivariance relaxation morphism, which preserves functionality while reparameterizing a group equivariant layer to operate with equivariance constraints on a subgroup, as well as the $[G]$-mixed equivariant layer, which mixes layers constrained to different groups to enable within-layer equivariance optimization. We further present evolutionary and differentiable neural architecture search (NAS) algorithms that utilize these mechanisms respectively for equivariance-aware architectural optimization. Experiments across a variety of datasets show the benefit of dynamically constrained equivariance to find effective architectures with approximate equivariance."}}
{"id": "lxpXEuDTdw", "cdate": 1660643204134, "mdate": 1660643204134, "content": {"title": "Structural Learning in Artificial Neural Networks: A Neural Operator Perspective", "abstract": "Over the history of Artificial Neural Networks (ANNs), only a minority of algorithms integrate structural changes of the network architecture into the learning process. Modern neuroscience has demonstrated that structural change is an important part of biological learning, with mechanisms such as synaptogenesis and neurogenesis present even in adult brains. Despite this history of artificial methods and biological inspiration, and furthermore the recent resurgence of neural methods in deep learning, relatively few current ANN methods include structural changes in learning compared to those that only adjust synaptic weights during the training process. We aim to draw connections between different approaches of structural learning that have similar abstractions in order to encourage collaboration and development. In this review, we provide a survey on structural learning methods in deep ANNs, including a new neural operator framework from a cellular neuroscience context and perspective, aimed at motivating research on this challenging topic. We then provide an overview of ANN methods which include structural changes within the neural operator framework in the learning process, characterizing each neural operator in detail and drawing connections to their biological counterparts. Finally, we present overarching trends in how these operators are implemented and discuss the open challenges in structural learning in ANNs."}}
{"id": "SWOg-arIg9", "cdate": 1645792504309, "mdate": null, "content": {"title": "When, where, and how to add new neurons to ANNs", "abstract": "Neurogenesis in ANNs is an understudied and difficult problem, even compared to other forms of structural learning like pruning. By decomposing it into triggers and initializations, we introduce a framework for studying the various facets of neurogenesis: when, where, and how to add neurons during the learning process. We present the Neural Orthogonality (NORTH*) suite of neurogenesis strategies, combining layer-wise triggers and initializations based on the orthogonality of activations or weights to dynamically grow performant networks that converge to an efficient size. We evaluate our contributions against other recent neurogenesis works across a variety of supervised learning tasks."}}
{"id": "3SKN-Rbp7ri", "cdate": 1609459200000, "mdate": 1633020583663, "content": {"title": "On Constrained Optimization in Differentiable Neural Architecture Search", "abstract": "Differentiable Architecture Search (DARTS) is a recent neural architecture search (NAS) method based on a differentiable relaxation. Due to its success, numerous variants analyzing and improving parts of the DARTS framework have recently been proposed. By considering the problem as a constrained bilevel optimization, we present and analyze DARTS-PRIME, a variant including improvements to architectural weight update scheduling and regularization towards discretization. We propose a dynamic schedule based on per-minibatch network information to make architecture updates more informed, as well as proximity regularization to promote well-separated discretization. Our results in multiple domains show that DARTS-PRIME improves both performance and reliability, comparable to state-of-the-art in differentiable NAS."}}
{"id": "-HwzT_lIT3e", "cdate": 1546300800000, "mdate": 1633020583664, "content": {"title": "Implementing evolutionary optimization to model neural functional connectivity", "abstract": "Computational models are crucial in understanding brain function. Their architecture is designed to replicate known brain structures, and the behavior that emerges is then compared to observed fMRI and other imaging techniques. As the models become more complex with more parameters, they can explain more of the observed phenomena, and may eventually be used for diagnosis and design of treatments of brain disorders. However, those parameters need to be carefully optimized for the models to work, which becomes intractable as the models grow. In this preliminary work, CMA-ES has been configured to optimize continuous parameters of a functional connectivity model, resulting in a better fit to empirical data than manually selected parameters in all trial runs. This approach will be combined with other EC techniques to optimize other parameters. The techniques will be scaled up to more detailed structural and functional data and local parameters."}}
{"id": "pw-5MX_wiPA", "cdate": 1483228800000, "mdate": 1633020583662, "content": {"title": "Effects of Soft Drinks on Resting State EEG and Brain-Computer Interface Performance", "abstract": "Motor imagery (MI)-based brain-computer interface (BCI) using electroencephalography (EEG) allows users to directly control a computer or external device by modulating and decoding the brain waves. A variety of factors could potentially affect the performance of BCI such as the health status of subjects or the environment. In this paper, we investigated the effects of soft drinks and regular coffee on EEG signals under resting state and on the performance of MI-based BCI. Twenty-six healthy human subjects participated in three or four BCI sessions with a resting period in each session. During each session, the subjects drank an unlabeled soft drink with either sugar (Caffeine Free Coca-Cola), caffeine (Diet Coke), neither ingredient (Caffeine Free Diet Coke), or a regular coffee if there was a fourth session. The resting state spectral power in each condition was compared; the analysis showed that power in alpha and beta band after caffeine consumption were decreased substantially compared with control and sugar condition. Although the attenuation of powers in the frequency range used for the online BCI control signal was shown, group averaged BCI online performance after consuming caffeine was similar to those of other conditions. This paper, for the first time, shows the effect of caffeine, sugar intake on the online BCI performance, and resting state brain signal."}}
