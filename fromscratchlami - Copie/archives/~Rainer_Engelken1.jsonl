{"id": "r6_zHM2POTd", "cdate": 1652737673345, "mdate": null, "content": {"title": "A time-resolved theory of information encoding in recurrent neural networks", "abstract": "Information encoding in neural circuits depends on how well time-varying stimuli are encoded by neural populations.\nSlow neuronal timescales, noise and network chaos can compromise reliable and rapid population response to external stimuli.\nA dynamic balance of externally incoming currents by strong recurrent inhibition was previously proposed as a mechanism to accurately and robustly encode a time-varying stimulus in balanced networks of binary neurons, but a theory for recurrent rate networks was missing. \nHere, we develop a non-stationary dynamic mean-field theory that transparently explains how a tight balance of excitatory currents by recurrent inhibition improves information encoding. We demonstrate that the mutual information rate of a time-varying input increases linearly with the tightness of balance, both in the presence of additive noise and with recurrently generated chaotic network fluctuations. We corroborated our findings in numerical experiments and demonstrated that recurrent networks with positive firing rates trained to transmit a time-varying stimulus generically use recurrent inhibition to increase the information rate. We also found that networks trained to transmit multiple independent time-varying signals spontaneously form multiple local inhibitory clusters, one for each input channel.\nOur findings suggest that feedforward excitatory input and local recurrent inhibition - as observed in many biological circuits - is a generic circuit motif for encoding and transmitting time-varying information in recurrent neural circuits."}}
{"id": "iV2qc0fPJGI", "cdate": 1640995200000, "mdate": 1681483804934, "content": {"title": "Input correlations impede suppression of chaos and learning in balanced rate networks", "abstract": ""}}
{"id": "8rAesj46Nd", "cdate": 1640995200000, "mdate": 1681483804933, "content": {"title": "Curriculum learning as a tool to uncover learning principles in the brain", "abstract": ""}}
{"id": "TpJMvo0_pu-", "cdate": 1632875552832, "mdate": null, "content": {"title": "Curriculum learning as a tool to uncover learning principles in the brain ", "abstract": "We present a novel approach to use curricula to identify principles by which a system learns. Previous work in curriculum learning has focused on how curricula can be designed to improve learning of a model on particular tasks. We consider the inverse problem: what can a curriculum tell us about how a learning system acquired a task? Using recurrent neural networks (RNNs) and models of common experimental neuroscience tasks, we demonstrate that curricula can be used to differentiate learning principles using target-based and a representation-based loss functions as use cases. In particular, we compare the performance of RNNs using target-based learning rules versus those using representational learning rules on three different curricula in the context of two tasks. We show that the learned state-space trajectories of RNNs trained by these two learning rules under all curricula tested are indistinguishable. However, by comparing learning times during different curricula, we can disambiguate the learning rules and challenge traditional approaches of interrogating learning systems. Although all animals in neuroscience lab settings are trained by curriculum-based procedures called shaping, almost no behavioral or neural data are collected or published on the relative successes or training times under different curricula. Our results motivate the systematic collection and curation of data during shaping by demonstrating curriculum learning in RNNs as a tool to probe and differentiate learning principles used by biological systems, over conventional statistical analyses of learned state spaces."}}
{"id": "HpjhBSIcb8", "cdate": 1599732216505, "mdate": null, "content": {"title": "Lyapunov spectra of chaotic recurrent neural networks", "abstract": "Brains process information through the collective dynamics of large neural networks. Collective chaos was suggested to underlie the complex ongoing dynamics observed in cerebral cortical circuits and determine the impact and processing of incoming information streams. In dissipative systems, chaotic dynamics takes place on a subset of phase space of reduced dimensionality and is organized by a complex tangle of stable, neutral and unstable manifolds. Key topological invariants of this phase space structure such as attractor dimension, and Kolmogorov-Sinai entropy so far remained elusive.\nHere we calculate the complete Lyapunov spectrum of recurrent neural networks. We show that chaos in these networks is extensive with a size-invariant Lyapunov spectrum and characterized by attractor dimensions much smaller than the number of phase space dimensions. We find that near the onset of chaos, for very intense chaos, and discrete-time dynamics, random matrix theory provides analytical approximations to the full Lyapunov spectrum. We show that a generalized time-reversal symmetry of the network dynamics induces a point-symmetry of the Lyapunov spectrum reminiscent of the symplectic structure of chaotic Hamiltonian systems. Fluctuating input reduces both the entropy rate and the attractor dimension. For trained recurrent networks, we find that Lyapunov spectrum analysis provides a quantification of error propagation and stability achieved. Our methods apply to systems of arbitrary connectivity, and we describe a comprehensive set of controls for the accuracy and convergence of Lyapunov exponents.\nOur results open a novel avenue for characterizing the complex dynamics of recurrent neural networks and the geometry of the corresponding chaotic attractors. They also highlight the potential of Lyapunov spectrum analysis as a diagnostic for machine learning applications of recurrent networks. "}}
{"id": "LD0Y40FpFq0", "cdate": 1590398002296, "mdate": null, "content": {"title": "Chaotic Neural Circuit Dynamics", "abstract": "Information is processed in the brain by the coordinated activity of large neural circuits. Yet, we are still only starting to understand how this high-dimensional complex system gives rise to functions such as processing sensory information, making decisions and controlling behavior. Technological advances such as optogenetics and cellular resolution imaging provide tools to measure and manipulate the activity of many neurons simultaneously. These developments open novel avenues for the interplay of theory and experiment in neuroscience and foster the development of mathematical approaches for the systematic dissection and understanding of cortical information processing. This will undoubtedly allow more systematic and comprehensive insights into the brain's structure, function, dynamics, and plasticity. But given the complexity of neural network dynamics, it is not yet clear to what extent this will also give rise to a better conceptual and quantitative understanding of principles underlying neural circuit information processing. Depending on the specific question, we might need a diversity of theoretical concepts and perspectives. Among these are both mechanistic bottom-up approaches which assemble simplified well-understood units into circuits giving rise to less-understood network dynamics and normative top-down approaches, starting for example from information theoretic, geometric or evolutionary constraints to infer how computations should be performed. How information is encoded, processed and transmitted by neural circuits is intimately related to their collective network dynamics. Therefore, it is desirable to better understand how different factors shape the patterns of activity across neural populations. Prominent factors that shape circuit dynamics include single-cell properties, synaptic features, network topology and external input statistics. In this thesis, we develop novel numerical and analytical techniques from dynamical systems, stochastic processes and information theory to characterize the evoked and spontaneous dynamics and phase space organization of large neural circuit models. Our target is to determine how biophysical properties of neurons and network parameters influence information transmission. We investigate the role and relevance of single-cell properties in the collective network dynamics and study how the statistics of external input spike trains affect the chaoticity and reliability of balanced target circuits. By varying the statistics of the streams of input spike trains and investigating the scaling of properties of the collective dynamics with different network parameters, we identify key parameters that regulate information transmission and the ability to control the activity states in a driven network.\n\nKeywords: theoretical neuroscience; network dynamics; random dynamical systems; dynamical systems; information theory; chaos; rate network; spiking network; network state control; suppression of chaos; neural control; cortical dynamics; attractor dimension; Kolmogorov-Sinai entropy; action potential onset dynamics; balanced state; rate chaos; Lyapunov spectrum; covariant Lyapunov vectors; chaos control; reliability; balanced network"}}
{"id": "xfNiIMrBl5", "cdate": 1590395334705, "mdate": null, "content": {"title": "The Transition to Control in Spiking Networks", "abstract": "How well streams of spikes from one circuit can control spiking dynamics in a subsequent circuit constrains its ability to encode and process information. It is currently not well understood how the statistics of the input and biophysical features of single neurons controls the recurrent dynamics of a target network. We demonstrate that streams of input spike trains suppress chaos in the dynamics of balanced circuits of neurons with adjustable spike mechanism. For sufficiently strong input, we find a transition towards complete network control, where the network state is independent of initial conditions. Fast spike onset of single neurons in the target network facilitates both control by external input and suppression of chaos. Our work opens a novel avenue to investigate the role of sensory streams of spike trains in shaping the dynamics of large neural networks."}}
