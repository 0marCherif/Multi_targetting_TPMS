{"id": "UeYQXtI7nsX", "cdate": 1665069646131, "mdate": null, "content": {"title": "Generating Intuitive Fairness Specifications for Natural Language Processing", "abstract": "Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation.  These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in toxicity classification. We also show how limited amounts of human feedback can be leveraged to learn a similarity specification."}}
{"id": "N_g8TT9Cy7f", "cdate": 1663850582549, "mdate": null, "content": {"title": "Human-Guided Fair Classification for Natural Language Processing", "abstract": "Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models. "}}
{"id": "w-HjafJkpN6", "cdate": 1640995200000, "mdate": 1681198855453, "content": {"title": "Human-Guided Fair Classification for Natural Language Processing", "abstract": ""}}
{"id": "zlj3SgVSMP", "cdate": 1609459200000, "mdate": 1681652101801, "content": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency", "abstract": ""}}
{"id": "LTzSK9nmf9", "cdate": 1609459200000, "mdate": 1681652101799, "content": {"title": "Algorithmic collusion: A critical review", "abstract": ""}}
{"id": "_QdvdkxOii6", "cdate": 1601308073970, "mdate": null, "content": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level."}}
