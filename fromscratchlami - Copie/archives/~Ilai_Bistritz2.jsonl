{"id": "r6e6zGhQnS", "cdate": 1672531200000, "mdate": 1681650059181, "content": {"title": "Equilibrium Bandits: Learning Optimal Equilibria of Unknown Dynamics", "abstract": ""}}
{"id": "pZ4AKL7DOq", "cdate": 1672531200000, "mdate": 1681650059158, "content": {"title": "Personalized and Energy-Efficient Health Monitoring: A Reinforcement Learning Approach", "abstract": ""}}
{"id": "bVrIoN2LU-_", "cdate": 1640995200000, "mdate": 1681650059076, "content": {"title": "Informational Cascades With Nonmyopic Agents", "abstract": ""}}
{"id": "OmmOgUrcxIo", "cdate": 1640995200000, "mdate": 1681650059132, "content": {"title": "Smart Greedy Distributed Energy Allocation: A Random Games Approach", "abstract": ""}}
{"id": "yme4Zy_NYY", "cdate": 1609459200000, "mdate": 1681650059296, "content": {"title": "Wireless Body Area Network Control Policies for Energy-Efficient Health Monitoring", "abstract": ""}}
{"id": "ljp2wLsm8f", "cdate": 1609459200000, "mdate": 1681650059023, "content": {"title": "Online Learning for Load Balancing of Unknown Monotone Resource Allocation Games", "abstract": ""}}
{"id": "ZI0nvWzdJO", "cdate": 1609459200000, "mdate": 1681650059174, "content": {"title": "Consensus-Based Stochastic Control for Model-Free Cell Balancing", "abstract": ""}}
{"id": "JCbwYdPEc7S", "cdate": 1609459200000, "mdate": null, "content": {"title": "No Discounted-Regret Learning in Adversarial Bandits with Delays", "abstract": "Consider a scenario where a player chooses an action in each round $t$ out of $T$ rounds and observes the incurred cost after a delay of $d_{t}$ rounds. The cost functions and the delay sequence are chosen by an adversary. We show that in a non-cooperative game, the expected weighted ergodic distribution of play converges to the set of coarse correlated equilibria if players use algorithms that have \"no weighted-regret\" in the above scenario, even if they have linear regret due to too large delays. For a two-player zero-sum game, we show that no weighted-regret is sufficient for the weighted ergodic average of play to converge to the set of Nash equilibria. We prove that the FKM algorithm with $n$ dimensions achieves an expected regret of $O\\left(nT^{\\frac{3}{4}}+\\sqrt{n}T^{\\frac{1}{3}}D^{\\frac{1}{3}}\\right)$ and the EXP3 algorithm with $K$ arms achieves an expected regret of $O\\left(\\sqrt{\\log K\\left(KT+D\\right)}\\right)$ even when $D=\\sum_{t=1}^{T}d_{t}$ and $T$ are unknown. These bounds use a novel doubling trick that, under mild assumptions, provably retains the regret bound for when $D$ and $T$ are known. Using these bounds, we show that FKM and EXP3 have no weighted-regret even for $d_{t}=O\\left(t\\log t\\right)$. Therefore, algorithms with no weighted-regret can be used to approximate a CCE of a finite or convex unknown game that can only be simulated with bandit feedback, even if the simulation involves significant delays."}}
{"id": "Djc5zyGVYB", "cdate": 1609459200000, "mdate": 1632955599409, "content": {"title": "One for All and All for One: Distributed Learning of Fair Allocations With Multi-Player Bandits", "abstract": "Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, represented as an N\u00d7M matrix. These utilities are unknown to the players. In each turn, players select an arm and receive a noisy observation of their utility for it. However, if any other players selected the same arm in that turn, all colliding players will receive zero utility due to the conflict. No communication between the players is possible. We propose two distributed algorithms which learn fair matchings between players and arms while minimizing the regret. We show that our first algorithm learns a max-min fairness matching with near- O(logT) regret (up to a loglogT factor). However, if one has a known target Quality of Service (QoS) (which may vary between players) then we show that our second algorithm learns a matching where all players obtain an expected reward of at least their QoS with constant regret, given that such a matching exists. In particular, if the max-min value is known, a max-min fairness matching can be learned with O(1) regret."}}
{"id": "7ONnnMQhjV5", "cdate": 1609459200000, "mdate": null, "content": {"title": "Game of Thrones: Fully Distributed Learning for Multiplayer Bandits", "abstract": "We consider an N-player multiarmed bandit game in which each player chooses one out of M arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are..."}}
