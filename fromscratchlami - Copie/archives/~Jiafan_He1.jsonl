{"id": "uatykDbeuQ", "cdate": 1676827094764, "mdate": null, "content": {"title": "Uniform-PAC Guarantees for Model-Based RL with Bounded Eluder Dimension", "abstract": "Recently, there has been remarkable progress in reinforcement learning (RL) with general function approximation. However, all these works only provide regret or sample complexity guarantees. It is still an open question if one can achieve stronger performance guarantees, i.e., the uniform probably approximate correctness (Uniform-PAC) guarantee that can imply both a sub-linear regret bound and a polynomial sample complexity for any target learning accuracy.  We study this problem by proposing algorithms for both nonlinear bandits and model-based episodic RL using the general function class with a bounded eluder dimension. The key idea of the proposed algorithms is to assign each action to different levels according to its width with respect to the confidence set. The achieved uniform-PAC sample complexity is tight in the sense that it matches the state-of-the-art regret bounds or sample complexity guarantees when reduced to the linear case. To the best of our knowledge, this is the first work for uniform-PAC guarantees on bandit and RL that goes beyond linear cases."}}
{"id": "3qWdwBUWTM", "cdate": 1676827093036, "mdate": null, "content": {"title": "Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL", "abstract": "The success of deep reinforcement learning (DRL) lies in its ability to learn a representation that is well-suited for the exploration and exploitation task. To understand how the choice of representation can improve the efficiency of reinforcement learning (RL), we study representation selection for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose an efficient algorithm, called ReLEX, for representation learning in both online and offline RL. Specifically, we show that the online version of ReLEX, calledReLEX-UCB, always performs no worse than the state-of-the-art algorithm without representation selection, and achieves a strictly better constant regret if the representation function class has a \"coverage\" property over the entire state-action space. For the offline counterpart, ReLEX-LCB, we show that the algorithm can find the optimal policy if the representation class can cover the state-action space and achieves gap-dependent sample complexity. This is the first result with constant sample complexity for representation learning in offline RL."}}
{"id": "fySLokohvj4", "cdate": 1663850214867, "mdate": null, "content": {"title": "Bandit Learning with General Function Classes: Heteroscedastic Noise and Variance-dependent Regret Bounds", "abstract": "We consider learning a stochastic bandit model, where the reward function belongs to a general class of uniformly bounded functions, and the additive noise can be heteroscedastic. Our model captures contextual linear bandits and generalized linear bandits as special cases. While previous works (Kirschner and Krause, 2018; Zhou et al., 2021) based on weighted ridge regression can deal with linear bandits with heteroscedastic noise, they are not directly applicable to our general model due to the curse of nonlinearity. In order to tackle this problem, we propose a \\emph{multi-level learning} framework for the general bandit model. The core idea of our framework is to partition the observed data into different levels according to the variance of their respective reward and perform online learning at each level collaboratively. Under our framework, we first design an algorithm that constructs the variance-aware confidence set based on empirical risk minimization and prove a variance-dependent regret bound. For generalized linear bandits, we further propose an algorithm based on follow-the-regularized-leader (FTRL) subroutine and online-to-confidence-set conversion, which can achieve a tighter variance-dependent regret under certain conditions."}}
{"id": "bHpOeIXvSX2", "cdate": 1663850207003, "mdate": null, "content": {"title": "On the Interplay Between Misspecification and Sub-optimality Gap: From Linear Contextual Bandits to Linear MDPs", "abstract": "We study linear contextual bandits in the misspecified setting, where the expected reward function can be approximated by a linear function class up to a bounded misspecification level $\\zeta>0$. We propose an algorithm based on a novel data selection scheme, which only selects the contextual vectors with large uncertainty for online regression. We show that, when the misspecification level $\\zeta$ is dominated by $\\tilde O(\\Delta / \\sqrt{d})$ with $\\Delta$ being the minimal sub-optimality gap and $d$ being the dimension of the contextual vectors, our algorithm enjoys the same gap-dependent regret bound $\\tilde O ({d^2} /{\\Delta})$ as in the well-specified setting up to logarithmic factors. Together with a lower bound adapted from Du et al. (2019); Lattimore et al.(2020), our result suggests an interplay between misspecification level and the sub-optimality gap: (1) the linear contextual bandit model is efficiently learnable when $\\zeta \\leq \\tilde O({\\Delta} / \\sqrt{d})$; and (2) it is not efficiently learnable when $\\zeta \\geq \\tilde \\Omega({\\Delta} / {\\sqrt{d}})$. We also extend our algorithm to reinforcement learning with linear Markov decision processes (linear MDPs), and obtain a parallel result of gap-dependent regret. Experiments on both synthetic and real-world datasets corroborate our theoretical results."}}
{"id": "YeuBRKq_yZ-", "cdate": 1652737577668, "mdate": null, "content": {"title": "Nearly Optimal Algorithms for Linear Contextual Bandits with Adversarial Corruptions", "abstract": "We study the linear contextual bandit problem in the presence of adversarial corruption, where the reward at each round is corrupted by an adversary, and the corruption level (i.e., the sum of corruption magnitudes over the horizon) is $C\\geq 0$. The best-known algorithms in this setting are limited in that they either are computationally inefficient or require a strong assumption on the corruption, or their regret is at least $C$ times worse than the regret without corruption. In this paper, to overcome these limitations, we propose a new algorithm based on the principle of optimism in the face of uncertainty. At the core of our algorithm is a weighted ridge regression where the weight of each chosen action depends on its confidence up to some threshold. We show that for both known $C$ and unknown $C$ cases, our algorithm with proper choice of hyperparameter achieves a regret that nearly matches the lower bounds. Thus, our algorithm is nearly optimal up to logarithmic factors for both cases. Notably, our algorithm achieves the near-optimal regret for both corrupted and uncorrupted cases ($C=0$) simultaneously."}}
{"id": "Fx7oXUVEPW", "cdate": 1652737483227, "mdate": null, "content": {"title": "A Simple and Provably Efficient Algorithm for Asynchronous Federated Contextual Linear Bandits", "abstract": "We study federated contextual linear bandits, where $M$ agents cooperate with each other to solve a global contextual linear bandit problem with the help of a central server. We consider the asynchronous setting, where all agents work independently and the communication between one agent and the server will not trigger other agents' communication. We propose a simple algorithm named FedLinUCB based on the principle of optimism. We prove that the regret of FedLinUCB is bounded by $\\widetilde{\\mathcal{O}}(d\\sqrt{\\sum_{m=1}^M T_m})$ and the communication complexity is $\\widetilde{O}(dM^2)$, where $d$ is the dimension of the contextual vector and $T_m$ is the total number of interactions with the environment by agent $m$. To the best of our knowledge, this is the first provably efficient algorithm that allows fully asynchronous communication for federated linear bandits, while achieving the same regret guarantee as in the single-agent setting. "}}
{"id": "adjl32ogfqD", "cdate": 1632875600080, "mdate": null, "content": {"title": "Learning Stochastic Shortest Path with Linear Function Approximation", "abstract": "We study the stochastic shortest path (SSP) problem in reinforcement learning with linear function approximation, where the transition kernel is represented as a linear mixture of unknown models. We call this class of SSP problems as linear mixture SSP. We propose a novel algorithm for learning the linear mixture SSP, which can attain a $\\tilde O(dB_{\\star}^{1.5}\\sqrt{K/c_{\\min}})$ regret. Here $K$ is the number of episodes, $d$ is the dimension of the feature mapping in the mixture model, $B_{\\star}$ bounds the expected cumulative cost of the optimal policy, and $c_{\\min}>0$ is the lower bound of the cost function. Our algorithm also applies to the case when $c_{\\min} = 0$, where a $\\tilde O(K^{2/3})$ regret is guaranteed. To the best of our knowledge, this is the first algorithm with a sublinear regret guarantee for learning linear mixture SSP. In complement to the regret upper bounds, we also prove a lower bound of $\\Omega(dB_{\\star} \\sqrt{K})$, which nearly matches our upper bound."}}
{"id": "9lwprXiGdR4", "cdate": 1621630172366, "mdate": null, "content": {"title": "Nearly Minimax Optimal Reinforcement Learning for Discounted MDPs", "abstract": " We study the reinforcement learning problem for discounted Markov Decision Processes (MDPs) under the tabular setting. We propose a model-based algorithm named UCBVI-$\\gamma$, which is based on the \\emph{optimism in the face of uncertainty principle} and the Bernstein-type bonus. We show that UCBVI-$\\gamma$ achieves an $\\tilde{O}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$ regret, where $S$ is the number of states, $A$ is the number of actions, $\\gamma$ is the discount factor and $T$ is the number of steps. In addition,  we construct a class of hard MDPs and show that for any algorithm, the expected regret is at least $\\tilde{\\Omega}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$. Our upper bound matches the minimax lower bound up to logarithmic factors, which suggests that UCBVI-$\\gamma$ is nearly minimax optimal for discounted MDPs."}}
{"id": "ZOeN0pU8jae", "cdate": 1621630043941, "mdate": null, "content": {"title": "Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation", "abstract": "We study reinforcement learning (RL) with linear function approximation. Existing algorithms for this problem only have high-probability regret and/or Probably Approximately Correct (PAC) sample complexity guarantees, which cannot guarantee the convergence to the optimal policy. In this paper, in order to overcome the limitation of existing algorithms, we propose a new algorithm called FLUTE, which enjoys uniform-PAC convergence to the optimal policy with high probability. The uniform-PAC guarantee is the strongest possible guarantee for reinforcement learning in the literature, which can directly imply both PAC and high probability regret bounds, making our algorithm superior to all existing algorithms with linear function approximation. At the core of our algorithm is a novel minimax value function estimator and a multi-level partition scheme to select the training samples from historical observations. Both of these techniques are new and of independent interest. "}}
{"id": "7BCtqGdWPaq", "cdate": 1546300800000, "mdate": null, "content": {"title": "Achieving a Fairer Future by Changing the Past", "abstract": "We study the problem of allocating T indivisible items that arrive online to agents with additive valuations. The allocation must satisfy a prominent fairness notion, envy-freeness up to one item (EF1), at each round. To make this possible, we allow the reallocation of previously allocated items, but aim to minimize these so-called adjustments. For the case of two agents, we show that algorithms that are informed about the values of future items can get by without any adjustments, whereas uninformed algorithms require Theta(T) adjustments. For the general case of three or more agents, we prove that even informed algorithms must use Omega(T) adjustments, and design an uninformed algorithm that requires only O(T^(3/2))."}}
