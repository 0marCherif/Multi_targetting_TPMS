{"id": "I1Cqn2xEItf", "cdate": 1668636621891, "mdate": 1668636621891, "content": {"title": "mSLAM: Massively multilingual joint pre-training for speech and text", "abstract": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research."}}
{"id": "PlFtf_pnkZu", "cdate": 1632875661767, "mdate": null, "content": {"title": "Examining Scaling and Transfer of Language Model Architectures for Machine Translation", "abstract": "Natural language understanding and generation models follow one of the two dominant architectural paradigms: language models (LMs) that process concatenated sequences in a single stack of layers, and encoder-decoder models (EncDec) that utilize separate layer stacks for input and output processing. In machine translation, EncDec has long been the favoured approach, but with few studies investigating the performance of LMs. In this work, we thoroughly examine the role of several architectural design choices on the performance of LMs on bilingual, (massively) multilingual and zero-shot translation tasks, under systematic variations of data conditions and model sizes. Our results show that: (i) Different LMs have different scaling properties, where architectural differences often have a significant impact on model performance at small scales, but the performance gap narrows as the number of parameters increases, (ii) Several design choices, including causal masking and language-modeling objectives for the source sequence, have detrimental effects on translation quality, and (iii) When paired with full-visible masking for source sequences, LMs could perform on par with EncDec on supervised bilingual and multilingual translation tasks, but improve greatly on zero-shot directions by facilitating the reduction of off-target translations."}}
{"id": "xKIQQtTowYv", "cdate": 1609459200000, "mdate": 1639487183369, "content": {"title": "Self-supervised and Supervised Joint Training for Resource-rich Machine Translation", "abstract": "Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-r..."}}
{"id": "u7rfxW8BwXp", "cdate": 1609459200000, "mdate": 1639487462673, "content": {"title": "Self-supervised and Supervised Joint Training for Resource-rich Machine Translation", "abstract": "Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-r..."}}
{"id": "HZVJ0IuJM1W", "cdate": 1609459200000, "mdate": 1639487182347, "content": {"title": "Self-supervised and Supervised Joint Training for Resource-rich Machine Translation", "abstract": "Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-rich NMT. In this paper, we propose a joint training approach, $F_2$-XEnDec, to combine self-supervised and supervised learning to optimize NMT models. To exploit complementary self-supervised signals for supervised learning, NMT models are trained on examples that are interbred from monolingual and parallel sentences through a new process called crossover encoder-decoder. Experiments on two resource-rich translation benchmarks, WMT'14 English-German and WMT'14 English-French, demonstrate that our approach achieves substantial improvements over several strong baseline methods and obtains a new state of the art of 46.19 BLEU on English-French when incorporating back translation. Results also show that our approach is capable of improving model robustness to input perturbations such as code-switching noise which frequently appears on social media."}}
{"id": "1yDrpckYHnN", "cdate": 1601308297092, "mdate": null, "content": {"title": "Self-supervised and Supervised Joint Training for Resource-rich Machine Translation", "abstract": "Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-rich NMT. In this paper, we propose a joint training approach, $F_2$-XEnDec, to combine self-supervised and supervised learning to optimize NMT models. To exploit complementary self-supervised signals for supervised learning,  NMT models are trained on examples that are interbred from monolingual and parallel  sentences through a new process called crossover encoder-decoder. Experiments on two resource-rich translation bench-marks, WMT\u201914 English-German and WMT\u201914 English-French, demonstrate that our approach achieves substantial improvements over a vanilla Transformer and obtains a new state of the art of 46 BLEU on English-French. Results also show that our approach is capable of improving model robustness against input perturbations which is known as a key weakness in contemporary NMT systems."}}
{"id": "zGoATr6eNhg", "cdate": 1577836800000, "mdate": 1639487182509, "content": {"title": "AdvAug: Robust Adversarial Augmentation for Neural Machine Translation", "abstract": "Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "cbrcQ6HPVm", "cdate": 1577836800000, "mdate": 1639487183369, "content": {"title": "AdvAug: Robust Adversarial Augmentation for Neural Machine Translation", "abstract": "In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, of which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over the Transformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g. back-translation) without using extra corpora."}}
{"id": "bCOluyLHVvm", "cdate": 1577836800000, "mdate": 1639487182515, "content": {"title": "Towards Web-based Etymological Hanzi Learning", "abstract": "Modern-day Chinese characters, or Hanzi, originate from the ancient oracle-bone scripts (\u7532\u9aa8\u6587). Such etymological relationship creates unique opportunities for Hanzi learning. This work proposes to use Web-based tools and the latest machine learning techniques to scale-up and enhance etymological Hanzi learning. By sharing our implementation details from launching an interactive sketch-based learning exhibition, we hope education-AI becomes more widely incorporated into today\u2019s commercial Web applications. Our demo video can be found at: https://youtube/1met8Uk5pA0"}}
{"id": "6Gw5sU4vFhm", "cdate": 1577836800000, "mdate": 1639487182347, "content": {"title": "Living Jiagu: Enabling Constructive Etymology for Chinese Learning", "abstract": "Living Jiagu is an interactive, wall-sized exhibition for the engaging learning of Chinese writing. Living Jiagu leverages state-of-the-art machine learning technologies to facilitate the recognition and recall of Chinese characters via constructive etymology in context - i.e., learning the writing and meaning of pictographic characters by designing them from image prompts similar to the creators of Oracle Bone Script (OBS) 3000 years ago and experiencing how these characters function and interact in natural scenes. An installation of Living Jiagu received positive feedback from over one thousand users."}}
