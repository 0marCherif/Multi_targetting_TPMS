{"id": "hRHX6XW9_Gu", "cdate": 1684340046202, "mdate": null, "content": {"title": "Diffusion Transformer for Adaptive Text-to-Speech", "abstract": "Given the success of diffusion in synthesizing realistic speech, we investigate how diffusion can be included in adaptive text-to-speech systems. Inspired by the adaptable layer norm modules for Transformer, we adapt a new backbone of diffusion models, Diffusion Transformer, for acoustic modeling. Specifically, the adaptive layer norm in the architecture is used to condition the diffusion network on text representations, which further enables parameter-efficient adaptation. We show the new architecture to be a faster alternative to its convolutional counterpart for general text-to-speech, while demonstrating a clear advantage on naturalness and similarity over the Transformer for few-shot and few-parameter adaptation. In the zero-shot scenario, while the new backbone is a decent alternative, the main benefit of such an architecture is to enable high-quality parameter-efficient adaptation when finetuning is performed."}}
{"id": "61XivIgknS", "cdate": 1672531200000, "mdate": 1682321402832, "content": {"title": "An investigation into the adaptability of a diffusion-based TTS model", "abstract": "Given the recent success of diffusion in producing natural-sounding synthetic speech, we investigate how diffusion can be used in speaker adaptive TTS. Taking cues from more traditional adaptation approaches, we show that adaptation can be included in a diffusion pipeline using conditional layer normalization with a step embedding. However, we show experimentally that, whilst the approach has merit, such adaptation alone cannot approach the performance of Transformer-based techniques. In a second experiment, we show that diffusion can be optimally combined with Transformer, with the latter taking the bulk of the adaptation load and the former contributing to improved naturalness."}}
{"id": "wmdu-0KCQwe", "cdate": 1640995200000, "mdate": 1682321402970, "content": {"title": "Bayesian Recurrent Units and the Forward-Backward Algorithm", "abstract": "Using Bayes's theorem, we derive a unit-wise recurrence as well as a backward recursion similar to the forward-backward algorithm. The resulting Bayesian recurrent units can be integrated as recurrent neural networks within deep learning frameworks, while retaining a probabilistic interpretation from the direct correspondence with hidden Markov models. Whilst the contribution is mainly theoretical, experiments on speech recognition indicate that adding the derived units at the end of state-of-the-art recurrent architectures can improve the performance at a very low cost in terms of trainable parameters."}}
{"id": "f1gq-XoXXU", "cdate": 1640995200000, "mdate": 1682321402872, "content": {"title": "Conversational Speech Recognition Needs Data? Experiments with Austrian German", "abstract": ""}}
{"id": "YeU9JyZ6NWj", "cdate": 1640995200000, "mdate": 1682321402872, "content": {"title": "Low-Level Physiological Implications of End-to-End Learning for Speech Recognition", "abstract": "Current speech recognition architectures perform very well from the point of view of machine learning, hence user interaction. This suggests that they are emulating the human biological system well. We investigate whether the inference can be inverted to provide insights into that biological system; in particular the hearing mechanism. Using SincNet, we confirm that end-to-end systems do learn well known filterbank structures. However, we also show that wider band-width filters are important in the learned structure. Whilst some benefits can be gained by initialising both narrow and wide-band filters, physiological constraints suggest that such filters arise in mid-brain rather than the cochlea. We show that standard machine learning architectures must be modified to allow this process to be emulated neurally."}}
{"id": "X2AJQE2BE2", "cdate": 1640995200000, "mdate": 1682321402863, "content": {"title": "Investigating a neural all pass warp in modern TTS applications", "abstract": ""}}
{"id": "I-SVux8C9x", "cdate": 1640995200000, "mdate": 1682321403020, "content": {"title": "Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition", "abstract": "Compared to conventional artificial neurons that produce dense and real-valued responses, biologically-inspired spiking neurons transmit sparse and binary information, which can also lead to energy-efficient implementations. Recent research has shown that spiking neural networks can be trained like standard recurrent neural networks using the surrogate gradient method. They have shown promising results on speech command recognition tasks. Using the same technique, we show that they are scalable to large vocabulary continuous speech recognition, where they are capable of replacing LSTMs in the encoder with only minor loss of performance. This suggests that they may be applicable to more involved sequence-to-sequence tasks. Moreover, in contrast to their recurrent non-spiking counterparts, they show robustness to exploding gradient problems without the need to use gates."}}
{"id": "EFosTcMIJ4U", "cdate": 1640995200000, "mdate": 1682321402822, "content": {"title": "Low-Level Physiological Implications of End-to-End Learning of Speech Recognition", "abstract": "Current speech recognition architectures perform very well from the point of view of machine learning, hence user interaction. This suggests that they are emulating the human biological system well. We investigate whether the inference can be inverted to provide insights into that biological system; in particular the hearing mechanism. Using SincNet, we confirm that end-to-end systems do learn well known filterbank structures. However, we also show that wider band-width filters are important in the learned structure. Whilst some benefits can be gained by initialising both narrow and wide-band filters, physiological constraints suggest that such filters arise in mid-brain rather than the cochlea. We show that standard machine learning architectures must be modified to allow this process to be emulated neurally."}}
{"id": "-7I0mr6_mu", "cdate": 1640995200000, "mdate": 1682321403021, "content": {"title": "Bayesian Recurrent Units and the Forward-Backward Algorithm", "abstract": "Using Bayes's theorem, we derive a unit-wise recurrence as well as a backward recursion similar to the forward-backward algorithm. The resulting Bayesian recurrent units can be integrated as recurrent neural networks within deep learning frameworks, while retaining a probabilistic interpretation from the direct correspondence with hidden Markov models. Whilst the contribution is mainly theoretical, experiments on speech recognition indicate that adding the derived units at the end of state-of-the-art recurrent architectures can improve the performance at a very low cost in terms of trainable parameters."}}
{"id": "kzHsUTBFbBm", "cdate": 1609459200000, "mdate": 1632152453169, "content": {"title": "A Bayesian Interpretation of the Light Gated Recurrent Unit", "abstract": "We summarise previous work showing that the basic sigmoid activation function arises as an instance of Bayes\u2019s theorem, and that recurrence follows from the prior. We derive a layerwise recurrence without the assumptions of previous work, and show that it leads to a standard recurrence with modest modifications to reflect use of log-probabilities. The resulting architecture closely resembles the Li-GRU which is the current state of the art for ASR. Although the contribution is mainly theoretical, we show that it is able to outperform the state of the art on the TIMIT and AMI datasets."}}
