{"id": "VeFQz5fimd", "cdate": 1668589693446, "mdate": 1668589693446, "content": {"title": "Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos", "abstract": "Unsupervised landmark learning is the task of learning semantic keypoint-like representations without the use of expensive\ninput keypoint annotations. A popular approach is to factorize an image into a pose and appearance data stream, then to reconstruct\nthe image from the factorized components. The pose representation should capture a set of consistent and tightly localized landmarks\nin order to facilitate reconstruction of the input image. Ultimately, we wish for our learned landmarks to focus on the foreground object of\ninterest. However, the reconstruction task of the entire image forces the model to allocate landmarks to model the background. Using a\nmotion-based foreground assumption, this work explores the effects of factorizing the reconstruction task into separate foreground and\nbackground reconstructions in an unsupervised way, allowing the model to condition only the foreground reconstruction on the\nunsupervised landmarks. Our experiments demonstrate that the proposed factorization results in landmarks that are focused on the\nforeground object of interest when measured against ground-truth foreground masks. Furthermore, the rendered background quality is\nalso improved as ill-suited landmarks are no longer forced to model this content. We demonstrate this improvement via improved image\nfidelity in a video-prediction task. Code is available at https://github.com/NVIDIA/UnsupervisedLandmarkLearning"}}
{"id": "upJ3vrFKaL", "cdate": 1663850192503, "mdate": null, "content": {"title": "Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures", "abstract": "Human pose transfer aims to synthesize a new view of a person under a given pose. Recent works achieve this via self-reconstruction, which disentangles pose and texture features from the person image, then combines the two features to reconstruct the person. Such feature-level disentanglement is a difficult and ill-defined problem that could lead to loss of details and unwanted artifacts. In this paper, we propose a self-driven human pose transfer method that permutes the textures at random, then reconstructs the image with a dual branch attention to achieve image-level disentanglement and detail-preserving texture transfer. We find that compared with feature-level disentanglement, image-level disentanglement is more controllable and reliable. Furthermore, we introduce a dual kernel encoder that gives different sizes of receptive fields in order to reduce the noise caused by permutation and thus recover clothing details while aligning pose and textures. Extensive experiments on DeepFashion and Market-1501 shows that our model improves the quality of generated images in terms of FID, LPIPS and SSIM over other self-driven methods, and even outperforming some fully-supervised methods. A user study also shows that among self-driven approaches, images generated by our method are preferred in 72\\% of cases over prior work."}}
{"id": "0NQwnnwAORi", "cdate": 1622637630291, "mdate": null, "content": {"title": "RAD-TTS: Parallel Flow-Based TTS with Robust Alignment Learning and Diverse Synthesis", "abstract": "This work introduces a predominantly parallel, end-to-end TTS model based on normalizing flows.\n It extends prior parallel approaches by additionally modeling speech rhythm as a separate generative distribution to facilitate variable token duration during inference. We further propose a robust framework for the on-line extraction of speech-text alignments -- a critical yet highly unstable learning problem in end-to-end TTS frameworks. Our experiments demonstrate that our proposed techniques yield improved alignment quality, better output diversity compared to controlled baselines."}}
{"id": "Ig53hpHxS4", "cdate": 1601308136366, "mdate": null, "content": {"title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis", "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \\href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}."}}
{"id": "ryen_CEFwr", "cdate": 1569439348167, "mdate": null, "content": {"title": "Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos", "abstract": "Unsupervised landmark learning is the task of learning semantic keypoint-like\nrepresentations without the use of expensive keypoint-level annotations. A popular approach is to factorize an image into a pose and appearance data stream, then to reconstruct the image from the factorized components. The pose representation should capture a set of consistent and tightly localized landmarks in order to facilitate reconstruction of the input image. Ultimately, we wish for our learned landmarks to focus on the foreground object of interest. However, the reconstruction task of the entire image forces the model to allocate landmarks to model the background. This work explores the effects of factorizing the reconstruction task into separate foreground and background reconstructions, conditioning only the foreground reconstruction on the unsupervised landmarks. Our experiments demonstrate that the proposed factorization results in landmarks that are focused on the foreground object of interest. Furthermore, the rendered background quality is also improved, as the background rendering pipeline no longer requires the ill-suited landmarks to model its pose and appearance. We demonstrate this improvement in the context of the video-prediction."}}
{"id": "ryeswQhIPr", "cdate": 1569272675030, "mdate": null, "content": {"title": "Improving Semantic Segmentation via Video Propagation and Label Relaxation", "abstract": ""}}
{"id": "BQW_JXgdTB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Graphical Contrastive Losses for Scene Graph Parsing.", "abstract": "Most scene graph parsers use a two-stage pipeline to detect visual relationships: the first stage detects entities, and the second predicts the predicate for each entity pair using a softmax distribution. We find that such pipelines, trained with only a cross entropy loss over predicate classes, suffer from two common errors. The first, Entity Instance Confusion, occurs when the model confuses multiple instances of the same type of entity (e.g. multiple cups). The second, Proximal Relationship Ambiguity, arises when multiple subject-predicate-object triplets appear in close proximity with the same predicate, and the model struggles to infer the correct subject-object pairings (e.g. mis-pairing musicians and their instruments). We propose a set of contrastive loss formulations that specifically target these types of errors within the scene graph parsing problem, collectively termed the Graphical Contrastive Losses. These losses explicitly force the model to disambiguate related and unrelated instances through margin constraints specific to each type of confusion. We further construct a relationship detector, called RelDN, using the aforementioned pipeline to demonstrate the efficacy of our proposed losses. Our model outperforms the winning method of the OpenImages Relationship Detection Challenge by 4.7% (16.5% relatively) on the test set. We also show improved results over the best previous methods on the Visual Genome and Visual Relationship Detection datasets."}}
{"id": "S1NXYqWuWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "SDC-Net: Video Prediction Using Spatially-Displaced Convolution", "abstract": "We present an approach for high-resolution video frame prediction by conditioning on both past frames and past optical flows. Previous approaches rely on resampling past frames, guided by a learned future optical flow, or on direct generation of pixels. Resampling based on flow is insufficient because it cannot deal with disocclusions. Generative models currently lead to blurry results. Recent approaches synthesis a pixel by convolving input patches with a predicted kernel. However, their memory requirement increases with kernel size. Here, we present spatially-displaced convolution (SDC) module for video frame prediction. We learn a motion vector and a kernel for each pixel and synthesize a pixel by applying the kernel at a displaced location in the source image, defined by the predicted motion vector. Our approach inherits the merits of both vector-based and kernel-based approaches, while ameliorating their respective disadvantages. We train our model on 428K unlabelled 1080p video game frames. Our approach produces state-of-the-art results, achieving an SSIM score of 0.904 on high-definition YouTube-8M videos, 0.918 on Caltech Pedestrian videos. Our model handles large motion effectively and synthesizes crisp frames with consistent motion."}}
{"id": "HkV81yZ_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Interpretable Spatial Operations in a Rich 3D Blocks World", "abstract": "In this paper, we study the problem of mapping natural language instructions to complex spatial actions in a 3D blocks world. We first introduce a new dataset that pairs complex 3D spatial operations to rich natural language descriptions that require complex spatial and pragmatic interpretations such as \"mirroring\", \"twisting\", and \"balancing\". This dataset, built on the simulation environment of Bisk, Yuret, and Marcu (2016), attains language that is significantly richer and more complex, while also doubling the size of the original dataset in the 2D environment with 100 new world configurations and 250,000 tokens. In addition, we propose a new neural architecture that achieves competitive results while automatically discovering an inventory of interpretable spatial operations (Figure 5)"}}
{"id": "BJ-z19Zd-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Image Inpainting for Irregular Holes Using Partial Convolutions", "abstract": "Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach."}}
