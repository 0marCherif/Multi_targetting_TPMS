{"id": "Ctq0d9LEuT", "cdate": 1685532018438, "mdate": null, "content": {"title": "Stochastic Rising Bandits: A Best Arm Identification Approach", "abstract": "Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected rewards of the available options increase every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper, focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we derive a lower bound on the error probability, matched by our R-SR (up to logarithmic factors), and illustrate how the need for a sufficiently large budget is unavoidable in the SRB setting. Finally, we numerically validate the proposed algorithms in both synthetic and real-world environments and compare them with the currently available BAI strategies."}}
{"id": "c0z-U2d9z-E", "cdate": 1672531200000, "mdate": 1681666287148, "content": {"title": "Optimal Rates and Efficient Algorithms for Online Bayesian Persuasion", "abstract": "Bayesian persuasion studies how an informed sender should influence beliefs of rational receivers who take decisions through Bayesian updating of a common prior. We focus on the online Bayesian persuasion framework, in which the sender repeatedly faces one or more receivers with unknown and adversarially selected types. First, we show how to obtain a tight $\\tilde O(T^{1/2})$ regret bound in the case in which the sender faces a single receiver and has partial feedback, improving over the best previously known bound of $\\tilde O(T^{4/5})$. Then, we provide the first no-regret guarantees for the multi-receiver setting under partial feedback. Finally, we show how to design no-regret algorithms with polynomial per-iteration running time by exploiting type reporting, thereby circumventing known intractability results on online Bayesian persuasion. We provide efficient algorithms guaranteeing a $O(T^{1/2})$ regret upper bound both in the single- and multi-receiver scenario when type reporting is allowed."}}
{"id": "D8slvx44yfT", "cdate": 1672531200000, "mdate": 1681490124753, "content": {"title": "Best Arm Identification for Stochastic Rising Bandits", "abstract": ""}}
{"id": "4brF6F9Imx", "cdate": 1672531200000, "mdate": 1681666287144, "content": {"title": "Constrained Phi-Equilibria", "abstract": "The computational study of equilibria involving constraints on players' strategies has been largely neglected. However, in real-world applications, players are usually subject to constraints ruling out the feasibility of some of their strategies, such as, e.g., safety requirements and budget caps. Computational studies on constrained versions of the Nash equilibrium have lead to some results under very stringent assumptions, while finding constrained versions of the correlated equilibrium (CE) is still unexplored. In this paper, we introduce and computationally characterize constrained Phi-equilibria -- a more general notion than constrained CEs -- in normal-form games. We show that computing such equilibria is in general computationally intractable, and also that the set of the equilibria may not be convex, providing a sharp divide with unconstrained CEs. Nevertheless, we provide a polynomial-time algorithm for computing a constrained (approximate) Phi-equilibrium maximizing a given linear function, when either the number of constraints or that of players' actions is fixed. Moreover, in the special case in which a player's constraints do not depend on other players' strategies, we show that an exact, function-maximizing equilibrium can be computed in polynomial time, while one (approximate) equilibrium can be found with an efficient decentralized no-regret learning algorithm."}}
{"id": "qHc5B5iEaSx", "cdate": 1668734790120, "mdate": null, "content": {"title": "A General Framework for Safe Decision Making: A Convex Duality Approach", "abstract": "We study the problem of online interaction in general decision making problems,\nwhere the objective is not only to find optimal strategies, but also to satisfy some\nsafety guarantees, expressed in terms of costs accrued. We propose a theoretical\nframework to address such problems and present BAN-SOLO, a UCB-like algorithm that, in an online interaction with an unknown environment, attains sublinear regret of order O(T^{1/2}) and plays safely with high probability at each iteration. At its core, BAN-SOLO relies on tools from convex duality to manage environment exploration while satisfying the safety constraints imposed by the problem."}}
{"id": "CflSnSkH--", "cdate": 1652737711386, "mdate": null, "content": {"title": "Sequential Information Design: Learning to Persuade in the Dark", "abstract": "We study a repeated information design problem faced by an informed sender who tries to influence the behavior of a self-interested receiver. We consider settings where the receiver faces a sequential decision making (SDM) problem. At each round, the sender observes the realizations of random events in the SDM problem. This begets the challenge of how to incrementally disclose such information to the receiver to persuade them to follow (desirable) action recommendations. We study the case in which the sender does not know random events probabilities, and, thus, they have to gradually learn them while persuading the receiver. Our goal is to design online learning algorithms that are no-regret for the sender, while at the same time being persuasive for the receiver. We start by providing a non-trivial polytopal approximation of the set of sender's persuasive information structures. This is crucial to design efficient learning algorithms. Next, we prove a negative result: no learning algorithm can be persuasive. Thus, we relax persuasiveness requirements by focusing on algorithms that guarantee that the receiver's regret in following recommendations grows sub-linearly. In the full-feedback setting---where the sender observes all random events realizations---, we provide an algorithm with $\\tilde{O}(\\sqrt{T})$ regret for both the sender and the receiver. Instead, in the bandit-feedback setting---where the sender only observes the realizations of random events actually occurring in the SDM problem---, we design an algorithm that, given an $\\alpha \\in [1/2, 1]$ as input, ensures $\\tilde{O}({T^\\alpha})$ and $\\tilde{O}( T^{\\max \\{ \\alpha, 1-\\frac{\\alpha}{2} \\} })$ regrets for the sender and the receiver, respectively. This result is complemented by a lower bound showing that such a regrets trade-off is essentially tight."}}
{"id": "m73ilNlR2V-", "cdate": 1640995200000, "mdate": 1672223037238, "content": {"title": "The Evolutionary Dynamics of Soft-Max Policy Gradient in Multi-Agent Settings", "abstract": ""}}
{"id": "lmTX12uF50i", "cdate": 1640995200000, "mdate": 1681490125014, "content": {"title": "Stochastic Rising Bandits", "abstract": ""}}
{"id": "ap8v-_YZaF", "cdate": 1640995200000, "mdate": 1681490124899, "content": {"title": "ARLO: A Framework for Automated Reinforcement Learning", "abstract": ""}}
{"id": "Yw89V0rWAs", "cdate": 1640995200000, "mdate": 1681490124891, "content": {"title": "Pricing the Long Tail by Explainable Product Aggregation and Monotonic Bandits", "abstract": ""}}
