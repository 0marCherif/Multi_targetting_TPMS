{"id": "jFuFNbdlWG", "cdate": 1581702593608, "mdate": null, "content": {"title": "Recognizing Part Attributes with Insufficient Data", "abstract": "Recognizing attributes of objects and their parts is important to many computer vision applications. Although\ngreat progress has been made to apply object-level recognition, recognizing the attributes of parts remains less applicable since the training data for part attributes recognition is usually scarce especially for internet-scale applications. Furthermore, most existing part attribute recognition methods rely on the part annotation which is more\nexpensive to obtain. To solve the data insufficiency problem and get rid of dependence on the part annotation, we\nintroduce a novel Concept Sharing Network (CSN) for part\nattribute recognition. A great advantage of CSN is its capability of recognizing the part attribute (a combination\nof part location and appearance pattern) that has insufficient or zero training data, by learning the part location\nand appearance pattern respectively from the training data\nthat usually mix them in a single label. Extensive experiments on CUB-200-2011 [51], CelebA [35] and a newly\nproposed human attribute dataset demonstrate the effectiveness of CSN and its advantages over other methods, especially for the attributes with few training samples. Further experiments show that CSN can also perform zero-shot\npart attribute recognition."}}
{"id": "r1ba7KZdZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Modulation Module for Multi-task Learning with Applications in Image Retrieval", "abstract": "Multi-task learning has been widely adopted in many computer vision tasks to improve overall computation efficiency or boost the performance of individual tasks, under the assumption that those tasks are correlated and complementary to each other. However, the relationships between the tasks are complicated in practice, especially when the number of involved tasks scales up. When two tasks are of weak relevance, they may compete or even distract each other during joint training of shared parameters, and as a consequence undermine the learning of all the tasks. This will raise destructive interference which decreases learning efficiency of shared parameters and lead to low quality loss local optimum w.r.t. shared parameters. To address the this problem, we propose a general modulation module, which can be inserted into any convolutional neural network architecture, to encourage the coupling and feature sharing of relevant tasks while disentangling the learning of irrelevant tasks with minor parameters addition. Equipped with this module, gradient directions from different tasks can be enforced to be consistent for those shared parameters, which benefits multi-task joint training. The module is end-to-end learnable without ad-hoc design for specific tasks, and can naturally handle many tasks at the same time. We apply our approach on two retrieval tasks, face retrieval on the CelebA dataset\u00a0[12] and product retrieval on the UT-Zappos50K dataset\u00a0[34, 35], and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency."}}
{"id": "ByNxZlM_WB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Pseudo Mask Augmented Object Detection", "abstract": "In this work, we present a novel and effective framework to facilitate object detection with the instance-level segmentation information that is only supervised by bounding box annotation. Starting from the joint object detection and instance segmentation network, we propose to recursively estimate the pseudo ground-truth object masks from the instance-level object segmentation network training, and then enhance the detection network with top-down segmentation feedbacks. The pseudo ground truth mask and network parameters are optimized alternatively to mutually benefit each other. To obtain the promising pseudo masks in each iteration, we embed a graphical inference that incorporates the low-level image appearance consistency and the bounding box annotations to refine the segmentation masks predicted by the segmentation network. Our approach progressively improves the object detection performance by incorporating the detailed pixel-wise information learned from the weakly-supervised segmentation network. Extensive evaluation on the detection task in PASCAL VOC 2007 and 2012 verifies that the proposed approach is effective."}}
{"id": "S1bqL5Zu-B", "cdate": 1451606400000, "mdate": null, "content": {"title": "Peak-Piloted Deep Network for Facial Expression Recognition", "abstract": "Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special-purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-of-the-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset."}}
