{"id": "xampQmrqD8U", "cdate": 1676827072215, "mdate": null, "content": {"title": "Approximate Thompson Sampling via Epistemic Neural Networks", "abstract": "Thompson sampling (TS) is a popular heuristic for action selection, but it requires sampling from a posterior distribution.\nUnfortunately, this can become computationally intractable in complex environments, such as those modeled using neural networks.\nApproximate posterior samples can produce effective actions, but only if they reasonably approximate joint predictive distributions of outputs across inputs.\nNotably, accuracy of marginal predictive distributions does not suffice.\nEpistemic neural networks (ENNs) are designed to produce accurate joint predictive distributions.\nWe compare a range of ENNs through computational experiments that assess their performance in approximating TS across bandit and reinforcement learning environments.  \nThe results indicate that ENNs serve this purpose well and illustrate how the quality of joint predictive distributions drives performance.\nFurther, we demonstrate that the \\textit{epinet} --- a small additive network that estimates uncertainty --- matches the performance of large ensembles at orders of magnitude lower computational cost.\nThis enables effective application of TS with computation that scales gracefully to complex environments."}}
{"id": "KUBjrkUTp_", "cdate": 1672531200000, "mdate": 1681712943103, "content": {"title": "Approximate Thompson Sampling via Epistemic Neural Networks", "abstract": "Thompson sampling (TS) is a popular heuristic for action selection, but it requires sampling from a posterior distribution. Unfortunately, this can become computationally intractable in complex environments, such as those modeled using neural networks. Approximate posterior samples can produce effective actions, but only if they reasonably approximate joint predictive distributions of outputs across inputs. Notably, accuracy of marginal predictive distributions does not suffice. Epistemic neural networks (ENNs) are designed to produce accurate joint predictive distributions. We compare a range of ENNs through computational experiments that assess their performance in approximating TS across bandit and reinforcement learning environments. The results indicate that ENNs serve this purpose well and illustrate how the quality of joint predictive distributions drives performance. Further, we demonstrate that the \\textit{epinet} -- a small additive network that estimates uncertainty -- matches the performance of large ensembles at orders of magnitude lower computational cost. This enables effective application of TS with computation that scales gracefully to complex environments."}}
{"id": "JyTT03dqCFD", "cdate": 1652737585404, "mdate": null, "content": {"title": "The Neural Testbed: Evaluating Joint Predictions", "abstract": "\nPredictive distributions quantify uncertainties ignored by point estimates. This paper introduces The Neural Testbed: an open source benchmark for controlled and principled evaluation of agents that generate such predictions. Crucially, the testbed assesses agents not only on the quality of their marginal predictions per input, but also on their joint predictions across many inputs. We evaluate a range of agents using a simple neural network data generating process.\n\nOur results indicate that some popular Bayesian deep learning agents do not fare well with joint predictions, even when they can produce accurate marginal predictions. We also show that the quality of joint predictions drives performance in downstream decision tasks. We find these results are robust across choice a wide range of generative models, and highlight the practical importance of joint predictions to the community."}}
{"id": "rFb8y8Io5e9", "cdate": 1646077511404, "mdate": null, "content": {"title": "Evaluating High-Order Predictive Distributions in Deep Learning", "abstract": "Most work on supervised learning research has focused on marginal predictions.\nIn decision problems, joint predictive distributions are essential for good performance.\nPrevious work has developed methods for assessing low-order predictive distributions with inputs sampled i.i.d. from the testing distribution.\nWith low-dimensional inputs, these methods distinguish agents that effectively estimate uncertainty from those that do not.\nWe establish that the predictive distribution order required for such differentiation increases greatly with input dimension, rendering these methods impractical.\nTo accommodate high-dimensional inputs, we introduce \\textit{dyadic sampling}, which focuses on predictive distributions associated with random \\textit{pairs} of inputs.\nWe demonstrate that this approach efficiently distinguishes agents in high-dimensional examples involving simple logistic regression as well as complex synthetic and empirical data."}}
{"id": "6WKOwIVScEI", "cdate": 1640995200000, "mdate": 1681712943007, "content": {"title": "Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping", "abstract": "In machine learning, an agent needs to estimate uncertainty to efficiently explore and adapt and to make effective decisions. A common approach to uncertainty estimation maintains an ensemble of models. In recent years, several approaches have been proposed for training ensembles, and conflicting views prevail with regards to the importance of various ingredients of these approaches. In this paper, we aim to address the benefits of two ingredients -- prior functions and bootstrapping -- which have come into question. We show that prior functions can significantly improve an ensemble agent's joint predictions across inputs and that bootstrapping affords additional benefits if the signal-to-noise ratio varies across inputs. Our claims are justified by both theoretical and experimental results."}}
{"id": "S7vWxSkqv_M", "cdate": 1632875612106, "mdate": null, "content": {"title": "Evaluating Predictive Distributions: Does Bayesian Deep Learning Work?", "abstract": "Posterior predictive distributions quantify uncertainties ignored by point estimates.\nThis paper introduces \\textit{The Neural Testbed}, which provides tools for the systematic evaluation of agents that generate such predictions.\nCrucially, these tools assess not only the quality of marginal predictions per input, but also joint predictions given many inputs.\nJoint distributions are often critical for useful uncertainty quantification, but they have been largely overlooked by the Bayesian deep learning community.\nWe benchmark several approaches to uncertainty estimation using a neural-network-based data generating process.\nOur results reveal the importance of evaluation beyond marginal predictions.\nFurther, they reconcile sources of confusion in the field, such as why Bayesian deep learning approaches that generate accurate marginal predictions perform poorly in sequential decision tasks, how incorporating priors can be helpful, and what roles epistemic versus aleatoric uncertainty play when evaluating performance.\nWe also present experiments on real-world challenge datasets, which show a high correlation with testbed results, and that the importance of evaluating joint predictive distributions carries over to real data.\nAs part of this effort, we opensource The Neural Testbed, including all implementations from this paper."}}
{"id": "PCtXLR9K8EG", "cdate": 1609459200000, "mdate": 1681712942924, "content": {"title": "Reinforcement Learning, Bit by Bit", "abstract": "Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We discuss concepts and regret analysis that together offer principled guidance. This line of thinking sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that highlight data efficiency."}}
{"id": "6GK9-NvmuN", "cdate": 1609459200000, "mdate": 1681712942999, "content": {"title": "Evaluating Predictive Distributions: Does Bayesian Deep Learning Work?", "abstract": "Predictive distributions quantify uncertainties ignored by point estimates. This paper introduces The Neural Testbed: an open-source benchmark for controlled and principled evaluation of agents that generate such predictions. Crucially, the testbed assesses agents not only on the quality of their marginal predictions per input, but also on their joint predictions across many inputs. We evaluate a range of agents using a simple neural network data generating process. Our results indicate that some popular Bayesian deep learning agents do not fare well with joint predictions, even when they can produce accurate marginal predictions. We also show that the quality of joint predictions drives performance in downstream decision tasks. We find these results are robust across choice a wide range of generative models, and highlight the practical importance of joint predictions to the community."}}
{"id": "aT4-zbRi_E", "cdate": 1577836800000, "mdate": 1681712942916, "content": {"title": "Hypermodels for Exploration", "abstract": "We study the use of hypermodels to represent epistemic uncertainty and guide exploration. This generalizes and extends the use of ensembles to approximate Thompson sampling. The computational cost of training an ensemble grows with its size, and as such, prior work has typically been limited to ensembles with tens of elements. We show that alternative hypermodels can enjoy dramatic efficiency gains, enabling behavior that would otherwise require hundreds or thousands of elements, and even succeed in situations where ensemble methods fail to learn regardless of size. This allows more accurate approximation of Thompson sampling as well as use of more sophisticated exploration schemes. In particular, we consider an approximate form of information-directed sampling and demonstrate performance gains relative to Thompson sampling. As alternatives to ensembles, we consider linear and neural network hypermodels, also known as hypernetworks. We prove that, with neural network base models, a linear hypermodel can represent essentially any distribution over functions, and as such, hypernetworks do not extend what can be represented."}}
{"id": "DQtIDDpbe0", "cdate": 1577836800000, "mdate": 1681712943046, "content": {"title": "Langevin DQN", "abstract": "Algorithms that tackle deep exploration -- an important challenge in reinforcement learning -- have relied on epistemic uncertainty representation through ensembles or other hypermodels, exploration bonuses, or visitation count distributions. An open question is whether deep exploration can be achieved by an incremental reinforcement learning algorithm that tracks a single point estimate, without additional complexity required to account for epistemic uncertainty. We answer this question in the affirmative. In particular, we develop Langevin DQN, a variation of DQN that differs only in perturbing parameter updates with Gaussian noise and demonstrate through a computational study that the presented algorithm achieves deep exploration. We also offer some intuition to how Langevin DQN achieves deep exploration. In addition, we present a modification of the Langevin DQN algorithm to improve the computational efficiency."}}
