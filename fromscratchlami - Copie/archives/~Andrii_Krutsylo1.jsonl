{"id": "xiBo1aP1wU", "cdate": 1704067200000, "mdate": 1706010152644, "content": {"title": "Batch Sampling for Experience Replay", "abstract": "Continual learning aims to build models that adapt to a sequence of tasks while preserving prior knowledge. Experience Replay, a strategy that involves the retention and replay of training samples from previous tasks, is one of the best approaches to mitigate the issue of catastrophic forgetting. Current replay-based methods typically select samples from the memory for replay based on their individual properties, overlooking the collective impact of a sample batch. To address this shortcoming, we introduce Batch Cosine Distance, a novel metric that measures changes in hidden representations within a batch before and after a model update. This metric not only identifies the samples most susceptible to forgetting, but also quantifies the diversity of affected regions in their embeddings. To empirically validate our metric, we propose Random Batch Sampling, a proof-of-concept method that ranks a small number of random batches sampled from the memory, selecting the highest scoring batch for replay based on the proposed metric. Despite its simplicity, our approach demonstrates competitive performance with sophisticated approaches such as MIR when evaluated on MNIST and CIFAR-10 datasets across various memory sizes. This study underscores the untapped potential of batch-oriented selection methods, offering a new direction for future investigation."}}
{"id": "byf3j5s3Wo", "cdate": 1672531200000, "mdate": 1681661505351, "content": {"title": "Hebbian Continual Representation Learning", "abstract": "Continual Learning aims to bring machine learning into a more realistic scenario, where tasks are learned sequentially and the i.i.d. assumption is not preserved. Although this setting is natural for biological systems, it proves very difficult for machine learning models such as artificial neural networks. To reduce this performance gap, we investigate the question whether biologically inspired Hebbian learning is useful for tackling continual challenges. In particular, we highlight a realistic and often overlooked unsupervised setting, where the learner has to build representations without any supervision. By combining sparse neural networks with Hebbian learning principle, we build a simple yet effective alternative (HebbCL) to typical neural network models trained via the gradient descent. Due to Hebbian learning, the network have easily interpretable weights, which might be essential in critical application such as security or healthcare. We demonstrate the efficacy of HebbCL in an unsupervised learning setting applied to MNIST and Omniglot datasets. We also adapt the algorithm to the supervised scenario and obtain promising results in the class-incremental learning."}}
{"id": "yKlbRoh_wTh", "cdate": 1640995200000, "mdate": 1699142988302, "content": {"title": "Diverse Memory for Experience Replay in Continual Learning", "abstract": ""}}
{"id": "4zJMxGMnuhQ", "cdate": 1640995200000, "mdate": 1663323857244, "content": {"title": "Hebbian Continual Representation Learning", "abstract": "Continual Learning aims to bring machine learning into a more realistic scenario, where tasks are learned sequentially and the i.i.d. assumption is not preserved. Although this setting is natural for biological systems, it proves very difficult for machine learning models such as artificial neural networks. To reduce this performance gap, we investigate the question whether biologically inspired Hebbian learning is useful for tackling continual challenges. In particular, we highlight a realistic and often overlooked unsupervised setting, where the learner has to build representations without any supervision. By combining sparse neural networks with Hebbian learning principle, we build a simple yet effective alternative (HebbCL) to typical neural network models trained via the gradient descent. Due to Hebbian learning, the network have easily interpretable weights, which might be essential in critical application such as security or healthcare. We demonstrate the efficacy of HebbCL in an unsupervised learning setting applied to MNIST and Omniglot datasets. We also adapt the algorithm to the supervised scenario and obtain promising results in the class-incremental learning."}}
{"id": "SU0FyXzV3ye", "cdate": 1609459200000, "mdate": 1654001185160, "content": {"title": "Remember More by Recalling Less: Investigating the Role of Batch Size in Continual Learning with Experience Replay (Student Abstract)", "abstract": "Experience replay is a simple and well-performing strategy for continual learning problems, often used as a basis for more advanced methods. However, the dynamics of experience replay are not yet well understood. To showcase this, we focus on a single component of this problem, namely choosing the batch size of the buffer samples. We find that small batches perform much better at stopping forgetting than larger batches, contrary to the intuitive assumption that it is better to recall more samples from the past to avoid forgetting. We show that this phenomenon does not disappear under learning rate tuning and we propose possible directions for further analysis."}}
