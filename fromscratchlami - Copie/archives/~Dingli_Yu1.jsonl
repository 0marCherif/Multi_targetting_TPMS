{"id": "jQGpIG9VeR", "cdate": 1675827736041, "mdate": null, "content": {"title": "A Kernel-Based View of Language Model Fine-Tuning", "abstract": "It has become standard to solve NLP tasks by  fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate  whether the Neural Tangent Kernel (NTK)---which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization---describes fine-tuning of pre-trained LMs.  This study was inspired by the decent performance of NTK  for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting often induces kernel-based dynamics during fine-tuning. Finally, we use this kernel view to propose an explanation for the success of parameter-efficient subspace-based fine-tuning methods."}}
{"id": "erHaiO9gz3m", "cdate": 1663850431676, "mdate": null, "content": {"title": "A Kernel-Based View of Language Model Fine-Tuning", "abstract": "It has become standard to solve NLP tasks by  fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK)--which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization--describes fine-tuning of pre-trained LMs.  This study was inspired by the decent performance of NTK  for computer vision tasks (Wei et al., 2022). We also extend the NTK formalism to  fine-tuning with Adam.  We present extensive experiments  that suggest that once the task is formulated as a masked language modeling problem through prompting, the NTK lens can often reasonably describe the model updates during fine-tuning with both SGD and Adam.\nThis kernel view also suggests an explanation for success of parameter-efficient subspace-based fine-tuning methods. Finally, we suggest a path toward a formal explanation for our findings via Tensor Programs (Yang, 2020)."}}
{"id": "opw858PBJl6", "cdate": 1652737854674, "mdate": null, "content": {"title": "New Definitions and Evaluations for Saliency Methods: Staying Intrinsic, Complete and Sound", "abstract": "Saliency methods compute heat maps that highlight portions of an input that were most important for the label assigned to it by a deep net. Evaluations of saliency methods convert this heat map into a new masked input by retaining the $k$ highest-ranked pixels of the original input and replacing the rest with \"uninformative\" pixels, and checking if the net's output is mostly unchanged. This is usually seen as an explanation of the output, but the current paper highlights reasons why this inference of causality may be suspect. Inspired by logic concepts of completeness & soundness, it observes that the above type of evaluation focuses on completeness of the explanation, but ignores soundness.  New evaluation metrics are introduced to capture both notions, while staying in an intrinsic framework---i.e., using the dataset and the net, but no separately trained nets, human evaluations, etc. A simple saliency method is described that matches or outperforms prior methods in the evaluations. Experiments also suggest new intrinsic justifications, based on soundness, for popular heuristic tricks such as TV regularization and upsampling."}}
{"id": "sof8l4cki9", "cdate": 1652737562957, "mdate": null, "content": {"title": "Fast Mixing of Stochastic Gradient Descent with Normalization and Weight Decay", "abstract": "We prove the Fast Equilibrium Conjecture proposed by Li et al., (2020), i.e., stochastic gradient descent (SGD) on a scale-invariant loss (e.g., using networks with various normalization schemes) with learning rate $\\eta$ and weight decay factor $\\lambda$ mixes in function space in $\\mathcal{\\tilde{O}}(\\frac{1}{\\lambda\\eta})$ steps,  under two standard assumptions: (1) the noise covariance matrix is non-degenerate and (2) the minimizers of the loss form a connected, compact and analytic manifold. The analysis uses the framework of Li et al., (2021) and shows that for every $T>0$, the iterates of SGD with learning rate $\\eta$ and weight decay factor $\\lambda$ on the scale-invariant loss converge in distribution in $\\Theta\\left(\\eta^{-1}\\lambda^{-1}(T+\\ln(\\lambda/\\eta))\\right)$  iterations as $\\eta\\lambda\\to 0$ while satisfying $\\eta \\le O(\\lambda)\\le O(1)$.  Moreover, the evolution of the limiting distribution can be described by a stochastic differential equation that mixes to the same equilibrium distribution for every initialization around the manifold of minimizers as $T\\to\\infty$. "}}
{"id": "nGggTYDIYl", "cdate": 1640995200000, "mdate": 1675098164597, "content": {"title": "New Definitions and Evaluations for Saliency Methods: Staying Intrinsic, Complete and Sound", "abstract": "Saliency methods compute heat maps that highlight portions of an input that were most {\\em important} for the label assigned to it by a deep net. Evaluations of saliency methods convert this heat map into a new {\\em masked input} by retaining the $k$ highest-ranked pixels of the original input and replacing the rest with \\textquotedblleft uninformative\\textquotedblright\\ pixels, and checking if the net's output is mostly unchanged. This is usually seen as an {\\em explanation} of the output, but the current paper highlights reasons why this inference of causality may be suspect. Inspired by logic concepts of {\\em completeness \\& soundness}, it observes that the above type of evaluation focuses on completeness of the explanation, but ignores soundness. New evaluation metrics are introduced to capture both notions, while staying in an {\\em intrinsic} framework -- i.e., using the dataset and the net, but no separately trained nets, human evaluations, etc. A simple saliency method is described that matches or outperforms prior methods in the evaluations. Experiments also suggest new intrinsic justifications, based on soundness, for popular heuristic tricks such as TV regularization and upsampling."}}
{"id": "bRUHiSwoHL0", "cdate": 1640995200000, "mdate": 1682322653020, "content": {"title": "A Kernel-Based View of Language Model Fine-Tuning", "abstract": "It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting often induces kernel-based dynamics during fine-tuning. Finally, we use this kernel view to propose an explanation for the success of parameter-efficient subspace-based fine-tuning methods."}}
{"id": "Mo9R9oqzPo", "cdate": 1632875765863, "mdate": null, "content": {"title": "New Definitions and Evaluations for Saliency Methods: Staying Intrinsic and Sound", "abstract": "  Saliency methods seek to provide human-interpretable explanations for the output of machine learning model on a given input. A plethora of saliency methods exist, as well as an extensive literature on their justifications/criticisms/evaluations. This paper focuses on heat maps based saliency methods that often provide explanations that look best to humans. It tries to introduce methods and evaluations for masked-based saliency methods that are {\\em intrinsic} --- use just the training dataset and the trained net, and do not use separately trained nets, distractor distributions, human evaluations or annotations. Since a mask can be seen as a \"certificate\" justifying the net's answer, we introduce notions of {\\em completeness} and {\\em soundness} (the latter being the new contribution) motivated by logical proof systems. These notions allow a new evaluation of  saliency methods, that experimentally provides a novel and stronger justification for several heuristic tricks in the field (T.V. regularization, upscaling). "}}
{"id": "oaqUCqfySS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Characterization of Group-strategyproof Mechanisms for Facility Location in Strictly Convex Space", "abstract": "We characterize the class of group-strategyproof mechanisms for the single facility location game in any unconstrained strictly convex space. A mechanism is group-strategyproof,if no group of agents can misreport so that all its members are strictlybetter off. A strictly convex space is a normed vector space where |x+y|<2 holds for any pair of different unit vectors x \u2260 y, e.g., any Lp space with p\u2208 (1,\u221e). We show that any deterministic, unanimous, group-strategyproof mechanism must be dictatorial, and that any randomized, unanimous, translation-invariant, group-strategyproof mechanism must be 2-dictatorial.Here a randomized mechanism is 2-dictatorial if the lottery output of the mechanism must be distributed on the line segment between two dictators' inputs. A mechanism is translation-invariant if the output of the mechanism follows the same translation of the input. Our characterization directly implies that any (randomized) translation-invariant approximation algorithm satisfying the group-strategyproofness property has a lower bound of 2-approximation for maximum cost (whenever n \u2265 3), and n/2 - 1 for social cost. We also find an algorithm that 2-approximates the maximum cost and n/2-approximates the social cost, proving the bounds to be (almost) tight."}}
{"id": "c31KSjnZFC0", "cdate": 1577836800000, "mdate": 1682374021180, "content": {"title": "Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee", "abstract": "Over-parameterized deep neural networks trained by simple first-order methods are known to be able to fit any labeling of data. Such over-fitting ability hinders generalization when mislabeled training examples are present. On the other hand, simple regularization methods like early-stopping can often achieve highly nontrivial performance on clean test data in these scenarios, a phenomenon not theoretically understood. This paper proposes and analyzes two simple and intuitive regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, we prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. Our generalization analysis relies on the connection between wide neural network and neural tangent kernel (NTK). The generalization bound is independent of the network size, and is comparable to the bound one can get when there is no label noise. Experimental results verify the effectiveness of these methods on noisily labeled datasets."}}
{"id": "NRSeAzB1RJh", "cdate": 1577836800000, "mdate": 1664445388100, "content": {"title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks", "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks. 1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets. 2. On CIFAR-10 with 10 \u2013 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%. 3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance. 4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK\u2019s efficacy may trace to lower variance of output."}}
