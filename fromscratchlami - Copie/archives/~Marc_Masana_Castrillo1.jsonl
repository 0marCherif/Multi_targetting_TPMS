{"id": "vpY4rGSZYIW", "cdate": 1667656388827, "mdate": 1667656388827, "content": {"title": "On the importance of cross-task features for class-incremental learning", "abstract": "In class-incremental learning, an agent with limited resources needs to learn a sequence of classification tasks, forming an ever growing classification problem, with the constraint of not being able to access data from previous tasks. The\nmain difference with task-incremental learning,\nwhere a task-ID is available at inference time, is\nthat the learner also needs to perform cross-task\ndiscrimination, i.e. distinguish between classes\nthat have not been seen together. Approaches\nto tackle this problem are numerous and mostly\nmake use of an external memory (buffer) of nonnegligible size. In this paper, we ablate the learning of cross-task features and study its influence\non the performance of basic replay strategies used\nfor class-IL. We also define a new forgetting measure for class-incremental learning, and see that\nforgetting is not the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning\nshould not only prevent forgetting, but also aim\nto improve the quality of the cross-task features,\nand the knowledge transfer between tasks. This is\nespecially important when tasks contain limited\namount of data."}}
{"id": "jw_bbLf96v", "cdate": 1640995200000, "mdate": 1667576796739, "content": {"title": "An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions", "abstract": "Although deep neural networks enable impressive visual perception performance for autonomous driving, their robustness to varying weather conditions still requires attention. When adapting these models for changed environments, such as different weather conditions, they are prone to forgetting previously learned information. This catastrophic forgetting is typically addressed via incremental learning approaches which usually re-train the model by either keeping a memory bank of training samples or keeping a copy of the entire model or model parameters for each scenario. While these approaches show impressive results, they can be prone to scalability issues and their applicability for autonomous driving in all weather conditions has not been shown. In this paper we propose DISC \u2013 Domain Incremental through Statistical Correction \u2013 a simple online zero-forgetting approach which can incrementally learn new tasks (i.e. weather conditions) without requiring re-training or expensive memory banks. The only information we store for each task are the statistical parameters as we categorize each domain by the change in first and second order statistics. Thus, as each task arrives, we simply \u2018plug and play\u2019 the statistical vectors for the corresponding task into the model and it immediately starts to perform well on that task. We show the efficacy of our approach by testing it for object detection in a challenging domain-incremental autonomous driving scenario where we encounter different adverse weather conditions, such as heavy rain, fog, and snow."}}
{"id": "Ajjb87p-X2D", "cdate": 1640995200000, "mdate": 1669194634871, "content": {"title": "A Continual Learning Survey: Defying Forgetting in Classification Tasks", "abstract": "Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern: (1) a taxonomy and extensive overview of the state-of-the-art; (2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner; (3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods; and (4) baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage."}}
{"id": "uXYSZopEgS", "cdate": 1609459200000, "mdate": 1664884765188, "content": {"title": "Ternary Feature Masks: Zero-Forgetting for Task-Incremental Learning", "abstract": "We propose an approach without any forgetting to continual learning for the task-aware regime, where at inference the task-label is known. By using ternary masks we can upgrade a model to new tasks, reusing knowledge from previous tasks while not forgetting anything about them. Using masks prevents both catastrophic forgetting and backward transfer. We argue -- and show experimentally -- that avoiding the former largely compensates for the lack of the latter, which is rarely observed in practice. In contrast to earlier works, our masks are applied to the features (activations) of each layer instead of the weights. This considerably reduces the number of mask parameters for each new task; with more than three orders of magnitude for most networks. The encoding of the ternary masks into two bits per feature creates very little overhead to the network, avoiding scalability issues. To allow already learned features to adapt to the current task without changing the behavior of these features for previous tasks, we introduce task-specific feature normalization. Extensive experiments on several finegrained datasets and ImageNet show that our method outperforms current state-of-the-art while reducing memory overhead in comparison to weight-based approaches."}}
{"id": "gA6C0oBM9I2", "cdate": 1609459200000, "mdate": 1668702651005, "content": {"title": "Avalanche: An End-to-End Library for Continual Learning", "abstract": ""}}
{"id": "byw46gqp58", "cdate": 1609459200000, "mdate": 1667375114489, "content": {"title": "On the importance of cross-task features for class-incremental learning", "abstract": "In class-incremental learning, an agent with limited resources needs to learn a sequence of classification tasks, forming an ever growing classification problem, with the constraint of not being able to access data from previous tasks. The main difference with task-incremental learning, where a task-ID is available at inference time, is that the learner also needs to perform cross-task discrimination, i.e. distinguish between classes that have not been seen together. Approaches to tackle this problem are numerous and mostly make use of an external memory (buffer) of non-negligible size. In this paper, we ablate the learning of cross-task features and study its influence on the performance of basic replay strategies used for class-IL. We also define a new forgetting measure for class-incremental learning, and see that forgetting is not the principal cause of low performance. Our experimental results show that future algorithms for class-incremental learning should not only prevent forgetting, but also aim to improve the quality of the cross-task features, and the knowledge transfer between tasks. This is especially important when tasks contain limited amount of data."}}
{"id": "qrKx27ctMm0", "cdate": 1577836800000, "mdate": 1681499669903, "content": {"title": "Saliency from High-Level Semantic Image Features", "abstract": ""}}
{"id": "OtmOlySiY2i", "cdate": 1577836800000, "mdate": 1682002733465, "content": {"title": "Lifelong Learning of Neural Networks: Detecting Novelty and Adapting to New Domains without Forgetting", "abstract": "La visi\u00f3 per computador ha experimentat canvis considerables en l\u2019\u00faltima d\u00e8cada, ja que les xarxes neuronals han passat a ser d\u2019\u00fas com\u00fa. A mesura que les capacitats computacionals disponibles han crescut, les xarxes neuronals han aconseguit aven\u00e7os en moltes tasques de visi\u00f3 per computador i fins i tot han superat el rendiment hum\u00e0 en altres. Un camp de recerca que ha experimentat un notable augment de l\u2019inter\u00e8s \u00e9s la dels sistemes d\u2019aprenentatge continuat. Aquests sistemes haurien de ser capa\u00e7os de realitzar tasques de manera eficient, identificar-ne i aprendre\u2019n de noves, a m\u00e9s de ser capa\u00e7os de desplegar versions m\u00e9s compactes d\u2019ells mateixos que siguin experts en tasques espec\u00edfiques. En aquesta tesi, contribu\u00efm a la investigaci\u00f3 sobre l\u2019aprenentatge continuat i abordem la compressi\u00f3 i adaptaci\u00f3 de xarxes a dominis m\u00e9s petits, l\u2019aprenentatge incremental de xarxes enfrontades a diverses tasques i, finalment, la detecci\u00f3 d\u2019anomalies i novetats en temps d\u2019infer\u00e8ncia. Explorem com es pot transferir el coneixement des de grans models pre-entrenats a xarxes amb tasques m\u00e9s espec\u00edfiques, capaces d\u2019executar-se en dispositius m\u00e9s limitats extraient la informaci\u00f3 m\u00e9s rellevant. L\u2019\u00fas d\u2019un model pre-entrenat proporciona representacions m\u00e9s robustes i una inicialitzaci\u00f3 m\u00e9s estable quan s\u2019apr\u00e8n una tasca m\u00e9s espec\u00edfica, cosa que comporta un rendiment m\u00e9s alt i es coneix com a adaptaci\u00f3 de domini. Tanmateix, aquests models s\u00f3n massa grans per a determinades aplicacions que cal desplegar en dispositius amb poca mem\u00f2ria i poca capacitat de c\u00e0lcul. En aquesta tesi demostrem que, despr\u00e9s de realitzar l\u2019adaptaci\u00f3 de domini, algunes activacions apreses amb prou feines contribueixen a les prediccions del model. Per tant, proposem aplicar la compressi\u00f3 de xarxa basada en la descomposici\u00f3 de matrius de baix rang mitjan\u00e7ant les estad\u00edstiques de les activacions. Aix\u00f2 es tradueix en una reducci\u00f3 significativa de la mida del model i del cost computacional. Igual que la intel\u00b7lig\u00e8ncia humana, el machine learning pret\u00e9n tenir la capacitat d\u2019aprendre i recordar el coneixement. Tot i aix\u00f2, quan una xarxa neuronal ja entrenada apr\u00e8n una nova tasca, s\u2019acaba oblidant de les anteriors. Aix\u00f2 es coneix com a oblit catastr\u00f2fic i s\u2019estudia la seva prevenci\u00f3 en l\u2019aprenentatge continu. El treball presentat en aquesta tesi estudia \u00e0mpliament les t\u00e8cniques d\u2019aprenentatge continu com una aproximaci\u00f3 per evitar oblits catastr\u00f2fics en escenaris d\u2019aprenentatge seq\u00fcencial. La nostra t\u00e8cnica es basa en l\u2019\u00fas de m\u00e0scares tern\u00e0ries per tal d\u2019actualitzar una xarxa a tasques noves, reutilitzant el coneixement d\u2019altres anteriors sense oblidar res d\u2019elles. A difer\u00e8ncia dels treballs anteriors, les nostres m\u00e0scares s\u2019apliquen a les activacions de cada capa en lloc dels pesos. Aix\u00f2 redueix considerablement el nombre de par\u00e0metres que s\u2019afegiran per a cada nova tasca. A m\u00e9s, analitzem l\u2019estat de l\u2019art en aprenentatge incremental sense acc\u00e9s a l\u2019identificador de tasca. Aix\u00f2 proporciona informaci\u00f3 sobre les direccions de recerca actuals que se centren a evitar l\u2019oblit catastr\u00f2fic mitjan\u00e7ant la regularitzaci\u00f3, l\u2019assaig de tasques anteriors des d\u2019una petita mem\u00f2ria externa o compensant el biaix de la tasca m\u00e9s recent. Les xarxes neuronals entrenades amb una funci\u00f3 de cost basada en entropia creuada obliguen les sortides del model a tendir cap a un vector codificat de sortida \u00fanica. Aix\u00f2 fa que els models tinguin massa confian\u00e7a quan intenten predir imatges o classes que no estaven presents a la distribuci\u00f3 original. La capacitat d\u2019un sistema per ser conscient dels l\u00edmits de les tasques apreses i identificar anomalies o classes que encara no s\u2019han apr\u00e8s \u00e9s clau per a l\u2019aprenentatge continu i els sistemes aut\u00f2noms. En aquesta tesi, presentem un enfocament d\u2019aprenentatge m\u00e8tric per a la detecci\u00f3 d\u2019anomalies que apr\u00e8n la tasca en un espai m\u00e8tric."}}
{"id": "IadFoamdOJ", "cdate": 1577836800000, "mdate": 1681499670603, "content": {"title": "Disentanglement of Color and Shape Representations for Continual Learning", "abstract": ""}}
{"id": "EtMEz6cJ1uy", "cdate": 1577836800000, "mdate": 1667375114565, "content": {"title": "On Class Orderings for Incremental Learning", "abstract": "The influence of class orderings in the evaluation of incremental learning has received very little attention. In this paper, we investigate the impact of class orderings for incrementally learned classifiers. We propose a method to compute various orderings for a dataset. The orderings are derived by simulated annealing optimization from the confusion matrix and reflect different incremental learning scenarios, including maximally and minimally confusing tasks. We evaluate a wide range of state-of-the-art incremental learning methods on the proposed orderings. Results show that orderings can have a significant impact on performance and the ranking of the methods."}}
