{"id": "zglnNwLN-V", "cdate": 1667374482899, "mdate": 1667374482899, "content": {"title": "StickyLocalization: Robust End-To-End Relocalization on Points Clouds using Graph Neural Networks", "abstract": "Relocalization inside pre-built maps provides a big benefit\nin the course of today\u2019s autonomous driving tasks where the\nmap can be considered as an additional sensor for refining\nthe estimated current pose of the vehicle. Due to potentially\nlarge drifts in the initial pose guess as well as maps con-\ntaining unfiltered dynamic and temporal static objects (e.g.\nparking cars), traditional methods like ICP tend to fail and\nshow high computation times. We propose a novel and fast\nrelocalization method for accurate pose estimation inside\na pre-built map based on 3D point clouds. The method is\nrobust against inaccurate initialization caused by low perfor-\nmance GPS systems and tolerates the presence of unfiltered\nobjects by specifically learning to extract significant features\nfrom current scans and adjacent map sections. More specif-\nically, we introduce a novel distance-based matching loss\nenabling us to simultaneously extract important information\nfrom raw point clouds and aggregating inner- and inter-\ncloud context by utilizing self- and cross-attention inside a\nGraph Neural Network. We evaluate StickyLocalization\u2019s\n(SL) performance through an extensive series of experiments\nusing two benchmark datasets in terms of Relocalization\non NuScenes and Loop Closing using KITTI\u2019s Odometry\ndataset. We found that SL outperforms state-of-the art point\ncloud registration and relocalization methods in terms of\ntransformation errors and runtime."}}
{"id": "Drmwuuy_tw", "cdate": 1667374315326, "mdate": 1667374315326, "content": {"title": "Points2Pix: 3D Point-Cloud to Image Translation using conditional GANs", "abstract": "We present the first approach for 3D point-cloud to im-\nage translation based on conditional Generative Adversarial Networks\n(cGAN). The model handles multi-modal information sources from dif-\nferent domains, i.e. raw point-sets and images. The generator is capable\nof processing three conditions, whereas the point-cloud is encoded as raw\npoint-set and camera projection. An image background patch is used as\nconstraint to bias environmental texturing. A global approximation func-\ntion within the generator is directly applied on the point-cloud (Point-\nNet). Hence, the representative learning model incorporates global 3D\ncharacteristics directly at the latent feature space. Conditions are used\nto bias the background and the viewpoint of the generated image. This\nopens up new ways in augmenting or texturing 3D data to aim the gener-\nation of fully individual images. We successfully evaluated our method on\nthe KITTI and SunRGBD dataset with an outstanding object detection\ninception score."}}
{"id": "ieDsTeeA3qs", "cdate": 1640995200000, "mdate": 1668629146520, "content": {"title": "SVDistNet: Self-Supervised Near-Field Distance Estimation on Surround View Fisheye Cameras", "abstract": "A 360\u00b0 perception of scene geometry is essential for automated driving, notably for parking and urban driving scenarios. Typically, it is achieved using surround-view fisheye cameras, focusing on the near-field area around the vehicle. The majority of current depth estimation approaches focus on employing just a single camera, which cannot be straightforwardly generalized to multiple cameras. The depth estimation model must be tested on a variety of cameras equipped to millions of cars with varying camera geometries. Even within a single car, intrinsics vary due to manufacturing tolerances. Deep learning models are sensitive to these changes, and it is practically infeasible to train and test on each camera variant. As a result, we present novel camera-geometry adaptive multi-scale convolutions which utilize the camera parameters as a conditional input, enabling the model to generalize to previously unseen fisheye cameras. Additionally, we improve the distance estimation by pairwise and patchwise vector-based self-attention encoder networks. We evaluate our approach on the Fisheye WoodScape surround-view dataset, significantly improving over previous approaches. We also show a generalization of our approach across different camera viewing angles and perform extensive experiments to support our contributions. To enable comparison with other approaches, we evaluate the front camera data on the KITTI dataset (pinhole camera images) and achieve state-of-the-art performance among self-supervised monocular methods. An overview video with qualitative results is provided at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://youtu.be/bmX0UcU9wtA</uri> . Baseline code and dataset will be made public. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><xref ref-type=\"fn\" rid=\"fn1\">1</xref></sup>"}}
{"id": "YbuRdVJ_Z9r", "cdate": 1640995200000, "mdate": 1668629146587, "content": {"title": "LiMoSeg: Real-time Bird's Eye View based LiDAR Motion Segmentation", "abstract": ""}}
{"id": "McQC_bLDez", "cdate": 1640995200000, "mdate": 1682331158994, "content": {"title": "StickyLocalization: Robust End-To-End Relocalization on Point Clouds using Graph Neural Networks", "abstract": "Relocalization inside pre-built maps provides a big benefit in the course of today\u2019s autonomous driving tasks where the map can be considered as an additional sensor for refining the estimated current pose of the vehicle. Due to potentially large drifts in the initial pose guess as well as maps containing unfiltered dynamic and temporal static objects (e.g. parking cars), traditional methods like ICP tend to fail and show high computation times. We propose a novel and fast relocalization method for accurate pose estimation inside a pre-built map based on 3D point clouds. The method is robust against inaccurate initialization caused by low performance GPS systems and tolerates the presence of unfiltered objects by specifically learning to extract significant features from current scans and adjacent map sections. More specifically, we introduce a novel distance-based matching loss enabling us to simultaneously extract important information from raw point clouds and aggregating inner- and inter-cloud context by utilizing self- and cross-attention inside a Graph Neural Network. We evaluate StickyLocalization\u2019s (SL) performance through an extensive series of experiments using two benchmark datasets in terms of Relocalization on NuScenes and Loop Closing using KITTI\u2019s Odometry dataset. We found that SL outperforms state-of-the art point cloud registration and relocalization methods in terms of transformation errors and runtime."}}
{"id": "uJky4fr54C", "cdate": 1609459200000, "mdate": 1668629146818, "content": {"title": "SVDistNet: Self-Supervised Near-Field Distance Estimation on Surround View Fisheye Cameras", "abstract": "A 360{\\deg} perception of scene geometry is essential for automated driving, notably for parking and urban driving scenarios. Typically, it is achieved using surround-view fisheye cameras, focusing on the near-field area around the vehicle. The majority of current depth estimation approaches focus on employing just a single camera, which cannot be straightforwardly generalized to multiple cameras. The depth estimation model must be tested on a variety of cameras equipped to millions of cars with varying camera geometries. Even within a single car, intrinsics vary due to manufacturing tolerances. Deep learning models are sensitive to these changes, and it is practically infeasible to train and test on each camera variant. As a result, we present novel camera-geometry adaptive multi-scale convolutions which utilize the camera parameters as a conditional input, enabling the model to generalize to previously unseen fisheye cameras. Additionally, we improve the distance estimation by pairwise and patchwise vector-based self-attention encoder networks. We evaluate our approach on the Fisheye WoodScape surround-view dataset, significantly improving over previous approaches. We also show a generalization of our approach across different camera viewing angles and perform extensive experiments to support our contributions. To enable comparison with other approaches, we evaluate the front camera data on the KITTI dataset (pinhole camera images) and achieve state-of-the-art performance among self-supervised monocular methods. An overview video with qualitative results is provided at https://youtu.be/bmX0UcU9wtA. Baseline code and dataset will be made public."}}
{"id": "dWqOl1jjvhO", "cdate": 1609459200000, "mdate": 1668629146787, "content": {"title": "LiMoSeg: Real-time Bird's Eye View based LiDAR Motion Segmentation", "abstract": "Moving object detection and segmentation is an essential task in the Autonomous Driving pipeline. Detecting and isolating static and moving components of a vehicle's surroundings are particularly crucial in path planning and localization tasks. This paper proposes a novel real-time architecture for motion segmentation of Light Detection and Ranging (LiDAR) data. We use three successive scans of LiDAR data in 2D Bird's Eye View (BEV) representation to perform pixel-wise classification as static or moving. Furthermore, we propose a novel data augmentation technique to reduce the significant class imbalance between static and moving objects. We achieve this by artificially synthesizing moving objects by cutting and pasting static vehicles. We demonstrate a low latency of 8 ms on a commonly used automotive embedded platform, namely Nvidia Jetson Xavier. To the best of our knowledge, this is the first work directly performing motion segmentation in LiDAR BEV space. We provide quantitative results on the challenging SemanticKITTI dataset, and qualitative results are provided in https://youtu.be/2aJ-cL8b0LI."}}
{"id": "XPhXRhI25K2", "cdate": 1609459200000, "mdate": 1668629146909, "content": {"title": "BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object Detection for Autonomous Driving", "abstract": "3D object detection based on LiDAR point clouds is a crucial module in autonomous driving particularly for long range sensing. Most of the research is focused on achieving higher accuracy and these models are not optimized for deployment on embedded systems from the perspective of latency and power efficiency. For high speed driving scenarios, latency is a crucial parameter as it provides more time to react to dangerous situations. Typically a voxel or point-cloud based 3D convolution approach is utilized for this module. Firstly, they are inefficient on embedded platforms as they are not suitable for efficient parallelization. Secondly, they have a variable runtime due to level of sparsity of the scene which is against the determinism needed in a safety system. In this work, we aim to develop a very low latency algorithm with fixed runtime. We propose a novel semantic segmentation architecture as a single unified model for object center detection using key points, box predictions and orientation prediction using binned classification in a simpler Bird's Eye View (BEV) 2D representation. The proposed architecture can be trivially extended to include semantic segmentation classes like road without any additional computation. The proposed model has a latency of 4 ms on the embedded Nvidia Xavier platform. The model is 5X faster than other top accuracy models with a minimal accuracy degradation of 2% in Average Precision at <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">${\\mathbf{I}\\mathbf{o}\\mathbf{U}=\\boldsymbol{0.5}}$</tex> on KITTI dataset."}}
{"id": "TaS3q3ZiUYO2", "cdate": 1609459200000, "mdate": 1668629146860, "content": {"title": "SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation Synergized with Semantic Segmentation for Autonomous Driving", "abstract": "State-of-the-art self-supervised learning approaches for monocular depth estimation usually suffer from scale ambiguity. They do not generalize well when applied on distance estimation for complex projection models such as in fisheye and omnidirectional cameras. This paper introduces a novel multi-task learning strategy to improve self-supervised monocular distance estimation on fisheye and pinhole camera images. Our contribution to this work is threefold: Firstly, we introduce a novel distance estimation network architecture using a self-attention based encoder coupled with robust semantic feature guidance to the decoder that can be trained in a one-stage fashion. Secondly, we integrate a generalized robust loss function, which improves performance significantly while removing the need for hyperparameter tuning with the reprojection loss. Finally, we reduce the artifacts caused by dynamic objects violating static world assumptions using a semantic masking strategy. We significantly improve upon the RMSE of previous work on fisheye by 25% reduction in RMSE. As there is little work on fisheye cameras, we evaluated the proposed method on KITTI using a pinhole model. We achieved state-of-the-art performance among self-supervised methods without requiring an external scale estimation."}}
{"id": "SZwpraO3GQg", "cdate": 1609459200000, "mdate": 1668629146724, "content": {"title": "OmniDet: Surround View Cameras Based Multi-Task Visual Perception Network for Autonomous Driving", "abstract": ""}}
