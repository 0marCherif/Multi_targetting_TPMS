{"id": "4fGx_EHhc2", "cdate": 1676827090171, "mdate": null, "content": {"title": "Benefits of Monotonicity in Safe Exploration with Gaussian Processes", "abstract": "We consider the problem of sequentially maximising an unknown function over a set of actions while ensuring that every sampled point has a function value below a given safety threshold. We model the function using kernel-based and Gaussian process methods, while differing from previous works in our assumption that the function is monotonically increasing with respect to a safety variable. This assumption is motivated by various practical applications such as adaptive clinical trial design and robotics.  Taking inspiration from the GP-UCB and SAFEOPT algorithms, we propose an algorithm, monotone safe UCB (M-SafeUCB) for this task.  We show that M-SafeUCB enjoys theoretical guarantees in terms of safety, a suitably-defined regret notion, and approximately finding the entire safe boundary. In addition, we illustrate that the monotonicity assumption yields significant benefits in terms of the guarantees obtained, as well as algorithmic simplicity and efficiency. We support our theoretical findings by performing empirical evaluations on a variety of functions, including a simulated clinical trial experiment."}}
{"id": "icGMu0iPonB", "cdate": 1652737703207, "mdate": null, "content": {"title": "A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits", "abstract": "We consider the sequential optimization of an unknown, continuous, and expensive to evaluate reward function, from noisy and adversarially corrupted observed rewards. When the corruption attacks are subject to a suitable budget $C$ and the function lives in a Reproducing Kernel Hilbert Space (RKHS), the problem can be posed as {\\em corrupted Gaussian process (GP) bandit optimization}. We propose a novel robust elimination-type algorithm that runs in epochs, combines exploration with infrequent switching to select a small subset of actions, and plays each action for multiple time instants. Our algorithm, {\\em Robust GP Phased Elimination (RGP-PE)}, successfully balances robustness to corruptions with exploration and exploitation such that its performance degrades minimally in the presence (or absence) of adversarial corruptions. When $T$ is the number of samples and $\\gamma_T$ is the maximal information gain, the corruption-dependent term in our regret bound is $O(C \\gamma_T^{3/2})$, which is significantly tighter than the existing $O(C \\sqrt{T \\gamma_T})$ for several commonly-considered kernels. We perform the first empirical study of robustness in the corrupted GP bandit setting, and show that our algorithm is robust against a variety of adversarial attacks."}}
{"id": "HF8-fgokPgq", "cdate": 1640995200000, "mdate": 1645832937743, "content": {"title": "Performance Bounds for Group Testing With Doubly-Regular Designs", "abstract": "In the group testing problem, the goal is to identify a subset of defective items within a larger set of items based on tests whose outcomes indicate whether any defective item is present. This problem is relevant in areas such as medical testing, DNA sequencing, and communications. In this paper, we study a doubly-regular design in which the number of tests-per-item and the number of items-per-test are fixed. We analyze the performance of this test design alongside the Definite Defectives (DD) decoding algorithm in several settings, namely, (i) the sub-linear regime $k=o(n)$ with exact recovery, (ii) the linear regime $k=\\Theta(n)$ with approximate recovery, and (iii) the size-constrained setting, where the number of items per test is constrained. Under setting (i), we show that our design together with the DD algorithm, matches an existing achievability result for the DD algorithm with the near-constant tests-per-item design, which is known to be asymptotically optimal in broad scaling regimes. Under setting (ii), we provide novel approximate recovery bounds that complement a hardness result regarding exact recovery. Lastly, under setting (iii), we improve on the best known upper and lower bounds in scaling regimes where the maximum test size grows with the total number of items."}}
{"id": "BzfZflsJvg5", "cdate": 1640995200000, "mdate": 1645832937742, "content": {"title": "Improved Convergence Rates for Sparse Approximation Methods in Kernel-Based Learning", "abstract": "Kernel-based models such as kernel ridge regression and Gaussian processes are ubiquitous in machine learning applications for regression and optimization. It is well known that a major downside for kernel-based models is the high computational cost; given a dataset of $n$ samples, the cost grows as $\\mathcal{O}(n^3)$. Existing sparse approximation methods can yield a significant reduction in the computational cost, effectively reducing the actual cost down to as low as $\\mathcal{O}(n)$ in certain cases. Despite this remarkable empirical success, significant gaps remain in the existing results for the analytical bounds on the error due to approximation. In this work, we provide novel confidence intervals for the Nystr\\\"om method and the sparse variational Gaussian process approximation method, which we establish using novel interpretations of the approximate (surrogate) posterior variance of the models. Our confidence intervals lead to improved performance bounds in both regression and optimization problems."}}
{"id": "BCMWMgj1vlc", "cdate": 1640995200000, "mdate": 1645832937741, "content": {"title": "A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits", "abstract": "We consider the sequential optimization of an unknown, continuous, and expensive to evaluate reward function, from noisy and adversarially corrupted observed rewards. When the corruption attacks are subject to a suitable budget $C$ and the function lives in a Reproducing Kernel Hilbert Space (RKHS), the problem can be posed as corrupted Gaussian process (GP) bandit optimization. We propose a novel robust elimination-type algorithm that runs in epochs, combines exploration with infrequent switching to select a small subset of actions, and plays each action for multiple time instants. Our algorithm, Robust GP Phased Elimination (RGP-PE), successfully balances robustness to corruptions with exploration and exploitation such that its performance degrades minimally in the presence (or absence) of adversarial corruptions. When $T$ is the number of samples and $\\gamma_T$ is the maximal information gain, the corruption-dependent term in our regret bound is $O(C \\gamma_T^{3/2})$, which is significantly tighter than the existing $O(C \\sqrt{T \\gamma_T})$ for several commonly-considered kernels. We perform the first empirical study of robustness in the corrupted GP bandit setting, and show that our algorithm is robust against a variety of adversarial attacks."}}
{"id": "pgir5f7ekAL", "cdate": 1632875466115, "mdate": null, "content": {"title": "Generative Principal Component Analysis", "abstract": "In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the first principal eigenvector lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\\sqrt{\\frac{k\\log L}{m}}$, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges exponentially fast to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis."}}
{"id": "k7Q71M4BPNK", "cdate": 1621629863191, "mdate": null, "content": {"title": "Towards Sample-Optimal Compressive Phase Retrieval with Sparse and Generative Priors", "abstract": "Compressive phase retrieval is a popular variant of the standard compressive sensing problem in which the measurements only contain magnitude information. In this paper, motivated by recent advances in deep generative models, we provide recovery guarantees with near-optimal sample complexity for phase retrieval with generative priors. We first show that when using i.i.d. Gaussian measurements and an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs, roughly $O(k \\log L)$ samples suffice to guarantee that any signal minimizing an amplitude-based empirical loss function is close to the true signal. Attaining this sample complexity with a practical algorithm remains a difficult challenge, and finding a good initialization for gradient-based methods has been observed to pose a major bottleneck. To partially address this, we further show that roughly $O(k \\log L)$ samples ensure sufficient closeness between the underlying signal and any {\\em globally optimal} solution to an optimization problem designed for spectral initialization (though finding such a solution may still be challenging). We also adapt this result to sparse phase retrieval, and show that $O(s \\log n)$ samples are sufficient for a similar guarantee when the underlying signal is $s$-sparse and $n$-dimensional, matching an information-theoretic lower bound. While these guarantees do not directly correspond to a practical algorithm, we propose a practical spectral initialization method motivated by our findings, and experimentally observe performance gains over various existing spectral initialization methods for sparse phase retrieval."}}
{"id": "RUbjIx295j", "cdate": 1620868447459, "mdate": null, "content": {"title": "Learning-based Compressive Subsampling", "abstract": "The problem of recovering a structured signal x\u2208\u2102p from a set of dimensionality-reduced linear measurements b=Ax arises in a variety of applications, such as medical imaging, spectroscopy, Fourier optics, and computerized tomography. Due to computational and storage complexity or physical constraints imposed by the problem, the measurement matrix A\u2208\u2102n\u00d7p is often of the form A=P\u03a9\u03a8 for some orthonormal basis matrix \u03a8\u2208\u2102p\u00d7p and subsampling operator P\u03a9:\u2102p\u2192\u2102n that selects the rows indexed by \u03a9. This raises the fundamental question of how best to choose the index set \u03a9 in order to optimize the recovery performance. Previous approaches to addressing this question rely on non-uniform \\emph{random} subsampling using application-specific knowledge of the structure of x. In this paper, we instead take a principled learning-based approach in which a \\emph{fixed} index set is chosen based on a set of training signals x1,\u2026,xm. We formulate combinatorial optimization problems seeking to maximize the energy captured in these signals in an average-case or worst-case sense, and we show that these can be efficiently solved either exactly or approximately via the identification of modularity and submodularity structures. We provide both deterministic and statistical theoretical guarantees showing how the resulting measurement matrices perform on signals differing from the training signals, and we provide numerical examples showing our approach to be effective on a variety of data sets."}}
{"id": "RVSev349aL7", "cdate": 1620868297685, "mdate": null, "content": {"title": "The Generalized Lasso with Nonlinear Observations and Generative Priors", "abstract": "In this paper, we study the problem of signal estimation from non-linear and noisy Gaussian observations when the unknown $n$-dimensional signal is in the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We make the assumption of sub-Gaussian observations, which is automatically satisfied by a wide range of measurement models, such as linear, logistic, 1-bit, and other quantized models.  We further consider corrupted observations due to adversarial noise. Our analysis is based on a generalized Lasso approach (Plan and Vershynin, 2016). We first provide a non-uniform recovery guarantee, which states that under i.i.d.~Gaussian measurements, roughly $O\\left(\\frac{k}{\\epsilon^2}\\log L\\right)$ samples suffice for recovery with an $\\ell_2$-error of $\\epsilon$, and that this scheme is robust to adversarial noise.  Then, we apply this result to neural network generative models, and discuss various extensions to other models and measurement matrices. Moreover, we show that our result can be extended to the uniform recovery case whenever a so-called local embedding property holds.  For instance, under 1-bit measurements, this recovers an existing $O\\left(\\frac{k}{\\epsilon^2}\\log L\\right)$ sample complexity bound with the advantage of considering an algorithm that is more amenable to practical implementation."}}
{"id": "V9zQ68qvVyx", "cdate": 1620868066172, "mdate": null, "content": {"title": "On the All-Or-Nothing Behavior of Bernoulli Group Testing", "abstract": "In this paper, we study the problem of non-adaptive group testing, in which one seeks to identify which items are defective given a set of suitably-designed tests whose outcomes indicate whether or not at least one defective item was included in the test. The most widespread recovery criterion seeks to exactly recover the entire defective set, and relaxed criteria such as approximate recovery and list decoding have also been considered. In this paper, we study the fundamental limits of group testing under the significantly relaxed {\\em weak recovery} criterion, which only seeks to identify a small fraction (e.g., 0.01) of the defective items. Given the near-optimality of i.i.d.~Bernoulli testing for exact recovery in sufficiently sparse scaling regimes, it is natural to ask whether this design additionally succeeds with much fewer tests under weak recovery. Our main negative result shows that this is not the case, and in fact, under i.i.d.~Bernoulli random testing in the sufficiently sparse regime, an {\\em all-or-nothing} phenomenon occurs: When the number of tests is slightly below a threshold, weak recovery is impossible, whereas when the number of tests is slightly above the same threshold, high-probability exact recovery is possible. In establishing this result, we additionally prove similar negative results under Bernoulli designs for the weak detection problem (distinguishing between the group testing model vs.~completely random outcomes) and the problem of identifying a single item that is definitely defective. On the positive side, we show that all three relaxed recovery criteria can be attained using considerably fewer tests under suitably-chosen non-Bernoulli designs."}}
