{"id": "WQDToWg4Vt", "cdate": 1675209600000, "mdate": 1682339004725, "content": {"title": "Block Factor-Width-Two Matrices and Their Applications to Semidefinite and Sum-of-Squares Optimization", "abstract": ""}}
{"id": "W-Mc7hQMzEd", "cdate": 1672531200000, "mdate": 1681660734483, "content": {"title": "Diagnosing and Preventing Instabilities in Recurrent Video Processing", "abstract": "Recurrent models are a popular choice for video enhancement tasks such as video denoising or super-resolution. In this work, we focus on their stability as dynamical systems and show that they tend to fail catastrophically at inference time on long video sequences. To address this issue, we (1) introduce a diagnostic tool which produces input sequences optimized to trigger instabilities and that can be interpreted as visualizations of temporal receptive fields, and (2) propose two approaches to enforce the stability of a model during training: constraining the spectral norm or constraining the stable rank of its convolutional layers. We then introduce <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Stable Rank Normalization for Convolutional layers</i> (SRN-C), a new algorithm that enforces these constraints. Our experimental results suggest that SRN-C successfully enforces stablility in recurrent video processing models without a significant performance loss."}}
{"id": "_BoPed4tYww", "cdate": 1663850294187, "mdate": null, "content": {"title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints", "abstract": "Many real-world settings involve costs for performing actions; transaction costs\nin financial systems and fuel costs being common examples. In these settings,\nperforming actions at each time step quickly accumulates costs leading to vastly\nsuboptimal outcomes. Additionally, repeatedly acting produces wear and tear and\nultimately, damage. Determining when to act is crucial for achieving successful\noutcomes and yet, the challenge of efficiently learning to behave optimally when\nactions incur minimally bounded costs remains unresolved. In this paper, we intro-\nduce a reinforcement learning (RL) framework named Learnable Impulse Control\nReinforcement Algorithm (LICRA), for learning to optimally select both when\nto act and which actions to take when actions incur costs. At the core of LICRA\nis a nested structure that combines RL and a form of policy known as impulse\ncontrol which learns to maximise objectives when actions incur costs. We prove\nthat LICRA, which seamlessly adopts any RL method, converges to policies that\noptimally select when to perform actions and their optimal magnitudes. We then\naugment LICRA to handle problems in which the agent can perform at most k < \u221e\nactions and more generally, faces a budget constraint. We show LICRA learns the\noptimal value function and ensures budget constraints are satisfied almost surely.\nWe demonstrate empirically LICRA\u2019s superior performance against benchmark\nRL methods in OpenAI gym\u2019s Lunar Lander and in Highway environments and a\nvariant of the Merton portfolio problem within finance."}}
{"id": "jK02XX9ZpJkt", "cdate": 1663850146012, "mdate": null, "content": {"title": "CAMA: A New Framework for Safe Multi-Agent Reinforcement Learning  Using Constraint Augmentation", "abstract": "With the widespread application of multi-agent reinforcement learning (MARL) in real-life settings, the ability to meet safety constraints has become an urgent problem to solve. For example, it is necessary to avoid collisions to reach a common goal in controlling multiple drones. We address this problem by introducing the Constraint Augmented Multi-Agent framework --- CAMA. CAMA can serve as a plug-and-play module to the popular MARL algorithms, including centralized training, decentralized execution and independent learning frameworks. In our approach, we represent the safety constraint as the sum of discounted safety costs bounded by the predefined value, which we call the safety budget. Experiments demonstrate that CAMA can converge quickly to a high degree of constraint satisfaction and surpasses other state-of-the-art safety counterpart algorithms in both cooperative and competitive settings. "}}
{"id": "jAD0chIdt_", "cdate": 1663850136308, "mdate": null, "content": {"title": "Impulse Control Arbitration for A Dual System of Exploitation and Exploration", "abstract": "Efficient reinforcement learning (RL) involves a trade-off between \"exploitative\" actions that maximise expected reward and ``explorative\" ones that lead to the visitation of \"novel\" states. To encourage exploration, existing methods proposed methods such as injecting stochasticity into action selection, implicit regularisation, and additive synthetic reward. However, these techniques do not necessarily offer entirely systematic approaches making this trade-off. Here we introduce SElective Reinforcement EXploration (SEREX), a plug-and-play framework that casts the exploration-exploitation trade-off as a game between an RL agent--- exploiter, which purely exploits task-dependent rewards, and another RL agent--- switcher, which chooses at which states to activate a pure exploration policy that is trained to minimise system uncertainty and override Exploiter. Using a form of policies known as impulse control, switcher is able to determine the best set of states to switch to the exploration policy while Exploiter is free to execute its actions everywhere else. We prove that SEREX converges quickly and induces a natural schedule towards pure exploitation. Through extensive empirical studies in both discrete and continuous control benchmarks, we show that with minimal modification, SEREX can be readily combined with existing RL algorithms and yields significant improvement in performance."}}
{"id": "PTrcWS4aRnH", "cdate": 1652963729132, "mdate": 1652963729132, "content": {"title": "SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation", "abstract": "Satisfying safety constraints almost surely (or with probability one) can be critical for deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows to view Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be \"sauteed\". Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance."}}
{"id": "zkk_7sV6gm8", "cdate": 1652737591088, "mdate": null, "content": {"title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints", "abstract": "Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage.  Determining when to act is crucial for achieving successful outcomes and yet, the challenge of efficiently \\textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a  reinforcement learning (RL) framework named Learnable Impulse Control Reinforcement Algorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \\textit{impulse control} which learns to maximise objectives when actions incur costs. We prove that LICRA, which seamlessly adopts any RL method, converges to policies that optimally select when to perform actions and their optimal magnitudes. We then augment LICRA to handle problems in which the agent can perform at most $k<\\infty$ actions and more generally, faces a budget constraint. We show LICRA learns the optimal value function and ensures budget constraints are satisfied almost surely. We demonstrate empirically LICRA's superior performance against benchmark RL methods in OpenAI gym's Lunar Lander and in Highway environments."}}
{"id": "GH4q4WmGAsl", "cdate": 1652737544339, "mdate": null, "content": {"title": "Enhancing Safe Exploration Using Safety State Augmentation", "abstract": "Safe exploration is a challenging and important problem in model-free reinforcement learning (RL). Often the safety cost is sparse and unknown, which unavoidably leads to constraint violations - a phenomenon ideally to be avoided in safety-critical applications. We tackle this problem by augmenting the state-space with a safety state, which is nonnegative if and only if the constraint is satisfied. The value of this state also serves as a distance toward constraint violation, while its initial value indicates the available safety budget. This idea allows us to derive policies for scheduling the safety budget during training. We call our approach Simmer (Safe policy IMproveMEnt for RL) to reflect the careful nature of these schedules. We apply this idea to two safe RL problems: RL with constraints imposed on an average cost, and RL with constraints imposed on a cost with probability one. Our experiments suggest that \"simmering\" a safe algorithm can improve safety during training for both settings. We further show that Simmer can stabilize training and improve the performance of safe RL with average constraints. "}}
{"id": "rg57WluTPJ3", "cdate": 1640995200000, "mdate": 1681726024898, "content": {"title": "SEREN: Knowing When to Explore and When to Exploit", "abstract": "Efficient reinforcement learning (RL) involves a trade-off between \"exploitative\" actions that maximise expected reward and \"explorative'\" ones that sample unvisited states. To encourage exploration, recent approaches proposed adding stochasticity to actions, separating exploration and exploitation phases, or equating reduction in uncertainty with reward. However, these techniques do not necessarily offer entirely systematic approaches making this trade-off. Here we introduce SElective Reinforcement Exploration Network (SEREN) that poses the exploration-exploitation trade-off as a game between an RL agent -- \\exploiter, which purely exploits known rewards, and another RL agent -- \\switcher, which chooses at which states to activate a pure exploration policy that is trained to minimise system uncertainty and override Exploiter. Using a form of policies known as impulse control, \\switcher is able to determine the best set of states to switch to the exploration policy while Exploiter is free to execute its actions everywhere else. We prove that SEREN converges quickly and induces a natural schedule towards pure exploitation. Through extensive empirical studies in both discrete (MiniGrid) and continuous (MuJoCo) control benchmarks, we show that SEREN can be readily combined with existing RL algorithms to yield significant improvement in performance relative to state-of-the-art algorithms."}}
{"id": "nljJ_I8yJr6", "cdate": 1640995200000, "mdate": 1682339004552, "content": {"title": "Saute RL: Almost Surely Safe Reinforcement Learning Using State Augmentation", "abstract": ""}}
