{"id": "GMANGs2ajm", "cdate": 1667243748340, "mdate": 1667243748340, "content": {"title": "iDECODe: In-distribution equivariance for conformal out-of-distribution detection", "abstract": "Machine learning methods such as deep neural networks (DNNs), despite their success across different domains, are known to often generate incorrect predictions with high confidence on inputs outside their training distribution. The deployment of DNNs in safety-critical domains requires detection of out-of-distribution (OOD) data so that DNNs can abstain from making predictions on those. A number of methods have been recently developed for OOD detection, but there is still room for improvement. We propose the new method iDECODe, leveraging in-distribution equivariance for conformal OOD detection. It relies on a novel base non-conformity measure and a new aggregation method, used in the inductive conformal anomaly detection framework, thereby guaranteeing a bounded false detection rate. We demonstrate the efficacy of iDECODe by experiments on image and audio datasets, obtaining state-of-the-art results. We also show that iDECODe can detect adversarial examples. Code, pre-trained models, and data are available at https://github. com/ramneetk/iDECODe."}}
{"id": "PnsDBLS7OlC", "cdate": 1667243644485, "mdate": 1667243644485, "content": {"title": "On Diverse System-Level Design Using Manifold Learning and Partial Simulated Annealing", "abstract": "The goal in system-level design is to generate a diverse set of high-performing design configurations that\nallow trade-offs across different objectives and avoid early concretization. We use deep generative models\nto learn a manifold of the valid design space, followed by Monte Carlo sampling to explore and optimize\ndesign over the learned manifold, producing a diverse set of optimal designs. We demonstrate the efficacy\nof our proposed approach on the design of an SAE race vehicle and propeller.\n"}}
{"id": "T3UwBTNXhmo", "cdate": 1667243567990, "mdate": 1667243567990, "content": {"title": "Detecting Out-Of-Context Objects Using Graph Context Reasoning Network", "abstract": "This paper presents an approach for detecting outof-context (OOC) objects in images. Given an image with a set of objects, our goal is to determine\nif an object is inconsistent with the contextual relations and detect the OOC object with a bounding\nbox. In this work, we consider common contextual relations such as co-occurrence relations, the\nrelative size of an object with respect to other objects, and the position of the object in the scene.\nWe posit that contextual cues are useful to determine object labels for in-context objects and inconsistent context cues are detrimental to determining object labels for out-of-context objects. To realize this hypothesis, we propose a graph contextual reasoning network (GCRN) to detect OOC objects. GCRN consists of two separate graphs to predict object labels based on the contextual cues in\nthe image: 1) a representation graph to learn object features based on the neighboring objects and\n2) a context graph to explicitly capture contextual\ncues from the neighboring objects. GCRN explicitly captures the contextual cues to improve the detection of in-context objects and identify objects\nthat violate contextual relations. In order to evaluate our approach, we create a large-scale dataset\nby adding OOC object instances to the COCO images. We also evaluate on recent OCD benchmark.\nOur results show that GCRN outperforms competitive baselines in detecting OOC objects and correctly detecting in-context objects. Code and data:\nhttps://nusci.csl.sri.com/project/trinity-ooc"}}
{"id": "eqNpg2HMNi1", "cdate": 1632875545230, "mdate": null, "content": {"title": "Physical System Design Using Hamiltonian Monte Carlo over Learned Manifolds", "abstract": "The design of complex physical systems entails satisfying several competing performance objectives. In practice, some design requirements are often implicit in the intuition and knowledge of designers who have many years of experience working with similar designs. Designers use this experience to sample a few promising candidates in the design space and evaluate or simulate them using detailed, typically slow multiphysics models. The goal in design is usually to generate a diverse set of high-performing design configurations that allow trade-offs across different objectives and avoid early concretization. In this paper, we develop a machine learning approach to automate physical system design. We use deep generative models to learn a manifold of the valid design space, followed by Hamiltonian Monte Carlo (HMC) with simulated annealing to explore and optimize design over the learned manifold, producing a diverse set of optimal designs. Our approach is akin to partial simulated annealing restricted to the learned design manifold, where the annealing schedule is varied to trade-off different objectives. To prevent our approach from traversing off the design manifold and proposing unreliable designs, we leverage Monte Carlo dropout as a way to detect and avoid design configurations where the learned model cannot be trusted. We demonstrate the efficacy of our proposed approach using several case studies that include the design of an SAE race vehicle, propeller, and air vehicle. Across these case studies, we successfully show how our method generates high-performing and diverse designs. "}}
{"id": "zleOqnAUZzl", "cdate": 1601308259920, "mdate": null, "content": {"title": "Are all outliers alike?  On Understanding the Diversity of Outliers for Detecting OODs", "abstract": "Deep neural networks (DNNs) are known to produce incorrect predictions with very high confidence on out-of-distribution (OOD) inputs. This limitation is one of the key challenges in the adoption of deep learning models in high-assurance systems such as autonomous driving, air traffic management, and medical diagnosis. This challenge has received significant attention recently, and several techniques have been developed to detect inputs where the model's prediction cannot be trusted. These techniques use different statistical, geometric, or topological signatures. This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. We demonstrate how different existing detection approaches fail to detect certain types of outliers. We utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. Our results include experiments on CIFAR10, SVHN and MNIST as in-distribution data and Imagenet, LSUN, SVHN (for CIFAR10), CIFAR10 (for SVHN), KMNIST, and F-MNIST as OOD data across different DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5."}}
{"id": "Hn0tM83cAS", "cdate": 1582834747346, "mdate": null, "content": {"title": "Monocular Depth Estimation Using Neural Regression Forest", "abstract": "\nThis paper presents a novel deep architecture, called\nneural regression forest (NRF), for depth estimation from\na single image. NRF combines random forests and convolutional neural networks (CNNs). Scanning windows extracted from the image represent samples which are passed\ndown the trees of NRF for predicting their depth. At every\ntree node, the sample is filtered with a CNN associated with\nthat node. Results of the convolutional filtering are passed\nto left and right children nodes, i.e., corresponding CNNs,\nwith a Bernoulli probability, until the leaves, where depth\nestimations are made. CNNs at every node are designed\nto have fewer parameters than seen in recent work, but\ntheir stacked processing along a path in the tree effectively\namounts to a deeper CNN. NRF allows for parallelizable\ntraining of all \u201cshallow\u201d CNNs, and efficient enforcing of\nsmoothness in depth estimation results. Our evaluation on\nthe benchmark Make3D and NYUv2 datasets demonstrates\nthat NRF outperforms the state of the art, and gracefully\nhandles gradually decreasing training datasets.\n"}}
{"id": "AtcpkKb2Ce", "cdate": 1582834642168, "mdate": null, "content": {"title": "A Multi-Scale CNN for Affordance Segmentation in RGB Images", "abstract": "Given a single RGB image our goal is to label every pixel\nwith an affordance type. By affordance, we mean an object\u2019s capability\nto readily support a certain human action, without requiring precursor\nactions. We focus on segmenting the following five affordance types in\nindoor scenes: \u2018walkable\u2019, \u2018sittable\u2019, \u2018lyable\u2019, \u2018reachable\u2019, and \u2018movable\u2019.\nOur approach uses a deep architecture, consisting of a number of multiscale convolutional neural networks, for extracting mid-level visual cues\nand combining them toward affordance segmentation. The mid-level cues\ninclude depth map, surface normals, and segmentation of four types of\nsurfaces \u2013 namely, floor, structure, furniture and props. For evaluation,\nwe augmented the NYUv2 dataset with new ground-truth annotations\nof the five affordance types. We are not aware of prior work which starts\nfrom pixels, infers mid-level cues, and combines them in a feed-forward\nfashion for predicting dense affordance maps of a single RGB image."}}
{"id": "n9w91QABwR", "cdate": 1582834517859, "mdate": null, "content": {"title": "Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation", "abstract": "\n\nThis paper addresses the problem of weakly supervised\nsemantic image segmentation. Our goal is to label every\npixel in a new image, given only image-level object labels\nassociated with training images. Our problem statement\ndiffers from common semantic segmentation, where pixelwise annotations are typically assumed available in training.\nWe specify a novel deep architecture which fuses three distinct computation processes toward semantic segmentation \u2013\nnamely, (i) the bottom-up computation of neural activations\nin a CNN for the image-level prediction of object classes;\n(ii) the top-down estimation of conditional likelihoods of\nthe CNN\u2019s activations given the predicted objects, resulting in probabilistic attention maps per object class; and\n(iii) the lateral attention-message passing from neighboring\nneurons at the same CNN layer. The fusion of (i)-(iii) is\nrealized via a conditional random field as recurrent network\naimed at generating a smooth and boundary-preserving segmentation. Unlike existing work, we formulate a unified\nend-to-end learning of all components of our deep architecture. Evaluation on the benchmark PASCAL VOC 2012\ndataset demonstrates that we outperform reasonable weakly\nsupervised baselines and state-of-the-art approaches.\n"}}
{"id": "6Md5bCKfvR", "cdate": 1579721146254, "mdate": null, "content": {"title": "Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment", "abstract": "We address the problem of grounding free-form textual phrases by using weak supervision from image-caption pairs. We propose a novel end-to-end model that uses caption-to-image retrieval as a `downstream' task to guide the process of phrase localization. Our method, as a first step, infers the latent correspondences between regions-of-interest (RoIs) and phrases in the caption and creates a discriminative image representation using these matched RoIs. In a subsequent step, this (learned) representation is aligned with the caption. Our key contribution lies in building this `caption-conditioned' image encoding which tightly couples both the tasks and allows the weak supervision to effectively guide visual grounding. We provide an extensive empirical and qualitative analysis to investigate the different components of our proposed model and compare it with competitive baselines. For phrase localization, we report an improvement of 4.9% (absolute) over the prior state-of-the-art on the VisualGenome dataset. We also report results that are at par with the state-of-the-art on the downstream caption-to-image retrieval task on COCO and Flickr30k datasets."}}
