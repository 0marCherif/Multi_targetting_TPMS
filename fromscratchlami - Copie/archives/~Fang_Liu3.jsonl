{"id": "ztwg92Nk31", "cdate": 1683902385163, "mdate": null, "content": {"title": "A New Bound for Privacy Loss from Bayesian Posterior Sampling", "abstract": "Differential privacy (DP) is a state-of-the-art concept that formalizes privacy guarantees. We derive a new bound for the privacy loss from releasing Bayesian posterior samples in the setting of DP. The new bound is tighter than the existing bounds for common Bayesian models and is also consistent with the likelihood principle. We apply the privacy loss quantified by the new bound to release differentially private synthetic data from Bayesian models in several experiments and show the improved utility of the synthetic data compared to those generated from explicitly designed randomization mechanisms that privatize posterior distributions."}}
{"id": "-7m8Iywd7H", "cdate": 1683902158055, "mdate": null, "content": {"title": "Disclosure Risk From Homogeneity Attack in Differentially Privately Sanitized Frequency Distribution", "abstract": "Differential privacy (DP) provides a robust model to achieve privacy guarantees for released information. We examine the protection potency of sanitized multi-dimensional frequency distributions (FDs) via DP mechanisms against homogeneity attack (HA). Adversaries can obtain the exact values on sensitive attributes of their targets through HA without having to identify them from released data. We propose measures for disclosure risk (DR) from HA and derive closed-form relations between the privacy loss parameters and DR from HA. The availability of the closed-form relations will assist practitioners in understanding the abstract concepts of DP and privacy loss parameters by putting them in the context of a concrete privacy attack and offer a perspective for choosing privacy loss parameters when employing DP mechanisms. We apply the derived mathematical relations in real data to demonstrate the assessment of DR from HA on differentially privately sanitized FDs at various privacy loss parameters. The results suggest that relations between DR from HA and privacy loss are S-shaped; the former may not disappear even when privacy loss approaches 0."}}
{"id": "mHc4LzSGgf", "cdate": 1672531200000, "mdate": 1684131015810, "content": {"title": "Privacy-Preserving Travel Time Prediction With Uncertainty Using GPS Trace Data", "abstract": "The rapid growth of GPS technology and mobile devices has led to a massive accumulation of location data, bringing considerable benefits to individuals and society. One of the major usages of such data is travel time prediction, a typical service provided by GPS navigation devices and apps. Meanwhile, the constant collection and analysis of the individual location data also pose unprecedented privacy threats. We leverage the notion of geo-indistinguishability, an extension of differential privacy to the location privacy setting, and propose a procedure for privacy-preserving travel time prediction without collecting actual individual GPS trace data. We propose new concepts to examine the impact of geo-indistinguishability-based sanitization on the usefulness of GPS traces and provide analytical and experimental utility analysis for privacy-preserving travel time prediction. We also propose new metrics to measure the adversary error in learning individual GPS traces from the collected sanitized data. Our experiment results suggest that the proposed procedure provides travel time prediction with satisfactory accuracy at reasonably small privacy costs."}}
{"id": "esdjPuKnmOq", "cdate": 1640995200000, "mdate": 1684131015841, "content": {"title": "Variational inference with NoFAS: Normalizing flow with adaptive surrogate for computationally expensive models", "abstract": ""}}
{"id": "QrnVzJbzZL", "cdate": 1640995200000, "mdate": 1684131015828, "content": {"title": "AdaAnn: Adaptive Annealing Scheduler for Probability Density Approximation", "abstract": "Approximating probability distributions can be a challenging task, particularly when they are supported over regions of high geometrical complexity or exhibit multiple modes. Annealing can be used to facilitate this task which is often combined with constant a priori selected increments in inverse temperature. However, using constant increments limit the computational efficiency due to the inability to adapt to situations where smooth changes in the annealed density could be handled equally well with larger increments. We introduce AdaAnn, an adaptive annealing scheduler that automatically adjusts the temperature increments based on the expected change in the Kullback-Leibler divergence between two distributions with a sufficiently close annealing temperature. AdaAnn is easy to implement and can be integrated into existing sampling approaches such as normalizing flows for variational inference and Markov chain Monte Carlo. We demonstrate the computational efficiency of the AdaAnn scheduler for variational inference with normalizing flows on a number of examples, including density approximation and parameter estimation for dynamical systems."}}
{"id": "yij8GYea7rR", "cdate": 1609459200000, "mdate": 1684131015845, "content": {"title": "Construction of Differentially Private Empirical Distributions from a Low-Order Marginals Set Through Solving Linear Equations with l2 Regularization", "abstract": "We introduce a new algorithm, Construction of dIfferentially Private Empirical Distributions from a low-order marginals set tHrough solving linear Equations with $$l_2$$ Regularization (CIPHER), that produces differentially private empirical joint distributions from a set of low-order marginals. CIPHER is conceptually simple and requires no more than decomposing joint probabilities via basic probability rules to construct a linear equation set and subsequently solving the equations. Compared to the full-dimensional histogram (FDH) sanitization, CIPHER has drastically lower requirements on computational storage and memory, which is practically attractive especially considering that the high-order signals preserved by the FDH sanitization are likely just sample randomness and rarely of interest. Our experiments demonstrate that CIPHER outperforms the multiplicative weighting exponential mechanism in preserving original information and has similar or superior cost-normalized utility to FDH sanitization at the same privacy budget."}}
{"id": "NiwlanTWPF", "cdate": 1609459200000, "mdate": 1684131015823, "content": {"title": "Variational Inference with NoFAS: Normalizing Flow with Adaptive Surrogate for Computationally Expensive Models", "abstract": "Fast inference of numerical model parameters from data is an important prerequisite to generate predictive models for a wide range of applications. Use of sampling-based approaches such as Markov chain Monte Carlo may become intractable when each likelihood evaluation is computationally expensive. New approaches combining variational inference with normalizing flow are characterized by a computational cost that grows only linearly with the dimensionality of the latent variable space, and rely on gradient-based optimization instead of sampling, providing a more efficient approach for Bayesian inference about the model parameters. Moreover, the cost of frequently evaluating an expensive likelihood can be mitigated by replacing the true model with an offline trained surrogate model, such as neural networks. However, this approach might generate significant bias when the surrogate is insufficiently accurate around the posterior modes. To reduce the computational cost without sacrificing inferential accuracy, we propose Normalizing Flow with Adaptive Surrogate (NoFAS), an optimization strategy that alternatively updates the normalizing flow parameters and surrogate model parameters. We also propose an efficient sample weighting scheme for surrogate model training that preserves global accuracy while effectively capturing high posterior density regions. We demonstrate the inferential and computational superiority of NoFAS against various benchmarks, including cases where the underlying model lacks identifiability. The source code and numerical experiments used for this study are available at https://github.com/cedricwangyu/NoFAS."}}
{"id": "3QAICuzcI4N", "cdate": 1609459200000, "mdate": 1684131015842, "content": {"title": "Continuous-Time Markov-Switching GARCH Process with Robust State Path Identification and Volatility Estimation", "abstract": "We propose a continuous-time Markov-switching generalized autoregressive conditional heteroskedasticity (COMS-GARCH) process for handling irregularly spaced time series with multiple volatility states. We employ a Gibbs sampler in the Bayesian framework to estimate the COMS-GARCH model parameters, the latent state path and volatilities. To improve the computational efficiency and robustness of the identified state path and estimated volatilities, we propose a multi-path sampling scheme and incorporate the Bernoulli noise injection in the computational procedure. We provide theoretical justifications for the improved stability and robustness with the Bernoulli noise injection through the concept of ensemble learning and the low sensitivity of the objective function to external perturbation in the time series. The experiment results demonstrate that our proposed COMS-GARCH process and computational procedure are able to predict volatility regimes and volatilities in a time series with satisfactory accuracy."}}
{"id": "VHvt8uTo1v", "cdate": 1577836800000, "mdate": 1684131015829, "content": {"title": "Adaptive Gaussian Noise Injection Regularization for Neural Networks", "abstract": "We propose whiteout, a family of Noise injection (NI) regularization techniques through injecting adaptive Gaussian noises. Through theoretical analysis, we 1) establish the regularization effect of whiteout in the framework of generalized linear models with a broad range of closed-form penalty terms, including $$l_{\\gamma }$$ (for $$\\gamma \\in (0,2]$$ ), the adaptive lasso, the group lasso, among others; 2) show that whiteout stabilizes the training of NNs with robustness or decreased sensitivity to small perturbations in the input; 3) prove that the noise-perturbed loss function with whiteout converges almost surely to the ideal loss function, and the minimizer of the former is consistent for the minimizer of the latter; 4) derive the tail bound on the noise-perturbed loss function to establish the practical feasibility for optimization. The superiority of whiteout over the Bernoulli NI techniques (dropout and shakeout) in prediction accuracy (up by 2 $$\\sim $$ 3%) in relatively small-sized training data and comparability in large-sized training data are demonstrated thorough experiments. This work is the first in-depth theoretical and methodological examination of the regularization effects of Gaussian NI in NNs in general."}}
{"id": "ITTUUL_-u44", "cdate": 1577836800000, "mdate": 1684131015829, "content": {"title": "Differentially Private Generation of Social Networks via Exponential Random Graph Models", "abstract": "Many social networks contain sensitive relational information. One approach to protect the sensitive relational information while offering flexibility for social network research and analysis is to release synthetic social networks at a pre-specified privacy risk level, given the original observed network. We propose the DP-ERGM procedure that synthesizes networks that satisfy the differential privacy (DP) via the exponential random graph model (EGRM). We apply DP-ERGM to a college student friendship network and compare its original network information preservation in the generated private networks with two other approaches: differentially private DyadWise Randomized Response (DWRR) and Sanitization of the Conditional probability of Edge given Attribute classes (SCEA). The results suggest that DP-EGRM preserves the original information significantly better than DWRR and SCEA in both network statistics and inferences from ERGMs and latent space models. In addition, DP-ERGM satisfies the node DP, a stronger notion of privacy than the edge DP that DWRR and SCEA satisfy."}}
