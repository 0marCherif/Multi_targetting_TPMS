{"id": "b5akM_1H-f", "cdate": 1672531200000, "mdate": 1683879598001, "content": {"title": "Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning", "abstract": "Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four datasets demonstrate that the proposed model achieves significant improvements in complex reasoning tasks involving diverse structures, and shows transferability to downstream tasks with limited training data and effectiveness for complex reasoning of KGs modality."}}
{"id": "uRx85J41bD9", "cdate": 1640995200000, "mdate": 1682414005861, "content": {"title": "MVPTR: Multi-Level Semantic Alignment for Vision-Language Pre-Training via Multi-Stage Learning", "abstract": "Previous vision-language pre-training models mainly construct multi-modal inputs with tokens and objects (pixels) followed by performing cross-modality interaction between them. We argue that the input of only tokens and object features limits high-level semantic alignment like phrase-to-region grounding. Meanwhile, multi-level alignments are inherently consistent and able to facilitate the representation learning synergistically. Therefore, in this paper, we propose to learn Multi-level semantic alignment for Vision-language Pre-TRaining (MVPTR). In MVPTR, we follow the nested structure of both modalities to introduce concepts as high-level semantics. To ease the learning from multi-modal multi-level inputs, our framework is split into two stages, the first stage focuses on intra-modality multi-level representation learning, the second enforces interactions across modalities via both coarse-grained and fine-grained semantic alignment tasks. In addition to the commonly used image-text matching and masked language model tasks, we introduce a masked concept recovering task in the first stage to enhance the concept representation learning, and two more tasks in the second stage to explicitly encourage multi-level alignments across modalities. Our model achieves state-of-the-art results on several vision and language tasks."}}
{"id": "sVPsowa5MIm", "cdate": 1640995200000, "mdate": 1684165304043, "content": {"title": "Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text", "abstract": ""}}
{"id": "mKNV4NTx57", "cdate": 1640995200000, "mdate": 1682414005878, "content": {"title": "A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training", "abstract": "Multi-modal pre-training and knowledge discovery are two important research topics in multi-modal machine learning. Nevertheless, none of existing works make attempts to link knowledge discovery with knowledge guided multi-modal pre-training. In this paper, we propose to unify them into a continuous learning framework for mutual improvement. Taking the open-domain uni-modal datasets of images and texts as input, we maintain a knowledge graph as the foundation to support these two tasks. For knowledge discovery, a pre-trained model is used to identify cross-modal links on the graph. For model pre-training, the knowledge graph is used as the external knowledge to guide the model updating. These two steps are iteratively performed in our framework for continuous learning. The experimental results on MS-COCO and Flickr30K with respect to both knowledge discovery and the pre-trained model validate the effectiveness of our framework."}}
{"id": "cun-fr74elR", "cdate": 1640995200000, "mdate": 1684165304108, "content": {"title": "Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering", "abstract": ""}}
{"id": "UxlB0lSr6gv", "cdate": 1640995200000, "mdate": 1684165304086, "content": {"title": "Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations", "abstract": "Wei Chen, Yeyun Gong, Can Xu, Huang Hu, Bolun Yao, Zhongyu Wei, Zhihao Fan, Xiaowu Hu, Bartuer Zhou, Biao Cheng, Daxin Jiang, Nan Duan. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "Kyfohki0GF", "cdate": 1640995200000, "mdate": 1684147706875, "content": {"title": "GENIE: Large Scale Pre-training for Text Generation with Diffusion Model", "abstract": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at https://github.com/microsoft/ProphetNet/tree/master/GENIE."}}
{"id": "HVUgmxBSyLu", "cdate": 1640995200000, "mdate": 1682414005867, "content": {"title": "Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval", "abstract": ""}}
{"id": "7vjd6penkG", "cdate": 1640995200000, "mdate": 1682414005862, "content": {"title": "Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval", "abstract": ""}}
{"id": "XguE9OYq78m", "cdate": 1609459200000, "mdate": 1636701888350, "content": {"title": "TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning", "abstract": "Existing research for image captioning usually represents an image using a scene graph with low-level facts (objects and relations) and fails to capture the high-level semantics. In this paper, we propose a Theme Concepts extended Image Captioning (TCIC) framework that incorporates theme concepts to represent high-level cross-modality semantics. In practice, we model theme concepts as memory vectors and propose Transformer with Theme Nodes (TTN) to incorporate those vectors for image captioning. Considering that theme concepts can be learned from both images and captions, we propose two settings for their representations learning based on TTN. On the vision side, TTN is configured to take both scene graph based features and theme concepts as input for visual representation learning. On the language side, TTN is configured to take both captions and theme concepts as input for text representation re-construction. Both settings aim to generate target captions with the same transformer-based decoder. During the training, we further align representations of theme concepts learned from images and corresponding captions to enforce the cross-modality learning. Experimental results on MS COCO show the effectiveness of our approach compared to some state-of-the-art models."}}
