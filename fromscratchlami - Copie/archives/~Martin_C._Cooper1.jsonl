{"id": "VFbKusUUgp", "cdate": 1677628800000, "mdate": 1681651801616, "content": {"title": "Tractability of explaining classifier decisions", "abstract": ""}}
{"id": "fpfd_k0zwh", "cdate": 1640995200000, "mdate": 1681679286899, "content": {"title": "Complexity of Minimum-Size Arc-Inconsistency Explanations", "abstract": "Explaining the outcome of programs has become one of the main concerns in AI research. In constraint programming, a user may want the system to explain why a given variable assignment is not feasible or how it came to the conclusion that the problem does not have any solution. One solution to the latter is to return to the user a sequence of simple reasoning steps that lead to inconsistency. Arc consistency is a well-known form of reasoning that can be understood by a human. We consider explanations as sequences of propagation steps of a constraint on a variable (i.e. the ubiquitous revise function in arc consistency algorithms) that lead to inconsistency. We characterize, on binary CSPs, cases for which providing a shortest such explanation is easy: when domains are Boolean or when variables have maximum degree two. However, these polynomial cases are tight. Providing a shortest explanation is NP-hard if the maximum degree is three, even if the number of variables is bounded, or if domain size is bounded by three. It remains NP-hard on trees, despite the fact that arc consistency is a decision procedure on trees. Finally, the problem is not FPT-approximable unless the Gap-ETH is false."}}
{"id": "fEMqEiclsmh", "cdate": 1640995200000, "mdate": 1681651801647, "content": {"title": "Provably Precise, Succinct and Efficient Explanations for Decision Trees", "abstract": ""}}
{"id": "_2h5NEhbhbk", "cdate": 1640995200000, "mdate": 1681651801791, "content": {"title": "Feature Necessity & Relevancy in ML Classifier Explanations", "abstract": ""}}
{"id": "RH8vN0S42z", "cdate": 1640995200000, "mdate": 1681679286759, "content": {"title": "Isomorphisms Between STRIPS Problems and Sub-Problems", "abstract": "Determining whether two STRIPS planning instances are isomorphic is the simplest form of comparison between planning instances. It is also a particular case of the problem concerned with finding an isomorphism between a planning instance P and a sub-instance of another instance P'. One application of such an isomorphism is to efficiently produce a compiled form containing all solutions to P from a compiled form containing all solutions to P'. In this paper, we study the complexity of both problems. We show that the former is GI-complete, and can thus be solved, in theory, in quasi-polynomial time. While we prove the latter to be NP-complete, we propose an algorithm to build an isomorphism, when possible. We report extensive experimental trials on benchmark problems which demonstrate conclusively that applying constraint propagation in preprocessing can greatly improve the efficiency of a SAT solver."}}
{"id": "L1Hh7OpWC5D", "cdate": 1640995200000, "mdate": 1681651801706, "content": {"title": "Tractable Explanations for d-DNNF Classifiers", "abstract": ""}}
{"id": "BZUIvBjJtG", "cdate": 1640995200000, "mdate": 1681651801625, "content": {"title": "On Computing Probabilistic Abductive Explanations", "abstract": ""}}
{"id": "oEt6a5V_Vf", "cdate": 1609459200000, "mdate": 1648708255224, "content": {"title": "Efficient Explanations for Knowledge Compilation Languages", "abstract": "Knowledge compilation (KC) languages find a growing number of practical uses, including in Constraint Programming (CP) and in Machine Learning (ML). In most applications, one natural question is how to explain the decisions made by models represented by a KC language. This paper shows that for many of the best known KC languages, well-known classes of explanations can be computed in polynomial time. These classes include deterministic decomposable negation normal form (d-DNNF), and so any KC language that is strictly less succinct than d-DNNF. Furthermore, the paper also investigates the conditions under which polynomial time computation of explanations can be extended to KC languages more succinct than d-DNNF."}}
{"id": "jwmMkCroZdW", "cdate": 1609459200000, "mdate": 1648708255226, "content": {"title": "A lightweight epistemic logic and its application to planning", "abstract": "We study multiagent epistemic planning with a simple epistemic logic whose language is a restriction of that of standard epistemic logic. Its formulas are boolean combinations of observability atoms: sequences of \u2018knowing whether\u2019 operators followed by propositional variables. This compares favourably with other restricted languages where formulas are boolean combinations of epistemic literals: sequences of \u2018knowing that\u2019 epistemic operators and negations followed by propositional variables; or in other terms: epistemic formulas without conjunctions or disjunctions. The reason is that our language enables a richer theory of mind: we can express statements such as \u201cI don't know whether p, but I know that you know whether p\u201d which are important in communication and more generally in interaction and which cannot be expressed with epistemic literals. Going beyond previous work, we also introduce a \u2018common knowledge whether\u2019 operator. We show that satisfiability is nevertheless NP-complete. We then define simple epistemic planning tasks as generalisations of classical planning tasks: action descriptions have sets of observability atoms as add- and delete-lists, initial states are sets of observability atoms, and goals are boolean combinations of observability atoms. We show that simple epistemic planning tasks can be polynomially translated into classical planning tasks. It follows that checking solvability of simple epistemic planning tasks is PSpace-complete. We present some application examples such as the gossip problem and some experimental results and clarify the relationship with Dynamic Epistemic Logic-based planning."}}
{"id": "j1rZqZhcX9m", "cdate": 1609459200000, "mdate": 1648708255229, "content": {"title": "On the Tractability of Explaining Decisions of Classifiers", "abstract": "Explaining decisions is at the heart of explainable AI. We investigate the computational complexity of providing a formally-correct and minimal explanation of a decision taken by a classifier. In the case of threshold (i.e. score-based) classifiers, we show that a complexity dichotomy follows from the complexity dichotomy for languages of cost functions. In particular, submodular classifiers allow tractable explanation of positive decisions, but not negative decisions (assuming P\u2260NP). This is an example of the possible asymmetry between the complexity of explaining positive and negative decisions of a particular classifier. Nevertheless, there are large families of classifiers for which explaining both positive and negative decisions is tractable, such as monotone or linear classifiers. We extend tractable cases to constrained classifiers (when there are constraints on the possible input vectors) and to the search for contrastive rather than abductive explanations. Indeed, we show that tractable classes coincide for abductive and contrastive explanations in the constrained or unconstrained settings."}}
