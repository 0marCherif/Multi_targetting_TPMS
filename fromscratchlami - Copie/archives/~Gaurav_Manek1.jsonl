{"id": "vK53GLZJes8", "cdate": 1652737346771, "mdate": null, "content": {"title": "The Pitfalls of Regularization in Off-Policy TD Learning", "abstract": "Temporal Difference (TD) learning is ubiquitous in reinforcement learning, where it is often combined with off-policy sampling and function approximation.  Unfortunately learning with this combination (known as the deadly triad), exhibits instability and unbounded error.  To account for this, modern Reinforcement Learning methods often implicitly (or sometimes explicitly) assume that regularization is sufficient to mitigate the problem in practice; indeed, the standard deadly triad examples from the literature can be ``fixed'' via proper regularization. In this paper, we introduce a series of new counterexamples to show that the instability and unbounded error of TD methods is not solved by regularization. We demonstrate that, in the off-policy setting with linear function approximation, TD methods can fail to learn a non-trivial value function under any amount of regularization; we further show that regularization can induce divergence under common conditions; and we show that one of the most promising methods to mitigate this divergence (Emphatic TD algorithms) may also diverge under regularization. We further demonstrate such divergence when using neural networks as function approximators.  Thus, we argue that the role of regularization in TD methods needs to be reconsidered, given that it is insufficient to prevent divergence and may itself introduce instability. There needs to be much more care in the practical and theoretical application of regularization to Reinforcement Learning methods.\n"}}
{"id": "9mls_1dBQS", "cdate": 1632875495721, "mdate": null, "content": {"title": "Model-based Reinforcement Learning with Ensembled Model-value Expansion", "abstract": "Model-based reinforcement learning (MBRL) methods are often more data-efficient and quicker to converge than their model-free counterparts, but typically rely crucially on accurate modeling of the environment dynamics and associated uncertainty in order to perform well. Recent approaches have used ensembles of dynamics models within MBRL to separately capture aleatoric and epistemic uncertainty of the learned dynamics, but many MBRL algorithms are still limited because they treat these dynamics models as a \"black box\" without fully exploiting the uncertainty modeling.\nIn this paper, we propose a simple but effective approach to improving the performance of MBRL by directly incorporating the ensemble prediction \\emph{into} the RL method itself: we propose constructing multiple value roll-outs using different members of the dynamics ensemble, and aggregating the separate estimates to form a joint estimate of the state value.  Despite its simplicity, we show that this method substantially improves the performance of MBRL methods: we comprehensively evaluate this technique on common locomotion benchmarks, with ablative experiments to show the added value of our proposed components."}}
{"id": "SkgngBHlUS", "cdate": 1567802612210, "mdate": null, "content": {"title": "Learning Stable Deep Dynamics Models", "abstract": "Deep networks are commonly used to model dynamical systems, predicting how the state of a system will evolve over time (either autonomously or in response to control inputs). Despite the predictive power of these systems, it has been difficult to make formal claims about the basic properties of the learned systems. In this paper, we propose an approach for learning dynamical systems that are guaranteed to be stable over the entire state space. The approach works by jointly learning a dynamics model and Lyapunov function that guarantees non-expansiveness of the dynamics under the learned Lyapunov function. We show that such learning systems are able to model simple dynamical systems and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion."}}
{"id": "BkXADmJDM", "cdate": 1518446539287, "mdate": null, "content": {"title": "Efficient GAN-Based Anomaly Detection", "abstract": "Generative adversarial networks (GANs) are able to model the complex high-dimensional\ndistributions of real-world data, which suggests they could be effective\nfor anomaly detection. However, few works have explored the use of GANs\nfor the anomaly detection task. We leverage recently developed GAN models for\nanomaly detection, and achieve state-of-the-art performance on image and network\nintrusion datasets, while being several hundred-fold faster at test time than\nthe only published GAN-based method."}}
{"id": "fv7K_XXOptS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Truly Multi-modal YouTube-8M Video Classification with Video, Audio, and Text", "abstract": "The YouTube-8M video classification challenge requires teams to classify 0.7 million videos into one or more of 4,716 classes. In this Kaggle competition, we placed in the top 3% out of 650 participants using released video and audio features. Beyond that, we extend the original competition by including text information in the classification, making this a truly multi-modal approach with vision, audio and text. The newly introduced text data is termed as YouTube-8M-Text. We present a classification framework for the joint use of text, visual and audio features, and conduct an extensive set of experiments to quantify the benefit that this additional mode brings. The inclusion of text yields state-of-the-art results, e.g. 86.7% GAP on the YouTube-8M-Text validation dataset."}}
{"id": "Xg1ss2LUm7w", "cdate": 1483228800000, "mdate": null, "content": {"title": "Pruning Convolutional Neural Networks for Image Instance Retrieval", "abstract": "In this work, we focus on the problem of image instance retrieval with deep descriptors extracted from pruned Convolutional Neural Networks (CNN). The objective is to heavily prune convolutional edges while maintaining retrieval performance. To this end, we introduce both data-independent and data-dependent heuristics to prune convolutional edges, and evaluate their performance across various compression rates with different deep descriptors over several benchmark datasets. Further, we present an end-to-end framework to fine-tune the pruned network, with a triplet loss function specially designed for the retrieval task. We show that the combination of heuristic pruning and fine-tuning offers 5x compression rate without considerable loss in retrieval performance."}}
{"id": "PsrfAi7NyRf", "cdate": 1483228800000, "mdate": null, "content": {"title": "Deep Learning for Lung Cancer Detection: Tackling the Kaggle Data Science Bowl 2017 Challenge", "abstract": "We present a deep learning framework for computer-aided lung cancer diagnosis. Our multi-stage framework detects nodules in 3D lung CAT scans, determines if each nodule is malignant, and finally assigns a cancer probability based on these results. We discuss the challenges and advantages of our framework. In the Kaggle Data Science Bowl 2017, our framework ranked 41st out of 1972 teams."}}
{"id": "HWFQ34s4mFk", "cdate": 1483228800000, "mdate": null, "content": {"title": "Region average pooling for context-aware object detection", "abstract": "Object detection has been a key task in computer vision with deep convolutional neural networks being a significant performer. We propose a method named Region Average Pooling that leverages object co-occurrence to improve object detection performance. Given regions of interest in an image, our method augments object detection networks with pooled contextual features from other regions of interest in the scene. We implement our scheme and evaluate it on the Pascal Visual Object Classes (VOC) 2007 and Microsoft Common Objects in Context (MS COCO) datasets. When used as part of the Faster R-CNN object detection framework with VGG-16, we show an increase in mAP from 24.2% to 25.5% over baseline Faster R-CNN and Global Average Pooling when testing on MS COCO."}}
