{"id": "ifSAG6LWGL5", "cdate": 1640995200000, "mdate": 1682573570003, "content": {"title": "Parameter Inference of Time Series by Delay Embeddings and Learning Differentiable Operators", "abstract": "We provide a method to identify system parameters of dynamical systems, called ID-ODE -- Inference by Differentiation and Observing Delay Embeddings. In this setting, we are given a dataset of trajectories from a dynamical system with system parameter labels. Our goal is to identify system parameters of new trajectories. The given trajectories may or may not encompass the full state of the system, and we may only observe a one-dimensional time series. In the latter case, we reconstruct the full state by using delay embeddings, and under sufficient conditions, Taken's Embedding Theorem assures us the reconstruction is diffeomorphic to the original. This allows our method to work on time series. Our method works by first learning the velocity operator (as given or reconstructed) with a neural network having both state and system parameters as variable inputs. Then on new trajectories we backpropagate prediction errors to the system parameter inputs giving us a gradient. We then use gradient descent to infer the correct system parameter. We demonstrate the efficacy of our approach on many numerical examples: the Lorenz system, Lorenz96, Lotka-Volterra Predator-Prey, and the Compound Double Pendulum. We also apply our algorithm on a real-world dataset: propulsion of the Hall-effect Thruster (HET)."}}
{"id": "SWw78nIWNKV", "cdate": 1640995200000, "mdate": 1682573570001, "content": {"title": "Multi-Agent Shape Control with Optimal Transport", "abstract": "We introduce a method called MASCOT (Multi-Agent Shape Control with Optimal Transport) to compute optimal control solutions of agents with shape/formation/density constraints. For example, we might want to apply shape constraints on the agents -- perhaps we desire the agents to hold a particular shape along the path, or we want agents to spread out in order to minimize collisions. We might also want a proportion of agents to move to one destination, while the other agents move to another, and to do this in the optimal way, i.e. the source-destination assignments should be optimal. In order to achieve this, we utilize the Earth Mover's Distance from Optimal Transport to distribute the agents into their proper positions so that certain shapes can be satisfied. This cost is both introduced in the terminal cost and in the running cost of the optimal control problem."}}
{"id": "GaH4nKlFNb", "cdate": 1640995200000, "mdate": 1682573570004, "content": {"title": "Wasserstein-Based Projections with Applications to Inverse Problems", "abstract": "Inverse problems consist of recovering a signal from a collection of noisy measurements. These are typically cast as optimization problems, with classic approaches using a data fidelity term and an analytic regularizer that stabilizes recovery. Recent plug-and-play (PnP) works propose replacing the operator for analytic regularization in optimization methods by a data-driven denoiser. These schemes obtain state-of-the-art results, but at the cost of limited theoretical guarantees. To bridge this gap, we present a new algorithm that takes samples from the manifold of true data as input and outputs an approximation of the projection operator onto this manifold. Under standard assumptions, we prove this algorithm generates a learned operator, called Wasserstein-based projection (WP), that approximates the true projection with high probability. Thus, WPs can be inserted into optimization methods in the same manner as PnP, but now with theoretical guarantees. Provided numerical examples show WPs obtain state-of-the-art results for unsupervised PnP signal recovery. All codes for this work can be found at https://github.com/swufung/WassersteinBasedProjections."}}
{"id": "rZXtNcdIx5", "cdate": 1609459200000, "mdate": 1645804080814, "content": {"title": "Wasserstein Proximal of GANs", "abstract": "We introduce a new method for training generative adversarial networks by applying the Wasserstein-2 metric proximal on the generators. The approach is based on Wasserstein information geometry. It defines a parametrization invariant natural gradient by pulling back optimal transport structures from probability space to parameter space. We obtain easy-to-implement iterative regularizers for the parameter updates of implicit deep generative models. Our experiments demonstrate that this method improves the speed and stability of training in terms of wall-clock time and Fr\u00e9chet Inception Distance."}}
{"id": "rYLHK4qO8xq", "cdate": 1609459200000, "mdate": 1645804081068, "content": {"title": "Wasserstein Proximal of GANs", "abstract": "We introduce a new method for training generative adversarial networks by applying the Wasserstein-2 metric proximal on the generators. The approach is based on Wasserstein information geometry. It defines a parametrization invariant natural gradient by pulling back optimal transport structures from probability space to parameter space. We obtain easy-to-implement iterative regularizers for the parameter updates of implicit deep generative models. Our experiments demonstrate that this method improves the speed and stability of training in terms of wall-clock time and Fr\\'echet Inception Distance."}}
{"id": "VuHCikM6d9U", "cdate": 1609459200000, "mdate": 1682573570073, "content": {"title": "Decentralized Multi-Agents by Imitation of a Centralized Controller", "abstract": "We consider a multi-agent reinforcement learning problem where each agent seeks to maximize a shared reward while interacting with other agents, and they may or may not be able to communicate. Typi..."}}
{"id": "QlzwlfOTTD3", "cdate": 1609459200000, "mdate": 1682573570038, "content": {"title": "Belief and Opinion Evolution in Social Networks: A High-Dimensional Mean Field Game Approach", "abstract": "Belief and opinion evolution in social networks (SNs) can aid in understanding how people influence others\u2019 decisions through social relationships as well as provide a solid foundation for many valuable social applications. As large numbers of users are involved in SNs, the complexity of traditional optimization techniques is high as they deal with the interactions between users separately. Moreover, the state variable (opinion) is high-dimensional because a person usually has opinions about many different social issues. To overcome those challenges, we formulate the opinion evolution in SNs as a high-dimensional stochastic mean field game (MFG). Numerical methods for high-dimensional MFGs are practically non-existent because of the need for grid-based spatial discretization. Thus, we propose a machine-learning based method, where we use an alternating population and agent control neural network (APAC-net), to tractably solve high-dimensional stochastic MFGs. Through APAC-net, solving MFGs can be regarded as a special case of training a generative adversarial network (GAN). To the best of our knowledge, the APAC-Net is the first model that can solve high-dimensional stochastic MFGs. The simulation results affirm the efficiency of the APAC-net."}}
{"id": "pzOD3lKv9Yw", "cdate": 1577836800000, "mdate": 1682573570073, "content": {"title": "APAC-Net: Alternating the Population and Agent Control via Two Neural Networks to Solve High-Dimensional Stochastic Mean Field Games", "abstract": "We present APAC-Net, an alternating population and agent control neural network for solving stochastic mean field games (MFGs). Our algorithm is geared toward high-dimensional instances of MFGs that are beyond reach with existing solution methods. We achieve this in two steps. First, we take advantage of the underlying variational primal-dual structure that MFGs exhibit and phrase it as a convex-concave saddle point problem. Second, we parameterize the value and density functions by two neural networks, respectively. By phrasing the problem in this manner, solving the MFG can be interpreted as a special case of training a generative adversarial network (GAN). We show the potential of our method on up to 100-dimensional MFG problems."}}
{"id": "iw5ehGF5QzX", "cdate": 1577836800000, "mdate": null, "content": {"title": "Projecting to Manifolds via Unsupervised Learning", "abstract": "Inverse problems consist of recovering a signal from a collection of noisy measurements. These are typically cast as optimization problems, with classic approaches using a data fidelity term and an analytic regularizer that stabilizes recovery. Recent Plug-and-Play (PnP) works propose replacing the operator for analytic regularization in optimization methods by a data-driven denoiser. These schemes obtain state of the art results, but at the cost of limited theoretical guarantees. To bridge this gap, we present a new algorithm that takes samples from the manifold of true data as input and outputs an approximation of the projection operator onto this manifold. Under standard assumptions, we prove this algorithm generates a learned operator, called Wasserstein-based projection (WP), that approximates the true projection with high probability. Thus, WPs can be inserted into optimization methods in the same manner as PnP, but now with theoretical guarantees. Provided numerical examples show WPs obtain state of the art results for unsupervised PnP signal recovery."}}
{"id": "ulkw5a8HCLL", "cdate": 1546300800000, "mdate": null, "content": {"title": "Wasserstein Diffusion Tikhonov Regularization", "abstract": "We propose regularization strategies for learning discriminative models that are robust to in-class variations of the input data. We use the Wasserstein-2 geometry to capture semantically meaningful neighborhoods in the space of images, and define a corresponding input-dependent additive noise data augmentation model. Expanding and integrating the augmented loss yields an effective Tikhonov-type Wasserstein diffusion smoothness regularizer. This approach allows us to apply high levels of regularization and train functions that have low variability within classes but remain flexible across classes. We provide efficient methods for computing the regularizer at a negligible cost in comparison to training with adversarial data augmentation. Initial experiments demonstrate improvements in generalization performance under adversarial perturbations and also large in-class variations of the input data."}}
