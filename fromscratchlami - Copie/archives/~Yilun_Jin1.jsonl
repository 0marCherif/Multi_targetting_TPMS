{"id": "x8wrs5eqSg", "cdate": 1672531200000, "mdate": 1706254054901, "content": {"title": "Transferable Graph Structure Learning for Graph-based Traffic Forecasting Across Cities", "abstract": "Graph-based deep learning models are powerful in modeling spatio-temporal graphs for traffic forecasting. In practice, accurate forecasting models rely on sufficient traffic data, which may not be accessible in real-world applications. To address this problem, transfer learning methods are designed to transfer knowledge from the source graph with abundant data to the target graph with limited data. However, existing methods adopt pre-defined graph structures for knowledge extraction and transfer, which may be noisy or biased and negatively impact the performance of knowledge transfer. To address the problem, we propose TransGTR, a transferable structure learning framework for traffic forecasting that jointly learns and transfers the graph structures and forecasting models across cities. TransGTR consists of a node feature network, a structure generator, and a forecasting model. We train the node feature network with knowledge distillation to extract city-agnostic node features, such that the structure generator, taking the node features as inputs, can be transferred across both cities. Furthermore, we train the structure generator via a temporal decoupled regularization, such that the spatial features learned with the generated graphs share similar distributions across cities and thus facilitate knowledge transfer for the forecasting model. We evaluate TransGTR on real-world traffic speed datasets, where under a fair comparison, TransGTR outperforms state-of-the-art baselines by up to 5.4%."}}
{"id": "tzL4TZCoQQ", "cdate": 1672531200000, "mdate": 1706254054898, "content": {"title": "Scalable and Efficient Full-Graph GNN Training for Large Graphs", "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools to capture structural information from graph-structured data, achieving state-of-the-art performance on applications such as recommendation, knowledge graph, and search. Graphs in these domains typically contain hundreds of millions of nodes and billions of edges. However, previous GNN systems demonstrate poor scalability because large and interleaved computation dependencies in GNN training cause significant overhead in current parallelization methods. We present G3, a distributed system that can efficiently train GNNs over billion-edge graphs at scale. G3 introduces GNN hybrid parallelism which synthesizes three dimensions of parallelism to scale out GNN training by sharing intermediate results peer-to-peer in fine granularity, eliminating layer-wise barriers for global collective communication or neighbor replications as seen in prior works. G3 leverages locality-aware iterative partitioning and multi-level pipeline scheduling to exploit acceleration opportunities by distributing balanced workload among workers and overlapping computation with communication in both inter-layer and intra-layer training processes. We show via a prototype implementation and comprehensive experiments that G3 can achieve as much as 2.24x speedup in a 16-node cluster, and better final accuracy over prior works."}}
{"id": "q-1Icxg1am8", "cdate": 1672531200000, "mdate": 1706254054902, "content": {"title": "A Survey on Vertical Federated Learning: From a Layered Perspective", "abstract": "Vertical federated learning (VFL) is a promising category of federated learning for the scenario where data is vertically partitioned and distributed among parties. VFL enriches the description of samples using features from different parties to improve model capacity. Compared with horizontal federated learning, in most cases, VFL is applied in the commercial cooperation scenario of companies. Therefore, VFL contains tremendous business values. In the past few years, VFL has attracted more and more attention in both academia and industry. In this paper, we systematically investigate the current work of VFL from a layered perspective. From the hardware layer to the vertical federated system layer, researchers contribute to various aspects of VFL. Moreover, the application of VFL has covered a wide range of areas, e.g., finance, healthcare, etc. At each layer, we categorize the existing work and explore the challenges for the convenience of further research and development of VFL. Especially, we design a novel MOSP tree taxonomy to analyze the core component of VFL, i.e., secure vertical federated machine learning algorithm. Our taxonomy considers four dimensions, i.e., machine learning model (M), protection object (O), security model (S), and privacy-preserving protocol (P), and provides a comprehensive investigation."}}
{"id": "jIpHGWVJo3s", "cdate": 1672531200000, "mdate": 1706254054901, "content": {"title": "Federated Learning without Full Labels: A Survey", "abstract": "Data privacy has become an increasingly important concern in real-world big data applications such as machine learning. To address the problem, federated learning (FL) has been a promising solution to building effective machine learning models from decentralized and private data. Existing federated learning algorithms mainly tackle the supervised learning problem, where data are assumed to be fully labeled. However, in practice, fully labeled data is often hard to obtain, as the participants may not have sufficient domain expertise, or they lack the motivation and tools to label data. Therefore, the problem of federated learning without full labels is important in real-world FL applications. In this paper, we discuss how the problem can be solved with machine learning techniques that leverage unlabeled data. We present a survey of methods that combine FL with semi-supervised learning, self-supervised learning, and transfer learning methods. We also summarize the datasets used to evaluate FL methods without full labels. Finally, we highlight future directions in the context of FL without full labels."}}
{"id": "_Dq0owZ1Dor", "cdate": 1672531200000, "mdate": 1706254054901, "content": {"title": "VERTICES: Efficient Two-Party Vertical Federated Linear Model with TTP-aided Secret Sharing", "abstract": "Vertical Federated Learning (VFL) has emerged as one of the most predominant approaches for secure collaborative machine learning where the training data is partitioned by features among multiple parties. Most VFL algorithms primarily rely on two fundamental privacy-preserving techniques: Homomorphic Encryption (HE) and secure Multi-Party Computation (MPC). Though generally considered with stronger privacy guarantees, existing general-purpose MPC frameworks suffer from expensive computation and communication overhead and are inefficient especially under VFL settings. This study centers around MPC-based VFL algorithms and presents a novel approach for two-party vertical federated linear models via an efficient secret sharing (SS) scheme with a trusted coordinator. Our approach can achieve significant acceleration of the training procedure in vertical federated linear models of between 2.5x and 6.6x than other existing MPC frameworks under the same security setting."}}
{"id": "KuYPaCGy68", "cdate": 1672531200000, "mdate": 1706254054890, "content": {"title": "Federated Learning without Full Labels: A Survey", "abstract": ""}}
{"id": "jBLaxZczGi", "cdate": 1640995200000, "mdate": 1671948484058, "content": {"title": "Selective Cross-City Transfer Learning for Traffic Prediction via Source City Region Re-Weighting", "abstract": "Deep learning models have been demonstrated powerful in modeling complex spatio-temporal data for traffic prediction. In practice, effective deep traffic prediction models rely on large-scale traffic data, which is not always available in real-world scenarios. To alleviate the data scarcity issue, a promising way is to use cross-city transfer learning methods to fine-tune well-trained models from source cities with abundant data. However, existing approaches overlook the divergence between source and target cities, and thus, the trained model from source cities may contain noise or even harmful source knowledge. To address the problem, we propose CrossTReS, a selective transfer learning framework for traffic prediction that adaptively re-weights source regions to assist target fine-tuning. As a general framework for fine-tuning-based cross-city transfer learning, CrossTReS consists of a feature network, a weighting network, and a prediction model. We train the feature network with node- and edge-level domain adaptation techniques to learn generalizable spatial features for both source and target cities. We further train the weighting network via source-target joint meta-learning such that source regions helpful to target fine-tuning are assigned high weights. Finally, the prediction model is selectively trained on the source city with the learned weights to initialize target fine-tuning. We evaluate CrossTReS using real-world taxi and bike data, where under the same settings, CrossTReS outperforms state-of-the-art baselines by up to 8%. Moreover, the learned region weights offer interpretable visualization."}}
{"id": "_ypfX_PzSB", "cdate": 1640995200000, "mdate": 1706254054901, "content": {"title": "Secure Forward Aggregation for Vertical Federated Neural Networks", "abstract": "Vertical federated learning (VFL) is attracting much attention because it enables cross-silo data cooperation in a privacy-preserving manner. While most research works in VFL focus on linear and tree models, deep models (e.g., neural networks) are not well studied in VFL. In this work, we focus on SplitNN, a well-known neural network framework in VFL, and identify a trade-off between data security and model performance in SplitNN. Briefly, SplitNN trains the model by exchanging gradients and transformed data. On the one hand, SplitNN suffers from the loss of model performance since multiply parties jointly train the model using transformed data instead of raw data, and a large amount of low-level feature information is discarded. On the other hand, a naive solution of increasing the model performance through aggregating at lower layers in SplitNN (i.e., the data is less transformed and more low-level feature is preserved) makes raw data vulnerable to inference attacks. To mitigate the above trade-off, we propose a new neural network protocol in VFL called Security Forward Aggregation (SFA). It changes the way of aggregating the transformed data and adopts removable masks to protect the raw data. Experiment results show that networks with SFA achieve both data security and high model performance."}}
{"id": "A7uBnnibjUX", "cdate": 1640995200000, "mdate": 1671948484015, "content": {"title": "Deep Convolutional Neural Network Based Medical Concept Normalization", "abstract": "Medical concept normalization is a critical problem in biomedical research and clinical applications. In this article, we focus on normalizing diagnostic and operation names in Chinese discharge summaries to standard concepts, which is formulated as a semantic matching problem. However, non-standard Chinese expressions, short-text normalization, heterogeneity of tasks and flexible input of disambiguation mentions pose critical challenges in our problem. We propose two models, the basic model and flexible model, to tackle these problems. The basic model solves the core problem (the first three challenges) in ambiguous mentions normalization, while the flexible model deals with flexible input of ambiguous mentions and further explores the correlation among them. Specifically, in the basic model, we present a general framework to disambiguate a diagnosis and its corresponding operation simultaneously, which introduces a tensor generator and a novel multi-view convolutional neural network (CNN) with a multi-task shared structure. We propose that the key to address non-standard expressions and the short-text problem is to incorporate a matching tensor with multiple granularities. Then a multi-view CNN is adopted to extract semantic matching patterns. Finally, the multi-task shared structure allows the model to exploit medical correlations between diagnosis and operation mentions to better perform disambiguation tasks. Subsequently, we design a flexible model based on the basic model. Specifically, we add a flexible attention layer to all procedure representation vectors, and then apply a flexible multi-task scheme to share the correlated information. Comprehensive experimental analysis indicates that our model outperforms existing baselines, demonstrating the effectiveness and robustness of our model."}}
{"id": "A3r9SIZl9I", "cdate": 1640995200000, "mdate": 1671948484016, "content": {"title": "Secure Forward Aggregation for Vertical Federated Neural Networks", "abstract": "Vertical federated learning (VFL) is attracting much attention because it enables cross-silo data cooperation in a privacy-preserving manner. While most research works in VFL focus on linear and tree models, deep models (e.g., neural networks) are not well studied in VFL. In this paper, we focus on SplitNN, a well-known neural network framework in VFL, and identify a trade-off between data security and model performance in SplitNN. Briefly, SplitNN trains the model by exchanging gradients and transformed data. On the one hand, SplitNN suffers from the loss of model performance since multiply parties jointly train the model using transformed data instead of raw data, and a large amount of low-level feature information is discarded. On the other hand, a naive solution of increasing the model performance through aggregating at lower layers in SplitNN (i.e., the data is less transformed and more low-level feature is preserved) makes raw data vulnerable to inference attacks. To mitigate the above trade-off, we propose a new neural network protocol in VFL called Security Forward Aggregation (SFA). It changes the way of aggregating the transformed data and adopts removable masks to protect the raw data. Experiment results show that networks with SFA achieve both data security and high model performance."}}
