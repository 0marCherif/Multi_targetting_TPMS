{"id": "O4G9WYOWtw9", "cdate": 1682351491720, "mdate": 1682351491720, "content": {"title": "AI Chat Assistants can Improve Conversations about Divisive Topics", "abstract": "A rapidly increasing volume of conversation occurs online, but divi-\nsiveness and conflict can fester in digital interactions. Such toxicity increases\npolarization and corrodes the capacity of diverse societies to cooperate in solv-\ning social problems. Scholars and civil society groups promote interventions\nthat make conversations less divisive or more productive, but scaling these ef-\nforts to online discourse is challenging. We conduct a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be im-\nproved with artificial intelligence tools. Specifically, we employ a large lan-\nguage model to make real-time, evidence-based recommendations intended to\nimprove participants\u2019 perception of feeling understood. These interventions\nimprove reported conversation quality, reduce political divisiveness, and im-\nprove the tone, without systematically changing the content of the conversation\nor moving people\u2019s policy attitudes."}}
{"id": "Tt_b9iIWTG", "cdate": 1682351308486, "mdate": 1682351308486, "content": {"title": "Out of One, Many: Using Language Models to Simulate Human Samples", "abstract": "We propose and explore the possibility that language models can be studied as effective proxies for\nspecific human sub-populations in social science research. Practical and research applications of artificial\nintelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which\nare often treated as uniform properties of the models. We show that the \u201calgorithmic bias\u201d within one\nsuch tool\u2013 the GPT-3 language model\u2013 is instead both fine-grained and demographically correlated,\nmeaning that proper conditioning will cause it to accurately emulate response distributions from a wide\nvariety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3.\nWe create \u201csilicon samples\u201d by conditioning the model on thousands of socio-demographic backstories\nfrom real human participants in multiple large surveys conducted in the United States. We then compare\nthe silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond\nsurface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes,\nand socio-cultural context that characterize human attitudes. We suggest that language models with\nsufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of\nhumans and society across a variety of disciplines."}}
{"id": "BH-SrrT2Vg", "cdate": 1682351248282, "mdate": 1682351248282, "content": {"title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels", "abstract": "Pre-trained language models derive substantial\nlinguistic and factual knowledge from the mas-\nsive corpora on which they are trained, and\nprompt engineering seeks to align these mod-\nels to specific tasks. Unfortunately, existing\nprompt engineering methods require signifi-\ncant amounts of labeled data, access to model\nparameters, or both. We introduce a new\nmethod for selecting prompt templates without\nlabeled examples and without direct access to\nthe model. Specifically, over a set of candi-\ndate templates, we choose the template that\nmaximizes the mutual information between\nthe input and the corresponding model output.\nAcross 8 datasets representing 7 distinct NLP\ntasks, we show that when a template has high\nmutual information, it also has high accuracy\non the task. On the largest model, selecting\nprompts with our method gets 90% of the way\nfrom the average prompt accuracy to the best\nprompt accuracy and requires no ground truth\nlabels."}}
{"id": "lVru4M9iiw", "cdate": 1672531200000, "mdate": 1682388469067, "content": {"title": "AI Chat Assistants can Improve Conversations about Divisive Topics", "abstract": "A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these interventions improve the reported quality of the conversation, reduce political divisiveness, and improve the tone, without systematically changing the content of the conversation or moving people's policy attitudes. These findings have important implications for future research on social media, political deliberation, and the growing community of scholars interested in the place of artificial intelligence within computational social science."}}
{"id": "tzQ2ornDAY", "cdate": 1640995200000, "mdate": 1682388469093, "content": {"title": "Leveraging Large Language Models for Multiple Choice Question Answering", "abstract": "While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., \"A\") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated."}}
{"id": "Z21YYwy0U7k", "cdate": 1640995200000, "mdate": 1682388469098, "content": {"title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels", "abstract": "Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \\textit{without labeled examples} and \\textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels."}}
{"id": "SrWwb8LsUKU", "cdate": 1640995200000, "mdate": 1682388469095, "content": {"title": "Out of One, Many: Using Language Models to Simulate Human Samples", "abstract": "We propose and explore the possibility that language models can be studied as effective proxies for specific human sub-populations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \"algorithmic bias\" within one such tool -- the GPT-3 language model -- is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property \"algorithmic fidelity\" and explore its extent in GPT-3. We create \"silicon samples\" by conditioning the model on thousands of socio-demographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and socio-cultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines."}}
{"id": "IYXdTNebIIx", "cdate": 1640995200000, "mdate": 1680659941030, "content": {"title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels", "abstract": ""}}
{"id": "urueR03mkng", "cdate": 1621630343457, "mdate": null, "content": {"title": "Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning", "abstract": "Large natural language models (LMs) (such as GPT-3 or T5) demonstrate impressive abilities across a range of general NLP tasks. Here, we show that the knowledge embedded in such models provides a useful inductive bias, not just on traditional NLP tasks, but also in the nontraditional task of training a symbolic reasoning engine. We observe that these engines learn quickly and generalize in a natural way that reflects human intuition. For example, training such a system to model block-stacking might naturally generalize to stacking other types of objects because of structure in the real world that has been partially captured by the language describing it. We study several abstract textual reasoning tasks, such as object manipulation and navigation, and demonstrate multiple types of generalization to novel scenarios and the symbols that comprise them. We also demonstrate the surprising utility of $\\textit{compositional learning}$, where a learner dedicated to mastering a complicated task gains an advantage by training on relevant simpler tasks instead of jumping straight to the complicated task. "}}
{"id": "ZODLQUKwUs", "cdate": 1609459200000, "mdate": 1682388469095, "content": {"title": "Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning", "abstract": "Large natural language models (such as GPT-3 or T5) demonstrate impressive abilities across a range of general NLP tasks. Here, we show that the knowledge embedded in such models provides a useful inductive bias, not just on traditional NLP tasks, but also in the nontraditional task of training a symbolic reasoning engine. We observe that these engines learn quickly and generalize in a natural way that reflects human intuition. For example, training such a system to model block-stacking might naturally generalize to stacking other types of objects because of structure in the real world that has been partially captured by the language describing it. We study several abstract textual reasoning tasks, such as object manipulation and navigation, and demonstrate multiple types of generalization to novel scenarios and the symbols that comprise them. We also demonstrate the surprising utility of \\textit{compositional learning}, where a learner dedicated to mastering a complicated task gains an advantage by training on relevant simpler tasks instead of jumping straight to the complicated task."}}
