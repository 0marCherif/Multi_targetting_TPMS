{"id": "nvQeDW19oOx", "cdate": 1693526400000, "mdate": 1695953632543, "content": {"title": "Optimizing recommendations under abandonment risks: Models and algorithms", "abstract": ""}}
{"id": "ku13JJDsDat", "cdate": 1686250302280, "mdate": null, "content": {"title": "On-Demand Communication for Asynchronous Multi-Agent Bandits", "abstract": "This paper studies a cooperative multi-agent multi-armed stochastic bandit problem where agents operate $\\textit{asynchronously}$ -- agent pull times and rates are unknown, irregular, and heterogeneous -- and face the same instance of a $K$-armed bandit problem. Agents can share reward information to speed up the learning process at additional communication costs. We propose $\\texttt{ODC}$, an on-demand communication protocol that tailors the communication of each pair of agents based on their empirical pull times. $\\texttt{ODC}$ is efficient when the pull times of agents are highly heterogeneous, and its communication complexity depends on the empirical pull times of agents. $\\texttt{ODC}$ is a generic protocol that can be integrated into most cooperative bandit algorithms without degrading their performance. We then incorporate $\\texttt{ODC}$ into the natural extensions of $\\texttt{UCB}$ and $\\texttt{AAE}$ algorithms and propose two communication-efficient cooperative algorithms. Our analysis shows that both algorithms are near-optimal in regret."}}
{"id": "8kKEz1bnIEp", "cdate": 1676827064170, "mdate": null, "content": {"title": "Exploration for Free: How Does Reward Heterogeneity Improve Regret in Cooperative Multi-agent Bandits?", "abstract": "This paper studies a cooperative multi-agent bandit scenario in which the rewards observed by agents are heterogeneous---one agent's meat can be another agent's poison. Specifically, the total reward observed by each agent is the sum of two values: an arm-specific reward, capturing the intrinsic value of the arm, and a privately-known agent-specific reward, which captures the personal preference/limitations of the agent. This heterogeneity in total reward leads to different local optimal arms for agents but creates an opportunity for *free exploration* in a cooperative setting---an agent can freely explore its local optimal arm with no regret and share this free observation with some other agents who would suffer regrets if they pull this arm since the arm is not optimal for them.\nWe first characterize a regret lower bound that captures free exploration, i.e., arms that can be freely explored have no contribution to the regret lower bound. Then, we present a cooperative bandit algorithm that takes advantage of free exploration and achieves a near-optimal regret upper bound which tightly matches the regret lower bound up to a constant factor.\nLastly, we run numerical simulations to compare our algorithm with various baselines without free exploration."}}
{"id": "jKoKsgXjXf", "cdate": 1672531200000, "mdate": 1695953632544, "content": {"title": "On-Demand Communication for Asynchronous Multi-Agent Bandits", "abstract": "This paper studies a cooperative multi-agent multi-armed stochastic bandit problem where agents operate asynchronously \u2013 agent pull times and rates are unknown, irregular, and heterogeneous \u2013 and f..."}}
{"id": "durbP3e2Ht", "cdate": 1672531200000, "mdate": 1695953632545, "content": {"title": "Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs", "abstract": "Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs."}}
{"id": "LLqY6QA1jV", "cdate": 1672531200000, "mdate": 1695953632545, "content": {"title": "Achieving Near-Optimal Individual Regret & Low Communications in Multi-Agent Bandits", "abstract": ""}}
{"id": "IbdJT31W2RX", "cdate": 1672531200000, "mdate": 1681884836289, "content": {"title": "On-Demand Communication for Asynchronous Multi-Agent Bandits", "abstract": "This paper studies a cooperative multi-agent multi-armed stochastic bandit problem where agents operate asynchronously -- agent pull times and rates are unknown, irregular, and heterogeneous -- and face the same instance of a K-armed bandit problem. Agents can share reward information to speed up the learning process at additional communication costs. We propose ODC, an on-demand communication protocol that tailors the communication of each pair of agents based on their empirical pull times. ODC is efficient when the pull times of agents are highly heterogeneous, and its communication complexity depends on the empirical pull times of agents. ODC is a generic protocol that can be integrated into most cooperative bandit algorithms without degrading their performance. We then incorporate ODC into the natural extensions of UCB and AAE algorithms and propose two communication-efficient cooperative algorithms. Our analysis shows that both algorithms are near-optimal in regret."}}
{"id": "0DL7q1V-p1X", "cdate": 1672531200000, "mdate": 1695953632544, "content": {"title": "Multi-Fidelity Multi-Armed Bandits Revisited", "abstract": ""}}
{"id": "QTXKTXJKIh", "cdate": 1663850327521, "mdate": null, "content": {"title": "Achieving Near-Optimal Individual Regret & Low Communications in Multi-Agent Bandits", "abstract": "Cooperative multi-agent multi-armed bandits (CM2AB) study how distributed agents cooperatively play the same multi-armed bandit game. Most existing CM2AB works focused on maximizing the group performance of all agents---the accumulation of all agents' individual performance (i.e., individual reward). However, in many applications, the performance of the system is more sensitive to the ``bad'' agent---the agent with the worst individual performance. For example, in a drone swarm, a ``bad'' agent may crash into other drones and severely degrade the system performance. In that case, the key of the learning algorithm design is to coordinate computational and communicational resources among agents so to optimize the individual learning performance of the ``bad'' agent. In CM2AB, maximizing the group performance is equivalent to minimizing the group regret of all agents, and minimizing the individual performance can be measured by minimizing the maximum (worst) individual regret among agents. Minimizing the maximum individual regret was largely ignored in prior literature, and currently, there is little work on how to minimize this objective with a low communication overhead. In this paper, we propose a near-optimal algorithm on both individual and group regrets, in addition,  we also propose a novel communication module in the algorithm, which only needs \\(O(\\log (\\log T))\\) communication times where \\(T\\) is the number of decision rounds. We also conduct simulations to illustrate the advantage of our algorithm by comparing it to other known baselines."}}
{"id": "oYmcgHCQNxQ", "cdate": 1640995200000, "mdate": 1672024424953, "content": {"title": "Multi-Player Multi-Armed Bandits with Finite Shareable Resources Arms: Learning Algorithms & Applications", "abstract": "Multi-player multi-armed bandits (MMAB) study how decentralized players cooperatively play the same multi-armed bandit so as to maximize their total cumulative rewards. Existing MMAB models mostly assume when more than one player pulls the same arm, they either have a collision and obtain zero rewards or have no collision and gain independent rewards, both of which are usually too restrictive in practical scenarios. In this paper, we propose an MMAB with shareable resources as an extension of the collision and non-collision settings. Each shareable arm has finite shareable resources and a \u201cper-load\u201d reward random variable, both of which are unknown to players. The reward from a shareable arm is equal to the \u201cper-load\u201d reward multiplied by the minimum between the number of players pulling the arm and the arm\u2019s maximal shareable resources. We consider two types of feedback: sharing demand information (SDI) and sharing demand awareness (SDA), each of which provides different signals of resource sharing. We design the DPE-SDI and SIC-SDA algorithms to address the shareable arm problem under these two cases of feedback respectively and prove that both algorithms have logarithmic regrets that are tight in the number of rounds. We conduct simulations to validate both algorithms\u2019 performance and show their utilities in wireless networking and edge computing."}}
