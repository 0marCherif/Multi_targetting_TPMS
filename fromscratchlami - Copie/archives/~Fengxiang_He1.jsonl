{"id": "uWD69CIURB6", "cdate": 1682899200000, "mdate": 1683886515986, "content": {"title": "Channel Exchanging Networks for Multimodal and Multitask Dense Image Prediction", "abstract": "Multimodal fusion and multitask learning are two vital topics in machine learning. Despite the fruitful progress, existing methods for both problems are still brittle to the same challenge\u2014it remains dilemmatic to integrate the common information across modalities (resp. tasks) meanwhile preserving the specific patterns of each modality (resp. task). Besides, while they are actually closely related to each other, multimodal fusion and multitask learning are rarely explored within the same methodological framework before. In this paper, we propose Channel-Exchanging-Network (CEN) which is self-adaptive, parameter-free, and more importantly, applicable for multimodal and multitask dense image prediction. At its core, CEN adaptively exchanges channels between subnetworks of different modalities. Specifically, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of Batch-Normalization (BN) scaling factor during training. For the application of dense image prediction, the validity of CEN is tested by four different scenarios: multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. Extensive experiments on semantic segmentation via RGB-D data and image translation through multi-domain input verify the effectiveness of CEN compared to state-of-the-art methods. Detailed ablation studies have also been carried out, which demonstrate the advantage of each component we propose. Our code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/yikaiw/CEN</uri> ."}}
{"id": "uVG_7x41bN", "cdate": 1676827082598, "mdate": null, "content": {"title": "$E(2)$-Equivariant Vision Transformer", "abstract": "Vision Transformer (ViT) has achieved remarkable performance in computer vision. However, positional encoding in ViT makes it substantially difficult to learn the intrinsic equivariance in data. Ini- tial attempts have been made on designing equiv- ariant ViT but are proved defective in some cases in this paper. To address this issue, we design a Group Equivariant Vision Transformer (GE-ViT) via a novel, effective positional encoding opera- tor. We prove that GE-ViT meets all the theoreti- cal requirements of an equivariant neural network. Comprehensive experiments are conducted on standard benchmark datasets, demonstrating that GE-ViT significantly outperforms non-equivariant self-attention networks. The code is available at https://github.com/ZJUCDSYangKaifan/GEVit."}}
{"id": "sP-Zleu8jl", "cdate": 1672531200000, "mdate": 1681652299640, "content": {"title": "OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System", "abstract": ""}}
{"id": "PRQep5e3Hef", "cdate": 1672531200000, "mdate": 1682360452679, "content": {"title": "Learning to Generalize Provably in Learning to Optimize", "abstract": "Learning to optimize (L2O) has gained increasing popularity, which automates the design of optimizers by data-driven approaches. However, current L2O methods often suffer from poor generalization performance in at least two folds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms of lowering their loss function values (optimizer generalization, or ``generalizable learning of optimizers\"); and (ii) the test performance of an optimizee (itself as a machine learning model), trained by the optimizer, in terms of the accuracy over unseen data (optimizee generalization, or ``learning to generalize\"). While the optimizer generalization has been recently studied, the optimizee generalization (or learning to generalize) has not been rigorously studied in the L2O context, which is the aim of this paper. We first theoretically establish an implicit connection between the local entropy and the Hessian, and hence unify their roles in the handcrafted design of generalizable optimizers as equivalent metrics of the landscape flatness of loss functions. We then propose to incorporate these two metrics as flatness-aware regularizers into the L2O framework in order to meta-train optimizers to learn to generalize, and theoretically show that such generalization ability can be learned during the L2O meta-training process and then transformed to the optimizee loss function. Extensive experiments consistently validate the effectiveness of our proposals with substantially improved generalization on multiple sophisticated L2O models and diverse optimizees. Our code is available at: https://github.com/VITA-Group/Open-L2O/tree/main/Model_Free_L2O/L2O-Entropy."}}
{"id": "9zzbZHJm7Gp", "cdate": 1672531200000, "mdate": 1683886516100, "content": {"title": "Mixer-Based Semantic Spread for Few-Shot Learning", "abstract": "Key semantics can come from everywhere on an image. Semantic alignment is a key part of few-shot learning but still remains challenging. In this paper, we design a Mixer-Based Semantic Spread (MBSS) algorithm that employs a <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">mixer</i> module to spread the key semantic on the whole image, so that one can directly compare the processed image pairs. We first adopt a convolutional neural network to extract features from both support and query images and separate each of them into multiple Local Descriptor-based Representations (LDRs). The LDRs are then fed into the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">mixer</i> for semantic spread, where every LDR attracts complementary information from its peers. In this way, the objective semantic is made spread on the whole image in a data-driven manner. The overall pipeline is supervised by a voting-based loss, guaranteeing a good <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">mixer</i> . Visualization results validate the feasibility of our <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">mixer</i> . Comprehensive experiments on three benchmark datasets, miniImageNet, tieredImageNet, and CUB, show that our algorithm achieves the state-of-the-art performance in both 5-way 1-shot and 5-way 5-shot settings."}}
{"id": "1L_XgdOzBHD", "cdate": 1672531200000, "mdate": 1683886516061, "content": {"title": "Global Nash Equilibrium in Non-convex Multi-player Game: Theory and Algorithms", "abstract": "Wide machine learning tasks can be formulated as non-convex multi-player games, where Nash equilibrium (NE) is an acceptable solution to all players, since no one can benefit from changing its strategy unilaterally. Attributed to the non-convexity, obtaining the existence condition of global NE is challenging, let alone designing theoretically guaranteed realization algorithms. This paper takes conjugate transformation to the formulation of non-convex multi-player games, and casts the complementary problem into a variational inequality (VI) problem with a continuous pseudo-gradient mapping. We then prove the existence condition of global NE: the solution to the VI problem satisfies a duality relation. Based on this VI formulation, we design a conjugate-based ordinary differential equation (ODE) to approach global NE, which is proved to have an exponential convergence rate. To make the dynamics more implementable, we further derive a discretized algorithm. We apply our algorithm to two typical scenarios: multi-player generalized monotone game and multi-player potential game. In the two settings, we prove that the step-size setting is required to be $\\mathcal{O}(1/k)$ and $\\mathcal{O}(1/\\sqrt k)$ to yield the convergence rates of $\\mathcal{O}(1/ k)$ and $\\mathcal{O}(1/\\sqrt k)$, respectively. Extensive experiments in robust neural network training and sensor localization are in full agreement with our theory."}}
{"id": "qbWGEJVX06", "cdate": 1668821614650, "mdate": 1668821614650, "content": {"title": "Bridged Transformer for Vision and Point Cloud 3D Object Detection", "abstract": "3D object detection is a crucial research topic in computer vision, which usually uses 3D point clouds as input in\nconventional setups. Recently, there is a trend of leveraging multiple sources of input data, such as complementing\nthe 3D point cloud with 2D images that often have richer\ncolor and fewer noises. However, due to the heterogeneous\ngeometrics of the 2D and 3D representations, it prevents\nus from applying off-the-shelf neural networks to achieve\nmultimodal fusion. To that end, we propose Bridged Transformer (BrT), an end-to-end architecture for 3D object detection. BrT is simple and effective, which learns to identify\n3D and 2D object bounding boxes from both points and image patches. A key element of BrT lies in the utilization\nof object queries for bridging 3D and 2D spaces, which\nunifies different sources of data representations in Transformer. We adopt a form of feature aggregation realized by\npoint-to-patch projections which further strengthen the interaction between images and points. Moreover, BrT works\nseamlessly for fusing the point cloud with multi-view images. We experimentally show that BrT surpasses state-ofthe-art methods on SUN RGB-D and ScanNetV2 datasets."}}
{"id": "7jCS9DP0waP", "cdate": 1667356787912, "mdate": 1667356787912, "content": {"title": "Siamese Network with Interactive Transformer for Video Object Segmentation", "abstract": "Semi-supervised video object segmentation (VOS) refers to segmenting the target object in remaining frames given its annotation\nin the first frame, which has been actively studied in recent years. The key challenge lies in finding effective ways to exploit the spatio-temporal context of past frames to help learn discriminative target representation of current frame. In this paper, we propose a novel Siamese network with a specifically designed interactive transformer, called SITVOS, to enable effective context propagation from historical to current frames. Technically, we use the transformer encoder and decoder to handle the past frames and current frame separately, i.e., the encoder encodes robust spatio-temporal context of target object from the past frames, while the decoder takes the\nfeature embedding of current frame as the query to retrieve the target from the encoder output. To further enhance the target representation, a feature interaction module (FIM) is devised to promote the information flow between the encoder and decoder. Moreover, we employ the Siamese architecture to extract backbone features of both past and current frames, which enables feature reuse and is more efficient than existing methods. Experimental results on three challenging benchmarks validate the superiority of SITVOS over state-of-theart methods."}}
{"id": "dkLQ9dl4vcY", "cdate": 1663850503135, "mdate": null, "content": {"title": "Probe Into Multi-agent Adversarial Reinforcement Learning through Mean-Field Optimal Control", "abstract": "Multi-agent adversarial reinforcement learning (MaARL) has shown promise in solving adversarial games. However, the theoretical tools for MaARL's analysis is still elusive. In this paper, we take the first step to theoretically understanding MaARL through mean-field optimal control. Specifically, we model MaARL as a mean-field quantitative differential game between two dynamical systems with implicit terminal constraints. Based on the game, we respectively study the optimal solution and the generalization of the fore-mentioned game. First of all, a two-sided extremism principle (TSEP) is then established as a necessary condition for the optimal solution of the game. We further show that TSEP is also sufficient given that the terminal time is sufficiently small. Secondly, based on the TSEP, a generalization bound for MaARL is proposed. The bound does not explicitly rely on the dimensions, norms, or other capacity measures of the model, which are usually prohibitively large in deep learning."}}
{"id": "_-eJYVfSYH", "cdate": 1663850329962, "mdate": null, "content": {"title": "Would decentralization hurt generalization?", "abstract": "Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices without controlling of a central server. Existing theory suggests that the decentralization degrades the generalizability, which conflicts with experimental results in the large-batch settings that D-SGD generalize better than centralized SGD (C-SGD). This work presents new theory that reconciles the conflict between the two perspectives. We prove that D-SGD introduces an implicit regularization that simultaneously penalizes (1) the sharpness of the learned minima and (2) the consensus distance between the consensus model and local models. We then prove that the implicit regularization is amplified in the large-batch settings when the linear scaling rule is applied. We further analyze the escaping efficiency of D-SGD, which suggests that D-SGD favors super-quadratic flat minima. Experiments are in full agreement with our theory. The code will be released publicly. To our best knowledge, this is the first work on the implicit regularization and escaping efficiency of D-SGD."}}
