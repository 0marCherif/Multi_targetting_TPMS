{"id": "ueDY9qgeQ3o", "cdate": 1672531200000, "mdate": 1695977747430, "content": {"title": "A Comprehensive Empirical Evaluation on Online Continual Learning", "abstract": "Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the results and basic experience replay, when properly tuned and implemented, is a very strong baseline. We release our modular and extensible codebase at https://github.com/AlbinSou/ocl_survey based on the avalanche framework to reproduce our results and encourage future research."}}
{"id": "9qrn3li8L4w", "cdate": 1672531200000, "mdate": 1695977747427, "content": {"title": "Improving Online Continual Learning Performance and Stability with Temporal Ensembles", "abstract": "Neural networks are very effective when trained on large datasets for a large number of iterations. However, when they are trained on non-stationary streams of data and in an online fashion, their performance is reduced (1) by the online setup, which limits the availability of data, (2) due to catastrophic forgetting because of the non-stationary nature of the data. Furthermore, several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.13452 showed that replay methods used in continual learning suffer from the stability gap, encountered when evaluating the model continually (rather than only on task boundaries). In this article, we study the effect of model ensembling as a way to improve performance and stability in online continual learning. We notice that naively ensembling models coming from a variety of training tasks increases the performance in online continual learning considerably. Starting from this observation, and drawing inspirations from semi-supervised learning ensembling methods, we use a lightweight temporal ensemble that computes the exponential moving average of the weights (EMA) at test time, and show that it can drastically increase the performance and stability when used in combination with several methods from the literature."}}
{"id": "vpY4rGSZYIW", "cdate": 1667656388827, "mdate": 1667656388827, "content": {"title": "On the importance of cross-task features for class-incremental learning", "abstract": "In class-incremental learning, an agent with limited resources needs to learn a sequence of classification tasks, forming an ever growing classification problem, with the constraint of not being able to access data from previous tasks. The\nmain difference with task-incremental learning,\nwhere a task-ID is available at inference time, is\nthat the learner also needs to perform cross-task\ndiscrimination, i.e. distinguish between classes\nthat have not been seen together. Approaches\nto tackle this problem are numerous and mostly\nmake use of an external memory (buffer) of nonnegligible size. In this paper, we ablate the learning of cross-task features and study its influence\non the performance of basic replay strategies used\nfor class-IL. We also define a new forgetting measure for class-incremental learning, and see that\nforgetting is not the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning\nshould not only prevent forgetting, but also aim\nto improve the quality of the cross-task features,\nand the knowledge transfer between tasks. This is\nespecially important when tasks contain limited\namount of data."}}
{"id": "f77FhfGzQWN", "cdate": 1663849897615, "mdate": null, "content": {"title": "Elastic Mean-Teacher Distillation Mitigates the Continual Learning Stability Gap", "abstract": "Nowadays, neural networks are being used to solve a variety of tasks. They are very effective when trained on large datasets. However, in continual learning, they are trained on non-stationary stream of data, which often results in forgetting of the previous knowledge. In the literature, continual learning models are exposed to a sequence of tasks, and must learn each task one by one. They are then evaluated at the end of each learning session. This allows to measure the average accuracy over all tasks encountered so far. Recently De Lange et al. (2022) showed that continual learning methods suffer from the Stability Gap, encountered when evaluating the model continually. Even when the performance at the end of training is high, the worst-case performance is low, which could be a problem in applications where the learner needs to always perform greatly on all tasks while learning the new task. In this paper, we propose to apply a refined variant of knowledge distillation, adapted to the class-incremental learning setting, and used in combination with replay, to improve the stability of the continual learning algorithms. We also propose to use a distillation method derived from the Mean teacher distillation training paradigm introduced in semi-supervised learning. We demonstrate empirically that the use of this method enhances the stability in the more challenging setting of online continual learning."}}
{"id": "CQGzeO9hET", "cdate": 1609459200000, "mdate": 1668509327150, "content": {"title": "On the importance of cross-task features for class-incremental learning", "abstract": "In class-incremental learning, an agent with limited resources needs to learn a sequence of classification tasks, forming an ever growing classification problem, with the constraint of not being able to access data from previous tasks. The main difference with task-incremental learning, where a task-ID is available at inference time, is that the learner also needs to perform cross-task discrimination, i.e. distinguish between classes that have not been seen together. Approaches to tackle this problem are numerous and mostly make use of an external memory (buffer) of non-negligible size. In this paper, we ablate the learning of cross-task features and study its influence on the performance of basic replay strategies used for class-IL. We also define a new forgetting measure for class-incremental learning, and see that forgetting is not the principal cause of low performance. Our experimental results show that future algorithms for class-incremental learning should not only prevent forgetting, but also aim to improve the quality of the cross-task features, and the knowledge transfer between tasks. This is especially important when tasks contain limited amount of data."}}
