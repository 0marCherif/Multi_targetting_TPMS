{"id": "w5-HpC6v-8", "cdate": 1682669530854, "mdate": null, "content": {"title": "A New Non-Central Model for Fisheye Calibration", "abstract": "A new non-central model suitable for calibrating fisheye cameras is proposed. It is a direct extension of the popular central model developed by Scaramuzza et al., used by Matlab Computer Vision Toolbox fisheye calibration tool. It allows adapting existing applications that are using this central model to a non-central projection that is more accurate, especially when objects captured in the images are close to the camera, and it makes it possible to switch easily between the more accurate non-central characterization of the fisheye camera and the more convenient central approximation, as needed. It is shown that the algorithms proposed by Scaramuzza et al. for their central model can be modified to accommodate the angle dependent axial viewpoint shift. This means, besides other, that a similar process can be used for calibration involving the viewpoint shift characterization and a user-friendly calibration tool can be produced with this new non-central model that does not require the user to provide detailed lens design specifications or an educated guess for the initial parameter values. Several other improvements to the Scaramuzza's central model are also introduced, helping to improve the performance of both the central model, and its non-central extension."}}
{"id": "B1W2pyzu-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Multi-Capture Dynamic Calibration of Multi-Camera Systems", "abstract": "Multi-camera systems have seen an emergence in various consumer devices enabling many applications e.g. bokeh (Apple IPhone), 3D measurement (Dell Venue 8) etc. An accurately calibrated multi-camera system is essential for proper functioning of these applications. Usually, a onetime factory calibration with technical targets is done to accurately calibrate such systems. Although accurate, factory calibration does not hold over the life time of the device as normal wear and tear, thermal effects, device usage etc. can cause calibration parameters to change. Thus, a dynamic or self-calibration based on multi-view image features is required to refine calibration parameters. One of the important factors governing the accuracy of dynamic calibration is the number and distribution of feature points in the captured scene. A dense feature distribution enables better sampling of the 3D scene, while avoiding degenerate situations (e.g. all features on one plane), thus sufficiently modeling the forward imaging process for calibration. But, single real life images with dense feature distribution are difficult or nearly impossible to capture e.g. texture-less indoor or occluded scenes. In this paper, we propose a new multi-capture paradigm for multi-camera dynamic calibration where multiple multi-view images of different 3D scenes (thus varying feature point distribution) are jointly used to calibrate the multi-camera system. We present a new optimality criteria to select the best set of candidate images from a pool of multi-view images, along with their order, to use for multi-capture dynamic calibration. We also propose a methodology to jointly model calibration parameters of multiple multi-view images. Finally, we show improved performance of multi-capture dynamic calibration over single-capture dynamic calibration in terms of lower epipolar rectification and 3D measurement error."}}
{"id": "SkbPKeGOZB", "cdate": 1420070400000, "mdate": null, "content": {"title": "On the Equivalence of Moving Entrance Pupil and Radial Distortion for Camera Calibration", "abstract": "Radial distortion for ordinary (non-fisheye) camera lenses has traditionally been modeled as an infinite series function of radial location of an image pixel from the image center. While there has been enough empirical evidence to show that such a model is accurate and sufficient for radial distortion calibration, there has not been much analysis on the geometric/physical understanding of radial distortion from a camera calibration perspective. In this paper, we show using a thick-lens imaging model, that the variation of entrance pupil location as a function of incident image ray angle is directly responsible for radial distortion in captured images. Thus, unlike as proposed in the current state-of-the-art in camera calibration, radial distortion and entrance pupil movement are equivalent and need not be modeled together. By modeling only entrance pupil motion instead of radial distortion, we achieve two main benefits, first, we obtain comparable if not better pixel re-projection error than traditional methods, second, and more importantly, we directly back-project a radially distorted image pixel along the true image ray which formed it. Using a thick-lens setting, we show that such a back-projection is more accurate than the two-step method of undistorting an image pixel and then back-projecting it. We have applied this calibration method to the problem of generative depth-from-focus using focal stack to get accurate depth estimates."}}
{"id": "PfkPNYglFcX", "cdate": 1388534400000, "mdate": 1623624382036, "content": {"title": "Generalized Radial Alignment Constraint for Camera Calibration", "abstract": "In camera calibration, the radial alignment constraint (RAC) has been proposed as a technique to obtain closed form solution to calibration parameters when the image distortion is purely radial about an axis normal to the sensor plane. But, in real images this normality assumption might be violated due to manufacturing limitations or intentional sensor tilt. A misaligned optic axis results in traditional formulation of RAC not holding for real images leading to calibration errors. In this paper, we propose a generalized radial alignment constraint (gRAC), which relaxes the optic axis-sensor normality constraint by explicitly modeling their configuration via rotation parameters which form a part of camera calibration parameter set. We propose a new analytical solution to solve the gRAC for a subset of calibration parameters. We discuss the resulting ambiguities in the analytical approach and propose methods to overcome them. The analytical solution is then used to compute the intersection of optic axis and the sensor about which overall distortion is indeed radial. Finally, the analytical estimates from gRAC are used to initialize the nonlinear refinement of calibration parameters. Using simulated and real data, we show the correctness of the proposed gRAC and the analytical solution in achieving accurate camera calibration."}}
{"id": "Ir3e_sEkRxc", "cdate": 1388534400000, "mdate": 1623624382028, "content": {"title": "Non-frontal Camera Calibration Using Focal Stack Imagery", "abstract": "A non-frontal camera has its lens and sensor plane misaligned either due to manufacturing limitations or an intentional tilting as in tilt-shift cameras. Under ideal perspective imaging, a geometric calibration of tilt is impossible as tilt parameters are correlated with the principal point location parameter. In other words, there are infinite combinations of principal point and sensor tilt parameters such that the perspective imaging equations are satisfied equally well. Previously, the non-frontal calibration problem (including sensor tilt estimation) has been solved by introducing constraints to align the principal point with the center of radial distortion. In this paper, we propose an additional constraint which incorporates image blur/defocus present in non-frontal camera images into the calibration framework. Specifically, it has earlier been shown that a non-frontal camera rotating about its center of projection captures images with varying focus. This stack of images is referred to as a focal stack. Given a focal stack of a known checkerboard (CB) pattern captured from a non-frontal camera, we combine geometric re-projection error and image bur error computed from current estimate of sensor tilt as the calibration optimization criteria. We show that the combined technique outperforms geometry-only methods while also additionally yielding blur kernel estimates at CB corners."}}
{"id": "Bk-gJkGdWr", "cdate": 1388534400000, "mdate": null, "content": {"title": "Generalized Pupil-centric Imaging and Analytical Calibration for a Non-frontal Camera", "abstract": "We consider the problem of calibrating a small field of view central perspective non-frontal camera whose lens and sensor planes may not be parallel to each other. This can be due to manufacturing defects or intentional tilting. Thus, as such all cameras can be modeled as being non-frontal with varying degrees. There are two approaches to model non- frontal cameras. The first one based on rotation parameterization of sensor non-frontalness/tilt increases the number of calibration parameters, thus requiring heuristics to initialize a few calibration parameters for the final non-linear optimization step. Additionally, for this parameterization, while it has been shown that pupil-centric imaging model leads to more accurate rotation estimates than a thin-lens imaging model, it has only been developed for a single axis lens-sensor tilt. But, in real cameras we can have arbitrary tilt. The second approach based on decentering distortion modeling is approximate as it can only handle small tilts and cannot explicitly estimate the sensor tilt. In this paper, we focus on rotation based non-frontal camera calibration and address the aforementioned problems of over-parameterization and inadequacy of existing pupil-centric imaging model. We first derive a generalized pupil-centric imaging model for arbitrary axis lens-sensor tilt. We then derive an analytical solution, in this setting, for a subset of calibration parameters including sensor rotation angles as a function of center of radial distortion (CoD). A radial alignment based constraint is then proposed to computationally estimate CoD leveraging on the proposed analytical solution. Our analytical technique also estimates pupil-centric parameters of entrance pupil location and optical focal length, which have typically been done optically. Given these analytical and computational calibration parameter estimates, we initialize the non-linear calibration optimization for a set of synthetic and real data captured from a non-frontal camera and show reduced pixel re-projection and undistortion errors compared to state of the art techniques in rotation and decentering based approaches to non-frontal camera calibration."}}
{"id": "Vc-pN2jTLR", "cdate": 1356998400000, "mdate": 1623624382041, "content": {"title": "Motion-based background subtraction and panoramic mosaicing for freight train analysis", "abstract": "We propose a new motion-based background removal technique which along with panoramic mosaicing forms the core of a vision system we have developed for analyzing the loading efficiency of intermodal freight trains. This analysis is critical for estimating the aerodynamic drag caused by air gaps present between loads in freight trains. The novelty of our background removal technique lies in using conventional motion estimates to design a cost function which can handle challenging textureless background regions, e.g. clear blue sky. Supplemented with domain knowledge, we have built a system which has outperformed some recent background removal methods applied to our problem. We also build an orthographic mosaic of the freight train allowing identification of load types and gap lengths between them. The complete system has been installed near Sibley, Missouri, US and processes about 20-30 (5-10 GB/train video data depending on train length) trains per day with high accuracy."}}
{"id": "2Cczos_lI_z", "cdate": 1356998400000, "mdate": 1623624382037, "content": {"title": "A generative focus measure with application to omnifocus imaging", "abstract": "Given a stack of registered images acquired using a range of focus settings (focal stack images), we propose a new focus measure to identify the most focused image. Although, most of the paper is concerned with the new focus measure, for evaluation purposes, we will present it in the context of an application to generating omnifocus images. An omnifocus image is the composite image in which each pixel is selected form the frame in the stack in which it appears to be in best focus. Conventional focus measures usually maximize some measure of image gradient in a window. They tend to fail when one of the edges of the window lies near the boundary of an intensity edge, or the pixel is near other complex edge patterns. This leads to the misidentification of the focused frame and formation of artifacts in omnifocused image. Our proposed measure does not attempt to identify the focused frame by calculating the degree of defocus, like the gradient based methods. Rather, it hypothesizes that a specific frame is in focus and then validates or rejects this hypothesis by recreating the defocused frames in the vicinity, and comparing them with the observed de-focused frames. This forward generative process leads to correct focus frame selection in regions where typical measures fail. This is because the conventional measures try to identify the focused frame from its distorted version which is the result of a complex convolution process. This involves a backward estimation for a many-to-one transformation. On the other hand, the generation of defocused frames from a hypothesized focused frame is more accurate since it involves applying an operator in the forward direction. We analytically show that under ideal imaging conditions, the proposed focus measure is unimodal in nature. This makes the search for the best focused image unambiguous. We evaluate our focus measure by generating omnifocus images from real focal stack images, and show that it performs better than all the existing focus measures."}}
{"id": "HRvvnayCmdE", "cdate": 1230768000000, "mdate": 1623624382040, "content": {"title": "Robust segmentation of freight containers in train monitoring videos", "abstract": "This paper is about a vision-based system that automatically monitors intermodal freight trains for the quality of how the loads (containers) are placed along the train. An accurate and robust algorithm to segment the foreground of containers in videos of the moving train is indispensable for this purpose. Given a video of a moving train consisting of containers of different types, this paper presents a method exploiting the information in both frequency and spatial domains to segment these containers. This method can accurately segment all types of containers under a variety of background conditions, e.g illumination variations and moving clouds, in the train videos shot by a fixed camera. The accuracy and robustness of the proposed method are substantiated through a large number of experiments on real data of train videos."}}
{"id": "2LSpJpetQXt", "cdate": 1167609600000, "mdate": 1623624382043, "content": {"title": "A Vision System for Monitoring Intermodal Freight Trains", "abstract": "We describe the design and implementation of a vision based Intermodal Train Monitoring System (ITMS) for extracting various features like length of gaps in an intermodal (IM) train which can later be used for higher level inferences. An intermodal train is a freight train consisting of two basic types of loads - containers and trailers. Our system first captures the video of an IM train, and applies image processing and machine learning techniques developed in this work to identify the various types of loads as containers and trailers. The whole process relies on a sequence of following tasks -robust background subtraction in each frame of the video, estimation of train velocity, creation of mosaic of the whole train from the video and classification of train loads into containers and trailers. Finally, the length of gaps between the loads of the IM train is estimated and is used to analyze the aerodynamic efficiency of the loading pattern of the train, which is a critical aspect of freight trains. This paper focusses on the machine vision aspect of the whole system"}}
