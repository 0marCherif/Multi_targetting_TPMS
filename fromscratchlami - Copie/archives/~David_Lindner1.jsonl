{"id": "zhDO3F35Uc", "cdate": 1668734780170, "mdate": null, "content": {"title": "Red-Teaming the Stable Diffusion Safety Filter", "abstract": "Stable Diffusion is a recent open-source image generation model comparable to proprietary models such as DALL\u00b7E, Imagen, or Parti. Stable Diffusion comes with a safety filter that aims to prevent generating explicit images. Unfortunately, the filter is obfuscated and poorly documented. This makes it hard for users to prevent misuse in their applications, and to understand the filter's limitations and improve it. We first show that it is easy to generate disturbing content that bypasses the safety filter. We then reverse-engineer the filter and find that while it aims to prevent sexual content, it ignores violence, gore, and other similarly disturbing content. Based on our analysis, we argue safety measures in future model releases should strive to be fully open and properly documented to stimulate security contributions from the community."}}
{"id": "TPOJzwv2pc", "cdate": 1652737700084, "mdate": null, "content": {"title": "Active Exploration for Inverse Reinforcement Learning", "abstract": "Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy, or they at least require access to a generative model. However, these assumptions are too strong for many real-world applications, where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm: Active exploration for Inverse Reinforcement Learning (AceIRL), which actively explores an unknown environment and expert policy to quickly learn the expert\u2019s reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample-complexity bounds that does not require a generative model of the environment. AceIRL matches the sample complexity of active IRL with a generative model in the worst case. Additionally, we establish a problem-dependent bound that relates the sample complexity of AceIRL to the suboptimality gap of a given IRL problem. We empirically evaluate AceIRL in simulations and find that it significantly outperforms more naive exploration strategies."}}
{"id": "zadoqXSPSfj", "cdate": 1640995200000, "mdate": 1673108075178, "content": {"title": "Humans are not Boltzmann Distributions: Challenges and Opportunities for Modelling Human Feedback and Interaction in Reinforcement Learning", "abstract": ""}}
{"id": "Z-O9QbkIH0", "cdate": 1640995200000, "mdate": 1673108075188, "content": {"title": "Active Exploration for Inverse Reinforcement Learning", "abstract": ""}}
{"id": "S2227UJSLZE", "cdate": 1640995200000, "mdate": 1673108075134, "content": {"title": "Interactively Learning Preference Constraints in Linear Bandits", "abstract": ""}}
{"id": "F3a8ptOsg_", "cdate": 1640995200000, "mdate": 1673108075190, "content": {"title": "Red-Teaming the Stable Diffusion Safety Filter", "abstract": ""}}
{"id": "BxbJh_cBeq", "cdate": 1640995200000, "mdate": 1645746343124, "content": {"title": "Scalable Safe Exploration for Global Optimization of Dynamical Systems", "abstract": "Learning optimal control policies directly on physical systems is challenging since even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. A notable exception is the GoSafe algorithm, which, unfortunately, cannot handle high-dimensional systems and hence cannot be applied to most real-world dynamical systems. This work proposes GoSafeOpt as the first algorithm that can safely discover globally optimal policies for high-dimensional systems while giving safety and optimality guarantees. We demonstrate the superiority of GoSafeOpt over competing model-free safe learning methods on a robot arm that would be prohibitive for GoSafe."}}
{"id": "t5-Mszu1UkO", "cdate": 1621630197189, "mdate": null, "content": {"title": "Information Directed Reward Learning for Reinforcement Learning", "abstract": "For many reinforcement learning (RL) applications, specifying a reward is difficult. In this paper, we consider an RL setting where the agent can obtain information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. From such expensive feedback, we aim to learn a model of the reward function that allows standard RL algorithms to achieve high expected return with as few expert queries as possible. For this purpose, we propose Information Directed Reward Learning (IDRL), which uses a Bayesian model of the reward function and selects queries that maximize the information gain about the difference in return between potentially optimal policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. Moreover, by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model, it achieves similar or better performance with significantly fewer queries. We support our findings with extensive evaluations  in multiple environments and with different types of queries."}}
{"id": "y1GdD82ycD", "cdate": 1609459200000, "mdate": 1673108075286, "content": {"title": "Learning What To Do by Simulating the Past", "abstract": ""}}
{"id": "wOdYwIpxPx5", "cdate": 1609459200000, "mdate": 1673108075257, "content": {"title": "Challenges for Using Impact Regularizers to Avoid Negative Side Effects", "abstract": ""}}
