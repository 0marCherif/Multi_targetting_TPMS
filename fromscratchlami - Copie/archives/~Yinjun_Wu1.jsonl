{"id": "7ySw8LX2pS", "cdate": 1672531200000, "mdate": 1683908204305, "content": {"title": "Learning to Select Pivotal Samples for Meta Re-weighting", "abstract": "Sample re-weighting strategies provide a promising mechanism to deal with imperfect training data in machine learning, such as noisily labeled or class-imbalanced data. One such strategy involves formulating a bi-level optimization problem called the meta re-weighting problem, whose goal is to optimize performance on a small set of perfect pivotal samples, called meta samples. Many approaches have been proposed to efficiently solve this problem. However, all of them assume that a perfect meta sample set is already provided while we observe that the selections of meta sample set is performance critical. In this paper, we study how to learn to identify such a meta sample set from a large, imperfect training set, that is subsequently cleaned and used to optimize performance in the meta re-weighting setting. We propose a learning framework which reduces the meta samples selection problem to a weighted K-means clustering problem through rigorously theoretical analysis. We propose two clustering methods within our learning framework, Representation-based clustering method (RBC) and Gradient-based clustering method (GBC), for balancing performance and computational efficiency. Empirical studies demonstrate the performance advantage of our methods over various baseline methods."}}
{"id": "2QVlLKnXac", "cdate": 1672531200000, "mdate": 1681860297993, "content": {"title": "Do Machine Learning Models Learn Common Sense?", "abstract": "Machine learning models can make critical errors that are easily hidden within vast amounts of data. Such errors often run counter to rules based on human intuition. However, rules based on human knowledge are challenging to scale or to even formalize. We thereby seek to infer statistical rules from the data and quantify the extent to which a model has learned them. We propose a framework SQRL that integrates logic-based methods with statistical inference to derive these rules from a model's training data without supervision. We further show how to adapt models at test time to reduce rule violations and produce more coherent predictions. SQRL generates up to 300K rules over datasets from vision, tabular, and language settings. We uncover up to 158K violations of those rules by state-of-the-art models for classification, object detection, and data imputation. Test-time adaptation reduces these violations by up to 68.7% with relative performance improvement up to 32%. SQRL is available at https://github.com/DebugML/sqrl."}}
{"id": "wKX0nNK6b2e", "cdate": 1640995200000, "mdate": 1683908204088, "content": {"title": "Provenance-based Model Maintenance: Implications for Privacy", "abstract": ""}}
{"id": "xUvqWrIAXXh", "cdate": 1609459200000, "mdate": 1683908204222, "content": {"title": "CHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label Uncertainties (Technical Report)", "abstract": "High-quality labels are expensive to obtain for many machine learning tasks, such as medical image classification tasks. Therefore, probabilistic (weak) labels produced by weak supervision tools are used to seed a process in which influential samples with weak labels are identified and cleaned by several human annotators to improve the model performance. To lower the overall cost and computational overhead of this process, we propose a solution called CHEF (CHEap and Fast label cleaning), which consists of the following three components. First, to reduce the cost of human annotators, we use Infl, which prioritizes the most influential training samples for cleaning and provides cleaned labels to save the cost of one human annotator. Second, to accelerate the sample selector phase and the model constructor phase, we use Increm-Infl to incrementally produce influential samples, and DeltaGrad-L to incrementally update the model. Third, we redesign the typical label cleaning pipeline so that human annotators iteratively clean smaller batch of samples rather than one big batch of samples. This yields better over all model performance and enables possible early termination when the expected model performance has been achieved. Extensive experiments show that our approach gives good model prediction performance while achieving significant speed-ups."}}
{"id": "ucLkiIgXZP", "cdate": 1609459200000, "mdate": 1683908204135, "content": {"title": "CHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label Uncertainties", "abstract": ""}}
{"id": "i6U88cr3Md", "cdate": 1609459200000, "mdate": 1673993807041, "content": {"title": "Dynamic Gaussian Mixture based Deep Generative Model For Robust Forecasting on Sparse Multivariate Time Series", "abstract": ""}}
{"id": "GiT09TMG5YS", "cdate": 1609459200000, "mdate": 1673993807139, "content": {"title": "Dynamic Gaussian Mixture based Deep Generative Model For Robust Forecasting on Sparse Multivariate Time Series", "abstract": ""}}
{"id": "kmb1Pyxp2D", "cdate": 1577836800000, "mdate": 1683902570580, "content": {"title": "DeltaGrad: Rapid retraining of machine learning models", "abstract": "Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art."}}
{"id": "hMS9q-c-UM2", "cdate": 1577836800000, "mdate": 1683908204415, "content": {"title": "Why data citation isn't working, and what to do about it", "abstract": "We describe a system that automatically generates from a curated database a collection of short conventional publications\u2014citation summaries\u2014that describe the contents of various components of the database. The purpose of these summaries is to ensure that the contributors to the database receive appropriate credit through the currently used measures such as h-indexes. Moreover, these summaries also serve to give credit to publications and people that are cited by the database. In doing this, we need to deal with granularity\u2014how many summaries should be generated to represent effectively the contributions to a database? We also need to deal with evolution\u2014for how long can a given summary serve as an appropriate reference when the database is evolving? We describe a journal specifically tailored to contain these citation summaries. We also briefly discuss the limitations that the current mechanisms for recording citations place on both the process and value of data citation."}}
{"id": "dWd4oOCkOBp", "cdate": 1577836800000, "mdate": 1683908204228, "content": {"title": "Lessons learned from the early performance evaluation of Intel optane DC persistent memory in DBMS", "abstract": "Non-volatile memory (NVM) is an emerging technology, which has the persistence characteristics of large capacity storage devices, while providing the low access latency and byte-addressablity of traditional DRAM memory. In this paper, we provide extensive performance evaluations on a recently released NVM device, Intel Optane DC Persistent Memory (PMem), under different configurations with several micro-benchmark tools. Further, we evaluate OLTP and OLAP database workloads with Microsoft SQL Server 2019 when using PMem as buffer pool or persistent storage. From the lessons learned we share some recommendations for future DBMS design with PMem, e.g. simple hardware or software changes are not enough for the best use of PMem in DBMSs."}}
