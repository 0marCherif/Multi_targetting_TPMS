{"id": "SnBDX5k-KuJ", "cdate": 1663850488319, "mdate": null, "content": {"title": "Solving Continual Learning via Problem Decomposition", "abstract": "This paper is concerned with class incremental learning (CIL) in continual learning (CL). CIL is the popular continual learning paradigm in which a system receives a sequence of tasks with different classes in each task and is expected to learn to predict the class of each test instance without given any task related information for the instance. Although many techniques have been proposed to solve CIL, it remains to be highly challenging due to the difficulty of dealing with catastrophic forgetting (CF). This paper starts from the first principle and proposes a novel method to solve the problem. The definition of CIL reveals that the problem can be decomposed into two probabilities: within-task prediction probability and task-id prediction probability. This paper proposes an effective technique to estimate these two probabilities based on the estimation of feature distributions in the latent space using incremental PCA and Mahalanobis distance. The proposed method does not require a memory buffer to save replay data and it outperforms strong baselines including replay-based methods."}}
{"id": "m_GDIItaI3o", "cdate": 1663850463925, "mdate": null, "content": {"title": "Continual Pre-training of Language Models", "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method."}}
{"id": "ncQCD9M8SwT", "cdate": 1663850462077, "mdate": null, "content": {"title": "Continual Learning Based on Sub-Networks and Task Similarity", "abstract": "Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT) across tasks. The existing literature mainly tries to overcome CF. Although some papers have focused on both CF and KT, they may still suffer from CF because of their ineffective handling of previous tasks and/or poor task similarity detection mechanisms to achieve KT. This work presents a new CL method that addresses the above issues. First, it overcomes CF by isolating the knowledge of each task via a learned mask that indicates a sub-network. Second, it proposes a novel technique to compute how important each mask is to the new task, which indicates how the new task is similar to an underlying old task. Similar tasks can share the same mask/subnetwork for KT, while dissimilar tasks use different masks/sub-networks for CF prevention. Comprehensive experiments have been conducted using a range of NLP problems, including classification, generation, and extraction to show that the proposed method consistently outperforms prior state-of-the-art baselines."}}
{"id": "Zz8_2A4iPS", "cdate": 1663850399703, "mdate": null, "content": {"title": "Continual Learning with Soft-Masking of Parameter-Level Gradient Flow", "abstract": "Existing research on task incremental learning in continual learning has primarily focused on preventing catastrophic forgetting (CF). Several techniques have achieved learning with no CF. However, they attain it by letting each task monopolize a sub-network in a shared network, which seriously limits knowledge transfer (KT) and causes over-consumption of the network capacity, i.e., as more tasks are learned, the performance deteriorates. The goal of this paper is threefold: (1) overcoming CF, (2) encouraging KT, and (3) tackling the capacity problem. A novel and simple technique (called SPG) is proposed that soft-masks (partially blocks) parameter updating in training based on the importance of each parameter to old tasks. Each task still uses the full network, i.e., no monopoly of any part of the network by any task, which enables maximum KT and reduction of capacity usage. Extensive experiments demonstrate the effectiveness of SPG in achieving all three objectives. More notably, it attains significant transfer of knowledge not only among similar tasks (with shared knowledge) but also among dissimilar tasks (with little shared knowledge) while preventing CF."}}
{"id": "bA8CYH5uEn_", "cdate": 1652737770763, "mdate": null, "content": {"title": "A Theoretical Study on Solving Continual Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally. There are two popular CL settings, class incremental learning (CIL) and task incremental learning (TIL). A major challenge of CL is catastrophic forgetting (CF). While a number of techniques are already available to effectively overcome CF for TIL, CIL remains to be highly challenging. So far, little theoretical study has been done to provide a principled guidance on how to solve the CIL problem. This paper performs such a study. It first shows that probabilistically, the CIL problem can be decomposed into two sub-problems: Within-task Prediction (WP) and Task-id Prediction (TP). It further proves that TP is correlated with out-of-distribution (OOD) detection, which connects CIL and OOD detection. The key conclusion of this study is that regardless of whether WP and TP or OOD detection are defined explicitly or implicitly by a CIL algorithm, good WP and good TP or OOD detection are necessary and sufficient for good CIL performances. Additionally, TIL is simply WP. Based on the theoretical result, new CIL methods are also designed, which outperform strong baselines in both CIL and TIL settings by a large margin."}}
{"id": "XjvzjjwVFAY", "cdate": 1649649017919, "mdate": null, "content": {"title": "Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks", "abstract": "Existing research on continual learning of a sequence of tasks focused on dealing\nwith catastrophic forgetting, where the tasks are assumed to be dissimilar and have\nlittle shared knowledge. Some work has also been done to transfer previously\nlearned knowledge to the new task when the tasks are similar and have shared\nknowledge. To the best of our knowledge, no technique has been proposed to learn a\nsequence of mixed similar and dissimilar tasks that can deal with forgetting and also\ntransfer knowledge forward and backward. This paper proposes such a technique\nto learn both types of tasks in the same network. For dissimilar tasks, the algorithm\nfocuses on dealing with forgetting, and for similar tasks, the algorithm focuses on\nselectively transferring the knowledge learned from some similar previous tasks to\nimprove the new task learning. Additionally, the algorithm automatically detects\nwhether a new task is similar to any previous tasks. Empirical evaluation using\nsequences of mixed tasks demonstrates the effectiveness of the proposed model."}}
{"id": "UshiGzocXZO", "cdate": 1649648815353, "mdate": null, "content": {"title": " Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally with the goal\nof achieving two main objectives: overcoming catastrophic forgetting (CF) and\nencouraging knowledge transfer (KT) across tasks. However, most existing techniques focus only on overcoming CF and have no mechanism to encourage KT,\nand thus do not do well in KT. Although several papers have tried to deal with\nboth CF and KT, our experiments show that they suffer from serious CF when\nthe tasks do not have much shared knowledge. Another observation is that most\ncurrent CL methods do not use pre-trained models, but it has been shown that such\nmodels can significantly improve the end task performance. For example, in natural\nlanguage processing, fine-tuning a BERT-like pre-trained language model is one of\nthe most effective approaches. However, for CL, this approach suffers from serious\nCF. An interesting question is how to make the best use of pre-trained models for\nCL. This paper proposes a novel model called CTR to solve these problems. Our\nexperimental results demonstrate the effectiveness of CTR."}}
{"id": "nMo44IjBHX5", "cdate": 1632875687601, "mdate": null, "content": {"title": "Continual Learning Using Pseudo-Replay via Latent Space Sampling", "abstract": "This paper investigates continual learning in the setting of class-incremental learning (CIL). Although numerous techniques have been proposed, CIL remains to be a highly challenging problem due to catastrophic forgetting (CF). However, so far few existing techniques have made use of pre-trained image feature extractors. In this paper, we propose to use a recently reported strong pre-trained feature extractor called CLIP and also propose a novel and yet simple pseudo-replay method to deal with CF. The proposed method is called PLS. Unlike the popular pseudo-replay approach that builds data generators to generate pseudo previous task data, PLS works in the latent space by sampling pseudo feature representations of previous tasks from the last layer of the pre-trained feature extractor. PLS is not only simple and efficient but also does not invade data privacy due to the fact that it works in the latent feature space. Experimental results show that the proposed method PLS outperforms state-of-the-art baselines by a large margin, where both PLS and the baselines leverage the CLIP pre-trained image feature extractor."}}
{"id": "0kwQV5SkHWW", "cdate": 1632875633971, "mdate": null, "content": {"title": "Partially Relaxed Masks for Lightweight Knowledge Transfer without Forgetting in Continual Learning", "abstract": "The existing research on continual learning (CL) has focused mainly on preventing catastrophic forgetting. In the task-incremental learning setting of CL, several approaches have achieved excellent results, with almost no forgetting. The goal of this work is to endow such systems with the additional ability to transfer knowledge among tasks when the tasks are similar and have shared knowledge to achieve higher accuracy. Since the existing system HAT is one of most effective task-incremental learning algorithms, this paper extends HAT with the aim of both objectives, i.e., overcoming catastrophic forgetting and transferring knowledge among tasks without introducing additional mechanisms into the architecture of HAT. The current study finds that task similarity, which indicates knowledge sharing and transfer, can be computed via the clustering of task embeddings optimized by HAT. Thus, we propose a new approach, named \u201cpartially relaxed masks\u201d (PRM), to exploit HAT\u2019s masks to not only keep some parameters from being modified in learning subsequent tasks as much as possible to prevent forgetting but also enable remaining parameters to be updated to facilitate knowledge transfer. Extensive experiments demonstrate that PRM performs competitively compared with the latest baselines while also requiring much less computation time."}}
{"id": "RJ7XFI15Q8f", "cdate": 1621630268419, "mdate": null, "content": {"title": "Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally with the goal of achieving two main objectives: overcoming catastrophic forgetting (CF) and encouraging knowledge transfer (KT) across tasks. However, most existing techniques focus only on overcoming CF and have no mechanism to encourage KT, and thus do not do well in KT. Although several papers have tried to deal with both CF and KT, our experiments show that they suffer from serious CF when the tasks do not have much shared knowledge. Another observation is that most current CL methods do not use pre-trained models, but it has been shown that such models can significantly improve the end task performance. For example, in natural language processing, fine-tuning a BERT-like pre-trained language model is one of the most effective approaches. However, for CL, this approach suffers from serious CF. An interesting question is how to make the best use of pre-trained models for CL. This paper proposes a novel model called CTR to solve these problems. Our experimental results demonstrate the effectiveness of CTR"}}
