{"id": "_IyCIML6Hm", "cdate": 1697445991852, "mdate": 1697445991852, "content": {"title": "Misinfo Reaction Frames: Reasoning about Readers\u2019 Reactions to News Headlines", "abstract": "Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer\u2019s intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news. We propose Misinfo Reaction Frames (MRF), a pragmatic formalism for modeling how readers might react to a news headline. In contrast to categorical schema, our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced dataset of reactions to over 25k news headlines focusing on global crises: the Covid-19 pandemic, climate change, and cancer. Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers\u2019 reactions to previously unseen news headlines. Additionally, our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation. Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation."}}
{"id": "yU1YZdqs571", "cdate": 1697445887092, "mdate": 1697445887092, "content": {"title": "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts", "abstract": "Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo\u2019s rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate."}}
{"id": "QwBjrxkjMRk", "cdate": 1687827610336, "mdate": 1687827610336, "content": {"title": "Towards Countering Essentialism through Social Bias Reasoning", "abstract": "Essentialist beliefs (i.e., believing that members of the same group are fundamentally\nalike) play a central role in social stereotypes\nand can lead to harm when left unchallenged.\nIn our work, we conduct exploratory studies\ninto the task of countering essentialist beliefs\n(e.g., \u201cliberals are stupid\u201d). Drawing on prior\nwork from psychology and NLP, we construct\nfive types of counterstatements and conduct\nhuman studies on the effectiveness of these different strategies. Our studies also investigate\nthe role in choosing a counterstatement of the\nlevel of explicitness with which an essentialist belief is conveyed. We find that statements\nthat broaden the scope of a stereotype (e.g., to\nother groups, as in \u201cconservatives can also be\nstupid\u201d) are the most popular countering strategy. We conclude with a discussion of challenges and open questions for future work in\nthis area (e.g., improving factuality, studying\ncommunity-specific variation) and we emphasize the importance of work at the intersection\nof NLP and psychology."}}
{"id": "uP9RiC4uVcR", "cdate": 1652737294696, "mdate": null, "content": {"title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment", "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind \u2014 the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of moral exception question answering (MoralExceptQA) of cases that involve potentially permissible moral exceptions \u2013 inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MoralCoT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MoralCoT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using MoralExceptQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT."}}
{"id": "UyGMP4r6qKe", "cdate": 1640995200000, "mdate": 1680542917702, "content": {"title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs", "abstract": ""}}
{"id": "TIzyBcwXdvv", "cdate": 1640995200000, "mdate": 1680542917703, "content": {"title": "Aligning to Social Norms and Values in Interactive Narratives", "abstract": ""}}
{"id": "Fq77m9QAQ2", "cdate": 1640995200000, "mdate": 1676869353615, "content": {"title": "ProsocialDialog: A Prosocial Backbone for Conversational Agents", "abstract": ""}}
{"id": "Cy-p22p_q22", "cdate": 1640995200000, "mdate": 1671116282833, "content": {"title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection", "abstract": "Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "5cUQL6uMBA", "cdate": 1640995200000, "mdate": 1680542917409, "content": {"title": "Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines", "abstract": ""}}
{"id": "3ap4f9nYyC", "cdate": 1640995200000, "mdate": 1680542917590, "content": {"title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection", "abstract": ""}}
