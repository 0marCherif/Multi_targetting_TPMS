{"id": "s-wINAnFBh", "cdate": 1672531200000, "mdate": 1681585771656, "content": {"title": "How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech", "abstract": ""}}
{"id": "VZoL8w_ROHi", "cdate": 1640995200000, "mdate": 1681585771650, "content": {"title": "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems", "abstract": ""}}
{"id": "FEC5aZB4ppw", "cdate": 1640995200000, "mdate": 1681585771656, "content": {"title": "Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages", "abstract": ""}}
{"id": "POjgSwBOP0K", "cdate": 1609459200000, "mdate": 1681585771656, "content": {"title": "How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN", "abstract": ""}}
{"id": "LVYILBZ6QYW", "cdate": 1597435625200, "mdate": null, "content": {"title": "Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks", "abstract": "Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure."}}
{"id": "w62RmMbQtmi", "cdate": 1597435536816, "mdate": null, "content": {"title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference", "abstract": "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area"}}
{"id": "x-GsIdvYxoy", "cdate": 1577836800000, "mdate": 1637077443928, "content": {"title": "Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks", "abstract": "Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure."}}
{"id": "dTiRwQWDJ8q", "cdate": 1577836800000, "mdate": 1637077442825, "content": {"title": "Discovering the Compositional Structure of Vector Representations with Role Learning Networks", "abstract": "Paul Soulos, R. Thomas McCoy, Tal Linzen, Paul Smolensky. Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 2020."}}
{"id": "ITmhi7Hnxbc", "cdate": 1577836800000, "mdate": 1637077443547, "content": {"title": "BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance", "abstract": "R. Thomas McCoy, Junghyun Min, Tal Linzen. Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 2020."}}
{"id": "E4qAe5iDnB", "cdate": 1577836800000, "mdate": 1681585771700, "content": {"title": "Universal linguistic inductive biases via meta-learning", "abstract": ""}}
