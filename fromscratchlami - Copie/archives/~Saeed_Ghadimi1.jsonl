{"id": "72QoIYIamQ", "cdate": 1676827096645, "mdate": null, "content": {"title": "A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization", "abstract": "We focus on decentralized stochastic non-convex optimization, where $n$ agents work together to optimize a composite objective function which is a sum of a smooth term and a non-smooth convex term. To solve this problem, we propose two single-time scale algorithms: Prox-DASA and Prox-DASA-GT. These algorithms can find $\\epsilon$-stationary points in $\\mathcal{O}(n^{-1}\\epsilon^{-2})$ iterations using constant batch sizes (i.e., $\\mathcal{O}(1)$). Unlike prior work, our algorithms achieve comparable complexity without requiring large batch sizes, more complex per-iteration operations (such as double loops), or stronger assumptions. Our theoretical findings are supported by extensive numerical experiments, which demonstrate the superiority of our algorithms over previous approaches. Our code is available at https://github.com/xuxingc/ProxDASA."}}
{"id": "xqyDqMojMfC", "cdate": 1652737649443, "mdate": null, "content": {"title": "Constrained Stochastic Nonconvex Optimization with State-dependent Markov Data", "abstract": "We study stochastic optimization algorithms for constrained nonconvex stochastic optimization problems with Markovian data. In particular, we focus on the case when the transition kernel of the Markov chain is state-dependent. Such stochastic optimization problems arise in various machine learning problems including strategic classification and reinforcement learning. For this problem, we study both projection-based and projection-free algorithms. In both cases, we establish that the number of calls to the stochastic first-order oracle to obtain an appropriately defined $\\epsilon$-stationary point is of the order $\\mathcal{O}(1/\\epsilon^{2.5})$. In the projection-free setting we additionally establish that the number of calls to the linear minimization oracle is of order $\\mathcal{O}(1/\\epsilon^{5.5})$. We also empirically demonstrate the performance of our algorithm on the problem of strategic classification with neural networks."}}
{"id": "b-SNWfqkZc", "cdate": 1652737602443, "mdate": null, "content": {"title": "A Projection-free Algorithm for Constrained Stochastic Multi-level Composition Optimization", "abstract": "We propose a projection-free conditional gradient-type algorithm for smooth stochastic multi-level composition optimization, where the objective function is a nested composition of $T$ functions and the constraint set is a closed convex set. Our algorithm assumes access to noisy evaluations of the functions and their gradients, through a stochastic first-order oracle satisfying certain standard unbiasedness and second-moment assumptions. We show that the number of calls to the stochastic first-order oracle and the linear-minimization oracle required by the proposed algorithm, to obtain an $\\epsilon$-stationary solution, are of order $\\mathcal{O}_T(\\epsilon^{-2})$ and $\\mathcal{O}_T(\\epsilon^{-3})$ respectively, where $\\mathcal{O}_T$ hides constants in $T$. Notably, the dependence of these complexity bounds on $\\epsilon$ and $T$ are separate in the sense that changing one does not impact the dependence of the bounds on the other. For the case of $T=1$, we also provide a high-probability convergence result that depends poly-logarithmically on the inverse confidence level. Moreover, our algorithm is parameter-free and does not require any (increasing) order of mini-batches to converge unlike the common practice in the analysis of stochastic conditional gradient-type algorithms."}}
{"id": "r1ESLw-ObB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates", "abstract": "In this paper, we propose and analyze zeroth-order stochastic approximation algorithms for nonconvex and convex optimization. Specifically, we propose generalizations of the conditional gradient algorithm achieving rates similar to the standard stochastic gradient algorithm using only zeroth-order information. Furthermore, under a structural sparsity assumption, we first illustrate an implicit regularization phenomenon where the standard stochastic gradient algorithm with zeroth-order information adapts to the sparsity of the problem at hand by just varying the step-size. Next, we propose a truncated stochastic gradient algorithm with zeroth-order information, whose rate of convergence depends only poly-logarithmically on the dimensionality."}}
