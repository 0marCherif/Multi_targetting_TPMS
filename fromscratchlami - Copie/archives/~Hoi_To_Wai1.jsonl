{"id": "mwIPkVDeFg", "cdate": 1652737820202, "mdate": null, "content": {"title": "Distributed Optimization for Overparameterized Problems: Achieving Optimal Dimension Independent Communication Complexity", "abstract": "Decentralized optimization are playing an important role in applications such as training large machine learning models, among others. Despite its superior practical performance, there has been some lack of fundamental understanding about its theoretical properties. In this work, we address the following open research question: To train an overparameterized model over a set of distributed nodes, what is the {\\it minimum} communication overhead (in terms of the bits got exchanged) that the system needs to sustain, while still achieving (near) zero training loss? We show that for a class of overparameterized models where the number of parameters $D$ is much larger than the total data samples $N$, the best possible communication complexity is ${\\Omega}(N)$, which is independent of the problem dimension $D$. Further, for a few specific overparameterized models (i.e., the linear regression, and certain multi-layer neural network with one wide layer), we develop a set of algorithms which uses certain linear compression followed by adaptive quantization, and show that they achieve dimension independent, and sometimes near optimal, communication complexity. To our knowledge, this is the first time that dimension independent communication complexity has been shown for distributed optimization."}}
{"id": "ufRSbXtgbOo", "cdate": 1652737684924, "mdate": null, "content": {"title": "Multi-agent Performative Prediction with Greedy Deployment and Consensus Seeking Agents", "abstract": "We consider a scenario where multiple agents are learning a common decision vector from data which can be influenced by the agents\u2019 decisions. This leads to the problem of multi-agent performative prediction (Multi-PfD). In this paper, we formulate Multi-PfD as a decentralized optimization problem that minimizes a sum of loss functions, where each loss function is based on a distribution influenced by the local decision vector. We first prove the necessary and sufficient condition for the Multi-PfD problem to admit a unique multi-agent performative stable (Multi-PS) solution. We show that enforcing consensus leads to a laxer condition for existence of Multi-PS solution with respect to the distributions\u2019 sensitivities, compared to the single agent case. Then, we study a decentralized extension to \u00a0the greedy deployment scheme [Mendler-D\u00fcnner et al., 2020], called the DSGD-GD  \u00a0scheme. We show that DSGD-GD converges to the Multi-PS solution and analyze its non asymptotic convergence rate. Numerical results validate our analysis. "}}
{"id": "0pdLvHwh-L", "cdate": 1652737680878, "mdate": null, "content": {"title": "Inducing Equilibria via Incentives: Simultaneous Design-and-Play Ensures Global Convergence", "abstract": "To regulate a social system comprised of self-interested agents, economic incentives are often required to induce a desirable outcome. This incentive design problem naturally possesses a bilevel structure, in which a designer modifies the payoffs of the agents with incentives while anticipating the response of the agents, who play a non-cooperative game that converges to an equilibrium. The existing bilevel optimization algorithms raise a dilemma when applied to this problem: anticipating how incentives affect the agents at equilibrium requires solving the equilibrium problem repeatedly, which is computationally inefficient; bypassing the time-consuming step of equilibrium-finding can reduce the computational cost, but may lead the designer to a sub-optimal solution. To address such a dilemma, we propose a method that tackles the designer\u2019s and agents\u2019 problems simultaneously in a single loop.  Specifically, at each iteration, both the designer and the agents only move one step. Nevertheless, we allow the designer to gradually learn the overall influence of the incentives on the agents, which guarantees optimality after convergence. The convergence rate of the proposed scheme is also established for a broad class of games."}}
{"id": "sTOO6vvXrZL", "cdate": 1640995200000, "mdate": 1653668433303, "content": {"title": "Minimization by Incremental Stochastic Surrogate Optimization for Large Scale Nonconvex Problems", "abstract": "Many constrained, nonconvex and nonsmooth optimization problems can be tackled using the majorization-minimization (MM) method which alternates between constructing a surrogate func- tion which upp..."}}
{"id": "orSUC73ijh6", "cdate": 1640995200000, "mdate": 1653668433269, "content": {"title": "Community Inference From Partially Observed Graph Signals: Algorithms and Analysis", "abstract": "This paper considers community inference methods for finding communities on a graph. We treat the setting where the edges are not fully observed. Instead, inference is based on partially observed filtered graph signals where observations from some nodes are missing. Under this setup, we treat two related tasks: <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\mathsf{A}$</tex-math></inline-formula> ) <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">blind</i> inference which recovers the inherited communities on the sub-graph; <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\mathsf{B}$</tex-math></inline-formula> ) <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">semi-blind</i> inference which recovers communities on the full graph with additional partial topology information. For task <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\mathsf{A}$</tex-math></inline-formula> , we suggest a spectral method which analyzes the principal components of the data covariance matrix. We prove that it succeeds in finding the \u2018true\u2019 communities if the graph filter is low-pass and the nodes are uniformly sampled. For task <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\mathsf{B}$</tex-math></inline-formula> , we propose a method using spectral interpolation with a Nystr\u00f6m extension. The latter approach is proven to succeed in finding the \u2018true\u2019 communities for modular graphs and low-pass graph filters. Numerical experiments on synthetic and real data corroborate our results."}}
{"id": "aOkZFBA03Sm", "cdate": 1640995200000, "mdate": 1653668433292, "content": {"title": "State Dependent Performative Prediction with Stochastic Approximation", "abstract": "This paper studies the performative prediction problem which optimizes a stochastic loss function with data distribution that depends on the decision variable. We consider a setting where the agent(s) provides samples adapted to both the learner\u2019s and agent\u2019s previous states. The samples are then used by the learner to update his/her state to optimize a loss function. Such closed loop update dynamics is studied as a state dependent stochastic approximation (SA) algorithm, which is shown to find a fixed point known as the performative stable solution. Our setting captures the unforgetful nature and reliance on past experiences of agents. Our contributions are three-fold. First, we present a framework for state dependent performative prediction with biased stochastic gradients driven by a controlled Markov chain whose transition probability depends on the learner\u2019s state. Second, we present a new finite-time performance analysis of the SA algorithm. We show that the expected squared distance to the performative stable solution decreases as O(1/k), where k is the iteration number. Third, numerical experiments verify our findings."}}
{"id": "DAjqe4cWNLq", "cdate": 1640995200000, "mdate": 1653668433310, "content": {"title": "DoCoM-SGT: Doubly Compressed Momentum-assisted Stochastic Gradient Tracking Algorithm for Communication Efficient Decentralized Learning", "abstract": "This paper proposes the Doubly Compressed Momentum-assisted Stochastic Gradient Tracking algorithm (DoCoM-SGT) for communication efficient decentralized learning. DoCoM-SGT utilizes two compression steps per communication round as the algorithm tracks simultaneously the averaged iterate and stochastic gradient. Furthermore, DoCoM-SGT incorporates a momentum based technique for reducing variances in the gradient estimates. We show that DoCoM-SGT finds a solution $\\bar{\\theta}$ in $T$ iterations satisfying $\\mathbb{E} [ \\| \\nabla f(\\bar{\\theta}) \\|^2 ] = {\\cal O}( 1 / T^{2/3} )$ for non-convex objective functions; and we provide competitive convergence rate guarantees for other function classes. Numerical experiments on synthetic and real datasets validate the efficacy of our algorithm."}}
{"id": "oj2yn1Q4Ett", "cdate": 1632875693200, "mdate": null, "content": {"title": "Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach", "abstract": "This work develops a novel framework for communication-efficient distributed learning where the models to be learned are overparameterized. We focus on a class of kernel learning problems (which includes the popular neural tangent kernel (NTK) learning as a special case) and propose a novel {\\it multi-agent kernel approximation} technique that allows the agents to distributedly estimate the full kernel function, and subsequently perform decentralized optimization, without directly exchanging any local data or parameters. The proposed framework is a significant departure from the classical consensus-based approaches, because the agents do not exchange problem parameters, and no consensus is required. We analyze the optimization and the generalization performance of the proposed framework for the $\\ell_2$ loss. We show that with $M$ agents and $N$ total samples when certain generalized inner-product kernels (resp. the random features kernel) are used, each agent needs to communicate $\\mathcal{O}\\big({N^2}/{M}\\big)$ bits (resp. $\\mathcal{O}\\big(N \\sqrt{N}/M \\big)$ real values) to achieve minimax optimal generalization performance. We validate the theoretical results on 90 UCI benchmarking datasets (with average data size $N \\approx 1000$) and show that each agent needs to share a total of $200N/M$ bits (resp. $3N/M$ real values) to closely match the performance of the centralized algorithms, and these numbers are independent of parameter and feature dimensions. "}}
{"id": "HjFtRc83eBB", "cdate": 1621630150475, "mdate": null, "content": {"title": "A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum", "abstract": "This paper proposes a new algorithm -- the  \\underline{S}ingle-timescale Do\\underline{u}ble-momentum \\underline{St}ochastic \\underline{A}pprox\\underline{i}matio\\underline{n} (SUSTAIN) -- for tackling stochastic unconstrained bilevel optimization problems. We focus on bilevel problems where the lower level subproblem is strongly-convex and the upper level objective function is smooth. Unlike prior works which rely on \\emph{two-timescale} or \\emph{double loop} techniques, we design a stochastic momentum-assisted gradient estimator for both the upper and lower level updates. The latter allows us to control the error in the stochastic gradient updates due to inaccurate solution to both subproblems. If the upper objective function is smooth but possibly non-convex, we show that {SUSTAIN}~requires $O(\\epsilon^{-3/2})$  iterations (each using $O(1)$ samples) to find an $\\epsilon$-stationary solution. The $\\epsilon$-stationary solution is defined as the point whose squared norm of the gradient of the outer function is less than or equal to $\\epsilon$.  The total number of stochastic gradient samples required for the upper and lower level objective functions matches the best-known complexity for single-level stochastic gradient algorithms. We also analyze the case when the upper level objective function is strongly-convex. "}}
{"id": "7nWS_1Gkqt", "cdate": 1621630009223, "mdate": null, "content": {"title": "Tight High Probability Bounds for Linear Stochastic Approximation with Fixed Stepsize", "abstract": "This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. This family of methods arises in many machine learning tasks and is used to obtain approximate solutions of a linear system $\\bar{A}\\theta = \\bar{b}$ for which $\\bar{A}$ and $\\bar{b}$ can only be accessed through random estimates $\\{({\\bf A}_n, {\\bf b}_n): n \\in \\mathbb{N}^*\\}$.  Our analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. We derive high probability bounds on the performance of LSA under weaker conditions on the sequence $\\{({\\bf A}_n, {\\bf b}_n): n \\in \\mathbb{N}^*\\}$ than previous works. However, in contrast, we establish polynomial concentration bounds with order depending on the stepsize. We show that our conclusions cannot be improved  without additional assumptions on the sequence of random matrices $\\{{\\bf A}_n: n \\in \\mathbb{N}^*\\}$, and in particular that no Gaussian or exponential high probability bounds can hold.  Finally, we pay a particular attention to establishing  bounds with sharp order with respect to the number of iterations and the stepsize and  whose leading terms contain the covariance matrices appearing in the central limit theorems."}}
