{"id": "JX1OCjfABRj", "cdate": 1663850054087, "mdate": null, "content": {"title": "Self-Adaptive Perturbation Radii for Adversarial Training", "abstract": "Adversarial training has been shown to be the most popular and effective  technique to protect models from imperceptible adversarial samples. Despite its success, it also accompanies the significant performance degeneration to clean data. To achieve a good performance on both clean and adversarial samples, the main effort  is searching for an adaptive perturbation radius for each training sample, which essentially suffers from a  conflict between exact searching  and  computational overhead. To address this conflict, in this paper, firstly we show the superiority of adaptive perturbation radii intuitively and theoretically regarding the   accuracy and robustness respectively. Then we propose our novel self-adaptive adjustment framework for perturbation radii without tedious searching. We also discuss this framework on both deep neural networks (DNNs) and kernel support vector machines (SVMs).  Finally, extensive experimental results show that our framework can improve not only natural generalization performance but also adversarial robustness. It is also competitive with existing searching strategies in terms of running time."}}
{"id": "2ZNPedOfwB", "cdate": 1652737490460, "mdate": null, "content": {"title": "Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity", "abstract": "$\\ell_0$ constrained optimization is prevalent in machine learning, particularly for high-dimensional problems, because it is a fundamental approach to achieve sparse learning. Hard-thresholding gradient descent is a dominant technique to solve this problem. However, first-order gradients of the objective function may be either unavailable or expensive to calculate in a lot of real-world problems, where zeroth-order (ZO) gradients could be a good surrogate. Unfortunately, whether ZO gradients can work with the hard-thresholding operator is still an unsolved problem.\nTo solve this puzzle, in this paper, we focus on the $\\ell_0$ constrained black-box stochastic optimization problems, and propose a new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm with  a general ZO gradient estimator powered by a novel random support sampling. We provide the convergence analysis of SZOHT under standard assumptions.   Importantly, we   reveal a conflict between  the deviation of  ZO estimators and  the expansivity of the hard-thresholding operator,  and provide a theoretical   minimal value of the number of random directions in ZO gradients. In addition,  we find that the query complexity of SZOHT is independent or weakly dependent on the dimensionality under different settings.  Finally, we illustrate the utility of our method on a portfolio optimization problem as well as black-box adversarial attacks."}}
{"id": "mvq4blDaCkN", "cdate": 1632875607645, "mdate": null, "content": {"title": "Efficient Semi-Supervised Adversarial Training without Guessing Labels", "abstract": "Adversarial training has been proved to be  the most effective defensive strategy to protect models from adversarial attacks.  In the practical application scenario of adversarial training, we face not only labeled data, but also an enormous amount of unlabeled data. However, existing adversarial training methods are naturally targeting  supervised learning problems.  To adapt to  semi-supervised learning problems, they need to estimate labels for unlabeled data in advance,  which inevitably degenerates the performance of the learned model due to the bias on the estimation of labels for unlabeled data. \nTo mitigate this degeneration, in this paper, we propose a new semi-supervised  adversarial training framework via maximizing  AUCs which is also a minimax problem but  treats the unlabeled samples as both positive and negative ones, so that we do not need to guess the labels for unlabeled data.  Unsurprisingly, the minimax problem can be solved via the traditional adversarial training algorithm by extending singly stochastic gradients to triply stochastic gradients, to adapt to the three (i.e. positive, negative, and unlabeled) data sources. To further accelerate the training procedure, we transform the minimax adversarial training problem into an equivalent minimization one based on the kernel perspective. For the minimization problem, we discuss scalable and efficient algorithms not only for deep neural networks  but also for kernel support vector machines.  Extensive experimental results show that our algorithms not only achieve better generalization performance against various adversarial attacks,  but also enjoy efficiency and scalability when considered from the kernel perspective."}}
