{"id": "yZB6nD4gNv", "cdate": 1640995200000, "mdate": 1668665343834, "content": {"title": "Rethinking Open-World Object Detection in Autonomous Driving Scenarios", "abstract": "Existing object detection models have been demonstrated to successfully discriminate and localize the predefined object categories under the seen or similar situations. However, the open-world object detection as required by autonomous driving perception systems refers to recognizing unseen objects under various scenarios. On the one hand, the knowledge gap between seen and unseen object categories poses extreme challenges for models trained with supervision only from the seen object categories. On the other hand, the domain differences across different scenarios also cause an additional urge to take the domain gap into consideration by aligning the sample or label distribution. Aimed at resolving these two challenges simultaneously, we firstly design a pre-training model to formulate the mappings between visual images and semantic embeddings from the extra annotations as guidance to link the seen and unseen object categories through a self-supervised manner. Within this formulation, the domain adaptation is then utilized for extracting the domain-agnostic feature representations and alleviating the misdetection of unseen objects caused by the domain appearance changes. As a result, the more realistic and practical open-world object detection problem is visited and resolved by our novel formulation, which could detect the unseen categories from unseen domains without any bounding box annotations while there is no obvious performance drop in detecting the seen categories. We are the first to formulate a unified model for open-world task and establish a new state-of-the-art performance for this challenge."}}
{"id": "u-lBxu0r3FF", "cdate": 1640995200000, "mdate": 1668665343471, "content": {"title": "Learning Cross-Modal Common Representations by Private-Shared Subspaces Separation", "abstract": "Due to the inconsistent distributions and representations of different modalities (e.g., images and texts), it is very challenging to correlate such heterogeneous data. A standard solution is to construct one common subspace, where the common representations of different modalities are generated to bridge the heterogeneity gap. Existing methods based on common representation learning mostly adopt a less effective two-stage paradigm: first, generating separate representations for each modality by exploiting the modality-specific properties as the complementary information, and then capturing the cross-modal correlation in the separate representations for common representation learning. Moreover, these methods usually neglect that there may exist interference in the modality-specific properties, that is, the unrelated objects and background regions in images or the noisy words and incorrect sentences in the text. In this article, we hypothesize that explicitly modeling the interference within each modality can improve the quality of common representation learning. To this end, we propose a novel model private\u2013shared subspaces separation (P3S) to explicitly learn different representations that are partitioned into two kinds of subspaces: 1) the common representations that capture the cross-modal correlation in a shared subspace and 2) the private representations that model the interference within each modality in two private subspaces. By employing the orthogonality constraints between the shared subspace and the private subspaces during the one-stage joint learning procedure, our model is able to learn more effective common representations for different modalities in the shared subspace by fully excluding the interference within each modality. Extensive experiments conducted on cross-modal retrieval verify the advantages of our P3S method compared with 15 state-of-the-art methods on four widely used cross-modal datasets."}}
{"id": "tl9waf7y9b", "cdate": 1640995200000, "mdate": 1668665343489, "content": {"title": "Universal Weighting Metric Learning for Cross-Modal Retrieval", "abstract": "Cross-modal retrieval has recently attracted growing attention, which aims to match instances captured from different modalities. The performance of cross-modal retrieval methods heavily relies on the capability of metric learning to mine and weight the informative pairs. While various metric learning methods have been developed for unimodal retrieval tasks, the cross-modal retrieval tasks, however, have not been explored to its fullest extent. In this paper, we develop a universal weighting metric learning framework for cross-modal retrieval, which can effectively sample informative pairs and assign proper weight values to them based on their similarity scores so that different pairs favor different penalty strength. Based on this framework, we introduce two types of polynomial loss for cross-modal retrieval, self-similarity polynomial loss and relative-similarity polynomial loss. The former provides a polynomial function to associate the weight values with self-similarity scores, and the latter defines a polynomial function to associate the weight values with relative-similarity scores. Both self and relative-similarity polynomial loss can be freely applied to off-the-shelf methods and further improve their retrieval performance. Extensive experiments on two image-text retrieval datasets, three video-text retrieval datasets and one fine-grained image retrieval dataset demonstrate that our proposed method can achieve a noticeable boost in retrieval performance."}}
{"id": "pIldUDGp1t", "cdate": 1640995200000, "mdate": 1668665343927, "content": {"title": "Med-BERT: A Pretraining Framework for Medical Records Named Entity Recognition", "abstract": "A large amount of data is generated every day with the development of Internet medical care, which is of great significance for the clinical decision support system and medical real-world research. Medical records named entity recognition (NER) is important on the aforementioned research topics under the premise of protecting patients\u2019 private information. In this article, we propose a medical dictionary enhanced bidirectional encoder representations from transformers (BERT), dubbed Med-BERT, to achieve better representations of long medical entities. On Med-BERT, we propose a span flat-lattice transformer (Span-FLAT) method on medical records NER, and the entity types include private information such as names and addresses, as well as medical information such as patient symptoms, signs, and diseases. Experimental results on two benchmark medical datasets show the effectiveness of Med-BERT, and the proposed Med-BERT-based Span-FLAT method remarkably outperforms the state-of-the-art methods on medical NER task."}}
{"id": "l4QIMJGO8DU", "cdate": 1640995200000, "mdate": 1668665344227, "content": {"title": "Multimodal Disentanglement Variational AutoEncoders for Zero-Shot Cross-Modal Retrieval", "abstract": "Zero-Shot Cross-Modal Retrieval (ZS-CMR) has recently drawn increasing attention as it focuses on a practical retrieval scenario, i.e., the multimodal test set consists of unseen classes that are disjoint with seen classes in the training set. The recently proposed methods typically adopt the generative model as the main framework to learn a joint latent embedding space to alleviate the modality gap. Generally, these methods largely rely on auxiliary semantic embeddings for knowledge transfer across classes and unconsciously neglect the effect of the data reconstruction manner in the adopted generative model. To address this issue, we propose a novel ZS-CMR model termed Multimodal Disentanglement Variational AutoEncoders (MDVAE), which consists of two coupled disentanglement variational autoencoders (DVAEs) and a fusion-exchange VAE (FVAE). Specifically, DVAE is developed to disentangle the original representations of each modality into modality-invariant and modality-specific features. FVAE is designed to fuse and exchange information of multimodal data by the reconstruction and alignment process without pre-extracted semantic embeddings. Moreover, an advanced counter-intuitive cross-reconstruction scheme is further proposed to enhance the informativeness and generalizability of the modality-invariant features for more effective knowledge transfer. The comprehensive experiments on four image-text retrieval and two image-sketch retrieval datasets consistently demonstrate that our method establishes the new state-of-the-art performance."}}
{"id": "kjm23g4Lmcw", "cdate": 1640995200000, "mdate": 1668665343471, "content": {"title": "Cognitive Memory-Guided AutoEncoder for Effective Intrusion Detection in Internet of Things", "abstract": "With the development of the Internet of Things (IoT) technology, intrusion detection has become a key technology that provides solid protection for IoT devices from network intrusion. At present, artificial intelligence technologies have been widely used in the intrusion detection task in previous methods. However, unknown attacks may also occur with the development of the network and the attack samples are difficult to collect, resulting in unbalanced sample categories. In this case, the previous intrusion detection methods have the problem of high false positive rates and low detection accuracy, which restricts the application of these methods in a real situation. In this article, we propose a novel method based on deep neural networks to tackle the intrusion detection task, which is termed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Cognitive Memory-guided AutoEncoder</i> (CMAE). The CMAE method leverages a memory module to enhance the ability to store normal feature patterns while inheriting the advantages of autoencoder. Therefore, it is robust to the imbalanced samples. Besides, using the reconstruction error as an evaluation criterion to detect attacks effectively detects unknown attacks. To obtain superior intrusion detection performance, we propose feature reconstruction loss and feature sparsity loss to constrain the proposed memory module, promoting the discriminative of memory items and the ability of representation for normal data. Compared to previous state-of-the-art methods, sufficient experimental results reveal that the proposed CMAE method achieves excellent performance and effectiveness for intrusion detection."}}
{"id": "fU-X1e-UY40", "cdate": 1640995200000, "mdate": 1668665343471, "content": {"title": "Cross-Modal Dynamic Networks for Video Moment Retrieval With Text Query", "abstract": "Video moment retrieval with text query aims to retrieve the most relevant segment from the whole video based on the given text query. It is a challenging cross-modal alignment task due to the huge gap between visual and linguistic modalities and the noise generated by manual labeling of time segments. Most of the existing works only use language information in the cross-modal fusion stage, neglecting that language information plays an important role in the retrieval stage. Besides, these works roughly compress the visual information in the video clips to reduce the computation cost which loses subtle video information in the long video. In this paper, we propose a novel model termed Cross-modal Dynamic Networks (CDN) which dynamically generates convolution kernel by visual and language features. In the feature extraction stage, we also propose a frame selection module to capture the subtle video information in the video segment. By this approach, the CDN can reduce the impact of the visual noise without significantly increasing the computation cost and leads to a better video moment retrieval result. The experiments on two challenge datasets, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i.e.</i> , Charades-STA and TACoS, show that our proposed CDN method outperforms a bundle of state-of-the-art methods with more accurately retrieved moment video clips. The implementation code and extensive instruction of our proposed CDN method are provided at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/CFM-MSG/Code_CDN</uri> ."}}
{"id": "_tP_L-HJrj", "cdate": 1640995200000, "mdate": 1668665345374, "content": {"title": "Semi-supervised Video Paragraph Grounding with Contrastive Encoder", "abstract": "Video events grounding aims at retrieving the most relevant moments from an untrimmed video in terms of a given natural language query. Most previous works focus on Video Sentence Grounding (VSG), which localizes the moment with a sentence query. Recently, researchers extended this task to Video Paragraph Grounding (VPG) by retrieving multiple events with a paragraph. However, we find the existing VPG methods may not perform well on context modeling and highly rely on video-paragraph annotations. To tackle this problem, we propose a novel VPG method termed Semi-supervised Video-Paragraph TRansformer (SVPTR), which can more effectively exploit contextual information in paragraphs and significantly reduce the dependency on annotated data. Our SVPTR method consists of two key components: (1) a base model VPTR that learns the video-paragraph alignment with contrastive encoders and tackles the lack of sentence-level contextual interactions and (2) a semi-supervised learning framework with multimodal feature perturbations that reduces the requirements of annotated training data. We evaluate our model on three widely-used video grounding datasets, i.e., ActivityNet-Caption, Charades-CD-OOD, and TACoS. The experimental results show that our SVPTR method establishes the new state-of-the-art performance on all datasets. Even under the conditions of fewer annotations, it can also achieve competitive results compared with recent VPG methods."}}
{"id": "_IYIzT73oQ", "cdate": 1640995200000, "mdate": 1668665342025, "content": {"title": "Query-based black-box attack against medical image segmentation model", "abstract": ""}}
{"id": "_7pGePfi1z2", "cdate": 1640995200000, "mdate": 1668665342038, "content": {"title": "Learning discriminative representations via variational self-distillation for cross-view geo-localization", "abstract": ""}}
