{"id": "yM7FkZo-Lp", "cdate": 1682087574768, "mdate": 1682087574768, "content": {"title": " Wyner-Ziv Estimators: Efficient Distributed Mean Estimation with Side-Information", "abstract": "Communication efficient distributed mean estimation is an important primitive that arises in many distributed learning and optimization\nscenarios such as federated learning. Without any probabilistic assumptions on the underlying data, we study the problem of distributed mean estimation where the server has access to side information. We propose Wyner-Ziv estimators, which are communication and computationally efficient and nearoptimal when an upper bound for the distance between the side information and the data is\nknown. In a different direction, when there is no knowledge assumed about the distance between side information and the data, we\npresent an alternative Wyner-Ziv estimator that uses correlated sampling. This latter setting offers universal recovery guarantees, and\nperhaps will be of interest in practice when the number of users is large and keeping track of the distances between the data and the side\ninformation may not be possible."}}
{"id": "h7aSBWbX7S4", "cdate": 1621629869731, "mdate": null, "content": {"title": "Information-constrained optimization: can adaptive processing of gradients help?", "abstract": "We revisit first-order optimization under local information constraints such as local privacy, gradient quantization, and computational constraints limiting access to a few coordinates of the gradient. In this setting, the optimization algorithm is not allowed to directly access the complete output of the gradient oracle, but only gets limited information about it subject to the local information constraints.   We study the role of adaptivity in processing the gradient output to obtain this limited information from it, and obtain tight or nearly tight bounds for both convex and strongly convex optimization when adaptive gradient processing is allowed."}}
