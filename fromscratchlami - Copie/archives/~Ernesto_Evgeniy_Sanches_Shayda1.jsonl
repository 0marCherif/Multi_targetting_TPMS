{"id": "kQgLvIFLyIu", "cdate": 1652737835386, "mdate": null, "content": {"title": "Coreset for Line-Sets Clustering", "abstract": "The input to the {line-sets $k$-median} problem is an integer $k \\geq 1$, and a set $\\mathcal{L} = \\{L_1,\\dots,L_n\\}$\nthat contains $n$ sets of lines in $\\mathbb{R}^d$. The goal is to compute a set $C$ of $k$ centers (points in $\\mathbb{R}^d$) that minimizes the sum $\\sum_{L \\in \\mathcal{L}}\\min_{\\ell\\in L, c\\in C}\\mathrm{dist}(\\ell,c)$ of Euclidean distances from each set to its closest center, where $\\mathrm{dist}(\\ell,c):=\\min_{x\\in \\ell}\\norm{x-c}_2$.\nAn \\emph{$\\varepsilon$-coreset} for this problem is a weighted subset of sets in $\\mathcal{L}$ that approximates this sum up to $1 \\pm \\varepsilon$ multiplicative factor, for every set $C$ of $k$ centers. We prove that \\emph{every} such input set $\\set{L}$ has a small $\\varepsilon$-coreset, and provide the first coreset construction for this problem and its variants. The coreset consists of $O(\\log^2n)$ weighted line-sets from $\\set{L}$, and is constructed in $O(n\\log n)$ time for every fixed $d, k\\geq 1$ and $\\varepsilon \\in (0,1)$. The main technique is based on a novel reduction to a ``fair clustering'' of colored points to colored centers. We then provide a coreset for this coloring problem, which may be of independent interest. Open source code and experiments are also provided."}}
{"id": "GvU4RvMwlGo", "cdate": 1621629786921, "mdate": null, "content": {"title": "Coresets for Decision Trees of Signals", "abstract": "A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix (2D-signal) into $k\\geq 1$ block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classification loss to a given matrix $D$ of $N$ entries (labels) is the sum of squared differences over every label in $D$ and its assigned label by $t$.\nGiven an error parameter $\\varepsilon\\in(0,1)$, a $(k,\\varepsilon)$-coreset $C$ of $D$ is a small summarization that provably approximates this loss to \\emph{every} such tree, up to a multiplicative factor of $1\\pm\\varepsilon$. In particular, the optimal $k$-tree of $C$ is a $(1+\\varepsilon)$-approximation to the optimal $k$-tree of $D$.\n\nWe provide the first algorithm that outputs such a $(k,\\varepsilon)$-coreset for \\emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial in $k\\log(N)/\\varepsilon$, and its construction takes $O(Nk)$ time.\nThis is by forging a link between decision trees from machine learning -- to partition trees in computational geometry. \n\nExperimental results on \\texttt{sklearn} and \\texttt{lightGBM} show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x$10$, while keeping similar accuracy. Full open source code is provided."}}
{"id": "4JHdr4lgpVT", "cdate": 1621629786921, "mdate": null, "content": {"title": "Coresets for Decision Trees of Signals", "abstract": "A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix (2D-signal) into $k\\geq 1$ block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classification loss to a given matrix $D$ of $N$ entries (labels) is the sum of squared differences over every label in $D$ and its assigned label by $t$.\nGiven an error parameter $\\varepsilon\\in(0,1)$, a $(k,\\varepsilon)$-coreset $C$ of $D$ is a small summarization that provably approximates this loss to \\emph{every} such tree, up to a multiplicative factor of $1\\pm\\varepsilon$. In particular, the optimal $k$-tree of $C$ is a $(1+\\varepsilon)$-approximation to the optimal $k$-tree of $D$.\n\nWe provide the first algorithm that outputs such a $(k,\\varepsilon)$-coreset for \\emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial in $k\\log(N)/\\varepsilon$, and its construction takes $O(Nk)$ time.\nThis is by forging a link between decision trees from machine learning -- to partition trees in computational geometry. \n\nExperimental results on \\texttt{sklearn} and \\texttt{lightGBM} show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x$10$, while keeping similar accuracy. Full open source code is provided."}}
