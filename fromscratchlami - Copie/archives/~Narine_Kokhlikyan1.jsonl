{"id": "syZ09IRE6Pl", "cdate": 1683634741368, "mdate": null, "content": {"title": "Error Discovery by Clustering Influence Embeddings", "abstract": "We present a method for identifying groups of test examples\u2014slices\u2014on which\na pre-trained model under-performs, a task now known as slice discovery. We\nformalize coherence, a requirement that erroneous predictions within returned\nslices should be wrong for the same reason, as a key property that a slice discovery\nmethod should satisfy. We then leverage influence functions (Koh & Liang, 2017)\nto derive a new slice discovery method, InfEmbed, which satisfies coherence\nby returning slices whose examples are influenced similarly by the training data.\nInfEmbed is computationally simple, consisting of applying K-Means clustering\nto a novel representation we deem influence embeddings. Empirically, we show\nInfEmbed outperforms current state-of-the-art methods on a slice discovery\nbenchmark, and is effective for model debugging across several case studies."}}
{"id": "Rj8TUzl8nT8", "cdate": 1676472363481, "mdate": null, "content": {"title": "Error Discovery by Clustering Influence Embeddings", "abstract": "We present a method for identifying groups of test examples\u2014slices\u2014on which a pre-trained model under-performs, a task now known as slice discovery. We formalize coherence, a requirement that erroneous predictions within returned slices should be wrong for the same reason, as a key property that a slice discovery method should satisfy. We then leverage influence functions (Koh & Liang, 2017) to derive a new slice discovery method, InfEmbed, which satisfies coherence by returning slices whose examples are influenced similarly by the training data. InfEmbed is computationally simple, consisting of applying K-Means clustering to a novel representation we deem influence embeddings. Empirically, we show InfEmbed outperforms current state-of-the-art methods on a slice discovery benchmark, and is effective for model debugging across several case studies."}}
{"id": "cWmtUcsYC3V", "cdate": 1663850479713, "mdate": null, "content": {"title": "Mind the Pool: Convolutional Neural Networks Can Overfit Input Size", "abstract": "We demonstrate how convolutional neural networks can overfit the input size: The accuracy drops significantly when using certain sizes, compared with favorable ones. This issue is inherent to pooling arithmetic, with standard downsampling layers playing a major role in favoring certain input sizes and skewing the weights accordingly. We present a solution to this problem by depriving these layers from the arithmetic cues they use to overfit the input size. Through various examples, we show how our proposed spatially-balanced pooling improves the generalization of the network to arbitrary input sizes and its robustness to translational shifts."}}
{"id": "8YnDrbx8bnh", "cdate": 1663849928145, "mdate": null, "content": {"title": "Bias Mitigation Framework for Intersectional Subgroups in Neural Networks", "abstract": "We propose a fairness-aware learning framework that mitigates intersectional subgroup bias associated with protected attributes. Prior research has primarily focused on mitigating one kind of bias by incorporating complex fairness-driven constraints into optimization objectives or designing additional layers that focus on specific protected attributes. We introduce a simple and generic bias mitigation framework that prevents models from learning relationships between protected attributes and output variable by reducing mutual information. We demonstrate that our approach is effective in reducing bias with little or no drop in accuracy. We also show that our approach mitigates intersectional bias even when other attributes in the dataset are correlated with protected attributes. Finally, we validate our approach by studying feature interactions between protected and non-protected attributes. We demonstrate that these interactions are significantly reduced when applying our bias mitigation.\n"}}
{"id": "5oF-Z7Uk0tH", "cdate": 1634055188666, "mdate": null, "content": {"title": "Are Convolutional Networks Inherently Foveated?", "abstract": "When convolutional layers apply no padding, central pixels have more ways to contribute to the convolution than peripheral pixels. Such discrepancy grows exponentially with the number of layers, leading to implicit foveation of the input pixels. We show that this discrepancy can persist even when padding is applied. In particular, with the commonly-used zero-padding, foveation effects are significantly reduced but not eliminated. We explore how different aspects of convolution arithmetic impact the extent and magnitude of these effects, and elaborate on which alternative padding techniques can mitigate it. Finally, we compare our findings with foveation in human vision, suggesting that both effects possibly have similar nature and implications."}}
{"id": "0YRkrxe2blh", "cdate": 1632235869473, "mdate": null, "content": {"title": "Debugging the Internals of Convolutional Networks", "abstract": "The filters learned by Convolutional Neural Networks (CNNs) and the feature maps these filters compute are sensitive to convolution arithmetic. Several architectural choices that dictate this arithmetic can result in feature-map artifacts. These artifacts can interfere with the downstream task and impact the accuracy and robustness. We provide a number of visual-debugging means to surface feature-map artifacts and to analyze how they emerge in CNNs. Our means help analyze the impact of these artifacts on the weights learned by the model. Guided by our analysis, model developers can make informed architectural choices that can verifiably mitigate harmful artifacts and improve the model\u2019s accuracy and its shift robustness."}}
{"id": "8ZOCv_0K7BQ", "cdate": 1616081549722, "mdate": null, "content": {"title": "Convolution Can Incur Foveation Effects", "abstract": "This exhibit demonstrates how boundary treatment in convolutional networks can incur foveation effects: Impacted pixels have fewer ways to contribute to the computation than central pixels. Different padding mechanisms can either eliminate or aggravate these effects, which is made obvious by an interactive visualization."}}
{"id": "m1CD7tPubNy", "cdate": 1601308016353, "mdate": null, "content": {"title": "Mind the Pad -- CNNs Can Develop Blind Spots", "abstract": "We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We explore alternative padding methods and propose solutions for analyzing and mitigating spatial bias.\n"}}
{"id": "SJWMwXZ_-S", "cdate": 1356998400000, "mdate": null, "content": {"title": "Measuring the Structural Importance through Rhetorical Structure Index", "abstract": "In this paper, we propose a novel Rhetorical Structure Index (RSI) to measure the structural importance of a word or a phrase. Unlike TF-IDF and other content-driven measurements, RSI identifies words or phrases that are structural cues in an unstructured document. We show structurally motivated features with high RSI values are more useful than content-driven features for applications such as segmenting unstructured lecture transcripts into meaningful segments. Experiments show that using RSI significantly improves the segmentation accuracy compared to TF-IDF, a traditional content-based feature weighting scheme."}}
