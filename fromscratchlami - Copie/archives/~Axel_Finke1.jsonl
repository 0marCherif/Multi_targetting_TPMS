{"id": "5xilKiGFDu-", "cdate": 1696407321047, "mdate": 1696407321047, "content": {"title": "Conditional sequential Monte Carlo in high dimensions", "abstract": "The iterated conditional sequential Monte Carlo (i-CSMC) algorithm from Andrieu, Doucet and Holenstein (J. R. Stat. Soc. Ser. B Stat. Methodol. 72 (2010) 269\u2013342) is an MCMC approach for efficiently sampling from the joint posterior distribution of the T latent states in challenging time-series models, for example, in nonlinear or non-Gaussian state-space models. It is also the main ingredient in particle Gibbs samplers which infer unknown model parameters alongside the latent states. In this work, we first prove that the i-CSMC algorithm suffers from a curse of dimension in the dimension of the states, D: it breaks down unless the number of samples (\u2018particles\u2019), N, proposed by the algorithm grows exponentially with D. Then we present a novel \u2018local\u2019 version of the algorithm which proposes particles using Gaussian random-walk moves that are suitably scaled with D. We prove that this iterated random-walk conditional sequential Monte Carlo (i-RW-CSMC) algorithm avoids the curse of dimension: for arbitrary N, its acceptance rates and expected squared jumping distance converge to nontrivial limits as \nD\n\u2192\n\u221e\n. If \nT\n=\nN\n=\n1\n, our proposed algorithm reduces to a Metropolis\u2013Hastings or Barker\u2019s algorithm with Gaussian random-walk moves and we recover the well-known scaling limits for such algorithms."}}
{"id": "ryg7jhEtPB", "cdate": 1569438875079, "mdate": null, "content": {"title": "On importance-weighted autoencoders", "abstract": "The importance weighted autoencoder (IWAE) (Burda et al., 2016) is a popular variational-inference method which achieves a tighter evidence bound (and hence a lower bias) than standard variational autoencoders by optimising a multi-sample objective, i.e. an objective that is expressible as an integral over $K > 1$ Monte Carlo samples. Unfortunately, IWAE crucially relies on the availability of reparametrisations and even if these exist, the multi-sample objective leads to inference-network gradients which break down as $K$ is increased (Rainforth et al., 2018). This breakdown can only be circumvented by removing high-variance score-function terms, either by heuristically ignoring them (which yields the 'sticking-the-landing' IWAE (IWAE-STL) gradient from Roeder et al. (2017)) or through an identity from Tucker et al. (2019) (which yields the 'doubly-reparametrised' IWAE (IWAE-DREG) gradient). In this work, we argue that directly optimising the proposal distribution in importance sampling as in the reweighted wake-sleep (RWS) algorithm from Bornschein & Bengio (2015) is preferable to optimising IWAE-type multi-sample objectives. To formalise this argument, we introduce an adaptive-importance sampling framework termed adaptive importance sampling for learning (AISLE) which slightly generalises the RWS algorithm. We then show that AISLE admits IWAE-STL and IWAE-DREG (i.e. the IWAE-gradients which avoid breakdown) as special cases."}}
