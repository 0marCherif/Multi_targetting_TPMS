{"id": "m5fU5hJaGnC", "cdate": 1684352921123, "mdate": 1684352921123, "content": {"title": "SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics", "abstract": "We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure, {\\it the leap}, which measures how \u201chierarchical\u201d target functions are. \nFor uniform Boolean or isotropic Gaussian data, our main conjecture is that the time complexity for SGD to learn a function $f$ with low-dimensional support is controlled by its leap, i.e., it is $$\\Tilde \\Theta (d^{\\max(\\mathrm{Leap}(f),2)}) \\,\\,.$$   \nWe prove a version of this conjecture for a specific class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training  sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from [Abbe et al.'22] by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity scaling that matches that of Correlational Statistical Query (CSQ) lower-bounds."}}
{"id": "TERVhuQVTe", "cdate": 1652737589839, "mdate": null, "content": {"title": "GULP: a prediction-based metric between representations", "abstract": "Comparing the representations learned by different neural networks has recently emerged as a key tool to understand various architectures and ultimately optimize them. In this work, we introduce GULP, a family of distance measures between representations that is explicitly motivated by  downstream predictive tasks. By construction, GULP provides uniform control over the difference in prediction performance between two representations, with respect to regularized linear prediction tasks. Moreover, it satisfies several desirable structural properties, such as the triangle inequality and invariance under orthogonal transformations, and thus lends itself to data embedding and visualization. We extensively evaluate GULP relative to other methods, and demonstrate that it correctly differentiates between architecture families, converges over the course of training, and captures generalization performance on downstream linear tasks. "}}
{"id": "Leg6spUEFFf", "cdate": 1652737542925, "mdate": null, "content": {"title": "On the non-universality of deep learning: quantifying the cost of symmetry", "abstract": "We prove limitations on what neural networks trained by noisy gradient descent (GD) can efficiently learn. Our results apply whenever GD training is equivariant, which holds for many standard architectures and initializations. As applications, (i) we characterize the functions that fully-connected networks can weak-learn on the binary hypercube and unit sphere, demonstrating that depth-2 is as powerful as any other depth for this task; (ii) we extend the merged-staircase necessity result for learning with latent low-dimensional structure [ABM22] to beyond the mean-field regime. Under cryptographic assumptions, we also show hardness results for learning with fully-connected networks trained by stochastic gradient descent (SGD)."}}
{"id": "xDkmVKHREhX", "cdate": 1633111008183, "mdate": null, "content": {"title": "Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure", "abstract": "Multimarginal Optimal Transport (MOT) has attracted significant interest due to applications in machine learning, statistics, and the sciences. However, in most applications, the success of MOT is severely limited by a lack of efficient algorithms. Indeed, MOT in general requires exponential time in the number of marginals k and their support sizes n. This paper develops a general theory about what \"structure\" makes MOT solvable in poly(n,k) time.\nWe develop a unified algorithmic framework for solving MOT in poly(n,k) time by characterizing the \"structure\" that different algorithms require in terms of simple variants of the dual feasibility oracle. This framework has several benefits. First, it enables us to show that the Sinkhorn algorithm, which is currently the most popular MOT algorithm, requires strictly more structure than other algorithms do to solve MOT in poly(n,k) time. Second, our framework makes it much simpler to develop poly(n,k) time algorithms for a given MOT problem. In particular, it is necessary and sufficient to (approximately) solve the dual feasibility oracle -- which is much more amenable to standard algorithmic techniques.\nWe illustrate this ease-of-use by developing poly(n,k) time algorithms for three general classes of MOT cost structures: (1) graphical structure; (2) set-optimization structure; and (3) low-rank plus sparse structure. For structure (1), we recover the known result that Sinkhorn has poly(n,k) runtime; moreover, we provide the first poly(n,k) time algorithms for computing solutions that are exact and sparse. For structures (2)-(3), we give the first poly(n,k) time algorithms, even for approximate computation. Together, these three structures encompass many -- if not most -- current applications of MOT."}}
{"id": "fj6rFciApc", "cdate": 1621630142923, "mdate": null, "content": {"title": "The staircase property: How hierarchical structure can guide deep learning", "abstract": "This paper identifies a structural property of data distributions that enables deep neural networks to learn hierarchically. We define the ``staircase'' property for functions over the Boolean hypercube, which posits that high-order Fourier coefficients are reachable from lower-order Fourier coefficients along increasing chains. We prove that functions satisfying this  property can be learned in polynomial time using layerwise  stochastic coordinate descent on regular neural networks -- a class of network architectures and initializations that have homogeneity properties. Our analysis shows that for such staircase functions and neural networks, the gradient-based algorithm learns high-level features by greedily combining lower-level features along the depth of the network. We further back our theoretical results with experiments showing that staircase functions are learnable by more standard ResNet architectures with stochastic gradient descent. Both the theoretical and experimental results support the fact that the staircase property has a role to play in understanding the capabilities of gradient-based learning on regular networks, in contrast to general polynomial-size networks that can emulate any Statistical Query or PAC algorithm, as recently shown."}}
