{"id": "FM8auLVlRMo", "cdate": 1621630205921, "mdate": null, "content": {"title": "A Faster Maximum Cardinality Matching Algorithm with Applications in Machine Learning", "abstract": "Maximum cardinality bipartite matching is an important graph optimization problem with several applications. For instance, maximum cardinality matching in a $\\delta$-disc graph can be used in the computation of the bottleneck matching as well as the $\\infty$-Wasserstein and the L\u00e9vy-Prokhorov distances between probability distributions. For any point sets $A, B \\subset \\mathbb{R}^2$, the $\\delta$-disc graph is a bipartite graph formed by connecting every pair of points $(a,b) \\in A\\times B$ by an edge if the Euclidean distance between them is at most $\\delta$. Using the classical Hopcroft-Karp algorithm, a maximum-cardinality matching on any $\\delta$-disc graph can be found in $\\tilde{O}(n^{3/2})$ time.~\\footnote{We use $\\tilde{O}(\\cdot)$ to suppress poly-logarithmic terms in the complexity.} In this paper, we present a simplification of a recent algorithm (Lahn and Raghvendra, JoCG 2021) for the maximum cardinality matching problem and describe how a maximum cardinality matching in a $\\delta$-disc graph can be computed asymptotically faster than $O(n^{3/2})$ time for any moderately dense point set. As applications, we show that if $A$ and $B$ are point sets drawn uniformly at random from a unit square, an exact bottleneck matching can be computed in $\\tilde{O}(n^{4/3})$ time. On the other hand, experiments suggest that the Hopcroft-Karp algorithm seems to take roughly $\\Theta (n^{3/2})$ time for this case. This translates to substantial improvements in execution time for larger inputs."}}
{"id": "X5MjJDSsEb", "cdate": 1609459200000, "mdate": 1675246917374, "content": {"title": "A Faster Maximum Cardinality Matching Algorithm with Applications in Machine Learning", "abstract": "Maximum cardinality bipartite matching is an important graph optimization problem with several applications. For instance, maximum cardinality matching in a $\\delta$-disc graph can be used in the computation of the bottleneck matching as well as the $\\infty$-Wasserstein and the L\u00e9vy-Prokhorov distances between probability distributions. For any point sets $A, B \\subset \\mathbb{R}^2$, the $\\delta$-disc graph is a bipartite graph formed by connecting every pair of points $(a,b) \\in A\\times B$ by an edge if the Euclidean distance between them is at most $\\delta$. Using the classical Hopcroft-Karp algorithm, a maximum-cardinality matching on any $\\delta$-disc graph can be found in $\\tilde{O}(n^{3/2})$ time.~\\footnote{We use $\\tilde{O}(\\cdot)$ to suppress poly-logarithmic terms in the complexity.} In this paper, we present a simplification of a recent algorithm (Lahn and Raghvendra, JoCG 2021) for the maximum cardinality matching problem and describe how a maximum cardinality matching in a $\\delta$-disc graph can be computed asymptotically faster than $O(n^{3/2})$ time for any moderately dense point set. As applications, we show that if $A$ and $B$ are point sets drawn uniformly at random from a unit square, an exact bottleneck matching can be computed in $\\tilde{O}(n^{4/3})$ time. On the other hand, experiments suggest that the Hopcroft-Karp algorithm seems to take roughly $\\Theta (n^{3/2})$ time for this case. This translates to substantial improvements in execution time for larger inputs."}}
{"id": "Cg5O0Uq-3I", "cdate": 1609459200000, "mdate": 1675246917458, "content": {"title": "An O(n5/4) Time \u220a-Approximation Algorithm for RMS Matching in a Plane", "abstract": "The 2-Wasserstein distance (or RMS distance) is a useful measure of similarity between probability distributions with exciting applications in machine learning. For discrete distributions, the problem of computing this distance can be expressed in terms of finding a minimum-cost perfect matching on a complete bipartite graph given by two multisets of points A, B \u2282 \u211d2, with |A| = |B| = n, where the ground distance between any two points is the squared Euclidean distance between them. Although there is a near-linear time relative \u220a-approximation algorithm for the case where the ground distance is Euclidean (Sharathkumar and Agarwal, JACM 2020), all existing relative \u220a-approximation algorithms for the RMS distance take \u03a9(n3/2) time. This is primarily because, unlike Euclidean distance, squared Euclidean distance is not a metric. In this paper, for the RMS distance, we present a new \u220a-approximation algorithm that runs in O(n5/4 poly{log n, 1/\u220a}) time. Our algorithm is inspired by a recent approach for finding a minimum-cost perfect matching in bipartite planar graphs (Asathulla et al, TALG 2020). Their algorithm depends heavily on the existence of sublinear sized vertex separators as well as shortest path data structures that require planarity. Surprisingly, we are able to design a similar algorithm for a complete geometric graph that is far from planar and does not have any vertex separators. Central components of our algorithm include a quadtree-based distance that approximates the squared Euclidean distance and a data structure that supports both Hungarian search and augmentation in sublinear time."}}
{"id": "g1DQBaSU86", "cdate": 1577836800000, "mdate": 1675246917404, "content": {"title": "A weighted approach to the maximum cardinality bipartite matching problem with applications in geometric settings", "abstract": "We present a weighted approach to compute a maximum cardinality matching in an arbitrary bipartite graph. Our main result is a new algorithm that takes as input a weighted bipartite graph $G(A\\cup B,E)$ with edge weights of $0$ or $1$. Let $w \\leq n$ be an upper bound on the weight of any matching in $G$. Consider the subgraph induced by all the edges of $G$ with a weight $0$. Suppose every connected component in this subgraph has $O(r)$ vertices and $O(mr/n)$ edges. We present an algorithm to compute a maximum cardinality matching in $G$ in $\\tilde{O}( m(\\sqrt{w}+ \\sqrt{r}+\\frac{wr}{n}))$ time.When all the edge weights are $1$ (symmetrically when all weights are $0$), our algorithm will be identical to the well-known Hopcroft-Karp (HK) algorithm, which runs in $O(m\\sqrt{n})$ time. However, if we can carefully assign weights of $0$ and $1$ on its edges such that both $w$ and $r$ are sub-linear in $n$ and $wr=O(n^{\\gamma})$ for $\\gamma &lt; 3/2$, then we can compute a maximum cardinality matching in $o(m\\sqrt{n})$ time. Using our algorithm, we obtain a new $\\tilde{O}(n^{4/3}/\\varepsilon^3)$ time algorithm to compute an $\\varepsilon$-approximate bottleneck matching of $A,B\\subset \\mathbb{R}^2$ and a $\\frac{1}{\\varepsilon^{O(d)}}n^{1+\\frac{d-1}{2d-1}}\\mathrm{poly}\\log n$ time algorithm for computing an $\\varepsilon$-approximate bottleneck matching in $d$-dimensions. All previous algorithms take $\\Omega(n^{3/2})$ time.Our algorithm also applies to any graph $G(A \\cup B,E)$ that has an easily computable balanced vertex separator of size $|V'|^{\\delta}$, for every subgraph $G'(V',E')$ where $\\delta\\in [1/2,1)$. By applying our algorithm, we can compute a maximum matching in $\\tilde{O}(mn^{\\frac{\\delta}{1+\\delta}})$ time, improving upon the $O(m\\sqrt{n})$ time taken by the HK-Algorithm."}}
{"id": "TGq9L9J0ahG", "cdate": 1577836800000, "mdate": 1675246917444, "content": {"title": "A Faster Algorithm for Minimum-cost Bipartite Perfect Matching in Planar Graphs", "abstract": "Given a weighted planar bipartite graph G(A \u222a B, E) where each edge has an integer edge cost, we give an \u00d5(n4/3log nC) time algorithm to compute minimum-cost perfect matching; here C is the maximum edge cost in the graph. The previous best-known planarity exploiting algorithm has a running time of O(n3/2log n) and is achieved by using planar separators (Lipton and Tarjan \u201980).                                                                                                                                                                                           Our algorithm is based on the bit-scaling paradigm (Gabow and Tarjan \u201989). For each scale, our algorithm first executes O(n1/3) iterations of Gabow and Tarjan\u2019s algorithm in O(n4/3) time leaving only O(n2/3) vertices unmatched. Next, it constructs a compressed residual graph H with O(n2/3) vertices and O(n) edges. This is achieved by using an r-division of the planar graph G with r=n2/3. For each partition of the r-division, there is an edge between two vertices of H if and only if they are connected by a directed path inside the partition. Using existing efficient shortest-path data structures, the remaining O(n2/3) vertices are matched by iteratively computing a minimum-cost augmenting path, each taking \u00d5(n2/3) time. Augmentation changes the residual graph, so the algorithm updates the compressed representation for each partition affected by the change in \u00d5(n2/3) time. We bound the total number of affected partitions over all the augmenting paths by O(n2/3 log n). Therefore, the total time taken by the algorithm is \u00d5(n4/3)."}}
{"id": "zgci0grAF0v", "cdate": 1546300800000, "mdate": 1675246917445, "content": {"title": "A Weighted Approach to the Maximum Cardinality Bipartite Matching Problem with Applications in Geometric Settings", "abstract": ""}}
{"id": "LkBgj8r3S-", "cdate": 1546300800000, "mdate": 1675246917455, "content": {"title": "A Graph Theoretic Additive Approximation of Optimal Transport", "abstract": "Transportation cost is an attractive similarity measure between probability distributions due to its many useful theoretical properties. However, solving optimal transport exactly can be prohibitively expensive. Therefore, there has been significant effort towards the design of scalable approximation algorithms. Previous combinatorial results [Sharathkumar, Agarwal STOC '12, Agarwal, Sharathkumar STOC '14] have focused primarily on the design of near-linear time multiplicative approximation algorithms. There has also been an effort to design approximate solutions with additive errors [Cuturi NIPS '13, Altschuler \\etal\\ NIPS '17, Dvurechensky \\etal\\, ICML '18, Quanrud, SOSA '19] within a time bound that is linear in the size of the cost matrix and polynomial in $C/\\delta$; here $C$ is the largest value in the cost matrix and $\\delta$ is the additive error. We present an adaptation of the classical graph algorithm of Gabow and Tarjan and provide a novel analysis of this algorithm that bounds its execution time by $\\BigO(\\frac{n^2 C}{\\delta}+ \\frac{nC^2}{\\delta^2})$. Our algorithm is extremely simple and executes, for an arbitrarily small constant $\\eps$, only $\\lfloor \\frac{2C}{(1-\\eps)\\delta}\\rfloor + 1$ iterations, where each iteration consists only of a Dijkstra-type search followed by a depth-first search. We also provide empirical results that suggest our algorithm is competitive with respect to a sequential implementation of the Sinkhorn algorithm in execution time. Moreover, our algorithm quickly computes a solution for very small values of $\\delta$ whereas Sinkhorn algorithm slows down due to numerical instability."}}
{"id": "EX3G1VcByvq", "cdate": 1546300800000, "mdate": 1675246917465, "content": {"title": "A Faster Algorithm for Minimum-Cost Bipartite Matching in Minor-Free Graphs", "abstract": "We give an \u00d5(n7/5 log(nC)) time1 algorithm to compute a minimum-cost maximum cardinality matching (optimal matching) in Kh-minor free graphs with h = O(1) and integer edge weights having magnitude at most C. This improves upon the \u00d5(n10/7 log C) algorithm of Cohen et al\u2025 [SODA 2017] and the O(n3/2 log(nC)) algorithm of Gabow and Tarjan [SIAM J. Comput. 1989]. For a graph with m edges and n vertices, the well-known Hungarian Algorithm computes a shortest augmenting path in each phase in O(m) time, yielding an optimal matching in O(mn) time. The Gabow-Tarjan [SIAM J. Comput. 1989] algorithm computes, in each phase, a maximal set of vertex-disjoint shortest augmenting paths (for appropriately defined costs) in O(m) time. This reduces the number of phases from n to and the total execution time to . To obtain our speed-up, we relax the conditions on the augmenting paths and iteratively compute, in each phase, a set of carefully selected augmenting paths that are not restricted to be shortest or vertex-disjoint. As a result, our algorithm computes substantially more augmenting paths in each phase, reducing the number of phases from to O(n2/5). By using small vertex separators, the execution of each phase takes \u00d5(m) time on average. For planar graphs, we combine our algorithm with efficient shortest path data structures to obtain a minimum-cost perfect matching in \u00d5(n6/5 log (nC)) time. This improves upon the recent \u00d5(n4/3 log (nC)) time algorithm by Asathulla et al. [SODA 2018]."}}
{"id": "7bvU_Xc8vO", "cdate": 1546300800000, "mdate": 1675246917510, "content": {"title": "A Weighted Approach to the Maximum Cardinality Bipartite Matching Problem with Applications in Geometric Settings", "abstract": "We present a weighted approach to compute a maximum cardinality matching in an arbitrary bipartite graph. Our main result is a new algorithm that takes as input a weighted bipartite graph $G(A\\cup B,E)$ with edge weights of $0$ or $1$. Let $w \\leq n$ be an upper bound on the weight of any matching in $G$. Consider the subgraph induced by all the edges of $G$ with a weight $0$. Suppose every connected component in this subgraph has $\\mathcal{O}(r)$ vertices and $\\mathcal{O}(mr/n)$ edges. We present an algorithm to compute a maximum cardinality matching in $G$ in $\\tilde{\\mathcal{O}}( m(\\sqrt{w}+ \\sqrt{r}+\\frac{wr}{n}))$ time. When all the edge weights are $1$ (symmetrically when all weights are $0$), our algorithm will be identical to the well-known Hopcroft-Karp (HK) algorithm, which runs in $\\mathcal{O}(m\\sqrt{n})$ time. However, if we can carefully assign weights of $0$ and $1$ on its edges such that both $w$ and $r$ are sub-linear in $n$ and $wr=\\mathcal{O}(n^{\\gamma})$ for $\\gamma < 3/2$, then we can compute maximum cardinality matching in $G$ in $o(m\\sqrt{n})$ time. Using our algorithm, we obtain a new $\\tilde{\\mathcal{O}}(n^{4/3}/\\varepsilon^4)$ time algorithm to compute an $\\varepsilon$-approximate bottleneck matching of $A,B\\subset\\mathbb{R}^2$ and an $\\frac{1}{\\varepsilon^{\\mathcal{O}(d)}}n^{1+\\frac{d-1}{2d-1}}\\mathrm{poly}\\log n$ time algorithm for computing $\\varepsilon$-approximate bottleneck matching in $d$-dimensions. All previous algorithms take $\\Omega(n^{3/2})$ time. Given any graph $G(A \\cup B,E)$ that has an easily computable balanced vertex separator for every subgraph $G'(V',E')$ of size $|V'|^{\\delta}$, for $\\delta\\in [1/2,1)$, we can apply our algorithm to compute a maximum matching in $\\tilde{\\mathcal{O}}(mn^{\\frac{\\delta}{1+\\delta}})$ time improving upon the $\\mathcal{O}(m\\sqrt{n})$ time taken by the HK-Algorithm."}}
{"id": "JsnXGZcpkhQ", "cdate": 1514764800000, "mdate": 1675246917469, "content": {"title": "A Faster Algorithm for Minimum-Cost Bipartite Perfect Matching in Planar Graphs", "abstract": "Given a weighted planar bipartite graph G(A \u222a B, E) where each edge has a positive integer edge cost, we give an \u00d5(n4/3 log nC) time algorithm to compute minimum-cost perfect matching; here C is the maximum edge cost in the graph. The previous best known planarity exploiting algorithm has a running time of O(n3/2 log n) and is achieved by using planar separators (Lipton and Tarjan \u201980). Our algorithm is based on the bit-scaling paradigm (Gabow and Tarjan \u201989). For each scale, our algorithm first executes O(n1/3) iterations of Gabow and Tarjan's algorithm in O(n4/3) time leaving only O(n2/3) vertices unmatched. Next, it constructs a compressed residual graph H with O(n2/3) vertices and O(n) edges. This is achieved by using an r-division of the planar graph G with r = n2/3. For each partition of the r-division, there is an edge between two vertices of H if and only if they are connected by a directed path inside the partition. Using existing efficient shortest-path data structures, the remaining O(n2/3) vertices are matched by iteratively computing a minimum-cost augmenting path each taking \u00d5(n2/3) time. Augmentation changes the residual graph, so the algorithm updates the compressed representation for each affected partition in O(n2/3) time. We bound the total number of affected partitions over all the augmenting paths by O(n2/3 log n). Therefore, the total time taken by the algorithm is \u00d5(n4/3)."}}
