{"id": "d5MN87JhdiN", "cdate": 1664731454437, "mdate": null, "content": {"title": "Rieoptax: Riemannian Optimization in JAX", "abstract": "We present Rieoptax, an open source Python library for Riemannian optimization in JAX. We show that many differential geometric primitives, such as Riemannian exponential and logarithm maps, are usually faster in Rieoptax than existing frameworks in Python, both on CPU and GPU. We support a range of basic and advanced stochastic optimization solvers like Riemannian stochastic gradient, stochastic variance reduction, and adaptive gradient methods. A distinguishing feature of the proposed toolbox is that we also support differentially private optimization on Riemannian manifolds. "}}
{"id": "f61mn-fZnPn", "cdate": 1603119169859, "mdate": null, "content": {"title": "Temperature Scaling for Quantile Calibration", "abstract": "Deep learning models are often poorly calibrated, i.e., they may produce overconfident predictions that are  wrong, implying that their uncertainty estimates are unreliable. While a  number of approaches have been proposed recently to calibrate classification models, relatively little work exists on calibrating regression models. Temperature Scaling is one of the most popular methods for \\emph{classification calibration}. It  performs better than or equal to more sophisticated methods. We investigate the use of Temperature Scaling  for \\emph{regression calibration} under notion of quantile calibration. "}}
{"id": "I3zV6igAT9", "cdate": 1601308034103, "mdate": null, "content": {"title": "Quantile Regularization : Towards Implicit Calibration of Regression Models", "abstract": "Recent works have shown that most deep learning models are often poorly  calibrated, i.e., they may produce overconfident\npredictions that are wrong, implying that their uncertainty estimates are unreliable. While a  number of approaches have been proposed recently to calibrate classification models, relatively little work exists on calibrating regression models. Isotonic Regression has recently been advocated for regression calibration. We provide a detailed formal analysis of the \\emph{side-effects} of Isotonic Regression when used for regression  calibration. To address this, we  recast quantile calibration as entropy estimation, and leverage this idea to construct a novel quantile regularizer, which can be used in any optimization based probabilisitc regression models. Unlike most of the existing approaches for calibrating regression models, which are based on \\emph{post-hoc} processing of the model's output, and require an additional dataset, our method is trainable in an end-to-end fashion, without requiring an additional dataset. We provide empirical results demonstrating that our approach improves  calibration for regression models trained on diverse architectures that  provide uncertainty estimates, such as Dropout VI, Deep Ensembles"}}
