{"id": "gubB5Dn1xX4", "cdate": 1640995200000, "mdate": 1649926851372, "content": {"title": "Geometric Aspects of Data-Processing of Markov Chains", "abstract": "We consider data-processing of Markov chains through the lens of information geometry. We first develop a theory of congruent Markov morphisms in the context of Markov kernels that we show to correspond to the congruent embeddings with respect to the lumping operation. Furthermore, we inspect information projections onto geodesically convex sets of Markov kernels, and show that under some conditions, m-projecting onto doubly convex submanifolds can be regarded as a data-processing operation. Finally, we show that the family of lumpable kernels can be meaningfully endowed with the structure of a foliated manifold."}}
{"id": "HFvPfNDTShj", "cdate": 1621629820853, "mdate": null, "content": {"title": "Dimension-free empirical entropy estimation", "abstract": "We seek an entropy estimator for discrete distributions with fully empirical accuracy bounds. As stated, this goal is infeasible without some prior assumptions on the distribution. We discover that a certain information moment assumption renders the problem feasible. We argue that the moment assumption is natural and, in some sense, {\\em minimalistic} --- weaker than finite support or tail decay conditions. Under the moment assumption, we provide the first finite-sample entropy estimates for infinite alphabets, nearly recovering the known minimax rates. Moreover, we demonstrate that our empirical bounds are significantly sharper than the state-of-the-art bounds, for various natural distributions and non-trivial sample regimes. Along the way, we give a dimension-free analogue of the Cover-Thomas result on entropy continuity (with respect to total variation distance) for finite alphabets, which may be of independent interest."}}
{"id": "KtkZvhMD8OS", "cdate": 1617672498493, "mdate": null, "content": {"title": "Statistical estimation of ergodic Markov chain kernel over discrete state space", "abstract": "We investigate the statistical complexity of estimating the parameters of a discrete-state Markov chain kernel from a single long sequence of state observations. In the finite case, we characterize (modulo logarithmic factors) the minimax sample complexity of estimation with respect to the operator infinity norm, while in the countably infinite case, we analyze the problem with respect to a natural entry-wise norm derived from total variation. We show that in both cases, the sample complexity is governed by the mixing properties of the unknown chain, for which, in the finite-state case, there are known finite-sample estimators with fully empirical confidence intervals."}}
{"id": "qVuG4qu0VmS", "cdate": 1609459200000, "mdate": 1649926851586, "content": {"title": "Information Geometry of Reversible Markov Chains", "abstract": "We analyze the information geometric structure of time reversibility for parametric families of irreducible transition kernels of Markov chains. We define and characterize reversible exponential families of Markov kernels, and show that irreducible and reversible Markov kernels form both a mixture family and, perhaps surprisingly, an exponential family in the set of all stochastic kernels. We propose a parametrization of the entire manifold of reversible kernels, and inspect reversible geodesics. We define information projections onto the reversible manifold, and derive closed-form expressions for the e-projection and m-projection, along with Pythagorean identities with respect to information divergence, leading to some new notion of reversiblization of Markov kernels. We show the family of edge measures pertaining to irreducible and reversible kernels also forms an exponential family among distributions over pairs. We further explore geometric properties of the reversible family, by comparing them with other remarkable families of stochastic matrices. Finally, we show that reversible kernels are, in a sense we define, the minimal exponential family generated by the m-family of symmetric kernels, and the smallest mixture family that comprises the e-family of memoryless kernels."}}
{"id": "htErI8HUYKY", "cdate": 1609459200000, "mdate": 1649926851380, "content": {"title": "On the \u03b1-lazy version of Markov chains in estimation and testing problems", "abstract": "Given access to a single long trajectory generated by an unknown irreducible Markov chain $M$, we simulate an $\\alpha$-lazy version of $M$ which is ergodic. This enables us to generalize recent results on estimation and identity testing that were stated for ergodic Markov chains in a way that allows fully empirical inference. In particular, our approach shows that the pseudo spectral gap introduced by Paulin [2015] and defined for ergodic Markov chains may be given a meaning already in the case of irreducible but possibly periodic Markov chains."}}
{"id": "D7yneZapUxy", "cdate": 1609459200000, "mdate": 1649926851375, "content": {"title": "Identity testing of reversible Markov chains", "abstract": "We consider the problem of identity testing of Markov chain transition matrices based on a single trajectory of observations under the distance notion introduced by Daskalakis et al. [2018a] and further analyzed by Cherapanamjeri and Bartlett [2019]. Both works made the restrictive assumption that the Markov chains under consideration are symmetric. In this work we relax the symmetry assumption and show that it is possible to perform identity testing under the much weaker assumption of reversibility, provided that the stationary distributions of the reference and of the unknown Markov chains are close under a distance notion related to the separation distance. Additionally, we provide intuition on the distance notion of Daskalakis et al. [2018a] by showing how it behaves under several natural operations. In particular, we address some of their open questions."}}
{"id": "1QRLF8Keb8t", "cdate": 1609459200000, "mdate": 1649926851375, "content": {"title": "Dimension-Free Empirical Entropy Estimation", "abstract": "We seek an entropy estimator for discrete distributions with fully empirical accuracy bounds. As stated, this goal is infeasible without some prior assumptions on the distribution. We discover that a certain information moment assumption renders the problem feasible. We argue that the moment assumption is natural and, in some sense, {\\em minimalistic} -- weaker than finite support or tail decay conditions. Under the moment assumption, we provide the first finite-sample entropy estimates for infinite alphabets, nearly recovering the known minimax rates. Moreover, we demonstrate that our empirical bounds are significantly sharper than the state-of-the-art bounds, for various natural distributions and non-trivial sample regimes. Along the way, we give a dimension-free analogue of the Cover-Thomas result on entropy continuity (with respect to total variation distance) for finite alphabets, which may be of independent interest. Additionally, we resolve all of the open problems posed by J\\\"urgensen and Matthews, 2010."}}
{"id": "trdpBi4hY46", "cdate": 1577836800000, "mdate": null, "content": {"title": "Mixing Time Estimation in Ergodic Markov Chains from a Single Trajectory with Contraction Methods", "abstract": "The mixing time $t_{\\mathsf{mix}}$ of an ergodic Markov chain measures the rate of convergence towards its stationary distribution $\\boldsymbol{\\pi}$. We consider the problem of estimating $t_{\\mathsf{mix}}$ from one single trajectory of $m$ observations $(X_1, \u2026, X_m)$, in the case where the transition kernel $\\boldsymbol{M}$ is unknown, a research program started by Hsu et al. [2015]. The community has so far focused primarily on leveraging spectral methods to estimate the relaxation time $t_{\\mathsf{rel}}$ of a reversible Markov chain as a proxy for $t_{\\mathsf{mix}}$. Although these techniques have recently been extended to tackle non-reversible chains, this general setting remains much less understood. Our new approach based on contraction methods is the first that aims at directly estimating $t_{\\mathsf{mix}}$ up to multiplicative small universal constants instead of $t_{\\mathsf{rel}}$. It does so by introducing a generalized version of Dobrushin\u2019s contraction coefficient $\\kappa_{\\mathsf{gen}}$, which is shown to control the mixing time regardless of reversibility. We subsequently design fully data-dependent high confidence intervals around $\\kappa_{\\mathsf{gen}}$ that generally yield better convergence guarantees and are more practical than state-of-the-art."}}
{"id": "l37rxQyjpsF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning discrete distributions with infinite support", "abstract": "We present a novel approach to estimating discrete distributions with (potentially) infinite support in the total variation metric. In a departure from the established paradigm, we make no structural assumptions whatsoever on the sampling distribution. In such a setting, distribution-free risk bounds are impossible, and the best one could hope for is a fully empirical data-dependent bound. We derive precisely such bounds, and demonstrate that these are, in a well-defined sense, the best possible. Our main discovery is that the half-norm of the empirical distribution provides tight upper and lower estimates on the empirical risk. Furthermore, this quantity decays at a nearly optimal rate as a function of the true distribution. The optimality follows from a minimax result, of possible independent interest. Additional structural results are provided, including an exact Rademacher complexity calculation and apparently a first connection between the total variation risk and the missing mass."}}
{"id": "fkUp1mE8EUj", "cdate": 1577836800000, "mdate": 1649926851378, "content": {"title": "Minimax Testing of Identity to a Reference Ergodic Markov Chain", "abstract": "We exhibit an efficient procedure for testing, based on a single long state sequence, whether an unknown Markov chain is identical to or e-far from a given reference chain. We obtain nearly matchin..."}}
