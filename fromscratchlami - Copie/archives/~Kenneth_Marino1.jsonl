{"id": "9BkgKhRVt_", "cdate": 1664943343923, "mdate": null, "content": {"title": "Collaborating with language models for embodied reasoning", "abstract": "Reasoning in a complex and ambiguous embodied environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance."}}
{"id": "YoS-abmWjJc", "cdate": 1664358385369, "mdate": null, "content": {"title": "Collaborating with language models for embodied reasoning", "abstract": "Reasoning in a complex and ambiguous embodied environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance."}}
{"id": "wGF5mreJVN", "cdate": 1652737763383, "mdate": null, "content": {"title": "Learning to Navigate Wikipedia by Taking Random Walks", "abstract": "A fundamental ability of an intelligent web-based agent is seeking out and acquiring new information. Internet search engines reliably find the correct vicinity but the top results may be a few links away from the desired target. A complementary approach is navigation via hyperlinks, employing a policy that comprehends local content and selects a link that moves it closer to the target. In this paper, we show that behavioral cloning of randomly sampled trajectories is sufficient to learn an effective link selection policy. We demonstrate the approach on a graph version of Wikipedia with 38M nodes and 387M edges. The model is able to efficiently navigate between nodes 5 and 20 steps apart 96% and 92% of the time, respectively. We then use the resulting embeddings and policy in downstream fact verification and question answering tasks where, in combination with basic TF-IDF search and ranking methods, they are competitive results to the state-of-the-art methods."}}
{"id": "Y87Ri-GNHYu", "cdate": 1601308187234, "mdate": null, "content": {"title": "Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning", "abstract": "Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions."}}
{"id": "XbJiphOWXiU", "cdate": 1601308141942, "mdate": null, "content": {"title": "Empirically Verifying Hypotheses Using Reinforcement Learning", "abstract": "This paper formulates hypothesis verification as an RL problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world, can take actions to generate observations which can help predict whether the hypothesis is true or false. Existing RL algorithms fail to solve this task, even for simple environments. \nIn order to train the agents, we exploit the underlying structure of many hypotheses, factorizing them as {pre-condition, action sequence, post-condition} triplets. By leveraging this structure we show that RL agents are able to succeed at the task. Furthermore, subsequent fine-tuning of the policies allows the agent to correctly verify hypotheses not amenable to the above factorization."}}
{"id": "xNndvBvVX-", "cdate": 1582759553844, "mdate": null, "content": {"title": "The Pose Knows: Video Forecasting by Generating Pose Futures", "abstract": "Current approaches to video forecasting attempt to generate\nvideos directly in pixel space using Generative Adversarial\nNetworks (GANs) or Variational Autoencoders\n(VAEs). However, since these approaches try to model all\nthe structure and scene dynamics at once, in unconstrained\nsettings they often generate uninterpretable results. Our insight\nis that forecasting needs to be done first at a higher\nlevel of abstraction. Specifically, we exploit human pose detectors\nas a free source of supervision and break the video\nforecasting problem into two discrete steps. First we explicitly\nmodel the high level structure of active objects in the\nscene (humans) and use a VAE to model the possible future\nmovements of humans in the pose space. We then use\nthe future poses generated as conditional information to a\nGAN to predict the future frames of the video in pixel space.\nBy using the structured space of pose as an intermediate\nrepresentation, we sidestep the problems that GANs have in\ngenerating video pixels directly. We show through quantitative\nand qualitative evaluation that our method outperforms\nstate-of-the-art methods for video prediction."}}
{"id": "Syxss0EYPS", "cdate": 1569439394680, "mdate": null, "content": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment."}}
{"id": "HobDHXXxdpS", "cdate": 1546300800000, "mdate": null, "content": {"title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge.", "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain."}}
{"id": "SJz1x20cFQ", "cdate": 1538087911348, "mdate": null, "content": {"title": "Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies", "abstract": "In this paper we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks.\nThe agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a ``phase function.'' The high-level policy is trained using a sparse, task-dependent reward, and operates by choosing which of the low-level policies to run at any given time. Using this approach, we solve difficult maze and navigation tasks with sparse rewards using the Mujoco Ant and Humanoid agents and show improvement over recent hierarchical methods. "}}
{"id": "S14fF-f_ZB", "cdate": 1483228800000, "mdate": null, "content": {"title": "The Pose Knows: Video Forecasting by Generating Pose Futures", "abstract": "Current approaches to video forecasting attempt to generate videos directly in pixel space using Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all the structure and scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our insight is that forecasting needs to be done first at a higher level of abstraction. Specifically, we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps. First we explicitly model the high level structure of active objects in the scene (humans) and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantitative and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction."}}
