{"id": "enoU_Kp7Dz", "cdate": 1664731446037, "mdate": null, "content": {"title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability", "abstract": "Traditional analyses of gradient descent with learning rate $\\eta$ show that when the largest eigenvalue of the Hessian of the loss, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is \"stable\" and the training loss decreases monotonically. However, Cohen et al. (2021) recently observed two important phenomena. The first, \\emph{progressive sharpening}, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, \\emph{edge of stability}, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss non-monotonically decreases.\nWe demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call \\emph{self-stabilization}, is a general property of gradient descent and explains its behavior at the edge of stability.\nA key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows \\emph{projected} gradient descent (PGD) under the constraint $S(\\theta) \\le 2/\\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability."}}
{"id": "nhKHA59gXz", "cdate": 1663850180296, "mdate": null, "content": {"title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability", "abstract": "Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is \"stable\" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen at al. (2021) detailed two important phenomena. The first, dubbed \\emph{progressive sharpening}, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, dubbed \\emph{edge of stability}, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call \\emph{self-stabilization}, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows \\emph{projected} gradient descent (PGD) under the constraint $S(\\theta) \\le 2/\\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability."}}
{"id": "x2TMPhseWAW", "cdate": 1621629825078, "mdate": null, "content": {"title": "Label Noise SGD Provably Prefers Flat Global Minimizers", "abstract": "In overparametrized models, the noise in stochastic gradient descent (SGD) implicitly regularizes the optimization trajectory and determines which local minimum SGD converges to. Motivated by empirical studies that demonstrate that training with noisy labels improves generalization, we study the implicit regularization effect of SGD with label noise. We show that SGD with label noise converges to a stationary point of a regularized loss $L(\\theta) +\\lambda R(\\theta)$, where $L(\\theta)$ is the training loss, $\\lambda$ is an effective regularization parameter depending on the step size, strength of the label noise, and the batch size, and $R(\\theta)$ is an explicit regularizer that penalizes sharp minimizers. Our analysis uncovers an additional regularization effect of large learning rates beyond the linear scaling rule that penalizes large eigenvalues of the Hessian more than small ones. We also prove extensions to classification with general loss functions, significantly strengthening the prior work of Blanc et al. to global convergence and large learning rates and of HaoChen et al. to general models."}}
