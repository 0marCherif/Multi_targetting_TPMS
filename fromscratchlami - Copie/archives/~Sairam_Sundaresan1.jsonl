{"id": "HExcb-arUxq", "cdate": 1645792505380, "mdate": null, "content": {"title": "A Hardware-Aware Framework for Accelerating Neural Architecture Search Across Modalities", "abstract": "Recent advances in Neural Architecture Search (NAS) such as one-shot NAS offer the ability to extract specialized hardware-aware sub-network configurations from a task-specific super-network. While considerable effort has been employed towards improving the first stage, namely, the training of the super-network, the search for derivative high-performing sub-networks is still under-explored. We propose a flexible search framework that automatically and efficiently finds sub-networks that are optimized for different performance metrics and hardware configurations. Specifically, we demonstrate how various evolutionary algorithms when paired with lightly trained objective predictors can accelerate architecture search in a multi-objective setting for various modalities including machine translation, recommendation, and image classification. "}}
{"id": "MXrIVw-F_a4", "cdate": 1632875544254, "mdate": null, "content": {"title": "FLOAT:  FAST LEARNABLE ONCE-FOR-ALL ADVERSARIAL TRAINING FOR TUNABLE TRADE-OFF BETWEEN ACCURACY AND ROBUSTNESS", "abstract": "Training a model that can be robust against adversarially-perturbed images with-out compromising accuracy on clean-images has proven to be challenging. Recent research has tried to resolve this issue by incorporating an additional layer after each  batch-normalization  layer  in  a  network,  that  implements  feature-wise  linear modulation (FiLM). These extra layers enable in-situ calibration of a trained model, allowing the user to configure the desired priority between robustness and clean-image performance after deployment.  However, these extra layers significantly increase training time, parameter count, and add latency which can prove costly  for  time  or  memory  constrained  applications.   In  this  paper,  we  present Fast Learnable Once-for-all Adversarial Training (FLOAT) which transforms the weight tensors without using extra layers, thereby incurring no significant increase in parameter count, training time, or network latency compared to a standard adversarial training. In particular, we add configurable scaled noise to the weight tensors that enables a \u2018continuous\u2019 trade-off between clean and adversarial performance.  Additionally, we extend FLOAT to slimmable neural networks to enable a three-way in-situ trade-off between robustness, accuracy, and complexity.  Extensive experiments show that FLOAT can yield state-of-the-art performance improving both clean and perturbed image classification by up to \u223c6.5% and \u223c14.5%,  respectively,  while  requiring  up  to 1.47x fewer  parameters with similar hyperparameter settings compared to FiLM-based alternatives."}}
{"id": "rszGQJYf9S0", "cdate": 1620340294755, "mdate": null, "content": {"title": "ATTENTIONLITE: TOWARDS EFFICIENT SELF-ATTENTION MODELS FOR VISION", "abstract": "We propose a novel framework for producing a class of parameter and compute efficient models called AttentionLite\nsuitable for resource constrained applications. Prior work has primarily focused on optimizing models either via knowledge\ndistillation or pruning. In addition to fusing these two mechanisms, our joint optimization framework also leverages recent\nadvances in self-attention as a substitute for convolutions. We can simultaneously distill knowledge from a compute heavy\nteacher while also pruning the student model in a single pass of training thereby reducing training and fine tuning times considerably. We evaluate the merits of our proposed approach on the CIFAR-10, CIFAR-100 and Tiny-ImageNet datasets. Not only\ndo our AttentionLite models significantly outperform their unoptimized counterparts in accuracy, we find that in some cases, that they perform almost as well as their compute-heavy teachers while consuming only a fraction of the parameters and FLOPs. Concretely, AttentionLite models can achieve up to 30\u00d7 parameter efficiency and 2\u00d7 computation efficiency with no significant accuracy drop compared to their teacher."}}
{"id": "zZU31P7JKIE", "cdate": 1609459200000, "mdate": 1632699085545, "content": {"title": "AttentionLite: Towards Efficient Self-Attention Models for Vision", "abstract": "We propose a novel framework for producing a class of parameter and compute efficient models called AttentionLite suitable for resource constrained applications. Prior work has primarily focused on optimizing models either via knowledge distillation or pruning. In addition to fusing these two mechanisms, our joint optimization framework also leverages recent advances in self-attention as a substitute for convolutions. We can simultaneously distill knowledge from a compute heavy teacher while also pruning the student model in a single pass of training thereby reducing training and fine tuning times considerably. We evaluate the merits of our proposed approach on the CIFAR-10, CIFAR-100 and Tiny-ImageNet datasets. Not only do our AttentionLite models significantly outperform their unoptimized counterparts in accuracy, we find that in some cases, that they perform almost as well as their compute-heavy teachers while consuming only a fraction of the parameters and FLOPs. Concretely, AttentionLite models can achieve up to 30\u00d7 parameter efficiency and 2\u00d7 computation efficiency with no significant accuracy drop compared to their teacher."}}
{"id": "JKvHis0D30P", "cdate": 1577836800000, "mdate": null, "content": {"title": "Attention-based Image Upsampling", "abstract": "Convolutional layers are an integral part of many deep neural network solutions in computer vision. Recent work shows that replacing the standard convolution operation with mechanisms based on self-attention leads to improved performance on image classification and object detection tasks. In this work, we show how attention mechanisms can be used to replace another canonical operation: strided transposed convolution. We term our novel attention-based operation attention-based upsampling since it increases/upsamples the spatial dimensions of the feature maps. Through experiments on single image super-resolution and joint-image upsampling tasks, we show that attention-based upsampling consistently outperforms traditional upsampling methods based on strided transposed convolution or based on adaptive filters while using fewer parameters. We show that the inherent flexibility of the attention mechanism, which allows it to use separate sources for calculating the attention coefficients and the attention targets, makes attention-based upsampling a natural choice when fusing information from multiple image modalities."}}
{"id": "48xD9NIyvQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Logic2Text: High-Fidelity Natural Language Generation from Logical Forms", "abstract": "Previous studies on Natural Language Generation (NLG) from structured data have primarily focused on surface-level descriptions of record sequences. However, for complex structured data, e.g., multi-row tables, it is often desirable for an NLG system to describe interesting facts from logical inferences across records. If only provided with the table, it is hard for existing models to produce controllable and high-fidelity logical generations. In this work, we formulate high-fidelity NLG as generation from logical forms in order to obtain controllable and faithful generations. We present a new large-scale dataset, Logic2Text, with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which pose great challenges on the model\u2019s ability to understand the semantics. We experiment on (1) Fully-supervised training with the full datasets, and (2) Few-shot setting, provided with hundreds of paired examples; We compare several popular generation models and analyze their performances. We hope our dataset can encourage research towards building an advanced NLG system capable of natural, faithful, and human-like generation. The dataset and code is available at https://github.com/czyssrs/Logic2Text."}}
{"id": "28NBGVQ8V1p", "cdate": 1577836800000, "mdate": 1632699085586, "content": {"title": "RotNet: Fast and Scalable Estimation of Stellar Rotation Periods Using Convolutional Neural Networks", "abstract": "Magnetic activity in stars manifests as dark spots on their surfaces that modulate the brightness observed by telescopes. These light curves contain important information on stellar rotation. However, the accurate estimation of rotation periods is computationally expensive due to scarce ground truth information, noisy data, and large parameter spaces that lead to degenerate solutions. We harness the power of deep learning and successfully apply Convolutional Neural Networks to regress stellar rotation periods from Kepler light curves. Geometry-preserving time-series to image transformations of the light curves serve as inputs to a ResNet-18 based architecture which is trained through transfer learning. The McQuillan catalog of published rotation periods is used as ansatz to groundtruth. We benchmark the performance of our method against a random forest regressor, a 1D CNN, and the Auto-Correlation Function (ACF) - the current standard to estimate rotation periods. Despite limiting our input to fewer data points (1k), our model yields more accurate results and runs 350 times faster than ACF runs on the same number of data points and 10,000 times faster than ACF runs on 65k data points. With only minimal feature engineering our approach has impressive accuracy, motivating the application of deep learning to regress stellar parameters on an even larger scale"}}
{"id": "aPRXJqfNSt0", "cdate": 1546300800000, "mdate": 1632699085564, "content": {"title": "Compact Scene Graphs for Layout Composition and Patch Retrieval", "abstract": "Structured representations such as scene graphs serve as an efficient and compact representation that can be used for downstream rendering or retrieval tasks. However, existing efforts to generate realistic images from scene graphs perform poorly on scene composition for cluttered or complex scenes. We propose two contributions to improve the scene composition. First, we enhance the scene graph representation with heuristic-based relations, which add minimal storage overhead. Second, we use extreme points representation to supervise the learning of the scene composition network. These methods achieve significantly higher performance over existing work (69.0% vs 51.2% in relation score metric). We additionally demonstrate how scene graphs can be used to retrieve pose-constrained image patches that are semantically similar to the source query. Improving structured scene graph representations for rendering or retrieval are an important step towards realistic image generation."}}
{"id": "H7R-3NmxuaH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Compact Scene Graphs for Layout Composition and Patch Retrieval.", "abstract": "Structured representations such as scene graphs serve as an efficient and compact representation that can be used for downstream rendering or retrieval tasks. However, existing efforts to generate realistic images from scene graphs perform poorly on scene composition for cluttered or complex scenes. We propose two contributions to improve the scene composition. First, we enhance the scene graph representation with heuristic-based relations, which add minimal storage overhead. Second, we use extreme points representation to supervise the learning of the scene composition network. These methods achieve significantly higher performance over existing work (69.0% vs 51.2% in relation score metric). We additionally demonstrate how scene graphs can be used to retrieve pose-constrained image patches that are semantically similar to the source query. Improving structured scene graph representations for rendering or retrieval are an important step towards realistic image generation."}}
