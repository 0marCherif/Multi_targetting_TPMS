{"id": "S0PRWzdI2Fv", "cdate": 1668597834787, "mdate": 1668597834787, "content": {"title": "Pali: A jointly-scaled multilingual language-image model", "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks.PaLI(PathwaysLanguage andImage model) extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design."}}
{"id": "X6U3mimfmeq", "cdate": 1668457432751, "mdate": 1668457432751, "content": {"title": "CoCa: Contrastive Captioners are Image-Text Foundation Models", "abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision\nbecause these models can be quickly transferred to many downstream tasks. This paper\npresents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text\nencoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby\nsubsuming model capabilities from contrastive approaches like CLIP and generative methods\nlike SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers\nattend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers\nto encode unimodal text representations, and cascades the remaining decoder layers which\ncross-attend to the image encoder for multimodal image-text representations. We apply a\ncontrastive loss between unimodal image and text embeddings, in addition to a captioning\nloss on the multimodal decoder outputs which predicts text tokens autoregressively. By\nsharing the same computational graph, the two training objectives are computed efficiently\nwith minimal overhead. CoCa is pretrained end-to-end and from scratch on both webscale alt-text data and annotated images by treating all labels simply as text, seamlessly\nunifying natural language supervision for representation learning. Empirically, CoCa achieves\nstate-of-the-art performance with zero-shot transfer or minimal task-specific adaptation\non a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT),\nmultimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO,\nNoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy,\n90.6% with a frozen encoder and learned classification head, and 91.0% with a finetuned\nencoder."}}
{"id": "mWVoBz4W0u", "cdate": 1663850174653, "mdate": null, "content": {"title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI, a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design."}}
{"id": "HoobHfrgd6r", "cdate": 1451606400000, "mdate": null, "content": {"title": "Semantic Image Segmentation with Contextual Hierarchical Models.", "abstract": "Semantic segmentation is the problem of assigning an object label to each pixel. It unifies the image segmentation and object recognition problems. The importance of using contextual information in semantic segmentation frameworks has been widely realized in the field. We propose a contextual framework, called contextual hierarchical model (CHM), which learns contextual information in a hierarchical framework for semantic segmentation. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. This training strategy allows for optimization of a joint posterior probability at multiple resolutions through the hierarchy. Contextual hierarchical model is purely based on the input image patches and does not make use of any fragments or shape examples. Hence, it is applicable to a variety of problems such as object segmentation and edge detection. We demonstrate that CHM performs at par with state-of-the-art on Stanford background and Weizmann horse datasets. It also outperforms state-of-the-art edge detection methods on NYU depth dataset and achieves state-of-the-art on Berkeley segmentation dataset (BSDS 500)."}}
{"id": "HkW4ZWGd-S", "cdate": 1356998400000, "mdate": null, "content": {"title": "Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks", "abstract": "Contextual information plays an important role in solving vision problems such as image segmentation. However, extracting contextual information and using it in an effective way remains a difficult problem. To address this challenge, we propose a multi-resolution contextual framework, called cascaded hierarchical model (CHM), which learns contextual information in a hierarchical framework for image segmentation. At each level of the hierarchy, a classifier is trained based on down sampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. We repeat this procedure by cascading the hierarchical framework to improve the segmentation accuracy. Multiple classifiers are learned in the CHM, therefore, a fast and accurate classifier is required to make the training tractable. The classifier also needs to be robust against over fitting due to the large number of parameters learned during training. We introduce a novel classification scheme, called logistic disjunctive normal networks (LDNN), which consists of one adaptive layer of feature detectors implemented by logistic sigmoid functions followed by two fixed layers of logical units that compute conjunctions and disjunctions, respectively. We demonstrate that LDNN outperforms state-of-the-art classifiers and can be used in the CHM to improve object segmentation performance."}}
