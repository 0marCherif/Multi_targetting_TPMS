{"id": "1Qnz4ywf7M", "cdate": 1681930723770, "mdate": 1681930723770, "content": {"title": "AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving", "abstract": "Model parallelism is conventionally viewed as a method to scale a single large deep learning model beyond the memory limits of a single device. In this paper, we demonstrate that model parallelism can be additionally used for the statistical multiplexing of multiple devices when serving multiple models, even when a single model can fit into a single device. Our work reveals a fundamental trade-off between the overhead introduced by model parallelism and the opportunity to exploit statistical multiplexing to reduce serving latency in the presence of bursty workloads. We explore the new trade-off space and present a novel serving system, AlpaServe, that determines an efficient strategy for placing and parallelizing collections of large deep learning models across a distributed cluster. Evaluation results on production workloads show that AlpaServe can process requests at up to 10x higher rates or 6x more burstiness while staying within latency constraints for more than 99% of requests."}}
{"id": "wcNk7H-uZYU", "cdate": 1681930066177, "mdate": 1681930066177, "content": {"title": "High-throughput Generative Inference of Large Language Models with a Single GPU", "abstract": "The high computational and memory requirements of large language model (LLM) inference traditionally make it feasible only with multiple high-end accelerators. In this paper, we study how to lower the requirements of LLM inference down to one commodity GPU and achieve practical performance. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. Through a linear programming optimizer, it searches for the best pattern to store and access the tensors, including weights, activations, and attention key/value cache. FlexGen further compresses both weights and KV cache to 4 bits with negligible accuracy loss. Compared with state-of-the-art offloading systems, FlexGen runs OPT-175B up to 100 faster on a single 16GB GPU and achieves a practical generation throughput of 1 token/s for the first time. FlexGen also comes with a pipeline parallelism runtime to allow super-linear scaling on decoding if more distributed GPUs are given."}}
{"id": "2kO-mhgVAO4", "cdate": 1672531200000, "mdate": 1675752760077, "content": {"title": "TensorIR: An Abstraction for Automatic Tensorized Program Optimization", "abstract": ""}}
{"id": "obPcCiGeew", "cdate": 1640995200000, "mdate": 1675752760076, "content": {"title": "On Optimizing the Communication of Model Parallelism", "abstract": "We study a novel and important communication pattern in large-scale model-parallel deep learning (DL), which we call cross-mesh resharding. This pattern emerges when the two paradigms of model parallelism - intra-operator and inter-operator parallelism - are combined to support large models on large clusters. In cross-mesh resharding, a sharded tensor needs to be sent from a source device mesh to a destination device mesh, on which the tensor may be distributed with the same or different layouts. We formalize this as a many-to-many multicast communication problem, and show that existing approaches either are sub-optimal or do not generalize to different network topologies or tensor layouts, which result from different model architectures and parallelism strategies. We then propose two contributions to address cross-mesh resharding: an efficient broadcast-based communication system, and an \"overlapping-friendly\" pipeline schedule. On microbenchmarks, our overall system outperforms existing ones by up to 10x across various tensor and mesh layouts. On end-to-end training of two large models, GPT-3 and U-Transformer, we improve throughput by 10% and 50%, respectively."}}
{"id": "mIPw7tiI9M", "cdate": 1640995200000, "mdate": 1675752760076, "content": {"title": "Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning", "abstract": ""}}
{"id": "9qR0tcpJp-", "cdate": 1640995200000, "mdate": 1675752760074, "content": {"title": "NumS: Scalable Array Programming for the Cloud", "abstract": "Scientists increasingly rely on Python tools to perform scalable distributed memory array operations using rich, NumPy-like expressions. However, many of these tools rely on dynamic schedulers optimized for abstract task graphs, which often encounter memory and network bandwidth-related bottlenecks due to sub-optimal data and operator placement decisions. Tools built on the message passing interface (MPI), such as ScaLAPACK and SLATE, have better scaling properties, but these solutions require specialized knowledge to use. In this work, we present NumS, an array programming library which optimizes NumPy-like expressions on task-based distributed systems. This is achieved through a novel scheduler called Load Simulated Hierarchical Scheduling (LSHS). LSHS is a local search method which optimizes operator placement by minimizing maximum memory and network load on any given node within a distributed system. Coupled with a heuristic for load balanced data layouts, our approach is capable of attaining communication lower bounds on some common numerical operations, and our empirical study shows that LSHS enhances performance on Ray by decreasing network load by a factor of 2x, requiring 4x less memory, and reducing execution time by 10x on the logistic regression problem. On terabyte-scale data, NumS achieves competitive performance to SLATE on DGEMM, up to 20x speedup over Dask on a key operation for tensor factorization, and a 2x speedup on logistic regression compared to Dask ML and Spark's MLlib."}}
{"id": "aIfp8kLuvc9", "cdate": 1622976572830, "mdate": null, "content": {"title": "TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers", "abstract": "Search-based tensor compilers can greatly accelerate the execution of machine learning models by generating high-performance tensor programs, such as matrix multiplications and convolutions. These compilers take a high-level mathematical expression as input and search for the fastest low-level implementations. At the core of the search procedure is a cost model which estimates the performance of different candidates to reduce the frequency of time-consuming on-device measurements. There has been a growing interest in using machine learning techniques to learn a cost model to ease the effort of building an analytical model. However, a standard dataset for pre-training and benchmarking learned cost models is lacking.\n\nWe introduce TenSet, a large-scale tensor program performance dataset. TenSet contains 52 million program performance records collected from 6 hardware platforms. We provide comprehensive studies on how to learn and evaluate the cost models, including data collection, model architectures, loss functions, transfer learning, and evaluation metrics. We also show that a cost model pre-trained on TenSet can accelerate the search time in the state-of-the-art tensor compiler by up to 10$\\times$. The dataset is available at https://github.com/tlc-pack/tenset."}}
{"id": "kx87gILP_l", "cdate": 1609459200000, "mdate": 1675752760079, "content": {"title": "Simple and Automatic Distributed Machine Learning on Ray", "abstract": ""}}
{"id": "E2zgr4quap", "cdate": 1609459200000, "mdate": 1675752760284, "content": {"title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training", "abstract": "The increasing size of neural network models has been critical for improvements in their accuracy, but device memory is not growing at the same rate. This creates fundamental challenges for trainin..."}}
{"id": "qEpovMrh8q5", "cdate": 1577836800000, "mdate": 1651978972664, "content": {"title": "Ansor: Generating High-Performance Tensor Programs for Deep Learning", "abstract": ""}}
