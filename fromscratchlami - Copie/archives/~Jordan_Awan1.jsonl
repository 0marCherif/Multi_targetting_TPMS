{"id": "q-snd9xOG3b", "cdate": 1652737719231, "mdate": null, "content": {"title": "Log-Concave and Multivariate Canonical Noise Distributions for Differential Privacy", "abstract": " A canonical noise distribution (CND) is an additive mechanism designed to satisfy $f$-differential privacy ($f$-DP), without any wasted privacy budget. $f$-DP is a hypothesis testing-based formulation of privacy phrased in terms of tradeoff functions, which captures the difficulty of a hypothesis test. In this paper, we consider the existence and construction of both log-concave CNDs and multivariate CNDs. Log-concave distributions are important to ensure that higher outputs of the mechanism correspond to higher input values, whereas multivariate noise distributions are important to ensure that a joint release of multiple outputs has a tight privacy characterization. We show that the existence and construction of CNDs for both types of problems is related to whether the tradeoff function can be decomposed by functional composition (related to group privacy) or mechanism composition. In particular, we show that pure $\\epsilon$-DP cannot be decomposed in either way and that there is neither a log-concave CND nor any multivariate CND for $\\epsilon$-DP. On the other hand, we show that Gaussian-DP, $(0,\\delta)$-DP, and Laplace-DP each have both log-concave and multivariate CNDs. "}}
{"id": "tTWCQrgjuM", "cdate": 1652737462546, "mdate": null, "content": {"title": "Data Augmentation MCMC for Bayesian Inference from Privatized Data", "abstract": "Differentially private mechanisms protect privacy by introducing additional randomness into the data. Restricting access to only the privatized data makes it challenging to perform valid statistical inference on parameters underlying the confidential data. Specifically, the likelihood function of the privatized data requires integrating over the large space of confidential databases and is typically intractable. For Bayesian analysis, this results in a posterior distribution that is doubly intractable, rendering traditional MCMC techniques inapplicable. We propose an MCMC framework to perform Bayesian inference from the privatized data, which is applicable to a wide range of statistical models and privacy mechanisms. Our MCMC algorithm augments the model parameters with the unobserved confidential data, and alternately updates each one. For the potentially challenging step of updating the confidential data, we propose a generic approach that exploits the privacy guarantee of the mechanism to ensure efficiency. We give results on the computational complexity, acceptance rate, and mixing properties of our MCMC. We illustrate the efficacy and applicability of our methods on a na\u00efve-Bayes log-linear model and on a linear regression model."}}
{"id": "pr_6JwMBLa1", "cdate": 1631624333642, "mdate": null, "content": {"title": "Canonical Noise Distributions and Private Hypothesis Tests", "abstract": "In the setting of $f$-DP, we propose the concept \\emph{canonical noise distribution} (CND) which captures whether an additive privacy mechanism is tailored for a given $f$, and give a construction of a CND for an arbitrary tradeoff function $f$. We show that private hypothesis tests are intimately related to CNDs, allowing for the release of private $p$-values at no additional privacy cost as well as the construction of uniformly most powerful (UMP) tests for binary data. We apply our techniques to difference of proportions testing."}}
{"id": "E59HmNab0CB", "cdate": 1631622977911, "mdate": null, "content": {"title": "Privacy-Aware Rejection Sampling", "abstract": " Differential privacy (DP) offers strong protection against adversaries with arbitrary side-information and computational power. However, many implementations of DP mechanisms leave themselves vulnerable to side channel attacks, such as timing attacks. As many privacy mechanisms, such as the exponential mechanism, do not lend themselves to easy implementations, when sampling methods such as MCMC or rejection sampling are used, the runtime can leak privacy. In this work, we quantify the privacy cost due to the runtime of a rejection sampler in terms of  $(\\epsilon,\\delta)$-DP. We also propose three modifications to the rejection sampling algorithm, to protect against timing attacks by making the runtime independent of the data. We also use our techniques to develop an adaptive rejection sampler for log-Holder densities, which also has data-independent runtime."}}
{"id": "JNvdZKpFyDu", "cdate": 1620045181641, "mdate": null, "content": {"title": "Approximate Co-Sufficient Sampling for Goodness-of-fit Tests and Synthetic Data", "abstract": "Co-sufficient sampling refers to resampling the data conditional on a sufficient statistic, a useful technique for statistical problems such as goodness-of-fit tests, model selection, and confidence interval construction; it is also a powerful tool to generate synthetic data which limits the disclosure risk of sensitive data. However, sampling from such conditional distributions is both technically and computationally challenging, and is inapplicable in models without low-dimensional sufficient statistics. \nWe study an indirect inference approach to approximate co-sufficient sampling, which only requires an efficient statistic rather than a sufficient statistic. Given an efficient estimator, we prove that the expected KL divergence goes to zero between the true conditional distribution and the resulting approximate distribution. We also propose a one-step approximate solution to the optimization problem that preserves the original estimator with an error of o_p(n^{\u22121/2}), which suffices for asymptotic optimality. The one-step method is easily implemented, highly computationally efficient, and applicable to a wide variety of models, only requiring the ability to sample from the model and compute an efficient statistic. We implement our methods via simulations to tackle problems in synthetic data, hypothesis testing, and differential privacy."}}
{"id": "_K2s_n7bjfs", "cdate": 1620045026876, "mdate": null, "content": {"title": "Differentially Private Inference for Binomial Data", "abstract": "We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of differential privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests can be written in terms of linear constraints, and for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a \u2018Neyman-Pearson Lemma\u2019 for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a DP summary statistic, whose distribution we coin \u201cTruncated-Uniform-Laplace\u201d (Tulap), a generalization of the Staircase and discrete Laplace distributions. We show that by post-processing the Tulap statistic, we are able to obtain exact p-values corresponding to the DP-UMP, uniformly most accurate (UMA) one-sided confidence intervals, optimal confidence distributions, uniformly most powerful unbiased (UMPU) two-sided tests, and uniformly most accurate unbiased (UMAU) two-sided confidence intervals. As each of these quantities are a post-processing of the same summary statistic, there is no increased cost to privacy by including these additional results, allowing for a complete statistical analysis at a fixed privacy cost. We also show that our results can be applied to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that all our tests have exact type I error, and are more powerful than current techniques."}}
{"id": "Hp5bUx9-pMB", "cdate": 1620044887624, "mdate": null, "content": {"title": "Structure and Sensitivity in Differential Privacy: Comparing K-Norm Mechanisms", "abstract": "Differential privacy (DP) provides a framework for provable privacy protection against arbitrary adversaries, while allowing the release of summary statistics and synthetic data. We address the problem of releasing a noisy real-valued statistic vector T, a function of sensitive data under DP, via the class of K-norm mechanisms with the goal of minimizing the noise added to achieve privacy. First, we introduce the sensitivity space of T, which extends the concepts of sensitivity polytope and sensitivity hull to the setting of arbitrary statistics T. We then propose a framework consisting of three methods for comparing the K-norm mechanisms: (1) a multivariate extension of stochastic dominance, (2) the entropy of the mechanism, and (3) the conditional variance given a direction, to identify the optimal K-norm mechanism. In all of these criteria, the optimal K-norm mechanism is generated by the convex hull of the sensitivity space. Using our methodology, we extend the objective perturbation and functional mechanisms and apply these tools to logistic and linear regression, allowing for private releases of statistical results. Via simulations and an application to a housing price dataset, we demonstrate that our proposed methodology offers a substantial improvement in utility for the same level of risk."}}
{"id": "ByljkBHxLB", "cdate": 1567802594702, "mdate": null, "content": {"title": "KNG: The K-Norm Gradient Mechanism", "abstract": " This paper presents a new mechanism for producing sanitized statistical summaries that achieve {\\it differential privacy}, called the {\\it K-Norm Gradient} Mechanism, or KNG. This new approach maintains the strong flexibility of the exponential mechanism, while achieving the powerful utility performance of objective perturbation. KNG starts with an inherent objective function (often an empirical risk), and promotes summaries that are close to minimizing the objective by weighting according to how far the gradient of the objective function is from zero.  Working with the gradient instead of the original objective function allows for additional flexibility as one can penalize using different norms.  We show that, unlike the exponential mechanism, the noise added by KNG is asymptotically negligible compared to the statistical error for many problems. In addition to theoretical guarantees on privacy and utility, we confirm the utility of KNG empirically in the settings of linear and quantile regression through simulations."}}
{"id": "Syg5JHBxUr", "cdate": 1567802594290, "mdate": null, "content": {"title": "Elliptical Perturbations for Differential Privacy", "abstract": "We study elliptical distributions in locally convex vector spaces, and determine conditions when they can or cannot be used to satisfy differential privacy (DP). A requisite condition for a sanitized statistical summary to satisfying DP is that the corresponding privacy mechanism must induce equivalent measures for all possible input databases. We show that elliptical distributions with the same dispersion operator, $C$, are equivalent if the difference of their means lies in the Cameron-Martin space of $C$. In the case of releasing finite-dimensional projections using elliptical perturbations, we show that the privacy parameter $\\ep$ can be computed in terms of a one-dimensional maximization problem. We apply this result to consider multivariate Laplace, $t$, Gaussian, and $K$-norm noise. Surprisingly, we show that the multivariate Laplace noise does not achieve $\\ep$-DP in any dimension greater than one. Finally, we show that when the dimension of the space is infinite, no elliptical distribution can be used to give $\\ep$-DP, only $(\\epsilon,\\delta)$-DP is possible."}}
{"id": "SkZEDnbOZr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Benefits and Pitfalls of the Exponential Mechanism with Applications to Hilbert Spaces and Functional PCA", "abstract": "The exponential mechanism is a fundamental tool of Differential Privacy (DP) due to its strong privacy guarantees and flexibility. We study its extension to settings with summaries based on infinit..."}}
