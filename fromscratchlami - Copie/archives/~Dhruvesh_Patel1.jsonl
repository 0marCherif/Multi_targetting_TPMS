{"id": "_QN0nKLbVZ", "cdate": 1696308098130, "mdate": null, "content": {"title": "Pretrained Language Models as Visual Planners for Human Assistance", "abstract": "In our pursuit of advancing multi-modal AI assistants ca- pable of guiding users to achieve complex multi-step goals, we propose the task of \u2018Visual Planning for Assistance (VPA)\u2019. Given a succinct natural language goal, e.g., \u201cmake a shelf\u201d, and a video of the user\u2019s progress so far, the aim of VPA is to devise a plan, i.e. a sequence of actions such as \u201csand shelf\u201d, \u201cpaint shelf\u201d, etc. to realize the speci- fied goal. This requires assessing the user\u2019s progress from the (untrimmed) video, and relating it to the requirements of natural language goal, i.e. which actions to select and in what order? Consequently, this requires handling long video history and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. Importantly, we ex- periment by formulating the forecasting step as a multi- modal sequence modeling problem, allowing us to leverage the strength of pre-trained LMs (as the sequence model). This novel approach, which we call Visual Language Model based Planner (VLaMP), outperforms baselines across a suite of metrics that gauge the quality of the generated plans. Furthermore, through comprehensive ablations, we also isolate the value of each component \u2013 language pre- training, visual observations, and goal information. We have open-sourced all the data, model checkpoints, and training code."}}
{"id": "F0DowhX7_x", "cdate": 1652737524939, "mdate": null, "content": {"title": "Structured Energy Network As a Loss", "abstract": "Belanger & McCallum (2016) and Gygli et al. (2017) have shown that an energy network can capture arbitrary dependencies amongst the output variables in structured prediction; however, their reliance on gradient-based inference (GBI) makes the inference slow and unstable. In this work, we propose Structured Energy As Loss (SEAL) to take advantage of the expressivity of energy networks without incurring the high inference cost. This is a novel learning framework that uses an energy network as a trainable loss function (loss-net) to train a separate neural network (task-net), which is then used to perform the inference through a forward pass. We establish SEAL as a general framework wherein various learning strategies like margin-based, regression, and noise-contrastive, could be employed to learn the parameters of loss-net.  Through extensive evaluation on multi-label classification, semantic role labeling, and image segmentation, we demonstrate that SEAL provides various useful design choices, is faster at inference than GBI, and leads to significant performance gains over the baselines.\n"}}
{"id": "OE4kTmxv9_", "cdate": 1640995200000, "mdate": 1666979439359, "content": {"title": "Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings", "abstract": "Shib Dasgupta, Michael Boratko, Siddhartha Mishra, Shriya Atmakuri, Dhruvesh Patel, Xiang Li, Andrew McCallum. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "KjmFnWaONz", "cdate": 1640995200000, "mdate": 1666979439359, "content": {"title": "Modeling Label Space Interactions in Multi-label Classification using Box Embeddings", "abstract": "Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic..."}}
{"id": "8US8b7f2-TI", "cdate": 1640995200000, "mdate": 1666979439365, "content": {"title": "Event-Event Relation Extraction using Probabilistic Box Embedding", "abstract": "EunJeong Hwang, Jay-Yoon Lee, Tianyi Yang, Dhruvesh Patel, Dongxu Zhang, Andrew McCallum. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2022."}}
{"id": "tyTH9kOxcvh", "cdate": 1632875724461, "mdate": null, "content": {"title": "Modeling Label Space Interactions in Multi-label Classification using Box Embeddings", "abstract": "Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accurately represent the fundamentals of problem setting. In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018).  Box embeddings can be understood as trainable Venn-diagrams based on hyper-rectangles.  Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels.  Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classification datasets, we show that BBM can significantly improve taxonomic consistency while preserving or surpassing the state-of-the-art predictive performance."}}
{"id": "dEOeQgQTyvt", "cdate": 1632875507170, "mdate": null, "content": {"title": "Structured Energy Network as a dynamic loss function. Case study. A case study with multi-label Classification", "abstract": "We propose SEAL which utilizes this energy network as a trainable loss function for a simple feedfoward network. Structured prediction energy networks (SPENs) (Belanger & McCallum, 2016; Gygli et al., 2017) have shown that a neural network (i.e. energy network) can learn a reasonable energy function over the candidate structured outputs. We find that rather than using SPEN as a prediction network, using it as a trainable loss function is not only computationally efficient but also results in higher performance. compared to SPENs in both training and inference time. As the energy loss function is trainable, we propose SEAL to be dynamic which can adapt energy function to focus on the region where feedforward model will be affected most. We find this to be effective in ablation study comparing SEAL to the static version (\u00a74) where energy function is fixed after pretraining. We show the relation to previous work on the joint optimization model of energy network and feedforward model (INFNET) as we show that it is equivalent to SEAL using margin-based loss if INFNET relaxes their loss function. Based on the unique architecture of SEAL, we further propose a variant of SEAL that utilizes noise contrastive ranking (NCE) loss that by itself does not perform well as a structured energy network, but embodied in SEAL, it shows the greatest performance among the variants we study. We demonstrate the effectiveness of SEAL on 7 feature-based and 3 text-based multi-label classification datasets. The best version of SEAL that uses NCE ranking method achieves close to +2.85, +2.23 respective F1 point gain in average over cross-entropy and INFNET on the feature-based datasets, excluding one outlier that has an excessive gain of +50.0 F1 points. Lastly, examining whether the proposed framework is effective on a large pre-trained model as well, we observe SEAL achieving +0.87 F1 point gain in average on top of BERT-based adapter model o text datasets."}}
{"id": "wSCqNyUodoF", "cdate": 1609459200000, "mdate": 1643336783678, "content": {"title": "Box Embeddings: An open-source library for representation learning using geometric structures", "abstract": "A major factor contributing to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with geometric structures (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacities. In this work, we introduce Box Embeddings, a Python library that enables researchers to easily apply and extend probabilistic box embeddings."}}
{"id": "rKI4sNeKL-5", "cdate": 1609459200000, "mdate": 1646854195225, "content": {"title": "Word2Box: Learning Word Representation Using Box Embeddings", "abstract": "Learning representations of words in a continuous space is perhaps the most fundamental task in NLP, however words interact in ways much richer than vector dot product similarity can provide. Many relationships between words can be expressed set-theoretically, for example, adjective-noun compounds (eg. \"red cars\"$\\subseteq$\"cars\") and homographs (eg. \"tongue\"$\\cap$\"body\" should be similar to \"mouth\", while \"tongue\"$\\cap$\"language\" should be similar to \"dialect\") have natural set-theoretic interpretations. Box embeddings are a novel region-based representation which provide the capability to perform these set-theoretic operations. In this work, we provide a fuzzy-set interpretation of box embeddings, and learn box representations of words using a set-theoretic training objective. We demonstrate improved performance on various word similarity tasks, particularly on less common words, and perform a quantitative and qualitative analysis exploring the additional unique expressivity provided by Word2Box."}}
{"id": "IxQ12VgKIdr", "cdate": 1609459200000, "mdate": 1681995536143, "content": {"title": "Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization", "abstract": "Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, Kartik Talamadupula. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
