{"id": "GdY0JxD1WJ8", "cdate": 1704067200000, "mdate": 1699226558117, "content": {"title": "Learning consensus-aware semantic knowledge for remote sensing image captioning", "abstract": ""}}
{"id": "LI7a7d3XJ7K", "cdate": 1683880966747, "mdate": 1683880966747, "content": {"title": "Domain-Adaptive Few-Shot Learning for Hyperspectral Image Classification", "abstract": "Recently, hyperspectral image (HSI) classification by deep learning is flourishing. However, only a few labeled samples are available in practice since it is time-and-labor\u0002consuming to label pixels in HSI (called target domain). This letter proposes a domain-adaptive few-shot learning (DAFSL) method to tackle this problem. Specifically, some other HSIs (called source domain) with large labeled samples are fully used as complementary information and a generative architecture is employed to adapt embedded features in the source domain to that of the target domain. We first perform domain adaptation with unsupervised learning. In detail, the embedded features are generated by the encoder of an autoencoder, where both source and target samples could be well recovered and the reconstruction loss is used to measure the gap between the source domain and the target domain. At the same time, the embedded features are put into a metric space for classification in the source domain and the encoder parameter is fine-tuned together with the classifier in the target domain with few labels so that both general and discriminative features are well captured. The experiment results show that DAFSL outperforms the other mainstream methods with limited labeled samples."}}
{"id": "e3j2GIZqJN", "cdate": 1675209600000, "mdate": 1682498720820, "content": {"title": "Hierarchical Lifelong Machine Learning With \"Watchdog\"", "abstract": "Most existing lifelong machine learning works focus on how to exploit previously accumulated experiences (e.g., knowledge library) from earlier tasks, and transfer it to learn a new task. However, when a lifelong learning system encounters a large pool of candidate tasks, the knowledge among various coming tasks are imbalance, and the system should intelligently choose the next one to learn. In this paper, an effective \u201chuman cognition\u201d strategy is taken into consideration via actively sorting the importance of new tasks in the process of unknown-to-known, and preferentially selecting the most valuable task with more information to learn. To be specific, we assess the importance of each new coming task (e.g., unknown or not) as an outlier detection issue, and propose to employ a \u201cwatchdog\u201d knowledge library to reconstruct each task under <inline-formula><tex-math notation=\"LaTeX\">$\\ell _0$</tex-math></inline-formula> -norm constraint. The coming candidate tasks are then sorted depending on the sparse reconstruction scores in a descending order, which is referred to as a \u201cwatchdog\u201d mechanism. Following this, we design a hierarchical knowledge library for the lifelong learning framework to encode new task with higher reconstruction score, where the library consists of two-level task descriptors, i.e., a high-dimensional one with low-rank constraint and a low-dimensional one. Both \u201cwatchdog\u201d knowledge library and hierarchy knowledge library can be optimized with knowledge from both previously learned tasks and current task automatically. For model optimization, we explore an alternating method to iteratively update our proposed framework with a guaranteed convergence. Experimental results on several existing benchmarks demonstrate that our proposed model outperforms various state-of-the-art task selection methods."}}
{"id": "ykxQAtOLAw", "cdate": 1672531200000, "mdate": 1696561829308, "content": {"title": "WNet: W-Shaped Hierarchical Network for Remote-Sensing Image Change Detection", "abstract": "Change detection (CD) is a hot research topic in the remote-sensing (RS) community. With the increasing availability of high-resolution (HR) RS images, there is a growing demand for CD models with high detection accuracy and generalization ability. In other words, the CD models are expected to work well for various HRRS images. Convolutional neural networks (CNNs) have been dominated in HRRS image CD due to their excellent information extraction and nonlinear fitting capabilities. However, they are not skilled in modeling long-range contexts hidden in HRRS images, which limits their performance in CD tasks more or less. Recently, the Transformer, which is good at extracting global context dependencies, has become popular in the RS community. Nevertheless, detailed local knowledge receives insufficient emphasis in common Transformers. Considering the above discussion, we combine CNNs and Transformers and propose a new W-shaped dual-Siamese branch hierarchical network for HRRS image CD named W-shaped hierarchical network (WNet). WNet first incorporates a Siamese CNN and a Siamese Transformer into a dual-branch encoder to extract multilevel local fine-grained features and global long-range contextual dependencies. Also, we introduce deformable ideas into the Siamese CNN and Transformer to make WNet understand the critical and irregular areas within HRRS images. Second, the difference enhancement module (DEM) is developed and embedded into the encoder to produce the difference feature maps at different levels. Using simple pixel-wise subtraction and channel-wise concatenation, the changes of interest and irrelevant changes can be highlighted and suppressed in a learnable manner. Next, the multilevel difference feature maps are fused stage by stage by CNN\u2013Transformer fusion modules (CTFMs), which are the basic units of the decoder in WNet. In CTFMs, the local, global, and cross-scale clues are taken into account to ensure the integrity of information. Finally, a simple classifier is constructed and added at the top of the decoder to predict the change maps. Positive experimental results counted on four public datasets demonstrate that the proposed WNet is helpful in HRRS image CD tasks. Our source codes are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/TangXu-Group/Remote-Sensing-Image-Change-Detection/tree/main/WNet</uri> ."}}
{"id": "ykXoEDOfEB", "cdate": 1672531200000, "mdate": 1699226558157, "content": {"title": "User Migration across Multiple Social Media Platforms", "abstract": "After Twitter's ownership change and policy shifts, many users reconsidered their go-to social media outlets and platforms like Mastodon, Bluesky, and Threads became attractive alternatives in the battle for users. Based on the data from over 16,000 users who migrated to these platforms within the first eight weeks after the launch of Threads, our study examines: (1) distinguishing attributes of Twitter users who migrated, compared to non-migrants; (2) temporal migration patterns and associated challenges for sustainable migration faced by each platform; and (3) how these new platforms are perceived in relation to Twitter. Our research proceeds in three stages. First, we examine migration from a broad perspective, not just one-to-one migration. Second, we leverage behavioral analysis to pinpoint the distinct migration pattern of each platform. Last, we employ a large language model (LLM) to discern stances towards each platform and correlate them with the platform usage. This in-depth analysis illuminates migration patterns amid competition across social media platforms."}}
{"id": "xYW7tO8zt5u", "cdate": 1672531200000, "mdate": 1696561829432, "content": {"title": "SoftMatch Distance: A Novel Distance for Weakly-Supervised Trend Change Detection in Bi-Temporal Images", "abstract": "General change detection (GCD) and semantic change detection (SCD) are common methods for identifying changes and distinguishing object categories involved in those changes, respectively. However, the binary changes provided by GCD is often not practical enough, while annotating semantic labels for training SCD models is very expensive. Therefore, there is a novel solution that intuitively dividing changes into three trends (``appear'', ``disappear'' and ``transform'') instead of semantic categories, named it trend change detection (TCD) in this paper. It offers more detailed change information than GCD, while requiring less manual annotation cost than SCD. However, there are limited public data sets with specific trend labels to support TCD application. To address this issue, we propose a softmatch distance which is used to construct a weakly-supervised TCD branch in a simple GCD model, using GCD labels instead of TCD label for training. Furthermore, a strategic approach is presented to successfully explore and extract background information, which is crucial for the weakly-supervised TCD task. The experiment results on four public data sets are highly encouraging, which demonstrates the effectiveness of our proposed model."}}
{"id": "wz7rUDkQOV", "cdate": 1672531200000, "mdate": 1699226558539, "content": {"title": "OvarNet: Towards Open-Vocabulary Object Attribute Recognition", "abstract": "In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary scenario. To achieve this goal, we make the following contributions: (i) we start with a naive two-stage approach for open-vocabulary object detection and attribute classification, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for semantic category and attributes; (ii) we combine all available datasets and train with a federated strategy to finetune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs under weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowledge distillation, that performs class-agnostic object proposals and classification on semantic categories and attributes with classifiers generated from a text encoder; Finally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recognition of semantic category and attributes is complementary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outperform existing approaches that treat the two tasks independently, demonstrating strong generalization ability to novel attributes and categories."}}
{"id": "tr53m10kLPd", "cdate": 1672531200000, "mdate": 1683291658717, "content": {"title": "OvarNet: Towards Open-vocabulary Object Attribute Recognition", "abstract": "In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary scenario. To achieve this goal, we make the following contributions: (i) we start with a naive two-stage approach for open-vocabulary object detection and attribute classification, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for semantic category and attributes; (ii) we combine all available datasets and train with a federated strategy to finetune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs under weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowledge distillation, that performs class-agnostic object proposals and classification on semantic categories and attributes with classifiers generated from a text encoder; Finally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recognition of semantic category and attributes is complementary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outperform existing approaches that treat the two tasks independently, demonstrating strong generalization ability to novel attributes and categories."}}
{"id": "rdT_bbg8P5", "cdate": 1672531200000, "mdate": 1698986695978, "content": {"title": "Adversarial Domain Alignment With Contrastive Learning for Hyperspectral Image Classification", "abstract": "Recently, deep learning-based hyperspectral image (HSI) classification techniques are flourishing and exhibit good performance, where cross-domain information is usually utilized to reduce the dependence on large labeled samples. However, the gap between the source domain and the target domain makes it difficult to carry out knowledge transfer directly. In this article, an adversarial domain alignment with a contrastive learning method is designed for the HSI classification task to achieve feature consistency that benefits transferring knowledge. In detail, spectral alignment and semantic alignment are conducted at local and global levels, respectively, in an adversarial learning way, and the adversarial loss acts on both source and target domains. In order to learn specific features for objects with different spatial scales, a multiscale selection module is constructed in semantic alignment to select channel features adaptively. Moreover, contrastive learning is employed to increase both robustness and sensitiveness, where augmented data from the same/different samples are forced to be similar/dissimilar with each other. The training process is conducted in a few-shot learning way; then, the few-shot classification loss, the adversarial loss, and the contrastive loss are optimized together. Tested on one source dataset and four target datasets, the experimental results show that the proposed method outperforms the other comparisons."}}
{"id": "qYd91LhfRi", "cdate": 1672531200000, "mdate": 1696561829308, "content": {"title": "Interacting-Enhancing Feature Transformer for Cross-Modal Remote-Sensing Image and Text Retrieval", "abstract": "Cross-modal remote-sensing image\u2013text retrieval (CMRSITR) is a challenging topic in the remote-sensing (RS) community. It has gained growing attention because it can be flexibly used in many practical applications. In the current deep era, with the help of deep convolutional neural networks (DCNNs), many successful CMRSITR methods have been proposed. Most of them first learn valuable features from RS images and texts, respectively. Then, the obtained visual and textual features are mapped into a common space for the final retrieval. The above operations are feasible; however, two difficulties are still to be solved. One is that the semantics within the visual and textual features are misaligned due to the independent learning manner. The other one is that the deep links between RS images and texts cannot be fully explored by simple common space mapping. To overcome the above challenges, we propose a new model named interacting-enhancing feature transformer (IEFT) for CMRSITR, which regards the RS images and texts as a whole. First, a simple feature embedding module (FEM) is developed to map images and texts into the visual and textual feature spaces. Second, an information interacting-enhancing module (IIEM) is designed to simultaneously model the inner relationships between RS images and texts and enhance the visual features. IIEM consists of three feature interacting-enhancing (FIE) blocks, each of which contains an intermodality relationship interacting (IMRI) subblock and a visual feature enhancing (VFE) subblock. The duty of IMRI is to exploit the hidden relations between cross-modal data, while the responsibility of VFE is to improve the visual features. By combining them, semantic bias can be mitigated, and the complex contents of RS images can be studied. Finally, the retrieval module (RM) is constructed to generate the matching scores for deciding the search results. Extensive experiments are conducted on four public RS datasets. The positive results demonstrate that our IEFT can achieve superior retrieval performance compared with many existing methods. Our source codes are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/TangXu-Group/Cross-modal-remote-sensing-image-and-text-retrieval-models/tree/main/IEFT</uri> ."}}
