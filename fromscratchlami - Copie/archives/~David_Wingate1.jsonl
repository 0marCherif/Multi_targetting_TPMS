{"id": "O4G9WYOWtw9", "cdate": 1682351491720, "mdate": 1682351491720, "content": {"title": "AI Chat Assistants can Improve Conversations about Divisive Topics", "abstract": "A rapidly increasing volume of conversation occurs online, but divi-\nsiveness and conflict can fester in digital interactions. Such toxicity increases\npolarization and corrodes the capacity of diverse societies to cooperate in solv-\ning social problems. Scholars and civil society groups promote interventions\nthat make conversations less divisive or more productive, but scaling these ef-\nforts to online discourse is challenging. We conduct a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be im-\nproved with artificial intelligence tools. Specifically, we employ a large lan-\nguage model to make real-time, evidence-based recommendations intended to\nimprove participants\u2019 perception of feeling understood. These interventions\nimprove reported conversation quality, reduce political divisiveness, and im-\nprove the tone, without systematically changing the content of the conversation\nor moving people\u2019s policy attitudes."}}
{"id": "dJ3gyEMwYtb", "cdate": 1682351360242, "mdate": 1682351360242, "content": {"title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models", "abstract": "We explore the idea of compressing the\nprompts used to condition language models,\nand show that compressed prompts can re-\ntain a substantive amount of information about\nthe original prompt. For severely compressed\nprompts, while fine-grained information is\nlost, abstract information and general senti-\nments can be retained with surprisingly few pa-\nrameters, which can be useful in the context\nof decode-time algorithms for controllability\nand toxicity reduction. We explore contrastive\nconditioning to steer language model gener-\nation towards desirable text and away from\nundesirable text, and find that some complex\nprompts can be effectively compressed into a\nsingle token to guide generation. We also show\nthat compressed prompts are largely composi-\ntional, and can be constructed such that they\ncan be used to control independent aspects of\ngenerated text."}}
{"id": "Tt_b9iIWTG", "cdate": 1682351308486, "mdate": 1682351308486, "content": {"title": "Out of One, Many: Using Language Models to Simulate Human Samples", "abstract": "We propose and explore the possibility that language models can be studied as effective proxies for\nspecific human sub-populations in social science research. Practical and research applications of artificial\nintelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which\nare often treated as uniform properties of the models. We show that the \u201calgorithmic bias\u201d within one\nsuch tool\u2013 the GPT-3 language model\u2013 is instead both fine-grained and demographically correlated,\nmeaning that proper conditioning will cause it to accurately emulate response distributions from a wide\nvariety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3.\nWe create \u201csilicon samples\u201d by conditioning the model on thousands of socio-demographic backstories\nfrom real human participants in multiple large surveys conducted in the United States. We then compare\nthe silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond\nsurface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes,\nand socio-cultural context that characterize human attitudes. We suggest that language models with\nsufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of\nhumans and society across a variety of disciplines."}}
{"id": "BH-SrrT2Vg", "cdate": 1682351248282, "mdate": 1682351248282, "content": {"title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels", "abstract": "Pre-trained language models derive substantial\nlinguistic and factual knowledge from the mas-\nsive corpora on which they are trained, and\nprompt engineering seeks to align these mod-\nels to specific tasks. Unfortunately, existing\nprompt engineering methods require signifi-\ncant amounts of labeled data, access to model\nparameters, or both. We introduce a new\nmethod for selecting prompt templates without\nlabeled examples and without direct access to\nthe model. Specifically, over a set of candi-\ndate templates, we choose the template that\nmaximizes the mutual information between\nthe input and the corresponding model output.\nAcross 8 datasets representing 7 distinct NLP\ntasks, we show that when a template has high\nmutual information, it also has high accuracy\non the task. On the largest model, selecting\nprompts with our method gets 90% of the way\nfrom the average prompt accuracy to the best\nprompt accuracy and requires no ground truth\nlabels."}}
{"id": "yKbprarjc5B", "cdate": 1663850539553, "mdate": null, "content": {"title": "Leveraging Large Language Models for Multiple Choice Question Answering", "abstract": "While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., \u201cA\u201d) associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated."}}
{"id": "urueR03mkng", "cdate": 1621630343457, "mdate": null, "content": {"title": "Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning", "abstract": "Large natural language models (LMs) (such as GPT-3 or T5) demonstrate impressive abilities across a range of general NLP tasks. Here, we show that the knowledge embedded in such models provides a useful inductive bias, not just on traditional NLP tasks, but also in the nontraditional task of training a symbolic reasoning engine. We observe that these engines learn quickly and generalize in a natural way that reflects human intuition. For example, training such a system to model block-stacking might naturally generalize to stacking other types of objects because of structure in the real world that has been partially captured by the language describing it. We study several abstract textual reasoning tasks, such as object manipulation and navigation, and demonstrate multiple types of generalization to novel scenarios and the symbols that comprise them. We also demonstrate the surprising utility of $\\textit{compositional learning}$, where a learner dedicated to mastering a complicated task gains an advantage by training on relevant simpler tasks instead of jumping straight to the complicated task. "}}
{"id": "rJeuMREKwS", "cdate": 1569439248479, "mdate": null, "content": {"title": "Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning", "abstract": "In the multi-objective reinforcement learning (MORL) paradigm, the relative importance of each environment objective is often unknown prior to training, so agents must learn to specialize their behavior to optimize different combinations of environment objectives that are specified post-training. These are typically linear combinations, so the agent is effectively parameterized by a weight vector that describes how to balance competing environment objectives. However, many real world behaviors require non-linear combinations of objectives. Additionally, the conversion between desired behavior and weightings is often unclear.\nIn this work, we explore the use of a language based on propositional logic with quantitative semantics--in place of weight vectors--for specifying non-linear behaviors in an interpretable way. We use a recurrent encoder to encode logical combinations of objectives, and train a MORL agent to generalize over these encodings. We test our agent in several grid worlds with various objectives and show that our agent can generalize to many never-before-seen specifications with performance comparable to single policy baseline agents. We also demonstrate our agent's ability to generate meaningful policies when presented with novel specifications and quickly specialize to novel specifications."}}
{"id": "HkWMVCeObB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Threat, Explore, Barter, Puzzle: A Semantically-Informed Algorithm for Extracting Interaction Modes", "abstract": "In the world of online gaming, not all actions are created equal. For example, when a player's character is confronted with a closed door, it would not make much sense to brandish a weapon, apply a healing potion, or attempt to barter. A more reasonable response would be to either open or unlock the door. The term interaction mode embodies the idea that many potential actions are neither useful nor applicable in a given situation. This paper presents a AEGIM, an algorithm for the automated extraction of game interaction modes via a semantic embedding space. AEGIM uses an image captioning system in conjunction with a semantic vector space model to create a gestalt representation of in-game screenshots, thus enabling it to detect the interaction mode evoked by the game."}}
{"id": "HyNbYHGO-B", "cdate": 1483228800000, "mdate": null, "content": {"title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings", "abstract": "Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance extraction is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a tagged Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance in most cases. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose."}}
{"id": "rkblW3bOWr", "cdate": 1388534400000, "mdate": null, "content": {"title": "A Physics-Based Model Prior for Object-Oriented MDPs", "abstract": "One of the key challenges in using reinforcement learning in robotics is the need for models that capture natural world structure. There are, methods that formalize multi-object dynamics using rela..."}}
