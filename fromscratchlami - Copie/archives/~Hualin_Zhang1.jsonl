{"id": "KDhFkA6MQsW", "cdate": 1663850097931, "mdate": null, "content": {"title": "Faster Gradient-Free Methods for Escaping Saddle Points", "abstract": "Escaping from saddle points has become an important research topic in non-convex optimization. In this paper, we study the case when calculations of explicit gradients are expensive or even infeasible, and only function values are accessible. \nCurrently, there have  two types of gradient-free (zeroth-order) methods based on  random perturbation and negative curvature finding  proposed to escape saddle points  efficiently and converge to an $\\epsilon$-approximate second-order stationary point. \nNesterov's accelerated gradient descent (AGD) method can escape saddle points faster than gradient descent (GD) which have been verified in first-order algorithms. However, whether  AGD could accelerate the gradient-free methods is still unstudied. To  unfold this mystery, in this paper, we propose two accelerated  variants for the two types of gradient-free methods of escaping saddle points. We show that our algorithms can find an $\\epsilon$-approximate second-order stationary point with $\\tilde{\\mathcal{O}}(1/\\epsilon^{1.75})$ iteration complexity and $\\tilde{\\mathcal{O}}(d/\\epsilon^{1.75})$ oracle complexity, where $d$ is the problem dimension. Thus, our methods achieve a comparable convergence rate to their first-order counterparts and have fewer oracle complexity compared to prior derivative-free methods for finding second-order stationary points."}}
{"id": "2ZNPedOfwB", "cdate": 1652737490460, "mdate": null, "content": {"title": "Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity", "abstract": "$\\ell_0$ constrained optimization is prevalent in machine learning, particularly for high-dimensional problems, because it is a fundamental approach to achieve sparse learning. Hard-thresholding gradient descent is a dominant technique to solve this problem. However, first-order gradients of the objective function may be either unavailable or expensive to calculate in a lot of real-world problems, where zeroth-order (ZO) gradients could be a good surrogate. Unfortunately, whether ZO gradients can work with the hard-thresholding operator is still an unsolved problem.\nTo solve this puzzle, in this paper, we focus on the $\\ell_0$ constrained black-box stochastic optimization problems, and propose a new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm with  a general ZO gradient estimator powered by a novel random support sampling. We provide the convergence analysis of SZOHT under standard assumptions.   Importantly, we   reveal a conflict between  the deviation of  ZO estimators and  the expansivity of the hard-thresholding operator,  and provide a theoretical   minimal value of the number of random directions in ZO gradients. In addition,  we find that the query complexity of SZOHT is independent or weakly dependent on the dimensionality under different settings.  Finally, we illustrate the utility of our method on a portfolio optimization problem as well as black-box adversarial attacks."}}
{"id": "Setj8nJ-YB8", "cdate": 1652737442762, "mdate": null, "content": {"title": "Zeroth-Order Negative Curvature Finding: Escaping Saddle Points  without Gradients", "abstract": "We consider escaping saddle points of nonconvex problems where only the function evaluations can be accessed. Although a variety of works have been proposed, the majority of them require either second or first-order information, and only a few of them have exploited zeroth-order methods, particularly the technique of negative curvature finding with zeroth-order methods which has been proven to be the most efficient method for escaping saddle points. To fill this gap,  in this paper, we propose two zeroth-order negative curvature finding frameworks that can replace Hessian-vector product computations without increasing the iteration complexity. We apply the proposed frameworks to ZO-GD, ZO-SGD, ZO-SCSG, ZO-SPIDER and prove that these ZO algorithms can converge to $(\\epsilon,\\delta)$-approximate second-order stationary points with less query complexity compared with prior zeroth-order works for finding local minima."}}
{"id": "HKLcvLY-Rhk", "cdate": 1640995200000, "mdate": 1681194459667, "content": {"title": "Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity", "abstract": ""}}
{"id": "66nNnSH25x", "cdate": 1640995200000, "mdate": 1681194459677, "content": {"title": "Zeroth-Order Negative Curvature Finding: Escaping Saddle Points without Gradients", "abstract": ""}}
