{"id": "p3miKOi5kt", "cdate": 1672531200000, "mdate": 1695952700886, "content": {"title": "LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images", "abstract": "We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pretrained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet."}}
{"id": "TD0ndor2hYv", "cdate": 1672531200000, "mdate": 1697166872083, "content": {"title": "Benchmarking Low-Shot Robustness to Natural Distribution Shifts", "abstract": "Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance."}}
{"id": "Fj1S0SV8p3U", "cdate": 1663850459524, "mdate": null, "content": {"title": "Augmentation Curriculum Learning For Generalization in RL", "abstract": "Many Reinforcement Learning tasks rely solely on pixel-based observations of\nthe environment. During deployment, these observations can fall victim to visual\nperturbations and distortions, causing the agent\u2019s policy to significantly degrade\nin performance. This motivates the need for robust agents that can generalize in\nthe face of visual distribution shift. One common technique for doing this is to ap-\nply augmentations during training; however, it comes at the cost of performance.\nWe propose Augmentation Curriculum Learning a novel curriculum learning ap-\nproach that schedules augmentation into training into a weak augmentation phase\nand strong augmentation phase. We also introduce a novel visual augmentation\nstrategy that proves to aid in the benchmarks we evaluate on. Our method achieves\nstate-of-the-art performance on Deep Mind Control Generalization Benchmark."}}
{"id": "5FqeE2SojJi", "cdate": 1663850234721, "mdate": null, "content": {"title": "Proportional Amplitude Spectrum Training Augmentation for Synthetic-to-Real Domain Generalization", "abstract": "Synthetic data offers the promise of cheap and bountiful training data for settings where lots of labeled real-world data for some task is unavailable. However, models trained on synthetic data significantly underperform on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA involves perturbing the amplitude spectrums of the synthetic images in the Fourier domain to generate augmented views. We design PASTA to perturb the amplitude spectrums in a structured manner such that high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV\u2192Real), object detection (Sim10K\u2192Real), and object recognition (VisDAC Syn\u2192Real), across a total of 5 syn-to-real shifts, we find that PASTA  either outperforms or is consistently competitive with more complex state-of-the-art methods while being complementary to other generalization approaches."}}
{"id": "tzOLzWrkEvU", "cdate": 1640995200000, "mdate": 1697166872073, "content": {"title": "PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization", "abstract": "Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complementary to the same."}}
{"id": "Un_1CCBMFvO", "cdate": 1622220826807, "mdate": null, "content": {"title": "Likelihood Landscapes: A Unifying Principle Behind Many Adversarial Defenses", "abstract": "Convolutional Neural Networks have been shown to be vulnerable to adversarial examples, which are known to locate in subspaces close to where normal data lies but are not naturally occurring and of low probability. In this work, we investigate the potential effect defense techniques have on the geometry of the likelihood landscape - likelihood of the input images under the trained model. We first propose a way to visualize the likelihood landscape leveraging an energy-based model interpretation of discriminative classifiers. Then we introduce a measure to quantify the flatness of the likelihood landscape. We observe that a subset of adversarial defense techniques results in a similar effect of flattening the likelihood landscape. We further explore directly regularizing towards a flat landscape for adversarial robustness."}}
{"id": "0G7V5VDNBtB", "cdate": 1609459200000, "mdate": 1662576779773, "content": {"title": "RobustNav: Towards Benchmarking Robustness in Embodied Navigation", "abstract": "As an attempt towards assessing the robustness of embodied navigation agents, we propose RobustNav, a framework to quantify the performance of embodied navigation agents when exposed to a wide variety of visual \u2013 affecting RGB inputs \u2013 and dynamics \u2013 affecting transition dynamics \u2013 corruptions. Most recent efforts in visual navigation have typically focused on generalizing to novel target environments with similar appearance and dynamics characteristics. With RobustNav, we find that some standard embodied navigation agents significantly underperform (or fail) in the presence of visual or dynamics corruptions. We systematically analyze the kind of idiosyncrasies that emerge in the behavior of such agents when operating under corruptions. Finally, for visual corruptions in RobustNav, we show that while standard techniques to improve robustness such as data-augmentation and self-supervised adaptation offer some zero-shot resistance and improvements in navigation performance, there is still a long way to go in terms of recovering lost performance relative to clean \"non-corrupt\" settings, warranting more research in this direction. Our code is available at https://github.com/allenai/robustnav."}}
{"id": "-joJT7lMS3b", "cdate": 1609459200000, "mdate": 1662576779777, "content": {"title": "RobustNav: Towards Benchmarking Robustness in Embodied Navigation", "abstract": "As an attempt towards assessing the robustness of embodied navigation agents, we propose RobustNav, a framework to quantify the performance of embodied navigation agents when exposed to a wide variety of visual - affecting RGB inputs - and dynamics - affecting transition dynamics - corruptions. Most recent efforts in visual navigation have typically focused on generalizing to novel target environments with similar appearance and dynamics characteristics. With RobustNav, we find that some standard embodied navigation agents significantly underperform (or fail) in the presence of visual or dynamics corruptions. We systematically analyze the kind of idiosyncrasies that emerge in the behavior of such agents when operating under corruptions. Finally, for visual corruptions in RobustNav, we show that while standard techniques to improve robustness such as data-augmentation and self-supervised adaptation offer some zero-shot resistance and improvements in navigation performance, there is still a long way to go in terms of recovering lost performance relative to clean \"non-corrupt\" settings, warranting more research in this direction. Our code is available at https://github.com/allenai/robustnav"}}
{"id": "wt_JnQnRmU3", "cdate": 1601908329200, "mdate": null, "content": {"title": "Evaluating visual conversational agents via cooperative human-ai games", "abstract": "As AI continues to advance, human-AI teams are inevitable.\nHowever, progress in AI is routinely measured in isolation,\nwithout a human in the loop. It is crucial to benchmark\nprogress in AI, not just in isolation, but also in terms of how\nit translates to helping humans perform certain tasks, i.e., the\nperformance of human-AI teams.\nIn this work, we design a cooperative game \u2013 GuessWhich \u2013\nto measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the\nAI. The AI, which we call ALICE, is provided an image which\nis unseen by the human. Following a brief description of the\nimage, the human questions ALICE about this secret image to\nidentify it from a fixed pool of images.\nWe measure performance of the human-ALICE team by the\nnumber of guesses it takes the human to correctly identify the\nsecret image after a fixed number of dialog rounds with ALICE. We compare performance of the human-ALICE teams for\ntwo versions of ALICE. Our human studies suggest a counterintuitive trend \u2013 that while AI literature shows that one version outperforms the other when paired with an AI questioner\nbot, we find that this improvement in AI-AI performance does\nnot translate to improved human-AI performance. This suggests a mismatch between benchmarking of AI in isolation\nand in the context of human-AI teams."}}
{"id": "oyrKURfah_l", "cdate": 1577836800000, "mdate": 1662576779788, "content": {"title": "Learning to Balance Specificity and Invariance for In and Out of Domain Generalization", "abstract": "We introduce Domain-specific Masks for Generalization, a model for improving both in-domain and out-of-domain generalization performance. For domain generalization, the goal is to learn from a set of source domains to produce a single model that will best generalize to an unseen target domain. As such, many prior approaches focus on learning representations which persist across all source domains with the assumption that these domain agnostic representations will generalize well. However, often individual domains contain characteristics which are unique and when leveraged can significantly aid in-domain recognition performance. To produce a model which best generalizes to both seen and unseen domains, we propose learning domain specific masks. The masks are encouraged to learn a balance of domain-invariant and domain-specific features, thus enabling a model which can benefit from the predictive power of specialized features while retaining the universal applicability of domain-invariant features. We demonstrate competitive performance compared to naive baselines and state-of-the-art methods on both PACS and DomainNet."}}
