{"id": "ErzyBArv6Ue", "cdate": 1664310940009, "mdate": null, "content": {"title": "Score-Based Generative Models with L\u00e9vy Processes", "abstract": "Time reversibility of stochastic processes is a primary cornerstone of the score-based generative models through stochastic differential equations (SDEs). \nWhile a broader class of Markov processes is reversible, previous continuous-time approaches restrict the range of noise processes to Brownian motion (BM) since the closed-form of the time reversal formula is only known for diffusion processes. \nIn this paper, to expand the class of noise distribution, we propose a class of score-based probabilistic generative models, L\u00e9vy-It\u014d Model (LIM), which utilizes $\\alpha$-stable distribution for noise injection. \nTo this end, we derive an approximate time reversal formula for the SDEs with L\u00e9vy processes that can allow discontinuous pure jump motion.\nConsequently, we advance the score-based generative models with a broad range of non-Gaussian Markov processes.\nEmpirical results on MNIST, CIFAR-10, CelebA, and CelebA-HQ show that our approach is valid. "}}
{"id": "VezcnWFSd2d", "cdate": 1663850254591, "mdate": null, "content": {"title": "On Threshold Functions in Learning to Generate Feasible Solutions of Mixed Integer Programs", "abstract": "Finding a high-quality feasible solution to a combinatorial optimization problem in a given time budget is a challenging task due to its discrete nature. Neural diving is a learning-based approach to generating partial assignments for the discrete variables in MIP. We find that there usually is a small range of selection rates which lead to feasible and optimal solutions; when too many parameters are selected, the solution space is too restricted to find a feasible solution; when too few parameters are selected, the solution space is too wide to efficiently find a feasible solution. Therefore, the choice of selection rate is the critical determinant of the Neural diving performance. In this context, we present theoretical insights that there exist threshold functions in feasibility and feasible optimality over the selection rate. Based on the theoretical foundations, we introduce a post-hoc method, and a learning-based approach to optimize the selection rate for partial discrete variable assignments in MIP more efficiently. A key idea is to jointly learn to restrict the selection rate search space, and to predict the selection rate in the learned search space that results in a high-quality feasible solution. MIP solver is integrated into the end-to-end learning framework. We suggest that learning a deep neural network to generate a threshold-aware selection rate is effective in finding high-quality feasible solutions more quickly. Experimental results demonstrate that our method achieves state-of-the-art performance in NeurIPS ML4CO datasets. In the workload apportionment dataset, our method achieves the optimality gap of 0.45%, which is around 10\u00d7 better than SCIP, at the one-minute time limit."}}
{"id": "Z7VhFVRVqeU", "cdate": 1632875741667, "mdate": null, "content": {"title": "Neural Bootstrapping Attention for Neural Processes", "abstract": "Neural Processes (NP) learn to fit a broad class of stochastic processes with neural networks. Modeling functional uncertainty is an important aspect of learning stochastic processes. Recently, Bootstrapping (Attentive) Neural Processes (B(A)NP) propose a bootstrap method to capture the functional uncertainty which can replace the latent variable in (Attentive) Neural Processes ((A)NP), thus overcoming the limitations of Gaussian assumption on the latent variable. However, B(A)NP conduct bootstrapping in a non-parallelizable and memory-inefficient way and fail to capture diverse patterns in the stochastic processes. Furthermore, we found that ANP and BANP both tend to overfit in some cases. To resolve these problems, we propose an efficient and easy-to-implement approach, Neural Bootstrapping Attentive Neural Processes (NeuBANP). NeuBANP learns to generate the bootstrap distribution of random functions by injecting multiple random weights into the encoder and the loss function. We evaluate our models in benchmark experiments including Bayesian optimization and contextual multi-armed bandit. NeuBANP achieves state-of-the-art performance in both of the sequential decision-making tasks, and this empirically shows that our method greatly improves the quality of functional uncertainty modeling."}}
{"id": "b4jq1xzirPS", "cdate": 1632875512889, "mdate": null, "content": {"title": "An Attention-LSTM Hybrid Model for the Coordinated Routing of Multiple Vehicles", "abstract": "Reinforcement learning has recently shown promise in learning quality solutions in a number of combinatorial optimization problems. In particular, the attention-based encoder-decoder models show high effectiveness on various routing problems, including the Traveling Salesman Problem (TSP).  Unfortunately, they perform poorly for the TSP with Drones (TSP-D), requiring routing a heterogeneous fleet of vehicles in coordination. In TSP-D, two different types of vehicles are moving in tandem and may need to wait at a node for the other vehicle to join. State-less attention-based decoder fails to make such coordination between vehicles. We propose an attention encoder-LSTM decoder hybrid model, in which the decoder's hidden state can represent the sequence of actions made. We empirically demonstrate that such a hybrid model improves upon a purely attention-based model for both solution quality and computational efficiency. Our experiments on the min-max Capacitated Vehicle Routing Problem (mmCVRP) also confirm that the hybrid model is more suitable for coordinated routing of multiple vehicles than the attention-based model."}}
{"id": "Hk2oOy4GJlH", "cdate": 1621629873332, "mdate": null, "content": {"title": "Neural Bootstrapper", "abstract": "Bootstrapping has been a primary tool for ensemble and uncertainty quantification in machine learning and statistics. \nHowever, due to its nature of multiple training and resampling, bootstrapping deep neural networks is computationally burdensome; hence it has difficulties in practical application to the uncertainty estimation and related tasks. To overcome this computational bottleneck, we propose a novel approach called Neural Bootstrapper (NeuBoots), which learns to generate bootstrapped neural networks through single model training. NeuBoots injects the bootstrap weights into the high-level feature layers of the backbone network and outputs the bootstrapped predictions of the target, without additional parameters and the repetitive computations from scratch. We apply NeuBoots to various machine learning tasks related to uncertainty quantification, including prediction calibrations in image classification and semantic segmentation, active learning, and detection of out-of-distribution samples. Our empirical results show that NeuBoots outperforms other bagging based methods under a much lower computational cost without losing the validity of bootstrapping."}}
{"id": "M6PP1Gq076C", "cdate": 1601308323126, "mdate": null, "content": {"title": "Neural Bootstrapper", "abstract": "Bootstrapping has been a primary tool for uncertainty quantification, and their theoretical and computational properties have been investigated in the field of statistics and machine learning. However, due to its nature of repetitive computations, the computational burden required to implement bootstrap procedures for the neural network is painfully heavy, and this fact seriously hurdles the practical use of these procedures on the uncertainty estimation of modern deep learning. To overcome the inconvenience, we propose a procedure called \\emph{Neural Bootstrapper} (NeuBoots). We reveal that the NeuBoots stably generate valid bootstrap samples that coincide with the desired target samples with minimal extra computational cost compared to traditional bootstrapping. \nConsequently, NeuBoots makes it feasible to construct bootstrap confidence intervals of outputs of neural networks and quantify their predictive uncertainty. We also suggest NeuBoots for deep convolutional neural networks to consider its utility in image classification tasks, including calibration, detection of out-of-distribution samples, and active learning. Empirical results demonstrate that NeuBoots is significantly beneficial for the above purposes.  "}}
{"id": "otvlmDIbDk", "cdate": 1582513082850, "mdate": null, "content": {"title": "Fast AutoAugment", "abstract": "Data augmentation is an essential technique for improving generalization ability\nof deep learning models. Recently, AutoAugment [3] has been proposed as an\nalgorithm to automatically search for augmentation policies from a dataset and has\nsignificantly enhanced performances on many image recognition tasks. However,\nits search method requires thousands of GPU hours even for a relatively small\ndataset. In this paper, we propose an algorithm called Fast AutoAugment that finds\neffective augmentation policies via a more efficient search strategy based on density\nmatching. In comparison to AutoAugment, the proposed algorithm speeds up the\nsearch time by orders of magnitude while achieves comparable performances on\nimage recognition tasks with various models and datasets including CIFAR-10,\nCIFAR-100, SVHN, and ImageNet."}}
{"id": "qKVPF9V-Uhq", "cdate": 1577836800000, "mdate": null, "content": {"title": "AutoCLINT: The Winning Method in AutoCV Challenge 2019", "abstract": "NeurIPS 2019 AutoDL challenge is a series of six automated machine learning competitions. Particularly, AutoCV challenges mainly focused on classification tasks on visual domain. In this paper, we introduce the winning method in the competition, AutoCLINT. The proposed method implements an autonomous training strategy, including efficient code optimization, and applies an automated data augmentation to achieve the fast adaptation of pretrained networks. We implement a light version of Fast AutoAugment to search for data augmentation policies efficiently for the arbitrarily given image domains. We also empirically analyze the components of the proposed method and provide ablation studies focusing on AutoCV datasets."}}
{"id": "dAAJUzckIfS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Monte Carlo Tree Search in Continuous Spaces Using Voronoi Optimistic Optimization with Regret Bounds", "abstract": "Many important applications, including robotics, data-center management, and process control, require planning action sequences in domains with continuous state and action spaces and discontinuous objective functions. Monte Carlo tree search (MCTS) is an effective strategy for planning in discrete action spaces. We provide a novel MCTS algorithm (voot) for deterministic environments with continuous action spaces, which, in turn, is based on a novel black-box function-optimization algorithm (voo) to efficiently sample actions. The voo algorithm uses Voronoi partitioning to guide sampling, and is particularly efficient in high-dimensional spaces. The voot algorithm has an instance of voo at each node in the tree. We provide regret bounds for both algorithms and demonstrate their empirical effectiveness in several high-dimensional problems including two difficult robotics planning problems."}}
{"id": "M1MdFc915-G", "cdate": 1577836800000, "mdate": null, "content": {"title": "Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks", "abstract": "In this paper, we focus on weakly supervised learning with noisy training data for both classification and regression problems. We assume that the training outputs are collected from a mixture of a target and correlated noise distributions. Our proposed method simultaneously estimates the target distribution and the quality of each data which is defined as the correlation between the target and data generating distributions. The cornerstone of the proposed method is a Cholesky Block that enables modeling dependencies among mixture distributions in a differentiable manner where we maintain the distribution over the network weights. We first provide illustrative examples in both regression and classification tasks to show the effectiveness of the proposed method. Then, the proposed method is extensively evaluated in a number of experiments where we show that it constantly shows comparable or superior performances compared to existing baseline methods in the handling of noisy data."}}
