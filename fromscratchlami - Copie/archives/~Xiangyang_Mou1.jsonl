{"id": "yOb7let8of", "cdate": 1640995200000, "mdate": 1682327903989, "content": {"title": "MBTI Personality Prediction for Fictional Characters Using Movie Scripts", "abstract": "An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character's MBTI or Big 5 personality types based on the narratives of the character. Experiments show that our task is challenging for the existing text classification models, as none is able to largely outperform random guesses. We further proposed a multi-view model for personality prediction using both verbal and non-verbal descriptions, which gives improvement compared to using only verbal descriptions. The uniqueness and challenges in our dataset call for the development of narrative comprehension techniques from the perspective of understanding characters."}}
{"id": "yFkd-_lFSx", "cdate": 1640995200000, "mdate": 1682327904127, "content": {"title": "Efficient Long Sequence Encoding via Synchronization", "abstract": "Pre-trained Transformer models have achieved successes in a wide range of NLP tasks, but are inefficient when dealing with long input sequences. Existing studies try to overcome this challenge via segmenting the long sequence followed by hierarchical encoding or post-hoc aggregation. We propose a synchronization mechanism for hierarchical encoding. Our approach first identifies anchor tokens across segments and groups them by their roles in the original input sequence. Then inside Transformer layer, anchor embeddings are synchronized within their group via a self-attention module. Our approach is a general framework with sufficient flexibility -- when adapted to a new task, it is easy to be enhanced with the task-specific anchor definitions. Experiments on two representative tasks with different types of long input texts, NarrativeQA summary setting and wild multi-hop reasoning from HotpotQA, demonstrate that our approach is able to improve the global information exchange among segments while maintaining efficiency."}}
{"id": "uLJ8VqUhTP", "cdate": 1640995200000, "mdate": 1682327903971, "content": {"title": "A Survey of Machine Narrative Reading Comprehension Assessments", "abstract": ""}}
{"id": "nB8uyqULZDf", "cdate": 1640995200000, "mdate": 1682327904172, "content": {"title": "A Survey of Machine Narrative Reading Comprehension Assessments", "abstract": "As the body of research on machine narrative comprehension grows, there is a critical need for consideration of performance assessment strategies as well as the depth and scope of different benchmark tasks. Based on narrative theories, reading comprehension theories, as well as existing machine narrative reading comprehension tasks and datasets, we propose a typology that captures the main similarities and differences among assessment tasks; and discuss the implications of our typology for new task design and the challenges of narrative reading comprehension."}}
{"id": "JL30GWVkzs", "cdate": 1640995200000, "mdate": 1682327903935, "content": {"title": "MBTI Personality Prediction for Fictional Characters Using Movie Scripts", "abstract": ""}}
{"id": "AVYBZ_jIldK", "cdate": 1640995200000, "mdate": 1682327903910, "content": {"title": "TVShowGuess: Character Comprehension in Stories as Speaker Guessing", "abstract": "Yisi Sang, Xiangyang Mou, Mo Yu, Shunyu Yao, Jing Li, Jeffrey Stanton. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "9fhjEMOumh", "cdate": 1640995200000, "mdate": 1682327904055, "content": {"title": "TVShowGuess: Character Comprehension in Stories as Speaker Guessing", "abstract": "We propose a new task for assessing machines' skills of understanding fictional characters in narrative stories. The task, TVShowGuess, builds on the scripts of TV series and takes the form of guessing the anonymous main characters based on the backgrounds of the scenes and the dialogues. Our human study supports that this form of task covers comprehension of multiple types of character persona, including understanding characters' personalities, facts and memories of personal experience, which are well aligned with the psychological and literary theories about the theory of mind (ToM) of human beings on understanding fictional characters during reading. We further propose new model architectures to support the contextualized encoding of long scene texts. Experiments show that our proposed approaches significantly outperform baselines, yet still largely lag behind the (nearly perfect) human performance. Our work serves as a first step toward the goal of narrative character comprehension."}}
{"id": "albMHJa-vUD", "cdate": 1609459200000, "mdate": 1639509980940, "content": {"title": "Complementary Evidence Identification in Open-Domain Question Answering", "abstract": "This paper proposes a new problem of complementary evidence identification for open-domain question answering (QA). The problem aims to efficiently find a small set of passages that covers full evidence from multiple aspects as to answer a complex question. To this end, we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set, in addition to the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain."}}
{"id": "9xTniqtXxy8", "cdate": 1609459200000, "mdate": 1682327903943, "content": {"title": "Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study", "abstract": "Recent advancements in open-domain question answering (ODQA), that is, finding answers from large open-domain corpus like Wikipedia, have led to human-level performance on many datasets. However, progress in QA over book stories (Book QA) lags despite its similar task formulation to ODQA. This work provides a comprehensive and quantitative analysis about the difficulty of Book QA: (1) We benchmark the research on the NarrativeQA dataset with extensive experiments with cutting-edge ODQA techniques. This quantifies the challenges Book QA poses, as well as advances the published state-of-the-art with a \u223c7% absolute improvement on ROUGE-L. (2) We further analyze the detailed challenges in Book QA through human studies.1 Our findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios."}}
{"id": "8pbq-DrGJhN", "cdate": 1609459200000, "mdate": 1634763396370, "content": {"title": "Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study", "abstract": "Recent advancements in open-domain question answering (ODQA), i.e., finding answers from large open-domain corpus like Wikipedia, have led to human-level performance on many datasets. However, progress in QA over book stories (Book QA) lags behind despite its similar task formulation to ODQA. This work provides a comprehensive and quantitative analysis about the difficulty of Book QA: (1) We benchmark the research on the NarrativeQA dataset with extensive experiments with cutting-edge ODQA techniques. This quantifies the challenges Book QA poses, as well as advances the published state-of-the-art with a $\\sim$7\\% absolute improvement on Rouge-L. (2) We further analyze the detailed challenges in Book QA through human studies.\\footnote{\\url{https://github.com/gorov/BookQA}.} Our findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios."}}
