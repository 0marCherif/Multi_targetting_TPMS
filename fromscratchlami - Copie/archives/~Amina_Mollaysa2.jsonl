{"id": "QGmPoN4gJZd", "cdate": 1672531200000, "mdate": 1682340168089, "content": {"title": "SimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting", "abstract": ""}}
{"id": "aVyUq3N1DTC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Goal-directed Generation of Discrete Structures with Conditional Generative Models", "abstract": "Despite recent advances, goal-directed generation of structured discrete data remains challenging. For problems such as program synthesis (generating source code) and materials design (generating molecules), finding examples which satisfy desired constraints or exhibit desired properties is difficult. In practice, expensive heuristic search or reinforcement learning algorithms are often employed. In this paper we investigate the use of conditional generative models which directly attack this inverse problem, by modeling the distribution of discrete structures given properties of interest. Unfortunately, maximum likelihood training of such models often fails with the samples from the generative model inadequately respecting the input properties. To address this, we introduce a novel approach to directly optimize a reinforcement learning objective, maximizing an expected reward. We avoid high-variance score-function estimators that would otherwise be required by sampling from an approximation to the normalized rewards, allowing simple Monte Carlo estimation of model gradients. We test our methodology on two tasks: generating molecules with user-defined properties and identifying short python expressions which evaluate to a given target value. In both cases, we find improvements over maximum likelihood estimation and other baselines."}}
{"id": "6WvAXqTxPcu", "cdate": 1577836800000, "mdate": null, "content": {"title": "Goal-directed Generation of Discrete Structures with Conditional Generative Models", "abstract": "Despite recent advances, goal-directed generation of structured discrete data remains challenging. For problems such as program synthesis (generating source code) and materials design (generating molecules), finding examples which satisfy desired constraints or exhibit desired properties is difficult. In practice, expensive heuristic search or reinforcement learning algorithms are often employed. In this paper, we investigate the use of conditional generative models which directly attack this inverse problem, by modeling the distribution of discrete structures given properties of interest. Unfortunately, the maximum likelihood training of such models often fails with the samples from the generative model inadequately respecting the input properties. To address this, we introduce a novel approach to directly optimize a reinforcement learning objective, maximizing an expected reward. We avoid high-variance score-function estimators that would otherwise be required by sampling from an approximation to the normalized rewards, allowing simple Monte Carlo estimation of model gradients. We test our methodology on two tasks: generating molecules with user-defined properties and identifying short python expressions which evaluate to a given target value. In both cases, we find improvements over maximum likelihood estimation and other baselines."}}
{"id": "BkxthxHYvr", "cdate": 1569439920685, "mdate": null, "content": {"title": "Conditional generation of molecules from disentangled representations", "abstract": "Though machine learning approaches have shown great success in estimating properties of small molecules, the inverse problem of generating molecules with desired properties remains challenging. This difficulty is in part because the set of molecules which have a given property is structurally very diverse. Treating this inverse problem as a conditional distribution estimation task, we draw upon work in learning disentangled representations to learn a conditional distribution over molecules given a desired property, where the molecular structure is encoded in a continuous latent random variable. By including property information as an input factor independent from the structure representation, one can perform conditional molecule generation via a ``style transfer'' process, in which we explicitly set the property to a desired value at generation time. In contrast to existing approaches, we disentangle the latent factors from the property factors using a regularization term which constrains the generated molecules to have the property provided to the  generation network, no matter how the latent factor changes."}}
{"id": "VdGki9LdTNe", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning to Augment with Feature Side-information", "abstract": "Neural networks typically need huge amounts of data to train in order to get reasonable generalizable results. A common approach is to artificially generate samples by using prior knowledge of the ..."}}
{"id": "a2CbA2uzd3", "cdate": 1483228800000, "mdate": 1682340168157, "content": {"title": "Regularising Non-linear Models Using Feature Side-information", "abstract": ""}}
