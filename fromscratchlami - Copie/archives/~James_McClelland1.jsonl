{"id": "G7VnBCk0V3j", "cdate": 1694385289293, "mdate": 1694385289293, "content": {"title": "Placing language in an integrated understanding system", "abstract": "Language is crucial for human intelligence, but what exactly is its role? We take language to be a part of\na system for understanding and communicating about situations. In humans, these abilities emerge gradually\nfrom experience and depend on domain-general principles of biological neural networks: connection-based\nlearning, distributed representation, and context-sensitive, mutual constraint satisfaction-based processing.\nCurrent artificial language processing systems rely on the same domain general principles, embodied in artificial\nneural networks. Indeed, recent progress in this field depends on query-based attention, which extends the\nability of these systems to exploit context and has contributed to remarkable breakthroughs. Nevertheless,\nmost current models focus exclusively on language-internal tasks, limiting their ability to perform tasks that\ndepend on understanding situations. These systems also lack memory for the contents of prior situations\noutside of a fixed contextual span. We describe the organization of the brain\u2019s distributed understanding\nsystem, which includes a fast learning system that addresses the memory problem. We sketch a framework\nfor future models of understanding drawing equally on cognitive neuroscience and artificial intelligence and\nexploiting query-based attention. We highlight relevant current directions and consider further developments\nneeded to fully capture human-level language understanding in a computational system."}}
{"id": "qCi50QTfX3", "cdate": 1672143584777, "mdate": 1672143584777, "content": {"title": "Language models show human-like content effects on reasoning", "abstract": "Abstract reasoning is a key ability for an intelligent system. Large language models achieve abovechance performance on abstract reasoning tasks, but exhibit many imperfections. However, human\nabstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of\nthe reasoning problem. For example, humans reason much more reliably about logical rules that are\ngrounded in everyday situations than arbitrary rules about abstract attributes. The training experiences\nof language models similarly endow them with prior expectations that reflect human knowledge and\nbeliefs. We therefore hypothesized that language models would show human-like content effects on\nabstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason,\n1968). We find that state of the art large language models (with 7 or 70 billion parameters; Hoffmann\net al., 2022) reflect many of the same patterns observed in humans across these tasks \u2014 like humans,\nmodels reason more effectively about believable situations than unrealistic or abstract ones. Our findings have implications for understanding both these cognitive effects, and the factors that contribute\nto language model performance."}}
{"id": "et9T2bwf_v", "cdate": 1672143474007, "mdate": 1672143474007, "content": {"title": "Can language models learn from explanations in context?", "abstract": "Language Models (LMs) can perform new\ntasks by adapting to a few in-context examples.\nFor humans, explanations that connect examples to task principles can improve learning.\nWe therefore investigate whether explanations\nof few-shot examples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how different\ntypes of explanations, instructions, and controls affect zero- and few-shot performance.\nWe analyze these results using statistical multilevel modeling techniques that account for the\nnested dependencies among conditions, tasks,\nprompts, and models. We find that explanations can improve performance\u2014even without tuning. Furthermore, explanations handtuned for performance on a small validation set\noffer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform\ncarefully matched controls, suggesting that the\nbenefits are due to the link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit.\nIn summary, explanations can support the incontext lea"}}
{"id": "BTNaKmYdQmE", "cdate": 1664902718462, "mdate": null, "content": {"title": "Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks", "abstract": "Transformer networks have seen great success in natural language processing and machine vision, where task objectives such as next word prediction and image classification benefit from nuanced context sensitivity across high-dimensional inputs. However, there is an ongoing debate about how and when transformers can acquire highly structured behavior and achieve systematic generalization. Here, we explore how well a causal transformer can perform a set of algorithmic tasks, including copying, sorting, and hierarchical compositions of these operations. We demonstrate strong generalization to sequences longer than those used in training by replacing the standard positional encoding typically used in transformers with labels arbitrarily paired with items in the sequence. We searched for the layer and head configuration sufficient to solve the task, and performed attention ablation and analyzed encoded representations. We show that two-layer transformers learn generalizable solutions to multi-level problems, develop signs of systematic task decomposition, and exploit shared computation across related tasks. These results provide key insights into the possible structures of within-task and cross-task computations that stacks of attention layers can afford."}}
{"id": "pXDmbfVL_SB", "cdate": 1663850457066, "mdate": null, "content": {"title": "Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks", "abstract": "Transformer networks have seen great success in natural language processing and machine vision, where task objectives such as next word prediction and image classification benefit from nuanced context sensitivity across high-dimensional inputs. However, there is an ongoing debate about how and when transformers can acquire highly structured behavior and achieve systematic generalization. Here, we explore how well a causal transformer can perform a set of algorithmic tasks, including copying, sorting, and hierarchical compositions of these operations. We demonstrate strong generalization to sequences longer than those used in training by replacing the standard positional encoding typically used in transformers with labels arbitrarily paired with items in the sequence. By finding the layer and head configuration sufficient to solve the task, then performing ablation experiments and representation analysis, we show that two-layer transformers learn generalizable solutions to multi-level problems and develop signs of systematic task decomposition. They also exploit shared computation across related tasks. These results provide key insights into how transformer models may be capable of decomposing complex decisions into reusable, multi-level policies in tasks requiring structured behavior."}}
{"id": "3BOwNcqM_Wq", "cdate": 1663850421163, "mdate": null, "content": {"title": "Learning to reason with relational abstractions", "abstract": "Large language models have recently shown promising progress in mathematical reasoning when fine-tuned with human-generated sequences walking through a sequence of solution steps. However, the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce.\nIn this paper, we study how to build stronger reasoning capability in language models using the idea of relational abstractions. We introduce new types of sequences that more explicitly provide an abstract characterization of the transitions through intermediate solution steps to the goal state. We found that models that are supplied with such sequences as prompts can solve tasks with a significantly higher accuracy, and models that are trained to produce such sequences solve problems better than those that are trained with previously used human-generated sequences and other baselines. Our work thus takes several steps toward elucidating and improving how language models perform on tasks requiring multi-step mathematical reasoning."}}
{"id": "lHj-q9BSRjF", "cdate": 1652737415529, "mdate": null, "content": {"title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers", "abstract": "Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having a large number of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. Our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and indicate how future work might encourage both in-context and in-weights learning in domains beyond language. "}}
{"id": "5SbYHHaP20I", "cdate": 1620376290098, "mdate": null, "content": {"title": "Do estimates of numerosity really adhere to Weber\u2019s law? A reexamination of two case studies", "abstract": "Both humans and nonhuman animals can exhibit sensitivity to the approximate number of items in a visual array or events in a sequence, and across various paradigms, uncertainty in numerosity judgments increases with the number estimated or produced. The pattern of increase is usually described as exhibiting approximate adherence to Weber\u2019s law, such that uncertainty increases proportionally to the mean estimate, resulting in a constant coefficient of variation. Such a pattern has been proposed to be a signature characteristic of an innate \u201cnumber sense.\u201d We reexamine published behavioral data from two studies that have been cited as prototypical evidence of adherence to Weber\u2019s law and observe that in both cases variability increases less than this account would predict, as indicated by a decreasing coefficient of variation with an increase in number. We also consider evidence from numerosity discrimination studies that show deviations from the constant coefficient of variation pattern. Though behavioral data can sometimes exhibit approximate adherence to Weber\u2019s law, our findings suggest that such adherence is not a fixed characteristic of the mechanisms whereby humans and animals estimate numerosity. We suggest instead that the observed pattern of increase in variability with number depends on the circumstances of the task and stimuli, and reflects an adaptive ensemble of mechanisms composed to optimize performance under these circumstances."}}
{"id": "VrQqU4NUsvY", "cdate": 1620376044106, "mdate": null, "content": {"title": "Numerosity discrimination in deep neural networks: Initial competence, developmental refinement and experience statistics", "abstract": "Both humans and non\u2010human animals exhibit sensitivity to the approximate number of items in a visual array, as indexed by their performance in numerosity discrimination tasks, and even neonates can detect changes in numerosity. These findings are often interpreted as evidence for an innate \u2018number sense\u2019. However, recent simulation work has challenged this view by showing that human\u2010like sensitivity to numerosity can emerge in deep neural networks that build an internal model of the sensory data. This emergentist perspective posits a central role for experience in shaping our number sense and might explain why numerical acuity progressively increases over the course of development. Here we substantiate this hypothesis by introducing a progressive unsupervised deep learning algorithm, which allows us to model the development of numerical acuity through experience. We also investigate how the statistical distribution of numerical and non\u2010numerical features in natural environments affects the emergence of numerosity representations in the computational model. Our simulations show that deep networks can exhibit numerosity sensitivity prior to any training, as well as a progressive developmental refinement that is modulated by the statistical structure of the learning environment. To validate our simulations, we offer a refinement to the quantitative characterization of the developmental patterns observed in human children. Overall, our findings suggest that it may not be necessary to assume that animals are endowed with a dedicated system for processing numerosity, since domain\u2010general learning mechanisms can capture key characteristics others have attributed to an evolutionarily specialized number system."}}
{"id": "nchVlp5CPi", "cdate": 1596126548904, "mdate": null, "content": {"title": "Generative Continual Concept Learning.", "abstract": "After learning a concept, humans are also able to continually generalize their learned concepts to new domains by observing only a few labeled instances without any interference with the past learned knowledge. In contrast, learning concepts efficiently in a continual learning setting remains an open challenge for current Artificial Intelligence algorithms as persistent model retraining is necessary. Inspired by the ParallelDistributed Processing learning and the Complementary Learning Systems theories, we develop a computational model that is able to expand its previously learned concepts efficiently to new domains using a few labeled samples. We couple the newform of a concept to its past learned forms in an embedding space for effective continual learning. Doing so, a generative distribution is learned such that it is shared across the tasks in the embedding space and models the abstract concepts. This procedure enables the model to generate pseudo-data points to replay the past experience to tackle catastrophic forgetting."}}
