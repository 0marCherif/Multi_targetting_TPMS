{"id": "hPU1pQFvvpw", "cdate": 1640995200000, "mdate": 1682343545163, "content": {"title": "Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval", "abstract": "Sketch-based 3D shape retrieval is a challenging task due to the large domain discrepancy between sketches and 3D shapes. Since existing methods are trained and evaluated on the same categories, they cannot effectively recognize the categories that have not been used during training. In this paper, we propose a novel domain disentangled generative adversarial network (DD-GAN) for zero-shot sketch-based 3D retrieval, which can retrieve the unseen categories that are not accessed during training. Specifically, we first generate domain-invariant features and domain-specific features by disentangling the learned features of sketches and 3D shapes, where the domain-invariant features are used to align with the corresponding word embeddings. Then, we develop a generative adversarial network that combines the domain-specific features of the seen categories with the aligned domain-invariant features to synthesize samples, where the synthesized samples of the unseen categories are generated by using the corresponding word embeddings. Finally, we use the synthesized samples of the unseen categories combined with the real samples of the seen categories to train the network for retrieval, so that the unseen categories can be recognized. In order to reduce the domain shift problem, we utilized unlabeled unseen samples to enhance the discrimination ability of the discriminator. With the discriminator distinguishing the generated samples from the unlabeled unseen samples, the generator can generate more realistic unseen samples. Extensive experiments on the SHREC'13 and SHREC'14 datasets show that our method significantly improves the retrieval performance of the unseen categories."}}
{"id": "IuwOH9aqaN", "cdate": 1640995200000, "mdate": 1680056750449, "content": {"title": "Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval", "abstract": ""}}
{"id": "EbeVnPX3xi", "cdate": 1640995200000, "mdate": 1681555742249, "content": {"title": "Semantic Contrastive Embedding for Generalized Zero-Shot Learning", "abstract": ""}}
{"id": "SzE9_Mo7W4Q", "cdate": 1609459200000, "mdate": 1668686572017, "content": {"title": "Contrastive Embedding for Generalized Zero-Shot Learning", "abstract": "Generalized zero-shot learning (GZSL) aims to recognize objects from both seen and unseen classes, when only the labeled examples from seen classes are provided. Recent feature generation methods learn a generative model that can synthesize the missing visual features of unseen classes to mitigate the data-imbalance problem in GZSL. However, the original visual feature space is suboptimal for GZSL classification since it lacks discriminative information. To tackle this issue, we propose to integrate the generation model with the embedding model, yielding a hybrid GZSL framework. The hybrid GZSL approach maps both the real and the synthetic samples produced by the generation model into an embedding space, where we perform the final GZSL classification. Specifically, we propose a contrastive embedding (CE) for our hybrid GZSL framework. The proposed contrastive embedding can leverage not only the class-wise supervision but also the instance-wise supervision, where the latter is usually neglected by existing GZSL researches. We evaluate our proposed hybrid GZSL framework with contrastive embedding, named CE-GZSL, on five benchmark datasets. The results show that our CEGZSL method can outperform the state-of-the-arts by a significant margin on three datasets. Our codes are available on https://github.com/Hanzy1996/CE-GZSL."}}
{"id": "75YPCLPN3j7", "cdate": 1609459200000, "mdate": 1681555742251, "content": {"title": "Contrastive Embedding for Generalized Zero-Shot Learning", "abstract": ""}}
{"id": "4j62oIToTU", "cdate": 1609459200000, "mdate": 1681651884565, "content": {"title": "Inference guided feature generation for generalized zero-shot learning", "abstract": ""}}
{"id": "r_H_hj5XulP", "cdate": 1577836800000, "mdate": 1695959291844, "content": {"title": "Learning the Redundancy-free Features for Generalized Zero-Shot Object Recognition", "abstract": "Zero-shot object recognition or zero-shot learning aims to transfer the object recognition ability among the semantically related categories, such as fine-grained animal or bird species. However, the images of different fine-grained objects tend to merely exhibit subtle differences in appearance, which will severely deteriorate zero-shot object recognition. To reduce the superfluous information in the fine-grained objects, in this paper, we propose to learn the redundancy-free features for generalized zero-shot learning. We achieve our motivation by projecting the original visual features into a new (redundancy-free) feature space and then restricting the statistical dependence between these two feature spaces. Furthermore, we require the projected features to keep and even strengthen the category relationship in the redundancy-free feature space. In this way, we can remove the redundant information from the visual features without losing the discriminative information. We extensively evaluate the performance on four benchmark datasets. The results show that our redundancy-free feature based generalized zero-shot learning (RFF-GZSL) approach can achieve competitive results compared with the state-of-the-arts."}}
{"id": "g6xQn7JcYs7", "cdate": 1577836800000, "mdate": 1681651884569, "content": {"title": "Learning the Redundancy-Free Features for Generalized Zero-Shot Object Recognition", "abstract": ""}}
