{"id": "DA4Ay4rJfCL", "cdate": 1671498480303, "mdate": 1671498480303, "content": {"title": "CMG: A Class-Mixed Generation Approach to Out-of-Distribution Detection", "abstract": "Recently, contrastive learning with data and class augmentations has been shown to produce markedly better results for out-of-distribution (OOD) detection than previous approaches. However, a major shortcoming of this approach is that it is extremely slow due to the significant increase in data size and in the number of classes and the quadratic pairwise similarity computation. This paper shows that this heavy machinery is unnecessary. A novel approach, called CMG (Class-Mixed Generation), is proposed, which generates pseudo-OOD data by mixing class embeddings as abnormal conditions to CVAE (conditional variational Auto-Encoder) and then uses the data to fine-tune a classifier built using the given in-distribution (IND) data. To our surprise, the obvious approach of using the IND data and the pseudo-OOD data to directly train an OOD model is a very poor choice. The fine-tuning based approach turns out to be markedly better. Empirical evaluation shows that CMG not only produces new state-of-the-art results but also is much more efficient than contrastive learning, at least 10 times faster."}}
{"id": "SnBDX5k-KuJ", "cdate": 1663850488319, "mdate": null, "content": {"title": "Solving Continual Learning via Problem Decomposition", "abstract": "This paper is concerned with class incremental learning (CIL) in continual learning (CL). CIL is the popular continual learning paradigm in which a system receives a sequence of tasks with different classes in each task and is expected to learn to predict the class of each test instance without given any task related information for the instance. Although many techniques have been proposed to solve CIL, it remains to be highly challenging due to the difficulty of dealing with catastrophic forgetting (CF). This paper starts from the first principle and proposes a novel method to solve the problem. The definition of CIL reveals that the problem can be decomposed into two probabilities: within-task prediction probability and task-id prediction probability. This paper proposes an effective technique to estimate these two probabilities based on the estimation of feature distributions in the latent space using incremental PCA and Mahalanobis distance. The proposed method does not require a memory buffer to save replay data and it outperforms strong baselines including replay-based methods."}}
{"id": "m_GDIItaI3o", "cdate": 1663850463925, "mdate": null, "content": {"title": "Continual Pre-training of Language Models", "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method."}}
{"id": "ncQCD9M8SwT", "cdate": 1663850462077, "mdate": null, "content": {"title": "Continual Learning Based on Sub-Networks and Task Similarity", "abstract": "Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT) across tasks. The existing literature mainly tries to overcome CF. Although some papers have focused on both CF and KT, they may still suffer from CF because of their ineffective handling of previous tasks and/or poor task similarity detection mechanisms to achieve KT. This work presents a new CL method that addresses the above issues. First, it overcomes CF by isolating the knowledge of each task via a learned mask that indicates a sub-network. Second, it proposes a novel technique to compute how important each mask is to the new task, which indicates how the new task is similar to an underlying old task. Similar tasks can share the same mask/subnetwork for KT, while dissimilar tasks use different masks/sub-networks for CF prevention. Comprehensive experiments have been conducted using a range of NLP problems, including classification, generation, and extraction to show that the proposed method consistently outperforms prior state-of-the-art baselines."}}
{"id": "Zz8_2A4iPS", "cdate": 1663850399703, "mdate": null, "content": {"title": "Continual Learning with Soft-Masking of Parameter-Level Gradient Flow", "abstract": "Existing research on task incremental learning in continual learning has primarily focused on preventing catastrophic forgetting (CF). Several techniques have achieved learning with no CF. However, they attain it by letting each task monopolize a sub-network in a shared network, which seriously limits knowledge transfer (KT) and causes over-consumption of the network capacity, i.e., as more tasks are learned, the performance deteriorates. The goal of this paper is threefold: (1) overcoming CF, (2) encouraging KT, and (3) tackling the capacity problem. A novel and simple technique (called SPG) is proposed that soft-masks (partially blocks) parameter updating in training based on the importance of each parameter to old tasks. Each task still uses the full network, i.e., no monopoly of any part of the network by any task, which enables maximum KT and reduction of capacity usage. Extensive experiments demonstrate the effectiveness of SPG in achieving all three objectives. More notably, it attains significant transfer of knowledge not only among similar tasks (with shared knowledge) but also among dissimilar tasks (with little shared knowledge) while preventing CF."}}
{"id": "PXRN-uxHoIE", "cdate": 1663850065861, "mdate": null, "content": {"title": "Learning Invariant Features for Online Continual Learning", "abstract": "It has been shown recently that learning only discriminative features that are sufficient to separate the classes in a task using a traditional learning method has a major shortcoming for continual learning (CL). This is because many features that are not learned may be necessary for distinguishing classes of some future tasks. When such a future task arrives, these features have to be learned by updating the network, which causes catastrophic forgetting (CF). A recent work on online CL showed that if the learning method can learn as many features as possible from each class, called holistic representations, CF can be significantly reduced to achieve a large performance gain. This paper argues that learning only holistic representations is still insufficient. The learned representations should also be invariant and those features that are present in the data but are irrelevant to the class (e.g., the background information) should be ignored for better generalization across tasks. This new condition further boosts the performance significantly. This paper proposes several strategies and a loss to learn holistic and invariant representations and evaluates their effectiveness in online CL."}}
{"id": "bA8CYH5uEn_", "cdate": 1652737770763, "mdate": null, "content": {"title": "A Theoretical Study on Solving Continual Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally. There are two popular CL settings, class incremental learning (CIL) and task incremental learning (TIL). A major challenge of CL is catastrophic forgetting (CF). While a number of techniques are already available to effectively overcome CF for TIL, CIL remains to be highly challenging. So far, little theoretical study has been done to provide a principled guidance on how to solve the CIL problem. This paper performs such a study. It first shows that probabilistically, the CIL problem can be decomposed into two sub-problems: Within-task Prediction (WP) and Task-id Prediction (TP). It further proves that TP is correlated with out-of-distribution (OOD) detection, which connects CIL and OOD detection. The key conclusion of this study is that regardless of whether WP and TP or OOD detection are defined explicitly or implicitly by a CIL algorithm, good WP and good TP or OOD detection are necessary and sufficient for good CIL performances. Additionally, TIL is simply WP. Based on the theoretical result, new CIL methods are also designed, which outperform strong baselines in both CIL and TIL settings by a large margin."}}
{"id": "-u5L5CItlKV", "cdate": 1649649181496, "mdate": null, "content": {"title": "HRN: A Holistic Approach to One Class Learning", "abstract": "Existing neural network based one-class learning methods mainly use various forms\nof auto-encoders or GAN style adversarial training to learn a latent representation\nof the given one class of data. This paper proposes an entirely different approach\nbased on a novel regularization, called holistic regularization (or H-regularization),\nwhich enables the system to consider the data holistically, not to produce a model\nthat biases towards some features. Combined with a proposed 2-norm instance \nlevel data normalization, we obtain an effective one-class learning method, called\nHRN. To our knowledge, the proposed regularization and the normalization method\nhave not been reported before. Experimental evaluation using both benchmark\nimage classification and traditional anomaly detection datasets show that HRN\nmarkedly outperforms the state-of-the-art existing deep/non-deep learning models.\nThe code of HRN can be found here.\n."}}
{"id": "XjvzjjwVFAY", "cdate": 1649649017919, "mdate": null, "content": {"title": "Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks", "abstract": "Existing research on continual learning of a sequence of tasks focused on dealing\nwith catastrophic forgetting, where the tasks are assumed to be dissimilar and have\nlittle shared knowledge. Some work has also been done to transfer previously\nlearned knowledge to the new task when the tasks are similar and have shared\nknowledge. To the best of our knowledge, no technique has been proposed to learn a\nsequence of mixed similar and dissimilar tasks that can deal with forgetting and also\ntransfer knowledge forward and backward. This paper proposes such a technique\nto learn both types of tasks in the same network. For dissimilar tasks, the algorithm\nfocuses on dealing with forgetting, and for similar tasks, the algorithm focuses on\nselectively transferring the knowledge learned from some similar previous tasks to\nimprove the new task learning. Additionally, the algorithm automatically detects\nwhether a new task is similar to any previous tasks. Empirical evaluation using\nsequences of mixed tasks demonstrates the effectiveness of the proposed model."}}
{"id": "UshiGzocXZO", "cdate": 1649648815353, "mdate": null, "content": {"title": " Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally with the goal\nof achieving two main objectives: overcoming catastrophic forgetting (CF) and\nencouraging knowledge transfer (KT) across tasks. However, most existing techniques focus only on overcoming CF and have no mechanism to encourage KT,\nand thus do not do well in KT. Although several papers have tried to deal with\nboth CF and KT, our experiments show that they suffer from serious CF when\nthe tasks do not have much shared knowledge. Another observation is that most\ncurrent CL methods do not use pre-trained models, but it has been shown that such\nmodels can significantly improve the end task performance. For example, in natural\nlanguage processing, fine-tuning a BERT-like pre-trained language model is one of\nthe most effective approaches. However, for CL, this approach suffers from serious\nCF. An interesting question is how to make the best use of pre-trained models for\nCL. This paper proposes a novel model called CTR to solve these problems. Our\nexperimental results demonstrate the effectiveness of CTR."}}
