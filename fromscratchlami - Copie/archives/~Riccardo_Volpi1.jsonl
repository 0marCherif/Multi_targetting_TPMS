{"id": "0vG8GbuPOH3", "cdate": 1663850293679, "mdate": null, "content": {"title": "Semantic Prior for Weakly Supervised Class-Incremental Segmentation", "abstract": "Class-incremental semantic image segmentation assumes multiple model updates, each enriching the model to segment new categories. This is typically carried out by providing pixel-level manual annotations for all new objects, limiting the adoption of such methods. Approaches which solely require image-level labels offer an attractive alternative, yet, such annotations lack crucial information about the location and boundary of new objects. In this paper we argue that, since classes represent not just indices but semantic entities, the conceptual relationships between them can provide valuable information that should be leveraged. We propose a weakly supervised approach that leverages such semantic relations in order to transfer some cues from the previously learned classes into the new ones, complementing the supervisory signal from image-level labels. We validate our approach on a number of continual learning tasks, and show how even a simple pairwise interaction between classes can significantly improve the segmentation mask quality of both old and new classes. We show these conclusions still hold for longer and, hence, more realistic sequences of tasks and for a challenging few-shot scenario."}}
{"id": "NENo__bExYu", "cdate": 1652737805789, "mdate": null, "content": {"title": "Make Some Noise: Reliable and Efficient Single-Step Adversarial Training", "abstract": "Recently, Wong et al. (2020) showed that adversarial training with single-step FGSM leads to a characteristic failure mode named catastrophic overfitting (CO), in which a model becomes suddenly vulnerable to multi-step attacks. Experimentally they showed that simply adding a random perturbation prior to FGSM (RS-FGSM) could prevent CO. However,  Andriushchenko & Flammarion (2020) observed that RS-FGSM still leads to CO for larger perturbations, and proposed a computationally expensive regularizer (GradAlign) to avoid it. In this work, we methodically revisit the role of noise and clipping in single-step adversarial training. Contrary to previous intuitions, we find that using a stronger noise around the clean sample combined with \\textit{not clipping} is highly effective in avoiding CO for large perturbation radii. We then propose Noise-FGSM (N-FGSM) that, while providing the benefits of single-step adversarial training, does not suffer from CO. Empirical analyses on a large suite of experiments show that N-FGSM is able to match or surpass the performance of previous state of-the-art GradAlign while achieving 3$\\times$ speed-up."}}
{"id": "IZR-h5ABN73", "cdate": 1649244574744, "mdate": 1649244574744, "content": {"title": "Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning", "abstract": "Most standard learning approaches lead to fragile models which are prone to drift when sequentially trained on samples of a different nature--the well-known\" catastrophic forgetting\" issue. In particular, when a model consecutively learns from different visual domains, it tends to forget the past domains in favor of the most recent ones. In this context, we show that one way to learn models that are inherently more robust against forgetting is domain randomization--for vision tasks, randomizing the current domain's distribution with heavy image manipulations. Building on this result, we devise a meta-learning strategy where a regularizer explicitly penalizes any loss associated with transferring the model from the current domain to different\" auxiliary\" meta-domains, while also easing adaptation to them. Such meta-domains are also generated through randomized image manipulations. We empirically demonstrate in a variety of experiments--spanning from classification to semantic segmentation--that our approach results in models that are less prone to catastrophic forgetting when transferred to new domains."}}
{"id": "doeF2FEK6bn", "cdate": 1640995200000, "mdate": 1668074781370, "content": {"title": "On the Road to Online Adaptation for Semantic Image Segmentation", "abstract": "We propose a new problem formulation and a corresponding evaluation framework to advance research on unsupervised domain adaptation for semantic image segmentation. The overall goal is fostering the development of adaptive learning systems that will continuously learn, without supervision, in ever-changing environments. Typical protocols that study adaptation algorithms for segmentation models are limited to few domains, adaptation happens offline, and human intervention is generally required, at least to annotate data for hyperparameter tuning. We argue that such constraints are incompatible with algorithms that can continuously adapt to different real-world situations. To address this, we propose a protocol where models need to learn online, from sequences of temporally correlated images, requiring continuous, frame-by-frame adaptation. We accompany this new protocol with a variety of baselines to tackle the proposed formulation, as well as an extensive analysis of their behaviors, which can serve as a starting point for future research."}}
{"id": "fRnRsdc_nR7", "cdate": 1632875626094, "mdate": null, "content": {"title": "Towards fast and effective single-step adversarial training", "abstract": "Recently, Wong et al. (2020) showed adversarial training with single-step FGSM leads  to  a  characteristic  failure  mode  named catastrophic  overfitting (CO),  in which  a  model  becomes  suddenly  vulnerable  to  multi-step  attacks.   Moreover, they showed adding a random perturbation prior to FGSM (RS-FGSM) seemed to be sufficient to prevent CO. However, Andriushchenko & Flammarion (2020) observed that RS-FGSM still leads to CO for larger perturbations and argue that the  only  contribution  of  the  random  step  is  to  reduce  the  magnitude  of  the  attacks.  They suggest a regularizer (GradAlign) that avoids CO but is significantly more expensive than RS-FGSM. In this work,  we methodically revisit the role of noise and clipping in single-step adversarial training.  Contrary to previous intuitions, we find that not clipping the perturbation around the clean sample and using a stronger noise is highly effective in avoiding CO for large perturbation radii,  despite leading to an increase in the magnitude of the attacks.   Based on these observations, we propose a method called Noise-FGSM (N-FGSM), which attacks noise-augmented samples directly using a single-step. Empirical analyses on a large suite of experiments show that N-FGSM is able to match or surpass the performance of GradAlign while achieving a 3x speed-up."}}
{"id": "mv3NmEhqzL9", "cdate": 1609459200000, "mdate": 1668074781371, "content": {"title": "Continual Adaptation of Visual Representations via Domain Randomization and Meta-Learning", "abstract": "Most standard learning approaches lead to fragile models which are prone to drift when sequentially trained on samples of a different nature -- the well-known \"catastrophic forgetting\" issue. In particular, when a model consecutively learns from different visual domains, it tends to forget the past domains in favor of the most recent ones. In this context, we show that one way to learn models that are inherently more robust against forgetting is domain randomization -- for vision tasks, randomizing the current domain's distribution with heavy image manipulations. Building on this result, we devise a meta-learning strategy where a regularizer explicitly penalizes any loss associated with transferring the model from the current domain to different \"auxiliary\" meta-domains, while also easing adaptation to them. Such meta-domains are also generated through randomized image manipulations. We empirically demonstrate in a variety of experiments -- spanning from classification to semantic segmentation -- that our approach results in models that are less prone to catastrophic forgetting when transferred to new domains."}}
{"id": "m4mRS8SzOe", "cdate": 1609459200000, "mdate": 1668074781529, "content": {"title": "Explainable Deep Classification Models for Domain Generalization", "abstract": "Conventionally, AI models are thought to trade off explainability for lower accuracy. We develop a training strategy that not only leads to a more explainable AI system for object classification, but as a consequence, suffers no perceptible accuracy degradation. Explanations are defined as regions of visual evidence upon which a deep classification network makes a decision. This is represented in the form of a saliency map conveying how much each pixel contributed to the network's decision. Our training strategy enforces a periodic saliency-based feedback to encourage the model to focus on the image regions that directly correspond to the ground-truth object. We quantify explainability using an automated metric, and using human judgement. We propose explainability as a means for bridging the visual-semantic gap between different domains where model explanations are used as a means of disentagling domain specific information from otherwise relevant features. We demonstrate that this leads to improved generalization to new domains without hindering performance on the original domain."}}
{"id": "0-E5QP_y9Ry", "cdate": 1609459200000, "mdate": 1668074781375, "content": {"title": "Learning Unbiased Representations via Mutual Information Backpropagation", "abstract": "We are interested in learning data-driven representations that can generalize well, even when trained on inherently biased data. In particular, we face the case where some attributes (bias) of the data, if learned by the model, can severely compromise its generalization properties. We tackle this problem through the lens of information theory, leveraging recent findings for a differentiable estimation of mutual information. We propose a novel end-to-end optimization strategy, which simultaneously estimates and minimizes the mutual information between the learned representation and the data attributes. When applied on standard benchmarks, our model shows comparable or superior classification performance with respect to the state-of-the-art. Moreover, our method is general enough to be applicable to the problem of \"algorithmic fairness\", with competitive results."}}
{"id": "VJCzFhTj7O", "cdate": 1581862585231, "mdate": null, "content": {"title": "Addressing Model Vulnerability to Distributional Shifts over Image Transformation Sets", "abstract": "We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts."}}
{"id": "BJ-QC_WObH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Generalizing to Unseen Domains via Adversarial Data Augmentation", "abstract": "We are concerned with learning models that generalize well to different unseen domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is \"hard\" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains."}}
