{"id": "GChV32EyYi", "cdate": 1683823732040, "mdate": 1683823732040, "content": {"title": "Segmenting across places: The need for fair transfer learning with satellite imagery", "abstract": "The increasing availability of high-resolution satellite imagery has enabled the use of machine learning to support land-cover measurement and inform policy-making. How- ever, labelling satellite images is expensive and is available for only some locations. This prompts the use of transfer learning to adapt models from data-rich locations to others. Given the potential for high-impact applications of satel- lite imagery across geographies, a systematic assessment of transfer learning implications is warranted. In this work, we consider the task of land-cover segmentation and study the fairness implications of transferring models across lo- cations. We leverage a large satellite image segmentation benchmark with 5987 images from 18 districts (9 urban and 9 rural). Via fairness metrics we quantify disparities in model performance along two axes \u2013 across urban-rural locations and across land-cover classes. Findings show that state-of-the-art models have better overall accuracy in ru- ral areas compared to urban areas, through unsupervised domain adaptation methods transfer learning better to ur- ban versus rural areas and enlarge fairness gaps. In analy- sis of reasons for these findings, we show that raw satellite images are overall more dissimilar between source and tar- get districts for rural than for urban locations. This work highlights the need to conduct fairness analysis for satel- lite imagery segmentation models and motivates the devel- opment of methods for fair transfer learning in order not to introduce disparities between places, particularly urban and rural locations."}}
{"id": "uyHc-O4IhZ", "cdate": 1672531200000, "mdate": 1695954664127, "content": {"title": "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts", "abstract": "Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, ..."}}
{"id": "LM9tJIG33dI", "cdate": 1672531200000, "mdate": 1695954664128, "content": {"title": "When do Minimax-fair Learning and Empirical Risk Minimization Coincide?", "abstract": "Minimax-fair machine learning minimizes the error for the worst-off group. However, empirical evidence suggests that when sophisticated models are trained with standard empirical risk minimization ..."}}
{"id": "GNnAVwCO-O", "cdate": 1672531200000, "mdate": 1695954664087, "content": {"title": "Measures of Disparity and their Efficient Estimation", "abstract": "Quantifying disparities, that is differences in outcomes among population groups, is an important task in public health, economics, and increasingly in machine learning. In this work, we study the question of how to collect data to measure disparities. The field of survey statistics provides extensive guidance on sample sizes necessary to accurately estimate quantities such as averages. However, there is limited guidance for estimating disparities. We consider a broad class of disparity metrics including those used in machine learning for measuring fairness of model outputs. For each metric, we derive the number of samples to be collected per group that increases the precision of disparity estimates given a fixed data collection budget. We also provide sample size calculations for hypothesis tests that check for significant disparities. Our methods can be used to determine sample sizes for fairness evaluations. We validate the methods on two nationwide surveys, used for understanding population-level attributes like employment and health, and a prediction model. Absent a priori information on the groups, we find that equally sampling the groups typically performs well."}}
{"id": "2RbyKK-l9x", "cdate": 1664928778098, "mdate": null, "content": {"title": "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts", "abstract": "Performance of machine learning models may differ significantly in novel environments compared to during training due to shifts in the underlying data distribution. Attributing performance changes to specific data shifts is critical for identifying sources of model failures and designing stable models. In this work, we design a novel method for attributing performance differences between environments to shifts in the underlying causal mechanisms. We formulate the problem as a cooperative game and derive an importance weighting method for computing the value of a coalition of distributions. The contribution of each distribution to the total performance change is then quantified as its Shapley value. We demonstrate the correctness and utility of our method on two synthetic datasets and two real-world case studies, showing its effectiveness in attributing performance changes to a wide range of distribution shifts."}}
{"id": "IjC2o2-69ni", "cdate": 1663867203062, "mdate": 1663867203062, "content": {"title": "Generalizability challenges of mortality risk prediction models: A retrospective analysis on a multi-center database", "abstract": "Modern predictive models require large amounts of data for training and evaluation, absence of which may result in models that are specific to certain locations, populations in them and clinical practices. Yet, best practices for clinical risk prediction models have not yet considered such challenges to generalizability. Here we ask whether population- and group-level performance of mortality prediction models vary significantly when applied to hospitals or geographies different from the ones in which they are developed. Further, what characteristics of the datasets explain the performance variation? In this multi-center cross-sectional study, we analyzed electronic health records from 179 hospitals across the US with 70,126 hospitalizations from 2014 to 2015. Generalization gap, defined as difference between model performance metrics across hospitals, is computed for area under the receiver operating characteristic curve (AUC) and calibration slope. To assess model performance by the race variable, we report differences in false negative rates across groups. Data were also analyzed using a causal discovery algorithm \u201cFast Causal Inference\u201d that infers paths of causal influence while identifying potential influences associated with unmeasured variables. When transferring models across hospitals, AUC at the test hospital ranged from 0.777 to 0.832 (1st-3rd quartile or IQR; median 0.801); calibration slope from 0.725 to 0.983 (IQR; median 0.853); and disparity in false negative rates from 0.046 to 0.168 (IQR; median 0.092). Distribution of all variable types (demography, vitals, and labs) differed significantly across hospitals and regions. The race variable also mediated differences in the relationship between clinical variables and mortality, by hospital/region. In conclusion, group-level performance should be assessed during generalizability checks to identify potential harms to the groups. Moreover, for developing methods to improve model performance in new environments, a better understanding and documentation of provenance of data and health processes are needed to identify and mitigate sources of variation."}}
{"id": "b7jXzuQMq8W", "cdate": 1663850014604, "mdate": null, "content": {"title": "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts", "abstract": "Performance of machine learning models may differ between training and deployment for many reasons. For instance, model performance can change between environments due to changes in data quality, observing a different population than the one in training, or changes in the relationship between labels and features. These manifest as changes to the underlying data generating mechanisms, and thereby result in distribution shifts across environments. Attributing performance changes to specific shifts, such as covariate or concept shifts, is critical for identifying sources of model failures, and for taking mitigating actions that ensure robust models. In this work, we introduce the problem of attributing performance differences between environments to shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game and derive an importance weighting method for computing the value of a coalition (or a set) of distributions. The contribution of each distribution to the total performance change is then quantified as its Shapley value. We demonstrate the correctness and utility of our method on two synthetic datasets and two real-world case studies, showing its effectiveness in attributing performance changes to a wide range of distribution shifts."}}
{"id": "DARoSj6S6vm", "cdate": 1653750181197, "mdate": null, "content": {"title": "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts", "abstract": "Performance of machine learning models may differ significantly in novel environments compared to during training due to shifts in the underlying data distribution. Attributing performance changes to specific data shifts is critical for identifying sources of model failures and designing stable models. In this work, we design a novel method for attributing performance difference between environments to shifts in the underlying causal mechanisms. To this end, we construct a cooperative game where the contribution of each mechanism is quantified as their Shapley value. We demonstrate the ability of the method to identify sources of spurious correlation and attribute performance drop to shifts in label and/or feature distributions on synthetic and real-world datasets."}}
{"id": "HnzgaOK1WW5", "cdate": 1646487924561, "mdate": null, "content": {"title": "Data Poisoning Attacks on Off-Policy Policy Evaluation Algorithms", "abstract": "Off-policy Evaluation (OPE) methods are crucial for evaluating policies in high-stakes domains such as healthcare, where exploration is often infeasible or expensive. However, the extent to which such methods can be trusted under adversarial threats to data quality is largely unexplored. \nIn this work, we make the first attempt at investigating the sensitivity of OPE methods to adversarial perturbations to the data.\nWe design a data poisoning attack framework that leverages influence functions to construct perturbations that maximize error in the policy value estimates. \nOur experimental results show that many OPE methods are highly prone to data poisoning attacks, even for small adversarial perturbations."}}
{"id": "BgbgH_Ls5lc", "cdate": 1646077548608, "mdate": null, "content": {"title": "Data Poisoning Attacks on Off-Policy Policy Evaluation Methods", "abstract": "Off-policy Evaluation (OPE) methods are a crucial tool for evaluating policies in high-stakes domains such as healthcare, where exploration is often infeasible, unethical, or expensive. However, the extent to which such methods can be trusted under adversarial threats to data quality is largely unexplored. In this work, we make the first attempt at investigating the sensitivity of OPE methods to marginal adversarial perturbations to the data.\nWe design a generic data poisoning attack framework leveraging influence functions from robust statistics to carefully construct perturbations that maximize error in the policy value estimates. We carry out extensive experimentation with multiple healthcare and control datasets. Our results demonstrate that many existing OPE methods are highly prone to generating value estimates with large errors when subject to data poisoning attacks, even for small adversarial perturbations. These findings question the reliability of policy values derived using OPE methods and motivate the need for developing OPE methods that are statistically robust to train-time data poisoning attacks.\n"}}
