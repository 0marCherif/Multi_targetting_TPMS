{"id": "UgBYfuBt9c", "cdate": 1695933757316, "mdate": 1695933757316, "content": {"title": "The Dynamics of Functional Diversity throughout Neural Network Training", "abstract": "Deep ensembles offer reduced generalization error and improved predictive uncertainty estimates.\nThese performance gains are attributed to functional diversity among the component models that\nmake up the ensembles: ensemble performance increases with the diversity of the components. A\nstandard way to generate a diversity of components is to train multiple networks on the same data,\nusing different minibatch orders, augmentations, etc. In this work, we focus on how and when this\ntype of diversity in the learned predictor decreases throughout training.\nIn order to study the diversity of networks still accessible via SGD after t iterations, we first train a\nsingle network for t iterations, then duplicate the state of the optimizer and finish the remainder of\ntraining k times, with independent randomness (minibatches, augmentations, etc) for each duplicated\nnetwork. The result is k distinct networks whose training has been coupled for t iterations. We use\nthis methodology\u2014recently exploited for k = 2 to study linear mode connectivity\u2014to construct a\nnovel probe for studying diversity.\nWe find that coupling k for even a few epochs severely restricts the diversity of functions accessible\nby SGD, as measured by the KL divergence between the predicted label distributions as well as the\ncalibration and test error of k-ensembles. We also find that the number of forgetting events [1] drops\noff rapidly.\nThe amount of independent training time decreases with coupling time t however. To control for this\nconfounder, we study extending the number of iterations of high-learning-rate optimization for an\nadditional t iterations post-coupling. We find that this does not restore functional diversity.\nWe also study how functional diversity is affected by retraining after reinitializing the weights in some\nlayers. We find that we recover significantly more diversity by reinitializing layers closer to the input\nlayer, compared to reinitializing layers closer to the output. In this case, we see that reinitialization\nupsets linear mode connectivity. This observation agrees with the performance improvements seen by\narchitectures that share the core of a network but train multiple instantiations of the input layers [2]."}}
{"id": "DnVJOnHdod", "cdate": 1683832120996, "mdate": 1683832120996, "content": {"title": "Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization", "abstract": "To date, no \"information-theoretic\" frameworks for reasoning about generalization error have been shown to establish minimax rates for gradient descent in the setting of stochastic convex optimization. In this work, we consider the prospect of establishing such rates via several existing information-theoretic frameworks: input-output mutual information bounds, conditional mutual information bounds and variants, PAC-Bayes bounds, and recent conditional variants thereof. We prove that none of these bounds are able to establish minimax rates. We then consider a common tactic employed in studying gradient methods, whereby the final iterate is corrupted by Gaussian noise, producing a noisy \"surrogate\" algorithm. We prove that minimax rates cannot be established via the analysis of such surrogates. Our results suggest that new ideas are required to analyze gradient descent using information-theoretic techniques."}}
{"id": "v6DfF4CHZh", "cdate": 1672531200000, "mdate": 1681612136933, "content": {"title": "Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization", "abstract": ""}}
{"id": "nBRNjUPhWr", "cdate": 1664872119734, "mdate": null, "content": {"title": "Unmasking the Lottery Ticket Hypothesis: Efficient Adaptive Pruning for Finding Winning Tickets", "abstract": "Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that require less compute and memory but can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets, that can be retrained from initialization or an early training stage. IMP operates by iterative cycles of training, masking a fraction of smallest magnitude weights, rewinding unmasked weights back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? We find that\u2014at higher sparsities\u2014pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training encodes information about the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. We leverage this observation to design a simple adaptive pruning heuristic for speeding up the discovery of winning tickets and achieve a 30% reduction in computation time on CIFAR-100. These results make progress toward demystifying the existence of winning tickets with an eye towards enabling the development of more efficient pruning algorithms."}}
{"id": "4OTBOcNkXBx", "cdate": 1664725485669, "mdate": null, "content": {"title": "The Effect of Data Dimensionality on Neural Network Prunability", "abstract": "Practitioners prune neural networks for efficiency gains and generalization im- provements, but few scrutinize the factors determining the prunability of a neural network \u2013 the maximum fraction of weights that pruning can remove without compromising the model\u2019s test accuracy. In this work, we study the properties of input data that may contribute to the prunability of a neural network. For high dimensional input data such as images, text, and audio, the manifold hypothesis suggests that these high dimensional inputs approximately lie on or near a significantly lower dimensional manifold. Prior work demonstrates that the underlying low dimensional structure of the input data may affect the sample efficiency of learning. In this paper, we investigate whether the low dimensional structure of the input data affects the prunability of a neural network."}}
{"id": "W7udwvFMnAd", "cdate": 1663850224582, "mdate": null, "content": {"title": "When Majorities Prevent Learning: Eliminating Bias to Improve Worst-group and Out-of-distribution Generalization", "abstract": "Modern neural networks trained on large datasets have achieved state-of-the-art (in-distribution) generalization performance on various tasks. However, their good generalization performance has been shown to be contributed largely to overfitting spurious biases in large datasets. This is evident by the poor generalization performance of such models on minorities and out-of-distribution data. To alleviate this issue, subsampling the majority groups has been shown to be very effective. However, it is not clear how to find the subgroups (e.g. within a class) in large real-world datasets. Besides, naively subsampling the majority groups can entirely deplete some of their smaller sub-populations and drastically harm the in-distribution performance. Here, we show that tracking gradient trajectories of examples in initial epochs allows for finding large subpopulations of data points. We leverage this observation and propose an importance sampling method that is biased towards selecting smaller subpopulations, and eliminates bias in the large subpopulations. Our experiments confirm the effectiveness of our approach in eliminating spurious biases and learning higher-quality models with superior in- and out-of-distribution performance on various datasets."}}
{"id": "xSsW2Am-ukZ", "cdate": 1663850165322, "mdate": null, "content": {"title": "Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?", "abstract": "As neural networks get larger and costlier, it is important to find sparse networks that require less compute and memory but can be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP iterates through cycles of training, pruning a fraction of smallest magnitude weights, rewinding unpruned weights back to an early training point, and repeating. Despite its simplicity, the principles underlying when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed, i.e. why can't we prune to very high sparsities in one shot? We investigate these questions through the lens of the geometry of the error landscape. First, we find that\u2014at higher sparsities\u2014pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey to the rewind point the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training limits the fraction of weights that can be pruned at each iteration of IMP. This analysis yields a new quantitative link between IMP performance and the Hessian eigenspectrum. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry in the algorithms used to find them."}}
{"id": "U5QRuy_LjUY", "cdate": 1653595785430, "mdate": null, "content": {"title": "Pre-Training on a Data Diet: Identifying Sufficient Examples for Early Training", "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that\u2014after just a few hundred steps of dense training\u2014the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e., random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP through the lens of the data distribution. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen)  data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on \"easy\" training data we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Combined, these results provide new insight into the role played by data in the early phase of training."}}
{"id": "QLPzCpu756J", "cdate": 1652737806961, "mdate": null, "content": {"title": "Lottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks", "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that\u2014after just a few hundred steps of dense training\u2014the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e. random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP both through the lens of the data distribution and the loss landscape geometry. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen) data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on \"easy\" training data, we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Finally, we identify novel properties of the loss landscape of dense networks that are predictive of IMP performance, showing in particular that more examples being linearly mode connected in the dense network correlates well with good initializations for IMP. Combined, these results provide new insight into the role played by the early phase training in IMP."}}
{"id": "OrcLKV9sKWp", "cdate": 1652737617328, "mdate": null, "content": {"title": "Pruning\u2019s Effect on Generalization Through the Lens of Training and Regularization", "abstract": "Practitioners frequently observe that pruning improves model generalization. A long-standing hypothesis based on bias-variance trade-off attributes this generalization improvement to model size reduction. However, recent studies on over-parameterization characterize a new model size regime, in which larger models achieve better generalization. Pruning models in this over-parameterized regime leads to a contradiction -- while theory predicts that reducing model size harms generalization, pruning to a range of sparsities nonetheless improves it. Motivated by this contradiction, we re-examine pruning\u2019s effect on generalization empirically.\n\nWe show that size reduction cannot fully account for the generalization-improving effect of standard pruning algorithms. Instead, we find that pruning leads to better training at specific sparsities, improving the training loss over the dense model. We find that pruning also leads to additional regularization at other sparsities, reducing the accuracy degradation due to noisy examples over the dense model. Pruning extends model training time and reduces model size. These two factors improve training and add regularization respectively. We empirically demonstrate that both factors are essential to fully explaining pruning's impact on generalization.\n"}}
