{"id": "EDn3fMFfCS", "cdate": 1695975093817, "mdate": 1695975093817, "content": {"title": "The Utility of \u201cEven if\u201d Semi-Factual Explanation to Optimize Positive Outcomes", "abstract": "When users receive either a positive or negative outcome from an automated system, eXplainable AI (XAI) has almost exclusively focused on how to mutate negative outcomes into positive ones by crossing a decision boundary using counterfactuals (e.g., \"If you earn 2k more, we will accept your loan application\"). In this work, we instead focus on positive outcomes, and take the novel step of using XAI to optimize them (e.g., \"Even if you wish to half your down-payment, we will still accept your loan application\"). Explanations such as these that employ \"even if\" reasoning and do not cross a decision boundary are known as semi-factuals. To instantiate semi-factuals in this context, we introduce the concept of \"gain\" (i.e., how much a user stands to benefit from the proposed explanation), and consider the first causal formalization of semi-factuals. Tests on benchmark datasets show that our algorithms are better at maximizing gain compared to prior work, and that causality is especially important in the process. Most importantly however, a user study supports our main hypothesis by showing that people clearly find semi-factual explanations more useful compared to counterfactuals when they receive the positive outcome of a loan acceptance."}}
{"id": "8rdn63j3cO", "cdate": 1679919137494, "mdate": 1679919137494, "content": {"title": "On Generating Plausible Counterfactual and Semi-Factual Explanations for Deep Learning", "abstract": "There is a growing concern that the recent progress made in\nAI, especially regarding the predictive competence of deep\nlearning models, will be undermined by a failure to properly\nexplain their operation and outputs. In response to this disquiet, counterfactual explanations have become very popular\nin eXplainable AI (XAI) due to their asserted computational,\npsychological, and legal benefits. In contrast however, semifactuals (which appear to be equally useful) have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly because the nondiscrete nature of images makes good counterfactuals difficult to define; indeed, generating plausible counterfactual images which lie on the data manifold is also problematic. This\npaper advances a novel method for generating plausible counterfactuals and semi-factuals for black-box CNN classifiers\ndoing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE),\nmodifies all \u201cexceptional\u201d features in a test image to be \u201cnormal\u201d from the perspective of the counterfactual class, to generate plausible counterfactual images. Two controlled experiments compare this method to others in the literature, showing that PIECE generates highly plausible counterfactuals\n(and the best semi-factuals) on several benchmark measures."}}
{"id": "1_-6Nz5cm2", "cdate": 1679919065787, "mdate": 1679919065787, "content": {"title": "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies", "abstract": "In this paper, we describe a post-hoc explanation-by-example approach to eXplainable AI (XAI), where a black-box, deep learning system is explained by reference to a more transparent, proxy model (in this situation a case-based reasoner), based on a feature-weighting analysis of the former that is used to find explanatory cases from the latter (as one instance of the so-called Twin Systems approach). A novel method (COLE-HP) for extracting the feature-weights from black-box models is demonstrated for a convolutional neural network (CNN) applied to the MNIST dataset; in which extracted feature-weights are used to find explanatory, nearest-neighbours for test instances. Three user studies are reported examining people's judgements of right and wrong classifications made by this XAI twin-system, in the presence/absence of explanations-by-example and different error-rates (from 3-60%). The judgements gathered include item-level evaluations of both correctness and reasonableness, and system-level evaluations of trust, satisfaction, correctness, and reasonableness. Several proposals are made about the user's mental model in these tasks and how it is impacted by explanations at an item- and system-level. The wider lessons from this work for XAI and its user studies are reviewed.\n\n"}}
{"id": "hWwY_Jq0xsN", "cdate": 1663850380065, "mdate": null, "content": {"title": "Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes", "abstract": "Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an \"interpretable-by-design\" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.\n"}}
{"id": "sBT5nxwt18Q", "cdate": 1632875729339, "mdate": null, "content": {"title": "Advancing Nearest Neighbor Explanation-by-Example with Critical Classification Regions", "abstract": "There is an increasing body of evidence suggesting that post-hoc explanation-by- example with nearest neighbors is a promising solution for the eXplainable Artificial Intelligence (XAI) problem. However, despite being thoroughly researched for decades, such post-hoc methods have never seriously explored how to enhance these explanations by highlighting specific important \"parts\" in a classification. Here, we propose the notion of Critical Classification Regions (CCRs) to do this, and several possible methods are experimentally compared to determine the best approach for this explanation strategy. CCRs supplement nearest neighbor examples by highlighting similar important \"parts\" in the image explanation. Experiments across multiple domains show that CCRs represent key features used by the CNN in both the testing and training data. Finally, a suitably-controlled user study (N=163) on ImageNet, shows CCRs improve people\u2019s assessments of the correctness of a CNN\u2019s predictions for difficult classifications due to ambiguity."}}
