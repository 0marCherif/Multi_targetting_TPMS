{"id": "0bZaUfELuW", "cdate": 1686324884680, "mdate": null, "content": {"title": "Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control", "abstract": "Our goal is for robots to follow natural language instructions like ``put the towel next to the microwave.'' But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desired change between the start and goal images that the instruction corresponds to. We then train a policy on this embedding: the policy benefits from all the unlabeled data, but the aligned embedding provides an *interface* for language to steer the policy. We show instruction following across a variety of manipulation tasks in different scenes, with generalization to language instructions outside of the labeled data."}}
{"id": "Q8BGLiWn2X", "cdate": 1686324881491, "mdate": null, "content": {"title": "PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining", "abstract": "A rich representation is key to general robotic manipulation, but existing approaches to representation learning require large amounts of multimodal demonstrations. In this work we propose PLEX, a transformer-based architecture that learns from a small amount of task-agnostic visuomotor trajectories and a much larger amount of task-conditioned object manipulation videos \u2014 a type of data available in quantity. PLEX uses visuomotor trajectories to induce a latent feature space and to learn task-agnostic manipulation routines, while diverse video-only demonstrations teach PLEX how to plan in the induced latent feature space for a wide variety of tasks. Experiments showcase PLEX\u2019s generalization on Meta-World and SOTA performance in challenging Robosuite environments. In particular, using relative positional encoding in PLEX\u2019s transformers greatly helps in low-data regimes of learning from human-collected demonstrations."}}
{"id": "n7gQw_u8uU", "cdate": 1672531200000, "mdate": 1681841613506, "content": {"title": "Provable Reset-free Reinforcement Learning by No-Regret Reduction", "abstract": "Reinforcement learning (RL) so far has limited real-world applications. One key challenge is that typical RL algorithms heavily rely on a reset mechanism to sample proper initial states; these reset mechanisms, in practice, are expensive to implement due to the need for human intervention or heavily engineered environments. To make learning more practical, we propose a generic no-regret reduction to systematically design reset-free RL algorithms. Our reduction turns the reset-free RL problem into a two-player game. We show that achieving sublinear regret in this two-player game would imply learning a policy that has both sublinear performance regret and sublinear total number of resets in the original RL problem. This means that the agent eventually learns to perform optimally and avoid resets. To demonstrate the effectiveness of this reduction, we design an instantiation for linear Markov decision processes, which is the first provably correct reset-free RL algorithm."}}
{"id": "YOqtbE9CBv9", "cdate": 1672531200000, "mdate": 1681841613455, "content": {"title": "PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining", "abstract": "A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task policy, not just an observational representation. We also show that using relative positional encoding in PLEX's transformers further increases its data efficiency when learning from human-collected demonstrations. Experiments showcase \\appr's generalization on Meta-World-v2 benchmark and establish state-of-the-art performance in challenging Robosuite environments."}}
{"id": "KuZcfebICL", "cdate": 1672531200000, "mdate": 1681841613451, "content": {"title": "MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations", "abstract": "We study a new paradigm for sequential decision making, called offline policy learning from observations (PLfO). Offline PLfO aims to learn policies using datasets with substandard qualities: 1) only a subset of trajectories is labeled with rewards, 2) labeled trajectories may not contain actions, 3) labeled trajectories may not be of high quality, and 4) the data may not have full coverage. Such imperfection is common in real-world learning scenarios, and offline PLfO encompasses many existing offline learning setups, including offline imitation learning (IL), offline IL from observations (ILfO), and offline reinforcement learning (RL). In this work, we present a generic approach to offline PLfO, called $\\textbf{M}$odality-agnostic $\\textbf{A}$dversarial $\\textbf{H}$ypothesis $\\textbf{A}$daptation for $\\textbf{L}$earning from $\\textbf{O}$bservations (MAHALO). Built upon the pessimism concept in offline RL, MAHALO optimizes the policy using a performance lower bound that accounts for uncertainty due to the dataset's insufficient coverage. We implement this idea by adversarially training data-consistent critic and reward functions, which forces the learned policy to be robust to data deficiency. We show that MAHALO consistently outperforms or matches specialized algorithms across a variety of offline PLfO tasks in theory and experiments. Our code is available at https://github.com/AnqiLi/mahalo."}}
{"id": "JkwhKPZtJa", "cdate": 1672531200000, "mdate": 1681501096623, "content": {"title": "Adversarial Model for Offline Reinforcement Learning", "abstract": ""}}
{"id": "1_XARk3k-M", "cdate": 1667893315782, "mdate": null, "content": {"title": "HEETR: Pretraining for Robotic Manipulation on Heteromodal Data", "abstract": "A good representation is a key to unlock efficient learning for real-world robot manipulation. However, common manipulation-relevant datasets do not always have all the modalities (e.g., videos, actions, proprioceptive states) presented in robotic manipulation. As a result, existing approaches to representation learning, which assume full data modalities, cannot be easily scaled to consume all the data; instead, they can only be applied to a subset of modality sufficient data, which limits the effectiveness of representation learning. \nIn this work, we present an end-to-end transformer-based pretraining method called HEETR (Heteromodal End-to-End Transformer for Robotic manipulation) that can learn a representation for efficient adaptation using all data regardless of their available modalities.\n\nWe demonstrate the merits of this design and establish new state-of-the-art performance on Robosuite/Robomimic and Meta-World benchmarks."}}
{"id": "Dyh6UeiVMVB", "cdate": 1664994277263, "mdate": null, "content": {"title": "AMORE: A Model-based Framework for Improving Arbitrary Baseline Policies with Offline Data", "abstract": "We propose a new model-based offline RL framework, called Adversarial Models for Offline Reinforcement Learning (AMORE), which can robustly learn policies to improve upon an arbitrary baseline policy regardless of data coverage. Based on the concept of relative pessimism, AMORE is designed to optimize for the worst-case relative performance when facing uncertainty. In theory, we prove that the learned policy of AMORE never degrades the performance of the baseline policy with any admissible hyperparameter, and can learn to compete with the best policy within data coverage when the hyperparameter is well tuned and the baseline policy is supported by the data. Such a robust policy improvement property makes AMORE especially suitable for building real-world learning systems, because in practice ensuring no performance degradation is imperative before considering any benefit learning can bring."}}
{"id": "Qd0p0bl-A9t", "cdate": 1663850196800, "mdate": null, "content": {"title": "Provably Efficient Lifelong Reinforcement Learning with Linear Representation", "abstract": "We theoretically study lifelong reinforcement learning (RL) with linear representation in a regret minimization setting. The goal of the agent is to learn a multi-task policy based on a linear representation while solving a sequence of tasks that may be adaptively chosen based on the agent's past behaviors. We frame the problem as a linearly parameterized contextual Markov decision process (MDP), where each task is specified by a context and the transition dynamics is context-independent, and we introduce a new completeness-style assumption on the representation which is sufficient to ensure the optimal multi-task policy is realizable under the linear representation. Under this assumption, we propose an algorithm, called UCB Lifelong Value Distillation (UCBlvd), that provably achieves sublinear regret for any sequence of tasks while using only sublinear planning calls. Specifically, for $K$ task episodes of horizon $H$, our algorithm has a regret bound $\\tilde{\\mathcal{O}}(\\sqrt{(d^3+d^\\prime d)H^4K})$ using $\\mathcal{O}(dH\\log(K))$ number of planning calls, where $d$ and $d^\\prime$ are the feature dimensions of the dynamics and rewards, respectively. This theoretical guarantee implies that our algorithm can enable a lifelong learning agent to learn to internalize experiences into a multi-task policy and rapidly solve new tasks."}}
{"id": "sWOdnSkB0qu", "cdate": 1653593969783, "mdate": null, "content": {"title": "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control", "abstract": "Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt.\n\nVideos of the results and links to the code and dataset are available at https://microsoft.github.io/MoCapAct."}}
