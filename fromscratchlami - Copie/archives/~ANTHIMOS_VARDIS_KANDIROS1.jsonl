{"id": "rko0_LtgOV", "cdate": 1640995200000, "mdate": 1683827973685, "content": {"title": "EM's Convergence in Gaussian Latent Tree Models", "abstract": "We study the optimization landscape of the log-likelihood function and the convergence of the Expectation-Maximization (EM) algorithm in latent Gaussian tree models, i.e.\u00a0tree-structured Gaussian g..."}}
{"id": "H9gQNDfERUt", "cdate": 1640995200000, "mdate": 1683827973693, "content": {"title": "Learning and Testing Latent-Tree Ising Models Efficiently", "abstract": "We provide time- and sample-efficient algorithms for learning and testing latent-tree Ising models, i.e. Ising models that may only be observed at their leaf nodes. On the learning side, we obtain efficient algorithms for learning a tree-structured Ising model whose leaf node distribution is close in Total Variation Distance, improving on the results of prior work. On the testing side, we provide an efficient algorithm with fewer samples for testing whether two latent-tree Ising models have leaf-node distributions that are close or far in Total Variation distance. We obtain our algorithms by showing novel localization results for the total variation distance between the leaf-node distributions of tree-structured Ising models, in terms of their marginals on pairs of leaves."}}
{"id": "1W2U-fCLXtO", "cdate": 1640995200000, "mdate": 1683827973688, "content": {"title": "EM's Convergence in Gaussian Latent Tree Models", "abstract": "We study the optimization landscape of the log-likelihood function and the convergence of the Expectation-Maximization (EM) algorithm in latent Gaussian tree models, i.e. tree-structured Gaussian graphical models whose leaf nodes are observable and non-leaf nodes are unobservable. We show that the unique non-trivial stationary point of the population log-likelihood is its global maximum, and establish that the expectation-maximization algorithm is guaranteed to converge to it in the single latent variable case. Our results for the landscape of the log-likelihood function in general latent tree models provide support for the extensive practical use of maximum likelihood based-methods in this setting. Our results for the EM algorithm extend an emerging line of work on obtaining global convergence guarantees for this celebrated algorithm. We show our results for the non-trivial stationary points of the log-likelihood by arguing that a certain system of polynomial equations obtained from the EM updates has a unique non-trivial solution. The global convergence of the EM algorithm follows by arguing that all trivial fixed points are higher-order saddle points."}}
{"id": "xLKJZNSAd1A", "cdate": 1609459200000, "mdate": 1683827973673, "content": {"title": "Learning Ising models from one or multiple samples", "abstract": "There have been two main lines of work on estimating Ising models: (1) estimating them from multiple independent samples under minimal assumptions about the model's interaction matrix ; and (2) estimating them from one sample in restrictive settings. We propose a unified framework that smoothly interpolates between these two settings, enabling significantly richer estimation guarantees from one, a few, or many samples. Our main theorem provides guarantees for one-sample estimation, quantifying the estimation error in terms of the metric entropy of a family of interaction matrices. As corollaries of our main theorem, we derive bounds when the model's interaction matrix is a (sparse) linear combination of known matrices, or it belongs to a finite set, or to a high-dimensional manifold. In fact, our main result handles multiple independent samples by viewing them as one sample from a larger model, and can be used to derive estimation bounds that are qualitatively similar to those obtained in the afore-described multiple-sample literature. Our technical approach benefits from sparsifying a model's interaction network, conditioning on subsets of variables that make the dependencies in the resulting conditional distribution sufficiently weak. We use this sparsification technique to prove strong concentration and anti-concentration results for the Ising model, which we believe have applications beyond the scope of this paper."}}
{"id": "aj5t-wAx1YP", "cdate": 1609459200000, "mdate": 1683827973734, "content": {"title": "Statistical Estimation from Dependent Data", "abstract": "We consider a general statistical estimation problem wherein binary labels across different observations are not independent conditioning on their feature vectors, but dependent, capturing settings..."}}
{"id": "4i6iBCuUpT", "cdate": 1609459200000, "mdate": 1683827973680, "content": {"title": "Statistical Estimation from Dependent Data", "abstract": "We consider a general statistical estimation problem wherein binary labels across different observations are not independent conditioned on their feature vectors, but dependent, capturing settings where e.g. these observations are collected on a spatial domain, a temporal domain, or a social network, which induce dependencies. We model these dependencies in the language of Markov Random Fields and, importantly, allow these dependencies to be substantial, i.e do not assume that the Markov Random Field capturing these dependencies is in high temperature. As our main contribution we provide algorithms and statistically efficient estimation rates for this model, giving several instantiations of our bounds in logistic regression, sparse logistic regression, and neural network settings with dependent data. Our estimation guarantees follow from novel results for estimating the parameters (i.e. external fields and interaction strengths) of Ising models from a {\\em single} sample. {We evaluate our estimation approach on real networked data, showing that it outperforms standard regression approaches that ignore dependencies, across three text classification datasets: Cora, Citeseer and Pubmed.}"}}
{"id": "cebfnWgGkpw", "cdate": 1577836800000, "mdate": 1683827973732, "content": {"title": "Node-Max-Cut and the Complexity of Equilibrium in Linear Weighted Congestion Games", "abstract": "In this work, we seek a more refined understanding of the complexity of local optimum computation for Max-Cut and pure Nash equilibrium (PNE) computation for congestion games with weighted players and linear latency functions. We show that computing a PNE of linear weighted congestion games is PLS-complete either for very restricted strategy spaces, namely when player strategies are paths on a series-parallel network with a single origin and destination, or for very restricted latency functions, namely when the latency on each resource is equal to the congestion. Our results reveal a remarkable gap regarding the complexity of PNE in congestion games with weighted and unweighted players, since in case of unweighted players, a PNE can be easily computed by either a simple greedy algorithm (for series-parallel networks) or any better response dynamics (when the latency is equal to the congestion). For the latter of the results above, we need to show first that computing a local optimum of a natural restriction of Max-Cut, which we call Node-Max-Cut, is PLS-complete. In Node-Max-Cut, the input graph is vertex-weighted and the weight of each edge is equal to the product of the weights of its endpoints. Due to the very restricted nature of Node-Max-Cut, the reduction requires a careful combination of new gadgets with ideas and techniques from previous work. We also show how to compute efficiently a (1+\u03b5)-approximate equilibrium for Node-Max-Cut, if the number of different vertex weights is constant."}}
{"id": "UNfLxJwax4l", "cdate": 1577836800000, "mdate": 1683827973688, "content": {"title": "Estimating Ising Models from One Sample", "abstract": "There have been two separate lines of work on estimating Ising models: (1) estimating them from multiple independent samples under minimal assumptions about the model's interaction matrix; and (2) estimating them from one sample in restrictive settings. We propose a unified framework that smoothly interpolates between these two settings, enabling significantly richer estimation guarantees from one, a few, or many samples. Our main theorem provides guarantees for one-sample estimation, quantifying the estimation error in terms of the metric entropy of a family of interaction matrices. As corollaries of our main theorem, we derive bounds when the model's interaction matrix is a (sparse) linear combination of known matrices, or it belongs to a finite set, or to a high-dimensional manifold. In fact, our main result handles multiple independent samples by viewing them as one sample from a larger model, and can be used to derive estimation bounds that are qualitatively similar to those obtained in the afore-described multiple-sample literature. Our technical approach benefits from sparsifying a model's interaction network, conditioning on subsets of variables that make the dependencies in the resulting conditional distribution sufficiently weak. We use this sparsification technique to prove strong concentration and anti-concentration results for the Ising model, which we believe have applications beyond the scope of this paper."}}
{"id": "VEYRHLd6JnF", "cdate": 1546300800000, "mdate": 1683827973699, "content": {"title": "Node Max-Cut and Computing Equilibria in Linear Weighted Congestion Games", "abstract": "In this work, we seek a more refined understanding of the complexity of local optimum computation for Max-Cut and pure Nash equilibrium (PNE) computation for congestion games with weighted players and linear latency functions. We show that computing a PNE of linear weighted congestion games is PLS-complete either for very restricted strategy spaces, namely when player strategies are paths on a series-parallel network with a single origin and destination, or for very restricted latency functions, namely when the latency on each resource is equal to the congestion. Our results reveal a remarkable gap regarding the complexity of PNE in congestion games with weighted and unweighted players, since in case of unweighted players, a PNE can be easily computed by either a simple greedy algorithm (for series-parallel networks) or any better response dynamics (when the latency is equal to the congestion). For the latter of the results above, we need to show first that computing a local optimum of a natural restriction of Max-Cut, which we call \\emph{Node-Max-Cut}, is PLS-complete. In Node-Max-Cut, the input graph is vertex-weighted and the weight of each edge is equal to the product of the weights of its endpoints. Due to the very restricted nature of Node-Max-Cut, the reduction requires a careful combination of new gadgets with ideas and techniques from previous work. We also show how to compute efficiently a $(1+\\eps)$-approximate equilibrium for Node-Max-Cut, if the number of different vertex weights is constant."}}
{"id": "Do4MC9ZPyr", "cdate": 1514764800000, "mdate": 1683827973708, "content": {"title": "Opinion Dynamics with Limited Information", "abstract": "We study opinion formation games based on the Friedkin-Johnsen (FJ) model. We are interested in simple and natural variants of the FJ model that use limited information exchange in each round and converge to the same stable point. As in the FJ model, we assume that each agent i has an intrinsic opinion $$s_i \\in [0,1]$$ and maintains an expressed opinion $$x_i(t) \\in [0,1]$$ in each round t. To model limited information exchange, we assume that each agent i meets with one random friend j at each round t and learns only $$x_j(t)$$ . The amount of influence j imposes on i is reflected by the probability $$p_{ij}$$ with which i meets j. Then, agent i suffers a disagreement cost that is a convex combination of $$(x_i(t) - s_i)^2$$ and $$(x_i(t) - x_j(t))^2$$ . An important class of dynamics in this setting are no regret dynamics. We show an exponential gap between the convergence rate of no regret dynamics and of more general dynamics that do not ensure no regret. We prove that no regret dynamics require roughly $${\\varOmega }(1/\\varepsilon )$$ rounds to be within distance $$\\varepsilon $$ from the stable point $$x^*$$ of the FJ model. On the other hand, we provide an opinion update rule that does not ensure no regret and converges to $$x^*$$ in $${\\tilde{O}}(\\log ^2(1/\\varepsilon ))$$ rounds. Finally, we show that the agents can adopt a simple opinion update rule that ensures no regret and converges to $$x^*$$ in $$\\mathrm {poly}(1/\\varepsilon )$$ rounds."}}
