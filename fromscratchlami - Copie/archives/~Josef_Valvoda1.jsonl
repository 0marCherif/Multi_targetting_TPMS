{"id": "dCOSgazG1g", "cdate": 1672531200000, "mdate": 1682931451790, "content": {"title": "On the Role of Negative Precedent in Legal Outcome Prediction", "abstract": "Every legal case sets a precedent by developing the law in one of the following two\u00a0ways. It either expands its scope, in which\u00a0case it sets positive precedent, or\u00a0it narrows\u00a0it, in which case it sets negative precedent.\u00a0Legal outcome prediction, the prediction of\u00a0positive outcome, is an increasingly popular\u00a0task in AI. In\u00a0contrast, we turn our focus\u00a0to negative outcomes here, and introduce\u00a0a new task of negative outcome prediction.\u00a0We discover an asymmetry in existing\u00a0models\u2019\u00a0ability to predict positive and negative outcomes. Where the state-of-the-art\u00a0outcome prediction model we used predicts\u00a0positive outcomes at\u00a075.06\u00a0F1, it predicts\u00a0negative outcomes at only\u00a010.09\u00a0F1, worse\u00a0than a random baseline. To address this performance gap, we develop two new models\u00a0inspired by the dynamics of a\u00a0court process.\u00a0Our first model significantly improves positive outcome prediction score to\u00a077.15\u00a0F1\u00a0and our second model more than doubles the\u00a0negative\u00a0outcome prediction performance to\u00a024.01\u00a0F1. Despite this improvement, shifting focus to negative outcomes reveals that\u00a0there is still much room for\u00a0improvement\u00a0for outcome prediction models."}}
{"id": "owtWjVopJVo", "cdate": 1640995200000, "mdate": 1682931451897, "content": {"title": "Learning Transductions to Test Systematic Compositionality", "abstract": "Recombining known primitive concepts into larger novel combinations is a quintessentially human cognitive capability. Whether large neural models in NLP can acquire this ability while learning from data is an open question. In this paper, we investigate this problem from the perspective of formal languages. We use deterministic finite-state transducers to make an unbounded number of datasets with controllable properties governing compositionality. By randomly sampling over many transducers, we explore which of their properties contribute to learnability of a compositional relation by a neural network. We find that the models either learn the relations completely or not at all. The key is transition coverage, setting a soft learnability limit at 400 examples per transition."}}
{"id": "n3qws25dFt", "cdate": 1640995200000, "mdate": 1682931451805, "content": {"title": "On the Machine Learning of Ethical Judgments from Natural Language", "abstract": "Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, Adina Williams. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "V-01WPCdbaA", "cdate": 1640995200000, "mdate": 1682931451806, "content": {"title": "On the Role of Negative Precedent in Legal Outcome Prediction", "abstract": "Every legal case sets a precedent by developing the law in one of the following two ways. It either expands its scope, in which case it sets positive precedent, or it narrows it, in which case it sets negative precedent. Legal outcome prediction, the prediction of positive outcome, is an increasingly popular task in AI. In contrast, we turn our focus to negative outcomes here, and introduce a new task of negative outcome prediction. We discover an asymmetry in existing models' ability to predict positive and negative outcomes. Where the state-of-the-art outcome prediction model we used predicts positive outcomes at 75.06 F1, it predicts negative outcomes at only 10.09 F1, worse than a random baseline. To address this performance gap, we develop two new models inspired by the dynamics of a court process. Our first model significantly improves positive outcome prediction score to 77.15 F1 and our second model more than doubles the negative outcome prediction performance to 24.01 F1. Despite this improvement, shifting focus to negative outcomes reveals that there is still much room for improvement for outcome prediction models."}}
{"id": "PTc5cJWGKwe", "cdate": 1640995200000, "mdate": 1682931451941, "content": {"title": "Attentional Probe: Estimating a Module's Functional Potential", "abstract": ""}}
{"id": "Ov-LB8V2vHO", "cdate": 1640995200000, "mdate": 1682931451906, "content": {"title": "Benchmarking Compositionality with Formal Languages", "abstract": ""}}
{"id": "MosIO7yRDPR", "cdate": 1640995200000, "mdate": 1682931451813, "content": {"title": "The Architectural Bottleneck Principle", "abstract": "In this paper, we seek to measure how much information a component in a neural network could extract from the representations fed into it. Our work stands in contrast to prior probing work, most of which investigates how much information a model's representations contain. This shift in perspective leads us to propose a new principle for probing, the architectural bottleneck principle: In order to estimate how much information a given component could extract, a probe should look exactly like the component. Relying on this principle, we estimate how much syntactic information is available to transformers through our attentional probe, a probe that exactly resembles a transformer's self-attention head. Experimentally, we find that, in three models (BERT, ALBERT, and RoBERTa), a sentence's syntax tree is mostly extractable by our probe, suggesting these models have access to syntactic information while composing their contextual representations. Whether this information is actually used by these models, however, remains an open question."}}
{"id": "I2M6971Mq1", "cdate": 1640995200000, "mdate": 1676906772693, "content": {"title": "UniMorph 4.0: Universal Morphology", "abstract": ""}}
{"id": "G0JHMIZOBYx", "cdate": 1640995200000, "mdate": 1681748104528, "content": {"title": "An Ordinal Latent Variable Model of Conflict Intensity", "abstract": "Measuring the intensity of events is crucial for monitoring and tracking armed conflict. Advances in automated event extraction have yielded massive data sets of \"who did what to whom\" micro-records that enable data-driven approaches to monitoring conflict. The Goldstein scale is a widely-used expert-based measure that scores events on a conflictual-cooperative scale. It is based only on the action category (\"what\") and disregards the subject (\"who\") and object (\"to whom\") of an event, as well as contextual information, like associated casualty count, that should contribute to the perception of an event's \"intensity\". This paper takes a latent variable-based approach to measuring conflict intensity. We introduce a probabilistic generative model that assumes each observed event is associated with a latent intensity class. A novel aspect of this model is that it imposes an ordering on the classes, such that higher-valued classes denote higher levels of intensity. The ordinal nature of the latent variable is induced from naturally ordered aspects of the data (e.g., casualty counts) where higher values naturally indicate higher intensity. We evaluate the proposed model both intrinsically and extrinsically, showing that it obtains comparatively good held-out predictive performance."}}
{"id": "AItTIoZRHR", "cdate": 1640995200000, "mdate": 1682931451822, "content": {"title": "Prompting for a conversation: How to control a dialog model?", "abstract": "Dialog modelling faces a difficult trade-off. Models are trained on a large amount of text, yet their responses need to be limited to a desired scope and style of a dialog agent. Because the datasets used to achieve the former contain language that is not compatible with the latter, pre-trained dialog models are fine-tuned on smaller curated datasets. However, the fine-tuning process robs them of the ability to produce diverse responses, eventually reducing them to dull conversation partners. In this paper we investigate if prompting can mitigate the above trade-off. Specifically, we experiment with conditioning the prompt on the query, rather than training a single prompt for all queries. By following the intuition that freezing the pre-trained language model will conserve its expressivity, we find that compared to fine-tuning, prompting can achieve a higher BLEU score and substantially improve the diversity and novelty of the responses."}}
