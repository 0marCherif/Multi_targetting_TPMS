{"id": "ZTJyvbMTVx", "cdate": 1640995200000, "mdate": 1681312012126, "content": {"title": "Learning from a Biased Sample", "abstract": ""}}
{"id": "Yp3lH-C6Zt", "cdate": 1640995200000, "mdate": 1681312012129, "content": {"title": "Policy Learning with Competing Agents", "abstract": ""}}
{"id": "iFF-zKCgzS", "cdate": 1621629923259, "mdate": null, "content": {"title": "Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration", "abstract": "When facing uncertainty, decision-makers want predictions they can trust. A machine learning provider can convey confidence to decision-makers by guaranteeing their predictions are distribution calibrated--- amongst the inputs that receive a predicted vector of class probabilities q, the actual distribution over classes is given by q. For multi-class prediction problems, however, directly optimizing predictions under distribution calibration tends to be infeasible, requiring sample complexity that grows exponentially in the number of classes C. In this work, we introduce a new notion---decision calibration---that requires the predicted distribution and true distribution over classes to be ``indistinguishable'' to downstream decision-makers. This perspective gives a new characterization of distribution calibration: a predictor is distribution calibrated if and only if it is decision calibrated with respect to all decision-makers. Our main result shows that under a mild restriction, unlike distribution calibration, decision calibration is actually feasible. We design a recalibration algorithm that provably achieves decision calibration efficiently, provided that the decision-makers have a bounded number of actions (e.g., polynomial in C). We validate our recalibration algorithm empirically: compared to existing methods, decision calibration improves decision-making on skin lesion and ImageNet classification with modern neural network predictors.\n"}}
{"id": "Mx-iNoxLU4t", "cdate": 1621629767397, "mdate": null, "content": {"title": "Reliable Decisions with Threshold Calibration", "abstract": "Decision makers rely on probabilistic forecasts to predict the loss of different decision rules before deployment. When the forecasted probabilities match the true frequencies, predicted losses will be accurate. Although perfect forecasts are typically impossible, probabilities can be calibrated to match the true frequencies on average. However, we find that this \\textit{average} notion of calibration, which is typically used in practice, does not necessarily guarantee accurate decision loss prediction. Specifically in the regression setting, the loss of threshold decisions, which are decisions based on whether the forecasted outcome falls above or below a cutoff, might not be predicted accurately. We propose a stronger notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. We provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated forecaster. Our procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."}}
{"id": "KNSi_LqyORt", "cdate": 1621629767397, "mdate": null, "content": {"title": "Reliable Decisions with Threshold Calibration", "abstract": "Decision makers rely on probabilistic forecasts to predict the loss of different decision rules before deployment. When the forecasted probabilities match the true frequencies, predicted losses will be accurate. Although perfect forecasts are typically impossible, probabilities can be calibrated to match the true frequencies on average. However, we find that this \\textit{average} notion of calibration, which is typically used in practice, does not necessarily guarantee accurate decision loss prediction. Specifically in the regression setting, the loss of threshold decisions, which are decisions based on whether the forecasted outcome falls above or below a cutoff, might not be predicted accurately. We propose a stronger notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. We provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated forecaster. Our procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."}}
{"id": "eklXhqopUEb", "cdate": 1609459200000, "mdate": 1681312012145, "content": {"title": "Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration", "abstract": ""}}
{"id": "_9gOvFQMk7", "cdate": 1577836800000, "mdate": 1681312012114, "content": {"title": "Deep Orientation Uncertainty Learning based on a Bingham Loss", "abstract": ""}}
{"id": "ryloogSKDS", "cdate": 1569439907184, "mdate": null, "content": {"title": "Deep Orientation Uncertainty Learning based on a Bingham Loss", "abstract": "Reasoning about uncertain orientations is one of the core problems in many perception tasks such as object pose estimation or motion estimation. In these scenarios, poor illumination conditions, sensor limitations, or appearance invariance may result in highly uncertain estimates. In this work, we propose a novel learning-based representation for orientation uncertainty. By characterizing uncertainty over unit quaternions with the Bingham distribution, we formulate a loss that naturally captures the antipodal symmetry of the representation. We discuss the interpretability of the learned distribution parameters and demonstrate the feasibility of our approach on several challenging real-world pose estimation tasks involving uncertain orientations."}}
