{"id": "pksX_ir0tGB", "cdate": 1684266203076, "mdate": 1684266203076, "content": {"title": "Contextualizing Enhances Gradient Based Meta Learning", "abstract": "Meta learning methods have found success when applied to few shot classification\nproblems, in which they quickly adapt to a small number of labeled examples.\nPrototypical representations, each representing a particular class, have been of\nparticular importance in this setting, as they provide a compact form to convey\ninformation learned from the labeled examples. However, these prototypes are\njust one method of representing this information, and they are narrow in their\nscope and ability to classify unseen examples. We propose the implementation of\ncontextualizers, which are generalizable prototypes that adapt to given examples\nand play a larger role in classification for gradient-based models. We demonstrate\nhow to equip meta learning methods with contextualizers and show that their use\ncan significantly boost performance on a range of few shot learning datasets. We\nalso present figures of merit demonstrating the potential benefits of contextualizers,\nalong with analysis of how models make use of them. Our approach is particularly\napt for low-data environments where it is difficult to update parameters without\noverfitting. Our implementation and instructions to reproduce the experiments are\navailable at https://github.com/naveace/proto-context/."}}
{"id": "djssHWljSA", "cdate": 1675970198818, "mdate": null, "content": {"title": "Studying Phase Transitions in Contrastive Learning With Physics-Inspired Datasets", "abstract": "In recent years contrastive learning has become a state-of-the-art technique in representation learning, but the exact mechanisms by which it trains are not well understood. By focusing on physics-inspired datasets with low intrinsic dimensionality, we are able to visualize and study contrastive training procedures in better resolution. We empirically study the geometric development of contrastively learned embeddings, discovering phase transitions between locally metastable embedding conformations towards an optimal structure. Ultimately we show a strong experimental link between stronger augmentations and decreased training time for contrastively learning more geometrically meaningful representations. "}}
{"id": "Qr5IPOpnLqu", "cdate": 1664725484201, "mdate": null, "content": {"title": "Model Stitching: Looking For Functional Similarity Between Representations", "abstract": "Model stitching (Lenc \\& Vedaldi 2015) is a compelling methodology to compare different neural network representations, because it allows us to measure to what degree they may be interchanged. We expand on a previous work from Bansal, Nakkiran \\& Barak which used model stitching to compare representations of the same shapes learned by differently seeded and/or trained neural networks of the same architecture. Our contribution enables us to compare the representations learned by layers with different shapes from neural networks with different architectures. We subsequently reveal unexpected behavior of model stitching. Namely, we find that stitching, based on convolutions, for small ResNets, can reach high accuracy if those layers come later in the first (sender) network than in the second (receiver), even if those layers are far apart.\n\nThis leads us to hypothesize that stitches are not in fact learning to match the representations expected by receiver layers, but instead finding different representations which nonetheless yield similar results. Thus, we suggest that model stitching, naively implemented, may not necessarily always be an accurate measure of similarity."}}
{"id": "7bHLCO5FQdB", "cdate": 1664294259840, "mdate": null, "content": {"title": "Model Stitching: Looking For Functional Similarity Between Representations", "abstract": "  Model stitching (Lenc \\& Vedaldi 2015) is a compelling methodology to compare\n  different neural network representations, because it allows us to measure to\n  what degree they may be interchanged.\n  We expand on a previous work from Bansal, Nakkiran \\& Barak which used model stitching to\n  compare representations of the same shapes learned by differently seeded\n  and/or trained neural networks of the same architecture.\n  Our contribution enables us to compare the representations learned by layers with\n  different shapes from neural networks with different architectures.\n  We subsequently reveal unexpected behavior of model stitching. Namely, we find that stitching, \n  based on convolutions, for small ResNets, can reach\n  high accuracy if those layers come later in the first (sender) network than in\n  the second (receiver), even if those layers are far apart.\n\n  This leads us to hypothesize that stitches are not in fact learning to match the\n  representations expected by receiver layers, but instead finding different representations which nonetheless\n  yield similar results. Thus, we believe that model stitching may not necessarily always be an\n  accurate measure of similarity."}}
{"id": "rmJGhLAlNo4", "cdate": 1664248839777, "mdate": null, "content": {"title": "Data-driven Acceleration of Quantum Optimization and Machine Learning via Koopman Operator Learning", "abstract": "Efficient optimization methods play a crucial role for quantum optimization and machine learning on near-term quantum computers. Unlike classical computers, obtaining gradients on quantum computers is costly with sample complexity scaling with the number of parameters and measurements. In this paper, we connect the natural gradient method in quantum optimization with Koopman operator theory, which provides a powerful framework for predicting nonlinear dynamics. We propose a data-driven approach for accelerating quantum optimization and machine learning via Koopman operator learning.  To predict parameter updates on quantum computers, we develop new methods including the sliding window dynamic mode decomposition (DMD) and the neural-network-based DMD. We apply our methods both on simulations and real quantum hardware. We  demonstrate efficient prediction and acceleration of gradient optimization on the variational quantum eigensolver and quantum machine learning. "}}
{"id": "wyjAf9GPD_", "cdate": 1663850433494, "mdate": null, "content": {"title": "Koopman Operator Learning for Accelerating Quantum Optimization and Machine Learning", "abstract": "Finding efficient optimization methods plays an important role for quantum optimization and quantum machine learning on near-term quantum computers. While backpropagation on classical computers is computationally efficient, obtaining gradients on quantum computers is not, because the computational complexity scales linearly with the number of parameters and measurements. In this paper, we connect Koopman operator theory, which has been successful in predicting nonlinear dynamics, with natural gradient methods in quantum optimization. We propose a data-driven approach using Koopman operator learning to accelerate quantum optimization and quantum machine learning. We develop two new families of methods: the sliding window dynamic mode decomposition (DMD) and the neural DMD for efficiently updating parameters on quantum computers. We show that our methods can predict gradient dynamics on quantum computers and accelerate the quantum variational eigensolver used in quantum optimization, as well as quantum machine learning. We further implement the learning algorithms on a real quantum computer and demonstrate their practical effectiveness. "}}
{"id": "EqDnVOyiVX", "cdate": 1663850216110, "mdate": null, "content": {"title": "Learning to Optimize Quasi-Newton Methods", "abstract": "We introduce a novel machine learning optimizer called LODO, which online meta-learns an implicit inverse Hessian of the loss as a subroutine of quasi-Newton optimization. Our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn neural representations of symmetric matrix vector products, which are more flexible than those in other quasi-Newton methods. Unlike other L2O methods, ours does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse Hessian in noisy loss landscapes and is capable of representing a wide range of inverse Hessians. We experimentally verify our algorithm's performance in the presence of noise, and show that simpler alternatives for representing the inverse Hessians worsen performance. Lastly, we use our optimizer to train a semi-realistic deep neural network with 95k parameters, and obtain competitive results against standard neural network optimizers."}}
{"id": "ml8_xBoTnVA", "cdate": 1663850205562, "mdate": null, "content": {"title": "On Assimilating Learned Views in Contrastive Learning", "abstract": "Transformations based on domain expertise (expert transformations), such as random-resized-crop and color-jitter, have proven critical to the success of contrastive learning techniques such as SimCLR. Recently, several attempts have been made to replace such domain-specific, human-designed transformations with generated views that are learned. However, for imagery data, so far none of these view generation methods has been able to outperform expert transformations. In this work, we tackle a different question: instead of replacing expert transformations with generated views, can we constructively assimilate generated views with expert transformations? We answer this question in the affirmative. To do so, we first propose an information-theoretic framework for designing view generation based on the analysis of Tian et al 2020b on what makes a \"good\" view in contrastive learning. Then, we present two simple yet effective assimilation methods that together with our view generation mechanisms improve the state-of-the-art by up to approximately 3.5% on four different datasets. Importantly, we conduct a detailed empirical study that systematically analyzes a range of view generation and assimilation methods and provides a holistic picture of the efficacy of learned views in contrastive representation learning."}}
{"id": "c-h2XSi-vEM", "cdate": 1663850199583, "mdate": null, "content": {"title": "On the Importance of Calibration in Semi-supervised Learning", "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data by combining techniques of consistency regularization and pseudo-labeling. During pseudo-labeling, the model's predictions on unlabeled data are used for training and thus, model calibration is important in mitigating confirmation bias. Yet, many SOTA methods are optimized for model performance, with little focus directed to improve model calibration. In this work, we empirically demonstrate that model calibration is strongly correlated with model performance and propose to improve calibration via approximate Bayesian techniques. \nWe introduce a family of new SSL models that optimizes for calibration and demonstrate their effectiveness across standard vision benchmarks of CIFAR-10, CIFAR-100 and ImageNet, giving up to 15.9\\% improvement in test accuracy. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science."}}
{"id": "gKLAAfiytI", "cdate": 1632875676802, "mdate": null, "content": {"title": "Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations", "abstract": "In state-of-the-art self-supervised learning (SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations prescribed from human knowledge. In fact, the property of invariance is a trivial instance of a broader class called equivariance, which can be intuitively understood as the property that representations transform according to the way the inputs transform. Here, we show that rather than using only invariance, pre-training that encourages non-trivial equivariance to some transformations, while maintaining invariance to other transformations, can be used to improve the semantic quality of representations. Specifically, we extend popular SSL methods to a more general framework which we name Equivariant Self-Supervised Learning (E-SSL). In E-SSL, a simple additional pre-training objective encourages equivariance by predicting the transformations applied to the input. We demonstrate E-SSL\u2019s effectiveness empirically on several popular computer vision benchmarks, e.g. improving SimCLR to 72.5% linear probe accuracy on ImageNet. Furthermore, we demonstrate usefulness of E-SSL for applications beyond computer vision; in particular, we show its utility on regression problems in photonics science. Our code, datasets and pre-trained models are available at https://github.com/rdangovs/essl to aid further research in E-SSL."}}
