{"id": "mRLuDIPGYu", "cdate": 1672531200000, "mdate": 1683655797500, "content": {"title": "A new randomized primal-dual algorithm for convex optimization with fast last iterate convergence rates", "abstract": "We develop a novel unified randomized block-coordinate primal-dual algorithm to solve a class of nonsmooth constrained convex optimization problems, which covers different existing variants and mod..."}}
{"id": "VmsZPzcEGBj", "cdate": 1640995200000, "mdate": 1683655772915, "content": {"title": "New Primal-Dual Algorithms for a Class of Nonsmooth and Nonlinear Convex-Concave Minimax Problems", "abstract": ""}}
{"id": "PPwPcI0acTj", "cdate": 1640995200000, "mdate": 1683655797484, "content": {"title": "A Newton Frank-Wolfe method for constrained self-concordant minimization", "abstract": "We develop a new Newton Frank\u2013Wolfe algorithm to solve a class of constrained self-concordant minimization problems using linear minimization oracles (LMO). Unlike L-smooth convex functions, where the Lipschitz continuity of the objective gradient holds globally, the class of self-concordant functions only has local bounds, making it difficult to estimate the number of linear minimization oracle (LMO) calls for the underlying optimization algorithm. Fortunately, we can still prove that the number of LMO calls of our method is nearly the same as that of the standard Frank-Wolfe method in the L-smooth case. Specifically, our method requires at most $${\\mathcal {O}}\\big (\\varepsilon ^{-(1 + \\nu )}\\big )$$ O ( \u03b5 - ( 1 + \u03bd ) ) LMO\u2019s, where $$\\varepsilon $$ \u03b5 is the desired accuracy, and $$\\nu \\in (0, 0.139)$$ \u03bd \u2208 ( 0 , 0.139 ) is a given constant depending on the chosen initial point of the proposed algorithm. Our intensive numerical experiments on three applications: portfolio design with the competitive ratio, D-optimal experimental design, and logistic regression with elastic-net regularizer, show that the proposed Newton Frank\u2013Wolfe method outperforms different state-of-the-art competitors."}}
{"id": "226WDmrINE8", "cdate": 1640995200000, "mdate": 1683655797476, "content": {"title": "New Primal-Dual Algorithms for a Class of Nonsmooth and Nonlinear Convex-Concave Minimax Problems", "abstract": ""}}
{"id": "xBt8_M2_atl", "cdate": 1609459200000, "mdate": 1669108723466, "content": {"title": "Robust and Generalizable Visual Representation Learning via Random Convolutions", "abstract": "While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation."}}
{"id": "GXs_8CCYnxN", "cdate": 1601853656142, "mdate": null, "content": {"title": "Hybrid Variance-Reduced SGD Algorithms For Nonconvex-Concave Minimax Problems", "abstract": "We develop a novel variance-reduced algorithm to solve a stochastic nonconvex-concave minimax problem which has various applications in different fields. This problem has several computational challenges due to its nonsmoothness, nonconvexity, nonlinearity, and non-separability of the objective functions. Our approach relies on a novel combination of recent ideas, including smoothing and hybrid stochastic variance-reduced techniques. Our algorithm and its variants can achieve $\\mathcal{O}(T^{-2/3})$-convergence rate in $T$, and the best-known oracle complexity under standard assumptions. They have several computational advantages compared to existing methods. They can also work with both single sample or mini-batch on derivative estimators, with constant or diminishing step-sizes. We demonstrate the benefits of our algorithms over existing methods through two numerical examples."}}
{"id": "BVSM0x3EDK6", "cdate": 1601308113694, "mdate": null, "content": {"title": "Robust and Generalizable Visual Representation Learning via Random Convolutions", "abstract": "While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation."}}
{"id": "XimG9WdgbAF", "cdate": 1577836800000, "mdate": 1683655797605, "content": {"title": "An Inexact Interior-Point Lagrangian Decomposition Algorithm with Inexact Oracles", "abstract": "We combine the Lagrangian dual decomposition, barrier smoothing, path-following, and proximal Newton techniques to develop a new inexact interior-point Lagrangian decomposition method to solve a broad class of constrained composite convex optimization problems. Our method allows one to approximately solve the primal subproblems (called the slave problems), which leads to inexact oracles (i.e., inexact function value, gradient, and Hessian) of the smoothed dual problem (called the master problem). By appropriately controlling the inexact computation in both the slave and master problems, we can still establish a polynomial-time iteration complexity of our algorithm and recover primal solutions. We illustrate the performance of our method through two numerical examples and compare it with existing methods."}}
{"id": "DKq7p_1cCoG", "cdate": 1577836800000, "mdate": 1631189741587, "content": {"title": "Hybrid Variance-Reduced SGD Algorithms For Minimax Problems with Nonconvex-Linear Function", "abstract": "We develop a novel and single-loop variance-reduced algorithm to solve a class of stochastic nonconvex-convex minimax problems involving a nonconvex-linear objective function, which has various applications in different fields such as ma- chine learning and robust optimization. This problem class has several compu- tational challenges due to its nonsmoothness, nonconvexity, nonlinearity, and non-separability of the objective functions. Our approach relies on a new combi- nation of recent ideas, including smoothing and hybrid biased variance-reduced techniques. Our algorithm and its variants can achieve $\\mathcal{O}(T^{-2/3})$-convergence rate and the best-known oracle complexity under standard assumptions, where T is the iteration counter. They have several computational advantages compared to exist- ing methods such as simple to implement and less parameter tuning requirements. They can also work with both single sample or mini-batch on derivative estimators, and with constant or diminishing step-sizes. We demonstrate the benefits of our algorithms over existing methods through two numerical examples, including a nonsmooth and nonconvex-non-strongly concave minimax model."}}
