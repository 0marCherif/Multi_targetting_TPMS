{"id": "zkIm6n5P_aU", "cdate": 1681852129383, "mdate": 1681852129383, "content": {"title": "Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem", "abstract": "Given a target function $f$, how large must a neural network be in order to approximate $f$? Recent works examine this basic question on neural network \\textit{expressivity} from the lens of dynamical systems and provide novel \"depth-vs-width\" tradeoffs for a large family of functions $f$. They suggest that such tradeoffs are governed by the existence of \\textit{periodic} points or \\emph{cycles} in $f$. Our work, by further deploying dynamical systems concepts, illuminates a more subtle connection between periodicity and expressivity: we prove that periodic points alone lead to suboptimal depth-width tradeoffs and we improve upon them by demonstrating that certain \"chaotic itineraries\" give stronger exponential tradeoffs, even in regimes where previous analyses only imply polynomial gaps. Contrary to prior works, our bounds are nearly-optimal, tighten as the period increases, and handle strong notions of inapproximability (e.g., constant $L_1$ error). More broadly, we identify a phase transition to the \\textit{chaotic regime} that exactly coincides with an abrupt shift in other notions of function complexity, including VC-dimension and topological entropy."}}
{"id": "0Kv4Z0SRam", "cdate": 1681852042989, "mdate": null, "content": {"title": "Near-Optimal Statistical Query Lower Bounds for Agnostically Learning Intersections of Halfspaces with Gaussian Marginals", "abstract": "We consider the well-studied problem of learning intersections of halfspaces under the Gaussian distribution in the challenging \\emph{agnostic learning} model. Recent work of Diakonikolas et al. (2021) shows that any Statistical Query (SQ) algorithm for agnostically learning the class of intersections of k halfspaces over \u211dn to constant excess error either must make queries of tolerance at most $n^{\u2212\\tilde\\Omega(\\sqrt{\\log k})}$ or must make $2^{n^{\\Omega(1)}}$ queries. We strengthen this result by improving the tolerance requirement to $n^{\u2212\\tilde\\Omega(\\log k)}$. This lower bound is essentially best possible since an SQ algorithm of Klivans et al. (2008) agnostically learns this class to any constant excess error using $n^{O(\\log k)}$ queries of tolerance $n^{\u2212O(\\log k)}$. We prove two variants of our lower bound, each of which combines ingredients from Diakonikolas et al. (2021) with (an extension of) a different earlier approach for agnostic SQ lower bounds for the Boolean setting due to Dachman-Soled et al. (2014). Our approach also yields lower bounds for agnostically SQ learning the class of \"convex subspace juntas\" (studied by Vempala, 2010) and the class of sets with bounded Gaussian surface area; all of these lower bounds are nearly optimal since they essentially match known upper bounds from Klivans et al. (2008).\n"}}
{"id": "4pe43i2epRY", "cdate": 1681851876743, "mdate": 1681851876743, "content": {"title": "Intrinsic dimensionality and generalization properties of the R-norm inductive bias", "abstract": "We study the structural and statistical properties of R-norm minimizing interpolants of datasets labeled by specific target functions. The \ue23e-norm is the basis of an inductive bias for two-layer neural networks, recently introduced to capture the functional effect of controlling the size of network weights, independently of the network width. We find that these interpolants are intrinsically multivariate functions, even when there are ridge functions that fit the data, and also that the R-norm inductive bias is not sufficient for achieving statistically optimal generalization for certain learning problems. Altogether, these results shed new light on an inductive bias that is connected to practical neural network training."}}
{"id": "k5idxiVdJ3p", "cdate": 1652737594679, "mdate": null, "content": {"title": "On Scrambling Phenomena for Randomly Initialized Recurrent Networks ", "abstract": "Recurrent Neural Networks (RNNs) frequently exhibit complicated dynamics, and their sensitivity to the initialization process often renders them notoriously hard to train. Recent works have shed light on such phenomena analyzing when exploding or vanishing gradients may occur, either of which is detrimental for training dynamics. In this paper, we point to a formal connection between RNNs and chaotic dynamical systems and prove a qualitatively stronger phenomenon about RNNs than what exploding gradients seem to suggest. Our main result proves that under standard initialization (e.g., He, Xavier etc.), RNNs will exhibit \\textit{Li-Yorke chaos} with \\textit{constant} probability \\textit{independent} of the network's width. This explains the experimentally observed phenomenon of \\textit{scrambling}, under which trajectories of nearby points may appear to be arbitrarily close during some timesteps, yet will be far away in future timesteps. In stark contrast to their feedforward counterparts, we show that chaotic behavior in RNNs is preserved under small perturbations and that their expressive power remains exponential in the number of feedback iterations. Our technical arguments rely on viewing RNNs as random walks under non-linear activations, and studying the existence of certain types of higher-order fixed points called \\textit{periodic points} in order to establish phase transitions from order to chaos."}}
{"id": "wt7cd9m2cz2", "cdate": 1652737589347, "mdate": null, "content": {"title": "Learning single-index models with shallow neural networks", "abstract": "Single-index models are a class of functions given by an unknown univariate ``link'' function applied to an unknown one-dimensional projection of the input. These models are particularly relevant in high dimension, when the data might present low-dimensional structure that learning algorithms should adapt to. While several statistical aspects of this model, such as the sample complexity of recovering the relevant (one-dimensional) subspace, are well-understood, they rely on tailored algorithms that exploit the specific structure of the target function. In this work, we introduce a natural class of shallow neural networks and study its ability to learn single-index models via gradient flow. More precisely, we consider shallow networks in which biases of the neurons are frozen at random initialization. We show that the corresponding optimization landscape is benign, which in turn leads to generalization guarantees that match the near-optimal sample complexity of dedicated semi-parametric methods."}}
{"id": "ypQQTuUwrfB", "cdate": 1627483303501, "mdate": 1627483303501, "content": {"title": "On the Approximation Power of Two-Layer Networks of Random ReLUs", "abstract": "This paper considers the following question: how well can depth-two ReLU networks with randomly initialized bottom-level weights represent smooth functions? We give near-matching upper- and lower-bounds for L2-approximation in terms of the Lipschitz constant, the desired accuracy, and the dimension of the problem, as well as similar results in terms of Sobolev norms. Our positive results employ tools from harmonic analysis and ridgelet representation theory, while our lower-bounds are based on (robust versions of) dimensionality arguments."}}
{"id": "9bqxRuRwBlu", "cdate": 1621630024976, "mdate": null, "content": {"title": "Support vector machines and linear regression coincide with very high-dimensional features", "abstract": "The support vector machine (SVM) and minimum Euclidean norm least squares regression are two fundamentally different approaches to fitting linear models, but they have recently been connected in models for very high-dimensional data through a phenomenon of support vector proliferation, where every training example used to fit an SVM becomes a support vector. In this paper, we explore the generality of this phenomenon and make the following contributions. First, we prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. We further identify a sharp phase transition in Gaussian feature models, bound the width of this transition, and give experimental support for its universality. Finally, we hypothesize that this phase transition occurs only in much higher-dimensional settings in the $\\ell_1$ variant of the SVM, and we present a new geometric characterization of the problem that may elucidate this phenomenon for the general $\\ell_p$ case. "}}
