{"id": "sUZhhOfXrmE", "cdate": 1640995200000, "mdate": 1668724745294, "content": {"title": "RemixIT: Continual Self-Training of Speech Enhancement Models via Bootstrapped Remixing", "abstract": "We present <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">RemixIT</i> , a simple yet effective self-supervised method for training speech enhancement without the need of a single isolated in-domain speech nor a noise waveform. Our approach overcomes limitations of previous methods which make them dependent on clean in-domain target signals and thus, sensitive to any domain mismatch between train and test samples. <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">RemixIT</i> is based on a continuous self-training scheme in which a pre-trained teacher model on out-of-domain data infers estimated pseudo-target signals for in-domain mixtures. Then, by permuting the estimated clean and noise signals and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">remixing</i> them together, we generate a new set of bootstrapped mixtures and corresponding pseudo-targets which are used to train the student network. Vice-versa, the teacher periodically refines its estimates using the updated parameters of the latest student models. Experimental results on multiple speech enhancement datasets and tasks not only show the superiority of our method over prior approaches but also showcase that <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">RemixIT</i> can be combined with any separation model as well as be applied towards any semi-supervised and unsupervised domain adaptation task. Our analysis, paired with empirical evidence, sheds light on the inside functioning of our self-training scheme wherein the student model keeps obtaining better performance while observing severely degraded pseudo-targets."}}
{"id": "lV1tuTxZQc", "cdate": 1640995200000, "mdate": 1678818593089, "content": {"title": "Compute and Memory Efficient Universal Sound Source Separation", "abstract": ""}}
{"id": "lUvsQzrDITdJ", "cdate": 1640995200000, "mdate": 1668724748505, "content": {"title": "RemixIT: Continual self-training of speech enhancement models via bootstrapped remixing", "abstract": "We present RemixIT, a simple yet effective self-supervised method for training speech enhancement without the need of a single isolated in-domain speech nor a noise waveform. Our approach overcomes limitations of previous methods which make them dependent on clean in-domain target signals and thus, sensitive to any domain mismatch between train and test samples. RemixIT is based on a continuous self-training scheme in which a pre-trained teacher model on out-of-domain data infers estimated pseudo-target signals for in-domain mixtures. Then, by permuting the estimated clean and noise signals and remixing them together, we generate a new set of bootstrapped mixtures and corresponding pseudo-targets which are used to train the student network. Vice-versa, the teacher periodically refines its estimates using the updated parameters of the latest student models. Experimental results on multiple speech enhancement datasets and tasks not only show the superiority of our method over prior approaches but also showcase that RemixIT can be combined with any separation model as well as be applied towards any semi-supervised and unsupervised domain adaptation task. Our analysis, paired with empirical evidence, sheds light on the inside functioning of our self-training scheme wherein the student model keeps obtaining better performance while observing severely degraded pseudo-targets."}}
{"id": "kcAbHcYqgX", "cdate": 1640995200000, "mdate": 1678818593089, "content": {"title": "Latent Iterative Refinement for Modular Source Separation", "abstract": ""}}
{"id": "j_7tEjRATN", "cdate": 1640995200000, "mdate": 1678818593091, "content": {"title": "Heterogeneous Target Speech Separation", "abstract": ""}}
{"id": "iCwy_MKFQN", "cdate": 1640995200000, "mdate": 1678818593094, "content": {"title": "AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation", "abstract": ""}}
{"id": "XUy70nU6gW", "cdate": 1640995200000, "mdate": 1678818593118, "content": {"title": "Optimal Condition Training for Target Source Separation", "abstract": ""}}
{"id": "Lo8370v7RQr", "cdate": 1640995200000, "mdate": 1667433045850, "content": {"title": "AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation", "abstract": "We introduce AudioScopeV2, a state-of-the-art universal audio-visual on-screen sound separation system which is capable of learning to separate sounds and associate them with on-screen objects by looking at in-the-wild videos. We identify several limitations of previous work on audio-visual on-screen sound separation, including the coarse resolution of spatio-temporal attention, poor convergence of the audio separation model, limited variety in training and evaluation data, and failure to account for the trade off between preservation of on-screen sounds and suppression of off-screen sounds. We provide solutions to all of these issues. Our proposed cross-modal and self-attention network architectures capture audio-visual dependencies at a finer resolution over time, and we also propose efficient separable variants that are capable of scaling to longer videos without sacrificing much performance. We also find that pre-training the separation model only on audio greatly improves results. For training and evaluation, we collected new human annotations of on-screen sounds from a large database of in-the-wild videos (YFCC100M). This new dataset is more diverse and challenging. Finally, we propose a calibration procedure that allows exact tuning of on-screen reconstruction versus off-screen suppression, which greatly simplifies comparing performance between models with different operating points. Overall, our experimental results show marked improvements in on-screen separation performance under much more general conditions than previous methods with minimal additional computational complexity."}}
{"id": "F1N_nZDXg-", "cdate": 1640995200000, "mdate": 1678818593095, "content": {"title": "Learning Representations for New Sound Classes With Continual Self-Supervised Learning", "abstract": ""}}
{"id": "3QETH541TDV", "cdate": 1640995200000, "mdate": 1678818593093, "content": {"title": "Learning Representations for New Sound Classes With Continual Self-Supervised Learning", "abstract": ""}}
