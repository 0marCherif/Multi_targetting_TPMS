{"id": "fUR6ujAeT2", "cdate": 1672531200000, "mdate": 1696003711446, "content": {"title": "The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation", "abstract": "Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of suc..."}}
{"id": "d19Dsqtw421", "cdate": 1652737857698, "mdate": null, "content": {"title": "A Few Expert Queries Suffices for Sample-Efficient RL with Resets and Linear Value Approximation", "abstract": "The current paper studies sample-efficient Reinforcement Learning (RL) in settings where only the optimal value function is assumed to be linearly-realizable. It has recently been understood that, even under this seemingly strong assumption and access to a generative model, worst-case sample complexities can be prohibitively (i.e., exponentially) large. We investigate the setting where the learner additionally has access to interactive demonstrations from an expert policy, and we present a statistically and computationally efficient algorithm (Delphi) for blending exploration with expert queries. In particular, Delphi requires $\\tilde O(d)$ expert queries and a $\\texttt{poly}(d,H,|A|,1/\\varepsilon)$ amount of exploratory samples to provably recover an $\\varepsilon$-suboptimal policy. Compared to pure RL approaches, this corresponds to an exponential improvement in sample complexity with surprisingly-little expert input. Compared to prior imitation learning (IL) approaches, our required number of expert demonstrations is independent of $H$ and logarithmic in $1/\\varepsilon$, whereas all prior work required at least linear factors of both in addition to the same dependence on $d$. Towards establishing the minimal amount of expert queries needed, we show that, in the same setting, any learner whose exploration budget is \\textit{polynomially-bounded} (in terms of $d,H,$ and $|A|$) will require \\textit{at least} $\\tilde\\Omega(\\sqrt{d})$ oracle calls to recover a policy competing with the expert's value function. Under the weaker assumption that the expert's policy is linear, we show that the lower bound increases to $\\tilde\\Omega(d)$."}}
{"id": "lgnI1L556-", "cdate": 1640995200000, "mdate": 1696003711442, "content": {"title": "A Few Expert Queries Suffices for Sample-Efficient RL with Resets and Linear Value Approximation", "abstract": "The current paper studies sample-efficient Reinforcement Learning (RL) in settings where only the optimal value function is assumed to be linearly-realizable. It has recently been understood that, even under this seemingly strong assumption and access to a generative model, worst-case sample complexities can be prohibitively (i.e., exponentially) large. We investigate the setting where the learner additionally has access to interactive demonstrations from an expert policy, and we present a statistically and computationally efficient algorithm (Delphi) for blending exploration with expert queries. In particular, Delphi requires $\\tilde O(d)$ expert queries and a $\\texttt{poly}(d,H,|A|,1/\\varepsilon)$ amount of exploratory samples to provably recover an $\\varepsilon$-suboptimal policy. Compared to pure RL approaches, this corresponds to an exponential improvement in sample complexity with surprisingly-little expert input. Compared to prior imitation learning (IL) approaches, our required number of expert demonstrations is independent of $H$ and logarithmic in $1/\\varepsilon$, whereas all prior work required at least linear factors of both in addition to the same dependence on $d$. Towards establishing the minimal amount of expert queries needed, we show that, in the same setting, any learner whose exploration budget is \\textit{polynomially-bounded} (in terms of $d,H,$ and $|A|$) will require \\textit{at least} $\\tilde\\Omega(\\sqrt{d})$ oracle calls to recover a policy competing with the expert's value function. Under the weaker assumption that the expert's policy is linear, we show that the lower bound increases to $\\tilde\\Omega(d)$."}}
{"id": "xXMuoChtTPR", "cdate": 1609459200000, "mdate": 1653001352067, "content": {"title": "On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function", "abstract": "We consider the problem of local planning in fixed-horizon Markov Decision Processes (MDPs) with a generative model under the assumption that the optimal value function lies close to the span of a ..."}}
{"id": "z3hvs9rxrUF", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Distributional Analysis of Sampling-Based Reinforcement Learning Algorithms", "abstract": "We present a distributional approach to theoretical analyses of reinforcement learning algorithms for constant step-sizes. We demonstrate its effectiveness by presenting simple and unified proofs of convergence for a variety of commonly-used methods. We show that value-based methods such as TD($\\lambda$) and $Q$-Learning have update rules which are contractive in the space of distributions of functions, thus establishing their exponentially fast convergence to a stationary distribution. We demonstrate that the stationary distribution obtained by any algorithm whose target is an expected Bellman update has a mean which is equal to the true value function. Furthermore, we establish that the distributions concentrate around their mean as the step-size shrinks. We further analyse the optimistic policy iteration algorithm, for which the contraction property does not hold, and formulate a probabilistic policy improvement property which entails the convergence of the algorithm."}}
{"id": "hN7DDINpNf", "cdate": 1577836800000, "mdate": null, "content": {"title": "Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions", "abstract": "We consider the problem of local planning in fixed-horizon and discounted Markov Decision Processes (MDPs) with linear function approximation and a generative model under the assumption that the optimal action-value function lies in the span of a feature map that is available to the planner. Previous work has left open the question of whether there exist sound planners that need only poly(H,d) queries regardless of the MDP, where H is the horizon and d is the dimensionality of the features. We answer this question in the negative: we show that any sound planner must query at least $\\min(\\exp({\\Omega}(d)), {\\Omega}(2^H))$ samples in the fized-horizon setting and $\\exp({\\Omega}(d))$ samples in the discounted setting. We also show that for any ${\\delta}>0$, the least-squares value iteration algorithm with $O(H^5d^{H+1}/{\\delta}^2)$ queries can compute a ${\\delta}$-optimal policy in the fixed-horizon setting. We discuss implications and remaining open questions."}}
{"id": "agioNcldrWF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Constrained Markov Decision Processes via Backward Value Functions", "abstract": "Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are ha..."}}
{"id": "66LrNP0oYya", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Variant of the Wang-Foster-Kakade Lower Bound for the Discounted Setting", "abstract": "Recently, Wang et al. (2020) showed a highly intriguing hardness result for batch reinforcement learning (RL) with linearly realizable value function and good feature coverage in the finite-horizon case. In this note we show that once adapted to the discounted setting, the construction can be simplified to a 2-state MDP with 1-dimensional features, such that learning is impossible even with an infinite amount of data."}}
{"id": "ry-3_1-_-B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Temporally Extended Metrics for Markov Decision Processes", "abstract": ""}}
{"id": "5pEO3Ycbbix", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Graph Weighted Models on Pictures", "abstract": "Graph Weighted Models (GWMs) have recently been proposed as a natural generalization of weighted automata over strings and trees to arbitrary families of labeled graphs (and hypergraphs). A GWM gen..."}}
