{"id": "HfuFAY9e5oS", "cdate": 1683672077696, "mdate": 1683672077696, "content": {"title": "Mask wearing in community settings reduces SARS-CoV-2 transmission", "abstract": "The effectiveness of mask wearing at controlling severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission has been unclear. While masks are known to substantially reduce disease transmission in healthcare settings [D. K. Chu et al., Lancet 395, 1973\u20131987 (2020); J. Howard et al., Proc. Natl. Acad. Sci. U.S.A. 118, e2014564118 (2021); Y. Cheng et al., Science eabg6296 (2021)], studies in community settings report inconsistent results [H. M. Ollila et al., medRxiv (2020); J. Brainard et al., Eurosurveillance 25, 2000725 (2020); T. Jefferson et al., Cochrane Database Syst. Rev. 11, CD006207 (2020)]. Most such studies focus on how masks impact transmission, by analyzing how effective government mask mandates are. However, we find that widespread voluntary mask wearing, and other data limitations, make mandate effectiveness a poor proxy for mask-wearing effectiveness. We directly analyze the effect of mask wearing on SARS-CoV-2 transmission, drawing on several datasets covering 92 regions on six continents, including the largest survey of wearing behavior (\ud835\udc5b=\n 20 million) [F. Kreuter et al., https://gisumd.github.io/COVID-19-API-Documentation (2020)]. Using a Bayesian hierarchical model, we estimate the effect of mask wearing on transmission, by linking reported wearing levels to reported cases in each region, while adjusting for mobility and nonpharmaceutical interventions (NPIs), such as bans on large gatherings. Our estimates imply that the mean observed level of mask wearing corresponds to a 19% decrease in the reproduction number R. We also assess the robustness of our results in 60 tests spanning 20 sensitivity analyses. In light of these results, policy makers can effectively reduce transmission by intervening to increase mask wearing."}}
{"id": "2JTsK7hlu0p", "cdate": 1683671864393, "mdate": 1683671864393, "content": {"title": "Understanding the effectiveness of government interventions against the resurgence of COVID-19 in Europe", "abstract": "European governments use non-pharmaceutical interventions (NPIs) to control resurging waves of COVID-19. However, they only have outdated estimates for how effective individual NPIs were in the first wave. We estimate the effectiveness of 17 NPIs in Europe\u2019s second wave from subnational case and death data by introducing a flexible hierarchical Bayesian transmission model and collecting the largest dataset of NPI implementation dates across Europe. Business closures, educational institution closures, and gathering bans reduced transmission, but reduced it less than they did in the first wave. This difference is likely due to organisational safety measures and individual protective behaviours\u2014such as distancing\u2014which made various areas of public life safer and thereby reduced the effect of closing them. Specifically, we find smaller effects for closing educational institutions, suggesting that stringent safety measures made schools safer compared to the first wave. Second-wave estimates outperform previous estimates at predicting transmission in Europe\u2019s third wave."}}
{"id": "dDwyNMuNdw", "cdate": 1640995200000, "mdate": 1681913045867, "content": {"title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt", "abstract": "Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling."}}
{"id": "_SR0Ih2XzMS", "cdate": 1640995200000, "mdate": 1684084901658, "content": {"title": "Mapping global dynamics of benchmark creation and saturation in artificial intelligence", "abstract": "Benchmarks are crucial to measuring and steering progress in artificial intelligence (AI). However, recent studies raised concerns over the state of AI benchmarking, reporting issues such as benchmark overfitting, benchmark saturation and increasing centralization of benchmark dataset creation. To facilitate monitoring of the health of the AI benchmarking ecosystem, we introduce methodologies for creating condensed maps of the global dynamics of benchmark creation and saturation. We curated data for 3765 benchmarks covering the entire domains of computer vision and natural language processing, and show that a large fraction of benchmarks quickly trended towards near-saturation, that many benchmarks fail to find widespread utilization, and that benchmark performance gains for different AI tasks were prone to unforeseen bursts. We analyze attributes associated with benchmark popularity, and conclude that future benchmarks should emphasize versatility, breadth and real-world utility."}}
{"id": "_0BuLNlLGH5", "cdate": 1640995200000, "mdate": 1681913046007, "content": {"title": "Seasonal variation in SARS-CoV-2 transmission in temperate climates: A Bayesian modelling study in 143 European regions", "abstract": "Author Summary Building on two state-of-the-art observational models and datasets, we adapt a fully Bayesian method for estimating the association between seasonality and SARS-CoV-2 transmission in 143 temperate European regions. This approach overcomes limitations of previous analyses that do not account for the implementation of non-pharmaceutical interventions (NPIs) or mobility during the first year of the pandemic and hence may yield biased estimates of seasonal effects. We find that the seasonality of SARS-CoV-2 transmission is comparable in magnitude to the most effective individual NPIs but less than the combined effect of multiple interventions. Our findings provide valuable insights for long-term modelling and policy planning. As seasons change, it is vital that policymakers employ accurate estimates of seasonal effects. In the summer, reductions in transmission that owe to seasonality should not be misattributed to population immunity. In the winter, policymakers must avoid anticipating a greater reduction due to seasonality than will actually occur."}}
{"id": "L_MkT-TgTS", "cdate": 1640995200000, "mdate": 1681913045825, "content": {"title": "Prioritized Training on Points that are Learnable, Worth Learning, and not yet Learnt", "abstract": "Training on web-scale data can take months. But much computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Redu..."}}
{"id": "Y0cGpgUhSvp", "cdate": 1632875496469, "mdate": null, "content": {"title": "Prioritized training on points that are learnable, worth learning, and not yet learned", "abstract": "We introduce reducible held-out loss selection (RHOLS), a technique for faster model training which selects a sequence of training points that are \u201cjust right\u201d. We propose a tractable information-theoretic acquisition function\u2014the reducible heldout loss\u2014to efficiently choose training points that maximize information about a holdout set. We show that the \u201chard\u201d (e.g. high loss) points usually selected in the optimization literature are typically noisy, leading to deterioration on real-world datasets. At the same time, \u201ceasy\u201d (e.g. low noise) samples, often prioritized for curriculum learning, confer less information. In contrast, RHOLS chooses points that are \u201cjust right\u201d and trains in fewer steps than the above approaches."}}
{"id": "-GyS3kjKMg9", "cdate": 1632235869648, "mdate": null, "content": {"title": "DeDUCE: Generating Counterfactual Explanations At Scale", "abstract": "When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines."}}
{"id": "wrIZisT_kYM", "cdate": 1609459200000, "mdate": 1684084901906, "content": {"title": "DeDUCE: Generating Counterfactual Explanations Efficiently", "abstract": "When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE."}}
{"id": "aNTx3gdgBFL", "cdate": 1609459200000, "mdate": 1627120341103, "content": {"title": "Prioritized training on points that are learnable, worth learning, and not yet learned", "abstract": "We introduce Goldilocks Selection, a technique for faster model training which selects a sequence of training points that are \"just right\". We propose an information-theoretic acquisition function -- the reducible validation loss -- and compute it with a small proxy model -- GoldiProx -- to efficiently choose training points that maximize information about a validation set. We show that the \"hard\" (e.g. high loss) points usually selected in the optimization literature are typically noisy, while the \"easy\" (e.g. low noise) samples often prioritized for curriculum learning confer less information. Further, points with uncertain labels, typically targeted by active learning, tend to be less relevant to the task. In contrast, Goldilocks Selection chooses points that are \"just right\" and empirically outperforms the above approaches. Moreover, the selected sequence can transfer to other architectures; practitioners can share and reuse it without the need to recreate it."}}
