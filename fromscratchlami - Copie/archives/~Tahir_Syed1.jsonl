{"id": "4ifsThp4uK", "cdate": 1693134393186, "mdate": 1693134393186, "content": {"title": "Self-supervision for Tabular Data by Learning to Predict Additive Homoskedastic Gaussian Noise as Pretext", "abstract": "The lack of scalability of data annotation translates to the need to decrease dependency on labels. Self-supervision offers a solution with data training themselves. However, it has received relatively less attention on tabular data, data that drive a large proportion of business and application domains. This work, which we name the Statistical Self-Supervisor (SSS), proposes a method for self-supervision on tabular data by defining a continuous perturbation as pretext. It enables a neural network to learn representations by learning to predict the level of additive isotropic Gaussian noise added to inputs. The choice of the pretext transformation is motivated by intrinsic characteristics of a neural network fundamentally performing linear fits under the widely adopted assumption of Gaussianity in its fitting error and the preservation of locality of a data example on the data manifold in the presence of small random perturbations. The transform condenses information in the generated representations, making them better employable for further task-specific prediction as evidenced by performance improvement of the downstream classifier. To evaluate the persistence of performance under low-annotation settings, SSS is evaluated against different levels of label availability to the downstream classifier (1% to 100%) and benchmarked against self- and semi-supervised methods. At the most label-constrained, 1% setting, we report a maximum increase of at least 2.5% against the next-best semi-supervised competing method. We report an increase of more than 1.5% against self-supervised state of the art. Ablation studies also reveal that increasing label availability from 0% to 1% results in a maximum increase of up to 50% on either of the five performance metrics and up to 15% thereafter, indicating diminishing returns in additional annotation."}}
{"id": "znxAIAENPzP", "cdate": 1693134286845, "mdate": 1693134286845, "content": {"title": "Cross-validation for incremental data observability settings under causal covariate shift by using a Fisher Information penalty", "abstract": "As the practice of machine learning shifts focus from algorithms to large-scale collaborative computing systems, data orchestrated in time and space pose a challenge at the fundamental cross-validation step. In the event of training with data that arrive in a sequence rather than in the classical in-memorysample form, evolving feature densities across batches of training data bias cross-validation ([1]), making model selection and assessment unreliable. This work takes a distributed density estimation angle to the training setting where data are temporally distributed. The method, which we simply call Causal Covariate Shift Correction (C3), accumulates knowledge about the data density of a training batch using Fisher Information for posterior estimation. This estimate contains information about the local second-order gradients for the batch, and in our formulation, it is used to penalize the loss in all subsequent batches. This penalty partially anchors a batch\u2019s temporally-local density in that of the preceding batches, making the density estimate for the entire dataset more robust to non-iid covariate-density shift. We observe the inclusion of previous gradient information improves accuracy over the next best method by 12.9% and all other benchmarking methods when a complete dataset is accessible. We outperform the standard cross-validation baseline with 20.3% accuracy at maximum in batchwise data availability across all batches. We extend the investigation to k-fold cross-validation, ie with a non-predetermined test set, and we report 5.9% improvement at minimum in foldwise setup."}}
{"id": "sxWrw1kkYxy", "cdate": 1640995200000, "mdate": 1682512268564, "content": {"title": "Real-time forecasting of petrol retail using dilated causal CNNs", "abstract": "The recent popularity of smart cities and smart homes has made the adoption of Internet of Things (IoT) devices ubiquitous. Most of these IoT devices are low-end devices with limited capabilities. For neural network based predictive models, the low processing power of connected things is a limitation when training them. In addition, it is still a common practice to deploy these models on cloud servers that possess dedicated high performance computing hardware. However, for IoT applications, it is not feasible to send voluminous raw data to the cloud or a remote backend server on account of high latency, information security concerns or lack of network coverage. In this work, we develop an integrated prediction system for a retail petrol station within the operational constraints of the IoT ecosystem. Our main contribution is the combination of the recent concepts of dilated convolution and the so-called causal convolution into the 1D dilated causal convolutional neural network for time-series prediction. This results in a significantly lightweight model with sound mathematical grounding. Empirical evaluation with a highly-relaxed grid search in the hyperparameter space shows an order-of-magnitude improvement over competing models, both in terms of predictive performance as well as computational cost while training. Compared to state-of-the-art models, our proposed model is able reduce the root mean squared error from 49.38 to 18.37, and training time from 93.07 to 10.21 s on the petrol sales prediction problem."}}
{"id": "CFP-PB0KuT7", "cdate": 1640995200000, "mdate": 1682571995680, "content": {"title": "Potential Deep Learning Solutions to Persistent and Emerging Big Data Challenges - A Practitioners' Cookbook", "abstract": ""}}
{"id": "j7NXpSBSZhv", "cdate": 1609459200000, "mdate": 1682571995676, "content": {"title": "Deep Generative Models to Counter Class Imbalance: A Model-Metric Mapping With Proportion Calibration Methodology", "abstract": "The most pervasive segment of techniques in managing class imbalance in machine learning are re-sampling-based methods. The emergence of deep generative models for augmenting the size of the under-represented class, prompts one to review the question of the suitability of the model chosen for data augmentation with the metric selected for the-goodness-of classification. This work defines this suitability by using newly-sampled data points from each generative model first to the degree of parity, and studying classification performance on a large set of metrics. We extend the investigation to different proportions of augmented data points for identifying the sensitivity of the metric to the degree of imbalance, leading to the discovery of an optimum proportion against the metric. The models used are GAN, VAE and RBM and the metrics include Precision, Recall, F1-Score, AUC, G-Mean and Balanced Accuracy. We offer a comparison of these models with the established class of data synthesizing counterparts on the aforementioned metrics. Deep generative models outperform the state-of-the-art on 5 metrics on multiple datasets and also comprehensively surpass the baselines. This work thereby recommends the following model-metric mappings: VAE for high Precision and F1-Score, RBM for high Recall and GAN for high AUC, G-Mean and Balanced Accuracy under various recommended proportions of the minority class."}}
{"id": "B1nTcXw_5s8", "cdate": 1609459200000, "mdate": 1682571995678, "content": {"title": "A New Pooling Approach Based on Zeckendorf's Theorem for Texture Transfer Information", "abstract": "The pooling layer is at the heart of every convolutional neural network (CNN) contributing to the invariance of data variation. This paper proposes a pooling method based on Zeckendorf\u2019s number series. The maximum pooling layers are replaced with Z pooling layer, which capture texels from input images, convolution layers, etc. It is shown that Z pooling properties are better adapted to segmentation tasks than other pooling functions. The method was evaluated on a traditional image segmentation task and on a dense labeling task carried out with a series of deep learning architectures in which the usual maximum pooling layers were altered to use the proposed pooling mechanism. Not only does it arbitrarily increase the receptive field in a parameterless fashion but it can better tolerate rotations since the pooling layers are independent of the geometric arrangement or sizes of the image regions. Different combinations of pooling operations produce images capable of emphasizing low/high frequencies, extract ultrametric contours, etc."}}
{"id": "8fFAGS0Z_sM", "cdate": 1546300800000, "mdate": 1667283819419, "content": {"title": "Focused Anchors Loss: cost-sensitive learning of discriminative features for imbalanced classification", "abstract": "Deep Neural Networks (DNNs) usually suffer performance penalties when there is a skewed label distribution. This phenomenon, class-imbalance, is most often mitigated peripheral to the classificatio..."}}
{"id": "m196Wq30cz1", "cdate": 1514764800000, "mdate": 1682571995675, "content": {"title": "Appearance-based data augmentation for image datasets using contrast preserving sampling", "abstract": "Data augmentation techniques have been employed to overcome the problem of model over-fitting in deep convolutional neural networks and have consistently shown improvement in classification. Most data augmentation techniques perform affine transformations on the image domain. However these techniques cannot be used when object position is significant in the image. In this work we propose a data augmentation technique based on sampling an representation built by inequality constraints propagated from local binary patterns. We sample nine distinct variations for an image in a manner meant to preserve local structure and differ only in the amount of contrast between pixels. These contrast invariants are then used to augmented the original images. We present evaluations on CIFAR-10 and validate our gains in performance across four criteria, accuracy, precision, recall and Fl-Score; using a 2-layer convolutional neural network with different configuration of filters, and report improvement by about 13%, 9%, 12%, and 22% respectively over the baseline."}}
{"id": "gj-muwxl0MD", "cdate": 1483228800000, "mdate": 1682571995675, "content": {"title": "Z-Images", "abstract": "Local patterns and other patch based features have been an integral part of various computer vision applications as they encode local structural and statistical information. In this paper, we propose an image coding technique that utilizes Zeckendorf representation of pixel intensities and basic mathematical operators such as intersection, set difference, maximum, summation etc. for summarization of image regions. The algorithm produces a Z-coded image that tells about the homogeneity or the contrast in image regions with all codes in a range of 0 to 255."}}
{"id": "RDOJm0uG7jg", "cdate": 1483228800000, "mdate": 1682571995675, "content": {"title": "Regression Analysis for ATM Cash Flow Prediction", "abstract": ""}}
