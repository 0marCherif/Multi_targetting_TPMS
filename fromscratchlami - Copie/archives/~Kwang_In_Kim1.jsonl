{"id": "fnsLhSwS8e", "cdate": 1672531200000, "mdate": 1699155346945, "content": {"title": "Active Deep Learning Guided by Efficient Gaussian Process Surrogates", "abstract": "The success of active learning relies on the exploration of the underlying data-generating distributions, populating sparsely labeled data areas, and exploitation of the information about the task gained by the baseline (neural network) learners. In this paper, we present a new algorithm that combines these two active learning modes. Our algorithm adopts a Bayesian surrogate for the baseline learner, and it optimizes the exploration process by maximizing the gain of information caused by new labels. Further, by instantly updating the surrogate learner for each new data instance, our model can faithfully simulate and exploit the continuous learning behavior of the learner without having to actually retrain it per label. In experiments with four benchmark classification datasets, our method demonstrated significant performance gain over state-of-the-arts."}}
{"id": "PSGmY7Ah1B", "cdate": 1667647599016, "mdate": 1667647599016, "content": {"title": "S^2Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning", "abstract": "Despite the recent efforts in accurate 3D annotations in hand and object datasets, there still exist gaps in 3D hand and object recon- structions. Existing works leverage contact maps to refine inaccurate hand-object pose estimations and generate grasps given object models. However, they require explicit 3D supervision which is seldom available and therefore, are limited to constrained settings, e.g., where thermal cameras observe residual heat left on manipulated objects. In this pa- per, we propose a novel semi-supervised framework that allows us to learn contact from monocular images. Specifically, we leverage visual and geometric consistency constraints in large-scale datasets for generating pseudo-labels in semi-supervised learning and propose an efficient graph- based network to infer contact. Our semi-supervised learning framework achieves a favourable improvement over the existing supervised learning methods trained on data with \u2018limited\u2019 annotations. Notably, our pro- posed model is able to achieve superior results with less than half the network parameters and memory access cost when compared with the commonly-used PointNet-based approach. We show benefits from using a contact map that rules hand-object interactions to produce more accu- rate reconstructions. We further demonstrate that training with pseudo- labels can extend contact map estimations to out-of-domain objects and generalise better across multiple datasets. Project page is available."}}
{"id": "Z1p_6m0Wz-_", "cdate": 1667647487033, "mdate": 1667647487033, "content": {"title": "Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution", "abstract": "Estimating the pose and shape of hands and objects un- der interaction finds numerous applications including aug- mented and virtual reality. Existing approaches for hand and object reconstruction require explicitly defined physical constraints and known objects, which limits its application domains. Our algorithm is agnostic to object models, and it learns the physical rules governing hand-object interaction. This requires automatically inferring the shapes and physi- cal interaction of hands and (potentially unknown) objects. We seek to approach this challenging problem by propos- ing a collaborative learning strategy where two-branches of deep networks are learning from each other. Specifically, we transfer hand mesh information to the object branch and vice versa for the hand branch. The resulting optimi- sation (training) problem can be unstable, and we address this via two strategies: (i) attention-guided graph convo- lution which helps identify and focus on mutual occlusion and (ii) unsupervised associative loss which facilitates the transfer of information between the branches. Experiments using four widely-used benchmarks show that our frame- work achieves beyond state-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D hand and object shapes. Each technical component above contributes mean- ingfully in the ablation study."}}
{"id": "xyvUcCWSA4m", "cdate": 1667448769458, "mdate": 1667448769458, "content": {"title": "Combining task predictors via enhancing joint predictability", "abstract": "Predictor combination aims to improve a (target) predictor of\na learning task based on the (reference) predictors of potentially relevant\ntasks, without having access to the internals of individual predictors. We\npresent a new predictor combination algorithm that improves the target\nby i) measuring the relevance of references based on their capabilities in\npredicting the target, and ii) strengthening such estimated relevance. Un-\nlike existing predictor combination approaches that only exploit pairwise\nrelationships between the target and each reference, and thereby ignore\npotentially useful dependence among references, our algorithm jointly\nassesses the relevance of all references by adopting a Bayesian framework.\nThis also offers a rigorous way to automatically select only relevant ref-\nerences. Based on experiments on seven real-world datasets from visual\nattribute ranking and multi-class classification scenarios, we demonstrate\nthat our algorithm offers a significant performance gain and broadens\nthe application range of existing predictor combination approaches."}}
{"id": "TyUdFUUaPy6", "cdate": 1667448656230, "mdate": 1667448656230, "content": {"title": "Testing using privileged information by adapting features with statistical dependence", "abstract": "Given an imperfect predictor, we exploit additional features at test time to improve the predictions made, without retraining and without knowledge of the prediction function. This scenario arises if training labels or data are proprietary, restricted, or no longer available, or if training itself is prohibitively expensive. We assume that the additional features are useful if they exhibit strong statistical dependence to the underlying perfect predictor. Then, we empirically estimate and strengthen the statistical dependence between the initial noisy predictor and the additional features via manifold denoising. As an example, we show that this approach leads to improvement in real-world visual attribute ranking."}}
{"id": "PN3Y2Oy39rk", "cdate": 1667448597195, "mdate": 1667448597195, "content": {"title": "Improving predictors via combination across diverse task categories", "abstract": "Predictor combination is the problem of improving a task predictor using predictors of other tasks when the forms of individual predictors are unknown. Previous work approached this problem by nonparametrically assessing predictor relationships based on their joint evaluations on a shared sample. This limits their application to cases where all predictors are defined on the same task category, eg all predictors estimate attributes of shoes. We present a new predictor combination algorithm that overcomes this limitation. Our algorithm aligns the heterogeneous domains of different predictors in a shared latent space to facilitate comparisons of predictors independently of the domains on which they are originally defined. We facilitate this by a new data alignment scheme that matches data distributions across task categories. Based on visual attribute ranking experiments on datasets that span diverse task categories (eg shoes and animals), we demonstrate that our approach often significantly improves the performances of the initial predictors."}}
{"id": "wDr81QvEdgH", "cdate": 1667448487032, "mdate": 1667448487032, "content": {"title": "Robust Combination of Distributed Gradients Under Adversarial Perturbations", "abstract": "We consider distributed (gradient descent-based) learning scenarios where the server combines the gradients of learning objectives gathered from local clients. As individual data collection and learning environments can vary, some clients could transfer erroneous gradients eg, due to adversarial data or gradient perturbations. Further, for data privacy and security, the identities of such affected clients are often unknown to the server. In such cases, naively aggregating the resulting gradients can mislead the learning process. We propose a new server-side learning algorithm that robustly combines gradients. Our algorithm embeds the local gradients into the manifold of normalized gradients and refines their combinations via simulating a diffusion process therein. The resulting algorithm is instantiated as a computationally simple and efficient weighted gradient averaging algorithm. In the experiments with five classification and three regression benchmark datasets, our algorithm demonstrated significant performance improvements over existing robust gradient combination algorithms as well as the baseline uniform gradient averaging algorithm."}}
{"id": "ch1I2AAnVj", "cdate": 1667448444203, "mdate": 1667448444203, "content": {"title": "Active Label Correction Using Robust Parameter Update and Entropy Propagation", "abstract": "Label noise is prevalent in real-world visual learning applica-\ntions and correcting all label mistakes can be prohibitively costly. Train-\ning neural network classifiers on such noisy datasets may lead to signifi-\ncant performance degeneration. Active label correction (ALC) attempts\nto minimize the re-labeling costs by identifying examples for which pro-\nviding correct labels will yield maximal performance improvements. Ex-\nisting ALC approaches typically select the examples that the classifier\nis least confident about (e.g. with the largest entropies). However, such\nconfidence estimates can be unreliable as the classifier itself is initially\ntrained on noisy data. Also, na \u0308\u0131vely selecting a batch of low confidence\nexamples can result in redundant labeling of spatially adjacent exam-\nples. We present a new ALC algorithm that addresses these challenges.\nOur algorithm robustly estimates label confidence values by regulating\nthe contributions of individual examples in the parameter update of the\nnetwork. Further, our algorithm avoids redundant labeling by promoting\ndiversity in batch selection through propagating the confidence of each\nnewly labeled example to the entire dataset. Experiments involving four\nbenchmark datasets and two types of label noise demonstrate that our\nalgorithm offers a significant improvement in re-labeling efficiency over\nstate-of-the-art ALC approaches."}}
{"id": "lH90qpjv2IQ", "cdate": 1640995200000, "mdate": 1699155347026, "content": {"title": "Active Label Correction Using Robust Parameter Update and Entropy Propagation", "abstract": "Label noise is prevalent in real-world visual learning applications and correcting all label mistakes can be prohibitively costly. Training neural network classifiers on such noisy datasets may lead to significant performance degeneration. Active label correction (ALC) attempts to minimize the re-labeling costs by identifying examples for which providing correct labels will yield maximal performance improvements. Existing ALC approaches typically select the examples that the classifier is least confident about (e.g. with the largest entropies). However, such confidence estimates can be unreliable as the classifier itself is initially trained on noisy data. Also, na\u00efvely selecting a batch of low confidence examples can result in redundant labeling of spatially adjacent examples. We present a new ALC algorithm that addresses these challenges. Our algorithm robustly estimates label confidence values by regulating the contributions of individual examples in the parameter update of the network. Further, our algorithm avoids redundant labeling by promoting diversity in batch selection through propagating the confidence of each newly labeled example to the entire dataset. Experiments involving four benchmark datasets and two types of label noise demonstrate that our algorithm offers a significant improvement in re-labeling efficiency over state-of-the-art ALC approaches."}}
{"id": "AAgG9wN8O1", "cdate": 1640995200000, "mdate": 1699155347077, "content": {"title": "Robust Combination of Distributed Gradients Under Adversarial Perturbations", "abstract": "We consider distributed (gradient descent-based) learning scenarios where the server combines the gradients of learning objectives gathered from local clients. As individual data collection and learning environments can vary, some clients could transfer erroneous gradients e.g. due to ad-versarial data or gradient perturbations. Further, for data privacy and security, the identities of such affected clients are often unknown to the server. In such cases, naively ag-gregating the resulting gradients can mislead the learning process. We propose a new server-side learning algorithm that robustly combines gradients. Our algorithm embeds the local gradients into the manifold of normalized gradients and refines their combinations via simulating a diffusion process therein. The resulting algorithm is instantiated as a compu-tationally simple and efficient weighted gradient averaging algorithm. In the experiments with five classification and three regression benchmark datasets, our algorithm demon-strated significant performance improvements over existing robust gradient combination algorithms as well as the base-line uniform gradient averaging algorithm."}}
