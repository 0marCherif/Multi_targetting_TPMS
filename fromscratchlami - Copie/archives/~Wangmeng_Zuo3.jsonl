{"id": "Pd9W0OuifWN", "cdate": 1682406257815, "mdate": null, "content": {"title": "Learning Hierarchical Dynamics with Spatial Adjacency for Image Enhancement", "abstract": "In various real-world image enhancement applications, the degradations are always non-uniform or non-homogeneous and diverse, which challenges most deep networks with fixed parameters during the inference phase. Inspired by the dynamic deep networks that adapt the model structures or parameters conditioned on the inputs, we propose a DCP-guided hierarchical dynamic mechanism for image enhancement to adapt the model parameters and features from local to global as well as to keep spatial adjacency within the region. Specifically, channel-spatial-level, structure-level, and region-level dynamic components are sequentially applied. Channel-spatial-level dynamics obtain channel- and spatial-wise representation variations, and structure-level dynamics enable modeling geometric transformations and augment sampling locations for the varying local features to better describe the structures. In addition, a novel region-level dynamic is proposed to generate spatially continuous masks for dynamic features which capitalizes on the Dark Channel Priors (DCP). The proposed region-level dynamics benefit from exploiting the statistical differences between distorted and undistorted images. Moreover, the DCP-guided region generations are inherently spatial coherent which facilitates capturing local coherence of the images. The proposed method achieves state-of-the-art performance and generates visually pleasing images for multiple enhancement tasks,i.e. , image dehazing, image deraining and low-light image enhancement. The codes are available at https://github.com/DongLiangSXU/HDM."}}
{"id": "NTy-LF1zImp", "cdate": 1682405945513, "mdate": 1682405945513, "content": {"title": "Self-supervised Learning and Adaptation for Single Image Dehazing", "abstract": "Existing deep image dehazing methods usually depend on supervised learning with a large number of hazy-clean image pairs which are expensive or difficult to collect. Moreover, dehazing performance of the learned model may deteriorate significantly when the training hazy-clean image pairs are insufficient and are different from real hazy images in applications. In this paper, we show that exploiting large scale training set and adapting to real hazy images are two critical issues in learning effective deep dehazing models. Under the depth guidance estimated by a well-trained depth estimation network, we leverage the conventional atmospheric scattering model to generate massive hazy-clean image pairs for the self-supervised pre-training of dehazing network. Furthermore, self-supervised adaptation is presented to adapt pre-trained network to real hazy images. Learning without forgetting strategy is also deployed in self-supervised adaptation by combining self-supervision and model adaptation via contrastive learning. Experiments show that our proposed method performs favorably against the state-of-the-art methods, and is quite efficient, i.e., handling a 4K image in 23 ms. The codes are available at https://github.com/DongLiangSXU/SLAdehazing."}}
{"id": "faHwz9ppEJ", "cdate": 1675413001737, "mdate": 1675413001737, "content": {"title": "An Intermediate-level Attack Framework on The Basis of Linear Regression", "abstract": "This paper substantially extends our work published at ECCV [1], in which an intermediate-level attack was proposed to improve the transferability of some baseline adversarial examples. Specifically, we advocate a framework in which a direct linear mapping from the intermediate-level discrepancies (between adversarial features and benign features) to prediction loss of the adversarial example is established. By delving deep into the core components of such a framework, we show that 1) a variety of linear regression models can all be considered in order to establish the mapping, 2) the magnitude of the finally obtained intermediate-level adversarial discrepancy is correlated with the transferability, 3) further boost of the performance can be achieved by performing multiple runs of the baseline attack with random initialization. In addition, by leveraging these findings, we achieve new state-of-the-arts on transfer-based l\u221e and l2 attacks. Our code is publicly available at https://github.com/qizhangli/ila-plus-plus-lr."}}
{"id": "1-aVx_A42n7", "cdate": 1668517654621, "mdate": 1668517654621, "content": {"title": "Learning class-agnostic pseudo mask generation for box-supervised semantic segmentation", "abstract": "Recently, several weakly supervised learning methods have been devoted to utilize bounding box supervision for training deep semantic segmentation models. Most existing methods usually leverage the generic proposal generators (e.g., dense CRF and MCG) to produce enhanced segmentation masks for further training segmentation models. These proposal generators, however, are generic and not specifically designed for box-supervised semantic segmentation, thereby leaving some leeway for improving segmentation performance. In this paper, we aim at seeking for a more accurate learning-based class-agnostic pseudo mask generator tailored to box-supervised semantic segmentation. To this end, we resort to an auxiliary dataset with pixel-level annotation while the class labels are non-overlapped with those of the box-annotated dataset. For learning pseudo mask generator from the auxiliary dataset, we present a bi-level optimization formulation. In particular, the lower subproblem is used to learn box-supervised semantic segmentation, while the upper subproblem is used to learn an optimal class-agnostic pseudo mask generator. The learned pseudo segmentation mask generator can then be deployed to the box-annotated dataset for improving weakly supervised semantic segmentation. Experiments on PASCAL VOC 2012 dataset show\nthat the learned pseudo mask generator is effective in boosting segmentation performance, and our method can largely close the performance gap between box-supervised and fully-supervised models."}}
{"id": "O5pC0Hd3h7O", "cdate": 1668517446984, "mdate": 1668517446984, "content": {"title": "Image inpainting with edge-guided learnable bidirectional attention maps", "abstract": "For image inpainting, the convolutional neural networks (CNN) in previous methods often adopt standard convolutional\noperator, which treats valid pixels and holes indistinguishably. As a result, they are limited in handling irregular holes and tend to\nproduce color-discrepant and blurry inpainting result. Partial convolution (PConv) copes with this issue by conducting masked\nconvolution and feature re-normalization conditioned only on valid pixels, but the mask-updating is handcrafted and independent with\nimage structural information. In this paper, we present an edge-guided learnable bidirectional attention map (Edge-LBAM) for\nimproving image inpainting of irregular holes with several distinct merits. Instead of using a hard 0-1 mask, a learnable attention map\nmodule is introduced for learning feature re-normalization and mask-updating in an end-to-end manner. Learnable reverse attention\nmaps are further proposed in the decoder for emphasizing on filling in unknown pixels instead of reconstructing all pixels. Motivated by\nthat the filling-in order is crucial to inpainting results and largely depends on image structures in exemplar-based methods, we further\nsuggest a multi-scale edge completion network to predict coherent edges. Our Edge-LBAM method contains dual procedures,\nincluding structure-aware mask-updating guided by predict edges and attention maps generated by masks for feature re-normalization.\nExtensive experiments show that our Edge-LBAM is effective in generating coherent image structures and preventing color\ndiscrepancy and blurriness, and performs favorably against the state-of-the-art methods in terms of qualitative metrics and visual\nquality. The source code and pre-trained models are available at https://github.com/wds1998/Edge-LBAM."}}
{"id": "O7qGCOrgQoN", "cdate": 1668517255828, "mdate": 1668517255828, "content": {"title": "Image Inpainting with Learnable Bidirectional Attention Maps", "abstract": "Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial\nconvolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature re-normalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available at: https://github.com/Vious/LBAM_inpainting/.\n"}}
{"id": "KJsKK8uvpX", "cdate": 1668511052174, "mdate": 1668511052174, "content": {"title": "Towards Photo-Realistic Virtual Try-On by Adaptively Generating-Preserving Image Content", "abstract": "Image visual try-on aims at transferring a target clothes image onto a reference person, and has become a hot topic in recent years. Prior arts usually focus on preserving the character of a clothes image (e.g. texture, logo, embroidery) when warping it to arbitrary human pose. However, it remains a big challenge to generate photo-realistic try-on images when large occlusions and human poses are presented in the reference person. To address this issue, we propose a novel visual try-on network, namely Adaptive Content Generating and Preserving Network (ACGPN). In particular, ACGPN first predicts semantic layout of the reference image that will be changed after try-on (e.g.long sleeve shirt-arm, arm-jacket), and then determines whether its image content needs to be generated or preserved according to the predicted semantic layout, leading to photo-realistic try-on and rich clothes details. ACGPN generally involves three major modules. First, a semantic layout generation module utilizes semantic segmentation of the reference image to progressively predict the desired semantic layout after try-on. Second, a clothes warping module warps clothes image according to the generated semantic layout, where a second-order difference constraint is introduced to stabilize the warping process during training.Third, an inpainting module for content fusion integrates all information (e.g. reference image, semantic layout, warped clothes) to adaptively produce each semantic part of human body. In comparison to the state-of-the-art methods, ACGPN can generate photo-realistic images with much better perceptual quality and richer fine-details."}}
{"id": "bjPPypbLre", "cdate": 1663850558453, "mdate": null, "content": {"title": "Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples", "abstract": "The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https://github.com/qizhangli/MoreBayesian-attack. "}}
{"id": "Z_tmYu060Kr", "cdate": 1663850243583, "mdate": null, "content": {"title": "Squeeze Training for Adversarial Robustness", "abstract": "The vulnerability of deep neural networks (DNNs) to adversarial examples has attracted great attention in the machine learning community. The problem is related to non-flatness and non-smoothness of normally obtained loss landscapes. Training augmented with adversarial examples (a.k.a., adversarial training) is considered as an effective remedy. In this paper, we highlight that some collaborative examples, nearly perceptually indistinguishable from both adversarial and benign examples yet show extremely lower prediction loss, can be utilized to enhance adversarial training. A novel method is therefore proposed to achieve new state-of-the-arts in adversarial robustness. Code: https://github.com/qizhangli/ST-AT."}}
{"id": "9MbhFHqrti9", "cdate": 1663849995081, "mdate": null, "content": {"title": "ImaginaryNet: Learning Object Detectors without Real Images and Annotations", "abstract": "Without the demand of training in reality, humans are able of detecting a new category of object simply based on the language description on its visual characteristics. Empowering deep learning with this ability undoubtedly enables the neural network to handle complex vision tasks, e.g., object detection, without collecting and annotating real images. To this end, this paper introduces a novel challenging learning paradigm Imaginary-Supervised Object Detection (ISOD), where neither real images nor manual annotations are allowed for training object detectors. To resolve this challenge, we propose ImaginaryNet, a framework to synthesize images by combining pretrained language model and text-to-image synthesis model. Given a class label, the language model is used to generate a full description of a scene with a target object, and the text-to-image model is deployed to generate a photo-realistic image. With the synthesized images and class labels, weakly supervised object detection can then be leveraged to accomplish ISOD. By gradually introducing real images and manual annotations, ImaginaryNet can collaborate with other supervision settings to further boost detection performance. Experiments show that ImaginaryNet can (i) obtain about 75% performance in ISOD compared with the weakly supervised counterpart of the same backbone trained on real data, (ii) significantly improve the baseline while achieving state-of-the-art or comparable performance by incorporating ImaginaryNet with other supervision settings.  Our code will be publicly available at https://github.com/kodenii/ImaginaryNet."}}
