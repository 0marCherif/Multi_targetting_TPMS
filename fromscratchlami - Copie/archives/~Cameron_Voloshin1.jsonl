{"id": "yZcPRIZEwOG", "cdate": 1652737697043, "mdate": null, "content": {"title": "Policy Optimization with Linear Temporal Logic Constraints", "abstract": "We study the problem of policy optimization (PO) with linear temporal logic (LTL) constraints. The language of LTL allows flexible description of tasks that may be unnatural to encode as a scalar cost function. We consider LTL-constrained PO as a systematic framework, decoupling task specification from policy selection, and an alternative to the standard of cost shaping. With access to a generative model, we develop a model-based approach that enjoys a sample complexity analysis for guaranteeing both task satisfaction and cost optimality (through a reduction to a reachability problem). Empirically, our algorithm can achieve strong  performance even in low sample regimes."}}
{"id": "IsK8iKbL-I", "cdate": 1623139482142, "mdate": null, "content": {"title": "Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning", "abstract": "We offer an experimental benchmark and empirical study for off-policy policy evaluation (OPE) in reinforcement learning, which is a key problem in many safety critical applications. Given the increasing interest in deploying learning-based methods, there has been a flurry of recent proposals for OPE method, leading to a need for standardized empirical analyses.  Our work takes a strong focus on diversity of experimental design to enable stress testing of OPE methods.  We provide a comprehensive benchmarking suite to study the interplay of different attributes on method performance. We distill the results into a summarized set of guidelines for OPE in practice. Our software package, the Caltech OPE Benchmarking Suite (COBS), is open-sourced and we invite interested researchers to further contribute to the benchmark."}}
{"id": "BJEgGi-OZH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Batch Policy Learning under Constraints", "abstract": "When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different..."}}
