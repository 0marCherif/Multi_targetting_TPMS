{"id": "ydi4tuCWgij", "cdate": 1680307200000, "mdate": 1682521324539, "content": {"title": "Causality-Inspired Single-Source Domain Generalization for Medical Image Segmentation", "abstract": "Deep learning models usually suffer from the domain shift issue, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data are only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. In this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation scenarios: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-site prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains."}}
{"id": "qyQzl-yM-CF", "cdate": 1640995200000, "mdate": 1668456928984, "content": {"title": "Improved Post-hoc Probability Calibration for Out-of-Domain MRI Segmentation", "abstract": "Probability calibration for deep models is highly desirable in safety-critical applications such as medical imaging. It makes output probabilities of deep networks interpretable, by aligning prediction probability with the actual accuracy in test data. In image segmentation, well-calibrated probabilities allow radiologists to identify regions where model-predicted segmentations are unreliable. These unreliable predictions often occur to out-of-domain (OOD) images that are caused by imaging artifacts or unseen imaging protocols. Unfortunately, most previous calibration methods for image segmentation perform sub-optimally on OOD images. To reduce the calibration error when confronted with OOD images, we propose a novel post-hoc calibration model. Our model leverages the pixel susceptibility against perturbations at the local level, and the shape prior information at the global level. The model is tested on cardiac MRI segmentation datasets that contain unseen imaging artifacts and images from an unseen imaging protocol. We demonstrate reduced calibration errors compared with the state-of-the-art calibration algorithm."}}
{"id": "oOmTHS3Do7", "cdate": 1640995200000, "mdate": 1681652897720, "content": {"title": "Enhancing MR image segmentation with realistic adversarial data augmentation", "abstract": ""}}
{"id": "mvXUxD1iO4", "cdate": 1640995200000, "mdate": 1668456928974, "content": {"title": "Cardiac segmentation on late gadolinium enhancement MRI: A benchmark study from multi-sequence cardiac MR segmentation challenge", "abstract": ""}}
{"id": "aoxBywOEBTA", "cdate": 1640995200000, "mdate": 1668456929072, "content": {"title": "MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation", "abstract": "Convolutional neural networks (CNNs) have achieved remarkable segmentation accuracy on benchmark datasets where training and test sets are from the same domain, yet their performance can degrade significantly on unseen domains, which hinders the deployment of CNNs in many clinical scenarios. Most existing works improve model out-of-domain (OOD) robustness by collecting multi-domain datasets for training, which is expensive and may not always be feasible due to privacy and logistical issues. In this work, we focus on improving model robustness using a single-domain dataset only. We propose a novel data augmentation framework called MaxStyle, which maximizes the effectiveness of style augmentation for model OOD performance. It attaches an auxiliary style-augmented image decoder to a segmentation network for robust feature learning and data augmentation. Importantly, MaxStyle augments data with improved image style diversity and hardness, by expanding the style space with noise and searching for the worst-case style composition of latent features via adversarial training. With extensive experiments on multiple public cardiac and prostate MR datasets, we demonstrate that MaxStyle leads to significantly improved out-of-distribution robustness against unseen corruptions as well as common distribution shifts across multiple, different, unseen sites and unknown image sequences under both low- and high-training data settings. The code can be found at https://github.com/cherise215/MaxStyle."}}
{"id": "SfVz22wwwF", "cdate": 1640995200000, "mdate": 1668456929094, "content": {"title": "The Extreme Cardiac MRI Analysis Challenge under Respiratory Motion (CMRxMotion)", "abstract": "The quality of cardiac magnetic resonance (CMR) imaging is susceptible to respiratory motion artifacts. The model robustness of automated segmentation techniques in face of real-world respiratory motion artifacts is unclear. This manuscript describes the design of extreme cardiac MRI analysis challenge under respiratory motion (CMRxMotion Challenge). The challenge aims to establish a public benchmark dataset to assess the effects of respiratory motion on image quality and examine the robustness of segmentation models. The challenge recruited 40 healthy volunteers to perform different breath-hold behaviors during one imaging visit, obtaining paired cine imaging with artifacts. Radiologists assessed the image quality and annotated the level of respiratory motion artifacts. For those images with diagnostic quality, radiologists further segmented the left ventricle, left ventricle myocardium and right ventricle. The images of training set (20 volunteers) along with the annotations are released to the challenge participants, to develop an automated image quality assessment model (Task 1) and an automated segmentation model (Task 2). The images of validation set (5 volunteers) are released to the challenge participants but the annotations are withheld for online evaluation of submitted predictions. Both the images and annotations of the test set (15 volunteers) were withheld and only used for offline evaluation of submitted containerized dockers. The image quality assessment task is quantitatively evaluated by the Cohen's kappa statistics and the segmentation task is evaluated by the Dice scores and Hausdorff distances."}}
{"id": "NRH755Inmb", "cdate": 1640995200000, "mdate": 1668456928977, "content": {"title": "Self-Supervised Learning for Few-Shot Medical Image Segmentation", "abstract": ""}}
{"id": "MXLnycQTWAY", "cdate": 1640995200000, "mdate": 1668456928990, "content": {"title": "MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation", "abstract": "Convolutional neural networks (CNNs) have achieved remarkable segmentation accuracy on benchmark datasets where training and test sets are from the same domain, yet their performance can degrade significantly on unseen domains, which hinders the deployment of CNNs in many clinical scenarios. Most existing works improve model out-of-domain (OOD) robustness by collecting multi-domain datasets for training, which is expensive and may not always be feasible due to privacy and logistical issues. In this work, we focus on improving model robustness using a single-domain dataset only. We propose a novel data augmentation framework called MaxStyle, which maximizes the effectiveness of style augmentation for model OOD performance. It attaches an auxiliary style-augmented image decoder to a segmentation network for robust feature learning and data augmentation. Importantly, MaxStyle augments data with improved image style diversity and hardness, by expanding the style space with noise and searching for the worst-case style composition of latent features via adversarial training. With extensive experiments on multiple public cardiac and prostate MR datasets, we demonstrate that MaxStyle leads to significantly improved out-of-distribution robustness against unseen corruptions as well as common distribution shifts across multiple, different, unseen sites and unknown image sequences under both low- and high-training data settings. The code can be found at https://github.com/cherise215/MaxStyle ."}}
{"id": "Em0mb_jY4-h", "cdate": 1640995200000, "mdate": 1682521324538, "content": {"title": "Fetal Cortex Segmentation with Topology and Thickness Loss Constraints", "abstract": "The segmentation of the fetal cerebral cortex from magnetic resonance imaging (MRI) is an important tool for neurobiological research about the developing human brain. Manual segmentation is difficult and time-consuming. Limited image resolution and partial volume effects introduce errors and labeling noise when attempting to automate the process through machine learning. The significant morphological changes observed during brain growth pose additional challenges for learning-based image segmentation methods, which may drastically increase the amount of necessary training data. In this paper, we propose a framework to learn from noisy labels by using additional regularization via shape priors for the accurate segmentation of the cortical gray matter (CGM) in 3D. Firstly, we introduce a novel structure consistency loss based on persistent homology analysis of the cortical topology. Secondly, a regularization loss term is proposed by integrating assumptions about the cortical thickness within each sample. Our experiments on the developing human connectome project (dHCP) dataset show that our method can predict accurate CGM segmentation learned from noisy labels."}}
{"id": "4Iv1QJTk0Y4", "cdate": 1640995200000, "mdate": 1682521324538, "content": {"title": "Context Label Learning: Improving Background Class Representations in Semantic Segmentation", "abstract": "Background samples provide key contextual information for segmenting regions of interest (ROIs). However, they always cover a diverse set of structures, causing difficulties for the segmentation model to learn good decision boundaries with high sensitivity and precision. The issue concerns the highly heterogeneous nature of the background class, resulting in multi-modal distributions. Empirically, we find that neural networks trained with heterogeneous background struggle to map the corresponding contextual samples to compact clusters in feature space. As a result, the distribution over background logit activations may shift across the decision boundary, leading to systematic over-segmentation across different datasets and tasks. In this study, we propose context label learning (CoLab) to improve the context representations by decomposing the background class into several subclasses. Specifically, we train an auxiliary network as a task generator, along with the primary segmentation model, to automatically generate context labels that positively affect the ROI segmentation accuracy. Extensive experiments are conducted on several challenging segmentation tasks and datasets. The results demonstrate that CoLab can guide the segmentation model to map the logits of background samples away from the decision boundary, resulting in significantly improved segmentation accuracy. Code is available."}}
