{"id": "fagZ6hUQ4Ff", "cdate": 1672531200000, "mdate": 1682348598060, "content": {"title": "Follow-the-Perturbed-Leader Achieves Best-of-Both-Worlds for Bandit Problems", "abstract": "This paper discusses the adversarial and stochastic $K$-armed bandit problems. In the adversarial setting, the best possible regret is known to be $O(\\sqrt{KT})$ for time horizon $T$. This bound ca..."}}
{"id": "LviF8pOnJp", "cdate": 1672531200000, "mdate": 1682348598127, "content": {"title": "Best-of-Both-Worlds Algorithms for Partial Monitoring", "abstract": "This study considers the partial monitoring problem with $k$-actions and $d$-outcomes and provides the first best-of-both-worlds algorithms, whose regrets are favorably bounded both in the stochast..."}}
{"id": "rF6zwkyMABn", "cdate": 1652737663399, "mdate": null, "content": {"title": "Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with Feedback Graphs", "abstract": "This study considers online learning with general directed feedback graphs. For this problem, we present best-of-both-worlds algorithms that achieve nearly tight regret bounds for adversarial environments as well as poly-logarithmic regret bounds for stochastic environments. As Alon et al. [2015] have shown, tight regret bounds depend on the structure of the feedback graph: strongly observable graphs yield minimax regret of $\\tilde{\\Theta}( \\alpha^{1/2} T^{1/2} )$, while weakly observable graphs induce minimax regret of $\\tilde{\\Theta}( \\delta^{1/3} T^{2/3} )$, where $\\alpha$ and $\\delta$, respectively, represent the independence number of the graph and the domination number of a certain portion of the graph. Our proposed algorithm for strongly observable graphs has a regret bound of $\\tilde{O}( \\alpha^{1/2} T^{1/2} )$ for adversarial environments, as well as of  $ {O} ( \\frac{\\alpha (\\ln T)^3 }{\\Delta_{\\min}} ) $ for stochastic environments, where $\\Delta_{\\min}$ expresses the minimum suboptimality gap. This result resolves an open question raised by Erez and Koren [2021]. We also provide an algorithm for weakly observable graphs that achieves a regret bound of $\\tilde{O}( \\delta^{1/3}T^{2/3} )$ for adversarial environments and poly-logarithmic regret for stochastic environments. The proposed algorithms are based on the follow-the-regularized-leader approach combined with newly designed update rules for learning rates."}}
{"id": "TIQfmR7IF6H", "cdate": 1652737616447, "mdate": null, "content": {"title": "Minimax Optimal Algorithms for Fixed-Budget Best Arm Identification", "abstract": "We consider the fixed-budget best arm identification problem where the goal is to find the arm of the largest mean with a fixed number of samples. It is known that the probability of misidentifying the best arm is exponentially small to the number of rounds. However, limited characterizations have been discussed on the rate (exponent) of this value. In this paper, we characterize the minimax optimal rate as a result of an optimization over all possible parameters. We introduce two rates, $R^{\\mathrm{go}}$ and $R^{\\mathrm{go}}_{\\infty}$, corresponding to lower bounds on the probability of misidentification, each of which is associated with a proposed algorithm. The rate $R^{\\mathrm{go}}$ is associated with $R^{\\mathrm{go}}$-tracking, which can be efficiently implemented by a neural network and is shown to outperform existing algorithms. However, this rate requires a nontrivial condition to be achievable. To address this issue, we introduce the second rate $R^{\\mathrm{go}}_\\infty$. We show that this rate is indeed achievable by introducing a conceptual algorithm called delayed optimal tracking (DOT)."}}
{"id": "uUgEM3KR2DL", "cdate": 1640995200000, "mdate": 1684096158104, "content": {"title": "Adversarially Robust Multi-Armed Bandit Algorithm with Variance-Dependent Regret Bounds", "abstract": "This paper considers the multi-armed bandit (MAB) problem and provides a new best-of-both-worlds (BOBW) algorithm that works nearly optimally in both stochastic and adversarial settings. In stochastic settings, some existing BOBW algorithms achieve tight gap-dependent regret bounds of $O(\\sum_{i: \\Delta_i>0} \\frac{\\log T}{\\Delta_i})$ for suboptimality gap $\\Delta_i$ of arm $i$ and time horizon $T$. As Audibert et al. [2007] have shown, however, that the performance can be improved in stochastic environments with low-variance arms. In fact, they have provided a stochastic MAB algorithm with gap-variance-dependent regret bounds of $O(\\sum_{i: \\Delta_i>0} (\\frac{\\sigma_i^2}{\\Delta_i} + 1) \\log T )$ for loss variance $\\sigma_i^2$ of arm $i$. In this paper, we propose the first BOBW algorithm with gap-variance-dependent bounds, showing that the variance information can be used even in the possibly adversarial environment. Further, the leading constant factor in our gap-variance dependent bound is only (almost) twice the value for the lower bound. Additionally, the proposed algorithm enjoys multiple data-dependent regret bounds in adversarial settings and works well in stochastic settings with adversarial corruptions. The proposed algorithm is based on the follow-the-regularized-leader method and employs adaptive learning rates that depend on the empirical prediction error of the loss, which leads to gap-variance-dependent regret bounds reflecting the variance of the arms."}}
{"id": "ojr181RrejF", "cdate": 1640995200000, "mdate": 1682348597988, "content": {"title": "Adversarially Robust Multi-Armed Bandit Algorithm with Variance-Dependent Regret Bounds", "abstract": "This paper considers the multi-armed bandit (MAB) problem and provides a new best-of-both-worlds (BOBW) algorithm that works nearly optimally in both stochastic and adversarial settings. In stochas..."}}
{"id": "gBUFyRU466q", "cdate": 1640995200000, "mdate": 1684096158169, "content": {"title": "Best-of-Both-Worlds Algorithms for Partial Monitoring", "abstract": "This study considers the partial monitoring problem with $k$-actions and $d$-outcomes and provides the first best-of-both-worlds algorithms, whose regrets are favorably bounded both in the stochastic and adversarial regimes. In particular, we show that for non-degenerate locally observable games, the regret is $O(m^2 k^4 \\log(T) \\log(k_{\\Pi} T) / \\Delta_{\\min})$ in the stochastic regime and $O(m k^{2/3} \\sqrt{T \\log(T) \\log k_{\\Pi}})$ in the adversarial regime, where $T$ is the number of rounds, $m$ is the maximum number of distinct observations per action, $\\Delta_{\\min}$ is the minimum suboptimality gap, and $k_{\\Pi}$ is the number of Pareto optimal actions. Moreover, we show that for globally observable games, the regret is $O(c_{\\mathcal{G}}^2 \\log(T) \\log(k_{\\Pi} T) / \\Delta_{\\min}^2)$ in the stochastic regime and $O((c_{\\mathcal{G}}^2 \\log(T) \\log(k_{\\Pi} T))^{1/3} T^{2/3})$ in the adversarial regime, where $c_{\\mathcal{G}}$ is a game-dependent constant. We also provide regret bounds for a stochastic regime with adversarial corruptions. Our algorithms are based on the follow-the-regularized-leader framework and are inspired by the approach of exploration by optimization and the adaptive learning rate in the field of online learning with feedback graphs."}}
{"id": "f7N5te4pcqI", "cdate": 1640995200000, "mdate": 1684096158116, "content": {"title": "Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with Feedback Graphs", "abstract": "This study considers online learning with general directed feedback graphs. For this problem, we present best-of-both-worlds algorithms that achieve nearly tight regret bounds for adversarial environments as well as poly-logarithmic regret bounds for stochastic environments. As Alon et al. [2015] have shown, tight regret bounds depend on the structure of the feedback graph: strongly observable graphs yield minimax regret of $\\tilde{\\Theta}( \\alpha^{1/2} T^{1/2} )$, while weakly observable graphs induce minimax regret of $\\tilde{\\Theta}( \\delta^{1/3} T^{2/3} )$, where $\\alpha$ and $\\delta$, respectively, represent the independence number of the graph and the domination number of a certain portion of the graph. Our proposed algorithm for strongly observable graphs has a regret bound of $\\tilde{O}( \\alpha^{1/2} T^{1/2} ) $ for adversarial environments, as well as of $ {O} ( \\frac{\\alpha (\\ln T)^3 }{\\Delta_{\\min}} ) $ for stochastic environments, where $\\Delta_{\\min}$ expresses the minimum suboptimality gap. This result resolves an open question raised by Erez and Koren [2021]. We also provide an algorithm for weakly observable graphs that achieves a regret bound of $\\tilde{O}( \\delta^{1/3}T^{2/3} )$ for adversarial environments and poly-logarithmic regret for stochastic environments. The proposed algorithms are based on the follow-the-regularized-leader approach combined with newly designed update rules for learning rates."}}
{"id": "Xe-0fDL4ne", "cdate": 1640995200000, "mdate": 1683896063972, "content": {"title": "Globally Optimal Algorithms for Fixed-Budget Best Arm Identification", "abstract": "We consider the fixed-budget best arm identification problem where the goal is to find the arm of the largest mean with a fixed number of samples. It is known that the probability of misidentifying the best arm is exponentially small to the number of rounds. However, limited characterizations have been discussed on the rate (exponent) of this value. In this paper, we characterize the minimax optimal rate as a result of an optimization over all possible parameters. We introduce two rates, $R^{\\mathrm{go}}$ and $R^{\\mathrm{go}}_{\\infty}$, corresponding to lower bounds on the probability of misidentification, each of which is associated with a proposed algorithm. The rate $R^{\\mathrm{go}}$ is associated with $R^{\\mathrm{go}}$-tracking, which can be efficiently implemented by a neural network and is shown to outperform existing algorithms. However, this rate requires a nontrivial condition to be achievable. To address this issue, we introduce the second rate $R^{\\mathrm{go}}_\\infty$. We show that this rate is indeed achievable by introducing a conceptual algorithm called delayed optimal tracking (DOT)."}}
{"id": "Ew3UYSCrqlX", "cdate": 1640995200000, "mdate": 1684096158103, "content": {"title": "Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with Feedback Graphs", "abstract": "This study considers online learning with general directed feedback graphs. For this problem, we present best-of-both-worlds algorithms that achieve nearly tight regret bounds for adversarial environments as well as poly-logarithmic regret bounds for stochastic environments. As Alon et al. [2015] have shown, tight regret bounds depend on the structure of the feedback graph: strongly observable graphs yield minimax regret of $\\tilde{\\Theta}( \\alpha^{1/2} T^{1/2} )$, while weakly observable graphs induce minimax regret of $\\tilde{\\Theta}( \\delta^{1/3} T^{2/3} )$, where $\\alpha$ and $\\delta$, respectively, represent the independence number of the graph and the domination number of a certain portion of the graph. Our proposed algorithm for strongly observable graphs has a regret bound of $\\tilde{O}( \\alpha^{1/2} T^{1/2} )$ for adversarial environments, as well as of $ {O} ( \\frac{\\alpha (\\ln T)^3 }{\\Delta_{\\min}} ) $ for stochastic environments, where $\\Delta_{\\min}$ expresses the minimum suboptimality gap. This result resolves an open question raised by Erez and Koren [2021]. We also provide an algorithm for weakly observable graphs that achieves a regret bound of $\\tilde{O}( \\delta^{1/3}T^{2/3} )$ for adversarial environments and poly-logarithmic regret for stochastic environments. The proposed algorithms are based on the follow-the-regularized-leader approach combined with newly designed update rules for learning rates."}}
