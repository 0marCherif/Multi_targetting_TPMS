{"id": "yQ1rrPPnD3w", "cdate": 1672531200000, "mdate": 1682323639621, "content": {"title": "Best of Both Worlds Policy Optimization", "abstract": "Policy optimization methods are popular reinforcement learning algorithms in practice. Recent works have built theoretical foundation for them by proving $\\sqrt{T}$ regret bounds even when the losses are adversarial. Such bounds are tight in the worst case but often overly pessimistic. In this work, we show that in tabular Markov decision processes (MDPs), by properly designing the regularizer, the exploration bonus and the learning rates, one can achieve a more favorable polylog$(T)$ regret when the losses are stochastic, without sacrificing the worst-case guarantee in the adversarial regime. To our knowledge, this is also the first time a gap-dependent polylog$(T)$ regret bound is shown for policy optimization. Specifically, we achieve this by leveraging a Tsallis entropy or a Shannon entropy regularizer in the policy update. Then we show that under known transitions, we can further obtain a first-order regret bound in the adversarial regime by leveraging the log-barrier regularizer."}}
{"id": "oLKpBd5e8wQ", "cdate": 1672531200000, "mdate": 1682323639620, "content": {"title": "A Blackbox Approach to Best of Both Worlds in Bandits and Beyond", "abstract": "Best-of-both-worlds algorithms for online learning which achieve near-optimal regret in both the adversarial and the stochastic regimes have received growing attention recently. Existing techniques often require careful adaptation to every new problem setup, including specialised potentials and careful tuning of algorithm parameters. Yet, in domains such as linear bandits, it is still unknown if there exists an algorithm that can simultaneously obtain $O(\\log(T))$ regret in the stochastic regime and $\\tilde{O}(\\sqrt{T})$ regret in the adversarial regime. In this work, we resolve this question positively and present a general reduction from best of both worlds to a wide family of follow-the-regularized-leader (FTRL) and online-mirror-descent (OMD) algorithms. We showcase the capability of this reduction by transforming existing algorithms that are only known to achieve worst-case guarantees into new algorithms with best-of-both-worlds guarantees in contextual bandits, graph bandits and tabular Markov decision processes."}}
{"id": "Py8__XDu6N", "cdate": 1672531200000, "mdate": 1675702645080, "content": {"title": "Refined Regret for Adversarial MDPs with Linear Function Approximation", "abstract": "We consider learning in an adversarial Markov Decision Process (MDP) where the loss functions can change arbitrarily over $K$ episodes and the state space can be arbitrarily large. We assume that the Q-function of any policy is linear in some known features, that is, a linear function approximation exists. The best existing regret upper bound for this setting (Luo et al., 2021) is of order $\\tilde{\\mathcal O}(K^{2/3})$ (omitting all other dependencies), given access to a simulator. This paper provides two algorithms that improve the regret to $\\tilde{\\mathcal O}(\\sqrt K)$ in the same setting. Our first algorithm makes use of a refined analysis of the Follow-the-Regularized-Leader (FTRL) algorithm with the log-barrier regularizer. This analysis allows the loss estimators to be arbitrarily negative and might be of independent interest. Our second algorithm develops a magnitude-reduced loss estimator, further removing the polynomial dependency on the number of actions in the first algorithm and leading to the optimal regret bound (up to logarithmic terms and dependency on the horizon). Moreover, we also extend the first algorithm to simulator-free linear MDPs, which achieves $\\tilde{\\mathcal O}(K^{8/9})$ regret and greatly improves over the best existing bound $\\tilde{\\mathcal O}(K^{14/15})$. This algorithm relies on a better alternative to the Matrix Geometric Resampling procedure by Neu & Olkhovskaya (2020), which could again be of independent interest."}}
{"id": "DO8yo04JcB", "cdate": 1672531200000, "mdate": 1682323639179, "content": {"title": "A Unified Algorithm for Stochastic Path Problems", "abstract": "We study reinforcement learning in stochastic path (SP) problems. The goal in these problems is to maximize the expected sum of rewards until the agent reaches a terminal state. We provide the firs..."}}
{"id": "VgX6ceDerh2", "cdate": 1652737568627, "mdate": null, "content": {"title": "Stochastic Online Learning with Feedback Graphs: Finite-Time and Asymptotic Optimality", "abstract": "We revisit the problem of stochastic online learning with feedback\ngraphs, with the goal of devising algorithms that are optimal, up to\nconstants, both asymptotically and in finite time. We show that,\nsurprisingly, the notion of optimal finite-time regret is not a\nuniquely defined property in this context and that, in general, it\nis decoupled from the asymptotic rate. We discuss alternative\nchoices and propose a notion of finite-time optimality that we argue\nis \\emph{meaningful}. For that notion, we give an algorithm that\nadmits quasi-optimal regret both in finite-time and asymptotically."}}
{"id": "Dqcoao24G8s", "cdate": 1652737469233, "mdate": null, "content": {"title": "A Best-of-Both-Worlds Algorithm for Bandits with Delayed Feedback", "abstract": "We present a modified tuning of the algorithm of  Zimmert and Seldin [2020] for adversarial multiarmed bandits with delayed feedback, which in addition to the minimax optimal adversarial regret guarantee shown by Zimmert and Seldin [2020] simultaneously achieves a near-optimal regret guarantee in the stochastic setting with fixed delays. Specifically, the adversarial regret guarantee is $\\mathcal{O}(\\sqrt{TK} + \\sqrt{dT\\log K})$, where $T$ is the time horizon, $K$ is the number of arms, and $d$ is the fixed delay, whereas the stochastic regret guarantee is $\\mathcal{O}\\left(\\sum_{i \\neq i^*}(\\frac{1}{\\Delta_i} \\log(T) + \\frac{d}{\\Delta_{i}}) + d K^{1/3}\\log K\\right)$, where $\\Delta_i$ are the suboptimality gaps. We also present an extension of the algorithm to the case of arbitrary delays, which is based on an oracle knowledge of the maximal delay $d_{max}$ and achieves $\\mathcal{O}(\\sqrt{TK} + \\sqrt{D\\log K} + d_{max}K^{1/3} \\log K)$ regret in the adversarial regime, where $D$ is the total delay, and $\\mathcal{O}\\left(\\sum_{i \\neq i^*}(\\frac{1}{\\Delta_i} \\log(T) + \\frac{\\sigma_{max}}{\\Delta_{i}}) + d_{max}K^{1/3}\\log K\\right)$ regret in the stochastic regime, where $\\sigma_{max}$ is the maximal number of outstanding observations. Finally, we present a lower bound that matches regret upper bound achieved by the skipping technique of  Zimmert and Seldin [2020] in the adversarial setting."}}
{"id": "z-9aLHbDo0L", "cdate": 1640995200000, "mdate": 1682323639707, "content": {"title": "Pushing the Efficiency-Regret Pareto Frontier for Online Learning of Portfolios and Quantum States", "abstract": "We revisit the classical online portfolio selection problem. It is widely assumed that a trade-off between computational complexity and regret is unavoidable, with Cover\u2019s Universal Portfolios algo..."}}
{"id": "rrgbGfMZFl5", "cdate": 1640995200000, "mdate": 1645969929697, "content": {"title": "Pushing the Efficiency-Regret Pareto Frontier for Online Learning of Portfolios and Quantum States", "abstract": "We revisit the classical online portfolio selection problem. It is widely assumed that a trade-off between computational complexity and regret is unavoidable, with Cover's Universal Portfolios algorithm, SOFT-BAYES and ADA-BARRONS currently constituting its state-of-the-art Pareto frontier. In this paper, we present the first efficient algorithm, BISONS, that obtains polylogarithmic regret with memory and per-step running time requirements that are polynomial in the dimension, displacing ADA-BARRONS from the Pareto frontier. Additionally, we resolve a COLT 2020 open problem by showing that a certain Follow-The-Regularized-Leader algorithm with log-barrier regularization suffers an exponentially larger dependence on the dimension than previously conjectured. Thus, we rule out this algorithm as a candidate for the Pareto frontier. We also extend our algorithm and analysis to a more general problem than online portfolio selection, viz. online learning of quantum states with log loss. This algorithm, called SCHRODINGER'S BISONS, is the first efficient algorithm with polylogarithmic regret for this more general problem."}}
{"id": "_pXV0dD94h", "cdate": 1640995200000, "mdate": 1681491241948, "content": {"title": "Stochastic Online Learning with Feedback Graphs: Finite-Time and Asymptotic Optimality", "abstract": ""}}
{"id": "Y9ur6FfA_zU", "cdate": 1640995200000, "mdate": 1671548292543, "content": {"title": "A Unified Algorithm for Stochastic Path Problems", "abstract": "We study reinforcement learning in stochastic path (SP) problems. The goal in these problems is to maximize the expected sum of rewards until the agent reaches a terminal state. We provide the first regret guarantees in this general problem by analyzing a simple optimistic algorithm. Our regret bound matches the best known results for the well-studied special case of stochastic shortest path (SSP) with all non-positive rewards. For SSP, we present an adaptation procedure for the case when the scale of rewards $B_\\star$ is unknown. We show that there is no price for adaptation, and our regret bound matches that with a known $B_\\star$. We also provide a scale adaptation procedure for the special case of stochastic longest paths (SLP) where all rewards are non-negative. However, unlike in SSP, we show through a lower bound that there is an unavoidable price for adaptation."}}
