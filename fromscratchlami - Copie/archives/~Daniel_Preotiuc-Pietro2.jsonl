{"id": "FCnohuR6AnM", "cdate": 1663849943827, "mdate": null, "content": {"title": "Dataless Knowledge Fusion by Merging Weights of Language Models", "abstract": "Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a data-less knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios."}}
{"id": "xyLOM-fLJQ", "cdate": 1640995200000, "mdate": 1682395427896, "content": {"title": "Combining Humor and Sarcasm for Improving Political Parody Detection", "abstract": "Xiao Ao, Danae Sanchez Villegas, Daniel Preotiuc-Pietro, Nikolaos Aletras. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "wKPPCs53VC-", "cdate": 1640995200000, "mdate": 1671339229423, "content": {"title": "Cross-lingual Few-Shot Learning on Unseen Languages", "abstract": "Genta Winata, Shijie Wu, Mayank Kulkarni, Thamar Solorio, Daniel Preotiuc-Pietro. Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2022."}}
{"id": "t3RBPRoWJv", "cdate": 1640995200000, "mdate": 1671339229415, "content": {"title": "EntSUM: A Data Set for Entity-Centric Extractive Summarization", "abstract": ""}}
{"id": "lV9d8ZxGh7S", "cdate": 1640995200000, "mdate": 1671339229536, "content": {"title": "EntSUM: A Data Set for Entity-Centric Summarization", "abstract": "Controllable summarization aims to provide summaries that take into account user-specified aspects and preferences to better assist them with their information need, as opposed to the standard summarization setup which build a single generic summary of a document. We introduce a human-annotated data set EntSUM for controllable summarization with a focus on named entities as the aspects to control. We conduct an extensive quantitative analysis to motivate the task of entity-centric summarization and show that existing methods for controllable summarization fail to generate entity-centric summaries. We propose extensions to state-of-the-art summarization approaches that achieve substantially better results on our data set. Our analysis and results show the challenging nature of this task and of the proposed data set."}}
{"id": "jYETMtKZsg9", "cdate": 1640995200000, "mdate": 1682395427904, "content": {"title": "Dataless Knowledge Fusion by Merging Weights of Language Models", "abstract": "Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios."}}
{"id": "YEjfQSFLxo0", "cdate": 1640995200000, "mdate": 1682395427896, "content": {"title": "Automatic Identification and Classification of Bragging in Social Media", "abstract": "Bragging is a speech act employed with the goal of constructing a favorable self-image through positive statements about oneself. It is widespread in daily communication and especially popular in social media, where users aim to build a positive image of their persona directly or indirectly. In this paper, we present the first large scale study of bragging in computational linguistics, building on previous research in linguistics and pragmatics. To facilitate this, we introduce a new publicly available data set of tweets annotated for bragging and their types. We empirically evaluate different transformer-based models injected with linguistic information in (a) binary bragging classification, i.e., if tweets contain bragging statements or not; and (b) multi-class bragging type prediction including not bragging. Our results show that our models can predict bragging with macro F1 up to 72.42 and 35.95 in the binary and multi-class classification tasks respectively. Finally, we present an extensive linguistic and error analysis of bragging prediction to guide future research on this topic."}}
{"id": "UHYZoUqoFq", "cdate": 1640995200000, "mdate": 1682395427911, "content": {"title": "Combining Humor and Sarcasm for Improving Political Parody Detection", "abstract": "Parody is a figurative device used for mimicking entities for comedic or critical purposes. Parody is intentionally humorous and often involves sarcasm. This paper explores jointly modelling these figurative tropes with the goal of improving performance of political parody detection in tweets. To this end, we present a multi-encoder model that combines three parallel encoders to enrich parody-specific representations with humor and sarcasm information. Experiments on a publicly available data set of political parody tweets demonstrate that our approach outperforms previous state-of-the-art methods."}}
{"id": "9H08z9Fmjs", "cdate": 1640995200000, "mdate": 1682395427897, "content": {"title": "Automatic Identification and Classification of Bragging in Social Media", "abstract": ""}}
{"id": "1UTbn1FQqS", "cdate": 1640995200000, "mdate": 1671339229421, "content": {"title": "Extractive Entity-Centric Summarization as Sentence Selection using Bi-Encoders", "abstract": "Ella Hofmann-Coyle, Mayank Kulkarni, Lingjue Xie, Mounica Maddela, Daniel Preotiuc-Pietro. Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2022."}}
