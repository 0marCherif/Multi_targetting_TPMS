{"id": "lHy09zPewmD", "cdate": 1652737786532, "mdate": null, "content": {"title": "Non-monotonic Resource Utilization in the Bandits with Knapsacks Problem", "abstract": "Bandits with knapsacks (BwK) is an influential model of sequential decision-making under uncertainty that incorporates resource consumption constraints. In each round, the decision-maker observes an outcome consisting of a reward and a vector of nonnegative resource consumptions, and the budget of each resource is decremented by its consumption. In this paper we introduce a natural generalization of the stochastic BwK problem that allows non-monotonic resource utilization. In each round, the decision-maker observes an outcome consisting of a reward and a vector of resource drifts that can be positive, negative or zero, and the budget of each resource is incremented by its drift. Our main result is a Markov decision process (MDP) policy that has constant regret against a linear programming (LP) relaxation when the decision-maker knows the true outcome distributions. We build upon this to develop a learning algorithm that has logarithmic regret against the same LP relaxation when the decision-maker does not know the true outcome distributions. We also present \na reduction from BwK to our model that shows our regret bound matches existing results."}}
{"id": "1EPt7V-o-Ul", "cdate": 1640995200000, "mdate": 1682338668797, "content": {"title": "Non-monotonic Resource Utilization in the Bandits with Knapsacks Problem", "abstract": ""}}
{"id": "rfbcTBdHg5", "cdate": 1609459200000, "mdate": 1645737409772, "content": {"title": "Homeomorphic-Invariance of EM: Non-Asymptotic Convergence in KL Divergence for Exponential Families via Mirror Descent", "abstract": "Expectation maximization (EM) is the default algorithm for fitting probabilistic models with missing or latent variables, yet we lack a full understanding of its non-asymptotic convergence properties. Previous works show results along the lines of \"EM converges at least as fast as gradient descent\" by assuming the conditions for the convergence of gradient descent apply to EM. This approach is not only loose, in that it does not capture that EM can make more progress than a gradient step, but the assumptions fail to hold for textbook examples of EM like Gaussian mixtures. In this work we first show that for the common setting of exponential family distributions, viewing EM as a mirror descent algorithm leads to convergence rates in Kullback-Leibler (KL) divergence. Then, we show how the KL divergence is related to first-order stationarity via Bregman divergences. In contrast to previous works, the analysis is invariant to the choice of parametrization and holds with minimal assumptions. We also show applications of these ideas to local linear (and superlinear) convergence rates, generalized EM, and non-exponential family distributions."}}
{"id": "tavcjUKC_5a", "cdate": 1577836800000, "mdate": 1682338668841, "content": {"title": "Retrieving Top Weighted Triangles in Graphs", "abstract": ""}}
