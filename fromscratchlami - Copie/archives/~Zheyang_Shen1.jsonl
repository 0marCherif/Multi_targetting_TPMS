{"id": "ZAh31ihNaoF", "cdate": 1621630217368, "mdate": null, "content": {"title": "De-randomizing MCMC dynamics with the diffusion Stein operator", "abstract": "Approximate Bayesian inference estimates descriptors of an intractable target distribution - in essence, an optimization problem within a family of distributions. For example, Langevin dynamics (LD) extracts asymptotically exact samples from a diffusion process because the time evolution of its marginal distributions constitutes a curve that minimizes the KL-divergence via steepest descent in the Wasserstein space. Parallel to LD, Stein variational gradient descent (SVGD) similarly minimizes the KL, albeit endowed with a novel Stein-Wasserstein distance, by deterministically transporting a set of particle samples, thus de-randomizes the stochastic diffusion process. We propose de-randomized kernel-based particle samplers to all diffusion-based samplers known as MCMC dynamics. Following previous work in interpreting MCMC dynamics, we equip the Stein-Wasserstein space with a fiber-Riemannian Poisson structure, with the capacity of characterizing a fiber-gradient Hamiltonian flow that simulates MCMC dynamics. Such dynamics discretizes into generalized SVGD (GSVGD), a Stein-type deterministic particle sampler, with particle updates coinciding with applying the diffusion Stein operator to a kernel function. We demonstrate empirically that GSVGD can de-randomize complex MCMC dynamics, which combine the advantages of auxiliary momentum variables and Riemannian structure, while maintaining the high sample quality from an interacting particle system."}}
{"id": "Dxw21L0CcYe", "cdate": 1609459200000, "mdate": 1681651999846, "content": {"title": "De-randomizing MCMC dynamics with the diffusion Stein operator", "abstract": ""}}
{"id": "DA4L6p1VEgT", "cdate": 1609459200000, "mdate": null, "content": {"title": "Sparse Gaussian Processes Revisited: Bayesian Approaches to Inducing-Variable Approximations", "abstract": "Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Besides enabling scalability, one of their main advantages over sparse approximations using direct marginal likelihood maximization is that they provide a robust alternative for point estimation of the inducing inputs, i.e. the location of the inducing variables. In this work we challenge the common wisdom that optimizing the inducing inputs in the variational framework yields optimal performance. We show that, by revisiting old model approximations such as the fully-independent training conditionals endowed with powerful sampling-based inference methods, treating both inducing locations and GP hyper-parameters in a Bayesian way can improve performance significantly. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable GP and deep GP models, and demonstrate its state-of-the-art performance through an extensive experimental campaign across several regression and classification problems."}}
{"id": "U7zrVd-EAmd", "cdate": 1577836800000, "mdate": 1681651999843, "content": {"title": "Learning spectrograms with convolutional spectral kernels", "abstract": ""}}
{"id": "ky2Sa944zG", "cdate": 1546300800000, "mdate": null, "content": {"title": "Harmonizable mixture kernels with variational Fourier features", "abstract": "The expressive power of Gaussian processes depends heavily on the choice of kernel. In this work we propose the novel harmonizable mixture kernel (HMK), a family of expressive, interpretable, non-s..."}}
