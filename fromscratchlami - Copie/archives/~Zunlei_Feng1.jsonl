{"id": "80DaWgFJwB", "cdate": 1669814172598, "mdate": null, "content": {"title": "Cell Segmenter: A General Framework for Multi-modality Cell Segmentation", "abstract": "Cell Segmentation is an initial and fundamental step in biomedical image analysis, which strongly affects the experimental results of this analysis. Recently, deep learning based segmentation methods have shown great power in segmentation accuracy and efficiency. However, these data-driven methods still face many challenges, such as lack of annotations, multi-modality, and complex morphology, where morphological complexity significantly limits model performance. In this paper, we propose a new all-purpose framework with high morphological adaptability for multi-modality cell segmentation, termed Cell Segmenter (CS). For high convex cells with an arbitrary size, the Anchor-based Watershed Framework (AWF) precisely locates well-defined cell centers and generates segmentation based on these markers. For those elongated or non-convex cells, the center-independent segmentation method Omnipose is adopted to obtain satisfying masks. In the inference time, confidence-based quality estimation is conducted on the branch predictions if needed, and then the better result is chosen as the final segmentation. The F1-score of the proposed method reaches 0.8537 on TuningSet and 0.6216 on the final test set of the NeurIPS 2022 Cell Segmentation Challenge."}}
{"id": "gPgI6mStqTc", "cdate": 1663850080404, "mdate": null, "content": {"title": "Relative Contribution Mechanism: A Unified Paradigm for Disassembling Convolutional Neural Networks", "abstract": "With the tremendous development of CNNs, obtaining an available CNN classifier is more challenging due to the massive number of parameters and deep structure. Recently, an emerging model disassembling and assembling task (MDA-Task) has been proposed to obtain new models easily from the perspective of model reusing. However, the existing methods are usually slow or inaccurate. In this paper, we put forward a contribution paradigm for MDA-Task, which unifies existing model disassembling and assembling methods into a universal formulation. We first propose a relative contribution mechanism that the prediction results of the CNN classifier are decided by the larger contribution value. Then, the analysis and two discoveries of contribution allocation and aggregation procedures are given around the above mechanism. Based on the two discoveries, we introduce a contribution attribution based CNN disassembling technique composed of single-layer contribution attribution and backward accumulation attribution, which can effectively find the category-aware components in each layer and associated components in adjacent layers, respectively. In addition, a contribution rescaling based CNN assembling technique is devised for assembling the above disassembled category-aware components from different CNN classifiers, which can achieve comparable accuracy performance with original CNN classifiers. Experiments on five benchmark datasets with three mainstream CNN classifiers verify the effectiveness of the proposed contribution paradigm and demonstrate that the contribution attribution based CNN disassembling and assembling technique can achieve significant accuracy increases and faster speed than the existing methods."}}
{"id": "Dbt_eecQY8t", "cdate": 1619159694828, "mdate": null, "content": {"title": "Factorizable Graph Convolutional Networks", "abstract": "Graphs have been widely adopted to denote structural connections between entities. The relations are in many cases heterogeneous, but entangled together and denoted merely as a single edge between a pair of nodes. For example, in a social network graph, users in different latent relationships like friends and colleagues, are usually connected via a bare edge that conceals such intrinsic connections. In this paper, we introduce a novel graph convolutional network (GCN), termed as factorizable graph convolutional network (FactorGCN), that explicitly disentangles such intertwined relations encoded in a graph. FactorGCN takes a simple graph as input, and disentangles it into several factorized graphs, each of which represents a latent and disentangled relation among nodes. The features of the nodes are then aggregated separately in each factorized latent space to produce disentangled features, which further leads to better performances for downstream tasks. We evaluate the proposed FactorGCN both qualitatively and quantitatively on the synthetic and real-world datasets, and demonstrate that it yields truly encouraging results in terms of both disentangling and feature aggregation."}}
{"id": "SixlbilDxupr", "cdate": 1546300800000, "mdate": null, "content": {"title": "CU-Net: Component Unmixing Network for Textile Fiber Identification.", "abstract": "Image-based nondestructive textile fiber identification is a challenging computer vision problem, that is practically useful in fashion, decoration, and design. Although deep learning now outperforms humans in many scenarios such as face and object recognition, image-based fiber identification is still an open problem for deep learning given imbalanced sample and small sample size samples. In this paper, we propose the Component Unmixing Network (CU-Net) for nondestructive textile fiber identification. CU-Net learns effective representations given imbalanced sample and small sample size samples to achieve high-performance textile fiber identification. CU-Net comprises a Deep Feature Extraction Module (DFE-Module) and a Component Unmixing Module (CU-Module). Initially, mixed deep features are extracted by DFE-Module from the input textile patches. Then, CU-Module is employed to extract unmixed representations of different fibers from the mixed deep features. In CU-Module, we introduce a self-interchange and a restraining loss to reduce the mixture between representations of different fibers. Furthermore, we extend CU-Net to the proportion analysis task with very good effect. Extensive experiments demonstrate that: (1) self-interchange and the restraining loss effectively unmix different fiber representations and improve fiber identification accuracy; and (2) CU-Net achieves more accurate fiber identification than the current state-of-the-art multi-label classification methods."}}
{"id": "rkWDiU-O-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Dual Swap Disentangling", "abstract": "Learning interpretable disentangled representations is a crucial yet challenging task. In this paper, we propose a weakly semi-supervised method, termed as Dual Swap Disentangling (DSD), for disentangling using both labeled and unlabeled data. Unlike conventional weakly supervised methods that rely on full annotations on the group of samples, we require only limited annotations on paired samples that indicate their shared attribute like the color. Our model takes the form of a dual autoencoder structure. To achieve disentangling using the labeled pairs, we follow a <code>encoding-swap-decoding'' process, where we first swap the parts of their encodings corresponding to the shared attribute, and then decode the obtained hybrid codes to reconstruct the original input pairs. For unlabeled pairs, we follow the</code>encoding-swap-decoding'' process twice on designated encoding parts and enforce the final outputs to approximate the input pairs. By isolating parts of the encoding and swapping them back and forth, we impose the dimension-wise modularity and portability of the encodings of the unlabeled samples, which implicitly encourages disentangling under the guidance of labeled pairs. This dual swap mechanism, tailored for semi-supervised setting, turns out to be very effective. Experiments on image datasets from a wide domain show that our model yields state-of-the-art disentangling performances."}}
{"id": "BkEHBqW_ZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields", "abstract": "The Fast Style Transfer methods have been recently proposed to transfer a photograph to an artistic style in real-time. This task involves controlling the stroke size in the stylized results, which remains an open challenge. In this paper, we present a stroke controllable style transfer network that can achieve continuous and spatial stroke size control. By analyzing the factors that influence the stroke size, we propose to explicitly account for the receptive field and the style image scales. We propose a StrokePyramid module to endow the network with adaptive receptive fields, and two training strategies to achieve faster convergence and augment new stroke sizes upon a trained model respectively. By combining the proposed runtime control strategies, our network can achieve continuous changes in stroke sizes and produce distinct stroke sizes in different spatial regions within the same output image."}}
