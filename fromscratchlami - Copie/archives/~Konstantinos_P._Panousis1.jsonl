{"id": "JQ0B9-8Q69k", "cdate": 1672531200000, "mdate": 1693744275476, "content": {"title": "Sparse Linear Concept Discovery Models", "abstract": "The recent mass adoption of DNNs, even in safety-critical scenarios, has shifted the focus of the research community towards the creation of inherently intrepretable models. Concept Bottleneck Models (CBMs) constitute a popular approach where hidden layers are tied to human understandable concepts allowing for investigation and correction of the network's decisions. However, CBMs usually suffer from: (i) performance degradation and (ii) lower interpretability than intended due to the sheer amount of concepts contributing to each decision. In this work, we propose a simple yet highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer. In stark contrast to related approaches, the sparsity in our framework is achieved via principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution. As we experimentally show, our framework not only outperforms recent CBM approaches accuracy-wise, but it also yields high per example concept sparsity, facilitating the individual investigation of the emerging concepts."}}
{"id": "zESX_pZdFSt", "cdate": 1640995200000, "mdate": 1681649673443, "content": {"title": "Competing Mutual Information Constraints with Stochastic Competition-Based Activations for Learning Diversified Representations", "abstract": ""}}
{"id": "nCaqdFg5Kc", "cdate": 1640995200000, "mdate": 1681649673324, "content": {"title": "Competing Mutual Information Constraints with Stochastic Competition-based Activations for Learning Diversified Representations", "abstract": ""}}
{"id": "tDgQoYPwTVo", "cdate": 1609459200000, "mdate": 1683814336552, "content": {"title": "Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition", "abstract": "Hidden Markov Models (HMMs) comprise a powerful generative approach for modeling sequential data and time-series in general. However, the commonly employed assumption of the dependence of the current time frame to a single or multiple immediately preceding frames is unrealistic; more complicated dynamics potentially exist in real world scenarios. This paper revisits conventional sequential modeling approaches, aiming to address the problem of capturing time-varying temporal dependency patterns. To this end, we propose a different formulation of HMMs, whereby the dependence on past frames is dynamically inferred from the data. Specifically, we introduce a hierarchical extension by postulating an additional latent variable layer; therein, the (time-varying) temporal dependence patterns are treated as latent variables over which inference is performed. We leverage solid arguments from the Variational Bayes framework and derive a tractable inference algorithm based on the forward-backward algorithm. As we experimentally show, our approach can model highly complex sequential data and can effectively handle data with missing values."}}
{"id": "oyAV1wuQ1a", "cdate": 1609459200000, "mdate": 1681649673319, "content": {"title": "Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation", "abstract": ""}}
{"id": "NzJadeMlFYg", "cdate": 1609459200000, "mdate": 1681649673429, "content": {"title": "Local Competition and Stochasticity for Adversarial Robustness in Deep Learning", "abstract": ""}}
{"id": "NC89dOPOw6l", "cdate": 1609459200000, "mdate": 1681649673450, "content": {"title": "Local Competition and Stochasticity for Adversarial Robustness in Deep Learning", "abstract": ""}}
{"id": "KrKMIuvGgI", "cdate": 1609459200000, "mdate": 1681649673446, "content": {"title": "Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness", "abstract": ""}}
{"id": "BJNbEiWdWB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Nonparametric Bayesian Deep Networks with Local Competition", "abstract": "The aim of this work is to enable inference of deep networks that retain high accuracy for the least possible model complexity, with the latter deduced from the data during inference. To this end, ..."}}
