{"id": "YSzTMntO1KY", "cdate": 1621630042280, "mdate": null, "content": {"title": "SIMONe: View-Invariant, Temporally-Abstracted Object Representations via Unsupervised Video Decomposition", "abstract": "To help agents reason about scenes in terms of their building blocks, we wish to extract the compositional structure of any given scene (in particular, the configuration and characteristics of objects comprising the scene). This problem is especially difficult when scene structure needs to be inferred while also estimating the agent\u2019s location/viewpoint, as the two variables jointly give rise to the agent\u2019s observations. We present an unsupervised variational approach to this problem. Leveraging the shared structure that exists across different scenes, our model learns to infer two sets of latent representations from RGB video input alone: a set of \"object\" latents, corresponding to the time-invariant, object-level contents of the scene, as well as a set of \"frame\" latents, corresponding to global time-varying elements such as viewpoint. This factorization of latents allows our model, SIMONe, to represent object attributes in an allocentric manner which does not depend on viewpoint. Moreover, it allows us to disentangle object dynamics and summarize their trajectories as time-abstracted, view-invariant, per-object properties. We demonstrate these capabilities, as well as the model's performance in terms of view synthesis and instance segmentation, across three procedurally generated video datasets.     "}}
{"id": "cUtX0mz1LOj", "cdate": 1451606400000, "mdate": null, "content": {"title": "A 3D shape inference model matches human visual object similarity judgments better than deep convolutional neural networks", "abstract": "In the past few years, deep convolutional neural networks (CNNs) trained on large image data sets have shown impressive visual object recognition performances. Consequently, these models have attracted the attention of the cognitive science community. Recent studies comparing CNNs with neural data from cortical area IT suggest that CNNs may\u2014in addition to providing good engineering solutions\u2014provide good models of biological visual systems. Here, we report evidence that CNNs are, in fact, not good models of human visual perception. We show that a 3D shape inference model explains human performance on an object shape similarity task better than CNNs. We argue that deep neural networks trained on large amounts of image data to maximize object recognition performance do not provide adequate models of human vision."}}
{"id": "56r4SnKWU0x", "cdate": 1451606400000, "mdate": null, "content": {"title": "Multisensory Part-based Representations of Objects in Human Lateral Occipital Cortex", "abstract": "The format of high-level object representations in temporal-occipital cortex is a fundamental and as yet unresolved issue. Here we use fMRI to show that human lateral occipital cortex (LOC) encodes novel 3-D objects in a multisensory and part-based format. We show that visual and haptic exploration of objects leads to similar patterns of neural activity in human LOC and that the shared variance between visually and haptically induced patterns of BOLD contrast in LOC reflects the part structure of the objects. We also show that linear classifiers trained on neural data from LOC on a subset of the objects successfully predict a novel object based on its component part structure. These data demonstrate a multisensory code for object representations in LOC that specifies the part structure of objects."}}
{"id": "q1onbLcuVjO", "cdate": 1420070400000, "mdate": null, "content": {"title": "From Sensory Signals to Modality-Independent Conceptual Representations: A Probabilistic Language of Thought Approach", "abstract": "Author Summary When viewing an object, people perceive the object\u2019s shape. Similarly, when grasping the same object, they also perceive its shape. In general, the perceived shape is identical in these two scenarios, illustrating modality invariance, an important type of perceptual constancy. Modality invariance suggests that people infer a modality-independent, conceptual representation that is the same regardless of the modality used to sense the environment. If so, how do people infer modality-independent representations from modality-specific sensory signals? We present a hypothesis about the components that any system will include if it infers modality-independent representations from sensory signals. This hypothesis is instantiated in a computational model that infers object shape representations from visual or haptic (i.e., active touch) signals. The model shows perfect modality invariance\u2014it infers the same shape representations regardless of the sensory modality used to sense objects. The model also provides a highly accurate account of data collected in an experiment in which people judged the similarity of pairs of objects that were viewed, grasped, or both. Conceptually, our research contributes to our understanding of modality invariance. Methodologically, it contributes to cognitive modeling by showing how symbolic and statistical approaches can be combined in order to understand aspects of human perception."}}
{"id": "K6OhsQoa9Hs", "cdate": 1388534400000, "mdate": null, "content": {"title": "Transfer of object shape knowledge across visual and haptic modalities", "abstract": "We investigate the hypothesis that multisensory representations mediate the crossmodal transfer of shape knowledge across visual and haptic modalities. In our experiment, participants rated the similarities of pairs of synthetic 3-D objects in visual, haptic, cross-modal, and multisensory settings. Our results offer two contributions. First, we provide evidence for a single multisensory shape representation common to both visual and haptic modalities. Second, our analyses suggest that these representations are part-based, representing objects as compositions of subparts."}}
