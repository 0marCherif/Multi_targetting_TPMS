{"id": "ltINLzmhHg", "cdate": 1672531200000, "mdate": 1706741368889, "content": {"title": "MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation", "abstract": ""}}
{"id": "bPGAxRF4vFV", "cdate": 1672531200000, "mdate": 1706741369086, "content": {"title": "DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion", "abstract": "Diffusion models have emerged as the new state-of-the-art family of deep generative models, and their promising potentials for text generation have recently attracted increasing attention. Existing studies mostly adopt a single encoder architecture with partially noising processes for conditional text generation, but its degree of flexibility for conditional modeling is limited. In fact, the encoder-decoder architecture is naturally more flexible for its detachable encoder and decoder modules, which is extensible to multilingual and multimodal generation tasks for conditions and target texts. However, the encoding process of conditional texts lacks the understanding of target texts. To this end, a spiral interaction architecture for encoder-decoder text diffusion (DiffuSIA) is proposed. Concretely, the conditional information from encoder is designed to be captured by the diffusion decoder, while the target information from decoder is designed to be captured by the conditional encoder. These two types of information flow run through multilayer interaction spirally for deep fusion and understanding. DiffuSIA is evaluated on four text generation tasks, including paraphrase, text simplification, question generation, and open-domain dialogue generation. Experimental results show that DiffuSIA achieves competitive performance among previous methods on all four tasks, demonstrating the effectiveness and generalization ability of the proposed method."}}
{"id": "4JfvBNggCx", "cdate": 1672531200000, "mdate": 1706741369087, "content": {"title": "Is ChatGPT a Good Multi-Party Conversation Solver?", "abstract": ""}}
{"id": "co7wdxdb5-", "cdate": 1640995200000, "mdate": 1681650509737, "content": {"title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences", "abstract": ""}}
{"id": "YbRbEG7-II", "cdate": 1640995200000, "mdate": 1681650509762, "content": {"title": "Neural Grapheme-To-Phoneme Conversion with Pre-Trained Grapheme Models", "abstract": ""}}
{"id": "FboBb1_eTHt", "cdate": 1640995200000, "mdate": 1664546928153, "content": {"title": "TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge", "abstract": ""}}
{"id": "D2Bx6H_t2C", "cdate": 1640995200000, "mdate": 1664546836999, "content": {"title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations", "abstract": "Jia-Chen Gu, Chao-Hong Tan, Chongyang Tao, Zhen-Hua Ling, Huang Hu, Xiubo Geng, Daxin Jiang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "9jInD9JjicF", "cdate": 1632875487908, "mdate": null, "content": {"title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences", "abstract": "Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 95.7 percent of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5 percent relative. Comprehensive ablation analysis demonstrates effectiveness of the designed multi-granularity pooling and pooling fusion for token mixing in long sequences and efficacy of the designed pre-training tasks for PoNet to learn transferable contextualized language representations."}}
{"id": "bQKcltk3mxg", "cdate": 1577836800000, "mdate": 1634730998031, "content": {"title": "Learning to Retrieve Entity-Aware Knowledge and Generate Responses with Copy Mechanism for Task-Oriented Dialogue Systems", "abstract": "Task-oriented conversational modeling with unstructured knowledge access, as track 1 of the 9th Dialogue System Technology Challenges (DSTC 9), requests to build a system to generate response given dialogue history and knowledge access. This challenge can be separated into three subtasks, (1) knowledge-seeking turn detection, (2) knowledge selection, and (3) knowledge-grounded response generation. We use pre-trained language models, ELECTRA and RoBERTa, as our base encoder for different subtasks. For subtask 1 and 2, the coarse-grained information like domain and entity are used to enhance knowledge usage. For subtask 3, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism. Meanwhile, some useful post-processing strategies are performed on the model's final output to make further knowledge usage in the generation task. As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics."}}
