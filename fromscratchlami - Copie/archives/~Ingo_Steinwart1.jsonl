{"id": "tZc7Age_FA6", "cdate": 1674993702023, "mdate": 1674993702023, "content": {"title": "Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers", "abstract": "Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models or analyses with a downstream application such that error quantification plays a key role. However, by entirely ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of a widely-applied theorem for conditioning GPs on a finite number of direct observations to observations made via an arbitrary bounded linear operator. Crucially, this probabilistic viewpoint allows to (1) quantify the inherent discretization error; (2) propagate uncertainty about the model parameters to the solution; and (3) condition on noisy measurements. Demonstrating the strength of this formulation, we prove that it strictly generalizes methods of weighted residuals, a central class of PDE solvers including collocation, finite volume, pseudospectral, and (generalized) Galerkin methods such as finite element and spectral methods. This class can thus be directly equipped with a structured error estimate and the capability to incorporate uncertain model parameters and observations. In summary, our results enable the seamless integration of mechanistic models as modular building blocks into probabilistic models."}}
{"id": "t5EmXZ3ZLR", "cdate": 1632875713724, "mdate": null, "content": {"title": "SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning", "abstract": "Pruning neural networks reduces inference time and memory costs. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an innovative second-order approximation, which enables saliency evaluations by fast Hessian-vector products. SOSP-H thereby scales like a first-order method despite taking into account the full Hessian. We validate SOSP-H by comparing it to our second method SOSP-I that uses a well-established Hessian approximation, and to numerous state-of-the-art methods. While SOSP-H performs on par or better in terms of accuracy, it has clear advantages in terms of scalability and efficiency. This allowed us to scale SOSP-H to large-scale vision tasks, even though it captures correlations across all layers of the network. To underscore the global nature of our pruning methods, we evaluate their performance not only by removing structures from a pretrained network, but also by detecting architectural bottlenecks. We show that our algorithms allow to systematically reveal architectural bottlenecks, which we then remove to further increase the accuracy of the networks."}}
{"id": "sUgpxb9QD", "cdate": 1621630170061, "mdate": null, "content": {"title": "SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning", "abstract": "Pruning neural networks reduces inference time and memory cost, as well as accelerates training when done at initialization. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise global saliency-based methods for second-order structured pruning (SOSP) which include correlations among structures, whereas highest efficiency is achieved by saliency approximations using fast Hessian-vector products. We achieve state-of-the-art results for various object classification benchmarks, especially for large pruning rates highly relevant for resource-constrained applications. We showcase that our approach scales to large-scale vision tasks, even though it captures correlations across all layers of the network. Further, we highlight two outstanding features of our methods. First, to reduce training costs our pruning objectives can also be applied at initialization with no or only minor degradation in accuracy compared to pruning after pre-training. Second, our structured pruning methods allow to reveal architectural bottlenecks, which we remove to further increase the accuracy of the networks."}}
{"id": "PjnPIp3CGhY", "cdate": 1620997519644, "mdate": null, "content": {"title": "Sobolev norm learning rates for regularized least-squares algorithm", "abstract": "Learning rates for least-squares regression are typically expressed in terms of L2-norms. In\nthis paper we extend these rates to norms stronger than the L2 -norm without requiring\nthe regression function to be contained in the hypothesis space. In the special case of\nSobolev reproducing kernel Hilbert spaces used as hypotheses spaces, these stronger norms\ncoincide with fractional Sobolev norms between the used Sobolev space and L2. As a\nconsequence, not only the target function but also some of its derivatives can be estimated\nwithout changing the algorithm. From a technical point of view, we combine the well-known\nintegral operator techniques with an embedding property, which so far has only been used\nin combination with empirical process arguments. This combination results in new finite\nsample bounds with respect to the stronger norms. From these finite sample bounds our\nrates easily follow. Finally, we prove the asymptotic optimality of our results in many cases.\n\n"}}
{"id": "djyJWFVtEPe", "cdate": 1598969214304, "mdate": null, "content": {"title": "Elicitation and identification of properties,", "abstract": "Properties of distributions are real-valued functionals such as the mean, quantile or conditional\nvalue at risk. A property is elicitable if there exists a scoring function such that minimization of the\nassociated risks recovers the property. We extend existing results to characterize the elicitability of\nproperties in a general setting. We further relate elicitability to identifiability (a notion introduced\nby Osband) and provide a general formula describing all scoring functions for an elicitable property.\nFinally, we draw some connections to the theory of coherent risk measures"}}
{"id": "OQ9n3ktfIdb", "cdate": 1598969039852, "mdate": null, "content": {"title": "Towards an axiomatic approach to hierarchical clustering of measures,", "abstract": "We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some\nelementary measures. This is done without the need of any notion of metric, similarity\nor dissimilarity. Our main results then show that for each suitable choice of user-defined\nclustering on elementary measures we obtain a unique notion of clustering on a large set\nof distributions satisfying a set of additivity and continuity axioms. We illustrate the\ndeveloped theory by numerous examples including some with and some without a density."}}
{"id": "FJ6P1m5WvrT", "cdate": 1598968828751, "mdate": null, "content": {"title": "Fully adaptive density-based clusterin", "abstract": "The clusters of a distribution are often defined by the connected\ncomponents of a density level set. However, this definition depends\non the user-specified level. We address this issue by proposing a simple, generic algorithm, which uses an almost arbitrary level set estimator to estimate the smallest level at which there are more than\none connected components. In the case where this algorithm is fed\nwith histogram-based level set estimates, we provide a finite sample\nanalysis, which is then used to show that the algorithm consistently\nestimates both the smallest level and the corresponding connected\ncomponents. We further establish rates of convergence for the two\nestimation problems, and last but not least, we present a simple,\nyet adaptive strategy for determining the width-parameter of the involved density estimator in a data-depending way"}}
{"id": "c94flLxdtn", "cdate": 1598968686815, "mdate": null, "content": {"title": "Adaptive clustering using kernel density estimators", "abstract": "We derive and analyze a generic, recursive algorithm for estimating all splits in a finite cluster tree as well as the corresponding clusters. We further investigate statistical properties of this generic clustering algorithm when it receives level set estimates from a kernel density estimator. In particular, we derive finite sample guarantees, consistency, rates of convergence, and an adaptive data-driven strategy for choosing the kernel bandwidth. For these results we do not need continuity assumptions on the density such as H\u00f6lder continuity, but only require intuitive geometric assumptions of non-parametric nature."}}
{"id": "-CJw4-QVSgQ", "cdate": 1598968573736, "mdate": null, "content": {"title": "A sober look at neural network initializations", "abstract": "Initializing the weights and the biases is a key part of the training process of a neural network. Unlike the subsequent optimization phase, however, the initialization phase has gained only limited attention in the literature. In this paper we discuss some consequences of commonly used initialization strategies for vanilla DNNs with ReLU activations. Based on these insights we then develop an alternative initialization strategy. Finally, we present some large scale experiments assessing the quality of the new initialization strategy."}}
{"id": "WhPE8uTmgTWT", "cdate": 1598968439910, "mdate": null, "content": {"title": "Global minima of DNNs: The plenty pantry", "abstract": "A common strategy to train deep neural networks (DNNs) is to use very large architectures and to train them until they (almost) achieve zero training error. Empirically observed good generalization performance on test data, even in the presence of lots of label noise, corroborate such a procedure. On the other hand, in statistical learning theory it is known that over-fitting models may lead to poor generalization properties, occurring in e.g. empirical risk minimization (ERM) over too large hypotheses classes. Inspired by this contradictory behavior, so-called interpolation methods have recently received much attention, leading to consistent and optimally learning methods for some local averaging schemes with zero training error. However, there is no theoretical analysis of interpolating ERM-like methods so far. We take a step in this direction by showing that for certain, large hypotheses classes, some interpolating ERMs enjoy very good statistical guarantees while others fail in the worst sense. Moreover, we show that the same phenomenon occurs for DNNs with zero training error and sufficiently large architectures."}}
