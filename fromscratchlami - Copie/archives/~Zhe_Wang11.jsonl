{"id": "mkNRGk-w96", "cdate": 1698556966661, "mdate": 1698556966661, "content": {"title": "Learning dynamical human-joint affinity for 3d pose estimation in videos", "abstract": "Graph Convolution Network (GCN) has been successfully used for 3D human pose estimation in videos. However, it is often built on the fixed human-joint affinity, according to human skeleton. This may reduce adaptation capacity of GCN to tackle complex spatio-temporal pose variations in videos. To alleviate this problem, we propose a novel Dynamical Graph Network (DG-Net), which can dynamically identify human-joint affinity, and estimate 3D pose by adaptively learning spatial/temporal joint relations from videos. Different from traditional graph convolution, we introduce Dynamical Spatial/Temporal Graph convolution (DSG/DTG) to discover spatial/temporal human-joint affinity for each video exemplar, depending on spatial distance/temporal movement similarity between human joints in this video. Hence, they can effectively understand which joints are spatially closer and/or have consistent motion."}}
{"id": "gj34vad3-mo", "cdate": 1668024466435, "mdate": 1668024466435, "content": {"title": "PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos", "abstract": "The end-to-end Human Mesh Recovery (HMR) approach has been successfully used for 3D body reconstruction. However, most HMR-based frameworks reconstruct human body by directly learning mesh parameters from images or videos, while lacking explicit guidance of 3D human pose in visual data. As a result, the generated mesh often exhibits incorrect pose for complex activities. To tackle this problem, we propose to exploit 3D pose to calibrate human mesh. Specifically, we develop two novel Pose Calibration frameworks, i.e., Serial PC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in a serial or parallel manner, these two frameworks can effectively correct human mesh with guidance of a concise pose calibration module. Furthermore, since the calibration module is designed via non-rigid pose transformation, our PC-HMR frameworks can flexibly tackle bone length variations to alleviate misplacement in the calibrated mesh. Finally, our frameworks are based on generic and complementary integration of data-driven learning and geometrical modeling. Via plug-and-play modules, they can be efficiently adapted for both image/video-based human mesh recovery. Additionally, they have no requirement of extra 3D pose annotations in the testing phase, which releases inference difficulties in practice. We perform extensive experiments on the popular bench-marks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks achieve the SOTA results"}}
{"id": "z9ZA5SuQfF", "cdate": 1580420254682, "mdate": null, "content": {"title": "Real-time Action Recognition with Enhanced Motion Vector CNNs", "abstract": "The deep two-stream architecture [23] exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method."}}
{"id": "B1eKN4n8wS", "cdate": 1569272881320, "mdate": null, "content": {"title": "Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images", "abstract": ""}}
{"id": "ryetZEnIvr", "cdate": 1569272833312, "mdate": null, "content": {"title": "Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition", "abstract": ""}}
{"id": "rkxA3QnLDH", "cdate": 1569272758509, "mdate": null, "content": {"title": "Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition", "abstract": ""}}
{"id": "rkecOQnIwS", "cdate": 1569272689933, "mdate": null, "content": {"title": "Structured Triplet Learning with POS-tag Guided Attention for Visual Question Answering", "abstract": ""}}
{"id": "SyggmmnUwr", "cdate": 1569272600269, "mdate": null, "content": {"title": "Geometric Pose Affordance: 3D Human Pose with Scene Constraints", "abstract": ""}}
{"id": "S1VegnWd-B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization", "abstract": "Two types of zeroth-order stochastic algorithms have recently been designed for nonconvex optimization respectively based on the first-order techniques SVRG and SARAH/SPIDER. This paper addresses s..."}}
{"id": "rkZdjK-_WB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Pose Guided Human Video Generation", "abstract": "Due to the emergence of Generative Adversarial Networks, video synthesis has witnessed exceptional breakthroughs. However, existing methods lack a proper representation to explicitly control the dynamics in videos. Human pose, on the other hand, can represent motion patterns intrinsically and interpretably, and impose the geometric constraints regardless of appearance. In this paper, we propose a pose guided method to synthesize human videos in a disentangled way: plausible motion prediction and coherent appearance generation. In the first stage, a Pose Sequence Generative Adversarial Network (PSGAN) learns in an adversarial manner to yield pose sequences conditioned on the class label. In the second stage, a Semantic Consistent Generative Adversarial Network (SCGAN) generates video frames from the poses while preserving coherent appearances in the input image. By enforcing semantic consistency between the generated and ground-truth poses at a high feature level, our SCGAN is robust to noisy or abnormal poses. Extensive experiments on both human action and human face datasets manifest the superiority of the proposed method over other state-of-the-arts."}}
