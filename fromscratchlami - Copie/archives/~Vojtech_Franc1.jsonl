{"id": "jiqFF09l4zB", "cdate": 1672531200000, "mdate": 1682326225449, "content": {"title": "Detection of Microscopic Fungi and Yeast in Clinical Samples Using Fluorescence Microscopy and Deep Learning", "abstract": ""}}
{"id": "DkvFuHhg5kt", "cdate": 1640995200000, "mdate": 1682326225537, "content": {"title": "Consistent and Tractable Algorithm for Markov Network Learning", "abstract": "Markov network (MN) structured output classifiers provide a transparent and powerful way to model dependencies between output labels. The MN classifiers can be learned using the M3N algorithm, which, however, is not statistically consistent and requires expensive fully annotated examples. We propose an algorithm to learn MN classifiers that is based on Fisher-consistent adversarial loss minimization. Learning is transformed into a tractable convex optimization that is amenable to standard gradient methods. We also extend the algorithm to learn from examples with missing labels. We show that the extended algorithm remains convex, tractable, and statistically consistent."}}
{"id": "xL_sPHWprca", "cdate": 1609459200000, "mdate": null, "content": {"title": "Optimal strategies for reject option classifiers", "abstract": "In classification with a reject option, the classifier is allowed in uncertain cases to abstain from prediction. The classical cost-based model of a reject option classifier requires the cost of rejection to be defined explicitly. An alternative bounded-improvement model, avoiding the notion of the reject cost, seeks for a classifier with a guaranteed selective risk and maximal cover. We coin a symmetric definition, the bounded-coverage model, which seeks for a classifier with minimal selective risk and guaranteed coverage. We prove that despite their different formulations the three rejection models lead to the same prediction strategy: a Bayes classifier endowed with a randomized Bayes selection function. We define a notion of a proper uncertainty score as a scalar summary of prediction uncertainty sufficient to construct the randomized Bayes selection function. We propose two algorithms to learn the proper uncertainty score from examples for an arbitrary black-box classifier. We prove that both algorithms provide Fisher consistent estimates of the proper uncertainty score and we demonstrate their efficiency on different prediction problems including classification, ordinal regression and structured output classification."}}
{"id": "pIPud-gYZ1", "cdate": 1609459200000, "mdate": 1682326225726, "content": {"title": "Dominant subject recognition by Bayesian learning", "abstract": "We tackle the problem of dominant subject recognition (DSR), which aims at identifying the faces of the subject whose faces appear most frequently in a given collection of images. We propose a simple algorithm solving the DSR problem in a principled way via Bayesian learning. The proposed algorithm has complexity quadratic in the number of detected faces, and it provides labeling of images along with an accurate estimate of the prediction confidence. The prediction confidence permits using the algorithm in semiautomatic mode when only a subset of images with uncertain labels are corrected manually. We demonstrate on a challenging IJB-B database, that the algorithm significantly reduces the number of images that need to be manually annotated to get the perfect performance of face verification and face identification systems using the face database created by the method."}}
{"id": "mehwCjdSHER", "cdate": 1609459200000, "mdate": 1682326225620, "content": {"title": "Learning Maximum Margin Markov Networks from examples with missing labels", "abstract": "Structured output classifiers based on the framework of Markov Networks provide a transparent way to model statistical dependencies between output labels. The Markov Network (MN) classifier can be ..."}}
{"id": "SFV-Ei33Vx8", "cdate": 1609459200000, "mdate": 1682326225420, "content": {"title": "Hairstyle Transfer between Face Images", "abstract": "We propose a neural network which takes two inputs, a hair image and a face image, and produces an output image having the hair of the hair image seamlessly merged with the inner face of the face image. Our architecture consists of neural networks mapping the input images into a latent code of a pretrained StyleGAN2 which generates the output high-definition image. We propose an algorithm for training parameters of the architecture solely from synthetic images generated by the StyleGAN2 itself without the need of any annotations or external dataset of hairstyle images. We empirically demonstrate the effectiveness of our method in applications including hair-style transfer, hair generation for 3D morphable models, and hair-style interpolation. Fidelity of the generated images is verified by a user study and by a novel hairstyle metric proposed in the paper."}}
{"id": "zDAJXbpPLq", "cdate": 1577836800000, "mdate": null, "content": {"title": "CNN Based Predictor of Face Image Quality", "abstract": "We propose a novel method for training Convolution Neural Network, named CNN-FQ, which takes a face image and outputs a scalar summary of the image quality. The CNN-FQ is trained from triplets of faces that are automatically labeled based on responses of a pre-trained face matcher. The quality scores extracted by the CNN-FQ are directly linked to the probability that the face matcher incorrectly ranks a randomly selected triplet of faces. We applied the proposed CNN-FQ, trained on CASIA database, for selection of the best quality image from a collection of face images capturing the same identity. The quality of the single face representation was evaluated on 1:1 Verification and 1:N Identification tasks defined by the challenging IJB-B protocol. We show that the recognition performance obtained when using faces selected based on the CNN-FQ scores is significantly higher than what can be achieved by competing state-of-the-art image quality extractors."}}
{"id": "7CkxiwWFTVk", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Model Evaluation Under Non-constant Class Imbalance", "abstract": "Many real-world classification problems are significantly class-imbalanced to detriment of the class of interest. The standard set of proper evaluation metrics is well-known but the usual assumption is that the test dataset imbalance equals the real-world imbalance. In practice, this assumption is often broken for various reasons. The reported results are then often too optimistic and may lead to wrong conclusions about industrial impact and suitability of proposed techniques. We introduce methods (Supplementary code related to techniques described in this paper is available at: https://github.com/CiscoCTA/nci_eval ) focusing on evaluation under non-constant class imbalance. We show that not only the absolute values of commonly used metrics, but even the order of classifiers in relation to the evaluation metric used is affected by the change of the imbalance rate. Finally, we demonstrate that using subsampling in order to get a test dataset with class imbalance equal to the one observed in the wild is not necessary, and eventually can lead to significant errors in classifier\u2019s performance estimate."}}
{"id": "6-fS5kXD7Rm", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Model Evaluation under Non-constant Class Imbalance", "abstract": "Many real-world classification problems are significantly class-imbalanced to detriment of the class of interest. The standard set of proper evaluation metrics is well-known but the usual assumption is that the test dataset imbalance equals the real-world imbalance. In practice, this assumption is often broken for various reasons. The reported results are then often too optimistic and may lead to wrong conclusions about industrial impact and suitability of proposed techniques. We introduce methods focusing on evaluation under non-constant class imbalance. We show that not only the absolute values of commonly used metrics, but even the order of classifiers in relation to the evaluation metric used is affected by the change of the imbalance rate. Finally, we demonstrate that using subsampling in order to get a test dataset with class imbalance equal to the one observed in the wild is not necessary, and eventually can lead to significant errors in classifier's performance estimate."}}
{"id": "HkW2QhbubS", "cdate": 1546300800000, "mdate": null, "content": {"title": "On discriminative learning of prediction uncertainty", "abstract": "In classification with a reject option, the classifier is allowed in uncertain cases to abstain from prediction. The classical cost based model of an optimal classifier with a reject option require..."}}
