{"id": "7IvloQtb6Q", "cdate": 1672531200000, "mdate": 1682372897378, "content": {"title": "Interpret Your Care: Predicting the Evolution of Symptoms for Cancer Patients", "abstract": "Cancer treatment is an arduous process for patients and causes many side-effects during and post-treatment. The treatment can affect almost all body systems and result in pain, fatigue, sleep disturbances, cognitive impairments, etc. These conditions are often under-diagnosed or under-treated. In this paper, we use patient data to predict the evolution of their symptoms such that treatment-related impairments can be prevented or effects meaningfully ameliorated. The focus of this study is on predicting the pain and tiredness level of a patient post their diagnosis. We implement an interpretable decision tree based model called LightGBM on real-world patient data consisting of 20163 patients. There exists a class imbalance problem in the dataset which we resolve using the oversampling technique of SMOTE. Our empirical results show that the value of the previous level of a symptom is a key indicator for prediction and the weighted average deviation in prediction of pain level is 3.52 and of tiredness level is 2.27."}}
{"id": "5PfL2FAfWG", "cdate": 1664806784521, "mdate": null, "content": {"title": "Performative Prediction in Time Series: A Case Study", "abstract": "Performative prediction is a phenomenon where a model's predictions, or the decisions based on these predictions, may influence the outcomes of the model. This is especially conspicuous in a time series prediction setting where interventions occur before outcomes are observed. These interventions dictate which data points in the time series can be used as inputs for future predictions. In this paper, we represent patient-reported symptom values collected during their oncology appointments as a time series. We use a decision-tree based model to predict a patient's future symptom values. Based on these predictions, clinicians decide which symptom values will be observed in the future. We propose methods to provide robustness against the problem of performative prediction in time series. Our results characterise how performative prediction may lead to a 29.4% to 40.7% higher error across different symptoms."}}
{"id": "Sc9ESMyTZ9", "cdate": 1647272508175, "mdate": null, "content": {"title": "Summarizing Societies: Agent Abstraction in Multi-Agent Reinforcement Learning", "abstract": "Agents cannot make sense of many-agent societies through direct consideration of small-scale, low-level agent identities, but instead must recognize emergent collective identities. Here, we take a first step towards a framework for recognizing this structure in large groups of low-level agents so that they can be modeled as a much smaller number of high-level agents\u2014a process that we call agent abstraction. We illustrate this process by extending bisimulation metrics for state abstraction in reinforcement learning to the setting of multi-agent reinforcement learning and analyze a straightforward, if crude, abstraction based on experienced joint actions. It addresses non-stationarity due to other learning agents by improving minimax regret by a intuitive factor. To test if this compression factor provides signal for higher-level agency, we applied it to a large dataset of human play of the popular social dilemma game Diplomacy. We find that it correlates strongly with the degree of ground-truth abstraction of low-level units into the human players."}}
{"id": "RRuF5FvrUrz", "cdate": 1640995200000, "mdate": 1682327821953, "content": {"title": "A Game-Theoretic Perspective on Risk-Sensitive Reinforcement Learning", "abstract": ""}}
{"id": "4-TFU9c6Wnb", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Reinforcement Learning Approach to Jointly Adapt Vehicular Communications and Planning for Optimized Driving", "abstract": "Our premise is that autonomous vehicles must optimize communications and motion planning jointly. Specifically, a vehicle must adapt its motion plan staying cognizant of communications rate related constraints and adapt the use of communications while being cognizant of motion planning related restrictions that may be imposed by the on-road environment. To this end, we formulate a reinforcement learning problem wherein an autonomous vehicle jointly chooses (a) a motion planning action that executes on-road and (b) a communications action of querying sensed information from the infrastructure. The goal is to optimize the driving utility of the autonomous vehicle. We apply the Q-Iearning algorithm to make the vehicle learn the optimal policy, which makes the optimal choice of planning and communications actions at any given time. We demonstrate the ability of the optimal policy to smartly adapt communications and planning actions, while achieving large driving utilities, using simulations."}}
