{"id": "b3itJyarLM0", "cdate": 1663850379836, "mdate": null, "content": {"title": "Distributed Extra-gradient with Optimal Complexity and Communication Guarantees", "abstract": "We consider monotone variational inequality (VI) problems in multi-GPU  settings where multiple processors/workers/clients have access to local stochastic dual vectors. This setting  includes a broad range of important problems from distributed convex minimization to min-max and games. Extra-gradient, which is a de facto algorithm  for monotone VI problems, has not been designed to be communication-efficient. To this end, we propose a quantized generalized extra-gradient (Q-GenX), which is an unbiased and adaptive compression method tailored to solve VIs. We provide an adaptive step-size rule, which  adapts to the respective noise profiles at hand and achieve a fast rate of  ${\\cal O}(1/T)$ under relative noise, and an order-optimal ${\\cal O}(1/\\sqrt{T})$ under absolute noise  and show distributed training accelerates convergence. Finally, we validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs.\n"}}
{"id": "Ho7W1yr8tV", "cdate": 1663850376806, "mdate": null, "content": {"title": "Handling Covariate Shifts in Federated Learning  with Generalization Guarantees", "abstract": "Covariate shift across clients is a major challenge for federated learning (FL). This work studies the generalization properties of FL under intra-client and inter-client covariate shifts. To this end, we propose Federated Importance-weighteD Empirical risk Minimization (FIDEM) to optimize a global FL model, along with new variants of density ratio matching methods, aiming to handle covariate shifts. These methods trade off some level of privacy for improving the overall generalization performance. We theoretically show that FIDEM achieves smaller generalization error than classical empirical risk minimization under some certain settings. Experimental results demonstrate the superiority of FIDEM over federated averaging (McMahan et al., 2017)  and other baselines, which would open the door to study FL under distribution shifts more systematically.\n"}}
{"id": "_5Q4covjmH", "cdate": 1663850364548, "mdate": null, "content": {"title": "A Simulation-based Framework for Robust Federated Learning to Training-time Attacks", "abstract": "Well-known robust aggregation schemes in federated learning (FL) are shown to be vulnerable to an informed adversary who can tailor training-time attacks [Fang et al., Xie et al.]. We frame robust distributed learning problem as a game between a server and an adversary that is able to optimize strong training-time attacks. We introduce RobustTailor, a simulation-based framework that prevents the adversary from being omniscient. The simulated game we propose enjoys theoretical guarantees through a regret analysis. RobustTailor improves robustness to training-time attacks significantly while preserving almost the same privacy guarantees as standard robust aggregation schemes in FL. Empirical results under challenging attacks show that RobustTailor performs similar to an upper bound with perfect knowledge of honest clients."}}
{"id": "Di9imc8dZ6", "cdate": 1640995200000, "mdate": 1682182208989, "content": {"title": "Robust Design of Multicell D2D Communication Under Partial CSI", "abstract": "We consider device-to-device (D2D) communication underlaid in a cellular network to share the uplink resource of cellular users (CUs). It is a key the emerging Internet of Things to support vehicle-to-everything communication networks. In a multicell scenario, both D2D pairs and CUs may cause significant significant intercell interference (ICI) to the neighboring cells. Furthermore, due to substantial signaling overhead, we assume only partial channel state information (CSI) of D2D links at the base station. We consider joint power control, beamforming, and CU-D2D matching problem, assuming partial CSI from D2D pairs under the general Nakagami fading model. We formulate a joint receive beamforming and robust power control optimization problem for a CU-D2D pair to expected sum rate under the power budget, while meeting the minimum SINR requirements and worst case ICI limits at neighboring cells in a probabilistic sense. We propose an efficient algorithm that combines an iterative D2D feasibility check and a ratio-of-expectation approximation. A performance upper bound is also developed for benchmarking. For multiple CUs and D2D pairs, due to orthogonal channelization within each cell, we first focus on the problem of joint power control and beamforming for a CU-D2D pair and show how our proposed solution can be leveraged to find a solution for this general problem. The complexity analysis of the proposed approach is also provided. Simulation results show that the proposed algorithm gives performance close to the upper bound."}}
{"id": "-oD4Ldacjg", "cdate": 1640995200000, "mdate": 1682182208849, "content": {"title": "MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks", "abstract": "Implementations of SGD on distributed systems create new vulnerabilities, which can be identified and misused by one or more adversarial agents. Recently, it has been shown that well-known Byzantine-resilient gradient aggregation schemes are indeed vulnerable to informed attackers that can tailor the attacks (Fang et al., 2020; Xie et al., 2020b). We introduce MixTailor, a scheme based on randomization of the aggregation strategies that makes it impossible for the attacker to be fully informed. Deterministic schemes can be integrated into MixTailor on the fly without introducing any additional hyperparameters. Randomization decreases the capability of a powerful adversary to tailor its attacks, while the resulting randomized aggregation scheme is still competitive in terms of performance. For both iid and non-iid settings, we establish almost sure convergence guarantees that are both stronger and more general than those available in the literature. Our empirical studies across various datasets, attacks, and settings, validate our hypothesis and show that MixTailor successfully defends when well-known Byzantine-tolerant schemes fail."}}
{"id": "HdnUQk9jbUO", "cdate": 1632875712017, "mdate": null, "content": {"title": "Linear Convergence of SGD on Overparametrized Shallow Neural Networks", "abstract": "Despite the non-convex landscape, first-order methods can be shown to reach global minima when training overparameterized neural networks, where the number of parameters far exceed the number of training data. In this work, we prove linear convergence of stochastic gradient descent when training a two-layer neural network with smooth activations. While the existing theory either requires a high degree of overparameterization or non-standard initialization and training strategies, e.g., training only a single layer, we show that a subquadratic scaling on the width is sufficient under standard initialization and training both layers simultaneously if the minibatch size is sufficiently large and it also grows with the number of training examples. Via the batch size, our results interpolate between the state-of-the-art subquadratic results for gradient descent and the quadratic results in the worst case."}}
{"id": "NhbFhfM960", "cdate": 1621630288827, "mdate": null, "content": {"title": "Subquadratic Overparameterization for Shallow Neural Networks", "abstract": "Overparameterization refers to the important phenomenon where the width of a neural network is chosen such that learning algorithms can provably attain zero loss in nonconvex training. The existing theory establishes such global convergence using various initialization strategies, training modifications, and width scalings. In particular, the state-of-the-art results require the width to scale quadratically with the number of training data under standard initialization strategies used in practice for best generalization performance. In contrast, the most recent results obtain linear scaling either with requiring initializations that lead to the \"lazy-training\",  or training only a single layer. In this work, we provide an analytical framework that allows us to adopt standard initialization strategies, possibly avoid lazy training, and train all layers simultaneously in basic shallow neural networks while attaining  a desirable subquadratic scaling on the network width. We achieve the desiderata via Polyak-Lojasiewicz condition, smoothness, and standard assumptions on data, and use tools from random matrix theory."}}
{"id": "rFWb0U2PkG9", "cdate": 1609459200000, "mdate": 1647438934506, "content": {"title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed to perform parallel model training. One popular communication-compression method for data-parallel SGD is QSGD (Alistarh et al., 2017), which quantizes and encodes gradients to reduce communication costs. The baseline variant of QSGD provides strong theoretical guarantees, however, for practical purposes, the authors proposed a heuristic variant which we call QSGDinf, which demonstrated impressive empirical gains for distributed training of large neural networks. In this paper, we build on this work to propose a new gradient quantization scheme, and show that it has both stronger theoretical guarantees than QSGD, and matches and exceeds the empirical performance of the QSGDinf heuristic and of other compression methods."}}
{"id": "n_CmzgdE7p6", "cdate": 1609459200000, "mdate": null, "content": {"title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed to perform parallel model training. One popular communication-compression method for data-parallel SGD is QSGD (Alistarh et al., 2017), which quantizes and encodes gradients to reduce communication costs. The baseline variant of QSGD provides strong theoretical guarantees, however, for practical purposes, the authors proposed a heuristic variant which we call QSGDinf, which demonstrated impressive empirical gains for distributed training of large neural networks. In this paper, we build on this work to propose a new gradient quantization scheme, and show that it has both stronger theoretical guarantees than QSGD, and matches and exceeds the empirical performance of the QSGDinf heuristic and of other compression methods."}}
{"id": "iYCoJTSJOD3", "cdate": 1609459200000, "mdate": 1682182209038, "content": {"title": "Subquadratic Overparameterization for Shallow Neural Networks", "abstract": "Overparameterization refers to the important phenomenon where the width of a neural network is chosen such that learning algorithms can provably attain zero loss in nonconvex training. The existing theory establishes such global convergence using various initialization strategies, training modifications, and width scalings. In particular, the state-of-the-art results require the width to scale quadratically with the number of training data under standard initialization strategies used in practice for best generalization performance. In contrast, the most recent results obtain linear scaling either with requiring initializations that lead to the \"lazy-training\", or training only a single layer. In this work, we provide an analytical framework that allows us to adopt standard initialization strategies, possibly avoid lazy training, and train all layers simultaneously in basic shallow neural networks while attaining a desirable subquadratic scaling on the network width. We achieve the desiderata via Polyak-Lojasiewicz condition, smoothness, and standard assumptions on data, and use tools from random matrix theory."}}
