{"id": "n5nyKmJ1Jl", "cdate": 1672531200000, "mdate": 1681739514122, "content": {"title": "Hardware-aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators", "abstract": "Analog in-memory computing (AIMC) -- a promising approach for energy-efficient acceleration of deep learning workloads -- computes matrix-vector multiplications (MVMs) but only approximately, due to nonidealities that often are non-deterministic or nonlinear. This can adversely impact the achievable deep neural network (DNN) inference accuracy as compared to a conventional floating point (FP) implementation. While retraining has previously been suggested to improve robustness, prior work has explored only a few DNN topologies, using disparate and overly simplified AIMC hardware models. Here, we use hardware-aware (HWA) training to systematically examine the accuracy of AIMC for multiple common artificial intelligence (AI) workloads across multiple DNN topologies, and investigate sensitivity and robustness to a broad set of nonidealities. By introducing a new and highly realistic AIMC crossbar-model, we improve significantly on earlier retraining approaches. We show that many large-scale DNNs of various topologies, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers, can in fact be successfully retrained to show iso-accuracy on AIMC. Our results further suggest that AIMC nonidealities that add noise to the inputs or outputs, not the weights, have the largest impact on DNN accuracy, and that RNNs are particularly robust to all nonidealities."}}
{"id": "2iQZkJiLq3x", "cdate": 1672531200000, "mdate": 1681739514130, "content": {"title": "Fast offset corrected in-memory training", "abstract": "In-memory computing with resistive crossbar arrays has been suggested to accelerate deep-learning workloads in highly efficient manner. To unleash the full potential of in-memory computing, it is desirable to accelerate the training as well as inference for large deep neural networks (DNNs). In the past, specialized in-memory training algorithms have been proposed that not only accelerate the forward and backward passes, but also establish tricks to update the weight in-memory and in parallel. However, the state-of-the-art algorithm (Tiki-Taka version 2 (TTv2)) still requires near perfect offset correction and suffers from potential biases that might occur due to programming and estimation inaccuracies, as well as longer-term instabilities of the device materials. Here we propose and describe two new and improved algorithms for in-memory computing (Chopped-TTv2 (c-TTv2) and Analog Gradient Accumulation with Dynamic reference (AGAD)), that retain the same runtime complexity but correct for any remaining offsets using choppers. These algorithms greatly relax the device requirements and thus expanding the scope of possible materials potentially employed for such fast in-memory DNN training."}}
{"id": "mQGF_GAu5-r", "cdate": 1640995200000, "mdate": 1681739514162, "content": {"title": "Analog-memory-based 14nm Hardware Accelerator for Dense Deep Neural Networks including Transformers", "abstract": "Analog non-volatile memory (NVM)-based accelerators for deep neural networks perform high-throughput and energy-efficient multiply-accumulate (MAC) operations (e.g., high TeraOPS/W) by taking advantage of massively parallelized analog MAC operations, implemented with Ohm\u2019s law and Kirchhoff\u2019s current law on array-matrices of resistive devices. While the wide-integer and floating-point operations offered by conventional digital CMOS computing are much more suitable than analog computing for conventional applications that require high accuracy and true reproducibility, deep neural networks can still provide competitive end-to-end results even with modest (e.g., 4-bit) precision in synaptic operations. In this paper, we describe a 14-nm inference chip, comprising multiple 512$\\times$ 512 arrays of Phase Change Memory (PCM) devices, which can deliver software-equivalent inference accuracy for MNIST handwritten-digit recognition and recurrent LSTM benchmarks, by using compensation techniques to finesse analog-memory challenges such as conductance drift and noise. We also project accuracy for Natural Language Processing (NLP) tasks performed with a state-of-art large Transformer-based model, BERT, when mapped onto an extended version of this same fundamental chip architecture."}}
{"id": "dW48uxqXQu", "cdate": 1640995200000, "mdate": 1681739514249, "content": {"title": "Pattern Training, Inference, and Regeneration Demonstration Using On-Chip Trainable Neuromorphic Chips for Spiking Restricted Boltzmann Machine", "abstract": ""}}
{"id": "JuQP2piQYy", "cdate": 1640995200000, "mdate": 1681739514133, "content": {"title": "Impact of Phase-Change Memory Flicker Noise and Weight Drift on Analog Hardware Inference for Large-Scale Deep Learning Networks", "abstract": ""}}
{"id": "t4onKwSjsc", "cdate": 1609459200000, "mdate": 1681739514140, "content": {"title": "A Flexible and Fast PyTorch Toolkit for Simulating Training and Inference on Analog Crossbar Arrays", "abstract": "We introduce the IBM ANALOG HARDWARE ACCELERATION KIT, a new and first of a kind open source toolkit to simulate analog crossbar arrays in a convenient fashion from within PYTORCH (freely available at https://github.com/IBM/aihwkit). The toolkit is under active development and is centered around the concept of an \u201canalog tile\u201d which captures the computations performed on a crossbar array. Analog tiles are building blocks that can be used to extend existing network modules with analog components and compose arbitrary artificial neural networks (ANNs) using the flexibility of the PYTORCH framework. Analog tiles can be conveniently configured to emulate a plethora of different analog hardware characteristics and their non-idealities, such as device-to-device and cycle-to-cycle variations, resistive device response curves, and weight and output noise. Additionally, the toolkit makes it possible to design custom unit cell configurations and to use advanced analog optimization algorithms such as Tiki-Taka. Moreover, the backward and update behavior can be set to \u201cideal\" to enable hardware-aware training features for chips that target inference acceleration only. To evaluate the inference accuracy of such chips over time, we provide statistical programming noise and drift models calibrated on phase-change memory hardware. Our new toolkit is fully GPU accelerated and can be used to conveniently estimate the impact of material properties and non-idealities of future analog technology on the accuracy for arbitrary ANNs."}}
{"id": "ihbS4Y4cvHr", "cdate": 1609459200000, "mdate": 1681739514117, "content": {"title": "Toward Software-Equivalent Accuracy on Transformer-Based Deep Neural Networks With Analog Memory Devices", "abstract": "Recent advances in deep learning have been driven by ever-increasing model sizes, with networks growing to millions or even billions of parameters. Such enormous models call for fast and energy-efficient hardware accelerators. We study the potential of Analog AI accelerators based on Non-Volatile Memory, in particular Phase Change Memory (PCM), for software-equivalent accurate inference of natural language processing applications. We demonstrate a path to software-equivalent accuracy for the GLUE benchmark on BERT (Bidirectional Encoder Representations from Transformers), by combining noise-aware training to combat inherent PCM drift and noise sources, together with reduced-precision digital attention-block computation down to INT6."}}
{"id": "3PBzmXA8Bx", "cdate": 1609459200000, "mdate": 1681739514138, "content": {"title": "A flexible and fast PyTorch toolkit for simulating training and inference on analog crossbar arrays", "abstract": "We introduce the IBM Analog Hardware Acceleration Kit, a new and first of a kind open source toolkit to simulate analog crossbar arrays in a convenient fashion from within PyTorch (freely available at https://github.com/IBM/aihwkit). The toolkit is under active development and is centered around the concept of an \"analog tile\" which captures the computations performed on a crossbar array. Analog tiles are building blocks that can be used to extend existing network modules with analog components and compose arbitrary artificial neural networks (ANNs) using the flexibility of the PyTorch framework. Analog tiles can be conveniently configured to emulate a plethora of different analog hardware characteristics and their non-idealities, such as device-to-device and cycle-to-cycle variations, resistive device response curves, and weight and output noise. Additionally, the toolkit makes it possible to design custom unit cell configurations and to use advanced analog optimization algorithms such as Tiki-Taka. Moreover, the backward and update behavior can be set to \"ideal\" to enable hardware-aware training features for chips that target inference acceleration only. To evaluate the inference accuracy of such chips over time, we provide statistical programming noise and drift models calibrated on phase-change memory hardware. Our new toolkit is fully GPU accelerated and can be used to conveniently estimate the impact of material properties and non-idealities of future analog technology on the accuracy for arbitrary ANNs."}}
{"id": "jyNth4Lx5ha", "cdate": 1577836800000, "mdate": 1681739514113, "content": {"title": "Training Large-scale Artificial Neural Networks on Simulated Resistive Crossbar Arrays", "abstract": "Resistive crossbar arrays are promising options for accelerating enormous computation needed for training modern deep neural networks (DNNs). However, verification of this idea has not been scaled up to realistically sized DNNs due to the nonideal device behavior and hardware design constraints. In this article, the authors propose a novel simulation framework to explore such design constraints on the large-scale problems and devise algorithmic measures to pave the way for robust resistive crossbar-based DNN training accelerators. -Jungwook Choi, IBM Research."}}
{"id": "HklpA4hIPH", "cdate": 1569273044794, "mdate": null, "content": {"title": "RAPA-ConvNets: Modified Convolutional Networks for Accelerated Training on Architectures With Analog Arrays", "abstract": ""}}
