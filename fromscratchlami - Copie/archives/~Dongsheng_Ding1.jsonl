{"id": "oiwpV-S4LO", "cdate": 1640995200000, "mdate": 1681759757160, "content": {"title": "Policy gradient primal-dual mirror descent for constrained MDPs with large state spaces", "abstract": "We study constrained sequential decision-making problems modeled by constrained Markov decision processes with potentially infinite state spaces. We propose a Bregman distance-based direct policy search method \u2013 policy gradient primal-dual mirror descent \u2013 which includes the natural policy primal-dual method and the projected policy primal-dual method as two special cases. When the exact gradient is known, we prove dimension-free global convergence with a sublinear rate in both optimality gap and constraint violation. When the exact gradient is not available, we instantiate our algorithm in the linear function approximation setting and establish sample complexity guarantees. The introduction of the Bregman-distance regularizers enjoys the dimension-free property with applicability to large-scale spaces, the first of its kind in the constrained RL literature."}}
{"id": "jGdZ0wHSaTk", "cdate": 1640995200000, "mdate": 1668641747179, "content": {"title": "Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs", "abstract": "We study sequential decision making problems aimed at maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we establish sublinear convergence rates up to a function approximation error caused by restricted policy parametrization. We also provide convergence and finite-sample complexity guarantees for two sample-based NPG-PD algorithms. Finally, we use computational experiments to showcase the merits and the effectiveness of our approach."}}
{"id": "ggH2lwqgYX", "cdate": 1640995200000, "mdate": 1681759757165, "content": {"title": "Independent Policy Gradient for Large-Scale Markov Potential Games: Sharper Rates, Function Approximation, and Game-Agnostic Convergence", "abstract": "We examine global non-asymptotic convergence properties of policy gradient methods for multi-agent reinforcement learning (RL) problems in Markov potential games (MPGs). To learn a Nash equilibrium..."}}
{"id": "Sq1_SbsVAhX", "cdate": 1640995200000, "mdate": 1681759757163, "content": {"title": "Convergence and optimality of policy gradient primal-dual method for constrained Markov decision processes", "abstract": "We study constrained Markov decision processes with finite state and action spaces. The optimal solution of a discounted infinite-horizon optimal control problem is obtained using a Policy Gradient Primal-Dual (PG-PD) method without any policy parametrization. This method updates the primal variable via projected policy gradient ascent and the dual variable via projected sub-gradient descent. Despite the lack of concavity of the constrained maximization problem in policy space, we exploit the underlying structure to provide non-asymptotic global convergence guarantees with sublinear rates in terms of both the optimality gap and the constraint violation. Furthermore, for a sample-based PG-PD algorithm, we quantify sample complexity and offer computational experiments to demonstrate the effectiveness of our results."}}
{"id": "QnaT-LzWq_", "cdate": 1640995200000, "mdate": 1681759757165, "content": {"title": "Independent Policy Gradient for Large-Scale Markov Potential Games: Sharper Rates, Function Approximation, and Game-Agnostic Convergence", "abstract": "We examine global non-asymptotic convergence properties of policy gradient methods for multi-agent reinforcement learning (RL) problems in Markov potential games (MPG). To learn a Nash equilibrium of an MPG in which the size of state space and/or the number of players can be very large, we propose new independent policy gradient algorithms that are run by all players in tandem. When there is no uncertainty in the gradient evaluation, we show that our algorithm finds an $\\epsilon$-Nash equilibrium with $O(1/\\epsilon^2)$ iteration complexity which does not explicitly depend on the state space size. When the exact gradient is not available, we establish $O(1/\\epsilon^5)$ sample complexity bound in a potentially infinitely large state space for a sample-based algorithm that utilizes function approximation. Moreover, we identify a class of independent policy gradient algorithms that enjoys convergence for both zero-sum Markov games and Markov cooperative games with the players that are oblivious to the types of games being played. Finally, we provide computational experiments to corroborate the merits and the effectiveness of our theoretical developments."}}
{"id": "o2ZhfTy74I", "cdate": 1609459200000, "mdate": 1681759757178, "content": {"title": "Discounted online Newton method for time-varying time series prediction", "abstract": "We develop an online convex optimization method for predicting time series based on streaming observations. We first approximate the evolution of time-varying autoregressive integrated moving average (ARIMA) processes and then propose a discounted online Newton method for estimating time-varying ARIMA time series. Under practical assumptions, we establish dynamic regret bounds that quantify the tracking performance of our algorithm. To verify the effectiveness and robustness of our method, we conduct experiments on prediction problems based on both artificial data and real-world COVID-19 data. To the best of our knowledge, we are the first to report a COVID-19 prediction that utilizes online learning."}}
{"id": "Q3etZFc2fb5", "cdate": 1609459200000, "mdate": 1681759757176, "content": {"title": "Provably Efficient Safe Exploration via Primal-Dual Policy Optimization", "abstract": "We study the safe reinforcement learning problem using the constrained Markov decision processes in which an agent aims to maximize the expected total reward subject to a safety constraint on the expected total value of a utility function. We focus on an episodic setting with the function approximation where the Markov transition kernels have a linear structure but do not impose any additional assumptions on the sampling model. Designing safe reinforcement learning algorithms with provable computational and statistical efficiency is particularly challenging under this setting because of the need to incorporate both the safety constraint and the function approximation into the fundamental exploitation/exploration tradeoff. To this end, we present an \\underline{O}ptimistic \\underline{P}rimal-\\underline{D}ual Proximal Policy \\underline{OP}timization \\mbox{(OPDOP)} algorithm where the value function is estimated by combining the least-squares policy evaluation and an additional bonus term for safe exploration. We prove that the proposed algorithm achieves an $\\tilde{O}(d H^{2.5}\\sqrt{T})$ regret and an $\\tilde{O}(d H^{2.5}\\sqrt{T})$ constraint violation, where $d$ is the dimension of the feature mapping, $H$ is the horizon of each episode, and $T$ is the total number of steps. These bounds hold when the reward/utility functions are fixed but the feedback after each episode is bandit. Our bounds depend on the capacity of the state-action space only through the dimension of the feature mapping and thus our results hold even when the number of states goes to infinity. To the best of our knowledge, we provide the first provably efficient online policy optimization algorithm for constrained Markov decision processes in the function approximation setting, with safe exploration."}}
{"id": "LGN68oEQs0", "cdate": 1609459200000, "mdate": 1681759757181, "content": {"title": "Byzantine-resilient distributed learning under constraints", "abstract": "We consider a class of convex distributed statistical learning problems with inequality constraints in an adversarial scenario. At each iteration, an \u03b1-fraction of m machines, which are supposed to compute stochastic gradients of the loss function and send them to a master machine, may act adversarially and send faulty gradients. To guard against defective information sharing, we develop a Byzantine primal-dual algorithm. For \u03b1 \u2208 [0,0.5), we prove that after T iterations the algorithm achieves ~O(1/T+1/\u221a{mT}-+\u03b1/\u221aT) statistical error bounds on both the optimality gap and the constraint violation. Our result holds for a class of normed vector spaces and, when specialized to the Euclidean space, it attains the optimal error bound for Byzantine stochastic gradient descent."}}
{"id": "X_MBWxucV6", "cdate": 1577836800000, "mdate": 1681759757215, "content": {"title": "Provably Efficient Safe Exploration via Primal-Dual Policy Optimization", "abstract": "We study the Safe Reinforcement Learning (SRL) problem using the Constrained Markov Decision Process (CMDP) formulation in which an agent aims to maximize the expected total reward subject to a safety constraint on the expected total value of a utility function. We focus on an episodic setting with the function approximation where the Markov transition kernels have a linear structure but do not impose any additional assumptions on the sampling model. Designing SRL algorithms with provable computational and statistical efficiency is particularly challenging under this setting because of the need to incorporate both the safety constraint and the function approximation into the fundamental exploitation/exploration tradeoff. To this end, we present an \\underline{O}ptimistic \\underline{P}rimal-\\underline{D}ual Proximal Policy \\underline{OP}timization (OPDOP) algorithm where the value function is estimated by combining the least-squares policy evaluation and an additional bonus term for safe exploration. We prove that the proposed algorithm achieves an $\\tilde{O}(d H^{2.5}\\sqrt{T})$ regret and an $\\tilde{O}(d H^{2.5}\\sqrt{T})$ constraint violation, where $d$ is the dimension of the feature mapping, $H$ is the horizon of each episode, and $T$ is the total number of steps. These bounds hold when the reward/utility functions are fixed but the feedback after each episode is bandit. Our bounds depend on the capacity of the state-action space only through the dimension of the feature mapping and thus our results hold even when the number of states goes to infinity. To the best of our knowledge, we provide the first provably efficient online policy optimization algorithm for CMDP with safe exploration in the function approximation setting."}}
{"id": "6j_CEfnEfdH", "cdate": 1577836800000, "mdate": 1681759757177, "content": {"title": "Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes", "abstract": "We study sequential decision-making problems in which each agent aims to maximize the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon Constrained Markov Decision Processes (CMDPs) problem. Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method for CMDPs which updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Even though the underlying maximization involves a nonconcave objective function and a nonconvex constraint set under the softmax policy parametrization, we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such a convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for the general smooth policy class, we establish sublinear rates of convergence regarding both the optimality gap and the constraint violation, up to a function approximation error caused by restricted policy parametrization. Finally, we show that two sample-based NPG-PD algorithms inherit such non-asymptotic convergence properties and provide finite-sample complexity guarantees. To the best of our knowledge, our work is the first to establish non-asymptotic convergence guarantees of policy-based primal-dual methods for solving infinite-horizon discounted CMDPs. We also provide computational results to demonstrate merits of our approach."}}
