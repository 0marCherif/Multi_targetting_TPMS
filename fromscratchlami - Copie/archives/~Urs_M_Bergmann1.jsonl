{"id": "u6p6mVR22f", "cdate": 1667378992129, "mdate": null, "content": {"title": "Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations", "abstract": "A classical problem in computer vision is to infer a 3D scene representation from few images that can be used to render novel views at interactive rates. Previous work focuses on reconstructing pre-defined 3D representations, eg textured meshes, or implicit representations, eg radiance fields, and often requires input images with precise camera poses and long processing times for each novel scene. In this work, we propose the Scene Representation Transformer (SRT), a method which processes posed or unposed RGB images of a new area, infers a\" set-latent scene representation\", and synthesises novel views, all in a single feed-forward pass. To calculate the scene representation, we propose a generalization of the Vision Transformer to sets of images, enabling global information integration, and hence 3D reasoning. An efficient decoder transformer parameterizes the light field by attending into the scene representation to render novel views. Learning is supervised end-to-end by minimizing a novel-view reconstruction error. We show that this method outperforms recent baselines in terms of PSNR and speed on synthetic datasets, including a new dataset created for the paper. Further, we demonstrate that SRT scales to support interactive visualization and semantic segmentation of real-world outdoor environments using Street View imagery."}}
{"id": "kIFTATmdaL", "cdate": 1640995200000, "mdate": 1667374820317, "content": {"title": "Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations", "abstract": "A classical problem in computer vision is to infer a 3D scene representation from few images that can be used to render novel views at interactive rates. Previous work focuses on reconstructing pre-defined 3D representations, e.g. textured meshes, or implicit representations, e.g. radiance fields, and often requires input images with precise camera poses and long processing times for each novel scene. In this work, we propose the Scene Representation Transformer (SRT), a method which processes posed or unposed RGB images of a new area, infers a \u201cset-latent scene representation \u201d, and synthesises novel views, all in a single feed-forward pass. To calculate the scene representation, we propose a generalization of the Vision Transformer to sets of images, enabling global information integration, and hence 3D reasoning. An efficient decoder transformer parameterizes the light field by attending into the scene representation to render novel views. Learning is supervised end-to-end by minimizing a novel-view reconstruction error. We show that this method outperforms recent baselines in terms of PSNR and speed on synthetic datasets, including a new dataset created for the paper. Further, we demonstrate that SRT scales to support interactive visualization and semantic segmentation of real-world outdoor environments using Street View imagery."}}
{"id": "j3ittHFPMV", "cdate": 1609459200000, "mdate": 1667374820309, "content": {"title": "Hebbian plasticity in parallel synaptic pathways: A circuit mechanism for systems memory consolidation", "abstract": "Author summary After new memories are acquired, they can be transferred over time into other brain areas\u2014a process called systems memory consolidation. For example, new declarative memories, which refer to the conscious memory of facts and events, depend on the hippocampus. Older declarative memories, however, also rely on neocortical networks. The cellular mechanisms underlying such a transfer are poorly understood. In this work, we show that a simple and in the brain ubiquitous connectivity pattern, combined with a standard learning rule, leads to gradual memory transfer. We illustrate our proposed mechanism in numerical simulations and mathematical analyses. At the neurophysiological level, our theory explains experimental findings on memory storage in the hippocampal formation when specific pathways between neural populations are disrupted. At the psychophysical level, we can account for the power-law forgetting curves typically found in humans. A consequence of the proposed model is that consolidated memories can yield faster responses because they are stored in increasingly shorter synaptic pathways between sensory and motor areas. By giving a mechanistic explanation of the consolidation process, we contribute to the understanding of the transfer of memories and the reorganization of memories over time."}}
{"id": "RCHXpDReIC", "cdate": 1609459200000, "mdate": 1667374820170, "content": {"title": "Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations", "abstract": "A classical problem in computer vision is to infer a 3D scene representation from few images that can be used to render novel views at interactive rates. Previous work focuses on reconstructing pre-defined 3D representations, e.g. textured meshes, or implicit representations, e.g. radiance fields, and often requires input images with precise camera poses and long processing times for each novel scene. In this work, we propose the Scene Representation Transformer (SRT), a method which processes posed or unposed RGB images of a new area, infers a \"set-latent scene representation\", and synthesises novel views, all in a single feed-forward pass. To calculate the scene representation, we propose a generalization of the Vision Transformer to sets of images, enabling global information integration, and hence 3D reasoning. An efficient decoder transformer parameterizes the light field by attending into the scene representation to render novel views. Learning is supervised end-to-end by minimizing a novel-view reconstruction error. We show that this method outperforms recent baselines in terms of PSNR and speed on synthetic datasets, including a new dataset created for the paper. Further, we demonstrate that SRT scales to support interactive visualization and semantic segmentation of real-world outdoor environments using Street View imagery."}}
{"id": "WiGQBFuVRv", "cdate": 1601308045185, "mdate": null, "content": {"title": "Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows", "abstract": "Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution  is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series."}}
{"id": "a0hxuzwFME6", "cdate": 1577836800000, "mdate": 1667374820241, "content": {"title": "Meta-Learning for Size and Fit Recommendation in Fashion", "abstract": "Fashion e-commerce has enjoyed an exponential growth in the last few years. A key challenge of the market players is to offer customers a personalized experience and to suggest relevant articles. In that respect, although product recommendation is a well-studied field, size and fit recommendation is still in its infancy. The size and fit topic is a very challenging problem as data is extremely sparse and noisy. Most approaches so far have exploited traditional machine learning techniques. In this work, we bring forward a meta-learning approach using an underlying deep neural network. The advantage of such an approach lies in its ability to exploit large scale data, learn across fashion categories, and absorb new data efficiently without re-training. We benchmark our method against 3 recent methods proven successful in the domain, and demonstrate various strengths of the proposed approach. To that end, we use a large-scale anonymized dataset of about 9.4 million customer-size interactions, collected over 5 years from around 384k customers."}}
{"id": "6WHopXsOUyS", "cdate": 1577836800000, "mdate": 1667374820234, "content": {"title": "Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows", "abstract": "Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multivariate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multivariate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series."}}
{"id": "S1gINCVYDH", "cdate": 1569439278465, "mdate": null, "content": {"title": "Posterior Sampling: Make Reinforcement Learning Sample Efficient Again", "abstract": "Machine learning thrives on leveraging structure in data, and many breakthroughs (e.g.\\ convolutional networks) have been made by designing algorithms which exploit the underlying structure of a distribution. Reinforcement Learning agents interact with worlds that are similarly full of structure. For example, no sequence of actions an agent takes will ever cause the laws of physics to change, therefore an agent which learns to generalize such laws through time and space will have an advantage. Sample efficient reinforcement learning can be accomplished when assuming that the world has structure and designing learning algorithms which exploit this assumption without knowing the actual structure beforehand. Posterior Sampling for Reinforcement Learning (PSRL) \\citep{strens2000bayesian} is such a method which assumes structure in the world and exploits it for learning. A PSLR learning agent first samples models of the environment which conform to both prior assumptions on the world's structure and past observations and then interacts with the true environment using a policy guided by the sampled model of the environment. While PSRL delivers theoretical Bayesian regret bounds, there are many open issues which must be addressed before PSRL can be applied to current benchmark continuous reinforcement reinforcement tasks. In this work, we identify these issues and find practical solutions to them leading to a novel algorithm we call Neural-PSRL. We validate the algorithm's effectiveness by achieving state of the art results in the HalfCheetah-v3 and Hopper-v3 domains."}}
{"id": "vRcsQjk3Ma", "cdate": 1546300800000, "mdate": 1667374820317, "content": {"title": "A Bandit Framework for Optimal Selection of Reinforcement Learning Agents", "abstract": "Deep Reinforcement Learning has been shown to be very successful in complex games, e.g. Atari or Go. These games have clearly defined rules, and hence allow simulation. In many practical applications, however, interactions with the environment are costly and a good simulator of the environment is not available. Further, as environments differ by application, the optimal inductive bias (architecture, hyperparameters, etc.) of a reinforcement agent depends on the application. In this work, we propose a multi-arm bandit framework that selects from a set of different reinforcement learning agents to choose the one with the best inductive bias. To alleviate the problem of sparse rewards, the reinforcement learning agents are augmented with surrogate rewards. This helps the bandit framework to select the best agents early, since these rewards are smoother and less sparse than the environment reward. The bandit has the double objective of maximizing the reward while the agents are learning and selecting the best agent after a finite number of learning steps. Our experimental results on standard environments show that the proposed framework is able to consistently select the optimal agent after a finite number of steps, while collecting more cumulative reward compared to selecting a sub-optimal architecture or uniformly alternating between different agents."}}
{"id": "pNhSvyDDFP", "cdate": 1546300800000, "mdate": 1667374820226, "content": {"title": "Generating High-Resolution Fashion Model Images Wearing Custom Outfits", "abstract": "Visualizing an outfit is an essential part of shopping for clothes. Due to the combinatorial aspect of combining fashion articles, the available images are limited to a pre-determined set of outfits. In this paper, we broaden these visualizations by generating high-resolution images of fashion models wearing a custom outfit under an input body pose. We show that our approach can not only transfer the style and the pose of one generated outfit to another, but also create realistic images of human bodies and garments."}}
