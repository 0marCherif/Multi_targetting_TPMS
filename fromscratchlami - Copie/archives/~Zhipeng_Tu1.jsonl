{"id": "Vbj5H7-lKfs", "cdate": 1652737773137, "mdate": null, "content": {"title": "Distributed Online Convex Optimization with Compressed Communication", "abstract": "We consider a distributed online convex optimization problem when streaming data are distributed among computing agents over a connected communication network. Since the data are high-dimensional or the network is large-scale, communication load can be a bottleneck for the efficiency of distributed algorithms. To tackle this bottleneck, we apply the state-of-art data compression scheme to the fundamental GD-based distributed online algorithms. Three algorithms with difference-compressed communication are proposed for full information feedback (DC-DOGD), one-point bandit feedback (DC-DOBD), and two-point bandit feedback (DC-DO2BD), respectively. We obtain regret bounds explicitly in terms of time horizon, compression ratio, decision dimension, agent number, and network parameters. Our algorithms are proved to be no-regret and match the same regret bounds, w.r.t. time horizon, with their uncompressed versions for both convex and strongly convex losses. Numerical experiments are given to validate the theoretical findings and illustrate that the proposed algorithms can effectively reduce the total transmitted bits for distributed online training compared with the uncompressed baseline."}}
{"id": "C8PxF4QKUa5", "cdate": 1640995200000, "mdate": 1681651834941, "content": {"title": "Distributed Communication-Sliding Mirror-Descent Algorithm for Nonsmooth Resource Allocation Problem", "abstract": ""}}
{"id": "y8y6GJUL01H", "cdate": 1621629976270, "mdate": null, "content": {"title": "No-regret Online Learning over Riemannian Manifolds", "abstract": "We consider online optimization over Riemannian manifolds, where a learner attempts to minimize a sequence of time-varying loss functions defined on Riemannian manifolds. Though many Euclidean online convex optimization algorithms have been proven useful in a wide range of areas, less attention has been paid to their Riemannian counterparts. In this paper, we study Riemannian online gradient descent (R-OGD) on Hadamard manifolds for both geodesically convex and strongly geodesically convex loss functions, and Riemannian bandit algorithm (R-BAN) on Hadamard homogeneous manifolds for geodesically convex functions. We establish upper bounds on the regrets of the problem with respect to time horizon, manifold curvature, and manifold dimension. We also find a universal lower bound for the achievable regret by constructing an online convex optimization problem on Hadamard manifolds. All the obtained regret bounds match the corresponding results are provided in Euclidean spaces. Finally, some numerical experiments validate our theoretical results."}}
{"id": "9gBGaPGQaXq", "cdate": 1609459200000, "mdate": 1681651834947, "content": {"title": "Multi-agent solver for non-negative matrix factorization based on optimization", "abstract": ""}}
{"id": "0Faj9c1KiM", "cdate": 1609459200000, "mdate": 1681651834965, "content": {"title": "No-regret Online Learning over Riemannian Manifolds", "abstract": ""}}
