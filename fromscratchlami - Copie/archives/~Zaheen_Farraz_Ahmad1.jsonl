{"id": "0qnPBmvJSaf", "cdate": 1621630110999, "mdate": null, "content": {"title": "Monte Carlo Tree Search With Iteratively Refining State Abstractions", "abstract": "Decision-time planning is the process of constructing a transient, local policy with the intent of using it to make the immediate decision. Monte Carlo tree search (MCTS), which has been leveraged to great success in Go, chess, shogi, Hex, Atari, and other settings, is perhaps the most celebrated decision-time planning algorithm. Unfortunately, in its original form, MCTS can degenerate to one-step search in domains with stochasticity. Progressive widening is one way to ameliorate this issue, but we argue that it possesses undesirable properties for some settings. In this work, we present a method, called abstraction refining, for extending MCTS to stochastic environments which, unlike progressive widening, leverages the geometry of the state space. We argue that leveraging the geometry of the space can offer advantages. To support this claim, we present a series of experimental examples in which abstraction refining outperforms progressive widening, given equal simulation budgets."}}
{"id": "LmxS4waBL6", "cdate": 1577836800000, "mdate": 1682569226742, "content": {"title": "Marginal Utility for Planning in Continuous or Large Discrete Action Spaces", "abstract": "Sample-based planning is a powerful family of algorithms for generating intelligent behavior from a model of the environment. Generating good candidate actions is critical to the success of sample-based planners, particularly in continuous or large action spaces. Typically, candidate action generation exhausts the action space, uses domain knowledge, or more recently, involves learning a stochastic policy to provide such search guidance. In this paper we explore explicitly learning a candidate action generator by optimizing a novel objective, marginal utility. The marginal utility of an action generator measures the increase in value of an action over previously generated actions. We validate our approach in both curling, a challenging stochastic domain with continuous state and action spaces, and a location game with a discrete but large action space. We show that a generator trained with the marginal utility objective outperforms hand-coded schemes built on substantial domain knowledge, trained stochastic policies, and other natural objectives for generating actions for sampled-based planners."}}
{"id": "SkWlymzdWS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Action Selection for Hammer Shots in Curling", "abstract": "Curling is an adversarial two-player game with a continuous state and action space, and stochastic transitions. This paper focuses on one aspect of the full game, namely, finding the optimal \"hammer shot\", which is the last action taken before a score is tallied. We survey existing methods for finding an optimal action in a continuous, low-dimensional space with stochastic outcomes, and adapt a method based on Delaunay triangulation to our application. Experiments using our curling physics simulator show that the adapted Delaunay triangulation's shot selection outperforms other algorithms, and with some caveats, exceeds Olympic-level human performance."}}
