{"id": "JFf-bPQu5RB", "cdate": 1663850219188, "mdate": null, "content": {"title": "Leveraged Asymmetric Loss with Disambiguation for Multi-label Recognition with One-Positive Annotations", "abstract": "In the problem of multi-label learning from single positive labels (SPL), we learn the potential multiple labels from one observable single positive annotation. Despite many efforts to solve this problem, an effective algorithm with sound theoretical understanding is still in need. In this paper,  we propose a novel loss function for the SPL problem, called leveraged asymmetric loss with disambiguation (LASD), where we introduce a pair of leverage parameters to address the severe negative-positive imbalance. From the theoretical perspective, we analyze the SPL problem, for the first time, from the perspective of risk consistency, which links the SPL loss with losses for ordinary multi-label classification. We prove the consistency of our proposed LASD loss to the cost-sensitive Hamming loss, which provides guidance to the empirical choice of our proposed leverage parameters. In experiments, we demonstrate the effectiveness of our proposed LASD loss function over other state-of-the-art methods and empirically verify our theoretical results."}}
{"id": "YSVbWFBDup", "cdate": 1663850212881, "mdate": null, "content": {"title": "How Weakly Supervised Information helps Contrastive Learning", "abstract": "Contrastive learning has shown outstanding performances in both supervised and unsupervised learning. However, little is known about when and how weakly supervised information helps improve contrastive learning, especially from the theoretical perspective. The major challenge is that the existing theory of contrastive learning based on supervised learning frameworks failed to distinguish between supervised and unsupervised contrastive learning. Therefore, we turn to the unsupervised learning frameworks, and based on the posterior probability of labels, we translate the weakly supervised information into a similarity graph under the framework of spectral clustering. In this paper, we investigate two typical weakly supervised learning problems, noisy label learning, and semi-supervised learning, and analyze their influence on contrastive learning within a unified framework. Specifically, we analyze the effect of weakly supervised information on the augmentation graph of unsupervised contrastive learning, and consequently on its corresponding error bound. Numerical experiments are carried out to verify the theoretical findings."}}
{"id": "r6wbo62QYg5", "cdate": 1609459200000, "mdate": 1645980866696, "content": {"title": "Leveraged Weighted Loss for Partial Label Learning", "abstract": "As an important branch of weakly supervised learning, partial label learning deals with data where each instance is assigned with a set of candidate labels, whereas only one of them is true. Despite many methodology studies on learning from partial labels, there still lacks theoretical understandings of their risk consistent properties under relatively weak assumptions, especially on the link between theoretical results and the empirical choice of parameters. In this paper, we propose a family of loss functions named \\textit{Leveraged Weighted} (LW) loss, which for the first time introduces the leverage parameter $\\beta$ to consider the trade-off between losses on partial labels and non-partial ones. From the theoretical side, we derive a generalized result of risk consistency for the LW loss in learning from partial labels, based on which we provide guidance to the choice of the leverage parameter $\\beta$. In experiments, we verify the theoretical guidance, and show the high effectiveness of our proposed LW loss on both benchmark and real datasets compared with other state-of-the-art partial label learning algorithms."}}
{"id": "SqLVjT3Xtl5", "cdate": 1609459200000, "mdate": 1645980867023, "content": {"title": "GBHT: Gradient Boosting Histogram Transform for Density Estimation", "abstract": "In this paper, we propose a density estimation algorithm called \\textit{Gradient Boosting Histogram Transform} (GBHT), where we adopt the \\textit{Negative Log Likelihood} as the loss function to make the boosting procedure available for the unsupervised tasks. From a learning theory viewpoint, we first prove fast convergence rates for GBHT with the smoothness assumption that the underlying density function lies in the space $C^{0,\\alpha}$. Then when the target density function lies in spaces $C^{1,\\alpha}$, we present an upper bound for GBHT which is smaller than the lower bound of its corresponding base learner, in the sense of convergence rates. To the best of our knowledge, we make the first attempt to theoretically explain why boosting can enhance the performance of its base learners for density estimation problems. In experiments, we not only conduct performance comparisons with the widely used KDE, but also apply GBHT to anomaly detection to showcase a further application of GBHT."}}
{"id": "SnGbqpnmYeq", "cdate": 1609459200000, "mdate": 1645980866364, "content": {"title": "Leveraged Weighted Loss for Partial Label Learning", "abstract": "As an important branch of weakly supervised learning, partial label learning deals with data where each instance is assigned with a set of candidate labels, whereas only one of them is true. Despit..."}}
{"id": "SU5-9pnmYeq", "cdate": 1609459200000, "mdate": 1645980866363, "content": {"title": "GBHT: Gradient Boosting Histogram Transform for Density Estimation", "abstract": "In this paper, we propose a density estimation algorithm called \\textit{Gradient Boosting Histogram Transform} (GBHT), where we adopt the \\textit{Negative Log Likelihood} as the loss function to ma..."}}
