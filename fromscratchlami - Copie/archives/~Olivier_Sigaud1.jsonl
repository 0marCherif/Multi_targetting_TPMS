{"id": "Q29Eaoknkm8", "cdate": 1693526400000, "mdate": 1695963090520, "content": {"title": "Toward Teachable Autotelic Agents", "abstract": "Autonomous discovery and direct instruction are two distinct sources of learning in children but education sciences demonstrate that mixed approaches such as assisted discovery or guided play result in improved skill acquisition. In the field of artificial intelligence (AI), these extremes, respectively, map to autonomous agents learning from their own signals and interactive learning agents fully taught by their teachers. In between should stand teachable autonomous agents (TAA): agents that learn from both internal and teaching signals to benefit from the higher efficiency of assisted discovery. Designing such agents will enable real-world nonexpert users to orient the learning trajectories of agents toward their expectations. More fundamentally, this may also be a key step to build agents with human-level intelligence. This article presents a roadmap toward the design of teachable autonomous agents from a developmental AI perspective. Building on developmental psychology and education sciences, we start by identifying key features enabling assisted discovery processes in child\u2013tutor interactions. This leads to the production of a checklist of features that future TAAs will need to demonstrate. The checklist allows us to precisely pinpoint the various limitations of current reinforcement learning agents and to identify the promising first steps toward TAAs. It also shows the way forward by highlighting key research directions toward the design or autonomous agents that can be taught by ordinary people via natural pedagogy."}}
{"id": "gPqTF5-hUH_", "cdate": 1672531200000, "mdate": 1695963090510, "content": {"title": "Enhancing Agent Communication and Learning through Action and Language", "abstract": "We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach."}}
{"id": "aEBNYkcdSXi", "cdate": 1672531200000, "mdate": 1695963090527, "content": {"title": "Human-Machine Co-Learning : Case Study on Motor Skill Acquisition", "abstract": ""}}
{"id": "TKOz8QGxaYE", "cdate": 1672531200000, "mdate": 1686315119097, "content": {"title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning", "abstract": "Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5."}}
{"id": "ExA3-9XyWya", "cdate": 1672531200000, "mdate": 1695963090503, "content": {"title": "Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning", "abstract": "In multi-goal Reinforcement Learning, an agent can share experience between related training tasks, resulting in better generalization for new tasks at test time. However, when the goal space has d..."}}
{"id": "-3dg8Jef46", "cdate": 1672531200000, "mdate": 1695963090510, "content": {"title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning", "abstract": "Recent works successfully leveraged Large Language Models\u2019 (LLM) abilities to capture abstract knowledge about world\u2019s physics to solve decision-making problems. Yet, the alignment between LLMs\u2019 kn..."}}
{"id": "Ijw_Am6DeQc", "cdate": 1664358384525, "mdate": null, "content": {"title": "Overcoming Referential Ambiguity in language-guided goal-conditioned Reinforcement Learning", "abstract": "Teaching an agent to perform new tasks using natural language can easily be hindered by ambiguities in interpretation. When a teacher provides an instruction to a learner about an object by referring to its features, the learner can misunderstand the teacher's intentions, for instance if the instruction ambiguously refer to features of the object, a phenomenon called referential ambiguity. We study how two concepts derived from cognitive sciences can help resolve those referential ambiguities: pedagogy (selecting the right instructions) and pragmatism (learning the preferences of the other agents using inductive reasoning). We apply those ideas to a teacher/learner setup with two artificial agents on a simulated robotic task (block-stacking). We show that these concepts improve sample efficiency for training the learner."}}
{"id": "iEE0MadUaZh", "cdate": 1663850549793, "mdate": null, "content": {"title": "Help Me Explore: Combining Autotelic and Social Learning via Active Goal Queries", "abstract": "Most approaches to open-ended skill learning train a single agent in a purely sensorimotor environment. But because no human child learns everything on their own, we argue that sociality will be a key component of open-ended learning systems. This paper enables learning agents to blend individual and socially-guided skill learning through a new interaction protocol named Help Me Explore (HME).\nIn social episodes triggered at the agent's demand, a social partner suggests a goal at the frontier of the agent's capabilities and, when the goal is reached, follows up with a new adjacent goal just beyond. In individual episodes, the agent practices skills autonomously by pursuing goals it already discovered through either its own experience or social suggestions. The idea of augmenting an individual goal exploration with social goal suggestions is simple, general and powerful. We demonstrate its efficiency on two notoriously hard exploration benchmarks: continuous mazes and a 5-block robotic manipulation task. With minimal social interventions, an HME-agent outperforms the purely social agent deprived of its autonomy, and the purely individual agent which fails to solve hard exploration problems."}}
{"id": "XnF9OtkASy", "cdate": 1663850338634, "mdate": null, "content": {"title": "Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning", "abstract": "Multi-goal Reinforcement Learning has recently attracted a large amount of research interest. By allowing experience to be shared between related training tasks, this setting favors generalization for new tasks at test time, whenever some smoothness exists in the considered representation space of goals. However, in settings with discontinuities in state or goal spaces (e.g. walls in a maze), a majority of goals are difficult to reach, due to the sparsity of rewards in the absence of expert knowledge. This implies hard exploration, for which some curriculum of goals must be discovered, to help agents learn by adapting training tasks to their current capabilities. We propose a novel approach: Stein Variational Goal Generation (SVGG), which builds on recent automatic curriculum learning techniques for goal-conditioned policies. SVGG seeks at preferably sampling new goals in the zone of proximal development of the agent, by leveraging a learned model of its abilities and a goal distribution modeled as particles in the exploration space. Our approach relies on Stein Variational Gradient Descent to dynamically attract the goal sampling distribution in areas of appropriate difficulty. We demonstrate the performances of the approach, in terms of success coverage in the goal space, compared to recent state-of-the-art RL methods for hard exploration problems."}}
{"id": "scfOjwTtZ8S", "cdate": 1652737859791, "mdate": null, "content": {"title": "EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL", "abstract": "Reinforcement learning (RL) in long horizon and sparse reward tasks is notoriously difficult and requires a lot of training steps. A standard solution to speed up the process is to leverage additional reward signals, shaping it to better guide the learning process.\nIn the context of language-conditioned RL, the abstraction and generalisation properties of the language input provide opportunities for more efficient ways of shaping the reward.\nIn this paper, we leverage this idea and propose an automated reward shaping method where the agent extracts auxiliary objectives from the general language goal. These auxiliary objectives use a question generation (QG) and a question answering (QA) system: they consist of questions leading the agent to try to reconstruct partial information about the global goal using its own trajectory.\nWhen it succeeds, it receives an intrinsic reward proportional to its confidence in its answer. \nThis incentivizes the agent to generate trajectories which unambiguously explain various aspects of the general language goal.\nOur experimental study using various BabyAI environments shows that this approach, which does not require engineer intervention to design the auxiliary objectives, improves sample efficiency by effectively directing the exploration."}}
