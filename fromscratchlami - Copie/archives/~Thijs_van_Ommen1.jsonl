{"id": "ATf_lVtXqg", "cdate": 1704067200000, "mdate": 1708694582666, "content": {"title": "Fundamental Properties of Causal Entropy and Information Gain", "abstract": "Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks through the study of recently-proposed information theoretic quantities grounded in considerations about causality."}}
{"id": "_-rfmi_ROK", "cdate": 1672531200000, "mdate": 1708694582670, "content": {"title": "Causal Entropy and Information Gain for Measuring Causal Control", "abstract": "Artificial intelligence models and methods commonly lack causal interpretability. Despite the advancements in interpretable machine learning (IML) methods, they frequently assign importance to features which lack causal influence on the outcome variable. Selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. Feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. However, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. To address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. Specifically, we introduce causal versions of entropy and mutual information, termed causal entropy and causal information gain, which are designed to assess how much control a feature provides over the outcome variable. These newly defined quantities capture changes in the entropy of a variable resulting from interventions on other variables. Fundamental results connecting these quantities to the existence of causal effects are derived. The use of causal information gain in feature selection is demonstrated, highlighting its superiority over standard mutual information in revealing which features provide control over a chosen outcome variable. Our investigation paves the way for the development of methods with improved interpretability in domains involving causation."}}
{"id": "tGUmEZFZnKJ", "cdate": 1641552726223, "mdate": 1641552726223, "content": {"title": "Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions", "abstract": "An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases, these different distributions can be modeled as different contexts of a single underlying system, in which each distribution corresponds to a different perturbation of the system, or in causal terms, an intervention. We focus on a class of such causal domain adaptation problems, where data for one or more source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data."}}
{"id": "QnszkNlHYtE", "cdate": 1640995200000, "mdate": 1666273941659, "content": {"title": "Graphical Representations for Algebraic Constraints of Linear Structural Equations Models", "abstract": "The observational characteristics of a linear structural equation model can be effectively described by polynomial constraints on the observed covariance matrix. However, these polynomials can be e..."}}
{"id": "KON_6fW5AYE", "cdate": 1620659593573, "mdate": null, "content": {"title": "Inconsistency of Bayesian inference for misspecified linear models, and a proposal for repairing it", "abstract": "We empirically show that Bayesian inference can be inconsistent under misspecification in simple linear regression problems, both in a model averaging/ selection and in a Bayesian ridge regression setting. We use the standard linear model, which assumes homoskedasticity, whereas the data are heteroskedastic (though, significantly, there are no outliers). As sample size increases, the posterior puts its mass on worse and worse models of ever higher dimension. This is caused by hypercompression, the phenomenon that the posterior puts its mass on distributions that have much larger KL divergence from the ground truth than their average, i.e. the Bayes predictive distribution. To remedy the problem, we equip the likelihood in Bayes\u2019 theorem with an exponent called the learning rate, and we propose the SafeBayesian method to learn the learning rate from the data. SafeBayes tends to select small learning rates, and regularizes more, as soon as hypercompression takes place. Its results on our data are quite encouraging."}}
{"id": "thIQgACVBYk", "cdate": 1546300800000, "mdate": null, "content": {"title": "Robust Causal Domain Adaptation in a Simple Diagnostic Setting", "abstract": ""}}
{"id": "IASC0c-koCm", "cdate": 1546300800000, "mdate": null, "content": {"title": "Efficient Algorithms for Minimax Decisions Under Tree-Structured Incompleteness", "abstract": "When decisions must be based on incomplete (coarsened) observations and the coarsening mechanism is unknown, a minimax approach offers the best guarantees on the decision maker\u2019s expected loss. Recent work has derived mathematical conditions characterizing minimax optimal decisions, but also found that computing such decisions is a difficult problem in general. This problem is equivalent to that of maximizing a certain conditional entropy expression. In this work, we present a highly efficient algorithm for the case where the coarsening mechanism can be represented by a tree, whose vertices are outcomes and whose edges are coarse observations."}}
{"id": "WqYLCN-GYD5", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Bayesian Networks by Branching on Constraints", "abstract": "We consider the Bayesian network structure learning problem, and present a new algorithm for enumerating the $k$ best Markov equivalence classes. This algorithm is score-based, but uses conditional..."}}
{"id": "BJ-nSOZu-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions", "abstract": "An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases, these different distributions can be modeled as different contexts of a single underlying system, in which each distribution corresponds to a different perturbation of the system, or in causal terms, an intervention. We focus on a class of such causal domain adaptation problems, where data for one or more source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data."}}
{"id": "5tIAL-JsSw", "cdate": 1514764800000, "mdate": 1708420640628, "content": {"title": "Algebraic Equivalence of Linear Structural Equation Models", "abstract": "Despite their popularity, many questions about the algebraic constraints imposed by linear structural equation models remain open problems. For causal discovery, two of these problems are especially important: the enumeration of the constraints imposed by a model, and deciding whether two graphs define the same statistical model. We show how the half-trek criterion can be used to make progress in both of these problems. We apply our theoretical results to a small-scale model selection problem, and find that taking the additional algebraic constraints into account may lead to significant improvements in model selection accuracy."}}
