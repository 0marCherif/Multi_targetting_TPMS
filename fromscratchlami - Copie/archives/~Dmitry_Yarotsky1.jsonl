{"id": "bzaPGEllsjE", "cdate": 1663849872928, "mdate": null, "content": {"title": "A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions,  benefit from negative momenta.", "abstract": "Mini-batch SGD with momentum is a fundamental algorithm for learning large predictive models. In this paper we develop a new analytic framework to analyze noise-averaged properties of mini-batch SGD for linear models at constant learning rates, momenta and sizes of batches. Our key idea is to consider the dynamics of the second moments of model parameters for a special family of \"Spectrally Expressible\" approximations. This allows to obtain an explicit expression for the generating function of the sequence of loss values. By analyzing this generating function, we find, in particular, that 1) the SGD dynamics exhibits several convergent and divergent regimes depending on the spectral distributions of the problem; 2) the convergent regimes admit explicit stability conditions, and explicit loss asymptotics in the case of power-law spectral distributions; 3) the optimal convergence rate can be achieved at negative momenta. We verify our theoretical predictions by extensive experiments with MNIST and synthetic problems, and find a good quantitative agreement."}}
{"id": "_qhSc7CTjN", "cdate": 1640995200000, "mdate": 1664551095591, "content": {"title": "Embedded Ensembles: infinite width limit and operating regimes", "abstract": "A memory efficient approach to ensembling neural networks is to share most weights among the ensembled models by means of a single reference network. We refer to this strategy as Embedded Ensembling (EE); its particular examples are BatchEnsembles and Monte-Carlo dropout ensembles. In this paper we perform a systematic theoretical and empirical analysis of embedded ensembles with different number of models. Theoretically, we use a Neural-Tangent-Kernel-based approach to derive the wide network limit of the gradient descent dynamics. In this limit, we identify two ensemble regimes - independent and collective - depending on the architecture and initialization strategy of ensemble models. We prove that in the independent regime the embedded ensemble behaves as an ensemble of independent models. We confirm our theoretical prediction with a wide range of experiments with finite networks, and further study empirically various effects such as transition between the two regimes, scaling of ensemble performance with the network width and number of models, and dependence of performance on a number of architecture and hyperparameter choices."}}
{"id": "Q5cKOSRuC7", "cdate": 1640995200000, "mdate": 1680426950082, "content": {"title": "A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta", "abstract": ""}}
{"id": "75SZJ-lGTl", "cdate": 1640995200000, "mdate": 1681650783381, "content": {"title": "Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions", "abstract": ""}}
{"id": "EHUsTBGIP17", "cdate": 1621630321066, "mdate": null, "content": {"title": "Explicit loss asymptotics in the gradient descent training of neural networks", "abstract": "Current theoretical results on optimization trajectories of neural networks trained by gradient descent typically have the form of rigorous but potentially loose bounds on the loss values. In the present work we take a different approach and show that the learning trajectory of a wide network in a lazy training regime can be characterized by an explicit asymptotic at large training times. Specifically, the leading term in the asymptotic expansion of the loss behaves as a power law $L(t) \\sim C t^{-\\xi}$ with exponent $\\xi$ expressed only through the data dimension, the smoothness of the activation function, and the class of function being approximated. Our results are based on spectral analysis of the integral operator representing the linearized evolution of a large network trained on the expected loss. Importantly, the techniques we employ do not require a specific form of the data distribution, for example Gaussian, thus making our findings sufficiently universal.   "}}
{"id": "n6N3pkELq7", "cdate": 1609459200000, "mdate": 1683885449505, "content": {"title": "Machine Learning-Assisted Channel Estimation in Massive MIMO Receiver", "abstract": "We propose a new Machine Learning (ML) approach to channel estimation (CE) in Massive Multiple Input Multiple Output (MIMO) receivers. The algorithm employs a recurrent neural network (RNN) for iterative channel tap search and nonlinear de-noising of their amplitudes in the time domain. Our method outperforms the sparse minimum mean square error (MMSE) CE that is based on time-domain windowing and a further linear denoising of channel taps within the window. Simulation results are presented for user speed of 5km/h in non-line-of-sight scenarios of the 5G QuaDRiGa 2.0 channel. The results are provided in both antenna and beamspace domains of the 64 antennas receiver."}}
{"id": "GSCggVUDC9", "cdate": 1609459200000, "mdate": 1683885449426, "content": {"title": "Elementary superexpressive activations", "abstract": "We call a finite family of activation functions \\emph{superexpressive} if any multivariate continuous function can be approximated by a neural network that uses these activations and has a fixed ar..."}}
{"id": "36I_y0qWY1", "cdate": 1609459200000, "mdate": 1681650783346, "content": {"title": "Explicit loss asymptotics in the gradient descent training of neural networks", "abstract": ""}}
{"id": "8cgkKfn2IRT", "cdate": 1577836800000, "mdate": null, "content": {"title": "Low-loss connection of weight vectors: distribution-based approaches", "abstract": "Recent research shows that sublevel sets of the loss surfaces of overparameterized networks are connected, exactly or approximately. We describe and compare experimentally a panel of methods used t..."}}
{"id": "7JGfcI6NpI2", "cdate": 1577836800000, "mdate": 1683885449393, "content": {"title": "The phase diagram of approximation rates for deep neural networks", "abstract": "We explore the phase diagram of approximation rates for deep neural networks and prove several new theoretical results. In particular, we generalize the existing result on the existence of deep discontinuous phase in ReLU networks to functional classes of arbitrary positive smoothness, and identify the boundary between the feasible and infeasible rates. Moreover, we show that all networks with a piecewise polynomial activation function have the same phase diagram. Next, we demonstrate that standard fully-connected architectures with a fixed width independent of smoothness can adapt to smoothness and achieve almost optimal rates. Finally, we consider deep networks with periodic activations (\"deep Fourier expansion\") and prove that they have very fast, nearly exponential approximation rates, thanks to the emerging capability of the network to implement efficient lookup operations."}}
