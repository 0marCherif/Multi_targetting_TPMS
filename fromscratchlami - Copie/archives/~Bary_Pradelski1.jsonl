{"id": "68jZhvLZZdK", "cdate": 1682899200000, "mdate": 1696013888201, "content": {"title": "Social influence: The Usage History heuristic", "abstract": ""}}
{"id": "Ojpk1fiEBbd", "cdate": 1672531200000, "mdate": 1696013888195, "content": {"title": "Collaborative Coalitions in Multi-Agent Systems: Quantifying the Strong Price of Anarchy for Resource Allocation Games", "abstract": "The emergence of new communication technologies allows us to expand our understanding of distributed control and consider collaborative decision-making paradigms. With collaborative algorithms, certain local decision-making entities (or agents) are enabled to communicate and collaborate on their actions with one another to attain better system behavior. By limiting the amount of communication, these algorithms exist somewhere between centralized and fully distributed approaches. To understand the possible benefits of this inter-agent collaboration, we model a multi-agent system as a common-interest game in which groups of agents can collaborate on their actions to jointly increase the system welfare. We specifically consider $k$-strong Nash equilibria as the emergent behavior of these systems and address how well these states approximate the system optimal, formalized by the $k$-strong price of anarchy ratio. Our main contributions are in generating tight bounds on the $k$-strong price of anarchy in finite resource allocation games as the solution to a tractable linear program. By varying $k$ --the maximum size of a collaborative coalition--we observe exactly how much performance is gained from inter-agent collaboration. To investigate further opportunities for improvement, we generate upper bounds on the maximum attainable $k$-strong price of anarchy when the agents' utility function can be designed."}}
{"id": "X1huR6fpHc", "cdate": 1640995200000, "mdate": 1683888468470, "content": {"title": "Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints", "abstract": "In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality."}}
{"id": "SIPOK0-1W4", "cdate": 1640995200000, "mdate": 1683888468681, "content": {"title": "Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints", "abstract": "In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality."}}
{"id": "RKia5sW3Y81", "cdate": 1640995200000, "mdate": 1696013888198, "content": {"title": "Double Auctions and Transaction Costs", "abstract": "Transaction costs are omnipresent in markets but are often omitted in economic models. We show that the presence of transaction costs can fundamentally alter incentive and welfare properties of Double Auctions, a canonical market organization. We further show that transaction costs can be categorized into two types. Double Auctions with homogeneous transaction costs---a category that includes fixed fees and price based fees---preserve the key advantages of Double Auctions without transaction costs: markets with homogeneous transaction costs are asymptotically strategyproof, and there is no efficiency-loss due to strategic behavior. In contrast, double auctions with heterogeneous transaction costs---such as spread fees---lead to complex strategic behavior (price guessing) and may result in severe market failures. Allowing for aggregate uncertainty, we extend these insights to market organizations other than Double Auctions."}}
{"id": "9D0OVNkgq6", "cdate": 1640995200000, "mdate": 1683295447657, "content": {"title": "Statistical Discrimination in Stable Matchings", "abstract": "Statistical discrimination results when a decision-maker observes an imperfect estimate of the quality of each candidate dependent on which demographic group they belong to [1,8]. Imperfect estimates have been modelled via noise, where the variance depends on the candidate's group ([4,6,7]). Prior literature, however, is limited to simple selection problems, where a single decision-maker tries to choose the best candidates among the applications they received. In this paper, we initiate the study of statistical discrimination in matching, where multiple decision-makers are simultaneously facing selection problems from the same pool of candidates. We consider the college admission problem as first introduced in [5] and recently extended to a model with a continuum of students [3]. We propose a model where two colleges A and B observe noisy estimates of each candidate's quality, where Ws, the vector of estimates for student s, is assumed to be a bivariate normal random variable. In this setting, the estimation noise controls a new key feature of the problem, namely correlation, \u03c1, between the estimates of the two colleges: if the noise is high, the correlation is low and if the noise is low the correlation is high. We assume that the population of students is divided into two groups G1 and G2, and that members of these two groups are subject to different correlation levels between their grades at colleges A and B. Concretely, for each student s, their grade vector (WAs, WBs) is drawn according to a centered bivariate normal distribution with variance 1 and covariance \u03c1Gs, where Gs is the group student s belongs to. We consider the stable matching induced by this distribution and characterize how key outcome characteristics vary with the parameters, in particular with the group-dependent correlation coefficient. Our results summarize as follows: We show that the probability that a student is assigned to their first choice is independent of the student's group, but that it decreases when the correlation of either group decreases. This means that higher measurement noise (inducing lower correlation) on one group hurts not only the students of that group, but the students of all groups. We show that the probability that a student is assigned to their second choice and the probability that they remain unassigned both depend on the student's group, which reveals the presence of statistical discrimination coming from the correlation effect alone. Specifically, we find that the probability that a student remains unmatched is decreasing when the correlation of their group decreases (higher measurement noise) and when the correlation of the other group increases. In other words, the higher the measurement noise of their own group, the better off students are with regard to getting assigned a college at all. This is somewhat counter-intuitive, but is explained by the observation that with high noise (i.e., low correlation) the fact that a student is rejected from one college gives only little information about the outcome at the other college. That is, a student has an independent second chance for admission. These two comparative static results give insights on the effect of correlation on the stable matching outcome for different demographic groups and show that indeed, statistical discrimination is an important theory to understand discrimination in matching problems. We also analyze a number of special cases of our model, in particular the case of a single group, to show that even in this case correlation affects the outcome. It is interesting to notice that the effect of correlation on the number of students getting their first choice in our model is the same as in [2], i.e., a higher correlation leads to more students getting their first choice. Our work is the first to investigate statistical discrimination in the context of matching. Overall we find that group-dependent measurement noises of the candidates quality---and the resulting group-dependent correlation between the colleges' estimates---play an important role in leading to unequal outcomes for different demographic groups, and in particular underrepresentation of one of the groups. Of course, we do not argue that statistical discrimination is the only possible cause of discrimination. In particular, if there is bias in the quality estimates for one group, then it will naturally also hurt the representation of that group. We do not model bias since our primary purpose is to isolate the effect of statistical discrimination. Throughout the paper, we make a number of other simplifying assumptions (e.g., focusing on two colleges) whose purpose is also to simplify our results and isolate the effect of correlation. Our analysis, however, extends to more general contexts."}}
{"id": "S_VpsFWBwy", "cdate": 1609459200000, "mdate": 1696013888396, "content": {"title": "The importance of memory for price discovery in decentralized markets", "abstract": ""}}
{"id": "ivqE53EVK8V", "cdate": 1577836800000, "mdate": 1631284154698, "content": {"title": "Quick or Cheap? Breaking Points in Dynamic Markets", "abstract": "We examine two-sided markets where players arrive stochastically over time and are drawn from a continuum of types. The cost of matching a client and provider varies, so a social planner is faced with two contending objectives: a) to reduce players' waiting time before getting matched; and b) to form efficient pairs in order to reduce matching costs. We show that such markets are characterized by a quick or cheap dilemma. Under a large class of distributional assumptions, there is no 'free lunch', i.e., there exists no clearing schedule that is approximately optimal in terms of both waiting time and matching cost. Building on the no free lunch result, we proceed to fill the spectrum between matching cost and waiting time minimization. We identify a unique breaking point signifying a stark reduction in matching cost contrasted by an increase in waiting time. To explore the finer aspects of this trade-off, we introduce a utility model for the social planner whereby the associated utility of matching cost is of the same order as the agents' utility of waiting time. Under this model, we show that there exists a non-trivial clearing schedule achieving this balance, and we show that this schedule is effectively unique (up to asymptotic order considerations). Finally, we generalize our key findings by studying different decay rates of matching costs (instead of focusing on one decay rate that results from the micro-founded match costs). We identify two regimes. One, where no free lunch continues to hold. The other, where the benefit from waiting is growing quickly enough, such that a window of opportunity opens and it is possible to get a free lunch. As before, in both regimes, greedy scheduling is generally sub-optimal. Compared to the existing literature [1,3-5] on the trade-off between waiting and mismatch (both in economics and computer science) our model introduces incomplete information about the distribution of past and future match costs and considers an infinite type space in a tractable model. As a consequence, the social planner tries to resolve the trade-off between matching optimally and waiting time in light of incomplete information. Incomplete information in our setting implies that the social planner must employ clearing schedules that do not take as input the relative strengths of current and future matches (since the latter is unknown), thus yielding qualitatively new results. In contrast to prior results for markets with one or two types of match costs where lack of information resulted in optimality of some form of greedy scheduling, we find that greedy clearing is generally not optimal in the presence of many types. Hence, the quick-versus-cheap trade-off is more intricate than previously found. Moreover, our results may actually also have consequences for applications that have been studied before too (e.g. kidney exchange) if other match value metrics (e.g. potential years of life lost or disability-adjusted life years) are used that would produce more than binary match values. By studying fully heterogeneous match costs we have to rely on different mathematical tools compared to previous analyses, which were often able to reduce the induced dynamics to discrete Markov processes. The key technical innovations of our paper concern the concurrent consideration of a continuum of types, independent arrivals, and incomplete information. In turn, these contributions rely on a range of previously unused tools from probability theory and disordered systems to obtain closed-form solutions. These underlying results are concerned with the expected matching cost for given instances of random, static assignment games. In particular, in static assignment games with the same number of clients and providers and exp(1) distributed edge weights, [2] proved the long-standing conjecture that the expected minimum weight matching converges to \u03c02/6 (i.e., as the number of players is growing). This result was later extended by [6] to assignment games with match costs drawn from non-identical exponential distributions. Building on this, we are able to compute the expected matching cost for every 'snapshot in time' of the dynamic clearing game. This provides strong foundations for our proofs which are then focused on estimating the fluctuations that result from the random arrival of clients and providers and their randomly drawn match costs. To achieve this, we use several approximation techniques (in particular, the approximation of the arrival process by a continuous-time Wiener process), which allow us to port over several results from martingale limit theory (such as the law of the iterated logarithm)."}}
{"id": "OadznqYvxN", "cdate": 1577836800000, "mdate": null, "content": {"title": "Quick or cheap? Breaking points in dynamic markets", "abstract": "We examine two-sided markets where players arrive stochastically over time and are drawn from a continuum of types. The cost of matching a client and provider varies, so a social planner is faced with two contending objectives: a) to reduce players' waiting time before getting matched; and b) to form efficient pairs in order to reduce matching costs. We show that such markets are characterized by a quick-or-cheap dilemma: Under a large class of distributional assumptions, there is no 'free lunch', i.e., there exists no clearing schedule that is simultaneously optimal along both objectives. We further identify a unique breaking point signifying a stark reduction in matching cost contrasted by an increase in waiting time. Generalizing this model, we identify two regimes: one, where no free lunch exists; the other, where a window of opportunity opens to achieve a free lunch. Remarkably, greedy scheduling is never optimal in this setting."}}
{"id": "EyalGtPNO4", "cdate": 1577836800000, "mdate": 1696013888206, "content": {"title": "Market sentiments and convergence dynamics in decentralized assignment economies", "abstract": "In two-sided markets with transferable utility (\u2018assignment games\u2019), we study the dynamics of trade arrangements and price adjustments as agents from the two market sides stochastically match, break up, and re-match in their pursuit of better opportunities. The underlying model of individual adjustments is based on the behavioral theories of adaptive learning and aspiration adjustment. Dynamics induced by this model converge to approximately optimal and stable market outcomes, but this convergence may be (exponentially) slow. We introduce the notion of a \u2018market sentiment\u2019 that governs which of the two market sides is temporarily more or less amenable to price adjustments, and show that such a feature may significantly speed up convergence."}}
