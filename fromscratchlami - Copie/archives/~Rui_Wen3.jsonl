{"id": "Lrxaf7IPVT", "cdate": 1663850323232, "mdate": null, "content": {"title": "On the Importance of Diversity in Data-free Model Stealing", "abstract": "Machine learning as a Service (MLaaS) allows users to query the machine learning model in an API manner, which provides an opportunity for users to enjoy the benefits brought by the high-performance model trained on valuable data. This interface boosts the flourish of machine learning based applications, while on the other hand, introduces the attack surface for model stealing attacks. Existing model stealing attacks have relaxed their attack assumptions to the data-free setting, while keeping the effectiveness. However, these methods are complex and consist of several components, which obscure the core on which the attack really depends. In this paper, we revisit the model stealing problem from a diversity perspective and demonstrate that keeping the generated data samples more diverse across all the classes is the critical point for improving the attack performance. Based on this conjecture, we provide a simplified attack framework. We empirically signify our conjecture by evaluating the effectiveness of our attack, and experimental results show that our approach is able to achieve comparable or even better performance compared with the state-of-the-art method. Furthermore, benefiting from the absence of redundant components, our method demonstrates its advantages in attack efficiency and query budget."}}
{"id": "v-rx235RlfI", "cdate": 1663850312705, "mdate": null, "content": {"title": "Model Stealing Attacks Against Vision-Language Models", "abstract": "Vision-language models have flourished these years and are regarded as promising solutions to vision-language tasks. However, training vision-language models always requires enormous effort, making the models valuable intellectual properties (IPs). In this paper, we pioneer to propose the first model stealing attack against the vision-language models, the goal of which is to steal the functionality of the target models. Specifically, we target fine-tuned CLIP models with black-box access. We query the model to extract model information through either the text-to-image retrieval or the image-to-text retrieval API and then leverage the information to train a local copy of the target model. Experiments show the effectiveness of the model stealing attacks. We validate that our attacks are query-efficient, API-agnostic, data-agnostic, and architecture-agnostic, which broaden the attack scenarios. As a counterpart, we examine a defense based on the idea of out-of-distribution detection, which is impotent without strong assumptions. Our research pressures the unprotected release and prevalence of powerful vision-language models and appeals to the community that their IP protections, if not the least, cannot be less."}}
{"id": "zKvm1ETDOq", "cdate": 1663850170452, "mdate": null, "content": {"title": "Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?", "abstract": "Indiscriminate data poisoning can decrease the clean test accuracy of a deep learning model by slightly perturbing its training samples.\nThere is a consensus that such poisons can hardly harm adversarially-trained (AT) models when the adversarial training budget is no less than the poison budget, i.e., $\\epsilon_\\mathrm{adv}\\geq\\epsilon_\\mathrm{poi}$. This consensus, however, is challenged in this paper based on our new attack strategy that induces \\textit{entangled features} (EntF). The existence of entangled features makes the poisoned data become less useful for training a model, no matter if AT is applied or not. We demonstrate that for attacking a CIFAR-10 AT model under a reasonable setting with $\\epsilon_\\mathrm{adv}=\\epsilon_\\mathrm{poi}=8/255$, our EntF yields an accuracy drop of $13.31\\%$, which is $7\\times$ better than existing methods and equal to discarding $83\\%$ training data. We further show the generalizability of EntF to more challenging settings, e.g., higher AT budgets, partial poisoning, unseen model architectures, and stronger (ensemble or adaptive) defenses. We finally provide new insights into the distinct roles of non-robust vs. robust features in poisoning standard vs. AT models and demonstrate the possibility of using a hybrid attack to poison standard and AT models simultaneously. Our code is available at~\\url{https://github.com/WenRuiUSTC/EntF}."}}
{"id": "Bgcp4BniE-U", "cdate": 1663850126173, "mdate": null, "content": {"title": "Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models", "abstract": "Generative models have shown their promising performance on various real-world tasks, which, at the same time, introduce the threat of leaking private information of their training data. Several membership inference attacks against generative models have been proposed in recent years and exhibit their effectiveness in different settings. However, these attacks all suffer from their own limitations and cannot be generalized to all generative models under all scenarios. In this paper, we propose the first generalized membership inference attack for generative models, which can be utilized to quantitatively evaluate the privacy leakage of various existing generative models. Compared with previous works, our attack has three main advantages, i.e., (i) only requires black-box access to the target model, (ii) is computationally efficient, and (iii) can be generalized to numerous generative models. Extensive experiments show that various existing generative models in a variety of applications are vulnerable to our attack. For example, our attack could achieve the AUC of 0.997 (0.997) and 0.998 (0.999) against the generative model of DDPM (DDIM) on the CelebA and CIFAR-10 datasets. These results demonstrate that private information can be effectively exploited by attackers in an efficient way, which calls on the community to be aware of privacy threats when designing generative models."}}
{"id": "XKHNu4OF6wn", "cdate": 1663849982292, "mdate": null, "content": {"title": "Node-Level Membership Inference Attacks Against Graph Neural Networks", "abstract": "Many real-world data are graphs, such as social networks and protein structures. To fully utilize the information contained in graph data, graph neural networks (GNNs) have been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose eight node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on four GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that node degree, graph density, and feature similarity have major impacts on the attack's success. We further investigate three defense mechanisms and show that differential privacy (DP) can better protect the membership privacy while preserving the model's utility."}}
{"id": "RhW4qjr4F4f", "cdate": 1640995200000, "mdate": 1680413886369, "content": {"title": "Dynamic Backdoor Attacks Against Machine Learning Models", "abstract": ""}}
{"id": "E-vg31lCSv", "cdate": 1640995200000, "mdate": 1668590066160, "content": {"title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models", "abstract": ""}}
{"id": "ZXJ3oBarTW", "cdate": 1609459200000, "mdate": 1668590066842, "content": {"title": "Node-Level Membership Inference Attacks Against Graph Neural Networks", "abstract": "Many real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss."}}
{"id": "NttCWEiyAbz", "cdate": 1577836800000, "mdate": 1680413886363, "content": {"title": "LEAF: A Faster Secure Search Algorithm via Localization, Extraction, and Reconstruction", "abstract": ""}}
