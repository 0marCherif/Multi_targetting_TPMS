{"id": "jQF3it4syG", "cdate": 1675970198052, "mdate": null, "content": {"title": "Learning to Initiate and Reason in Event-Driven Cascading Processes", "abstract": "Training agents to control a dynamic environment is a fundamental task in AI. In many environments, the dynamics can be summarized by a small set of events that capture the semantic behavior of the system. Typically, these events form chains or cascades. We often wish to change the system behavior using a single intervention that propagates through the cascade. For instance, one may trigger a biochemical cascade to switch the state of a cell.\n\nWe introduce a new learning setup called Cascade. An agent observes a system with known dynamics evolving from some initial state. The agent is given a structured semantic instruction and needs to make an intervention that triggers a cascade of events, such that the system reaches an alternative (counterfactual) behavior. We provide a test bed for this problem, consisting of physical objects. %This problem is hard because the cascades make search space highly fragmented and discontinuous.  \n\nWe devise an algorithm that learns to efficiently search in exponentially large semantic trees. We demonstrate that our approach learns to follow instructions to intervene in new complex scenes."}}
{"id": "hNmf1gnVllX", "cdate": 1663850120092, "mdate": null, "content": {"title": "Learning to Reason and Act in Cascading Processes", "abstract": "Training agents to control a dynamic environment is a fundamental task in AI. In many environments the dynamics can be summarized by a small set of events that capture the semantic behavior of the system. Typically, these events form chains or cascades. We often wish to change the system behavior using a single intervention that propagates through the cascade. For instance, one may trigger a biochemical cascade to switch the state of a cell, or reroute a truck in logistic chains to meet an unexpected, urgent  delivery.\nWe introduce a new supervised learning setup called \"Cascade\". An agent observes a system with a known dynamics evolving from some initial state. It is given a structured semantic instruction and needs to make an intervention that triggers a cascade of events, such that the system reaches an alternative (counterfactual) behavior. We provide a test-bed for this problem, consisting of physical objects.\nWe combine semantic tree search with an event-driven forward model and devise an algorithm that learns to efficiently search in exponentially large semantic trees of continuous spaces. We demonstrate that our approach learns to effectively follow instructions to intervene in previously unseen complex scenes. When provided an observed cascade of events, it can also reason about alternative outcomes."}}
{"id": "NAQvF08TcyG", "cdate": 1663849835866, "mdate": null, "content": {"title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "abstract": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.\nIn other words, we ask: how can we use language-guided models to turn *our* cat into a painting, or imagine a new product based on *our* favorite toy? \nHere we present a simple approach that allows such creative freedom. Using only $3$-$5$ images of a user-provided concept, like an object or a style, we learn to represent it through new ``words\" in the embedding space of a frozen text-to-image model.\nThese ``words\" can be composed into natural language sentences, guiding *personalized* creation in an intuitive way.\nNotably, we find evidence that a *single* word embedding is sufficient for capturing unique and varied concepts. \nWe compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available."}}
{"id": "rIQgLHdUcec", "cdate": 1646057533618, "mdate": null, "content": {"title": "Learning to reason about and to act on physical cascading events", "abstract": "Reasoning and interacting with dynamic environments is a fundamental problem in AI, but it becomes extremely challenging when actions can trigger cascades of cross-dependant events.\nWe introduce a new learning setup called Cascade where an agent is shown a video of a simulated physical dynamic scene, and is asked to intervene and trigger a cascade of events, such that the system reaches a \"counterfactual\" goal. For instance, the agent may be asked to \u201cMake the blue ball hit the red one, by pushing the green ball\u201d. The problem is very challenging because agent interventions are from a continuous space, and cascades of events make the dynamics highly non-linear.\n\nWe combine semantic tree search with an event-driven forward model and devise an algorithm that learns to search in semantic trees in continuous spaces. We demonstrate that our approach learns to effectively follow instructions to intervene in previously unseen complex scenes. Interestingly, it can use the observed cascade of events to reason about alternative counterfactual outcomes."}}
{"id": "FWHFL5qQce", "cdate": 1640995200000, "mdate": 1668431058426, "content": {"title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "abstract": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new \"words\" in the embedding space of a frozen text-to-image model. These \"words\" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.   Our code, data and new words will be available at: https://textual-inversion.github.io"}}
{"id": "7Gi9g7_Fue", "cdate": 1640995200000, "mdate": 1668431058403, "content": {"title": "\"This Is My Unicorn, Fluffy\": Personalizing Frozen Vision-Language Representations", "abstract": "Large Vision & Language models pretrained on web-scale data provide representations that are invaluable for numerous V &L problems. However, it is unclear how they can be extended to reason about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision & Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific (\u201cpersonalized\u201d) concepts \u201cin the wild\u201d. In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) without providing personalized negative examples. We propose an architecture for solving PerVL that operates by expanding the input vocabulary of a pretrained model with new word embeddings for the personalized concepts. The model can then simply employ them as part of a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and effectively applies them in image retrieval and semantic segmentation using rich textual queries. For example the model improves MRR by 51.1% (28.4% vs 18.8%) compared to the strongest baseline. The code and benchmark are available on github under NVlabs/PALAVRA ( https://github.com/NVlabs/PALAVRA ) and NVlabs/PerVLBenchmark ( https://github.com/NVlabs/PerVLBenchmark )."}}
{"id": "SclyMsq4-C", "cdate": 1609459200000, "mdate": 1668431058506, "content": {"title": "Known unknowns: Learning novel concepts using reasoning-by-elimination", "abstract": "People can learn new visual concepts without any samples, from information given by language or by deductive reasoning. For instance, people can use <em>elimination</em> to infer the meaning of nov..."}}
{"id": "50Gwp74d0J", "cdate": 1609459200000, "mdate": 1668431058304, "content": {"title": "From generalized zero-shot learning to long-tail with class descriptors", "abstract": "Real-world data is predominantly unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. Often, classes can be accompanied by side information like textual descriptions, but it is not fully clear how to use them for learning with unbalanced long-tail data. Such descriptions have been mostly used in (Generalized) Zero-shot learning (ZSL), suggesting that ZSL with class descriptions may also be useful for long- tail distributions.We describe Dragon, a late-fusion architecture for long-tail learning with class descriptors. It learns to (1) correct the bias towards head classes on a sample- by-sample basis; and (2) fuse information from class- descriptions to improve the tail-class accuracy. We also introduce new benchmarks CUB-LT, SUN-LT, AWA-LT for long-tail learning with class-descriptions, building on existing learning-with-attributes datasets and a version of Imagenet-LT with class descriptors. Dragon outperforms state-of-the-art models on the new benchmark. It is also a new SoTA on existing benchmarks for GFSL with class descriptors (GFSL-d) and standard (vision-only) long-tailed learning ImageNet-LT, CIFAR-10, 100, and Places365-LT."}}
{"id": "ZIclKS5kbxH", "cdate": 1577836800000, "mdate": 1668431058328, "content": {"title": "ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity and Visual Summarization", "abstract": ""}}
{"id": "33o4PfIReir", "cdate": 1577836800000, "mdate": null, "content": {"title": "A causal view of compositional zero-shot recognition", "abstract": "People easily recognize new visual categories that are new combinations of known components. This compositional generalization capacity is critical for learning in real-world domains like vision and language because the long tail of new combinations dominates the distribution. Unfortunately, learning systems struggle with compositional generalization because they often build on features that are correlated with class labels even if they are not \"essential\" for the class. This leads to consistent misclassification of samples from a new distribution, like new combinations of known components. Here we describe an approach for compositional generalization that builds on causal ideas. First, we describe compositional zero-shot learning from a causal perspective, and propose to view zero-shot inference as finding \"which intervention caused the image?\". Second, we present a causal-inspired embedding model that learns disentangled representations of elementary components of visual objects from correlated (confounded) training data. We evaluate this approach on two datasets for predicting new combinations of attribute-object pairs: A well-controlled synthesized images dataset and a real world dataset which consists of fine-grained types of shoes. We show improvements compared to strong baselines."}}
