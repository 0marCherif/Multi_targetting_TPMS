{"id": "ujl2B0Xmus", "cdate": 1672531200000, "mdate": 1681541312020, "content": {"title": "MANSA: Learning Fast and Slow in Multi-Agent Systems", "abstract": ""}}
{"id": "idzQGeVJyz5", "cdate": 1663850320393, "mdate": null, "content": {"title": "MANSA: LEARNING FAST AND SLOW IN MULTI-AGENT SYSTEMS WITH A GLOBAL SWITCHING AGENT", "abstract": "In multi-agent systems, independent learners (IL) often show remarkable performance and easily scale with the number of agents. Yet, training IL can sometimes be inefficient particularly in states that require coordinated exploration. Using observations of other agents\u2019 actions through centralised learning (CL) enables agents to quickly learn how to coordinate their behaviour but employing CL at all states is prohibitively expensive in many real-world applications. Besides, applying CL often needs strong representational constraints (such as individual-global-max condition) that can lead to poor performance if violated. In this paper, we introduce a novel IL framework named MANSA that selectively employs CL only at states that require coordination. Central to MANSA is the additional reinforcement learning (RL) agent that uses switching controls to quickly learn when and where to activate CL so as to boost the performance of IL while using only IL everywhere else. Our theory proves that MANSA\u2019s switching control mechanism, which can seamlessly adopt any existing multi-agent RL (MARL) algorithms, preserves MARL convergence properties in cooperative settings. Importantly, we prove that MANSA can improve performance and maximise performance given a limited budget of CL calls. We show empirically in Level-based Foraging and SMAC settings that MANSA achieves fast, superior training performance through its minimal selective use of CL. "}}
{"id": "PTrcWS4aRnH", "cdate": 1652963729132, "mdate": 1652963729132, "content": {"title": "SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation", "abstract": "Satisfying safety constraints almost surely (or with probability one) can be critical for deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows to view Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be \"sauteed\". Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance."}}
{"id": "nljJ_I8yJr6", "cdate": 1640995200000, "mdate": 1682339004552, "content": {"title": "Saute RL: Almost Surely Safe Reinforcement Learning Using State Augmentation", "abstract": ""}}
{"id": "dcotfmKeA4", "cdate": 1640995200000, "mdate": 1681541312091, "content": {"title": "LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning", "abstract": ""}}
{"id": "Hzj1gXbVLX", "cdate": 1640995200000, "mdate": 1684305820661, "content": {"title": "SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation", "abstract": "Satisfying safety constraints almost surely (or with probability one) can be critical for the deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows viewing the Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be \"Sauteed\". Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance."}}
{"id": "DUhG4KHMFb", "cdate": 1640995200000, "mdate": 1682339004738, "content": {"title": "Reinforcement Learning in Presence of Discrete Markovian Context Evolution", "abstract": ""}}
{"id": "ACFwJ1St4P", "cdate": 1640995200000, "mdate": 1682339004552, "content": {"title": "Reinforcement Learning in Presence of Discrete Markovian Context Evolution", "abstract": ""}}
{"id": "-DRdxKSqhtl", "cdate": 1640995200000, "mdate": 1682325686874, "content": {"title": "Semi-Centralised Multi-Agent Reinforcement Learning with Policy-Embedded Training", "abstract": "Centralised training with decentralised execution (CT-DE) serves as the foundation of many leading multi-agent reinforcement learning (MARL) algorithms. Despite its popularity, it suffers from a critical drawback due to its reliance on learning from a single sample of the joint-action at a given state. As agents explore and update their policies during training, these single samples may poorly represent the actual joint-policy of the system of agents leading to high variance gradient estimates that hinder learning. To address this problem, we propose an enhancement tool that accommodates any actor-critic MARL method. Our framework, Performance Enhancing Reinforcement Learning Apparatus (PERLA), introduces a sampling technique of the agents' joint-policy into the critics while the agents train. This leads to TD updates that closely approximate the true expected value under the current joint-policy rather than estimates from a single sample of the joint-action at a given state. This produces low variance and precise estimates of expected returns, minimising the variance in the critic estimators which typically hinders learning. Moreover, as we demonstrate, by eliminating much of the critic variance from the single sampling of the joint policy, PERLA enables CT-DE methods to scale more efficiently with the number of agents. Theoretically, we prove that PERLA reduces variance in value estimates similar to that of decentralised training while maintaining the benefits of centralised training. Empirically, we demonstrate PERLA's superior performance and ability to reduce estimator variance in a range of benchmarks including Multi-agent Mujoco, and StarCraft II Multi-agent Challenge."}}
{"id": "74cDdRwm4NV", "cdate": 1632875736757, "mdate": null, "content": {"title": "Learning to Shape Rewards using a Game of Two Partners", "abstract": "Reward shaping (RS) is a powerful method in reinforcement learning (RL) for  overcoming the problem of sparse or uninformative rewards. However, RS typically  relies on manually engineered shaping-reward functions whose construction is time consuming and error-prone. It also requires domain knowledge which runs contrary  to the goal of autonomous learning. We introduce Reinforcement Learning Optimal  Shaping Algorithm (ROSA), an automated RS framework in which the shaping reward function is constructed in a novel Markov game between two agents. A  reward-shaping agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. We prove that ROSA, which easily adopts existing RL algorithms, learns to construct a shaping reward function that is tailored to the task thus ensuring efficient convergence to high performance policies. We demonstrate ROSA\u2019s congenial properties in three carefully designed experiments and show its superior performance against state-of-the-art RS algorithms in challenging sparse reward environments."}}
