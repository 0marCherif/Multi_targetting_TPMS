{"id": "c0GVdLnaNyl", "cdate": 1672531200000, "mdate": 1681656649084, "content": {"title": "Are Gaussian data all you need? Extents and limits of universality in high-dimensional generalized linear estimation", "abstract": "In this manuscript we consider the problem of generalized linear estimation on Gaussian mixture data with labels given by a single-index model. Our first result is a sharp asymptotic expression for the test and training errors in the high-dimensional regime. Motivated by the recent stream of results on the Gaussian universality of the test and training errors in generalized linear estimation, we ask ourselves the question: \"when is a single Gaussian enough to characterize the error?\". Our formula allow us to give sharp answers to this question, both in the positive and negative directions. More precisely, we show that the sufficient conditions for Gaussian universality (or lack of thereof) crucially depend on the alignment between the target weights and the means and covariances of the mixture clusters, which we precisely quantify. In the particular case of least-squares interpolation, we prove a strong universality property of the training error, and show it follows a simple, closed-form expression. Finally, we apply our results to real datasets, clarifying some recent discussion in the literature about Gaussian universality of the errors in this context."}}
{"id": "Ki8FN3VwxFS", "cdate": 1672531200000, "mdate": 1681656649080, "content": {"title": "From high-dimensional & mean-field dynamics to dimensionless ODEs: A unifying approach to SGD in two-layers networks", "abstract": "This manuscript investigates the one-pass stochastic gradient descent (SGD) dynamics of a two-layer neural network trained on Gaussian data and labels generated by a similar, though not necessarily identical, target function. We rigorously analyse the limiting dynamics via a deterministic and low-dimensional description in terms of the sufficient statistics for the population risk. Our unifying analysis bridges different regimes of interest, such as the classical gradient-flow regime of vanishing learning rate, the high-dimensional regime of large input dimension, and the overparameterised \"mean-field\" regime of large network width, covering as well the intermediate regimes where the limiting dynamics is determined by the interplay between these behaviours. In particular, in the high-dimensional limit, the infinite-width dynamics is found to remain close to a low-dimensional subspace spanned by the target principal directions. Our results therefore provide a unifying picture of the limiting SGD dynamics with synthetic data."}}
{"id": "GL-3WEdNRM", "cdate": 1652737568922, "mdate": null, "content": {"title": "Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks", "abstract": "Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad \\& Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates."}}
{"id": "s9rqvbQi6Rp", "cdate": 1640995200000, "mdate": 1681656649077, "content": {"title": "Gaussian Universality of Linear Classifiers with Random Labels in High-Dimension", "abstract": "While classical in many theoretical settings - and in particular in statistical physics-inspired works - the assumption of Gaussian i.i.d. input data is often perceived as a strong limitation in the context of statistics and machine learning. In this study, we redeem this line of work in the case of generalized linear classification, a.k.a. the perceptron model, with random labels. We argue that there is a large universality class of high-dimensional input data for which we obtain the same minimum training loss as for Gaussian data with corresponding data covariance. In the limit of vanishing regularization, we further demonstrate that the training loss is independent of the data covariance. On the theoretical side, we prove this universality for an arbitrary mixture of homogeneous Gaussian clouds. Empirically, we show that the universality holds also for a broad range of real datasets."}}
{"id": "alJM7e1WabD", "cdate": 1640995200000, "mdate": 1652702007873, "content": {"title": "Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks", "abstract": "Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad & Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates."}}
{"id": "HX078lCUiQ", "cdate": 1640995200000, "mdate": 1681656649080, "content": {"title": "Sparse random hypergraphs: Non-backtracking spectra and community detection", "abstract": "We consider the community detection problem in a sparse q-uniform hypergraph G, assuming that G is generated according to the Hypergraph Stochastic Block Model (HSBM). We prove that a spectral method based on the non-backtracking operator for hypergraphs works with high probability down to the generalized Kesten-Stigum detection threshold conjectured by Angelini et al. (2015). We characterize the spectrum of the non-backtracking operator for the sparse HSBM and provide an efficient dimension reduction procedure using the Ihara-Bass formula for hypergraphs. As a result, community detection for the sparse HSBM on n vertices can be reduced to an eigenvector problem of a 2n\u00d72n non-normal matrix constructed from the adjacency matrix and the degree matrix of the hypergraph. To the best of our knowledge, this is the first provable and efficient spectral algorithm that achieves the conjectured threshold for HSBMs with r blocks generated according to a general symmetric probability tensor."}}
{"id": "k44i_nWMFwn", "cdate": 1620326888990, "mdate": null, "content": {"title": "Non-backtracking spectra of weighted inhomogeneous random graphs", "abstract": " We study a model of random graphs where each edge is drawn independently (but not necessarily identically distributed) from the others, and then assigned a random weight. When the mean degree of such a graph is low, it is known that the spectrum of the adjacency matrix A deviates significantly from that of its expected value EA. In contrast, we show that over a wide range of parameters the top eigenvalues of the non-backtracking matrix B -- a matrix whose powers count the non-backtracking walks between two edges -- are close to those of EA, and all other eigenvalues are confined in a bulk with known radius. We also obtain a precise characterization of the scalar product between the eigenvectors of B and their deterministic counterparts derived from the model parameters. This result has many applications, in domains ranging from (noisy) matrix completion to community detection, as well as matrix perturbation theory. In particular, we establish as a corollary that a result known as the Baik-Ben Arous-P\u00e9ch\u00e9 phase transition, previously established only for rotationally invariant random matrices, holds more generally for matrices A as above under a mild concentration hypothesis."}}
{"id": "kXKh51L3SJ5", "cdate": 1609459200000, "mdate": null, "content": {"title": "A simpler spectral approach for clustering in directed networks", "abstract": "We study the task of clustering in directed networks. We show that using the eigenvalue/eigenvector decomposition of the adjacency matrix is simpler than all common methods which are based on a combination of data regularization and SVD truncation, and works well down to the very sparse regime where the edge density has constant order. Our analysis is based on a Master Theorem describing sharp asymptotics for isolated eigenvalues/eigenvectors of sparse, non-symmetric matrices with independent entries. We also describe the limiting distribution of the entries of these eigenvectors; in the task of digraph clustering with spectral embeddings, we provide numerical evidence for the superiority of Gaussian Mixture clustering over the widely used k-means algorithm."}}
{"id": "zu-VM1bSTMh", "cdate": 1546300800000, "mdate": 1652702007870, "content": {"title": "Robustness of Spectral Methods for Community Detection", "abstract": "The present work is concerned with community detection. Specifically, we consider a random graph drawn according to the stochastic block model: its vertex set is partitioned into blocks, or communi..."}}
{"id": "I60LB7KJamt", "cdate": 1546300800000, "mdate": 1652702007855, "content": {"title": "Planting trees in graphs, and finding them back", "abstract": "In this paper we study the two inference problems of detection and reconstruction in the context of planted structures in sparse Erd\u0151s-R\u00e9nyi random graphs $\\mathcal G(n,\\lambda/n)$ with fixed avera..."}}
