{"id": "BJbzHGMd-r", "cdate": 1325376000000, "mdate": null, "content": {"title": "Exploring Topic Coherence over Many Models and Many Topics", "abstract": "Keith Stevens, Philip Kegelmeyer, David Andrzejewski, David Buttler. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. 2012."}}
{"id": "rJZNyEM_bH", "cdate": 1293840000000, "mdate": null, "content": {"title": "A Framework for Incorporating General Domain Knowledge into Latent Dirichlet Allocation Using First-Order Logic", "abstract": "Topic models have been used successfully for a variety of problems, often in the form of application-specific extensions of the basic Latent Dirichlet Allocation (LDA) model. Because deriving these new models in order to encode domain knowledge can be difficult and time-consuming, we propose the Fold\u010ball model, which allows the user to specify general domain knowledge in First-Order Logic (FOL). However, combining topic modeling with FOL can result in inference problems beyond the capabilities of existing techniques. We have therefore developed a scalable inference technique using stochastic gradient descent which may also be useful to the Markov Logic Network (MLN) research community. Experiments demonstrate the expressive power of Fold\u010ball, as well as the scalability of our proposed inference method."}}
{"id": "B1Z_y4Z_ZB", "cdate": 1293840000000, "mdate": null, "content": {"title": "Latent topic feedback for information retrieval", "abstract": "We consider the problem of a user navigating an unfamiliar corpus of text documents where document metadata is limited or unavailable, the domain is specialized, and the user base is small. These challenging conditions may hold, for example, within an organization such as a business or government agency. We propose to augment standard keyword search with user feedback on latent topics. These topics are automatically learned from the corpus in an unsupervised manner and presented alongside search results. User feedback is then used to reformulate the original query, resulting in improved information retrieval performance in our experiments."}}
{"id": "r1b8Y7-OWB", "cdate": 1230768000000, "mdate": null, "content": {"title": "May All Your Wishes Come True: A Study of Wishes and How to Recognize Them", "abstract": "Andrew B. Goldberg, Nathanael Fillmore, David Andrzejewski, Zhiting Xu, Bryan Gibson, Xiaojin Zhu. Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. 2009."}}
{"id": "HkWbr3Z_bB", "cdate": 1230768000000, "mdate": null, "content": {"title": "Incorporating domain knowledge into topic modeling via Dirichlet Forest priors", "abstract": "Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation framework. The prior is a mixture of Dirichlet tree distributions with special structures. We present its construction, and inference via collapsed Gibbs sampling. Experiments on synthetic and real datasets demonstrate our model's ability to follow and generalize beyond user-specified domain knowledge."}}
{"id": "BkbgF7W_bB", "cdate": 1167609600000, "mdate": null, "content": {"title": "Improving Diversity in Ranking using Absorbing Random Walks", "abstract": "Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, David Andrzejewski. Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference. 2007."}}
