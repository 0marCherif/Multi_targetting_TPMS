{"id": "1dpjkNpO4I", "cdate": 1640995200000, "mdate": 1666350446474, "content": {"title": "Leveraging Road Area Semantic Segmentation with Auxiliary Steering Task", "abstract": "Robustness of different pattern recognition methods is one of the key challenges in autonomous driving, especially when driving in the high variety of road environments and weather conditions, such as gravel roads and snowfall. Although one can collect data from these adverse conditions using cars equipped with sensors, it is quite tedious to annotate the data for training. In this work, we address this limitation and propose a CNN-based method that can leverage the steering wheel angle information to improve the road area semantic segmentation. As the steering wheel angle data can be easily acquired with the associated images, one could improve the accuracy of road area semantic segmentation by collecting data in new road environments without manual data annotation. We demonstrate the effectiveness of the proposed approach on two challenging data sets for autonomous driving and show that when the steering task is used in our segmentation model training, it leads to a 0.1\u20132.9% gain in the road area mIoU (mean Intersection over Union) compared to the corresponding reference transfer learning model."}}
{"id": "uF_Wl0xSA7O", "cdate": 1632875615563, "mdate": null, "content": {"title": "Independent Component Alignment for Multi-task Learning", "abstract": "We present a novel gradient-based multi-task learning (MTL) approach that balances training in multi-task systems by aligning the independent components of the training objective. In contrast to state-of-the-art MTL approaches, our method is stable and preserves the ratio of highly correlated tasks gradients. The method is scalable, reduces overfitting, and can seamlessly handle multi-task objectives with a large difference in gradient magnitudes. We demonstrate the effectiveness of the proposed approach on a variety of MTL problems including digit classification, multi-label image classification, camera relocalization, and scene understanding. Our approach performs favourably compared to other gradient-based adaptive balancing methods, and its performance is backed up by theoretical analysis."}}
{"id": "b0Wh69QT5T", "cdate": 1609459200000, "mdate": 1666350446473, "content": {"title": "Continual Learning for Image-Based Camera Localization", "abstract": "For several emerging technologies such as augmented reality, autonomous driving and robotics, visual localization is a critical component. Directly regressing camera pose/3D scene coordinates from the input image using deep neural networks has shown great potential. However, such methods assume a stationary data distribution with all scenes simultaneously available during training. In this paper, we approach the problem of visual localization in a continual learning setup \u2013 whereby the model is trained on scenes in an incremental manner. Our results show that similar to the classification domain, non-stationary data induces catastrophic forgetting in deep networks for visual localization. To address this issue, a strong baseline based on storing and replaying images from a fixed buffer is proposed. Furthermore, we propose a new sampling method based on coverage score (Buff-CS) that adapts the existing sampling strategies in the buffering process to the problem of visual localization. Results demonstrate consistent improvements over standard buffering methods on two challenging datasets \u2013 7Scenes, 12Scenes, and also 19Scenes by combining the former scenes <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> ."}}
{"id": "W2z4BcZGt2d", "cdate": 1609459200000, "mdate": 1632836559765, "content": {"title": "Continual Learning for Image-Based Camera Localization", "abstract": "For several emerging technologies such as augmented reality, autonomous driving and robotics, visual localization is a critical component. Directly regressing camera pose/3D scene coordinates from the input image using deep neural networks has shown great potential. However, such methods assume a stationary data distribution with all scenes simultaneously available during training. In this paper, we approach the problem of visual localization in a continual learning setup -- whereby the model is trained on scenes in an incremental manner. Our results show that similar to the classification domain, non-stationary data induces catastrophic forgetting in deep networks for visual localization. To address this issue, a strong baseline based on storing and replaying images from a fixed buffer is proposed. Furthermore, we propose a new sampling method based on coverage score (Buff-CS) that adapts the existing sampling strategies in the buffering process to the problem of visual localization. Results demonstrate consistent improvements over standard buffering methods on two challenging datasets -- 7Scenes, 12Scenes, and also 19Scenes by combining the former scenes."}}
{"id": "UYPAa1YuPY", "cdate": 1609459200000, "mdate": 1666350446474, "content": {"title": "Digging Into Self-Supervised Learning of Feature Descriptors", "abstract": "Fully-supervised CNN-based approaches for learning local image descriptors have shown remarkable results in a wide range of geometric tasks. However, most of them require per-pixel ground-truth keypoint correspondence data which is difficult to acquire at scale. To address this challenge, recent weakly-and self-supervised methods can learn feature descriptors from relative camera poses or using only synthetic rigid transformations such as homographies. In this work, we focus on understanding the limitations of existing self-supervised approaches and propose a set of improvements that combined lead to powerful feature descriptors. We show that increasing the search space from in-pair to in-batch for hard negative mining brings consistent improvement. To enhance the discriminativeness of feature descriptors, we propose a coarse-to-fine method for mining local hard negatives from a wider search space by using global visual image descriptors. We demonstrate that a combination of synthetic homography transformation, color augmentation, and photorealistic image stylization produces useful representations that are viewpoint and illumination invariant. The feature descriptors learned by the proposed approach perform competitively and surpass their fully- and weakly-supervised counterparts on various geometric benchmarks such as image-based localization, sparse feature matching, and image retrieval."}}
{"id": "d_Far8Sab", "cdate": 1577836800000, "mdate": 1632836560053, "content": {"title": "Geometric Image Correspondence Verification by Dense Pixel Matching", "abstract": "This paper addresses the problem of determining dense pixel correspondences between two images and its application to geometric correspondence verification in image retrieval. The main contribution is a geometric correspondence verification approach for re-ranking a shortlist of retrieved database images based on their dense pair-wise matching with the query image at a pixel level. We determine a set of cyclically consistent dense pixel matches between the pair of images and evaluate local similarity of matched pixels using neural network based image descriptors. Final re-ranking is based on a novel similarity function, which fuses the local similarity metric with a global similarity metric and a geometric consistency measure computed for the matched pixels. For dense matching our approach utilizes a modified version of a recently proposed dense geometric correspondence network (DGC-Net), which we also improve by optimizing the architecture. The proposed model and similarity metric compare favourably to the state-of-the-art image retrieval methods. In addition, we apply our method to the problem of long-term visual localization demonstrating promising results and generalization across datasets."}}
{"id": "_HT8r2_iC3I", "cdate": 1577836800000, "mdate": 1632836559971, "content": {"title": "Deep Learning Methods for Image Matching and Camera Relocalization", "abstract": "Deep learning and convolutional neural networks have revolutionized computer vision and become a dominant tool in many applications, such as\u00a0image classification, semantic segmentation, object recognition, and image\u00a0retrieval. Their strength lies in the ability to learn an efficient representation of images that makes a subsequent learning task easier. This thesis\u00a0presents deep learning approaches for a number of fundamental computer\u00a0vision problems that are closely related to each other; image matching,\u00a0image-based localization, ego-motion estimation, and scene understanding.\u00a0 In image matching, the thesis studies two methods utilizing a Siamese\u00a0network architecture for learning both patch-level and image-level descriptors for measuring similarity using Euclidean distance. Next, it introduces\u00a0a coarse-to-fine CNN-based approach for dense pixel correspondence estimation that can leverage the advantages of optical flow methods and\u00a0extend them to the case of wide baseline between two images. The method\u00a0demonstrates good generalization performance and it is applicable for image matching as well as for image alignment and relative camera pose\u00a0estimation.\u00a0 One of the contributions of the thesis is a novel approach for recovering\u00a0the absolute camera pose from ego-motion. In contrast to the existing CNN-based localization algorithms, the proposed method can be directly applied\u00a0to scenes which are not available at training stage and it does not require\u00a0scene-specific training of the network, thus, improving the scalability. The\u00a0thesis also shows that Siamese architecture can be successfully utilized in\u00a0the problem of relative camera pose estimation achieving better performance\u00a0in challenging scenarios compared to traditional image descriptors.\u00a0 Lastly, the thesis demonstrates how the advances of visual geometry can\u00a0help to efficiently learn depth, camera ego-motion, and optical flow for\u00a0the task of scene understanding. More specifically, it introduces a method\u00a0that can leverage temporally consistent geometric priors between frames\u00a0of monocular video sequences and jointly estimate ego-motion and depth\u00a0maps in a self-supervised manner."}}
{"id": "TAU9cEb4Y2l", "cdate": 1577836800000, "mdate": 1632836560080, "content": {"title": "Image Stylization for Robust Features", "abstract": "Local features that are robust to both viewpoint and appearance changes are crucial for many computer vision tasks. In this work we investigate if photorealistic image stylization improves robustness of local features to not only day-night, but also weather and season variations. We show that image stylization in addition to color augmentation is a powerful method of learning robust features. We evaluate learned features on visual localization benchmarks, outperforming state of the art baseline models despite training without ground-truth 3D correspondences using synthetic homographies only. We use trained feature networks to compete in Long-Term Visual Localization and Map-based Localization for Autonomous Driving challenges achieving competitive scores."}}
{"id": "JB_R6KPxWmW", "cdate": 1577836800000, "mdate": 1632836560079, "content": {"title": "Multimodal End-to-End Learning for Autonomous Steering in Adverse Road and Weather Conditions", "abstract": "Autonomous driving is challenging in adverse road and weather conditions in which there might not be lane lines, the road might be covered in snow and the visibility might be poor. We extend the previous work on end-to-end learning for autonomous steering to operate in these adverse real-life conditions with multimodal data. We collected 28 hours of driving data in several road and weather conditions and trained convolutional neural networks to predict the car steering wheel angle from front-facing color camera images and lidar range and reflectance data. We compared the CNN model performances based on the different modalities and our results show that the lidar modality improves the performances of different multimodal sensor-fusion models. We also performed on-road tests with different models and they support this observation."}}
{"id": "7KCznMs57L", "cdate": 1577836800000, "mdate": 1632836560649, "content": {"title": "Multimodal End-to-End Learning for Autonomous Steering in Adverse Road and Weather Conditions", "abstract": "Autonomous driving is challenging in adverse road and weather conditions in which there might not be lane lines, the road might be covered in snow and the visibility might be poor. We extend the previous work on end-to-end learning for autonomous steering to operate in these adverse real-life conditions with multimodal data. We collected 28 hours of driving data in several road and weather conditions and trained convolutional neural networks to predict the car steering wheel angle from front-facing color camera images and lidar range and reflectance data. We compared the CNN model performances based on the different modalities and our results show that the lidar modality improves the performances of different multimodal sensor-fusion models. We also performed on-road tests with different models and they support this observation."}}
