{"id": "-HtdviJbWd", "cdate": 1672531200000, "mdate": 1682353737889, "content": {"title": "On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study", "abstract": "Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty. We show that the probabilistic methods consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns of probabilistic methods widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout), cautioning the importance of choosing appropriate method for the data setting."}}
{"id": "vWNIB67vXc", "cdate": 1640995200000, "mdate": 1681717445211, "content": {"title": "Scaling Up Influence Functions", "abstract": "We address efficient calculation of influence functions for tracking predictions back to the training data. We propose and analyze a new approach to speeding up the inverse Hessian calculation based on Arnoldi iteration. With this improvement, we achieve, to the best of our knowledge, the first successful implementation of influence functions that scales to full-size (language and vision) Transformer models with several hundreds of millions of parameters. We evaluate our approach in image classification and sequence-to-sequence tasks with tens to a hundred of millions of training examples. Our code is available at https://github.com/google-research/jax-influence."}}
{"id": "o-VyxcU7_u", "cdate": 1640995200000, "mdate": 1684315992225, "content": {"title": "Breaking BERT: Evaluating and Optimizing Sparsified Attention", "abstract": "Transformers allow attention between all pairs of tokens, but there is reason to believe that most of these connections - and their quadratic time and memory - may not be necessary. But which ones? We evaluate the impact of sparsification patterns with a series of ablation experiments. First, we compare masks based on syntax, lexical similarity, and token position to random connections, and measure which patterns reduce performance the least. We find that on three common finetuning tasks even using attention that is at least 78% sparse can have little effect on performance if applied at later transformer layers, but that applying sparsity throughout the network reduces performance significantly. Second, we vary the degree of sparsity for three patterns supported by previous work, and find that connections to neighbouring tokens are the most significant. Finally, we treat sparsity as an optimizable parameter, and present an algorithm to learn degrees of neighboring connections that gives a fine-grained control over the accuracy-sparsity trade-off while approaching the performance of existing methods."}}
{"id": "TS8e3vI4Awo", "cdate": 1640995200000, "mdate": 1684315992147, "content": {"title": "\"Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification", "abstract": ""}}
{"id": "jTLvO3jpvM", "cdate": 1609459200000, "mdate": 1682379133049, "content": {"title": "PROVIDE: a probabilistic framework for unsupervised video decomposition", "abstract": "Unsupervised multi-object scene decomposition is a fast-emerging problem in representation learning. Despite significant progress in static scenes, such models are unable to leverage important dyna..."}}
{"id": "pVwU-8cdjQQ", "cdate": 1601308134973, "mdate": null, "content": {"title": "Unsupervised Video Decomposition using Spatio-temporal Iterative Inference", "abstract": "Unsupervised multi-object scene decomposition is a fast-emerging problem in representation learning. Despite significant progress in static scenes, such models are unable to leverage important dynamic cues present in video. We propose a novel spatio-temporal iterative inference framework that is powerful enough to jointly model complex multi-object representations and explicit temporal dependencies between latent variables across frames. This is achieved by leveraging 2D-LSTM, temporally conditioned inference and generation within the iterative amortized inference for posterior refinement. Our method improves the overall quality of decompositions, encodes information about the objects' dynamics, and can be used to predict trajectories of each object separately. Additionally, we show that our model has a high accuracy even without color information. We demonstrate the decomposition, segmentation, and prediction capabilities of our model and show that it outperforms the state-of-the-art on several benchmark datasets, one of which was curated for this work and will be made publicly available."}}
{"id": "MNyQYISqRzK", "cdate": 1546300800000, "mdate": 1668709175942, "content": {"title": "DwNet: Dense warp-based network for pose-guided human video generation", "abstract": ""}}
{"id": "CBtjE7pkwS", "cdate": 1451606400000, "mdate": 1682330812751, "content": {"title": "On Impact of Weather on Human Mobility in Cities", "abstract": ""}}
