{"id": "SBstNm4OajH", "cdate": 1676472362265, "mdate": null, "content": {"title": "Reconstructing Training Data from Multiclass Neural Networks", "abstract": "Reconstructing samples from the training set of trained neural networks is a major privacy concern. Haim et al. (2022) recently showed that it is possible to reconstruct training samples from neural network binary classifiers, based on theoretical results about the implicit bias of gradient methods. In this work, we present several improvements and new insights over this previous work. \nAs our main improvement, we show that training-data reconstruction is possible in the multi-class setting and that the reconstruction quality is even higher than in the case of binary classification.  \nMoreover, we show that using weight-decay during training increases the vulnerability to sample reconstruction. Finally, while in the previous work the training set was of size at most $1000$ from $10$ classes, we show preliminary evidence of the ability to reconstruct from a model trained on $5000$ samples from $100$ classes."}}
{"id": "XDZhagjfMP", "cdate": 1652737499005, "mdate": null, "content": {"title": "Gradient Methods Provably Converge to Non-Robust Networks", "abstract": "Despite a great deal of research, it is still unclear why neural networks are so susceptible to adversarial examples. \tIn this work, we identify natural settings where depth-$2$ ReLU networks trained with gradient flow are provably non-robust (susceptible to small adversarial $\\ell_2$-perturbations), even when robust networks that classify the training dataset correctly exist.\tPerhaps surprisingly, we show that the well-known implicit bias towards margin maximization induces bias towards non-robust networks, by proving that every network which satisfies the KKT conditions of the max-margin problem is non-robust."}}
{"id": "Sxk8Bse3RKO", "cdate": 1652737466104, "mdate": null, "content": {"title": "Reconstructing Training Data From Trained Neural Networks", "abstract": "Understanding to what extent neural networks memorize training data is an intriguing question with practical and theoretical implications. \nIn this paper we show that in some cases a significant fraction of the training data can in fact be reconstructed from the parameters of a trained neural network classifier.\nWe propose a novel reconstruction scheme that stems from recent theoretical results about the implicit bias in training neural networks with gradient-based methods.\nTo the best of our knowledge, our results are the first to show that reconstructing a large portion of the actual training samples from a trained neural network classifier is generally possible.\nThis has negative implications on privacy, as it can be used as an attack for revealing sensitive training data. \nWe demonstrate our method for binary MLP classifiers on a few standard computer vision datasets."}}
{"id": "MkTPtnjeYTV", "cdate": 1632875490500, "mdate": null, "content": {"title": "On the Optimal Memorization Power of ReLU Neural Networks", "abstract": "We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any $N$ points that satisfy a mild separability assumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known VC-dimension upper bounds imply that memorizing $N$ samples requires $\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using $\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters."}}
{"id": "m0l7vTv70BK", "cdate": 1621629908129, "mdate": null, "content": {"title": "Learning a Single Neuron with Bias Using Gradient Descent", "abstract": "We theoretically study the fundamental problem of learning a single neuron with a bias term ($\\mathbf{x}\\mapsto \\sigma(\\langle\\mathbf{w},\\mathbf{x}\\rangle + b)$) in the realizable setting with the ReLU activation, using gradient descent. Perhaps surprisingly, we show that this is a significantly different and more challenging problem than the bias-less case (which was the focus of previous works on single neurons), both in terms of the optimization geometry as well as the ability of gradient methods to succeed in some scenarios. We provide a detailed study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and providing positive convergence guarantees under different sets of assumptions. To prove our results, we develop some tools which may be of independent interest, and improve previous results on learning single neurons. "}}
{"id": "9p2CltauWEY", "cdate": 1601308064678, "mdate": null, "content": {"title": "On Size Generalization in Graph Neural Networks", "abstract": "Graph neural networks (GNNs) can process graphs of different sizes but their capacity to generalize across sizes is still not well understood. Size generalization is key to numerous GNN applications, from solving combinatorial optimization problems to learning in molecular biology. In such problems, obtaining labels and training on large graphs can be prohibitively expensive, but training on smaller graphs is possible. \n\nThis paper puts forward the size-generalization question and characterizes important aspects of that problem theoretically and empirically.\nWe prove that even for very simple tasks, such as counting the number of nodes or edges in a graph, GNNs do not naturally generalize to graphs of larger size. Instead, their generalization performance is closely related to the distribution of local patterns of connectivity and features and how that distribution changes from small to large graphs. Specifically, we prove that for many tasks, there are weight assignments for GNNs that can perfectly solve the task on small graphs but fail on large graphs, if there is a discrepancy between their local patterns. We further demonstrate on several tasks, that training GNNs on small graphs results in solutions which do not generalize to larger graphs. We then formalize size generalization as a domain-adaption problem and describe two learning setups where size generalization can be improved. First, as a self-supervised learning problem (SSL) over the target domain of large graphs. Second as a semi-supervised learning problem when few samples are available in the target domain. We demonstrate the efficacy of these solutions on a diverse set of benchmark graph datasets. "}}
{"id": "rd7AuvB9C1O", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Effects of Mild Over-parameterization on the Optimization Landscape of Shallow ReLU Neural Networks", "abstract": "We study the effects of mild over-parameterization on the optimization landscape of a simple ReLU neural network of the form $\\mathbf{x}\\mapsto\\sum_{i=1}^k\\max\\{0,\\mathbf{w}_i^{\\top}\\mathbf{x}\\}$, in a well-studied teacher-student setting where the target values are generated by the same architecture, and when directly optimizing over the population squared loss with respect to Gaussian inputs. We prove that while the objective is strongly convex around the global minima when the teacher and student networks possess the same number of neurons, it is not even \\emph{locally convex} after any amount of over-parameterization. Moreover, related desirable properties (e.g., one-point strong convexity and the Polyak-{\\L}ojasiewicz condition) also do not hold even locally. On the other hand, we establish that the objective remains one-point strongly convex in \\emph{most} directions (suitably defined), and show an optimization guarantee under this property. For the non-global minima, we prove that adding even just a single neuron will turn a non-global minimum into a saddle point. This holds under some technical conditions which we validate empirically. These results provide a possible explanation for why recovering a global minimum becomes significantly easier when we over-parameterize, even if the amount of over-parameterization is very moderate."}}
{"id": "oZnNpdSng7h", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning a Single Neuron with Gradient Methods", "abstract": "We consider the fundamental problem of learning a single neuron $x \\mapsto\\sigma(w^\\top x)$ using standard gradient methods. As opposed to previous works, which considered specific (and not always realistic) input distributions and activation functions $\\sigma(\\cdot)$, we ask whether a more general result is attainable, under milder assumptions. On the one hand, we show that some assumptions on the distribution and the activation function are necessary. On the other hand, we prove positive guarantees under mild assumptions, which go beyond those studied in the literature so far. We also point out and study the challenges in further strengthening and generalizing our results."}}
{"id": "TqTYHjQeKhi", "cdate": 1577836800000, "mdate": null, "content": {"title": "Proving the Lottery Ticket Hypothesis: Pruning is All You Need", "abstract": "The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training."}}
{"id": "CGGiT94Q3na", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning a Single Neuron with Gradient Methods", "abstract": "We consider the fundamental problem of learning a single neuron $\\mathbf{x}\\mapsto \\sigma(\\mathbf{w}^\\top\\mathbf{x})$ in a realizable setting, using standard gradient methods with random initialization, and under general families of input distributions and activations. On the one hand, we show that some assumptions on both the distribution and the activation function are necessary. On the other hand, we prove positive guarantees under mild assumptions, which go significantly beyond those studied in the literature so far. We also point out and study the challenges in further strengthening and generalizing our results."}}
