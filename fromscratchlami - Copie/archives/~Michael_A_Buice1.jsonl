{"id": "FbXmQtN1SO", "cdate": 1684167864591, "mdate": 1684167864591, "content": {"title": "Modeling functional cell types in spike train data", "abstract": "A major goal of computational neuroscience is to build accurate models of theactivity of neurons that can be used to interpret their function in circuits. Here, weexplore usingfunctional cell typesto refine single-cell models by grouping them intofunctionally relevant classes. Formally, we define a hierarchical generative model for celltypes, single-cell parameters, and neural responses, and then derive anexpectation-maximization algorithm with variational inference that maximizes thelikelihood of the neural recordings. We apply this \u201csimultaneous\u201d method to estimatecell types and fit single-cell models from simulated data, and find that it accuratelyrecovers the ground truth parameters. We then apply our approach toin vitroneuralrecordings from neurons in mouse primary visual cortex, and find that it yieldsimproved prediction of single-cell activity. We demonstrate that the discovered cell-typeclusters are well separated and generalizable, and thus amenable to interpretation. Wethen compare discovered cluster memberships with locational, morphological, andtranscriptomic data. Our findings reveal the potential to improve models of neuralresponses by explicitly allowing for shared functional properties across neurons."}}
{"id": "7mNtXCY5UDU", "cdate": 1665251238345, "mdate": null, "content": {"title": "VI$^2$N: A Network for Planning Under Uncertainty based on Value of Information", "abstract": "Despite of great success in the recent years, deep reinforcement learning architectures still face a tremendous challenge in many real-world scenarios due to perceptual ambiguity. Similarly, differentiable networks, known as value iteration networks, that performs well in novel situations by extracting the environment model from training setups, are mostly limited to fully observable tasks. In this paper, we propose a new architecture, the VI$^2$N (Value Iteration with Value of Information Network) that can learn to act in novel environments with high amount of uncertainty. Specifically, this architecture uses a heuristic that over-emphasizes on reducing the uncertainty before exploiting the reward. Our network outperforms the state of the art differentiable architecture for partially observable environments especially when long term planning is needed to resolve the uncertainty.  "}}
{"id": "bF1hSW-z_8", "cdate": 1664248832097, "mdate": null, "content": {"title": "Using Sum-Product Networks to estimate neural population structure in the brain  ", "abstract": "We present a computationally efficient framework to model a wide range of population structures with high order correlations and a large number of neurons. Our method is based on a special type of Bayesian network that has linear inference time and is founded upon the concept of contextual independence. Moreover, we use an efficient architecture learning method for network selection to model large neural populations even with a small amount of data. Our framework is both fast and accurate in approximating neural population structures. Furthermore, our approach enables us to reliably quantify higher order neural correlations. We test our method on publicly available large-scale neural recordings from the Allen Brain Observatory. Our approach significantly outperforms other models both in terms of statistical measures and alignment with experimental evidence."}}
{"id": "XC_yGI-0j9", "cdate": 1663850425952, "mdate": null, "content": {"title": "Efficient approximation of neural population structure and correlations with probabilistic circuits", "abstract": " We present a computationally efficient framework to model a wide range of population structures with high order correlations and a large number of neurons. Our method is based on a special type of Bayesian network that has linear inference time and is founded upon the concept of contextual independence. Moreover, we use an efficient architecture learning method for network selection to model large neural populations even with a small amount of data. Our framework is both fast and accurate in approximating neural population structures. Furthermore, our approach enables us to reliably quantify higher order neural correlations. We test our method on simulated neural populations commonly used to generate higher order correlations, as well as on publicly available large-scale neural recordings from the Allen Brain Observatory. Our approach significantly outperforms other models both in terms of statistical measures and alignment with experimental evidence."}}
{"id": "4B7azgAbzda", "cdate": 1652737812806, "mdate": null, "content": {"title": "Learning dynamics of deep linear networks with multiple pathways", "abstract": "  Not only have deep networks become standard in machine learning, they are increasingly of interest in neuroscience as models of cortical computation that capture relationships between structural and functional properties.  In addition they are a useful target of theoretical research into the properties of network computation.  Deep networks typically have a serial or approximately serial organization across layers, and this is often mirrored in models that purport to represent computation in mammalian brains.  There are, however, multiple examples of parallel pathways in mammalian brains.  In some cases, such as the mouse, the entire visual system appears arranged in a largely parallel, rather than serial fashion.  While these pathways may be formed by differing cost functions that drive different computations, here we present a new mathematical analysis of learning dynamics in networks that have parallel computational pathways driven by the same cost function.  We use the approximation of deep linear networks with large hidden layer sizes to show that, as the depth of the parallel pathways increases, different features of the training set (defined by the singular values of the input-output correlation) will typically concentrate in one of the pathways.  This result is derived analytically and demonstrated with numerical simulation.  Thus, rather than sharing stimulus and task features across multiple pathways, parallel network architectures learn to produce sharply diversified representations with specialized and specific pathways, a mechanism which may hold important consequences for codes in both biological and artificial systems."}}
{"id": "rJwDMui8DI", "cdate": 1621630105939, "mdate": null, "content": {"title": "Neural Regression, Representational Similarity, Model Zoology & Neural Taskonomy at Scale in Rodent Visual Cortex", "abstract": "How well do deep neural networks fare as models of mouse visual cortex? A majority of research to date suggests results far more mixed than those produced in the modeling of primate visual cortex. Here, we perform a large-scale benchmarking of dozens of deep neural network models in mouse visual cortex with both representational similarity analysis and neural regression. Using the Allen Brain Observatory's 2-photon calcium-imaging dataset of activity in over 6,000 reliable rodent visual cortical neurons recorded in response to natural scenes, we replicate previous findings and resolve previous discrepancies, ultimately demonstrating that modern neural networks can in fact be used to explain activity in the mouse visual cortex to a more reasonable degree than previously suggested. Using our benchmark as an atlas, we offer preliminary answers to overarching questions about levels of analysis (e.g. do models that better predict the representations of individual neurons also predict representational similarity across neural populations?); questions about the properties of models that best predict the visual system overall (e.g. is convolution or category-supervision necessary to better predict neural activity?); and questions about the mapping between biological and artificial representations (e.g. does the information processing hierarchy in deep nets match the anatomical hierarchy of mouse visual cortex?). Along the way, we catalogue a number of models (including vision transformers, MLP-Mixers, normalization free networks, Taskonomy encoders and self-supervised models) outside the traditional circuit of convolutional object recognition. Taken together, our results provide a reference point for future ventures in the deep neural network modeling of mouse visual cortex, hinting at novel combinations of mapping method, architecture, and task to more fully characterize the computational motifs of visual representation in a species so central to neuroscience, but with a perceptual physiology and ecology markedly different from the ones we study in primates."}}
{"id": "8v_4EVifBqX", "cdate": 1621630073526, "mdate": null, "content": {"title": "Tensor decompositions of higher-order correlations by nonlinear Hebbian plasticity", "abstract": "Biological synaptic plasticity exhibits nonlinearities that are not accounted for by classic Hebbian learning rules. Here, we introduce a simple family of generalized nonlinear Hebbian learning rules. We study the computations implemented by their dynamics in the simple setting of a neuron receiving feedforward inputs. These nonlinear Hebbian rules allow a neuron to learn tensor decompositions of its higher- order input correlations. The particular input correlation decomposed and the form of the decomposition depend on the location of nonlinearities in the plasticity rule. For simple, biologically motivated parameters, the neuron learns eigenvectors of higher-order input correlation tensors. We prove that tensor eigenvectors are attractors and determine their basins of attraction. We calculate the volume of those basins, showing that the dominant eigenvector has the largest basin of attraction. We then study arbitrary learning rules and find that any learning rule that admits a finite Taylor expansion into the neural input and output also has stable equilibria at generalized eigenvectors of higher-order input correlation tensors. Nonlinearities in synaptic plasticity thus allow a neuron to encode higher-order input correlations in a simple fashion."}}
{"id": "SJl24XtLIB", "cdate": 1568211764186, "mdate": null, "content": {"title": "Data-Driven Discovery of Functional Cell Types that Improve Models of Neural Activity", "abstract": "Computational neuroscience aims to fit reliable models of in vivo neural activity and interpret them as abstract computations. Recent work has shown that functional diversity of neurons may be limited to that of relatively few cell types; other work has shown that incorporating constraints into artificial neural networks (ANNs) can improve their ability to mimic neural data. Here we develop an algorithm that takes as input recordings of neural activity and returns clusters of neurons by cell type and models of neural activity constrained by these clusters. The resulting models are both more predictive and more interpretable, revealing the contributions of functional cell types to neural computation and ultimately informing the design of future ANNs."}}
{"id": "SyeWE7tU8H", "cdate": 1568211753045, "mdate": null, "content": {"title": "Flexible degrees of connectivity under synaptic weight constraints", "abstract": "Biological neural networks face homeostatic and resource constraints that restrict the allowed configurations of connection weights. If a constraint is tight it defines a very small solution space, and the size of these constraint spaces determines their potential overlap with the solutions for computational tasks. We study the geometry of the solution spaces for constraints on neurons' total synaptic weight and on individual synaptic weights, characterizing the connection degrees (numbers of partners) that maximize the size of these solution spaces. We then hypothesize that the size of constraints' solution spaces could serve as a cost function governing neural circuit development. We develop analytical approximations and bounds for the model evidence of the maximum entropy degree distributions under these cost functions. We test these on a published electron microscopic connectome of an associative learning center in the fly brain, finding evidence for a developmental progression in circuit structure."}}
