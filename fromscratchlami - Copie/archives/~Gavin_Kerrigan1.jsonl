{"id": "bCZpPHmz5Ep", "cdate": 1684073685247, "mdate": 1684073685247, "content": {"title": "Differentially Private Language Models Benefit from Public Pre-training", "abstract": "Language modeling is a keystone task in natu- ral language processing. When training a lan- guage model on sensitive information, differ- ential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultane- ously high-quality and privacy preserving by tuning a public base model on a private cor- pus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible."}}
{"id": "zAjJHV6V1R", "cdate": 1672531200000, "mdate": 1699287705697, "content": {"title": "Diffusion Generative Models in Infinite Dimensions", "abstract": "Diffusion generative models have recently been applied to domains where the available data can be seen as a discretization of an underlying function, such as audio signals or time series. However, ..."}}
{"id": "DRjmsK1y7G", "cdate": 1672531200000, "mdate": 1708535609086, "content": {"title": "Probabilistic Precipitation Downscaling with Optical Flow-Guided Diffusion", "abstract": "In climate science and meteorology, local precipitation predictions are limited by the immense computational costs induced by the high spatial resolution that simulation methods require. A common workaround is statistical downscaling (aka superresolution), where a low-resolution prediction is super-resolved using statistical approaches. While traditional computer vision tasks mainly focus on human perception or mean squared error, applications in weather and climate require capturing the conditional distribution of high-resolution patterns given low-resolution patterns so that reliable ensemble averages can be taken. Our approach relies on extending recent video diffusion models to precipitation superresolution: an optical flow on the high-resolution output induces temporally coherent predictions, whereas a temporally-conditioned diffusion model generates residuals that capture the correct noise characteristics and high-frequency patterns. We test our approach on X-SHiELD, an established large-scale climate simulation dataset, and compare against two state-of-the-art baselines, focusing on CRPS, MSE, precipitation distributions, as well as an illustrative case -- the complex terrain of California. Our approach sets a new standard for data-driven precipitation downscaling."}}
{"id": "9LZHTVz3hP", "cdate": 1672531200000, "mdate": 1696002684548, "content": {"title": "Functional Flow Matching", "abstract": "In this work, we propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate directly in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on synthetic and real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models."}}
{"id": "F62H_GsR0e", "cdate": 1640995200000, "mdate": 1678898396416, "content": {"title": "Diffusion Generative Models in Infinite Dimensions", "abstract": ""}}
{"id": "Pkzvd9ONEPr", "cdate": 1621630311818, "mdate": null, "content": {"title": "Combining Human Predictions with Model Probabilities via Confusion Matrices and Calibration", "abstract": "An increasingly common use case for machine learning models is augmenting the abilities of human decision makers. For classification tasks where neither the human nor model are perfectly accurate, a key step in obtaining high performance is combining their individual predictions in a manner that leverages their relative strengths. In this work, we develop a set of algorithms that combine the probabilistic output of a model with the class-level output of a human. We show theoretically that the accuracy of our combination model is driven not only by the individual human and model accuracies, but also by the model's confidence.  Empirical results on image classification with CIFAR-10 and a subset of ImageNet demonstrate that such human-model combinations consistently have higher accuracies than the model or human alone, and that the parameters of the combination method can be estimated effectively with as few as ten labeled datapoints."}}
{"id": "9VQZYzUU1O", "cdate": 1609459200000, "mdate": 1678898396415, "content": {"title": "Combining Human Predictions with Model Probabilities via Confusion Matrices and Calibration", "abstract": ""}}
{"id": "uCvvJ70TBF", "cdate": 1577836800000, "mdate": 1678898396414, "content": {"title": "Differentially Private Language Models Benefit from Public Pre-training", "abstract": ""}}
