{"id": "bpLjR1sYfs", "cdate": 1672531200000, "mdate": 1678441777799, "content": {"title": "Model-based Offline Reinforcement Learning with Local Misspecification", "abstract": ""}}
{"id": "klElp42K9U0", "cdate": 1652737756270, "mdate": null, "content": {"title": "Data-Efficient Pipeline for Offline Reinforcement Learning with Limited Data", "abstract": "Offline reinforcement learning (RL) can be used to improve future performance by leveraging historical data. There exist many different algorithms for offline RL, and it is well recognized that these algorithms, and their hyperparameter settings, can lead to decision policies with substantially differing performance. This prompts the need for pipelines that allow practitioners to systematically perform algorithm-hyperparameter selection for their setting. Critically, in most real-world settings, this pipeline must only involve the use of historical data. \nInspired by statistical model selection methods for supervised learning, we introduce a task- and method-agnostic pipeline for automatically training, comparing, selecting, and deploying the best policy when the provided dataset is limited in size. In particular, our work highlights the importance of performing multiple data splits to produce more reliable algorithm-hyperparameter selection. While this is a common approach in supervised learning, to our knowledge, this has not been discussed in detail in the offline RL setting. We show it can have substantial impacts when the dataset is small. Compared to alternate approaches, our proposed pipeline outputs higher-performing deployed policies from a broad range of offline policy learning algorithms and across various simulation domains in healthcare, education, and robotics. This work contributes toward the development of a general-purpose meta-algorithm for automatic algorithm-hyperparameter selection for offline RL."}}
{"id": "SeZEruIj5e9", "cdate": 1646077549095, "mdate": null, "content": {"title": "Offline Policy Optimization with Eligible Actions", "abstract": "Offline policy optimization could have a large impact on many real-world decision-making problems, as online learning may be infeasible in many applications. Importance sampling and its variants are a common used type of estimator in offline policy evaluation, and such estimators typically do not require assumptions on the properties and representational capabilities of value function or decision process model function classes. In this paper, we identify an important overfitting phenomenon in optimizing the importance weighted return, in which it may be possible for the learned policy to essentially avoid making aligned decisions for part of the initial state space. We propose an algorithm to avoid this overfitting through a new per-state-neighborhood normalization constraint, and provide a theoretical justification of the proposed algorithm. We also show the limitations of previous attempts to this approach. We test our algorithm in a healthcare-inspired simulator, a logged dataset collected from real hospitals and continuous control tasks. These experiments show the proposed method yields less overfitting and better test performance compared to state-of-the-art batch reinforcement learning algorithms."}}
{"id": "z-lbeMmXUXa", "cdate": 1640995200000, "mdate": 1678441777825, "content": {"title": "Data-Efficient Pipeline for Offline Reinforcement Learning with Limited Data", "abstract": ""}}
{"id": "Zu1Ev5CxFVD", "cdate": 1640995200000, "mdate": 1651070351544, "content": {"title": "SAAC: Safe Reinforcement Learning as an Adversarial Game of Actor-Critics", "abstract": "Although Reinforcement Learning (RL) is effective for sequential decision-making problems under uncertainty, it still fails to thrive in real-world systems where risk or safety is a binding constraint. In this paper, we formulate the RL problem with safety constraints as a non-zero-sum game. While deployed with maximum entropy RL, this formulation leads to a safe adversarially guided soft actor-critic framework, called SAAC. In SAAC, the adversary aims to break the safety constraint while the RL agent aims to maximize the constrained value function given the adversary's policy. The safety constraint on the agent's value function manifests only as a repulsion term between the agent's and the adversary's policies. Unlike previous approaches, SAAC can address different safety criteria such as safe exploration, mean-variance risk sensitivity, and CVaR-like coherent risk sensitivity. We illustrate the design of the adversary for these constraints. Then, in each of these variations, we show the agent differentiates itself from the adversary's unsafe actions in addition to learning to solve the task. Finally, for challenging continuous control tasks, we demonstrate that SAAC achieves faster convergence, better efficiency, and fewer failures to satisfy the safety constraints than risk-averse distributional RL and risk-neutral soft actor-critic algorithms."}}
{"id": "UeqTPqtcdo", "cdate": 1640995200000, "mdate": 1678441777635, "content": {"title": "Offline Policy Optimization with Eligible Actions", "abstract": ""}}
{"id": "G2UZ-eP_f_v", "cdate": 1640995200000, "mdate": 1678441777877, "content": {"title": "Offline policy optimization with eligible actions", "abstract": ""}}
{"id": "lF6aNwoI-H", "cdate": 1609459200000, "mdate": 1678441777970, "content": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "abstract": ""}}
{"id": "OGzvtInZvuj", "cdate": 1609459200000, "mdate": 1632496990800, "content": {"title": "Adversarially Guided Actor-Critic", "abstract": "Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks."}}
{"id": "CouWAY8vaW", "cdate": 1609459200000, "mdate": 1678441777604, "content": {"title": "Sample-Efficient Deep Reinforcement Learning for Control, Exploration and Safety. (Apprentissage par renforcement profond \u00e9fficace pour le contr\u00f4le, l'exploration et la s\u00fbret\u00e9)", "abstract": ""}}
