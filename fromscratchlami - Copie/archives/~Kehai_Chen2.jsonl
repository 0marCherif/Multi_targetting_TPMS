{"id": "xqeSZgOwx7I", "cdate": 1688739280191, "mdate": 1688739280191, "content": {"title": "Document-Level Relation Extraction with Sentences Importance Estimation and Focusing", "abstract": "Document-level relation extraction (DocRE) aims to determine the relation between two entities from a document of multiple sentences. Recent studies typically represent the entire document by sequence- or graph-based models to predict the relations of all entity pairs. However, we find that such a model is not robust and exhibits bizarre behaviors: it predicts correctly when an entire test document is fed as input, but errs when non-evidence sentences are removed. To this end, we propose a Sentence Importance Estimation and Focusing (SIEF) framework for DocRE, where we design a sentence importance score and a sentence focusing loss, encouraging DocRE models to focus on evidence sentences. Experimental results on two domains show that our SIEF not only improves overall performance, but also makes DocRE models more robust. Moreover, SIEF is a general framework, shown to be effective when combined with a variety of base DocRE models."}}
{"id": "m9nwICrube", "cdate": 1688739209950, "mdate": 1688739209950, "content": {"title": "Discriminative Reasoning for Document-level Relation Extraction", "abstract": "Document-level relation extraction (DocRE) models generally use graph networks to implicitly model the reasoning skill (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the relation between one entity pair in a document. In this paper, we propose a novel discriminative reasoning framework to explicitly model the paths of these reasoning skills between each entity pair in this document. Thus, a discriminative reasoning network is designed to estimate the relation probability distribution of different reasoning paths based on the constructed graph and vectorized document contexts for each entity pair, thereby recognizing their relation. Experimental results show that our method outperforms the previous state-of-the-art performance on the large-scale DocRE dataset. "}}
{"id": "ZvgpySNXkV", "cdate": 1688739076613, "mdate": 1688739076613, "content": {"title": "Document-Level Relation Extraction with Reconstruction", "abstract": "In document-level relation extraction (DocRE), graph structure is generally used to encode relation information in the input document to classify the relation category between each entity pair, and has greatly advanced the DocRE task over the past several years. However, the learned graph representation universally models relation information between all entity pairs regardless of whether there are relationships between these entity pairs. Thus, those entity pairs without relationships disperse the attention of the encoder-classifier DocRE for ones with relationships, which may further hind the improvement of DocRE. To alleviate this issue, we propose a novel encoder-classifier-reconstructor model for DocRE. The reconstructor manages to reconstruct the ground-truth path dependencies from the graph representation, to ensure that the proposed DocRE model pays more attention to encode entity pairs with relationships in the training. Furthermore, the reconstructor is regarded as a relationship indicator to assist relation classification in the inference, which can further improve the performance of DocRE model. Experimental results on a large-scale DocRE dataset show that the proposed model can significantly improve the accuracy of relation extraction on a strong heterogeneous graph-based baseline."}}
{"id": "wyRCffNtDet", "cdate": 1640995200000, "mdate": 1636954454254, "content": {"title": "A Pattern Driven Graph Ranking Approach to Attribute Extraction for Knowledge Graph", "abstract": "Attribution extraction refers to find the attributes for the instances of a given semantic class, which is essential to enhance the schema of a knowledge graph. To facilitate the attribution extraction from the query log, this article proposes a pattern driven graph ranking approach to jointly employ the pattern and context distribution information. First, a simple pattern on query text is applied to automatically acquire seed attributes. Then, a graph-based weight propagation is designed to rank the patterns by context distribution algorithm information. Experimental results show that, on a Chinese query log collected by Baidu, the automatically acquired seeds are more representative than the classical manually assembled seeds, achieving an improvement of 11.6% in MAP as compared to the baseline approach. And the graph-based ranking algorithm manipulates the two types of evidence more effectively, outperforming both the distributional similarity based baseline and the HITS algorithm by 29.2% and 11.3%, respectively."}}
{"id": "yGlK7CuxPh3", "cdate": 1609459200000, "mdate": 1636954454234, "content": {"title": "Discriminative Reasoning for Document-level Relation Extraction", "abstract": "Wang Xu, Kehai Chen, Tiejun Zhao. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021."}}
{"id": "aMNEfQmGpuB", "cdate": 1609459200000, "mdate": null, "content": {"title": "Modeling Future Cost for Neural Machine Translation", "abstract": "Existing neural machine translation (NMT) systems utilize sequence-to-sequence neural networks to generate target translation word by word, and then make the generated word at each time-step and the counterpart in the references as consistent as possible. However, the trained translation model tends to focus on ensuring the accuracy of the generated target word at the current time-step and does not consider its future cost which means the expected cost of generating the subsequent target translation (i.e., the next target word). To respond to this issue, in this article, we propose a simple and effective method to model the future cost of each target word for NMT systems. In detail, a future cost representation is learned based on the current generated target word and its contextual information to compute an additional loss to guide the training of the NMT model. Furthermore, the learned future cost representation at the current time-step is used to help the generation of the next target word in the decoding. Experimental results on three widely-used translation datasets, including the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English, show that the proposed approach achieves significant improvements over strong Transformer-based NMT baseline."}}
{"id": "XLfjwXXSpC", "cdate": 1609459200000, "mdate": 1636954454230, "content": {"title": "Context-aware positional representation for self-attention networks", "abstract": "In self-attention networks (SANs), positional embeddings are used to model order dependencies between words in the input sentence and are added with word embeddings to gain an input representation, which enables the SAN-based neural model to perform (multi-head) and to stack (multi-layer) self-attentive functions in parallel to learn the representation of the input sentence. However, this input representation only involves static order dependencies based on discrete position indexes of words, that is, is independent of context information, which may be weak in modeling the input sentence. To address this issue, we proposed a novel positional representation method to model order dependencies based on n-gram context or sentence context in the input sentence, which allows SANs to learn a more effective sentence representation. To validate the effectiveness of the proposed method, it is applied to the neural machine translation model, which adopts a typical SAN-based neural model. Experimental results on two widely used translation tasks, i.e., WMT14 English-to-German and WMT17 Chinese-to-English, showed that the proposed approach can significantly improve the translation performance over the strong Transformer baseline."}}
{"id": "StZABGaJxIy", "cdate": 1609459200000, "mdate": null, "content": {"title": "Text Compression-aided Transformer Encoding", "abstract": "Text encoding is one of the most important steps in Natural Language Processing (NLP). It has been done well by the self-attention mechanism in the current state-of-the-art Transformer encoder, which has brought about significant improvements in the performance of many NLP tasks. Though the Transformer encoder may effectively capture general information in its resulting representations, the backbone information, meaning the gist of the input text, is not specifically focused on. In this paper, we propose explicit and implicit text compression approaches to enhance the Transformer encoding and evaluate models using this approach on several typical downstream tasks that rely on the encoding heavily. Our explicit text compression approaches use dedicated models to compress text, while our implicit text compression approach simply adds an additional module to the main model to handle text compression. We propose three ways of integration, namely backbone source-side fusion, target-side fusion, and both-side fusion, to integrate the backbone information into Transformer-based models for various downstream tasks. Our evaluation on benchmark datasets shows that the proposed explicit and implicit text compression approaches improve results in comparison to strong baselines. We therefore conclude, when comparing the encodings to the baseline models, text compression helps the encoders to learn better language representations."}}
{"id": "Q54myKtKOr", "cdate": 1609459200000, "mdate": 1636954454640, "content": {"title": "Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios", "abstract": "Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "B5LqxOQ5Ym", "cdate": 1609459200000, "mdate": 1636954453847, "content": {"title": "Document-Level Relation Extraction with Reconstruction", "abstract": "In document-level relation extraction (DocRE), graph structure is generally used to encode relation information in the input document to classify the relation category between each entity pair, and has greatly advanced the DocRE task over the past several years. However, the learned graph representation universally models relation information between all entity pairs regardless of whether there are relationships between these entity pairs. Thus, those entity pairs without relationships disperse the attention of the encoder-classifier DocRE for ones with relationships, which may further hind the improvement of DocRE. To alleviate this issue, we propose a novel encoder-classifier-reconstructor model for DocRE. The reconstructor manages to reconstruct the ground-truth path dependencies from the graph representation, to ensure that the proposed DocRE model pays more attention to encode entity pairs with relationships in the training. Furthermore, the reconstructor is regarded as a relationship indicator to assist relation classification in the inference, which can further improve the performance of DocRE model. Experimental results on a large-scale DocRE dataset show that the proposed model can significantly improve the accuracy of relation extraction on a strong heterogeneous graph-based baseline. The code is publicly available at https://github.com/xwjim/DocRE-Rec."}}
