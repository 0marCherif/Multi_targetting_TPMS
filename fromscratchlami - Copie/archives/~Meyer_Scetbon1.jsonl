{"id": "HJZCgyI7dP", "cdate": 1672531200000, "mdate": 1682318668174, "content": {"title": "Robust Linear Regression: Gradient-descent, Early-stopping, and Beyond", "abstract": "In this work we study the robustness to adversarial attacks, of early-stopping strategies on gradient-descent (GD) methods for linear regression. More precisely, we show that early-stopped GD is optimally robust (up to an absolute constant) against Euclidean-norm adversarial attacks. However, we show that this strategy can be arbitrarily sub-optimal in the case of general Mahalanobis attacks. This observation is compatible with recent findings in the case of classification~\\cite{Vardi2022GradientMP} that show that GD provably converges to non-robust models. To alleviate this issue, we propose to apply instead a GD scheme on a transformation of the data adapted to the attack. This data transformation amounts to apply feature-depending learning rates and we show that this modified GD is able to handle any Mahalanobis attack, as well as more general attacks under some conditions. Unfortunately, choosing such adapted transformations can be hard for general attacks. To the rescue, we design a simple and tractable estimator whose adversarial risk is optimal up to within a multiplicative constant of 1.1124 in the population regime, and works for any norm."}}
{"id": "4btNeXKFAQ", "cdate": 1652737441134, "mdate": null, "content": {"title": "Low-rank Optimal Transport: Approximation, Statistics and Debiasing", "abstract": "The matching principles behind optimal transport (OT) play an increasingly important role in machine learning, a trend which can be observed when OT is used to disambiguate datasets in applications (e.g. single-cell genomics) or used to improve more complex methods (e.g. balanced attention in transformers or self-supervised learning). To scale to more challenging problems, there is a growing consensus that OT requires solvers that can operate on millions, not thousands, of points. The low-rank optimal transport (LOT) approach advocated in \\cite{scetbon2021lowrank} holds several promises in that regard, and was shown to complement more established entropic regularization approaches, being able to insert itself in more complex pipelines, such as quadratic OT. LOT restricts the search for low-cost couplings to those that have a low-nonnegative rank, yielding linear time algorithms in cases of interest. However, these promises can only be fulfilled if the LOT approach is seen as a legitimate contender to entropic regularization when compared on properties of interest, where the scorecard typically includes theoretical properties (statistical complexity and relation to other methods) or practical aspects (debiasing, hyperparameter tuning, initialization). We target each of these areas in this paper in order to cement the impact of low-rank approaches in computational OT."}}
{"id": "n6BGcEk4Y7", "cdate": 1640995200000, "mdate": 1682318668162, "content": {"title": "Linear-Time Gromov Wasserstein Distances using Low Rank Couplings and Costs", "abstract": "The ability to align points across two related yet incomparable point clouds (e.g. living in different spaces) plays an important role in machine learning. The Gromov-Wasserstein (GW) framework pro..."}}
{"id": "dAs8ifmNfyJ", "cdate": 1640995200000, "mdate": 1682318667644, "content": {"title": "Low-rank Optimal Transport: Approximation, Statistics and Debiasing", "abstract": "The matching principles behind optimal transport (OT) play an increasingly important role in machine learning, a trend which can be observed when OT is used to disambiguate datasets in applications (e.g. single-cell genomics) or used to improve more complex methods (e.g. balanced attention in transformers or self-supervised learning). To scale to more challenging problems, there is a growing consensus that OT requires solvers that can operate on millions, not thousands, of points. The low-rank optimal transport (LOT) approach advocated in \\cite{scetbon2021lowrank} holds several promises in that regard, and was shown to complement more established entropic regularization approaches, being able to insert itself in more complex pipelines, such as quadratic OT. LOT restricts the search for low-cost couplings to those that have a low-nonnegative rank, yielding linear time algorithms in cases of interest. However, these promises can only be fulfilled if the LOT approach is seen as a legitimate contender to entropic regularization when compared on properties of interest, where the scorecard typically includes theoretical properties (statistical complexity and relation to other methods) or practical aspects (debiasing, hyperparameter tuning, initialization). We target each of these areas in this paper in order to cement the impact of low-rank approaches in computational OT."}}
{"id": "HxcJBUzJC4", "cdate": 1640995200000, "mdate": 1682318668056, "content": {"title": "An Asymptotic Test for Conditional Independence using Analytic Kernel Embeddings", "abstract": "We propose a new conditional dependence measure and a statistical test for conditional independence. The measure is based on the difference between analytic kernel embeddings of two well-suited dis..."}}
{"id": "CgyQRgWW5t", "cdate": 1640995200000, "mdate": 1682318667706, "content": {"title": "Triangular Flows for Generative Modeling: Statistical Consistency, Smoothness Classes, and Fast Rates", "abstract": "Triangular flows, also known as Kn\u00f6the-Rosenblatt measure couplings, comprise an important building block of normalizing flow models for generative modeling and density estimation, including popular autoregressive flows such as real-valued non-volume preserving transformation models (Real NVP). We present statistical guarantees and sample complexity bounds for triangular flow statistical models. In particular, we establish the statistical consistency and the finite sample convergence rates of the minimum Kullback-Leibler divergence statistical estimator of the Kn\u00f6the-Rosenblatt measure coupling using tools from empirical process theory. Our results highlight the anisotropic geometry of function classes at play in triangular flows, shed light on optimal coordinate ordering, and lead to statistical guarantees for Jacobian flows. We conduct numerical experiments to illustrate the practical implications of our theoretical findings."}}
{"id": "rHCzkRd0UK", "cdate": 1621630236648, "mdate": null, "content": {"title": "Linear-Time Gromov Wasserstein Distances using Low Rank Couplings and Costs", "abstract": "The ability to compare and align related datasets living in heterogeneous spaces plays an increasingly important role in machine learning. The Gromov-Wasserstein (GW) formalism can help tackle this problem. Its main goal is to seek an assignment (more generally a coupling matrix) that can register points across otherwise incomparable datasets. As a non-convex and quadratic generalization of optimal transport (OT), GW is NP-hard. Yet, heuristics are known to work reasonably well in practice, the state of the art approach being to solve a sequence of nested regularized OT problems. While popular, that heuristic remains too costly to scale, with cubic complexity in the number of samples $n$. We show in this paper how a recent variant of the Sinkhorn algorithm can substantially speed up the resolution of GW. That variant restricts the set of admissible couplings to those admitting a low rank factorization as the product of two sub-couplings. By updating alternatively each sub-coupling, our algorithm computes a stationary point of the problem in quadratic time with respect to the number of samples. When cost matrices have themselves low rank, our algorithm has time complexity $\\mathcal{O}(n)$. We demonstrate the efficiency of our method on simulated and real data."}}
{"id": "yD3omTtmwQl", "cdate": 1621611213588, "mdate": null, "content": {"title": "Harmonic Decompositions of Convolutional Networks", "abstract": "We present a description of the function space and the smoothness class associated with a convolutional network using the machinery of reproducing kernel Hilbert spaces. We show that the mapping\nassociated with a convolutional network expands into a sum involving elementary functions akin to spherical harmonics. This functional decomposition can be related to the functional ANOVA decomposition\nin nonparametric statistics. Building off our functional characterization of convolutional networks, we\nobtain statistical bounds highlighting an interesting trade-off between the approximation error and the\nestimation error."}}
{"id": "d43ZYADaDJEA", "cdate": 1621611161556, "mdate": null, "content": {"title": "A Spectral Analysis of Dot-product Kernels", "abstract": "We present eigenvalue decay estimates of integral operators associated with compositional dot-product kernels. The estimates improve on previous ones established for power series kernels on spheres. This allows us to obtain the volumes of balls in the corresponding reproducing kernel Hilbert spaces. We discuss the consequences on statistical estimation with compositional dot product kernels and highlight interesting trade-offs between the approximation error and the statistical error depending on the number of compositions and the smoothness of the kernels."}}
{"id": "2ZVRol-pVCwh", "cdate": 1621611015210, "mdate": null, "content": {"title": "Linear Time Sinkhorn Divergences using Positive Features", "abstract": "Although Sinkhorn divergences are now routinely used in data sciences to compare probability distributions, the computational effort required to compute them remains expensive, growing in general quadratically in the size n of the support of these distributions. Indeed, solving optimal transport (OT) with an entropic regularization requires computing a n\u00d7n kernel matrix (the neg-exponential of a n\u00d7n pairwise ground cost matrix) that is repeatedly applied to a vector. We propose to use instead ground costs of the form $c(x,y)=-\\log\\dotp{\\varphi(x)}{\\varphi(y)}$ where \u03c6 is a map from the ground space onto the positive orthant $\\RR^r_+$, with r\u226an. This choice yields, equivalently, a kernel $k(x,y)=\\dotp{\\varphi(x)}{\\varphi(y)}$, and ensures that the cost of Sinkhorn iterations scales as O(nr). We show that usual cost functions can be approximated using this form. Additionaly, we take advantage of the fact that our approach yields approximation that remain fully differentiable with respect to input distributions, as opposed to previously proposed adaptive low-rank approximations of the kernel matrix, to train a faster variant of OT-GAN \\cite{salimans2018improving}."}}
