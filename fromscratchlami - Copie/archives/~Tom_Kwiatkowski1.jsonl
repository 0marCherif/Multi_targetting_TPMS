{"id": "L7cfSXpDRm-", "cdate": 1609459200000, "mdate": null, "content": {"title": "Decontextualization: Making Sentences Stand-Alone", "abstract": "Models for question answering, dialogue agents, and summarization often interpret the meaning of a sentence in a rich context and use that meaning in a new context. Taking excerpts of text can be problematic, as key pieces may not be explicit in a local window. We isolate and define the problem of sentence decontextualization: taking a sentence together with its context and rewriting it to be interpretable out of context, while preserving its meaning. We describe an annotation procedure, collect data on the Wikipedia corpus, and use the data to train models to automatically decontextualize sentences. We present preliminary studies that show the value of sentence decontextualization in a user facing task, and as preprocessing for systems that perform document understanding. We argue that decontextualization is an important subtask in many downstream applications, and that the definitions and resources provided can benefit tasks that operate on sentences that occur in a richer context."}}
{"id": "IXzYPFsGGuv", "cdate": 1609459200000, "mdate": null, "content": {"title": "NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned", "abstract": "We review the EfficientQA competition from NeurIPS 2020. The competition focused on open-domain question answering (QA), where systems take natural language questions as input and return natural language answers. The aim of the competition was to build systems that can predict correct answers while also satisfying strict on-disk memory budgets. These memory budgets were designed to encourage contestants to explore the trade-off between storing retrieval corpora or the parameters of learned models. In this report, we describe the motivation and organization of the competition, review the best submissions, and analyze system predictions to inform a discussion of evaluation for open-domain QA."}}
{"id": "iHXV8UGYyL", "cdate": 1581705817867, "mdate": null, "content": {"title": "Empirical Evaluation of Pretraining Strategies for Supervised Entity Linking", "abstract": "In this work, we present an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links.  Our model achieves the state-of-the-art on two commonly used entity linking datasets:  96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations.  Lastly, we present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data"}}
{"id": "EMj5cDPxLhy", "cdate": 1577836800000, "mdate": null, "content": {"title": "Entities as Experts: Sparse Memory Access with Entity Supervision", "abstract": "We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model - Entities as Experts (EAE) - that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EAE's entity representations are learned directly from text. We show that EAE's learned representations capture sufficient knowledge to answer TriviaQA questions such as \"Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?\", outperforming an encoder-generator Transformer model with 10x the parameters. According to the LAMA knowledge probes, EAE contains more factual knowledge than a similarly sized BERT, as well as previous approaches that integrate external sources of entity knowledge. Because EAE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EAE's performance."}}
{"id": "HygwvC4tPH", "cdate": 1569439327430, "mdate": null, "content": {"title": "Learning Cross-Context Entity Representations from Text", "abstract": "Language modeling tasks, in which words, or word-pieces, are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity-centric, we investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned. We show that large scale training of neural models allows us to learn high quality entity representations, and we demonstrate successful results on four domains: (1) existing entity-level typing benchmarks, including a 64% error reduction over previous work on TypeNet (Murty et al., 2018); (2) a novel few-shot category reconstruction task; (3) existing entity linking benchmarks, where we achieve a score of 87.3% on TAC-KBP 2010 without using any alias table, external knowledge base or in domain training data and (4) answering trivia questions, which uniquely identify entities. Our global entity representations encode fine-grained type categories, such as \"Scottish footballers\", and can answer trivia questions such as \"Who was the last inmate of Spandau jail in Berlin?\"."}}
{"id": "r1lSRtxDDS", "cdate": 1569290701241, "mdate": null, "content": {"title": "Learning Entity Representations for Few-Shot Reconstruction of Wikipedia Categories", "abstract": "Language modeling tasks, in which words are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases tend to be entity-centric, we investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the contexts in which those entities were mentioned.\nWe show that large scale training of neural models allows us to learn extremely high fidelity entity typing information, which we demonstrate with few-shot reconstruction of Wikipedia categories. Our learning approach is powerful enough to encode specialized topics such as Giro d\u2019Italia cyclists."}}
{"id": "BkeiMYlDPr", "cdate": 1569290514533, "mdate": null, "content": {"title": "Matching the Blanks: Distributional Similarity for Relation Learning", "abstract": "General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED."}}
{"id": "BJgum4Qgu4", "cdate": 1553114144362, "mdate": null, "content": {"title": "Learning Entity Representations for Few-Shot Reconstruction of Wikipedia Categories", "abstract": "Language modeling tasks, in which words are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code\nworld knowledge into machine readable knowledge bases tend to be entity-centric,\nwe investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the contexts in which those entities were mentioned.\nWe show that large scale training of neural models allows us to learn extremely high fidelity entity typing information, which we demonstrate with few-shot reconstruction of Wikipedia categories. Our learning approach is powerful enough\nto encode specialized topics such as Giro d\u2019Italia cyclists."}}
{"id": "ejZr_u8Z1Dn", "cdate": 1546300800000, "mdate": null, "content": {"title": "Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index.", "abstract": "Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query. In this paper, we introduce the query-agnostic indexable representation of document phrases that can drastically speed up open-domain QA and also allows us to reach long-tail targets. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging optimization strategies, our model can be trained in a single 4-GPU server and serve entire Wikipedia (up to 60 billion phrases) under 2TB with CPUs only. Our experiments on SQuAD-Open show that our model is more accurate than DrQA (Chen et al., 2017) with 6000x reduced computational cost, which translates into at least 58x faster end-to-end inference benchmark on CPUs."}}
{"id": "LQnLWMIuTr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Natural Questions: a Benchmark for Question Answering Research.", "abstract": "We present the Natural Questions corpus, a question answering dataset. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature."}}
