{"id": "vtZzRb1471-", "cdate": 1630638356120, "mdate": null, "content": {"title": "Perhaps PTLMs should go to School \u2013 A Task to Assess Open Book and Closed Book QA", "abstract": "Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a signi\ufb01cant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given signi\ufb01cant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government 2e) and humanities (U.S. History), hundreds of true/false statements based on review questions written by the textbook authors, validation/development tests based on the \ufb01rst eight chapters of the textbooks, blind tests based on the remaining textbook chapters, and baseline results given state-of-the-art PTLMs.\n\nSince the questions are balanced, random performance should be ~50%. T5, \ufb01ne-tuned with BoolQ achieves the same performance, suggesting that the textbook\u2019s content is not pre-represented in the PTLM. Taking the exam closed book, but having read the textbook (i.e., adding the textbook to T5\u2019s pre-training), yields at best minor improvement (56%), suggesting that the PTLM may not have \u201cunderstood\u201d the textbook (or perhaps misunderstood the questions). Performance is better (~60%) when the exam is taken open-book (i.e., allowing the machine to automatically retrieve a paragraph and use it to answer the question)."}}
{"id": "3k20LAiHYL2", "cdate": 1601308264582, "mdate": null, "content": {"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "abstract": "Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.\nOur proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin."}}
