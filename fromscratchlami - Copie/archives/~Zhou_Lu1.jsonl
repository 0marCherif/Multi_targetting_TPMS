{"id": "jLX7Rr_olo", "cdate": 1673375504035, "mdate": 1673375504035, "content": {"title": "Projection-free Adaptive Regret with Membership Oracles", "abstract": "In the framework of online convex optimization, most iterative algorithms require the computation of projections onto convex sets, which can be computationally expensive. To tackle this problem Hazan and Kale (2012) proposed the study of projection-free methods that replace projections with less expensive computations. The most common approach is based on the Frank-Wolfe method, that uses linear optimization computation in lieu of projections. Recent work by Garber and Kretzu (2022) gave sublinear adaptive regret guarantees with projection free algorithms based on the Frank Wolfe approach.\nIn this work we give projection-free algorithms that are based on a different technique, inspired by Mhammedi (2022), that replaces projections by set-membership computations. We propose a simple lazy gradient-based algorithm with a Minkowski regularization that attains near-optimal adaptive regret bounds. For general convex loss functions we improve previous adaptive regret bounds from $O(T^{3/4})$ to $O(\\sqrt{T})$, and further to tight interval dependent bound $\\tilde{O}(\\sqrt{I})$ where I denotes the interval length. For strongly convex functions we obtain the first poly-logarithmic adaptive regret bounds using a projection-free algorithm."}}
{"id": "Jqas82UP428", "cdate": 1663850039767, "mdate": null, "content": {"title": "Lower Bounds for Differentially Private ERM: Unconstrained and Non-Euclidean", "abstract": "We consider the lower bounds of differentially private empirical risk minimization (DP-ERM) for convex functions in both constrained and unconstrained cases concerning the general $\\ell_p$ norm beyond the $\\ell_2$ norm considered by most of the previous works.\n\nWe provide a simple black-box reduction approach that can generalize lower bounds in constrained to unconstrained cases.\nMoreover, for $(\\epsilon,\\delta)$-DP, we achieve the optimal $\\Omega(\\frac{\\sqrt{d \\log(1/\\delta)}}{\\epsilon n})$ lower bounds for both constrained and unconstrained cases and any $\\ell_p$ geometry where $p\\geq 1$ by considering $\\ell_1$ loss over the $\\ell_{\\infty}$ ball."}}
{"id": "bzpCoHn1Vc_", "cdate": 1663850039533, "mdate": null, "content": {"title": "The Convergence Rate of SGD's Final Iterate: Analysis on Dimension Dependence", "abstract": "Stochastic Gradient Descent (SGD) is among the simplest and most popular optimization and machine learning methods. Running SGD with a fixed step size and outputting the final iteration is an ideal strategy one can hope for, but it is still not well-understood even though SGD has been studied extensively for over 70 years. Given the $\\Theta(\\log T)$ gap between current upper and lower bounds for running SGD for $T$ steps, it was then asked by [Koren and Segal COLT 20'] how to characterize the final-iterate convergence of SGD with a fixed step size in the constant dimension setting, i.e., $d=O(1)$. \n\nIn this paper, we consider the more general setting for any $d\\leq T$, proving $\\Omega(\\log d/\\sqrt{T})$ lower bounds for the sub-optimality of the final iterate of SGD in minimizing non-smooth Lipschitz convex functions with standard step sizes. Our results provide the first general dimension-dependent lower bound on the convergence of SGD's final iterate, partially resolving the COLT open question raised by [Koren and Segal COLT 20'].\nMoreover, we present a new method in one dimension based on martingale and Freedman\u2019s inequality, which gets the tight $O(1/\\sqrt{T})$ upper bound with mild assumptions and recovers the same bounds $O(\\log T/\\sqrt{T})$ as previous best results under the standard assumptions."}}
{"id": "_tIZQEMcWyv", "cdate": 1663850039165, "mdate": null, "content": {"title": "Adaptive Gradient Methods with Local Guarantees", "abstract": "Adaptive gradient methods are the method of choice for optimization in machine learning and used to train the largest deep models. In this paper we study the problem of learning a local preconditioner, that can change as the data is changing along the optimization trajectory. We propose an adaptive gradient method that has provable adaptive regret guarantees vs. the best local preconditioner. To derive this guarantee, we prove a new adaptive regret bound in online learning that improves upon previous adaptive online learning methods. \nWe demonstrate the robustness of our method in automatically choosing the optimal learning rate schedule for popular benchmarking tasks in vision and language domains. Without the need to manually tune a learning rate schedule, our method can, in a single run, achieve comparable and stable task accuracy as a fine-tuned optimizer."}}
{"id": "wsnMW0c_Au", "cdate": 1652737427947, "mdate": null, "content": {"title": "Non-convex online learning via algorithmic equivalence", "abstract": "We study an algorithmic equivalence technique between non-convex gradient descent and convex mirror descent. We start by looking at a harder problem of regret minimization in online non-convex optimization. We show that under certain geometric and smoothness conditions, online gradient descent applied to  non-convex  functions is an approximation of online mirror descent applied to convex functions under reparameterization. In continuous time, the gradient flow with this reparameterization was shown to be \\emph{exactly} equivalent to continuous-time mirror descent by Amid and Warmuth, but theory for the analogous discrete time algorithms is left as an open problem. We prove an $O(T^{\\frac{2}{3}})$ regret bound for non-convex online gradient descent in this setting, answering this open problem. Our analysis is based on a new and simple algorithmic equivalence method. "}}
{"id": "6-ILW5Skr_", "cdate": 1650547878634, "mdate": null, "content": {"title": "Adaptive Gradient Methods with Local Guarantees", "abstract": "Adaptive gradient methods are the method of choice for optimization in machine learning and used to train the largest deep models. \nIn this paper we study the problem of learning a local preconditioner, that can change as the data is changing along the optimization trajectory. We propose an adaptive gradient method that has provable adaptive regret guarantees vs. the best local preconditioner. To derive this guarantee, we prove a new adaptive regret bound in online learning that improves upon previous adaptive online learning methods. \n\nWe demonstrate the robustness of our method in automatically choosing the optimal learning rate schedule for popular benchmarking tasks in vision and language domains. Without the need to manually tune a learning rate schedule, our method can, in a single run, achieve comparable and stable task accuracy as a fine-tuned optimizer."}}
{"id": "30nbp1eV0dJ", "cdate": 1632875533700, "mdate": null, "content": {"title": "Tight lower bounds for Differentially Private ERM", "abstract": "We consider the lower bounds of differentially private ERM for general convex functions. For approximate-DP, the well-known upper bound of DP-ERM is $O(\\frac{\\sqrt{p\\log(1/\\delta)}}{\\epsilon n})$, which is believed to be tight. However, current lower bounds are off by some logarithmic terms, in particular $\\Omega(\\frac{\\sqrt{p}}{\\epsilon n})$ for constrained case and $\\Omega(\\frac{\\sqrt{p}}{\\epsilon n \\log p})$ for unconstrained case.\n\nWe achieve tight $\\Omega(\\frac{\\sqrt{p \\log(1/\\delta)}}{\\epsilon n})$ lower bounds for both cases by introducing a novel biased mean property for fingerprinting codes. As for pure-DP, we utilize a novel $\\ell_2$ loss function instead of linear functions considered by previous papers, and achieve the first (tight) $\\Omega(\\frac{p}{\\epsilon n})$ lower bound. We also introduce an auxiliary dimension to simplify the computation brought by $\\ell_2$ loss. Our results close a gap in our understanding of DP-ERM by presenting the fundamental limits. Our techniques may be of independent interest, which help enrich the tools so that it readily applies to problems that are not (easily) reducible from one-way marginals."}}
{"id": "6FsCHsZ66Fp", "cdate": 1601308322456, "mdate": null, "content": {"title": "Towards certifying $\\ell_\\infty$ robustness using Neural networks with $\\ell_\\infty$-dist Neurons", "abstract": "It is well-known that standard neural networks, even with a high classification accuracy, are vulnerable to small $\\ell_\\infty$ perturbations. Many attempts have been tried to learn a network that can resist such adversarial attacks. However, most previous works either can only provide empirical verification of the defense to a particular attack method or can only develop a theoretical guarantee of the model robustness in limited scenarios. In this paper, we develop a theoretically principled neural network that inherently resists $\\ell_\\infty$ perturbations. In particular, we design a novel neuron that uses $\\ell_\\infty$ distance as its basic operation, which we call $\\ell_\\infty$-dist neuron. We show that the $\\ell_\\infty$-dist neuron is naturally a 1-Lipschitz function with respect to the $\\ell_\\infty$ norm, and the neural networks constructed with $\\ell_\\infty$-dist neuron ($\\ell_{\\infty}$-dist Nets) enjoy the same property. This directly provides a theoretical guarantee of the certified robustness based on the margin of the prediction outputs. We further prove that the $\\ell_{\\infty}$-dist Nets have enough expressiveness power to approximate any 1-Lipschitz function, and can generalize well as the robust test error can be upper-bounded by the performance of a large margin classifier on the training data. Preliminary experiments show that even without the help of adversarial training, the learned networks with high classification accuracy are already provably robust."}}
{"id": "WANRKeyBgBn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Boosting for Control of Dynamical Systems", "abstract": "We study the question of how to aggregate controllers for dynamical systems in order to improve their performance. To this end, we propose a framework of boosting for online control. Our main resul..."}}
{"id": "S1e4Eub_bH", "cdate": 1483228800000, "mdate": null, "content": {"title": "The Expressive Power of Neural Networks: A View from the Width", "abstract": "The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks."}}
