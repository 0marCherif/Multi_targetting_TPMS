{"id": "kAx_rZtFbY", "cdate": 1663850264301, "mdate": null, "content": {"title": "Instance-Specific Augmentation: Capturing Local Invariances", "abstract": "We introduce InstaAug, a method for automatically learning input-specific augmentations from data. Previous data augmentation methods have generally assumed independence between the original input and the transformation applied to that input. This can be highly restrictive, as the invariances that the augmentations are based on are themselves often highly input dependent; e.g., we can change a leaf from green to yellow while maintaining its label, but not a lime. InstaAug instead allows for input dependency by introducing an invariance module that maps inputs to tailored transformation distributions. It can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. We empirically demonstrate that InstaAug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks."}}
{"id": "DM9OzLy7MaC", "cdate": 1640995200000, "mdate": 1683621212294, "content": {"title": "On Incorporating Inductive Biases into VAEs", "abstract": "We explain why directly changing the prior can be a surprisingly ineffective mechanism for incorporating inductive biases into variational auto-encoders (VAEs), and introduce a simple and effective..."}}
{"id": "3moI0jlPnc", "cdate": 1640995200000, "mdate": 1683621212299, "content": {"title": "Learning Instance-Specific Data Augmentations", "abstract": "We introduce InstaAug, a method for automatically learning input-specific augmentations from data. Previous methods for learning augmentations have typically assumed independence between the original input and the transformation applied to that input. This can be highly restrictive, as the invariances we hope our augmentation will capture are themselves often highly input dependent. InstaAug instead introduces a learnable invariance module that maps from inputs to tailored transformation parameters, allowing local invariances to be captured. This can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. We empirically demonstrate that InstaAug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks."}}
{"id": "nzvbBD_3J-g", "cdate": 1632875540784, "mdate": null, "content": {"title": "On Incorporating Inductive Biases into VAEs", "abstract": "We explain why directly changing the prior can be a surprisingly ineffective mechanism for incorporating inductive biases into variational auto-encoders (VAEs), and introduce a simple and effective alternative approach: Intermediary Latent Space VAEs (InteL-VAEs). InteL-VAEs use an intermediary set of latent variables to control the stochasticity of the encoding process, before mapping these in turn to the latent representation using a parametric function that encapsulates our desired inductive bias(es). This allows us to impose properties like sparsity or clustering on learned representations, and incorporate human knowledge into the generative model. Whereas changing the prior only indirectly encourages behavior through regularizing the encoder, InteL-VAEs are able to directly enforce desired characteristics. Moreover, they bypass the computation and encoder design issues caused by non-Gaussian priors, while allowing for additional flexibility through training of the parametric mapping function. We show that these advantages, in turn, lead to both better generative models and better representations being learned."}}
{"id": "ocGAGba1vrF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods", "abstract": "It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach."}}
{"id": "DJX9Ofy0PBp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Maximum Likelihood Training for Text Generation with Density Ratio Estimation", "abstract": "Autoregressive neural sequence generative models trained by Maximum Likelihood Estimation suffer the exposure bias problem in practical finite sample scenarios. The crux is that the number of train..."}}
{"id": "1JHyySArtNI", "cdate": 1577836800000, "mdate": 1651067685992, "content": {"title": "Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation", "abstract": "Deep generative models are commonly used for generating images and text. Interpretability of these models is one important pursuit, other than the generation quality. Variational auto-encoder (VAE)..."}}
{"id": "wtoIIgRDb4BN", "cdate": 1546300800000, "mdate": null, "content": {"title": "Kernelized Bayesian Softmax for Text Generation", "abstract": "Neural models for text generation require a softmax layer with proper token embeddings during the decoding phase. Most existing approaches adopt single point embedding for each token. However, a word may have multiple senses according to different context, some of which might be distinct. In this paper, we propose KerBS, a novel approach for learning better embeddings for text generation. KerBS embodies two advantages: (a) it employs a Bayesian composition of embeddings for words with multiple senses; (b) it is adaptive to semantic variances of words and robust to rare sentence context by imposing learned kernels to capture the closeness of words (senses) in the embedding space. Empirical studies show that KerBS significantly boosts the performance of several text generation tasks."}}
{"id": "bIauJ93JFqp", "cdate": 1546300800000, "mdate": 1651067686091, "content": {"title": "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling", "abstract": ""}}
{"id": "OwjLN1F12-J", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generating Fluent Adversarial Examples for Natural Languages", "abstract": "Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHAoutperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance."}}
