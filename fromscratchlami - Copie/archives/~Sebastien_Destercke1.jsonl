{"id": "SYZ3tP8jcgc", "cdate": 1646077537456, "mdate": null, "content": {"title": "Quantification of Credal Uncertainty in Machine Learning: A Critical Analysis and Empirical Comparison", "abstract": "The representation and quantification of uncertainty has received increasing attention in machine learning in the recent past. The formalism of credal sets provides an interesting alternative in this regard, especially as it combines the representation of epistemic (lack of knowledge) and aleatoric (statistical) uncertainty in a rather natural way. In this paper, we elaborate on uncertainty measures for credal sets from the perspective of machine learning. More specifically, we provide an overview of proposals, discuss existing measures in a critical way, and also propose a new measure that is more tailored to the machine learning setting. Based on an experimental study, we conclude that theoretically well-justified measures also lead to better performance in practice. Besides, we corroborate the difficulty of the disaggregation problem, that is, of decomposing the amount of total uncertainty into aleatoric and epistemic uncertainty in a sound manner, thereby complementing theoretical findings with empirical evidence."}}
{"id": "weD0v8kYARK", "cdate": 1609459200000, "mdate": 1637140741690, "content": {"title": "Imprecise Gaussian discriminant classification", "abstract": "Highlights \u2022 We robustify Gaussian discriminant analysis by considering sets of estimates. \u2022 We use near-ignorance priors to derive bounding boxes on mean estimates. \u2022 We discuss the computational issue for generic and diagonal covariance matrices. \u2022 We make a full experimental study showing the benefits of using imprecise estimates. \u2022 We make a first exploration of the benefits of using the model in non i.i.d. situations. Abstract Gaussian discriminant analysis is a popular classification model, that in the precise case can produce unreliable predictions in case of high uncertainty (e.g., due to scarce or noisy data). While imprecise probability theory offers a nice theoretical framework to solve such issues, it has not been yet applied to Gaussian discriminant analysis. This work remedies this, by proposing a new Gaussian discriminant analysis based on robust Bayesian analysis and near-ignorance priors. The model delivers cautiouspredictions, in form of set-valued class, in case of limited or imperfect available information. We present and discuss results of experimentation on real and synthetic datasets, where for this latter we corrupt the test instance to see how our approach reacts to non i.i.d. samples. Experiments show that including an imprecise component in the Gaussian discriminant analysis produces reasonably cautious predictions, and that set-valued predictions correspond to instances for which the precise model performs poorly."}}
{"id": "nHGIaYNrZyW", "cdate": 1609459200000, "mdate": 1623653734320, "content": {"title": "Imprecise Gaussian discriminant classification", "abstract": "Highlights \u2022 We robustify Gaussian discriminant analysis by considering sets of estimates. \u2022 We use near-ignorance priors to derive bounding boxes on mean estimates. \u2022 We discuss the computational issue for generic and diagonal covariance matrices. \u2022 We make a full experimental study showing the benefits of using imprecise estimates. \u2022 We make a first exploration of the benefits of using the model in non i.i.d. situations. Abstract Gaussian discriminant analysis is a popular classification model, that in the precise case can produce unreliable predictions in case of high uncertainty (e.g., due to scarce or noisy data). While imprecise probability theory offers a nice theoretical framework to solve such issues, it has not been yet applied to Gaussian discriminant analysis. This work remedies this, by proposing a new Gaussian discriminant analysis based on robust Bayesian analysis and near-ignorance priors. The model delivers cautiouspredictions, in form of set-valued class, in case of limited or imperfect available information. We present and discuss results of experimentation on real and synthetic datasets, where for this latter we corrupt the test instance to see how our approach reacts to non i.i.d. samples. Experiments show that including an imprecise component in the Gaussian discriminant analysis produces reasonably cautious predictions, and that set-valued predictions correspond to instances for which the precise model performs poorly."}}
{"id": "lCOlXM18Q4C", "cdate": 1609459200000, "mdate": 1637140743584, "content": {"title": "Copula-based conformal prediction for multi-target regression", "abstract": "highlights \u2022 We use a conformal prediction setting for Multi-Target Regression. \u2022 We link copulas to the estimation of non-conformity scores. \u2022 We perform experiments with 3 types of copulas, and compare between them. \u2022 We show through experiments that our method performs better than a naive approach. Abstract There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions for inductive conformal prediction, and illustrate our proposal by applying it to deep neural networks and random forests. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets."}}
{"id": "UeiFXsCIpp", "cdate": 1609459200000, "mdate": 1637140742551, "content": {"title": "Copula-based conformal prediction for Multi-Target Regression", "abstract": "There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions applied to deep neural networks for inductive conformal prediction. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets."}}
{"id": "PZUfn_FWFgD", "cdate": 1609459200000, "mdate": 1637140742552, "content": {"title": "Logical and Evidential Inconsistencies Meet: First Steps", "abstract": "Measuring inconsistency has been and is still an active research topic in both logic and evidence theory. However, the two fields have developed distinct notions and measures of inconsistency, following different paths. In this paper, we attempt to build some first bridges between the two trends, suggesting some first means for one to enrich the other, and vice-versa."}}
{"id": "KRtG39cqhC5", "cdate": 1609459200000, "mdate": 1637140735679, "content": {"title": "Multi-label Chaining with Imprecise Probabilities", "abstract": "We present two different strategies to extend the classical multi-label chaining approach to handle imprecise probability estimates. These estimates use convex sets of distributions (or credal sets) in order to describe our uncertainty rather than a precise one. The main reasons one could have for using such estimations are (1) to make cautious predictions (or no decision at all) when a high uncertainty is detected in the chaining and (2) to make better precise predictions by avoiding biases caused in early decisions in the chaining. We adapt both strategies to the case of the naive credal classifier, showing that this adaptations are computationally efficient. Our experimental results on missing labels, which investigate how reliable these predictions are in both approaches, indicate that our approaches produce relevant cautiousness on those hard-to-predict instances where the precise models fail."}}
{"id": "Ff2VKvDVm-R", "cdate": 1609459200000, "mdate": 1623653734315, "content": {"title": "Copula-based conformal prediction for Multi-Target Regression", "abstract": "There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions applied to deep neural networks for inductive conformal prediction. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets."}}
{"id": "FLUmvE0IW3J", "cdate": 1609459200000, "mdate": 1637140742964, "content": {"title": "Incremental Elicitation of Preferences: Optimist or Pessimist?", "abstract": "In robust incremental elicitation, it is quite common to make recommendations and to select queries by using a minimax regret criterion, which corresponds to a pessimistic attitude. In this paper, we explore its optimistic counterpart, showing this new approach enjoys the same convergence properties. While this optimistic approach does not offer the same kind of guarantees than minimax approaches, it still offers some other interesting properties. Finally, we illustrate with some experiments that the best approach amongst the two approaches heavily depends on the underlying setting."}}
{"id": "BOIS7pjC_4C", "cdate": 1609459200000, "mdate": 1637140756252, "content": {"title": "Racing trees to query partial data", "abstract": "Dealing with partially known or missing data is a common problem in machine learning. This work is interested in the problem of querying the true value of data to improve the quality of the learned model, when those data are only partially known. This study is in the line of active learning, since we consider that the precise value of some partial data can be queried to reduce the uncertainty in the learning process, yet can consider any kind of partial data (not only entirely missing one). We propose a querying strategy based on the concept of racing algorithms in which several models are competing. The idea is to identify the query that will help the most to quickly decide the winning model in the competition. After discussing and formalizing the general ideas of our approach, we study the particular case of decision trees in case of interval-valued features and set-valued labels. The experimental results indicate that, in comparison with other baselines, the proposed approach significantly outperforms simpler strategies in the case of partially specified features, while it achieves similar performances in the case of partially specified labels."}}
