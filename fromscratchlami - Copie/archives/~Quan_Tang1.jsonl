{"id": "4R7YrAGhnve", "cdate": 1652737318102, "mdate": null, "content": {"title": "SegViT: Semantic Segmentation with Plain Vision Transformers", "abstract": "We explore the capability of plain Vision Transformers (ViTs) for semantic segmentation and propose the SegViT. Previous ViT-based segmentation networks usually learn a pixel-level representation from the output of the ViT. Differently, we make use of the fundamental component\u2014attention mechanism, to generate masks for semantic segmentation. Specifically, we propose the Attention-to-Mask (ATM) module, in which the similarity maps between a set of learnable class tokens and the spatial feature maps are transferred to the segmentation masks. Experiments show that our proposed SegViT using the ATM module outperforms its counterparts using the plain ViT backbone on the ADE20K dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and PASCAL-Context datasets. Furthermore, to reduce the computational cost of the ViT backbone, we propose query-based down-sampling (QD) and query-based up-sampling (QU) to build a Shrunk structure. With our Shrunk structure, the model can save up to 40% computations while maintaining competitive performance."}}
{"id": "ydqbxuVHXci", "cdate": 1640995200000, "mdate": 1666765422523, "content": {"title": "SegViT: Semantic Segmentation with Plain Vision Transformers", "abstract": "We explore the capability of plain Vision Transformers (ViTs) for semantic segmentation and propose the SegVit. Previous ViT-based segmentation networks usually learn a pixel-level representation from the output of the ViT. Differently, we make use of the fundamental component -- attention mechanism, to generate masks for semantic segmentation. Specifically, we propose the Attention-to-Mask (ATM) module, in which the similarity maps between a set of learnable class tokens and the spatial feature maps are transferred to the segmentation masks. Experiments show that our proposed SegVit using the ATM module outperforms its counterparts using the plain ViT backbone on the ADE20K dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and PASCAL-Context datasets. Furthermore, to reduce the computational cost of the ViT backbone, we propose query-based down-sampling (QD) and query-based up-sampling (QU) to build a Shrunk structure. With the proposed Shrunk structure, the model can save up to $40\\%$ computations while maintaining competitive performance."}}
{"id": "uJTOgMoDvz", "cdate": 1640995200000, "mdate": 1666765422473, "content": {"title": "Compensating for Local Ambiguity With Encoder-Decoder in Urban Scene Segmentation", "abstract": "Semantic segmentation plays a critical role in scene understanding for self-driving vehicles. A line of efforts has proven that global context matters in urban scene segmentation due to massive scale changes. However, we find that existing methods suffer from local ambiguities when dissipating continuous local context, i.e. scrambling to a huge receptive field of global cues by coarse pooling. To this end, this paper proposes a new Context Aggregation Module (CAM) that consists of two primary components: context encoding using no coarse pooling but encoder-decoders with appropriate sampling scales and gated fusion that extends gate attention mechanism to balance different-scale context during feature fusion. Weeding out coarse pooling and applying the encoder-decoder inherits the merits of exploring global context while avoiding the drawback of losing local contextual continuity. We then construct a Context Aggregation Network (CANet) and conduct extensive evaluations on challenging autonomous driving benchmarks of Cityscapes, CamVid and BDD100K. Consistently improved results evidence the effectiveness. Notably, we attain competitive mIoU 82.7% on Cityscapes and optimal mIoU 80.5% on CamVid."}}
{"id": "ozTCXylMVK", "cdate": 1640995200000, "mdate": 1666765422587, "content": {"title": "Utilize Spatial Prior in Ground Truth: Spatial-Enhanced Loss for Semantic Segmentation", "abstract": "Most supervised semantic segmentation methods to date choose cross-entropy loss (CE) as the default choice. Standard CE treats all pixels in the image indiscriminately, which lacks consideration of context differences between pixels, leading to the model being overwhelmed by numerous homogeneous pixels in large-scale objects. It ignores an essential spatial prior that can be deduced from Ground Truth-the segmentation edges, which can be practical to distinguish the excessive homogeneous pixels. Therefore, we propose a novel loss function termed Spatial-enhanced Loss (SL), in which the image is spatially separated into the edge region and the body region with the assistance of the edge derived from Ground Truth. Experiments evidence that SL has impressive superiority over Focal Loss, standard cross-entropy loss, class-balanced cross-entropy loss and Dice Loss. We achieve substantial improvements on multiple models without using any tricks, up to 1.60% mIoU."}}
{"id": "jTUAjaeXvT", "cdate": 1640995200000, "mdate": 1666765422588, "content": {"title": "A dynamic ensemble algorithm for anomaly detection in IoT imbalanced data streams", "abstract": ""}}
{"id": "Ix6tPPrQ1oE", "cdate": 1640995200000, "mdate": 1666765422481, "content": {"title": "Dynamic Incremental Ensemble Fuzzy Classifier for Data Streams in Green Internet of Things", "abstract": "Due to the fast, dynamic, and continuous arrival of data streams in the green Internet of Things (IoT) environment, the probability distribution of data streams changes over time. In real IoT scenarios such as unmanned aerial vehicle (UAV) detection and smart light switch control, data distribution changes have reduced the trained model\u2019s accuracy for data streams problems classification, making it challenging to detect UAV intruders and predict whether energy-saving lamps in smart buildings are on or off. In this paper, an incremental ensemble classification method is proposed to improve prediction accuracy for green IoT. Specifically, a fuzzy rule-based classifier is combined with a dynamic weighting algorithm for improving classification accuracy. Moreover, the model is updated by incrementally learning the characteristics of data streams, which can effectively handle concept drift caused by data distribution changes in data streams. Experimental evaluations of UAV intrusion detection, smart buildings, and other datasets show that the proposed approach yields 2% higher area under the curve (AUC) and geometric mean (G-mean) than existing methods on UAV Detection and Occupancy datasets and 5% higher AUC and G-mean on five benchmarking datasets. For all datasets, the proposed approach yields 50% faster average training time than other methods."}}
{"id": "10ml3-DHRP8", "cdate": 1640995200000, "mdate": 1666765422517, "content": {"title": "EPRNet: Efficient Pyramid Representation Network for Real-Time Street Scene Segmentation", "abstract": "Current scene segmentation methods suffer from cumbersome model structures and high computational complexity, impeding their applications to real-world scenarios that require real-time processing. This paper proposes a novel Efficient Pyramid Representation Network (EPRNet), which strikes an innovative record on segmentation accuracy, model lightness and inference efficiency. Unlike existing methods delivering transfer learning based on pixel features of limited receptive fields encoded by shallow image classification backbones, EPRNet distributes multi-scale representations throughout the feature encoding flow to quickly enlarge and enrich receptive fields. Specifically, we introduce an extremely lightweight and efficient Multi-scale Processing Unit (MPU) that encodes multi-scale features through parallel convolutions of different kernels. By combining MPU and residual learning, we propose a core Pyramid Representation Module (PRM) to correctly acquire and aggregate region-based contexts in both shallow and deep layers. In this way, EPRNet can encode discriminative and comprehensive representations of multi-scale objects with a compact structure. We conduct extensive experiments on Cityscapes and CamVid datasets, demonstrating the superiority. Without any extra and coarse labeled data, EPRNet obtains mIoU 73.9% on the Cityscapes test set with only 0.9 million parameters at a speed of 42 FPS."}}
{"id": "01kyL7h8pq", "cdate": 1640995200000, "mdate": 1666765422495, "content": {"title": "Alleviating Overconfident Failure Predictions via Masking Predictive Logits in Semantic Segmentation", "abstract": "Currently, semantic segmentation is formulated to a classification task as image classification with similar networks and training settings. We observe an excessive overconfidence phenomenon in semantic segmentation regarding the model\u2019s classification scores. Unlike image classification, segmentation networks yield undue-high predictive probabilities for failure predictions, which may carry severe repercussions in safety-sensitive applications. To this end, we propose manually perturbing the predicted probability distribution via masking predictive logits during training that explicitly enforces the model to re-learn potential patterns, based on the pure intuition that meaningful patterns help alleviate overconfident failure predictions. A direct instantiation is presented that randomly zeroes out the model\u2019s predictive logits but keeps their expectations unchanged before computing the loss in the training phase. This instantiation requires no additional computation cost or customized architectures but only a masking function. Empirical results from various network architectures indicate its feasibility and effectiveness of alleviating overconfident failure predictions in semantic segmentation."}}
{"id": "dwkh8ISfWeo", "cdate": 1609459200000, "mdate": 1666765422713, "content": {"title": "Attention-guided chained context aggregation for semantic segmentation", "abstract": ""}}
{"id": "CS3ufQDAVtm", "cdate": 1577836800000, "mdate": 1666765422518, "content": {"title": "Attention-guided Chained Context Aggregation for Semantic Segmentation", "abstract": "The way features propagate in Fully Convolutional Networks is of momentous importance to capture multi-scale contexts for obtaining precise segmentation masks. This paper proposes a novel series-parallel hybrid paradigm called the Chained Context Aggregation Module (CAM) to diversify feature propagation. CAM gains features of various spatial scales through chain-connected ladder-style information flows and fuses them in a two-stage process, namely pre-fusion and re-fusion. The serial flow continuously increases receptive fields of output neurons and those in parallel encode different region-based contexts. Each information flow is a shallow encoder-decoder with appropriate down-sampling scales to sufficiently capture contextual information. We further adopt an attention model in CAM to guide feature re-fusion. Based on these developments, we construct the Chained Context Aggregation Network (CANet), which employs an asymmetric decoder to recover precise spatial details of prediction maps. We conduct extensive experiments on six challenging datasets, including Pascal VOC 2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence that CANet achieves state-of-the-art performance."}}
