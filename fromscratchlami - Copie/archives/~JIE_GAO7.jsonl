{"id": "GY6dZnodaPL", "cdate": 1577836800000, "mdate": 1696336638197, "content": {"title": "RP-DNN: A Tweet Level Propagation Context Based Deep Neural Networks for Early Rumor Detection in Social Media", "abstract": ""}}
{"id": "23uICvK_on", "cdate": 1577836800000, "mdate": 1696336638155, "content": {"title": "RP-DNN: A Tweet level propagation context based deep neural networks for early rumor detection in Social Media", "abstract": "Early rumor detection (ERD) on social media platform is very challenging when limited, incomplete and noisy information is available. Most of the existing methods have largely worked on event-level detection that requires the collection of posts relevant to a specific event and relied only on user-generated content. They are not appropriate to detect rumor sources in the very early stages, before an event unfolds and becomes widespread. In this paper, we address the task of ERD at the message level. We present a novel hybrid neural network architecture, which combines a task-specific character-based bidirectional language model and stacked Long Short-Term Memory (LSTM) networks to represent textual contents and social-temporal contexts of input source tweets, for modelling propagation patterns of rumors in the early stages of their development. We apply multi-layered attention models to jointly learn attentive context embeddings over multiple context inputs. Our experiments employ a stringent leave-one-out cross-validation (LOO-CV) evaluation setup on seven publicly available real-life rumor event data sets. Our models achieve state-of-the-art(SoA) performance for detecting unseen rumors on large augmented data which covers more than 12 events and 2,967 rumors. An ablation study is conducted to understand the relative contribution of each component of our proposed model."}}
{"id": "stQmPR5tp6V", "cdate": 1546300800000, "mdate": 1696344516617, "content": {"title": "Active 10: Brisk Walking to Support Regular Physical Activity", "abstract": "We describe a methodology and a technology supporting an intervention carried out by Public Health England (PHE) to encourage physically inactive people (doing less than 30 minutes' physical activity per week) to initiate regular physical activity via 10 minutes of daily brisk walking. The intervention is designed to encourage the inclusion of short bouts of continuous brisk walking in everyday activities such as shopping or commuting. To this extent a behaviour change mobile application, Active 10, was developed and distributed freely for Android and iOS. The app was downloaded over 620,000 times and our server infrastructure has collected nearly a billion data points between March 2017 and January 2019. The paper describes the rationale for Active 10, the application supporting the intervention, the data architecture and the data collection approach. Then we discuss the complexity of developing a health tracking technology with such large number of users, producing a significant volume of data. Finally, we describe a preliminary data analysis, focussing on a cohort of 129,010 users who used the app for over 8 weeks: 73% of these users achieved less than ten minutes of brisk walking per day during the first week; by the end of the 8th week this subset of users showed, on average, a 10-fold increase in brisk walking. The most inactive section of the cohort, the 54% of users who showed virtually no brisk walking activity during week 1, seems to achieve the greatest proportional increase, and by the end of week 8 they appear to meet, on average, 10 minutes of continuous brisk walking per day. The increase is more evident within the 15% of the cohort who kept the app for over six months: on average a 12% increase in average activity was observed in this group with no sign of decline."}}
{"id": "p68Tp5My75", "cdate": 1546300800000, "mdate": 1696344516608, "content": {"title": "Neural Language Model Based Training Data Augmentation for Weakly Supervised Early Rumor Detection", "abstract": "The scarcity and class imbalance of training data are known issues in current rumor detection tasks. We propose a straight-forward and general-purpose data augmentation technique which is beneficial to early rumor detection relying on event propagation patterns. The key idea is to exploit massive unlabeled event data sets on social media to augment limited labeled rumor source tweets. This work is based on rumor spreading patterns revealed by recent rumor studies and semantic relatedness between labeled and unlabeled data. A state-of-the-art neural language model (NLM) and large credibility-focused Twitter corpora are employed to learn context-sensitive representations of rumor tweets. Six different real-world events based on three publicly available rumor datasets are employed in our experiments to provide a comparative evaluation of the effectiveness of the method. The results show that our method can expand the size of an existing rumor data set nearly by 200% and corresponding social context (i.e., conversational threads) by 100% with reasonable quality. Preliminary experiments with a state-of-the-art deep learning-based rumor detection model show that augmented data can alleviate over-fitting and class imbalance caused by limited train data and can help to train complex neural networks (NNs). With augmented data, the performance of rumor detection can be improved by 12.1% in terms of F-score. Our experiments also indicate that augmented training data can help to generalize rumor detection models on unseen rumors."}}
{"id": "GeLPW96lZpH", "cdate": 1546300800000, "mdate": 1696344516618, "content": {"title": "Neural language model based training data augmentation for weakly supervised early rumor detection", "abstract": "The scarcity and class imbalance of training data are known issues in current rumor detection tasks. We propose a straight-forward and general-purpose data augmentation technique which is beneficial to early rumor detection relying on event propagation patterns. The key idea is to exploit massive unlabeled event data sets on social media to augment limited labeled rumor source tweets. This work is based on rumor spreading patterns revealed by recent rumor studies and semantic relatedness between labeled and unlabeled data. A state-of-the-art neural language model (NLM) and large credibility-focused Twitter corpora are employed to learn context-sensitive representations of rumor tweets. Six different real-world events based on three publicly available rumor datasets are employed in our experiments to provide a comparative evaluation of the effectiveness of the method. The results show that our method can expand the size of an existing rumor data set nearly by 200% and corresponding social context (i.e., conversational threads) by 100% with reasonable quality. Preliminary experiments with a state-of-the-art deep learning-based rumor detection model show that augmented data can alleviate over-fitting and class imbalance caused by limited train data and can help to train complex neural networks (NNs). With augmented data, the performance of rumor detection can be improved by 12.1% in terms of F-score. Our experiments also indicate that augmented training data can help to generalize rumor detection models on unseen rumors."}}
{"id": "Xg3mxKWA8Mj", "cdate": 1514764800000, "mdate": 1696344516635, "content": {"title": "Mapping Mobility to Support Crisis Management", "abstract": ""}}
{"id": "FZCpMzdgz7", "cdate": 1514764800000, "mdate": 1696344516612, "content": {"title": "SemRe-Rank: Improving Automatic Term Extraction by Incorporating Semantic Relatedness with Personalised PageRank", "abstract": "Automatic Term Extraction (ATE) deals with the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a \u2018one-size-fit-all\u2019 solution that may never exist, we propose to develop generic methods to \u2018enhance\u2019 existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness\u2014an often overlooked venue\u2014into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute \u2018semantic importance\u2019 scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K\u2019s), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K, or up to 17 points in F1."}}
{"id": "anyvL6utY1", "cdate": 1483228800000, "mdate": 1639496401184, "content": {"title": "Supervised learning for robust term extraction", "abstract": "We propose a machine learning method to automatically classify the extracted ngrams from a corpus into terms and non-terms. We use 10 common statistics in previous term extraction literature as features for training. The proposed method, applicable to term recognition in multiple domains and languages, can help 1) avoid the laborious work in the post-processing (e.g. subjective threshold setting); 2) handle the skewness and demonstrate noticeable resilience to domain-shift issue of training data. Experiments are carried out on 6 corpora of multiple domains and languages, including GENIA and ACLRD-TEC(1.0) corpus as training set and four TTC subcorpora of wind energy and mobile technology in both Chinese and English as test set. Promising results are found, which indicate that this approach is capable of identifying both single word terms and multiword terms with reasonably good precision and recall."}}
{"id": "9sFMU6l_e4z", "cdate": 1483228800000, "mdate": 1696344516711, "content": {"title": "SemRe-Rank: Incorporating Semantic Relatedness to Improve Automatic Term Extraction Using Personalized PageRank", "abstract": "Automatic Term Extraction deals with the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a 'one-size-fit-all' solution that may never exist, we propose to develop generic methods to 'enhance' existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness - an often overlooked venue - into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute 'semantic importance' scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K's), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K, or up to 17 points in F1."}}
{"id": "MLjfBIiaRQt", "cdate": 1451606400000, "mdate": 1696344516616, "content": {"title": "JATE 2.0: Java Automatic Term Extraction with Apache Solr", "abstract": "Automatic Term Extraction (ATE) or Recognition (ATR) is a fundamental processing step preceding many complex knowledge engineering tasks. However, few methods have been implemented as public tools and in particular, available as open-source freeware. Further, little effort is made to develop an adaptable and scalable framework that enables customization, development, and comparison of algorithms under a uniform environment. This paper introduces JATE 2.0, a complete remake of the free Java Automatic Term Extraction Toolkit (Zhang et al., 2008) delivering new features including: (1) highly modular, adaptable and scalable ATE thanks to integration with Apache Solr, the open source free-text indexing and search platform; (2) an extended collection of state-of-the-art algorithms. We carry out experiments on two well-known benchmarking datasets and compare the algorithms along the dimensions of effectiveness (precision) and efficiency (speed and memory consumption). To the best of our knowledge, this is by far the only free ATE library offering a flexible architecture and the most comprehensive collection of algorithms."}}
