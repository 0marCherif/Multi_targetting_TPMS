{"id": "VGraE4R-Nb", "cdate": 1640995200000, "mdate": 1682369582857, "content": {"title": "Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits", "abstract": "We propose a linear contextual bandit algorithm with $O(\\sqrt{dT\\log T})$ regret bound, where $d$ is the dimension of contexts and $T$ isthe time horizon. Our proposed algorithm is equipped with a novel estimator in which exploration is embedded through explicit randomization. Depending on the randomization, our proposed estimator takes contributions either from contexts of all arms or from selected contexts. We establish a self-normalized bound for our estimator, which allows a novel decomposition of the cumulative regret into \\textit{additive} dimension-dependent terms instead of multiplicative terms. We also prove a novel lower bound of $\\Omega(\\sqrt{dT})$ under our problem setting. Hence, the regret of our proposed algorithm matches the lower bound up to logarithmic factors. The numerical experiments support the theoretical guarantees and show that our proposed method outperforms the existing linear bandit algorithms."}}
{"id": "MsG_9VFGABE", "cdate": 1640995200000, "mdate": 1682369582876, "content": {"title": "Double Doubly Robust Thompson Sampling for Generalized Linear Contextual Bandits", "abstract": "We propose a novel contextual bandit algorithm for generalized linear rewards with an $\\tilde{O}(\\sqrt{\\kappa^{-1} \\phi T})$ regret over $T$ rounds where $\\phi$ is the minimum eigenvalue of the covariance of contexts and $\\kappa$ is a lower bound of the variance of rewards. In several practical cases where $\\phi=O(d)$, our result is the first regret bound for generalized linear model (GLM) bandits with the order $\\sqrt{d}$ without relying on the approach of Auer [2002]. We achieve this bound using a novel estimator called double doubly-robust (DDR) estimator, a subclass of doubly-robust (DR) estimator but with a tighter error bound. The approach of Auer [2002] achieves independence by discarding the observed rewards, whereas our algorithm achieves independence considering all contexts using our DDR estimator. We also provide an $O(\\kappa^{-1} \\phi \\log (NT) \\log T)$ regret bound for $N$ arms under a probabilistic margin condition. Regret bounds under the margin condition are given by Bastani and Bayati [2020] and Bastani et al. [2021] under the setting that contexts are common to all arms but coefficients are arm-specific. When contexts are different for all arms but coefficients are common, ours is the first regret bound under the margin condition for linear models or GLMs. We conduct empirical studies using synthetic data and real examples, demonstrating the effectiveness of our algorithm."}}
{"id": "WBVbl8POq8v", "cdate": 1621629963718, "mdate": null, "content": {"title": "Doubly Robust Thompson Sampling with Linear Payoffs", "abstract": "A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing.    \nThe dependence of the arm choice on the past context and reward pairs compounds the complexity of regret analysis.\nWe propose a novel multi-armed contextual bandit algorithm called Doubly Robust Thompson Sampling (DRTS) employing the doubly-robust estimator used in missing data literature to Thompson Sampling with contexts (\\texttt{LinTS}).\nDifferent from previous works relying on missing data techniques (Dimakopoulou et al. [2019], Kim and Paik [2019]), the proposed algorithm is designed to allow a novel additive regret decomposition leading to an improved regret bound with the order of $\\tilde{O}(\\phi^{-2}\\sqrt{T})$, where $\\phi^2$ is the minimum eigenvalue of the covariance matrix of contexts.\nThis is the first regret bound of \\texttt{LinTS} using $\\phi^2$ without $d$,  where $d$ is the dimension of the context.\nApplying the relationship between $\\phi^2$ and $d$, the regret bound of the proposed algorithm is $\\tilde{O}(d\\sqrt{T})$ in many practical scenarios, improving the bound of \\texttt{LinTS} by a factor of $\\sqrt{d}$.\nA benefit of the proposed method is that it uses all the context data, chosen or not chosen, thus allowing to circumvent the technical definition of unsaturated arms used in theoretical analysis of \\texttt{LinTS}.\nEmpirical studies show the advantage of the proposed algorithm over \\texttt{LinTS}."}}
{"id": "GALTANJkHC", "cdate": 1609459200000, "mdate": 1682369582895, "content": {"title": "Doubly Robust Thompson Sampling with Linear Payoffs", "abstract": "A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing. The dependence of the arm choice on the past context and reward pairs compounds the complexity of regret analysis.We propose a novel multi-armed contextual bandit algorithm called Doubly Robust Thompson Sampling (DRTS) employing the doubly-robust estimator used in missing data literature to Thompson Sampling with contexts (\\texttt{LinTS}).Different from previous works relying on missing data techniques (Dimakopoulou et al. [2019], Kim and Paik [2019]), the proposed algorithm is designed to allow a novel additive regret decomposition leading to an improved regret bound with the order of $\\tilde{O}(\\phi^{-2}\\sqrt{T})$, where $\\phi^2$ is the minimum eigenvalue of the covariance matrix of contexts.This is the first regret bound of \\texttt{LinTS} using $\\phi^2$ without $d$, where $d$ is the dimension of the context.Applying the relationship between $\\phi^2$ and $d$, the regret bound of the proposed algorithm is $\\tilde{O}(d\\sqrt{T})$ in many practical scenarios, improving the bound of \\texttt{LinTS} by a factor of $\\sqrt{d}$.A benefit of the proposed method is that it uses all the context data, chosen or not chosen, thus allowing to circumvent the technical definition of unsaturated arms used in theoretical analysis of \\texttt{LinTS}.Empirical studies show the advantage of the proposed algorithm over \\texttt{LinTS}."}}
