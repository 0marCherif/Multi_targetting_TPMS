{"id": "HHel9lEEEy9", "cdate": 1644606450169, "mdate": 1644606450169, "content": {"title": "Direct Loss Minimization for Sparse Gaussian Processes", "abstract": "The paper provides a thorough investigation\nof Direct Loss Minimization (DLM), which\noptimizes the posterior to minimize predictive\nloss, in sparse Gaussian processes. For the\nconjugate case, we consider DLM for log-loss\nand DLM for square loss showing a significant\nperformance improvement in both cases. The\napplication of DLM in non-conjugate cases is\nmore complex because the logarithm of expectation in the log-loss DLM objective is often\nintractable and simple sampling leads to biased estimates of gradients. The paper makes\ntwo technical contributions to address this.\nFirst, a new method using product sampling\nis proposed, which gives unbiased estimates\nof gradients (uPS) for the objective function.\nSecond, a theoretical analysis of biased Monte\nCarlo estimates (bMC) shows that stochastic\ngradient descent converges despite the biased\ngradients. Experiments demonstrate empirical success of DLM. A comparison of the\nsampling methods shows that, while uPS is\npotentially more sample-efficient, bMC provides a better tradeoff in terms of convergence\ntime and computational efficiency.\n"}}
{"id": "HYbe-WX4NJq", "cdate": 1644606201311, "mdate": 1644606201311, "content": {"title": "Weighted Meta-Learning", "abstract": "Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples. However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant. We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, \u03b1-MAML. Finally, we demonstrate empirically on several regression problems that our weighted meta-learning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML."}}
{"id": "BgZgfwGV4yc", "cdate": 1644606042182, "mdate": 1644606042182, "content": {"title": "Differentiable Feature Selection by Discrete Relaxation", "abstract": "In this paper, we introduce Differentiable Feature Selection, a gradient-based search algorithm for feature selection. Our approach extends a recent result on the estimation of learnability in the sublinear data regime by showing that the calculation can be performed iteratively (i.e. in mini-batches) and in linear time and space with respect to both the number of features D and the sample size N. This, along with a discrete-to-continuous relaxation of the search domain, allows for an efficient, gradient-based search algorithm among feature subsets for very large datasets. Our algorithm utilizes higher-order correlations between features and targets for both the N>D and N"}}
{"id": "HygJq12VtH", "cdate": 1571237767451, "mdate": null, "content": {"title": "Pseudo-Bayesian Learning via Direct Loss Minimization with Applications to Sparse Gaussian Process Models", "abstract": "We propose that approximate Bayesian algorithms should optimize a new criterion, directly derived from the loss, to calculate their approximate posterior which we refer to as pseudo-posterior. Unlike standard variational inference which optimizes a lower bound on the log marginal likelihood, the new algorithms can be analyzed to provide loss guarantees on the predictions with the pseudo-posterior. Our criterion can be used to derive new sparse Gaussian process algorithms that have error guarantees applicable to various likelihoods."}}
{"id": "H1WfydW_-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Probabilistic Matrix Factorization for Automated Machine Learning", "abstract": "In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible ML pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art."}}
{"id": "rkbXoD-uWr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models", "abstract": "Bayesian models are established as one of the main successful paradigms for complex problems in machine learning. To handle intractable inference, research in this area has developed new approximation methods that are fast and effective. However, theoretical analysis of the performance of such approximations is not well developed. The paper furthers such analysis by providing bounds on the excess risk of variational inference algorithms and related regularized loss minimization algorithms for a large class of latent variable models with Gaussian latent variables. We strengthen previous results for variational algorithms by showing they are competitive with any point-estimate predictor. Unlike previous work, we also provide bounds on the risk of the \\emph{Bayesian} predictor and not just the risk of the Gibbs predictor for the same approximate posterior. The bounds are applied in complex models including sparse Gaussian processes and correlated topic models. Theoretical results are complemented by identifying novel approximations to the Bayesian objective that attempt to minimize the risk directly. An empirical evaluation compares the variational and new algorithms shedding further light on their performance."}}
{"id": "H14O12WuWH", "cdate": 1420070400000, "mdate": null, "content": {"title": "Sparse Variational Inference for Generalized GP Models", "abstract": "Gaussian processes (GP) provide an attractive machine learning model due to their non-parametric form, their flexibility to capture many types of observation data, and their generic inference proce..."}}
