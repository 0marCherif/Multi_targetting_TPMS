{"id": "bWDcAidgC9", "cdate": 1702820604772, "mdate": 1702820604772, "content": {"title": "Improving Continual Learning of Acoustic Scene Classification via Mutual Information Optimization", "abstract": "Continual learning, which aims to incrementally accumulate knowledge, has been an increasingly significant but challenging research topic for deep models that are prone to catastrophic forgetting. In this paper, we propose a novel replay-based continual learning approach in the context of class-incremental learning in acoustic scene classification, to classify audio recordings into an expanding set of classes that characterize the acoustic scenes. Our approach is improving both the modeling and memory selection mechanism via mutual information optimization in continual learning. By regarding incremental classes of acoustic scenes as different tasks, our model is expected to learn both task-agnostic and task-specific knowledge by replaying representative and informative samples. This optimization also enables the model to utilize past knowledge effectively and learn from new information during continual learning. We demonstrate that our approach has a superior performance compared to existing methods on multiple datasets and continual learning evaluation metrics."}}
{"id": "zxiJcyN4z0", "cdate": 1672531200000, "mdate": 1688690735162, "content": {"title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization", "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper"}}
{"id": "yuD7OVYFvLe", "cdate": 1672531200000, "mdate": 1695984353328, "content": {"title": "Semi-Autoregressive Streaming ASR With Label Context", "abstract": "Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming \"semi-autoregressive\" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming NAR model by 19% relative on Tedlium2, 16%/8% on Librispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB) / Callhome(CH) test sets. It also reduced the accuracy gap with streaming AR and non-streaming NAR models while achieving 2.5x lower latency. We also demonstrate that our approach can effectively utilize external text data to pre-train the LM subnetwork to further improve streaming ASR accuracy."}}
{"id": "yixK0L0Zf-y", "cdate": 1672531200000, "mdate": 1681484261990, "content": {"title": "PAAPLoss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement", "abstract": ""}}
{"id": "xDUS4eBiys", "cdate": 1672531200000, "mdate": 1682335934014, "content": {"title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit", "abstract": "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet."}}
{"id": "v9xB8AgiiIO", "cdate": 1672531200000, "mdate": 1681484262241, "content": {"title": "STFT-Domain Neural Speech Enhancement With Very Low Algorithmic Latency", "abstract": ""}}
{"id": "v8tL763SYm1", "cdate": 1672531200000, "mdate": 1681484261996, "content": {"title": "Improving Massively Multilingual ASR With Auxiliary CTC Objectives", "abstract": ""}}
{"id": "uJqKoWrA1B", "cdate": 1672531200000, "mdate": 1695984353204, "content": {"title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit", "abstract": "Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol\u00e1k, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). 2023."}}
{"id": "tqhTFVmTx5ZJ", "cdate": 1672531200000, "mdate": 1695984353350, "content": {"title": "Bayes Risk Transducer: Transducer with Controllable Alignment Prediction", "abstract": "Automatic speech recognition (ASR) based on transducers is widely used. In training, a transducer maximizes the summed posteriors of all paths. The path with the highest posterior is commonly defined as the predicted alignment between the speech and the transcription. While the vanilla transducer does not have a prior preference for any of the valid paths, this work intends to enforce the preferred paths and achieve controllable alignment prediction. Specifically, this work proposes Bayes Risk Transducer (BRT), which uses a Bayes risk function to set lower risk values to the preferred paths so that the predicted alignment is more likely to satisfy specific desired properties. We further demonstrate that these predicted alignments with intentionally designed properties can provide practical advantages over the vanilla transducer. Experimentally, the proposed BRT saves inference cost by up to 46% for non-streaming ASR and reduces overall system latency by 41% for streaming ASR."}}
{"id": "tZ9q1LPHLE7", "cdate": 1672531200000, "mdate": 1681484262105, "content": {"title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech", "abstract": ""}}
