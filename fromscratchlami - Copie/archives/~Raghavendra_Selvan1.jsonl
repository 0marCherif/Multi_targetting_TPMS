{"id": "Ar_vcqDyTxG", "cdate": 1664314532893, "mdate": null, "content": {"title": "DeepStruc: Towards structure solution from pair distribution function data using deep generative models", "abstract": "Structure solution of nanostructured materials that have limited long-range order remains a bottleneck in materials development. We present a deep learning algorithm, DeepStruc, that can solve a simple nanoparticle structure directly from a Pair Distribution Function (PDF) obtained from total scattering data by using a conditional variational autoencoder. We first apply DeepStruc to PDFs from seven different structure types of monometallic nanoparticles, and show that structures can be solved from both simulated and experimental PDFs, including PDFs from nanoparticles that are not present in the training distribution. We also apply DeepStruc to a system of hcp, fcc and stacking faulted nanoparticles, where DeepStruc recognizes stacking faulted nanoparticles as an interpolation between hcp and fcc nanoparticles and is able to solve stacking faulted structures from PDFs. Our findings suggests that DeepStruc is a step towards a general approach for structure solution of nanomaterials. "}}
{"id": "JOT6D9OFdOK", "cdate": 1664314532297, "mdate": null, "content": {"title": "Extracting Structural Motifs from Pair Distribution Function Data of Nanostructures using Explainable Machine Learning", "abstract": "Characterization of material structure with X-ray or neutron scattering using e.g. Pair Distribution Function (PDF) analysis most often rely on refining a structure model against an experimental dataset. However, identifying a suitable model is often a bottleneck. Recently, new automated approaches have made it possible to test thousands of models for each dataset, but these methods are computationally expensive, and analysing the output, i.e., extracting structural information from the resulting fits in a meaningful way is challenging. Our Machine Learning based Motif Extractor (ML-MotEx) trains an ML algorithm on thousands of fits, and uses SHAP (SHapley Additive exPlanation) values to identify which model features are important for the fit quality. We use the method for 4 different chemical systems including disordered nanomaterials and clusters. ML-MotEx opens for a new type of modelling where each feature in a model is assigned an importance value for the fit quality based on explainable ML."}}
{"id": "35PLkGkJOQ4", "cdate": 1663850297473, "mdate": null, "content": {"title": "Energy Consumption-Aware Tabular Benchmarks for Neural Architecture Search", "abstract": "The demand for large-scale computational resources for  Neural Architecture Search (NAS) has been lessened by tabular benchmarks for NAS. Evaluating NAS strategies is now possible on extensive search spaces and at a moderate computational cost. But so far, NAS has mainly focused on maximising performance on some hold-out validation/test set. However, energy consumption is a partially conflicting objective that should not be neglected. We hypothesise that constraining NAS to include the energy consumption of training the models could reveal a sub-space of undiscovered architectures that are more computationally efficient with a smaller carbon footprint. To support the hypothesis, an existing tabular benchmark for NAS is augmented with the energy consumption of each architecture. We then perform multi-objective optimisation that includes energy consumption as an additional objective. We demonstrate the usefulness of multi-objective NAS for uncovering the trade-off between performance and energy consumption as well as for finding more energy-efficient architectures. The updated tabular benchmark is open-sourced to encourage the further exploration of energy consumption-aware NAS."}}
{"id": "rRGzrXChq0F", "cdate": 1643986461382, "mdate": null, "content": {"title": "Identifying Partial Mouse Brain Microscopy Images from the Allen Reference Atlas using a Contrastively Learned Semantic Space", "abstract": "Registering mouse brain microscopy images to a reference atlas can help determine the locations of anatomical structures in the brain, which is an essential step for understanding the function of brain circuits. Most existing registration pipelines assume the identity of the reference plate -- to which the image slice is to be registered -- is known beforehand. This might not always be the case due to three main challenges in microscopy image data: missing image regions (partial data), different cutting angles compared to the atlas plates and a large number of high-resolution images to be identified. Manual identification of reference plates as an initial step can be biased, tedious and resource intensive. On the other hand, registering images to all atlas plates can be slow, limiting the application of automated registration methods when dealing with high-resolution image data. This work proposes to perform the image identification by learning a low-dimensional space that captures the similarity between microscopy images and the reference atlas plates. We employ Convolutional Neural Networks (CNNs), in the Siamese network configuration, to first obtain low-dimensional embeddings of microscopy image data and atlas plates. These embeddings are contrasted with positive and negative examples in order to learn a semantically meaningful space that can be used for identifying corresponding 2D atlas plates. At inference, atlas plates that are closest to the microscopy image data in the learned embedding space are presented as candidates for registration. Our method achieved TOP-3 and TOP-5 accuracy of 83.3% and 100%, respectively, compared to the SimpleElastix-based baseline which obtained 25% in both the Top-3 and Top-5 accuracy."}}
{"id": "89Qoxo1kqZa", "cdate": 1639154683353, "mdate": null, "content": {"title": "Interpreting Latent Spaces of Generative Models for Medical Images using Unsupervised Methods", "abstract": "Generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) play an increasingly important role in medical image analysis. They are used to synthesize, de-noise, super-resolve, and augment medical images. The latent spaces of these models often show semantically meaningful directions corresponding to human-interpretable image transformations. However, until now, their exploration for medical images has been limited due to the requirement of supervised data. Recently, several methods for unsupervised discovery of interpretable directions in GAN latent spaces have shown interesting results on natural images. This work explores the potential of applying these techniques on medical images by training a deep convolutional GAN and a VAE on thoracic CT scans and using an unsupervised method to discover interpretable directions in the resulting latent space. We find several directions corresponding to non-trivial image transformations, such as rotation or breast size, as well as directions showing that the generative models capture 3D structure despite being presented only with two-dimensional data. The results show that unsupervised methods to discover interpretable directions in generative model latent spaces generalize to VAEs and can be applied to medical images. This could open a wide array of future work using these methods in medical image analysis."}}
{"id": "swvVpnzro9q", "cdate": 1639123014081, "mdate": null, "content": {"title": "Hybrid Ladder Transformers with Efficient Parallel-Cross Attention for Medical Image Segmentation", "abstract": "Most existing transformer-based network architectures for computer vision tasks are large (in number of parameters) and  require large-scale datasets for training. However, the relatively small number of data samples in medical imaging compared to the datasets for vision applications makes it difficult to effectively train transformers for medical imaging applications. Further, transformer-based architectures encode long-range dependencies in the data and are able to learn more global representations. This could bridge the gap with convolutional neural networks (CNNs), which primarily operate on features extracted in local image neighbourhoods. In this work, we present a hybrid transformer-based approach for segmentation of medical images that works in conjunction with a CNN. We propose to use learnable global attention heads along with the traditional convolutional segmentation network architecture to encode long-range dependencies. Specifically, in our proposed architecture the local information extracted by the convolution operations and the global information learned by the self-attention mechanisms are fused using bi-directional cross attention during the encoding process, resulting in what we call a {\\em hybrid ladder transformer} (HyLT). We evaluate the proposed network on two different medical image segmentation datasets.  The results show that it achieves better results than the relevant CNN- and transformer-based architectures."}}
{"id": "PLSdnHPx-W6", "cdate": 1617706861188, "mdate": null, "content": {"title": "Deep ensemble model for segmenting microscopy images in the presence of limited labeled data", "abstract": "Obtaining large amounts of high quality labeled microscopy data is expensive and time-consuming. To overcome this issue, we propose a deep ensemble model which aims to utilise limited labeled training data. We train multiple identical Convolutional Neural Network (CNN) segmentation models on training data that is partitioned into folds in two steps. First, the data is split based on sample diversity or expert knowledge reflecting the possible {\\em modes} of the underlying data distribution. In the second step, these partitions are split into random folds like in a cross-validation setting. Segmentation models based on the U-net architecture are trained on each of these resulting folds yielding the candidate models for our deep ensemble model which are aggregated to obtain the final prediction. The proposed deep ensemble model is compared to relevant baselines, in their ability to segment interneurons in microscopic images of mice spinal cord, showing improved performance on an independent test set."}}
{"id": "1TPRpNyyj2L", "cdate": 1617196556767, "mdate": null, "content": {"title": "Carbon footprint driven deep learning model selection for medical imaging", "abstract": "Selecting task appropriate deep learning models is a resource intensive process; more so when working with large quantities of high dimensional data that are encountered in medical imaging. Model selection procedures that are primarily aimed at improving performance measures such as accuracy could become biased towards resource intensive models. In this work, we propose to inform and drive the model selection procedure using the carbon footprint of training deep learning models as a complementary measure along with other standard performance metrics. We experimentally demonstrate that increasing carbon footprint of large models might not necessarily translate into proportional performance gains, and suggest useful trade-offs to obtain resource efficient models."}}
{"id": "jjjvJpB4up", "cdate": 1609459200000, "mdate": 1623603377888, "content": {"title": "Dynamic \u03b2-VAEs for quantifying biodiversity by clustering optically recorded insect signals", "abstract": "While insects are the largest and most diverse group of terrestrial animals, constituting ca. 80% of all known species, they are difficult to study due to their small size and similarity between species. Conventional monitoring techniques depend on time consuming trapping methods and tedious microscope-based work by skilled experts in order to identify the caught insect specimen at species, or even family level. Researchers and policy makers are in urgent need of a scalable monitoring tool in order to conserve biodiversity and secure human food production due to the rapid decline in insect numbers. In order to improve upon existing insect clustering methods, we propose an adaptive variant of the variational autoencoder (VAE) which is capable of clustering data by phylogenetic groups. The proposed dynamic beta-VAE dynamically adapts the scaling of the reconstruction and regularization loss terms (beta value) yielding useful latent representations of the input data. We demonstrate the usefulness of the dynamic beta-VAE on optically recorded insect signals from regions of southern Scandinavia to cluster unlabelled targets into possible species. We also demonstrate improved clustering performance in a semi-supervised setting using a small subset of labelled data. These experimental results, in both unsupervised- and semi-supervised settings, with the dynamic beta-VAE are promising and, in the near future, can be deployed to monitor insects and conserve the rapidly declining insect biodiversity."}}
{"id": "U8f2flEHvQS", "cdate": 1609459200000, "mdate": 1623603377880, "content": {"title": "Detection of foraging behavior from accelerometer data using U-Net type convolutional networks", "abstract": "Highlights \u2022 Narwhals may not always make big jerks when foraging like harbor porpoises or sperm whales, hence their hunting pattern might differ from them. \u2022 Reliable buzz detectors are derived from high-frequency-sampling, back-mounted accelerometer using state-of-the-art machine learning algorithms. \u2022 Deep learning is showed to be a superior algorithm to learn patterns from accelerometer data. Abstract Narwhal (Monodon monoceros) is one of the most elusive marine mammals, due to its isolated habitat in the Arctic region. Tagging is a technology that has the potential to explore the activities of this species, where behavioral information can be collected from instrumented individuals. This includes accelerometer data, diving and acoustic data as well as GPS positioning. An essential element in understanding the ecological role of toothed whales is to characterize their feeding behavior and estimate the amount of food consumption. Buzzes are sounds emitted by toothed whales that are related directly to the foraging behaviors. It is therefore of interest to measure or estimate the rate of buzzing to estimate prey intake. The main goal of this paper is to find a way to detect prey capture attempts directly from accelerometer data, and thus be able to estimate food consumption without the need for the more demanding acoustic data. We develop three automated buzz detection methods based on accelerometer and depth data solely. We use a dataset from five narwhals instrumented in East Greenland in 2018 to train, validate and test a logistic regression model and the state-of-the art machine learning algorithms random forest and deep learning, using the buzzes detected from acoustic data as the ground truth. The deep learning algorithm performed best among the tested methods. We conclude that reliable buzz detectors can be derived from high-frequency-sampling, back-mounted accelerometer tags, thus providing an alternative tool for studies of foraging ecology of marine mammals in their natural environments. We also compare buzz detection with certain movement patterns, such as sudden changes in acceleration (jerks), found in other marine mammal species for estimating prey capture. We find that narwhals do not seem to make big jerks when foraging and conclude that their hunting patterns in that respect might differ from other marine mammals."}}
