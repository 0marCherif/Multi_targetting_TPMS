{"id": "qnRlh8BdMY", "cdate": 1675827737137, "mdate": null, "content": {"title": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns", "abstract": "We present TabRet, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called retokenizing, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains. The code is available at https://github.com/pfnet-research/tabret."}}
{"id": "dYDEg9cs-t", "cdate": 1672531200000, "mdate": 1681654366934, "content": {"title": "TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns", "abstract": ""}}
{"id": "WGfppY9Yx7", "cdate": 1640995200000, "mdate": 1681654366931, "content": {"title": "Universal approximation property of invertible neural networks", "abstract": ""}}
{"id": "tUUW27dwEqj", "cdate": 1609459200000, "mdate": 1681654367102, "content": {"title": "Fast Estimation Method for the Stability of Ensemble Feature Selectors", "abstract": ""}}
{"id": "vtSfZkUD8wY", "cdate": 1577836800000, "mdate": 1681654367102, "content": {"title": "Graph Neural Networks Exponentially Lose Expressive Power for Node Classification", "abstract": ""}}
{"id": "_hqiKCc4lCJ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators", "abstract": "Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself."}}
{"id": "Y8LsNmKyC0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Weisfeiler-Lehman Embedding for Molecular Graph Neural Networks", "abstract": "A graph neural network (GNN) is a good choice for predicting the chemical properties of molecules. Compared with other deep networks, however, the current performance of a GNN is limited owing to the \"curse of depth.\" Inspired by long-established feature engineering in the field of chemistry, we expanded an atom representation using Weisfeiler-Lehman (WL) embedding, which is designed to capture local atomic patterns dominating the chemical properties of a molecule. In terms of representability, we show WL embedding can replace the first two layers of ReLU GNN -- a normal embedding and a hidden GNN layer -- with a smaller weight norm. We then demonstrate that WL embedding consistently improves the empirical performance over multiple GNN architectures and several molecular graph datasets."}}
{"id": "QU416G1lUmU", "cdate": 1577836800000, "mdate": 1631157560677, "content": {"title": "Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators", "abstract": "Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself."}}
{"id": "OfpgjKgGLl", "cdate": 1577836800000, "mdate": 1681654366935, "content": {"title": "Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks", "abstract": ""}}
{"id": "DiVC9Un-HK", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks", "abstract": "It is known that the current graph neural networks (GNNs) are difficult to make themselves deep due to the problem known as over-smoothing. Multi-scale GNNs are a promising approach for mitigating the over-smoothing problem. However, there is little explanation of why it works empirically from the viewpoint of learning theory. In this study, we derive the optimization and generalization guarantees of transductive learning algorithms that include multi-scale GNNs. Using the boosting theory, we prove the convergence of the training error under weak learning-type conditions. By combining it with generalization gap bounds in terms of transductive Rademacher complexity, we show that a test error bound of a specific type of multi-scale GNNs that decreases corresponding to the number of node aggregations under some conditions. Our results offer theoretical explanations for the effectiveness of the multi-scale structure against the over-smoothing problem. We apply boosting algorithms to the training of multi-scale GNNs for real-world node prediction tasks. We confirm that its performance is comparable to existing GNNs, and the practical behaviors are consistent with theoretical observations. Code is available at https://github.com/delta2323/GB-GNN."}}
