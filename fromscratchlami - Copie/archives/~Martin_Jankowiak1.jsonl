{"id": "_h4cqWBv7RQ", "cdate": 1675348217257, "mdate": 1675348217257, "content": {"title": "Deep Sigma Point Processes", "abstract": "We introduce Deep Sigma Point Processes, a class of parametric models inspired by the compositional structure of Deep Gaussian Processes (DGPs). Deep Sigma Point Processes (DSPPs) retain many of the attractive features of (variational) DGPs, including mini-batch training and predictive uncertainty that is controlled by kernel basis functions. Importantly, since DSPPs admit a simple maximum likelihood inference procedure, the resulting predictive distributions are not degraded by any posterior approximations. In an extensive empirical comparison on univariate and multivariate regression tasks we find that the resulting predictive distributions are significantly better calibrated than those obtained with other probabilistic methods for scalable regression, including variational DGPs--often by as much as a nat per datapoint."}}
{"id": "lZZb159SPn", "cdate": 1675348142443, "mdate": 1675348142443, "content": {"title": "Parametric Gaussian Process Regressors", "abstract": "The combination of inducing point methods with stochastic variational inference has enabled approximate Gaussian Process (GP) inference on large datasets. Unfortunately, the resulting predictive distributions often exhibit substantially underestimated uncertainties. Notably, in the regression case the predictive variance is typically dominated by observation noise, yielding uncertainty estimates that make little use of the input-dependent function uncertainty that makes GP priors attractive. In this work we propose two simple methods for scalable GP regression that address this issue and thus yield substantially improved predictive uncertainties. The first applies variational inference to FITC (Fully Independent Training Conditional; Snelson et.~al.~2006). The second bypasses posterior approximations and instead directly targets the posterior predictive distribution. In an extensive empirical comparison with a number of alternative methods for scalable GP regression, we find that the resulting predictive distributions exhibit significantly better calibrated uncertainties and higher log likelihoods--often by as much as half a nat per datapoint."}}
{"id": "Od7OEyf8PcU", "cdate": 1652125708274, "mdate": 1652125708274, "content": {"title": "Fast Matrix Square Roots with Applications to Gaussian Processes and Bayesian Optimization", "abstract": "Matrix square roots and their inverses arise frequently in machine learning, eg, when sampling from high-dimensional Gaussians N (0, K) or \u201cwhitening\u201d a vector b against covariance matrix K. While existing methods typically require O (N^ 3) computation, we introduce a highly-efficient quadratic-time algorithm for computing K^{1/2} b, K^{-1/2} b, and their derivatives through matrix-vector multiplication (MVMs). Our method combines Krylov subspace methods with a rational approximation and typically achieves 4 decimal places of accuracy with fewer than 100 MVMs. Moreover, the backward pass requires little additional computation. We demonstrate our method's applicability on matrices as large as 50,000 by 50,000-well beyond traditional methods-with little approximation error. Applying this increased scalability to variational Gaussian processes, Bayesian optimization, and Gibbs sampling results in more powerful models with higher accuracy. In particular, we perform variational GP inference with up to 10,000 inducing points and perform Gibbs sampling on a 25,000-dimensional problem.\n"}}
{"id": "B53PimTUnpX", "cdate": 1652125372265, "mdate": 1652125372265, "content": {"title": "High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces", "abstract": "Bayesian optimization (BO) is a powerful paradigm for efficient optimization of black-box objective functions. High-dimensional BO presents a particular challenge, in part because the curse of dimensionality makes it difficult to define\u2014as well as do inference over\u2014a suitable class of surrogate models. We argue that Gaussian process surrogate models defined on sparse axis-aligned subspaces offer an attractive compromise between flexibility and parsimony. We demonstrate that our approach, which relies on Hamiltonian Monte Carlo for inference, can rapidly identify sparse subspaces relevant to modeling the unknown objective function, enabling sample-efficient high-dimensional BO. In an extensive suite of experiments comparing to existing methods for high-dimensional BO we demonstrate that our algorithm, Sparse AxisAligned Subspace BO (SAASBO), achieves excellent performance on several synthetic and real-world problems without the need to set problem-specific hyperparameters."}}
{"id": "SkxwUDLdPH", "cdate": 1569380174553, "mdate": null, "content": {"title": "Pyro: Deep Universal Probabilistic Programming", "abstract": "Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional\nmodels, Pyro uses stochastic variational inference algorithms and probability distributions\nbuilt on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library\nof composable building blocks for modifying the behavior of probabilistic programs."}}
{"id": "HkecHuIaUS", "cdate": 1568659521936, "mdate": null, "content": {"title": "Functional Tensors for Probabilistic Programming", "abstract": "It is a significant challenge to design probabilistic programming systems that can accommodate a wide variety of inference strategies within a unified framework. Noting that the versatility of modern automatic differentiation frameworks is based in large part on the unifying concept of tensors, we describe a software abstraction, functional tensors, that captures many of the benefits of tensors, while also being able to describe continuous probability distributions. We demonstrate the versatility of functional tensors by integrating them into the modeling frontend and inference backend of the Pyro probabilistic programming language. As an example application, we perform approximate inference on a switching linear dynamical system."}}
{"id": "H1g1niFhIB", "cdate": 1568607143324, "mdate": null, "content": {"title": "Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro", "abstract": "NumPyro is a lightweight library that provides an alternate NumPy backend to the Pyro probabilistic programming language with the same modeling interface, language primitives and effect handling abstractions. Effect handlers allow Pyro's modeling API to be extended to NumPyro despite its being built atop a fundamentally different JAX-based functional backend. In this work, we demonstrate the power of composing Pyro's effect handlers with the program transformations that enable hardware acceleration, automatic differentiation, and vectorization in JAX. In particular, NumPyro provides an iterative formulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT compiled, yielding an implementation that is much faster than existing alternatives in both the small and large dataset regimes."}}
{"id": "SkEMrsbu-B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Tensor Variable Elimination for Plated Factor Graphs", "abstract": "A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a comp..."}}
{"id": "SJ4Cw3WOZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Pathwise Derivatives Beyond the Reparameterization Trick", "abstract": "We observe that gradients computed via the reparameterization trick are in direct correspondence with solutions of the transport equation in the formalism of optimal transport. We use this perspect..."}}
