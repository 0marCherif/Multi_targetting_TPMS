{"id": "xBMe63cDf2", "cdate": 1704067200000, "mdate": 1707254973713, "content": {"title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions", "abstract": "We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training."}}
{"id": "1MpcWOr58N", "cdate": 1698796800000, "mdate": 1701872610645, "content": {"title": "Aoba_v3 bot: a multimodal chatbot system combining rules and various response generation models", "abstract": "In this paper, we present a multimodal dialogue system combining a neural response generation mechanism, a reranking mechanism, and a rule-based avatar control mechanism. Our system was submitted t..."}}
{"id": "nrvtnXa_BJs", "cdate": 1686207612183, "mdate": 1686207612183, "content": {"title": "Examining the effect of whitening on static and contextualized word embeddings", "abstract": "Static word embeddings (SWE) and contextualized word embeddings (CWE) are the foundation of modern natural language processing. However, these embeddings suffer from spatial bias in the form of anisotropy, which has been demonstrated to reduce their performance. A method to alleviate the anisotropy is the \u201cwhitening\u201d transformation. Whitening is a standard method in signal processing and other areas, however, its effect on SWE and CWE is not well understood. In this study, we conduct an experiment to elucidate the effect of whitening on SWE and CWE. The results indicate that whitening predominantly removes the word frequency bias in SWE, and biases other than the word frequency bias in CWE."}}
{"id": "zb0REeHV99", "cdate": 1682899200000, "mdate": 1687413984458, "content": {"title": "Extracting representative subset from extensive text data for training pre-trained language models", "abstract": ""}}
{"id": "rIj_VjD1ywU", "cdate": 1682899200000, "mdate": 1687413984481, "content": {"title": "Examining the effect of whitening on static and contextualized word embeddings", "abstract": ""}}
{"id": "ptQELaTWA7T", "cdate": 1672531200000, "mdate": 1707254973716, "content": {"title": "Investigating the Effectiveness of Multiple Expert Models Collaboration", "abstract": ""}}
{"id": "nq1F-IddLnj", "cdate": 1672531200000, "mdate": 1701872610648, "content": {"title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism", "abstract": "Large language models (LLMs) take advantage of step-by-step reasoning instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their ability to perform CoT-style reasoning robustly is of interest from a probing perspective. In this study, we inspect the step-by-step reasoning ability of LLMs with a focus on negation, which is a core linguistic phenomenon that is difficult to process. In particular, we introduce several controlled settings (e.g., reasoning in case of fictional entities) to evaluate the logical reasoning abilities of the models. We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible ->implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family."}}
{"id": "kYYU5VJAlV", "cdate": 1672531200000, "mdate": 1688012793179, "content": {"title": "Exploring the Robustness of Large Language Models for Solving Programming Problems", "abstract": "Using large language models (LLMs) for source code has recently gained attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have been shown to be highly capable of solving a wide range of programming problems. However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet. To explore this research question, we conduct experiments to understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series models, capable of tackling code generation tasks in introductory programming problems. Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance. Furthermore, we observe that Codex relies on variable names, as randomized variables decrease the solved rate significantly. However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT, show higher robustness to superficial modifications and have an outstanding capability for solving programming problems. This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high-quality code generation, while the SOTA models are becoming more robust to perturbations."}}
{"id": "ghRPIx5lumJ", "cdate": 1672531200000, "mdate": 1702385801960, "content": {"title": "SKIM at WMT 2023 General Translation Task", "abstract": ""}}
{"id": "eq2VYGy3JcF", "cdate": 1672531200000, "mdate": 1691447450817, "content": {"title": "Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications", "abstract": ""}}
