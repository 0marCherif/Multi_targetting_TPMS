{"id": "fo3b0XjTkU", "cdate": 1664731448880, "mdate": null, "content": {"title": "Annealed Training for Combinatorial Optimization on Graphs", "abstract": "Learning neural networks for CO problems is notoriously difficult given the lack of labeled data as the training gets trapped easily at local optima. However, the hardness of combinatorial optimization (CO) problems hinders collecting solutions for supervised learning. We propose a simple but effective unsupervised annealed training framework for CO problems in this work. In particular, we transform CO problems into unbiased energy-based models (EBMs). We carefully selected the penalties terms to make the EBMs as smooth as possible. Then we train graph neural networks to approximate the EBMs and we introduce an annealed loss function to prevent the training from being stuck at local optima near the initialization.\nAn experimental evaluation demonstrates that our annealed training framework obtains substantial improvements. In four types of CO problems, our method achieves performance substantially better than other unsupervised neural methods on both synthetic and real-world graphs."}}
{"id": "YHCR6CFAK6v", "cdate": 1663850208433, "mdate": null, "content": {"title": "Annealed Training for Combinatorial Optimization on Graphs", "abstract": "The hardness of combinatorial optimization (CO) problems hinders collecting solutions for supervised learning. However, learning neural networks for CO problems is notoriously difficult in lack of the labeled data as the training is easily trapped at local optima. In this work, we propose a simple but effective annealed training framework for CO problems. In particular, we transform CO problems into the smoothest unbiased energy-based models (EBMs) by adding carefully selected penalties, then train graph neural networks to approximate the EBMs. We prevent the training from being stuck at local optima near the initialization by introducing an annealed loss function.\nAn experimental evaluation demonstrates that our annealed training framework obtains substantial improvements. In four types of CO problems, our method achieves performance substantially better than other unsupervised neural methods on both synthetic and real-world graphs."}}
{"id": "fYzLpCsGZVf", "cdate": 1663850200057, "mdate": null, "content": {"title": "On Accelerated Perceptrons and Beyond", "abstract": "The classical Perceptron algorithm of Rosenblatt can be used to find a linear threshold function to correctly classify $n$ linearly separable data points, assuming the classes are separated by some margin $\\gamma > 0$. A foundational result is that Perceptron converges after  $\\Omega(1/\\gamma^{2})$ iterations. There have been several recent works that managed to improve this rate by a quadratic factor, to $\\Omega(\\sqrt{\\log n}/\\gamma)$, with more sophisticated algorithms. In this paper, we unify these existing results under one framework by showing that they can all be described through the lens of solving min-max problems using modern acceleration techniques, mainly through \\emph{optimistic} online learning.  We then show that the proposed framework also leads to improved results for a series of problems beyond the standard Perceptron setting. Specifically, a) for the margin maximization problem, we improve the state-of-the-art result from $O(\\log t/t^2)$ to $O(1/t^2)$, where $t$ is the number of iterations; b) we provide the first result on identifying the implicit bias property of the classical Nesterov's accelerated gradient descent (NAG) algorithm, and show NAG can maximize the margin with an $O(1/t^2)$ rate; c) for the classical $p$-norm Perceptron problem, we provide an algorithm with $\\Omega(\\sqrt{(p-1)\\log n}/\\gamma)$ convergence rate, while existing algorithms suffer the $\\Omega({(p-1)}/\\gamma^2)$ convergence rate."}}
{"id": "aYUroxDEnN", "cdate": 1640995200000, "mdate": 1683941315633, "content": {"title": "On Accelerated Perceptrons and Beyond", "abstract": "The classical Perceptron algorithm of Rosenblatt can be used to find a linear threshold function to correctly classify $n$ linearly separable data points, assuming the classes are separated by some margin $\\gamma > 0$. A foundational result is that Perceptron converges after $\\Omega(1/\\gamma^{2})$ iterations. There have been several recent works that managed to improve this rate by a quadratic factor, to $\\Omega(\\sqrt{\\log n}/\\gamma)$, with more sophisticated algorithms. In this paper, we unify these existing results under one framework by showing that they can all be described through the lens of solving min-max problems using modern acceleration techniques, mainly through optimistic online learning. We then show that the proposed framework also lead to improved results for a series of problems beyond the standard Perceptron setting. Specifically, a) For the margin maximization problem, we improve the state-of-the-art result from $O(\\log t/t^2)$ to $O(1/t^2)$, where $t$ is the number of iterations; b) We provide the first result on identifying the implicit bias property of the classical Nesterov's accelerated gradient descent (NAG) algorithm, and show NAG can maximize the margin with an $O(1/t^2)$ rate; c) For the classical $p$-norm Perceptron problem, we provide an algorithm with $\\Omega(\\sqrt{(p-1)\\log n}/\\gamma)$ convergence rate, while existing algorithms suffer the $\\Omega({(p-1)}/\\gamma^2)$ convergence rate."}}
