{"id": "fbiGerKXM6Q", "cdate": 1676070140625, "mdate": null, "content": {"title": "PF-ABGen: A Reliable and Efficient Antibody Generator via Poisson Flow", "abstract": "An antibody is a special type of protein in the immune system to recognize and neutralize pathogenic targets, including bacteria and viruses. Antibody design is therefore valuable for the development of new therapeutics, while experimental based methods are generally inefficient and expensive. Despite the fruitful progress in protein design with generative neural networks, including diffusion models, they still suffer from high computational costs. In this paper, we propose Poisson Flow based AntiBody Generator (PF-ABGen), a novel antibody structure and sequence designer. We adopt the protein structure representation with torsion and bond angles, which allows us to represent the conformations more elegantly, and take advantage of the efficient sampling procedure of the Poisson Flow Generative Model. Our computational experiments demonstrate that PF-ABGen can generate natural and realistic antibodies in an efficient and reliable way. Notably, PF-ABGen can also be applied to antibody design with variable lengths."}}
{"id": "541PxiEKN3F", "cdate": 1632875476559, "mdate": null, "content": {"title": "Acceleration of Federated Learning with Alleviated Forgetting in Local Training", "abstract": "Federated learning (FL) enables distributed optimization of machine learning models while protecting privacy by independently training local models on each client and then aggregating parameters on a central server, thereby producing an effective global model. Although a variety of FL algorithms have been proposed, their training efficiency remains low when the data are not independently and identically distributed (non-i.i.d.) across different clients. We observe that the slow convergence rates of the existing methods are (at least partially) caused by the catastrophic forgetting issue during the local training stage on each individual client, which leads to a large increase in the loss function concerning the previous training data provided at other clients. Here, we propose FedReg, an algorithm to accelerate FL with alleviated knowledge forgetting in the local training stage by regularizing locally trained parameters with the loss on generated pseudo data, which encode the knowledge of previous training data learned by the global model. Our comprehensive experiments demonstrate that FedReg not only significantly improves the convergence rate of FL, especially when the neural network architecture is deep and the clients' data are extremely non-i.i.d., but is also able to protect privacy better in classification problems and more robust against gradient inversion attacks."}}
