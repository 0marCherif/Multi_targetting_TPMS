{"id": "gnJFs54EEs", "cdate": 1672531200000, "mdate": 1681728366540, "content": {"title": "Archetypal Analysis++: Rethinking the Initialization Strategy", "abstract": "Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones."}}
{"id": "MKXL_cN7THv", "cdate": 1609459200000, "mdate": 1681728434083, "content": {"title": "Principled Interpolation in Normalizing Flows", "abstract": "Generative models based on normalizing flows are very successful in modeling complex data distributions using simpler ones. However, straightforward linear interpolations show unexpected side effects, as interpolation paths lie outside the area where samples are observed. This is caused by the standard choice of Gaussian base distributions and can be seen in the norms of the interpolated samples as they are outside the data manifold. This observation suggests that changing the way of interpolating should generally result in better interpolations, but it is not clear how to do that in an unambiguous way. In this paper, we solve this issue by enforcing a specific manifold and, hence, change the base distribution, to allow for a principled way of interpolation. Specifically, we use the Dirichlet and von Mises-Fisher base distributions on the probability simplex and the hypersphere, respectively. Our experimental results show superior performance in terms of bits per dimension, Fr\u00e9chet Inception Distance (FID), and Kernel Inception Distance (KID) scores for interpolation, while maintaining the generative performance."}}
{"id": "S4ABH5yC1pA", "cdate": 1577836800000, "mdate": null, "content": {"title": "Principled Interpolation in Normalizing Flows", "abstract": "Generative models based on normalizing flows are very successful in modeling complex data distributions using simpler ones. However, straightforward linear interpolations show unexpected side effects, as interpolation paths lie outside the area where samples are observed. This is caused by the standard choice of Gaussian base distributions and can be seen in the norms of the interpolated samples. This observation suggests that correcting the norm should generally result in better interpolations, but it is not clear how to correct the norm in an unambiguous way. In this paper, we solve this issue by enforcing a fixed norm and, hence, change the base distribution, to allow for a principled way of interpolation. Specifically, we use the Dirichlet and von Mises-Fisher base distributions. Our experimental results show superior performance in terms of bits per dimension, Fr\\'echet Inception Distance (FID), and Kernel Inception Distance (KID) scores for interpolation, while maintaining the same generative performance."}}
{"id": "HJx73VSxIH", "cdate": 1567802538856, "mdate": null, "content": {"title": "Coresets for Archetypal Analysis", "abstract": "Archetypal analysis represents instances as linear mixtures of prototypes (the archetypes) that lie on the boundary of the convex hull of the data. Archetypes are thus  often better interpretable than factors computed by other matrix factorization techniques. However, the interpretability comes with high computational cost due to additional convexity-preserving constraints. In this paper, we propose efficient coresets for archetypal analysis. Theoretical guarantees are derived by showing that quantization errors of k-means upper bound archetypal analysis; the computation of a provable absolute-coreset can be performed in only two passes over the data. Empirically, we show that the coresets lead to improved performances on several data sets."}}
{"id": "5doxOuwmt16", "cdate": 1514764800000, "mdate": 1681728434081, "content": {"title": "Distributed robust Gaussian Process regression", "abstract": ""}}
{"id": "-XahoqviL-", "cdate": 1514764800000, "mdate": 1681728434082, "content": {"title": "Frame-Based Optimal Design", "abstract": ""}}
{"id": "r1ExH2b_WB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Frame-based Data Factorizations", "abstract": "Archetypal Analysis is the method of choice to compute interpretable matrix factorizations. Every data point is represented as a convex combination of factors, i.e., points on the boundary of the c..."}}
