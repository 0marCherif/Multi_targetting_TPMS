{"id": "HIhnwt-9jD4", "cdate": 1672531200000, "mdate": 1681749572717, "content": {"title": "Efficient Testable Learning of Halfspaces with Adversarial Label Noise", "abstract": "We give the first polynomial-time algorithm for the testable learning of halfspaces in the presence of adversarial label noise under the Gaussian distribution. In the recently introduced testable learning model, one is required to produce a tester-learner such that if the data passes the tester, then one can trust the output of the robust learner on the data. Our tester-learner runs in time $\\poly(d/\\eps)$ and outputs a halfspace with misclassification error $O(\\opt)+\\eps$, where $\\opt$ is the 0-1 error of the best fitting halfspace. At a technical level, our algorithm employs an iterative soft localization technique enhanced with appropriate testers to ensure that the data distribution is sufficiently similar to a Gaussian."}}
{"id": "gzUeJ7gsRS", "cdate": 1640995200000, "mdate": 1681749572748, "content": {"title": "Learning a Single Neuron with Adversarial Label Noise via Gradient Descent", "abstract": "We study the fundamental problem of learning a single neuron, i.e., a function of the form $\\mathbf{x}\\mapsto\\sigma(\\mathbf{w}\\cdot\\mathbf{x})$ for monotone activations $\\sigma:\\mathbb{R}\\mapsto\\mathbb{R}$, with respect to the $L_2^2$-loss in the presence of adversarial label noise. Specifically, we are given labeled examples from a distribution $D$ on $(\\mathbf{x}, y)\\in\\mathbb{R}^d \\times \\mathbb{R}$ such that there exists $\\mathbf{w}^\\ast\\in\\mathbb{R}^d$ achieving $F(\\mathbf{w}^\\ast)=\\epsilon$, where $F(\\mathbf{w})=\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[(\\sigma(\\mathbf{w}\\cdot \\mathbf{x})-y)^2]$. The goal of the learner is to output a hypothesis vector $\\mathbf{w}$ such that $F(\\mathbb{w})=C\\, \\epsilon$ with high probability, where $C>1$ is a universal constant. As our main contribution, we give efficient constant-factor approximate learners for a broad class of distributions (including log-concave distributions) and activation functions. Concretely, for the class of isotropic log-concave distributions, we obtain the following important corollaries: For the logistic activation, we obtain the first polynomial-time constant factor approximation (even under the Gaussian distribution). Our algorithm has sample complexity $\\widetilde{O}(d/\\epsilon)$, which is tight within polylogarithmic factors. For the ReLU activation, we give an efficient algorithm with sample complexity $\\tilde{O}(d\\, \\polylog(1/\\epsilon))$. Prior to our work, the best known constant-factor approximate learner had sample complexity $\\tilde{\\Omega}(d/\\epsilon)$. In both of these settings, our algorithms are simple, performing gradient-descent on the (regularized) $L_2^2$-loss. The correctness of our algorithms relies on novel structural results that we establish, showing that (essentially all) stationary points of the underlying non-convex loss are approximately optimal."}}
{"id": "g7nu-js7KaJ", "cdate": 1640995200000, "mdate": 1681749572728, "content": {"title": "Learning general halfspaces with general Massart noise under the Gaussian distribution", "abstract": "We study the problem of PAC learning halfspaces on \u211dd with Massart noise under the Gaussian distribution. In the Massart model, an adversary is allowed to flip the label of each point x with unknown probability \u03b7(x) \u2264 \u03b7, for some parameter \u03b7 \u2208 [0,1/2]. The goal is to find a hypothesis with misclassification error of OPT + \u0454, where OPT is the error of the target halfspace. This problem had been previously studied under two assumptions: (i) the target halfspace is homogeneous (i.e., the separating hyperplane goes through the origin), and (ii) the parameter \u03b7 is strictly smaller than 1/2. Prior to this work, no nontrivial bounds were known when either of these assumptions is removed. We study the general problem and establish the following: [leftmargin = *] For \u03b7 <1/2, we give a learning algorithm for general halfspaces with sample and computational complexity dO\u03b7(log(1/\u03b3))poly(1/\u0454), where \u03b3 max{\u0454, min{Pr[f(x) = 1], Pr[f(x) = \u22121]} } is the \u201cbias\u201d of the target halfspace f. Prior efficient algorithms could only handle the special case of \u03b3 = 1/2. Interestingly, we establish a qualitatively matching lower bound of d\u03a9(log(1/\u03b3)) on the complexity of any Statistical Query (SQ) algorithm. For \u03b7 = 1/2, we give a learning algorithm for general halfspaces with sample and computational complexity O\u0454(1) \u00a0dO(log(1/\u0454)). This result is new even for the subclass of homogeneous halfspaces; prior algorithms for homogeneous Massart halfspaces provide vacuous guarantees for \u03b7=1/2. We complement our upper bound with a nearly-matching SQ lower bound of d\u03a9(log(1/\u0454) ), which holds even for the special case of homogeneous halfspaces. Taken together, our results qualitatively characterize the complexity of learning general halfspaces with general Massart noise under Gaussian marginals. Our techniques rely on determining the existence (or non-existence) of low-degree polynomials whose expectations distinguish Massart halfspaces from random noise."}}
{"id": "WU9CV4kkSC", "cdate": 1640995200000, "mdate": 1681749572725, "content": {"title": "Learning General Halfspaces with Adversarial Label Noise via Online Gradient Descent", "abstract": "We study the problem of learning general {\u2014} i.e., not necessarily homogeneous {\u2014} halfspaces with adversarial label noise under the Gaussian distribution. Prior work has provided a sophisticated p..."}}
{"id": "GqkRc8IgeAf", "cdate": 1640995200000, "mdate": 1681749572730, "content": {"title": "Learning a Single Neuron with Adversarial Label Noise via Gradient Descent", "abstract": "We study the fundamental problem of learning a single neuron, i.e., a function of the form $\\x \\mapsto \\sigma(\\vec w \\cdot \\x)$ for monotone activations $\\sigma:\\R \\mapsto \\R$, with respect to the $L_2^2$-loss in the presence of adversarial label noise. Specifically, we are given labeled examples from a distribution $D$ on $(\\x{}, y) \\in \\R^d \\times \\R$ such that there exists $\\vec w^\\ast \\in \\R^d$ achieving $F(\\vec w^\\ast) = \\opt$, where $F(\\vec w) = \\E_{(\\x{},y) \\sim D}[(\\sigma(\\vec w\\cdot \\x) - y)^2]$. The goal of the learner is to output a hypothesis vector $\\wt{\\vec w}$ such that $F(\\wt{\\vec w}) = C \\, \\opt+\\eps$ with high probability, where $C$ is a universal constant. As our main contribution, we give efficient constant-factor approximate learners for a broad class of distributions (including log-concave distributions) and activation functions (including ReLUs and sigmoids). Concretely, for the class of isotropic log-concave distributions, we obtain the following important corollaries: \\begin{itemize}[leftmargin=3pc, rightmargin = 1.5pc] \\item For the logistic activation, i.e., $\\sigma(t) = 1/(1+e^{-t})$, we obtain the first polynomial-time constant factor approximation, even under the Gaussian distribution. Moreover, our algorithm has sample complexity $\\wt{O}(d/\\eps)$, which is tight within polylogarithmic factors. \\item For the ReLU activation, i.e., $\\sigma(t) = \\max(0,t)$, we give an efficient algorithm with sample complexity $\\wt{O}(d \\, \\polylog(1/\\eps))$. Prior to our work, the best known constant-factor approximate learner had sample complexity $\\Omega(d/\\eps)$. \\end{itemize} In both settings, our algorithms are simple, performing gradient-descent on the (regularized) $L_2^2$-loss. The correctness of our algorithms relies on novel structural results that we establish, showing that (essentially all) stationary points of the underlying non-convex loss are approximately optimal."}}
{"id": "hlli3VIJo7", "cdate": 1609459200000, "mdate": 1681749572734, "content": {"title": "Agnostic Proper Learning of Halfspaces under Gaussian Marginals", "abstract": "We study the problem of agnostically learning halfspaces under the Gaussian distribution. Our main result is the {\\em first proper} learning algorithm for this problem whose running time qualitativ..."}}
{"id": "VvhxLirT7qk", "cdate": 1609459200000, "mdate": 1664346295972, "content": {"title": "Reallocating multiple facilities on the line", "abstract": ""}}
{"id": "RpQR6sAPFY", "cdate": 1609459200000, "mdate": 1681749572733, "content": {"title": "Efficiently learning halfspaces with Tsybakov noise", "abstract": "We study the problem of PAC learning homogeneous halfspaces with Tsybakov noise. In the Tsybakov noise model, the label of every example is independently flipped with an adversarially controlled probability that can be arbitrarily close to 1/2 for a fraction of the examples. We give the first polynomial-time algorithm for this fundamental learning problem. Our algorithm learns the true halfspace within any desired accuracy and succeeds under a broad family of well-behaved distributions including log-concave distributions. This extended abstract is a merge of two papers. In an earlier work, a subset of the authors developed an efficient reduction from learning to certifying the non-optimality of a candidate halfspace and gave a quasi-polynomial time certificate algorithm. In a subsequent work, the authors of the this paper developed a polynomial-time certificate algorithm."}}
{"id": "QNXeVSktz92", "cdate": 1609459200000, "mdate": 1681749572737, "content": {"title": "Learning Online Algorithms with Distributional Advice", "abstract": "We study the problem of designing online algorithms given advice about the input. While prior work had focused on deterministic advice, we only assume distributional access to the instances of inte..."}}
{"id": "NNdBUJrU_6", "cdate": 1609459200000, "mdate": null, "content": {"title": "Agnostic Proper Learning of Halfspaces under Gaussian Marginals", "abstract": "We study the problem of agnostically learning halfspaces under the Gaussian distribution. Our main result is the {\\em first proper} learning algorithm for this problem whose sample complexity and computational complexity qualitatively match those of the best known improper agnostic learner. Building on this result, we also obtain the first proper polynomial-time approximation scheme (PTAS) for agnostically learning homogeneous halfspaces. Our techniques naturally extend to agnostically learning linear models with respect to other non-linear activations, yielding in particular the first proper agnostic algorithm for ReLU regression."}}
