{"id": "KIp0_ldQhLG", "cdate": 1698621149161, "mdate": 1698621149161, "content": {"title": "iDisc: Internal Discretization for Monocular Depth Estimation", "abstract": "Monocular depth estimation is fundamental for 3D scene\nunderstanding and downstream applications. However, even\nunder the supervised setup, it is still challenging and ill-posed\ndue to the lack of full geometric constraints. Although\na scene can consist of millions of pixels, there are fewer\nhigh-level patterns. We propose iDisc to learn those patterns\nwith internal discretized representations. The method implicitly\npartitions the scene into a set of high-level patterns.\nIn particular, our new module, Internal Discretization (ID),\nimplements a continuous-discrete-continuous bottleneck to\nlearn those concepts without supervision. In contrast to\nstate-of-the-art methods, the proposed model does not enforce\nany explicit constraints or priors on the depth output.\nThe whole network with the ID module can be trained end-to-\nend, thanks to the bottleneck module based on attention.\nOur method sets the new state of the art with significant\nimprovements on NYU-Depth v2 and KITTI, outperforming\nall published methods on the official KITTI benchmark.\niDisc can also achieve state-of-the-art results on surface\nnormal estimation. Further, we explore the model generalization\ncapability via zero-shot testing. We observe the\ncompelling need to promote diversification in the outdoor\nscenario. Hence, we introduce splits of two autonomous\ndriving datasets, DDAD and Argoverse. Code is available\nat http://vis.xyz/pub/idisc"}}
{"id": "JDr941SHyb-", "cdate": 1694171785831, "mdate": 1694171785831, "content": {"title": "OVTrack: Open-Vocabulary Multiple Object Tracking", "abstract": "The ability to recognize, localize and track dynamic objects in a scene is fundamental to many real-world applications, such as self-driving and robotic systems. Yet, traditional multiple object tracking (MOT) benchmarks rely only on a few object categories that hardly represent the multitude of possible objects that are encountered in the real world. This leaves contemporary MOT methods limited to a small set of pre-defined object categories. In this paper, we address this limitation by tackling a novel task, open-vocabulary MOT, that aims to evaluate tracking beyond pre-defined training categories. We further develop OVTrack, an open-vocabulary tracker that is capable of tracking arbitrary object classes. Its design is based on two key ingredients: First, leveraging vision-language models for both classification and association via knowledge distillation; second, a data hallucination strategy for robust appearance feature learning from denoising diffusion probabilistic models. The result is an extremely data-efficient open-vocabulary tracker that sets a new state-of-the-art on the large-scale, large-vocabulary TAO benchmark, while being trained solely on static images."}}
{"id": "IyBlj_JCtwQ", "cdate": 1684250514641, "mdate": 1684250514641, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a\nsafety-critical challenge inevitably faced by all autonomous\ndriving systems. Existing image and video driving datasets,\nhowever, fall short of capturing the mutable nature of the\nreal world. In this paper, we introduce the largest multitask synthetic dataset for autonomous driving, SHIFT. It\npresents discrete and continuous shifts in cloudiness, rain\nand fog intensity, time of day, and vehicle and pedestrian\ndensity. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT\nallows investigating the degradation of a perception system performance at increasing levels of domain shift, fostering the development of continuous adaptation strategies\nto mitigate this problem and assess model robustness and\ngenerality. Our dataset and benchmark toolkit are publicly\navailable at www.vis.xyz/shift."}}
{"id": "MosSgoAdSmQ", "cdate": 1668678400597, "mdate": 1668678400597, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous-driving systems. Existing image- and video-based driving datasets, however, fall short of capturing the mutable nature of the real world. In this paper, we introduce the largest multi-task synthetic dataset for autonomous driving, SHIFT. It presents discrete and continuous shifts in cloudiness, rain and fog intensity, time of day, and vehicle and pedestrian density. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT allows to investigate how a perception systems' performance degrades at increasing levels of domain shift, fostering the development of continuous adaptation strategies to mitigate this problem and assessing the robustness and generality of a model. Our dataset and benchmark toolkit are publicly available at www.vis.xyz/shift."}}
{"id": "TPWX8LnVDiX", "cdate": 1668437154683, "mdate": 1668437154683, "content": {"title": "Tracking Every Thing in the Wild", "abstract": "Current multi-category Multiple Object Tracking (MOT) metrics use class labels to group tracking results for per-class evaluation. Similarly, MOT methods typically only associate objects with the same class predictions. These two prevalent strategies in MOT implicitly assume that the classification performance is near-perfect. However, this is far from the case in recent large-scale MOT datasets, which contain large numbers of classes with many rare or semantically similar categories. Therefore, the resulting inaccurate classification leads to sub-optimal tracking and inadequate benchmarking of trackers. We address these issues by disentangling classification from tracking. We introduce a new metric, Track Every Thing Accuracy (TETA), breaking tracking measurement into three sub-factors: localization, association, and classification, allowing comprehensive benchmarking of tracking performance even under inaccurate classification. TETA also deals with the challenging incomplete annotation problem in large-scale tracking datasets. We further introduce a Track Every Thing tracker (TETer), that performs association using Class Exemplar Matching (CEM). Our experiments show that TETA evaluates trackers more comprehensively, and TETer achieves significant improvements on the challenging large-scale datasets BDD100K and TAO compared to the state-of-the-art."}}
{"id": "Le0MTcBsnF", "cdate": 1667477081505, "mdate": 1667477081505, "content": {"title": "SAGA: Stochastic Whole-Body Grasping with Contact", "abstract": "The synthesis of human grasping has numerous applications including AR/VR, video games and robotics. While methods have been proposed to generate realistic hand-object interaction for object grasping and manipulation, these typically only consider interacting hand alone. Our goal is to synthesize whole-body grasping motions. Starting from an arbitrary initial pose, we aim to generate diverse and natural whole-body human motions to approach and grasp a target object in 3D space. This task is challenging as it requires modeling both whole-body dynamics and dexterous finger movements. To this end, we propose SAGA (StochAstic whole-body Grasping with contAct), a framework which consists of two key components: (a) Static whole-body grasping pose generation. Specifically, we propose a multi-task generative model, to jointly learn static whole-body grasping poses and human-object contacts. (b) Grasping motion infilling. Given an initial pose and the generated whole-body grasping pose as the start and end of the motion respectively, we design a novel contact-aware generative motion infilling module to generate a diverse set of grasp-oriented motions. We demonstrate the effectiveness of our method, which is a novel generative framework to synthesize realistic and expressive whole-body motions that approach and grasp randomly placed unseen objects. Code and models are available at https://jiahaoplus.github.io/SAGA/saga.html."}}
{"id": "vqSyt8D3ny", "cdate": 1663849932016, "mdate": null, "content": {"title": "Towards Robust Object Detection Invariant to Real-World Domain Shifts", "abstract": "Safety-critical applications such as autonomous driving require robust object detection invariant to real-world domain shifts. Such shifts can be regarded as different domain styles, which can vary substantially due to environment changes and sensor noises, but deep models only know the training domain style. Such domain style gap impedes object detection generalization on diverse real-world domains. Existing classification domain generalization (DG) methods cannot effectively solve the robust object detection problem, because they either rely on multiple source domains with large style variance or destroy the content structures of the original images. In this paper, we analyze and investigate effective solutions to overcome domain style overfitting for robust object detection without the above shortcomings. Our method, dubbed as Normalization Perturbation (NP), perturbs the channel statistics of source domain low-level features to synthesize various latent styles, so that the trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training. This approach is motivated by the observation that feature channel statistics of the target domain images deviate around the source domain statistics. We further explore the style-sensitive channels for effective style synthesis. Normalization Perturbation only relies on a single source domain and is surprisingly simple and effective, contributing a practical solution by effectively adapting or generalizing classification DG methods to robust object detection. Extensive experiments demonstrate the effectiveness of our method for generalizing object detectors under real-world domain shifts."}}
{"id": "e1u9PVnwNr", "cdate": 1663849803490, "mdate": null, "content": {"title": "BiBench: Benchmarking and Analyzing Network Binarization", "abstract": "Neural network binarization emerges as one of the most promising compression approaches with extraordinary computation and memory savings by minimizing the bit-width of weight and activation. However, despite being a generic technique, recent works reveal that applying binarization in a wide range of realistic scenarios involving diverse tasks, architectures, and hardware is not trivial. Moreover, common challenges, such as severe degradation in accuracy and limited efficiency gains, suggest that specific attributes of binarization are not thoroughly studied and adequately understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production setting. We thus define the evaluation tracks and metrics for a fair and systematic investigation. We then perform a comprehensive evaluation with a rich collection of milestone binarization algorithms. Our benchmark results show binarization still faces severe accuracy challenges but diminishing improvements brought by newer state-of-the-art binarization algorithms, even at the expense of efficiency. Moreover, the actual deployment of certain binarization operations reveals a surprisingly large deviation from their theoretical consumption. Finally, we provide suggestions based on our benchmark results and analysis, devoted to establishing a paradigm for accurate and efficient binarization among existing techniques. We hope BiBench paves the way towards more extensive adoption of network binarization and serves as a foundation for future research."}}
{"id": "nBnHXevkjZ", "cdate": 1655376352583, "mdate": null, "content": {"title": "CC-3DT: Panoramic 3D Object Tracking via Cross-Camera Fusion", "abstract": "To track the 3D locations and trajectories of the other traffic participants at any given time, modern autonomous vehicles are equipped with multiple cameras that cover the vehicle's full surroundings. Yet, camera-based 3D object tracking methods prioritize optimizing the single-camera setup and resort to post-hoc fusion in a multi-camera setup. In this paper, we propose a method for panoramic 3D object tracking, called CC-3DT, that associates and models object trajectories both temporally and across views, and improves the overall tracking consistency. In particular, our method fuses 3D detections from multiple cameras before association, reducing identity switches significantly and improving motion modeling. Our experiments on large-scale driving datasets show that fusion before association leads to a large margin of improvement over post-hoc fusion. We set a new state-of-the-art with 12.6% improvement in average multi-object tracking accuracy (AMOTA) among all camera-based methods on the competitive NuScenes 3D tracking benchmark, outperforming previously published methods by 6.5% in AMOTA with the same 3D detector. "}}
{"id": "xyQXTqGcQIu", "cdate": 1640995200000, "mdate": 1654723611551, "content": {"title": "Normalizing Flow as a Flexible Fidelity Objective for Photo-Realistic Super-resolution", "abstract": "Super-resolution is an ill-posed problem, where a ground-truth high-resolution image represents only one possibility in the space of plausible solutions. Yet, the dominant paradigm is to employ pixel-wise losses, such as L <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</inf> , which drive the prediction towards a blurry average. This leads to fundamentally conflicting objectives when combined with adversarial losses, which degrades the final quality. We address this issue by revisiting the L <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</inf> loss and show that it corresponds to a one-layer conditional flow. Inspired by this relation, we explore general flows as a fidelity-based alternative to the L <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</inf> objective. We demonstrate that the flexibility of deeper flows leads to better visual quality and consistency when combined with adversarial losses. We conduct extensive user studies for three datasets and scale factors, where our approach is shown to outperform state-of-the-art methods for photo-realistic super-resolution. Code and trained models: git.io/AdFlow"}}
