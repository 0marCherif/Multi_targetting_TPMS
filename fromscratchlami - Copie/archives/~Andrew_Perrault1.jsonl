{"id": "BNU_N-7EIR", "cdate": 1686662698298, "mdate": null, "content": {"title": "Pandemic Data Collection, Management, Analysis and Decision Support: A Large Urban University Retrospective", "abstract": "The COVID-19 pandemic has disrupted the world. During this crisis, data has emerged as a critical resource for understanding, monitoring, and mitigating the impact of the disease. \nWe present The Ohio State University's data-driven framework for comprehensive monitoring of the COVID-19 pandemic. We discuss the challenges associated with data collection, investigate the roles and limitations of data analysis in supporting intervention choice and implementation strategies amid the complexities of the pandemic as it unfolded. Balancing privacy, consent, and transparency and ensuring the responsible handling of sensitive information is crucial in maintaining public trust. We examine privacy-preserving techniques, ethical frameworks, and legal regulations aimed at safeguarding individuals' rights while harnessing the power of data.\nIn our experience, conscientious data architecture provided a foundation for meaningful ethical applications of data products, which not only helped mitigate the current crisis, but also can provide valuable insights for better addressing future public health emergencies."}}
{"id": "Ql4CuaB3-D", "cdate": 1686662698106, "mdate": null, "content": {"title": "Using Reinforcement Learning for Multi-Objective Cluster-Level NPI Optimization", "abstract": "Non-pharmaceutical interventions (NPIs) play a critical role in the defense against emerging pathogens. Among these interventions, familiar measures such as travel bans, event cancellations, social distancing, curfews, and lockdowns have become integral components of our response strategy. Contact tracing is especially widely adopted. However, the optimization of contact tracing involves navigating various trade-offs, including the simultaneous goals of minimizing virus transmission and reducing costs. Reinforcement learning (RL) techniques provides a promising avenue to model intricate decision-making processes and optimize policies to achieve specific objectives, but even modern deep RL techniques struggle in the high dimensional partially observable problem setting presented by contact tracing. We propose a novel RL approach to optimize a multi-objective infectious disease control policy that combines supervised learning with RL, allowing us to capitalize on the strengths of both techniques. Through extensive experimentation and evaluation, we show that our optimized policy surpasses the performance of five benchmark policies."}}
{"id": "N0qlvDjnEv", "cdate": 1686662697982, "mdate": null, "content": {"title": "Risk-Based Ring Vaccination: A Strategy for Pandemic Control and Vaccine Allocation", "abstract": "Throughout an infectious disease crisis, resources that can be used to slow and prevent spread are often scarce or expensive. Designing control policies to optimally allocate these resources to maximize objectives is challenging. Here, we study the case of ring vaccination, a strategy that is used to control the spread of infection by vaccinating the contacts of identified infected individuals and their contacts of contacts. Using agent-based modeling to simulate an Ebola outbreak, we introduce a risk-based ring vaccination strategy in which individuals in a ring are prioritized based on their relative infection risks. Assuming the risk of transmission by contact type is known and a fixed supply of vaccine doses is available on each day, we compared this strategy to ring vaccination without prioritization and randomized vaccination. We find that risk-based ring vaccination offers a substantial advantage over standard ring vaccination when the number of doses are limited, including reducing the daily infected count and death count, and shifting the pandemic peak by a considerable amount of time. We believe that control policies based on estimated risk can often offer significant benefits without increasing the burden of administering the policy by an unacceptable amount."}}
{"id": "eN2lQxjWL05", "cdate": 1652737371769, "mdate": null, "content": {"title": "Decision-Focused Learning without Decision-Making: Learning Locally Optimized Decision Losses", "abstract": "Decision-Focused Learning (DFL) is a paradigm for tailoring a predictive model to a downstream optimization task that uses its predictions in order to perform better \\textit{on that specific task}. The main technical challenge associated with DFL is that it requires being able to differentiate through the optimization problem, which is difficult due to discontinuous solutions and other challenges. Past work has largely gotten around this this issue by \\textit{handcrafting} task-specific surrogates to the original optimization problem that provide informative gradients when differentiated through. However, the need to handcraft surrogates for each new task limits the usability of DFL. In addition, there are often no guarantees about the convexity of the resulting surrogates and, as a result, training a predictive model using them can lead to inferior local optima. In this paper, we do away with surrogates altogether and instead \\textit{learn} loss functions that capture task-specific information. To the best of our knowledge, ours is the first approach that entirely replaces the optimization component of decision-focused learning with a loss that is automatically learned. Our approach (a) only requires access to a black-box oracle that can solve the optimization problem and is thus \\textit{generalizable}, and (b) can be \\textit{convex by construction} and so can be easily optimized over. We evaluate our approach on three resource allocation problems from the literature and find that our approach outperforms learning without taking into account task-structure in all three domains, and even hand-crafted surrogates from the literature."}}
{"id": "sesaHjLjR2p", "cdate": 1640995200000, "mdate": 1656342877413, "content": {"title": "Learning (Local) Surrogate Loss Functions for Predict-Then-Optimize Problems", "abstract": "Decision-Focused Learning (DFL) is a paradigm for tailoring a predictive model to a downstream optimization task that uses its predictions in order to perform better on that specific task. The main technical challenge associated with DFL is that it requires being able to differentiate through the optimization problem, which is difficult due to discontinuous solutions and other challenges. Past work has largely gotten around this this issue by handcrafting task-specific surrogates to the original optimization problem that provide informative gradients when differentiated through. However, the need to handcraft surrogates for each new task limits the usability of DFL. In addition, there are often no guarantees about the convexity of the resulting surrogates and, as a result, training a predictive model using them can lead to inferior local optima. In this paper, we do away with surrogates altogether and instead learn loss functions that capture task-specific information. To the best of our knowledge, ours is the first approach that entirely replaces the optimization component of decision-focused learning with a loss that is automatically learned. Our approach (a) only requires access to a black-box oracle that can solve the optimization problem and is thus generalizable, and (b) can be convex by construction and so can be easily optimized over. We evaluate our approach on three resource allocation problems from the literature and find that our approach outperforms learning without taking into account task-structure in all three domains, and even hand-crafted surrogates from the literature."}}
{"id": "6vkzF28Hur8", "cdate": 1632875592185, "mdate": null, "content": {"title": "Training Transition Policies via Distribution Matching for Complex Tasks", "abstract": "Humans decompose novel complex tasks into simpler ones to exploit previously learned skills. Analogously, hierarchical reinforcement learning seeks to leverage lower-level policies for simple tasks to solve complex ones. However, because each lower-level policy induces a different distribution of states, transitioning from one lower-level policy to another may fail due to an unexpected starting state. We introduce transition policies that smoothly connect lower-level policies by producing a distribution of states and actions that matches what is expected by the next policy. Training transition policies is challenging because the natural reward signal---whether the next policy can execute its subtask successfully---is sparse. By training transition policies via adversarial inverse reinforcement learning to match the distribution of expected states and actions, we avoid relying on task-based reward. To further improve performance, we use deep Q-learning with a binary action space to determine when to switch from a transition policy to the next pre-trained policy, using the success or failure of the next subtask as the reward. Although the reward is still sparse, the problem is less severe due to the simple binary action space. We demonstrate our method on continuous bipedal locomotion and arm manipulation tasks that require diverse skills. We show that it smoothly connects the lower-level policies, achieving higher success rates than previous methods that search for successful trajectories based on a reward function, but do not match the state distribution.\n"}}
{"id": "-mGv2KxQ43D", "cdate": 1621630129568, "mdate": null, "content": {"title": "Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Making by Reinforcement Learning", "abstract": "In the predict-then-optimize framework, the objective is to train a predictive model, mapping from environment features to parameters of an optimization problem, which maximizes decision quality when the optimization is subsequently solved. Recent work on decision-focused learning shows that embedding the optimization problem in the training pipeline can improve decision quality and help generalize better to unseen tasks compared to relying on an intermediate loss function for evaluating prediction quality. We study the predict-then-optimize framework in the context of sequential decision problems (formulated as MDPs) that are solved via reinforcement learning. In particular, we are given environment features and a set of trajectories from training MDPs, which we use to train a predictive model that generalizes to unseen test MDPs without trajectories. Two significant computational challenges arise in applying decision-focused learning to MDPs: (i) large state and action spaces make it infeasible for existing techniques to differentiate through MDP problems, and (ii) the high-dimensional policy space, as parameterized by a neural network, makes differentiating through a policy expensive. We resolve the first challenge by sampling provably unbiased derivatives to approximate and differentiate through optimality conditions, and the second challenge by using a low-rank approximation to the high-dimensional sample-based derivatives. We implement both Bellman-based and policy gradient-based decision-focused learning on three different MDP problems with missing parameters, and show that decision-focused learning performs better in generalization to unseen tasks."}}
{"id": "Dgs2_rn29o", "cdate": 1620616112914, "mdate": null, "content": {"title": "Risk-Aware Interventions in Public Health: Planning with Restless Multi-Armed Bandits", "abstract": "Community Health Workers (CHWs) form an important component\nof health-care systems globally, especially in low-resource settings.\nCHWs are often tasked with monitoring the health of and intervening on their patient cohort. Previous work has developed several\nclasses of Restless Multi-Armed Bandits (RMABs) that are computationally tractable and indexable, a condition that guarantees\nasymptotic optimality, for solving such health monitoring and intervention problems (HMIPs). However, existing solutions to HMIPs\nfail to account for risk-sensitivity considerations of CHWs in the\nplanning stage and may run the danger of ignoring some patients\ncompletely because they are deemed less valuable to intervene on.\nAdditionally, these also rely on patients reporting their state of adherence accurately when intervened upon. Towards tackling these\nissues, our contributions in this paper are as follows: (1) We develop\nan RMAB solution to HMIPs that allows for reward functions that\nare monotone increasing, rather than linear, in the belief state and\nalso supports a wider class of observations. (2) We prove theoretical\nguarantees on the asymptotic optimality of our algorithm for any\narbitrary reward function. Additionally, we show that for the specific reward function considered in previous work, our theoretical\nconditions are stronger than the state-of-the-art guarantees. (3) We\nshow the applicability of these new results for addressing the three\nissues pertaining to: risk-sensitive planning, equitable allocation\nand reliance on perfect observations as highlighted above. We evaluate these techniques on both simulated as well as real data from\na prevalent CHW task of monitoring adherence of tuberculosis\npatients to their prescribed medication in Mumbai, India and show\nimproved performance over the state-of-the-art. Full paper and code\nis available at: https://github.com/AdityaMate/risk-aware-bandits"}}
{"id": "HJx2G1iNKv", "cdate": 1620613884947, "mdate": null, "content": {"title": "Collapsing Bandits and Their Applications to Public Health Interventions", "abstract": "We propose and study Collpasing Bandits, a new restless multi-armed bandit (RMAB) setting in which each arm follows a binary-state Markovian process with a special structure: when an arm is played, the state is fully observed, thus \"collapsing\" any uncertainty, but when an arm is passive, no observation is made, thus allowing uncertainty to evolve. The goal is to keep as many arms in the \"good\" state as possible by planning a limited budget of actions per round. Such Collapsing Bandits are natural models for many healthcare domains in which workers must simultaneously monitor patients and deliver interventions in a way that maximizes the health of their patient cohort. Our main contributions are as follows: (i) Building on the Whittle index technique for RMABs, we derive conditions under which the Collapsing Bandits problem is indexable. Our derivation hinges on novel conditions that characterize when the optimal policies may take the form of either \"forward\" or \"reverse\" threshold policies. (ii) We exploit the optimality of threshold policies to build fast algorithms for computing the Whittle index, including a closed-form. (iii) We evaluate our algorithm on several data distributions including data from a real-world healthcare task in which a worker must monitor and deliver interventions to maximize their patients' adherence to tuberculosis medication. Our algorithm achieves a 3-order-of-magnitude speedup compared to state-of-the-art RMAB techniques while achieving similar performance."}}
{"id": "JKLTJhZhNfD", "cdate": 1620546589386, "mdate": null, "content": {"title": "Beyond \"To Act or Not to Act\": Fast Lagrangian Approaches to General Multi-Action Restless Bandits", "abstract": "This paper presents new algorithms and theoretical results for solutions to Multi-action Multi-armed Restless Bandits, an important\nbut insufficiently studied generalization of traditional Multi-armed\nRestless Bandits (MARBs). Though MARBs are popular for modeling many problems, they are restricted to binary actions, i.e., \"to act\nor not to act\". This renders them unable to capture critical complexities faced by planners in real domains, such as a system manager balancing maintenance, repair, and job scheduling, or a health worker\ndeciding among treatments for a given patient. Limited previous\nwork on Multi-action MARBs has only been specialized to subproblems. Here we derive multiple algorithms for use on general\nMulti-action MARBs using Lagrangian relaxation techniques, leading to the following contributions: (i) We develop BLam, a bound optimization algorithm which leverages problem convexity to quickly\nand provably converge to the well-performing Lagrange policy; (ii)\nWe develop SampleLam, a fast sampling technique for estimating\nthe Lagrange policy, and derive a concentration bound to investigate its convergence properties; (iii) We derive best and worst case\ncomputational complexities for our algorithms as well as our main\ncompetitor; (iv) We provide experimental results comparing our\nalgorithms to baselines on simulated distributions, including one\nmotivated by a real-world community health intervention task. Our\napproach achieves significant, up to ten-fold speedups over more\ngeneral methods without sacrificing performance and is widely\napplicable across general Multi-action MARBs. Code is availabl"}}
