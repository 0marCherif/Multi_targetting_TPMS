{"id": "fPH7rHWWyG", "cdate": 1695959647611, "mdate": 1695959647611, "content": {"title": "Bayesian Approach to Blood Rheological Uncertainties in Aortic Hemodynamics", "abstract": "Computational hemodynamics has received increasing attention recently. Patient-specific simulations require questionable model assumptions, for example, for geometry, boundary conditions, and material parameters. Consequently, the credibility of these simulations is much doubted, and rightly so. Yet, the matter may be addressed by a rigorous uncertainty quantification. In this contribution, we investigated the impact of blood rheological models on wall shear stress uncertainties in aortic hemodynamics obtained in numerical simulations. Based on shear-rheometric experiments, we compare the non-Newtonian Carreau model to a simple Newtonian model and a Reynolds number-equivalent Newtonian model. Bayesian Probability Theory treats uncertainties consistently and allows to include elusive assumptions such as the comparability of flow regimes. We overcome the prohibitively high computational cost for the simulation with a surrogate model, and account for the uncertainties of the surrogate model itself, too. We have two main findings: (1) The Newtonian models mostly underestimate the uncertainties as compared to the non-Newtonian model. (2) The wall shear stresses of specific persons cannot be distinguished due to largely overlapping uncertainty bands, implying that a more precise determination of person-specific blood rheological properties is necessary for person-specific simulations. While we refrain from a general recommendation for one rheological model, we have quantified the error of the uncertainty quantification associated with these modeling choices."}}
{"id": "cTMpUbr_jP", "cdate": 1695959579980, "mdate": 1695959579980, "content": {"title": "Bayesian Source Separation of Electrical Bioimpedance Signals,", "abstract": "For physicians, it is often crucial to monitor hemodynamic parameters to provide appropriate treatment for patients. Such hemodynamic parameters can be estimated via electrical bioimpedance (EBI) signal measurements. Time dependent changes of the measured EBI signal occur due to several different phenomena in the human body. Most of the time one is just interested in a single component of the EBI signal, such as the part caused by cardiac activities, wherefore it is necessary to decompose the EBI signal into its different source terms. The changes of the signal are mostly caused by respiration and cardiac activity (pulse). Since these fluctuations are periodic in sufficiently small time windows, the signal can be approximated by a harmonic series with two different fundamental frequencies and an unknown number of higher harmonics. In this work, we present Bayesian Probability Theory as the adequate and rigorous method for this decomposition. The proposed method allows, in contrast to other methods, to consistently identify the model-function, compute parameter estimates and predictions, and to quantify uncertainties. Further, the method can handle a very low signal-to-noise ratio. The results suggest that EBI-based estimation of hemodynamic parameters and their monitoring can be improved and its reliability assessed."}}
{"id": "ZGcQp2j04G", "cdate": 1695959499549, "mdate": 1695959499549, "content": {"title": "Bayesian Analysis of Femtosecond Pump-Probe Photoelectron-Photoion Coincidence Spectra with Fluctuating Laser Intensities,", "abstract": "This paper employs Bayesian probability theory for analyzing data generated in femtosecond pump-probe photoelectron-photoion coincidence (PEPICO) experiments. These experiments allow investigating ultrafast dynamical processes in photoexcited molecules. Bayesian probability theory is consistently applied to data analysis problems occurring in these types of experiments such as background subtraction and false coincidences. We previously demonstrated that the Bayesian formalism has many advantages, amongst which are compensation of false coincidences, no overestimation of pump-only contributions, significantly increased signal-to-noise ratio, and applicability to any experimental situation and noise statistics. Most importantly, by accounting for false coincidences, our approach allows running experiments at higher ionization rates, resulting in an appreciable reduction of data acquisition times. In addition to our previous paper, we include fluctuating laser intensities, of which the straightforward implementation highlights yet another advantage of the Bayesian formalism. Our method is thoroughly scrutinized by challenging mock data, where we find a minor impact of laser fluctuations on false coincidences, yet a noteworthy influence on background subtraction. We apply our algorithm to data obtained in experiments and discuss the impact of laser fluctuations on the data analysis."}}
{"id": "7ptHdU2VT_", "cdate": 1695959458479, "mdate": 1695959458479, "content": {"title": "Bayesian Uncertainty Quantification with Multi-Fidelity Data and Gaussian Processes for Impedance Cardiography of Aortic Dissection", "abstract": "first_page\nsettings\nOrder Article Reprints\nOpen AccessArticle\nBayesian Uncertainty Quantification with Multi-Fidelity Data and Gaussian Processes for Impedance Cardiography of Aortic Dissection\nby Sascha Ranftl\n1,* [ORCID] , Gian Marco Melito\n2 [ORCID] , Vahid Badeli\n3 [ORCID] , Alice Reinbacher-K\u00f6stinger\n3 [ORCID] , Katrin Ellermann\n2 and Wolfgang von der Linden\n1,* [ORCID]\n1\nInstitute of Theoretical Physics-Computational Physics, Graz University of Technology, 8010 Graz, Austria\n2\nInstitute of Mechanics, Graz University of Technology, 8010 Graz, Austria\n3\nInstitute of Fundamentals and Theory in Electrical Engineering, Graz University of Technology, 8010 Graz, Austria\n*\nAuthors to whom correspondence should be addressed.\nEntropy 2020, 22(1), 58; https://doi.org/10.3390/e22010058\nReceived: 26 November 2019 / Revised: 26 December 2019 / Accepted: 27 December 2019 / Published: 31 December 2019\n(This article belongs to the Special Issue MaxEnt 2019\u2014The 39th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering)\nDownload Browse Figures\nVersions Notes\n\nAbstract\nIn 2000, Kennedy and O\u2019Hagan proposed a model for uncertainty quantification that combines data of several levels of sophistication, fidelity, quality, or accuracy, e.g., a coarse and a fine mesh in finite-element simulations. They assumed each level to be describable by a Gaussian process, and used low-fidelity simulations to improve inference on costly high-fidelity simulations. Departing from there, we move away from the common non-Bayesian practice of optimization and marginalize the parameters instead. Thus, we avoid the awkward logical dilemma of having to choose parameters and of neglecting that choice\u2019s uncertainty. We propagate the parameter uncertainties by averaging the predictions and the prediction uncertainties over all the possible parameters. This is done analytically for all but the nonlinear or inseparable kernel function parameters. What is left is a low-dimensional and feasible numerical integral depending on the choice of kernels, thus allowing for a fully Bayesian treatment. By quantifying the uncertainties of the parameters themselves too, we show that \u201clearning\u201d or optimising those parameters has little meaning when data is little and, thus, justify all our mathematical efforts. The recent hype about machine learning has long spilled over to computational engineering but fails to acknowledge that machine learning is a big data problem and that, in computational engineering, we usually face a little data problem. We devise the fully Bayesian uncertainty quantification method in a notation following the tradition of E.T. Jaynes and find that generalization to an arbitrary number of levels of fidelity and parallelisation becomes rather easy. We scrutinize the method with mock data and demonstrate its advantages in its natural application where high-fidelity data is little but low-fidelity data is not. We then apply the method to quantify the uncertainties in finite element simulations of impedance cardiography of aortic dissection. Aortic dissection is a cardiovascular disease that frequently requires immediate surgical treatment and, thus, a fast diagnosis before. While traditional medical imaging techniques such as computed tomography, magnetic resonance tomography, or echocardiography certainly do the job, Impedance cardiography too is a clinical standard tool and promises to allow earlier diagnoses as well as to detect patients that otherwise go under the radar for too long."}}
{"id": "WJ24lWKZWa", "cdate": 1695959414946, "mdate": 1695959414946, "content": {"title": "Cross Entropy Learning for Aortic Pathology Classification of Artificial Multi-Sensor Impedance Cardiography Signals", "abstract": "first_page\nsettings\nOrder Article Reprints\nOpen AccessArticle\nCross-Entropy Learning for Aortic Pathology Classification of Artificial Multi-Sensor Impedance Cardiography Signals\nby Tobias Spindelb\u00f6ck\n1, Sascha Ranftl\n1,2,* [ORCID] and Wolfgang von der Linden\n1,2 [ORCID]\n1\nInstitute of Theoretical and Computational Physics, Graz University of Technology, Petersgasse 16, 8010 Graz, Austria\n2\nGraz Center of Computational Engineering, Graz University of Technology, Krenngasse 37, 8010 Graz, Austria\n*\nAuthor to whom correspondence should be addressed.\nEntropy 2021, 23(12), 1661; https://doi.org/10.3390/e23121661\nReceived: 21 October 2021 / Revised: 22 November 2021 / Accepted: 24 November 2021 / Published: 10 December 2021\n(This article belongs to the Special Issue Entropy in Biomedical Engineering)\nDownload Browse Figures\nVersions Notes\n\nAbstract\nAn aortic dissection, a particular aortic pathology, occurs when blood pushes through a tear between the layers of the aorta and forms a so-called false lumen. Aortic dissection has a low incidence compared to other diseases, but a relatively high mortality that increases with disease progression. An early identification and treatment increases patients\u2019 chances of survival. State-of-the-art medical imaging techniques have several disadvantages; therefore, we propose the detection of aortic dissections through their signatures in impedance cardiography signals. These signatures arise due to pathological blood flow characteristics and a blood conductivity that strongly depends on the flow field, i.e., the proposed method is, in principle, applicable to any aortic pathology that changes the blood flow characteristics. For the signal classification, we trained a convolutional neural network (CNN) with artificial impedance cardiography data based on a simulation model for a healthy virtual patient and a virtual patient with an aortic dissection. The network architecture was tailored to a multi-sensor, multi-channel time-series classification with a categorical cross-entropy loss function as the training objective. The trained network typically yielded a specificity of (93.9\u00b10.1)%\nand a sensitivity of (97.5\u00b10.1)%. A study of the accuracy as a function of the size of an aortic dissection yielded better results for a small false lumen with larger noise, which emphasizes the question of the feasibility of detecting aortic dissections in an early state."}}
{"id": "52GP0w3bsUI", "cdate": 1695959340320, "mdate": 1695959340320, "content": {"title": "Bayesian Inference of Multi-Sensors Impedance Cardiography for Detection of Aortic Dissection.,", "abstract": "Purpose\n\nThis paper aims to introduce a non-invasive and convenient method to detect a life-threatening disease called aortic dissection. A Bayesian inference based on enhanced multi-sensors impedance cardiography (ICG) method has been applied to classify signals from healthy and sick patients.\nDesign/methodology/approach\n\nA 3D numerical model consisting of simplified organ geometries is used to simulate the electrical impedance changes in the ICG-relevant domain of the human torso. The Bayesian probability theory is used for detecting an aortic dissection, which provides information about the probabilities for both cases, a dissected and a healthy aorta. Thus, the reliability and the uncertainty of the disease identification are found by this method and may indicate further diagnostic clarification.\nFindings\n\nThe Bayesian classification shows that the enhanced multi-sensors ICG is more reliable in detecting aortic dissection than conventional ICG. Bayesian probability theory allows a rigorous quantification of all uncertainties to draw reliable conclusions for the medical treatment of aortic dissection.\nOriginality/value\n\nThis paper presents a non-invasive and reliable method based on a numerical simulation that could be beneficial for the medical management of aortic dissection patients. With this method, clinicians would be able to monitor the patient\u2019s status and make better decisions in the treatment procedure of each patient."}}
{"id": "2qdCseOe9b", "cdate": 1694374550247, "mdate": 1694374550247, "content": {"title": "Robust Bayesian target value optimization", "abstract": "We consider the problem of finding an input to a stochastic black box function such that the scalar output of the black box function is as close as possible to a target value in the sense of the expected squared error. While the optimization of stochastic black boxes is classic in (robust) Bayesian optimization, the current approaches based on Gaussian processes predominantly focus either on (i) maximization/minimization rather than target value optimization or (ii) on the expectation, but not the variance of the output, ignoring output variations due to stochasticity in uncontrollable environmental variables. In this work, we fill this gap and derive acquisition functions for common criteria such as the expected improvement, the probability of improvement, and the lower confidence bound, assuming that aleatoric effects are Gaussian with known variance. Our experiments illustrate that this setting is compatible with certain extensions of Gaussian processes, and show that the thus derived acquisition functions can outperform classical Bayesian optimization even if the latter assumptions are violated. An industrial use case in billet forging is presented."}}
{"id": "iLm-KXYAtM", "cdate": 1694374394472, "mdate": 1694374394472, "content": {"title": "Bayesian uncertainty quantification with multi-fidelity data and gaussian processes for impedance cardiography of aortic dissection", "abstract": "In 2000, Kennedy and O\u2019Hagan proposed a model for uncertainty quantification that combines data of several levels of sophistication, fidelity, quality, or accuracy, e.g., a coarse and a fine mesh in finite-element simulations. They assumed each level to be describable by a Gaussian process, and used low-fidelity simulations to improve inference on costly high-fidelity simulations. Departing from there, we move away from the common non-Bayesian practice of optimization and marginalize the parameters instead. Thus, we avoid the awkward logical dilemma of having to choose parameters and of neglecting that choice\u2019s uncertainty. We propagate the parameter uncertainties by averaging the predictions and the prediction uncertainties over all the possible parameters. This is done analytically for all but the nonlinear or inseparable kernel function parameters. What is left is a low-dimensional and feasible numerical integral depending on the choice of kernels, thus allowing for a fully Bayesian treatment. By quantifying the uncertainties of the parameters themselves too, we show that \u201clearning\u201d or optimising those parameters has little meaning when data is little and, thus, justify all our mathematical efforts. The recent hype about machine learning has long spilled over to computational engineering but fails to acknowledge that machine learning is a big data problem and that, in computational engineering, we usually face a little data problem. We devise the fully Bayesian uncertainty quantification method in a notation following the tradition of E.T. Jaynes and find that generalization to an arbitrary number of levels of fidelity and parallelisation becomes rather easy. We scrutinize the method with mock data and demonstrate its advantages in its natural application where high-fidelity data is little but low-fidelity data is not. We then apply the method to quantify the uncertainties in finite element simulations of impedance cardiography of aortic dissection. Aortic dissection is a cardiovascular disease that frequently requires immediate surgical treatment and, thus, a fast diagnosis before. While traditional medical imaging techniques such as computed tomography, magnetic resonance tomography, or echocardiography certainly do the job, Impedance cardiography too is a clinical standard tool and promises to allow earlier diagnoses as well as to detect patients that otherwise go under the radar for too long."}}
{"id": "0TguWgKt9nj", "cdate": 1694374272313, "mdate": null, "content": {"title": "Stochastic Modeling of Inhomogeneities in the Aortic Wall and Uncertainty Quantification using a Bayesian Encoder-Decoder Surrogate", "abstract": "Inhomogeneities in the aortic wall can lead to localized stress accumulations, possibly initiating dissection. In many cases, a dissection results from pathological changes such as fragmentation or loss of elastic fibers. But it has been shown that even the healthy aortic wall has an inherent heterogeneous microstructure. Some parts of the aorta are particularly susceptible to the development of inhomogeneities due to pathological changes, however, the distribution in the aortic wall and the spatial extent, such as size, shape and type, are difficult to predict. Motivated by this observation, we describe the heterogeneous distribution of elastic fiber degradation in the dissected aortic wall using a stochastic constitutive model. For this purpose, random field realizations, which model the stochastic distribution of degraded elastic fibers, are generated over a non-equidistant grid. The random field then serves as input for a uniaxial extension test of the pathological aortic wall, solved with the finite element method. To include the microstructure of the dissected aortic wall, a constitutive model developed in a previous study is applied, which also includes an approach to model the degradation of interlamellar elastic fibers. To then assess the uncertainty in the output stress distribution due to the proposed stochastic constitutive model, a convolutional neural network, specifically a Bayesian encoder\u2013decoder, was used as a surrogate model that maps the random input fields to the output stress distribution obtained from the finite element analysis. The results show that the neural network is able to predict the stress distribution of the finite element analysis while also significantly reducing the computational time. In addition, this provides the probability for exceeding certain rupture stresses within the aortic wall, which could allow for the prediction of delamination or fatal rupture."}}
{"id": "CddC6rPSxZ", "cdate": 1694374170399, "mdate": null, "content": {"title": "Bayesian surrogate analysis and uncertainty propagation", "abstract": "The quantification of uncertainties of computer simulations due to input parameter uncertainties is paramount to assess a model\u2019s credibility. For computationally expensive simulations, this is often feasible only via surrogate models that are learned from a small set of simulation samples. The surrogate models are commonly chosen and deemed trustworthy based on heuristic measures, and substituted for the simulation in order to approximately propagate the simulation input uncertainties to the simulation output. In the process, the contribution of the uncertainties of the surrogate itself to the simulation output uncertainties is usually neglected. In this work, we specifically address the case of doubtful surrogate trustworthiness, i.e., non-negligible surrogate uncertainties. We find that Bayesian probability theory yields a natural measure of surrogate trustworthiness, and that surrogate uncertainties can easily be included in simulation output uncertainties. For a Gaussian likelihood for the simulation data, with unknown surrogate variance and given a generalized linear surrogate model, the resulting formulas reduce to simple matrix multiplications. The framework contains Polynomial Chaos Expansions as a special case, and is easily extended to Gaussian Process Regression. Additionally, we show a simple way to implicitly include spatio-temporal correlations. Lastly, we demonstrate a numerical example where surrogate uncertainties are in part negligible and in part non-negligible."}}
