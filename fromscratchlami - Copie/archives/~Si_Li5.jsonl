{"id": "ZQwUti_aM2", "cdate": 1672531200000, "mdate": 1682397699536, "content": {"title": "Polarization Guided HDR Reconstruction via Pixel-Wise Depolarization", "abstract": "Taking photos with digital cameras often accompanies saturated pixels due to their limited dynamic range, and it is far too ill-posed to restore them. Capturing multiple low dynamic range images with bracketed exposures can make the problem less ill-posed, however, it is prone to ghosting artifacts caused by spatial misalignment among images. A polarization camera can capture four spatially-aligned and temporally-synchronized polarized images with different polarizer angles in a single shot, which can be used for ghost-free high dynamic range (HDR) reconstruction. However, real-world scenarios are still challenging since existing polarization-based HDR reconstruction methods treat all pixels in the same manner and only utilize the spatially-variant exposures of the polarized images (without fully exploiting the degree of polarization (DoP) and the angle of polarization (AoP) of the incoming light to the sensor, which encode abundant structural and contextual information of the scene) to handle the problem still in an ill-posed manner. In this paper, we propose a pixel-wise depolarization strategy to solve the polarization guided HDR reconstruction problem, by classifying the pixels based on their levels of ill-posedness in HDR reconstruction procedure and applying different solutions to different classes. To utilize the strategy with better generalization ability and higher robustness, we propose a network-physics-hybrid polarization-based HDR reconstruction pipeline along with a neural network tailored to it, fully exploiting the DoP and AoP. Experimental results show that our approach achieves state-of-the-art performance on both synthetic and real-world images."}}
{"id": "UgaLap-Z9P", "cdate": 1672531200000, "mdate": 1681651100545, "content": {"title": "Physics-Guided Reflection Separation From a Pair of Unpolarized and Polarized Images", "abstract": ""}}
{"id": "_AakMp6-Hr", "cdate": 1667366705742, "mdate": 1667366705742, "content": {"title": "UniCoRN: A Unified Conditional Image Repainting Network", "abstract": "Conditional image repainting (CIR) is an advanced image editing task, which requires the model to generate visual content in user-specified regions conditioned on multiple cross-modality constraints, and composite the visual content with the provided background seamlessly. Existing methods based on two-phase architecture design assume dependency between phases and cause color-image incongruity. To solve these problems, we propose a novel Unified Conditional image Repainting Network (UniCoRN). We break the two-phase assumption in the CIR task by constructing the interaction and dependency relationship between background and other conditions. We further introduce the hierarchical structure into cross-modality similarity model to capture feature patterns at different levels and bridge the gap between visual content and color condition. A new LANDSCAPE-CIR dataset is collected and annotated to expand the application scenarios of the CIR task. Experiments show that UniCoRN achieves higher synthetic quality, better condition consistency, and more realistic compositing effect."}}
{"id": "No_fRCikFB", "cdate": 1667366596754, "mdate": 1667366596754, "content": {"title": "CT2 : Colorization Transformer via Color Tokens", "abstract": "Automatic image colorization is an ill-posed problem with multi-modal uncertainty, and there remains two main challenges with previous methods: incorrect semantic colors and under-saturation. In this paper, we propose an end-to-end transformer-based model to overcome these challenges. Benefited from the long-range context extraction of transformer and our holistic architecture, our method could colorize images with more diverse colors. Besides, we introduce color tokens into our approach and treat the colorization task as a classification problem, which increases the saturation of results. We also propose a series of modules to make image features interact with color tokens, and restrict the range of possible color candidates, which makes our results visually pleasing and reasonable. In addition, our method does not require any additional external priors, which ensures its well generalization capability. Extensive experiments and user studies demonstrate that our method achieves superior performance than previous works."}}
{"id": "q2RocwH7mB", "cdate": 1667366470084, "mdate": 1667366470084, "content": {"title": "L-CoDer: Language-based Colorization with Color-object Decoupling Transformer", "abstract": "Language-based colorization requires the colorized image to be consistent with the the user-provided language caption. A most recent work proposes to decouple the language into color and object conditions in solving the problem. Though decent progress has been made, its performance is limited by three key issues. (i) The large gap between vision and language modalities using independent feature extractors makes it difficult to fully understand the language. (ii) The inaccurate language features are never refined by the image features such that the language may fail to colorize the image precisely. (iii) The local region does not perceive the whole image, producing global inconsistent colors. In this work, we introduce transformer into language-based colorization to tackle the aforementioned issues while keeping the language decoupling property. Our method unifies the modalities of image and language, and further performs color conditions evolving with image features in a coarse-to-fine manner. In addition, thanks to the global receptive field, our method is robust to the strong local variation. Extensive experiments demonstrate our method is able to produce realistic colorization and outperforms prior arts in terms of consistency with the caption."}}
{"id": "xfdu59XQgts", "cdate": 1640995200000, "mdate": 1683788718031, "content": {"title": "CorefDPR: A Joint Model for Coreference Resolution and Dropped Pronoun Recovery in Chinese Conversations", "abstract": "In this work, we present that coreference resolution and dropped pronoun recovery are two strongly related tasks in Chinese conversations, as recovering the dropped pronoun needs to explore the referent of the pronoun at first. Meanwhile, the omitted entity mention should be recovered before its coreferences are resolved. This motivates us to propose CorefDPR, a novel model to jointly resolve these two tasks and make them enhance each other. CorefDPR firstly utilizes a pre-trained language model to encode tokens in the conversation snippet. Then, the coreference resolution layer detects all entity mentions from the candidate text spans and groups them as coreferent mention clusters based on the contextualized token states. Furthermore, the pronoun recovery layer explores the referent of each dropped pronoun from the coreferent mention clusters and predicts the probability distribution over pronoun category for each token. Finally, a general conditional random fields (GCRF) is employed to globally optimize the pronoun recovery sequence of the snippet by modeling both intra-utterance and cross-utterance pronoun dependencies, and the recovered pronouns are further linked back to corresponding mention clusters to complete them. Experimental results on the benchmark demonstrate that our proposed model outperformed the state-of-the-art baselines of both these two tasks, and the exploratory experiments also demonstrate that these two tasks mutually benefit each other."}}
{"id": "mPp9d0goTK", "cdate": 1640995200000, "mdate": 1667982774953, "content": {"title": "AdsCVLR: Commercial Visual-Linguistic Representation Modeling in Sponsored Search", "abstract": "Sponsored search advertisements (ads) appear next to search results when consumers look for products and services on search engines. As the fundamental basis of search ads, relevance modeling has attracted increasing attention due to the significant research challenges and tremendous practical value. In this paper, we address the problem of multi-modal modeling in sponsored search, which models the relevance between user query and commercial ads with multi-modal structured information. To solve this problem, we propose a transformer architecture with Ads data on Commercial Visual-Linguistic Representation (AdsCVLR) with contrastive learning that naturally extends the transformer encoder with the complementary multi-modal inputs, serving as a strong aggregator of image-text features. We also make a public advertising dataset, which includes 480K labeled query-ad pairwise data with structured information of image, title, seller, description, and so on. Empirically, we evaluate the AdsCVLR model over the large industry dataset, and the experimental results of online/offline tests show the superiority of our method."}}
{"id": "h3nPyQINKRS", "cdate": 1640995200000, "mdate": 1681651100850, "content": {"title": "Hybrid Face Reflectance, Illumination, and Shape From a Single Image", "abstract": ""}}
{"id": "h0_sqM258i", "cdate": 1640995200000, "mdate": 1681651100558, "content": {"title": "CT2: Colorization Transformer via Color Tokens", "abstract": ""}}
{"id": "aiyGJII7iU0", "cdate": 1640995200000, "mdate": 1683788718221, "content": {"title": "A Tag-Based Post-Hoc Framework for Explainable Conversational Recommendation", "abstract": "Explanations are important for conversational recommendation. They help users to understand how the recommender system works, and elicit user's responses by allowing users to provide informative feedback on them. During the interaction with users, explainable conversational recommender system provides explanations and collects user feedback to further refine the recommendations. Current conversational recommender systems, however, usually make recommendations with black-box prediction models, bringing difficulty in model explainability. Moreover, existing methods for explainable recommendation commonly provide one-shot explanations and fail to leverage user feedback. In this paper, we propose a tag-based post-hoc framework for explainable conversational recommendation (TPECR), which enables black-box recommendation models to provide explanations and refine recommendations based on user preference on tags (e.g., item attributes). Specifically, given the recommendation model being explained, TPECR trains a generation model to construct user embeddings based on their tag preferences. The explanations are provided by utilizing the generation model to estimate the contributions of different tags with respect to each item prediction. Given user feedback, the recommendation at the next turn is refined by tuning the tag preference and generating modified user embedding with the generation model. We instantiate the generation model with conditional variational auto-encoder (CVAE), which reconstructs user embedding conditioned on his tag preference. We conducted experiments by applying TPECR to different models and the results demonstrated the effectiveness of our TPECR on both synthetic and real datasets."}}
