{"id": "mzaX5eyWnY", "cdate": 1684343102175, "mdate": 1684343102175, "content": {"title": "Spatially Invariant Unsupervised 3D Object-Centric Learning and Scene Decomposition", "abstract": "We tackle the problem of object-centric learning on point clouds, which is crucial for high-level relational reasoning and scalable machine intelligence. In particular, we introduce a framework, SPAIR3D, to factorize a 3D point cloud into a spatial mixture model where each component corresponds to one object. To model the spatial mixture model on point clouds, we derive the Chamfer Mixture Loss, which fits naturally into our variational training pipeline. Moreover, we adopt an object-specification scheme that describes each object\u2019s location relative to its local voxel grid cell. Such a scheme allows SPAIR3D to model scenes with an arbitrary number of objects. We evaluate our method on the task of unsupervised scene decomposition. Experimental results demonstrate that SPAIR3D has strong scalability and is capable of detecting and segmenting an unknown number of objects from a point cloud in an unsupervised manner."}}
{"id": "zp-GWa6RIk", "cdate": 1672531200000, "mdate": 1699145464888, "content": {"title": "LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network", "abstract": "Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent blur is a challenging task.~Existing blur map-based deblurring methods have demonstrated promising results. In this paper, we propose, to the best of our knowledge, the first framework to introduce the contrastive language-image pre-training framework (CLIP) to achieve accurate blur map estimation from DP pairs unsupervisedly. To this end, we first carefully design text prompts to enable CLIP to understand blur-related geometric prior knowledge from the DP pair. Then, we propose a format to input stereo DP pair to the CLIP without any fine-tuning, where the CLIP is pre-trained on monocular images. Given the estimated blur map, we introduce a blur-prior attention block, a blur-weighting loss and a blur-aware loss to recover the all-in-focus image. Our method achieves state-of-the-art performance in extensive experiments."}}
{"id": "lwsvxCihqhu", "cdate": 1672531200000, "mdate": 1699145464881, "content": {"title": "VisFusion: Visibility-aware Online 3D Scene Reconstruction from Videos", "abstract": "We propose VisFusion, a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at: https://github.com/huiyu-gao/VisFusion"}}
{"id": "RG5S8C65HyV", "cdate": 1672531200000, "mdate": 1699145464870, "content": {"title": "Variational Inference for Scalable 3D Object-centric Learning", "abstract": "We tackle the task of scalable unsupervised object-centric representation learning on 3D scenes. Existing approaches to object-centric representation learning show limitations in generalizing to larger scenes as their learning processes rely on a fixed global coordinate system. In contrast, we propose to learn view-invariant 3D object representations in localized object coordinate systems. To this end, we estimate the object pose and appearance representation separately and explicitly map object representations across views while maintaining object identities. We adopt an amortized variational inference pipeline that can process sequential input and scalably update object latent distributions online. To handle large-scale scenes with a varying number of objects, we further introduce a Cognitive Map that allows the registration and query of objects on a per-scene global map to achieve scalable representation learning. We explore the object-centric neural radiance field (NeRF) as our 3D scene representation, which is jointly modeled within our unsupervised object-centric learning framework. Experimental results on synthetic and real datasets show that our proposed method can infer and maintain object-centric representations of 3D scenes and outperforms previous models."}}
{"id": "JmA2yVVFAB", "cdate": 1672531200000, "mdate": 1699145464896, "content": {"title": "Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird's Eye View", "abstract": "Recent vision-only perception models for autonomous driving achieved promising results by encoding multi-view image features into Bird's-Eye-View (BEV) space. A critical step and the main bottleneck of these methods is transforming image features into the BEV coordinate frame. This paper focuses on leveraging geometry information, such as depth, to model such feature transformation. Existing works rely on non-parametric depth distribution modeling leading to significant memory consumption, or ignore the geometry information to address this problem. In contrast, we propose to use parametric depth distribution modeling for feature transformation. We first lift the 2D image features to the 3D space defined for the ego vehicle via a predicted parametric depth distribution for each pixel in each view. Then, we aggregate the 3D feature volume based on the 3D space occupancy derived from depth to the BEV frame. Finally, we use the transformed features for downstream tasks such as object detection and semantic segmentation. Existing semantic segmentation methods do also suffer from an hallucination problem as they do not take visibility information into account. This hallucination can be particularly problematic for subsequent modules such as control and planning. To mitigate the issue, our method provides depth uncertainty and reliable visibility-aware estimations. We further leverage our parametric depth modeling to present a novel visibility-aware evaluation metric that, when taken into account, can mitigate the hallucination problem. Extensive experiments on object detection and semantic segmentation on the nuScenes datasets demonstrate that our method outperforms existing methods on both tasks."}}
{"id": "AdoLc5e-MER", "cdate": 1667535170362, "mdate": 1667535170362, "content": {"title": "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo", "abstract": "We propose a cost volume-based neural network for depth inference from multi-view images. We demonstrate that building a cost volume pyramid in a coarse-to-fine manner instead of constructing a cost volume at a fixed resolution leads to a compact, lightweight network and allows us inferring high resolution depth maps to achieve better reconstruction results. To this end, we first build a cost volume based on uniform sampling of fronto-parallel planes across the entire depth range at the coarsest resolution of an image. Then, given current depth estimate, we construct new cost volumes iteratively on the pixel wise depth residual to perform depth map refinement. While sharing similar in-sight with Point-MVSNet as predicting and refining depth iteratively, we show that working on cost volume pyramid can lead to a more compact, yet efficient network structure compared with the Point-MVSNet on 3D points. We further provide detailed analyses of the relation between (residual)depth sampling and image resolution, which serves as a principle for building compact cost volume pyramid. Experimental results on benchmark datasets show that our model can perform 6x faster and has similar performance as state-of-the-art methods."}}
{"id": "R1qL8VgJGdY", "cdate": 1665616560336, "mdate": 1665616560336, "content": {"title": "Self-supervised Learning of Depth Inference for Multi-view Stereo", "abstract": "Recent supervised multi-view depth estimation networks\nhave achieved promising results. Similar to all supervised approaches, these networks require ground-truth data\nduring training. However, collecting a large amount of\nmulti-view depth data is very challenging. Here, we propose a self-supervised learning framework for multi-view\nstereo that exploit pseudo labels from the input data. We\nstart by learning to estimate depth maps as initial pseudo\nlabels under an unsupervised learning framework relying on image reconstruction loss as supervision. We\nthen refine the initial pseudo labels using a carefully designed pipeline leveraging depth information inferred from\na higher resolution image and neighboring views. We use\nthese high-quality pseudo labels as the supervision signal to train the network and improve, iteratively, its performance by self-training. Extensive experiments on the\nDTU dataset show that our proposed self-supervised learning framework outperforms existing unsupervised multiview stereo networks by a large margin and performs\non par compared to the supervised counterpart. Code\nis available at https://github.com/JiayuYANG/\nSelf-supervised-CVP-MVSNet."}}
{"id": "jJgJM9I-P7", "cdate": 1665616280649, "mdate": null, "content": {"title": "Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo", "abstract": "Recent cost volume pyramid based deep neural networks have unlocked the potential of efficiently leveraging\nhigh-resolution images for depth inference from multi-view\nstereo. In general, those approaches assume that the depth\nof each pixel follows a unimodal distribution. Boundary\npixels usually follow a multi-modal distribution as they represent different depths; Therefore, the assumption results in\nan erroneous depth prediction at the coarser level of the\ncost volume pyramid and can not be corrected in the refinement levels leading to wrong depth predictions. In contrast,\nwe propose constructing the cost volume by non-parametric\ndepth distribution modeling to handle pixels with unimodal\nand multi-modal distributions. Our approach outputs multiple depth hypotheses at the coarser level to avoid errors\nin the early stage. As we perform local search around\nthese multiple hypotheses in subsequent levels, our approach does not maintain the rigid depth spatial ordering\nand, therefore, we introduce a sparse cost aggregation network to derive information within each volume. We evaluate\nour approach extensively on two benchmark datasets: DTU\nand Tanks & Temples. Our experimental results show that\nour model outperforms existing methods by a large margin\nand achieves superior performance on boundary regions."}}
{"id": "F7f4BYnDAIc", "cdate": 1663850247334, "mdate": null, "content": {"title": "Sampled Transformer for Point Sets", "abstract": "The sparse transformer can reduce the computational complexity of the self-attention layers to $O(n)$, whilst still being a universal approximator of continuous sequence-to-sequence functions. However, this permutation variant operation is not appropriate for direct application to sets. In this paper, we proposed an $O(n)$ complexity sampled transformer that can process point set elements directly without any additional inductive bias. Our sampled transformer introduces random element sampling, which randomly splits point sets into subsets, followed by applying a shared Hamiltonian self-attention mechanism to each subset. The overall attention mechanism can be viewed as a Hamiltonian cycle in the complete attention graph, and the permutation of point set elements is equivalent to randomly sampling Hamiltonian cycles. This mechanism implements a Monte Carlo simulation of the $O(n^2)$ dense attention connections. We show that it is a universal approximator for continuous set-to-set functions.  Experimental results for classification and few-shot learning on point-clouds show comparable or better accuracy with significantly reduced computational complexity compared to the dense transformer or alternative sparse attention schemes."}}
{"id": "yvF7mAuWv3z", "cdate": 1663850059752, "mdate": null, "content": {"title": "Scalable 3D Object-centric Learning", "abstract": "We tackle the task of unsupervised 3D object-centric representation learning on scenes of potentially unbounded scale. \n  Existing approaches to object-centric representation learning exhibit significant limitations in achieving scalable inference due to their dependencies on a fixed global coordinate system. \n  In contrast, we propose to learn view-invariant 3D object representations in localized object coordinate systems. \n  To this end, we estimate the object pose and appearance representation separately and explicitly project object representations across views. \n  We adopt amortized variational inference to process sequential input and update object representations online. \n  To scale up our model to scenes with an arbitrary number of objects, we further introduce a Cognitive Map that allows the registration and querying of objects on a global map. \n  We employ the object-centric neural radiance field (NeRF) as our 3D scene representation, which is jointly inferred by our unsupervised object-centric learning framework. \n  Experimental results demonstrate that our method can infer and maintain object-centric representations of unbounded 3D scenes. \n  Further combined with a per-object NeRF finetuning process, our model can achieve scalable high-quality object-aware scene reconstruction."}}
