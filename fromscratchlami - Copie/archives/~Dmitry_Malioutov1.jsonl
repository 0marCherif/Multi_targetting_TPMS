{"id": "pdll8nmP0WI", "cdate": 1483228800000, "mdate": null, "content": {"title": "Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017).", "abstract": "This is the Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10, 2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag."}}
{"id": "UWoRa2A2K5l", "cdate": 1451606400000, "mdate": null, "content": {"title": "Covariance Matrix Estimation for Interest-Rate Risk Modeling via Smooth and Monotone Regularization", "abstract": "Estimating covariance matrices in high-dimensional settings is a challenging problem central to modern finance. The sample covariance matrix is well-known to give poor estimates in high dimensions with insufficient samples, and may cause severe risk underestimates of optimized portfolios in the Markowitz framework. In order to provide useful estimates in this regime, a variety of improved covariance matrix estimates have been developed that exploit additional structure in the data. Popular approaches include low-rank (principal component and factor analysis) models, banded structure, sparse inverse covariances, and parametric models. We investigate a novel nonparametric prior for random vectors which have a spatial ordering: we assume that the covariance is monotone and smooth with respect to this ordering. This applies naturally to problems such as interest-rate risk modeling, where correlations decay for contracts that are further apart in terms of expiration dates. We propose a convex optimization (semi-definite programming) formulation for this estimation problem, and develop efficient algorithms. We apply our framework for risk measurement and forecasting with Eurodollar futures, investigate limited, missing and asynchronous data, and show that it provides valid (positive-definite) covariance estimates more accurate than existing methods."}}
{"id": "KOQxMpC-hch", "cdate": 1451606400000, "mdate": null, "content": {"title": "Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)", "abstract": "This is the Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016. Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang, and Hanna Wallach."}}
{"id": "I4bZn6bPffm", "cdate": 1451606400000, "mdate": null, "content": {"title": "Interpretable Two-level Boolean Rule Learning for Classification", "abstract": "As a contribution to interpretable machine learning research, we develop a novel optimization framework for learning accurate and sparse two-level Boolean rules. We consider rules in both conjunctive normal form (AND-of-ORs) and disjunctive normal form (OR-of-ANDs). A principled objective function is proposed to trade classification accuracy and interpretability, where we use Hamming loss to characterize accuracy and sparsity to characterize interpretability. We propose efficient procedures to optimize these objectives based on linear programming (LP) relaxation, block coordinate descent, and alternating minimization. Experiments show that our new algorithms provide very good tradeoffs between accuracy and interpretability."}}
{"id": "DOiHwI666qv", "cdate": 1451606400000, "mdate": null, "content": {"title": "Learning sparse two-level boolean rules", "abstract": "This paper develops a novel optimization framework for learning accurate and sparse two-level Boolean rules for classification, both in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) and in Disjunctive Normal Form (DNF, i.e. OR-of-ANDs). In contrast to opaque models (e.g. neural networks), sparse two-level Boolean rules gain the crucial benefit of interpretability, which is necessary in a wide range of applications such as law and medicine and is attracting considerable attention in machine learning. This paper introduces two principled objective functions to trade off classification accuracy and sparsity, where 0-1 error and Hamming loss are used to characterize accuracy. We propose efficient procedures to optimize these objectives based on linear programming (LP) relaxation, block coordinate descent, and alternating minimization. We also describe a new approach to rounding any fractional values in the optimal solutions of LP relaxations. Experiments show that our new algorithms based on the Hamming loss objective provide excellent tradeoffs between accuracy and sparsity with improvements over state-of-the-art methods."}}
{"id": "ryWvyiWd-r", "cdate": 1420070400000, "mdate": null, "content": {"title": "Large-scale log-determinant computation through stochastic Chebyshev expansions", "abstract": "Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of ..."}}
{"id": "ezPoy8L95O7", "cdate": 1420070400000, "mdate": null, "content": {"title": "Interpretable Two-level Boolean Rule Learning for Classification", "abstract": "This paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule. Two formulations are proposed. The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule. We generalize a previously proposed linear programming (LP) relaxation from one-level to two-level rules. The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample. Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed. Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison. A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective."}}
{"id": "V8JFds8RKzj", "cdate": 1420070400000, "mdate": null, "content": {"title": "Learning interpretable classification rules using sequential rowsampling", "abstract": "In our previous work we have presented an approach to learn interpretable classification rules using a Boolean compressed sensing formulation. Our approach uses a linear programming (LP) relaxation and allows us to find interpretable (sparse) classification rules that achieve good generalization accuracy. However, the resulting LP representation for problems with either a large number of samples or large number of continuous features tends to become challenging for off-the-shelf LP solvers. We have explored a screening approach which allows us to dramatically reduce the number of active features without sacrificing optimality. In this work we explore reducing the number of samples in a sequential setting where we can certify reaching a near-optimal solution while only solving the LP on a small fraction of the available data points. In a batch setting this approach can dramatically reduce the computational complexity of the rule-learning LP formulation. In an online setting we derive stochastic upper and lower bounds on the the LP objective for unseen samples. This allows early stopping when we detect that the classifier will not change significantly with additional samples. The upper bounds are related to the learning curve literature in machine learning, and our lower bounds appear not to have been explored. Finally, we discuss a quick approach to compute the complete regularization path balancing rule interpretability versus accuracy."}}
{"id": "s615VMX6V-W", "cdate": 1388534400000, "mdate": null, "content": {"title": "Screening for learning classification rules via Boolean compressed sensing", "abstract": "Convex relaxations for sparse representation problems, which aim to find sparse solutions to systems of equations, have enabled a variety of exciting applications in high-dimensional settings. Yet, with dimensions large enough, even these convex formulations become prohibitively expensive. Screening methods attempt to use duality theory to dramatically reduce the size of the optimization problem through easily computable certificates that many of the variables must be zero in the optimal solution. In this paper we consider learning sparse classification rules via Boolean compressed sensing and develop screening procedures that can significantly reduce the size of the resulting linear program. Boolean compressed sensing deals with systems of Boolean equations (instead of linear equations in traditional compressed sensing); we develop screening methods specifically for this setting. We demonstrate the effectiveness of our screening rules on several real-world classification data sets."}}
{"id": "B1Ndeh-dbH", "cdate": 1388534400000, "mdate": null, "content": {"title": "Convex Total Least Squares", "abstract": "We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fi..."}}
