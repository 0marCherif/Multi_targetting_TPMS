{"id": "9Nj_gNdvqYf", "cdate": 1663850309084, "mdate": null, "content": {"title": "Leveraging Importance Weights in Subset Selection", "abstract": "We present a subset selection algorithm designed to work with arbitrary model families in a practical batch setting. In such a setting, an algorithm can sample examples one at a time but, in order to limit overhead costs, is only able to update its state (i.e. further train model weights) once a large enough batch of examples is selected.  Our algorithm, IWeS, selects examples by importance sampling where the sampling probability assigned to each example is based on the entropy of models trained on previously selected batches. IWeS admits significant performance improvement compared to other subset selection algorithms for seven publicly available datasets. Additionally, it is competitive in an active learning setting, where the label information is not available at selection time. We also provide an initial theoretical analysis to support our importance weighting approach, proving generalization and sampling rate bounds."}}
{"id": "bt25vx3aW_", "cdate": 1652737785991, "mdate": null, "content": {"title": "Adversarial Robustness is at Odds with Lazy Training", "abstract": "Recent works show that adversarial examples exist for random neural networks [Daniely and Schacham, 2020] and that these examples can be found using a single step of gradient ascent [Bubeck et al., 2021]. In this work, we extend this line of work to ``lazy training'' of neural networks -- a dominant model in deep learning theory in which neural networks are provably efficiently learnable. We show that over-parametrized neural networks that are guaranteed to generalize well and enjoy strong computational guarantees remain vulnerable to attacks generated using a single step of gradient ascent. "}}
{"id": "92Unp0nulR4", "cdate": 1609459200000, "mdate": 1648684547926, "content": {"title": "Robust Learning for Data Poisoning Attacks", "abstract": "We investigate the robustness of stochastic approximation approaches against data poisoning attacks. We focus on two-layer neural networks with ReLU activation and show that under a specific notion..."}}
{"id": "aaisQE18kxH", "cdate": 1577836800000, "mdate": 1648684547910, "content": {"title": "Making Recommendations when Users Experience Fatigue", "abstract": ""}}
{"id": "zOpGc-EsNu", "cdate": 1546300800000, "mdate": 1648684547924, "content": {"title": "Base belief function: an efficient method of conflict management", "abstract": "Dempster\u2013Shafer evidence theory is widely used in many applications such as decision making and pattern recognition. However, Dempster\u2019s combination rule often produces results that do not reflect the actual distribution of belief when collected evidence highly conflicts each other. In this paper, a base belief function is proposed to modify the classical basic probability assignment before combination in closed-world. Base belief function focuses on making combination result intuitive especially when evidences highly conflict each other. Compared to other methods, the combination result produced by proposed method is logical and consistent with real world with less computational complexity and better performance. The advantage of base belief function is that it can avoid high conflicts between evidences and is especially suitable for the situation where the evidences appear in sequence. Several numerical examples as well as experiments using real data sets from the UCI machine learning repository for classification are employed to verify the rationality of the proposed method."}}
{"id": "C5Zsvt4R7BO", "cdate": 1546300800000, "mdate": 1648684547920, "content": {"title": "Thompson Sampling for a Fatigue-aware Online Recommendation System", "abstract": "In this paper we consider an online recommendation setting, where a platform recommends a sequence of items to its users at every time period. The users respond by selecting one of the items recommended or abandon the platform due to fatigue from seeing less useful items. Assuming a parametric stochastic model of user behavior, which captures positional effects of these items as well as the abandoning behavior of users, the platform's goal is to recommend sequences of items that are competitive to the single best sequence of items in hindsight, without knowing the true user model a priori. Naively applying a stochastic bandit algorithm in this setting leads to an exponential dependence on the number of items. We propose a new Thompson sampling based algorithm with expected regret that is polynomial in the number of items in this combinatorial setting, and performs extremely well in practice."}}
