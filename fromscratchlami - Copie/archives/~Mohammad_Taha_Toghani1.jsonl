{"id": "gnVFs9pt2e", "cdate": 1664924969474, "mdate": null, "content": {"title": "PersA-FL: Personalized Asynchronous Federated Learning", "abstract": "We study the personalized federated learning problem under asynchronous updates. In this problem, each client seeks to obtain a personalized model that simultaneously outperforms local and global models. We consider two optimization-based frameworks for personalization: (i) Model-Agnostic Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a joint model adapted for each client through fine-tuning, whereas ME requires a bi-level optimization problem with implicit gradients to enforce personalization via regularized losses. We focus on improving the scalability of personalized federated learning by removing the synchronous communication assumption. Moreover, we extend the studied function class by removing boundedness assumptions on the gradient norm. Our main technical contribution is a unified proof for asynchronous federated learning with bounded staleness that we apply to MAML and ME personalization frameworks. For the smooth and non-convex functions class, we show the convergence of our method to a first-order stationary point. We illustrate the performance of our method and its tolerance to staleness through experiments for classification tasks over heterogeneous datasets."}}
{"id": "MCULxzgbFMC", "cdate": 1663939410391, "mdate": null, "content": {"title": "Unbounded Gradients in Federated Learning with Buffered Asynchronous Aggregation", "abstract": "Synchronous updates may compromise the efficiency of cross-device federated learning once the number of active clients increases. The FedBuff algorithm (Nguyen et al.) alleviates this problem by allowing asynchronous updates (staleness), which enhances the scalability of training while preserving privacy via secure aggregation. We revisit the FedBuff algorithm for asynchronous federated learning and extend the existing analysis by removing the boundedness assumptions from the gradient norm. This paper presents a theoretical analysis of the convergence rate of this algorithm when heterogeneity in data, batch size, and delay are considered."}}
{"id": "B97oWNza7ej", "cdate": 1653595781755, "mdate": null, "content": {"title": "PARS-Push: Personalized, Asynchronous and Robust Decentralized Optimization", "abstract": "We study the decentralized multi-step Model-Agnostic Meta-Learning (MAML) framework where a group of $n$ agents seeks to find a common point that enables ``few-shot'' learning (personalization) via local stochastic gradient steps on their local functions. We formulate the personalized optimization problem under the MAML framework and propose PARS-Push, a decentralized asynchronous algorithm robust to message failures, communication delays, and directed message sharing. We characterize the convergence rate of PARS-Push for smooth and strongly convex and smooth and non-convex functions under arbitrary multi-step personalization. Moreover, we provide numerical experiments showing its performance under heterogeneous data setups."}}
{"id": "uZwkAm0yUUY", "cdate": 1648673408543, "mdate": null, "content": {"title": "On Arbitrary Compression for Decentralized Consensus and Stochastic Optimization over Directed Networks", "abstract": "We study the decentralized consensus and stochastic optimization problems under compressed message sharing over a fixed directed graph. Our goal is to minimize the sum of $n$ functions, each accessible to a single node, where local communications are subject to a directed network. In this work, we propose an iterative push-sum algorithm with compressed message sharing to reduce the communication overhead on the network. Contrary to existing literature, we allow for arbitrary compression ratios in the communicated messages. We provide explicit convergence rates for the stochastic optimization problem on smooth functions that are either (i) strongly convex, (ii) convex, or (iii) non-convex. Finally, we provide numerical experiments to illustrate the arbitrary compression and communication efficiency of our algorithm."}}
{"id": "TPX9pAlUYOu", "cdate": 1640995200000, "mdate": 1651561470703, "content": {"title": "On Arbitrary Compression for Decentralized Consensus and Stochastic Optimization over Directed Networks", "abstract": "We study the decentralized consensus and stochastic optimization problems with compressed communications over static directed graphs. We propose an iterative gradient-based algorithm that compresses messages according to a desired compression ratio. The proposed method provably reduces the communication overhead on the network at every communication round. Contrary to existing literature, we allow for arbitrary compression ratios in the communicated messages. We show a linear convergence rate for the proposed method on the consensus problem. Moreover, we provide explicit convergence rates for decentralized stochastic optimization problems on smooth functions that are either (i) strongly convex, (ii) convex, or (iii) non-convex. Finally, we provide numerical experiments to illustrate convergence under arbitrary compression ratios and the communication efficiency of our algorithm."}}
{"id": "EKpTpk3u1-3", "cdate": 1640995200000, "mdate": 1651561470678, "content": {"title": "Local Stochastic Factored Gradient Descent for Distributed Quantum State Tomography", "abstract": "We propose a distributed Quantum State Tomography (QST) protocol, named Local Stochastic Factored Gradient Descent (Local SFGD), to learn the low-rank factor of a density matrix over a set of local machines. QST is the canonical procedure to characterize the state of a quantum system, which we formulate as a stochastic nonconvex smooth optimization problem. Physically, the estimation of a low-rank density matrix helps characterizing the amount of noise introduced by quantum computation. Theoretically, we prove the local convergence of Local SFGD for a general class of restricted strongly convex/smooth loss functions, i.e., Local SFGD converges locally to a small neighborhood of the global optimum at a linear rate with a constant step size, while it locally converges exactly at a sub-linear rate with diminishing step sizes. With a proper initialization, local convergence results imply global convergence. We validate our theoretical findings with numerical simulations of QST on the Greenberger-Horne-Zeilinger (GHZ) state."}}
{"id": "w_HYNnAD6lg", "cdate": 1609459200000, "mdate": 1631744325337, "content": {"title": "Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs", "abstract": "We present a novel, practical, and provable approach for solving diagonally constrained semi-definite programming (SDP) problems at scale using accelerated non-convex programming. Our algorithm non-trivially combines acceleration motions from convex optimization with coordinate power iteration and matrix factorization techniques. The algorithm is extremely simple to implement, and adds only a single extra hyperparameter -- momentum. We prove that our method admits local linear convergence in the neighborhood of the optimum and always converges to a first-order critical point. Experimentally, we showcase the merits of our method on three major application domains: MaxCut, MaxSAT, and MIMO signal detection. In all cases, our methodology provides significant speedups over non-convex and convex SDP solvers -- 5X faster than state-of-the-art non-convex solvers, and 9 to 10^3 X faster than convex SDP solvers -- with comparable or improved solution quality."}}
{"id": "s9XCTUSQL2h", "cdate": 1609459200000, "mdate": 1631744325290, "content": {"title": "MP-Boost: Minipatch Boosting via Adaptive Feature and Observation Sampling", "abstract": "Boosting methods are among the best generalpurpose and off-the-shelf machine learning approaches, gaining widespread popularity. In this paper, we seek to develop a boosting method that yields comparable accuracy to popular AdaBoost and gradient boosting methods, yet is faster computationally and whose solution is more interpretable. We achieve this by developing MP-Boost, an algorithm loosely based on AdaBoost that learns by adaptively selecting small subsets of instances and features, or what we term minipatches (MP), at each iteration. By sequentially learning on tiny subsets of the data, our approach is computationally faster than classic boosting algorithms. MP-Boost upweights important features and challenging instances, hence adaptively selects the most relevant minipatches for learning. The learned probability distributions aid in interpretation of our method. We empirically demonstrate the interpretability and comparative accuracy of our algorithm on a variety of binary classification tasks."}}
{"id": "UB0EPf7bOjY", "cdate": 1609459200000, "mdate": 1651561470705, "content": {"title": "Communication-Efficient and Fault-Tolerant Social Learning", "abstract": "We study the problem of decentralized non-Bayesian (social) learning with compressed communications in the presence of adversarial agents, i.e., Byzantine. The non-adversarial agents seek to collaboratively agree on a hypothesis that best describes an unknown state of the world. We propose a robust update rule where, under appropriate assumptions, non-faulty agents can reach a consensus on the true state of the world by sharing arbitrarily compressed messages over the network. We show almost sure asymptotic convergence of the beliefs of non-faulty agents around the optimal hypothesis and provide numerical evidence for the communication efficiency and robustness of the proposed algorithm."}}
{"id": "MK5aQQYeJ0n", "cdate": 1609459200000, "mdate": 1631744325337, "content": {"title": "Communication-Efficient Distributed Cooperative Learning with Compressed Beliefs", "abstract": "We study the problem of distributed cooperative learning, where a group of agents seeks to agree on a set of hypotheses that best describes a sequence of private observations. In the scenario where the set of hypotheses is large, we propose a belief update rule where agents share compressed (either sparse or quantized) beliefs with an arbitrary positive compression rate. Our algorithm leverages a unified communication rule that enables agents to access wide-ranging compression operators as black-box modules. We prove the almost sure asymptotic exponential convergence of beliefs around the set of optimal hypotheses. Additionally, we show a non-asymptotic, explicit, and linear concentration rate in probability of the beliefs on the optimal hypothesis set. We provide numerical experiments to illustrate the communication benefits of our method. The simulation results show that the number of transmitted bits can be reduced to 5-10% of the non-compressed method in the studied scenarios."}}
