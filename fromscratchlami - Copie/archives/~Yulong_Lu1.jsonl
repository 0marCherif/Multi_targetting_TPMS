{"id": "3njKGEaCOeM", "cdate": 1672531200000, "mdate": 1682374715874, "content": {"title": "On the convergence of Sobolev gradient flow for the Gross-Pitaevskii eigenvalue problem", "abstract": "We study the convergences of three projected Sobolev gradient flows to the ground state of the Gross-Pitaevskii eigenvalue problem. They are constructed as the gradient flows of the Gross-Pitaevskii energy functional with respect to the $H^1_0$-metric and two other equivalent metrics on $H_0^1$, including the iterate-independent $a_0$-metric and the iterate-dependent $a_u$-metric. We first prove the energy dissipation property and the global convergence to a critical point of the Gross-Pitaevskii energy for the discrete-time $H^1$ and $a_0$-gradient flow. We also prove local exponential convergence of all three schemes to the ground state."}}
{"id": "Y5H0x31zZBI", "cdate": 1640995200000, "mdate": 1682374716074, "content": {"title": "A Regularity Theory for Static Schr\u00f6dinger Equations on Rd in Spectral Barron Spaces", "abstract": "Spectral Barron spaces have received considerable interest recently as it is the natural function space for approximation theory of two-layer neural networks with a dimension-free convergence rate. In this paper we study the regularity of solutions to the whole-space static Schr\\\"odinger equation in spectral Barron spaces. We prove that if the source of the equation lies in the spectral Barron space $\\mathcal{B}^s(\\mathbb{R}^d)$ and the potential function admitting a non-negative lower bound decomposes as a positive constant plus a function in $\\mathcal{B}^s(\\mathbb{R}^d)$, then the solution lies in the spectral Barron space $\\mathcal{B}^{s+2}(\\mathbb{R}^d)$."}}
{"id": "TjMB4cfrbG", "cdate": 1640995200000, "mdate": 1682374715935, "content": {"title": "Transfer Learning Enhanced DeepONet for Long-Time Prediction of Evolution Equations", "abstract": "Deep operator network (DeepONet) has demonstrated great success in various learning tasks, including learning solution operators of partial differential equations. In particular, it provides an efficient approach to predict the evolution equations in a finite time horizon. Nevertheless, the vanilla DeepONet suffers from the issue of stability degradation in the long-time prediction. This paper proposes a {\\em transfer-learning} aided DeepONet to enhance the stability. Our idea is to use transfer learning to sequentially update the DeepONets as the surrogates for propagators learned in different time frames. The evolving DeepONets can better track the varying complexities of the evolution equations, while only need to be updated by efficient training of a tiny fraction of the operator networks. Through systematic experiments, we show that the proposed method not only improves the long-time accuracy of DeepONet while maintaining similar computational cost but also substantially reduces the sample size of the training set."}}
{"id": "QVikXDzHB2J", "cdate": 1640995200000, "mdate": 1682374715757, "content": {"title": "Two-Scale Gradient Descent Ascent Dynamics Finds Mixed Nash Equilibria of Continuous Games: A Mean-Field Perspective", "abstract": "Finding the mixed Nash equilibria (MNE) of a two-player zero sum continuous game is an important and challenging problem in machine learning. A canonical algorithm to finding the MNE is the noisy gradient descent ascent method which in the infinite particle limit gives rise to the {\\em Mean-Field Gradient Descent Ascent} (GDA) dynamics on the space of probability measures. In this paper, we first study the convergence of a two-scale Mean-Field GDA dynamics for finding the MNE of the entropy-regularized objective. More precisely we show that for each finite temperature (or regularization parameter), the two-scale Mean-Field GDA with a suitable {\\em finite} scale ratio converges exponentially to the unique MNE without assuming the convexity or concavity of the interaction potential. The key ingredient of our proof lies in the construction of new Lyapunov functions that dissipate exponentially along the Mean-Field GDA. We further study the simulated annealing of the Mean-Field GDA dynamics. We show that with a temperature schedule that decays logarithmically in time the annealed Mean-Field GDA converges to the MNE of the original unregularized objective."}}
{"id": "ST1P270dwOE", "cdate": 1621630014214, "mdate": null, "content": {"title": "On the Representation of Solutions to Elliptic PDEs in Barron Spaces", "abstract": "Numerical solutions to high-dimensional partial differential equations (PDEs) based on neural networks have seen exciting developments. This paper derives complexity estimates of the solutions of $d$-dimensional second-order elliptic PDEs in the Barron space, that is a set of functions admitting the integral of certain parametric ridge function against a probability measure on the parameters. We prove under some appropriate assumptions that if the coefficients and the source term of the elliptic PDE lie in Barron spaces, then the solution of the PDE is $\\epsilon$-close with respect to the $H^1$ norm to a Barron function. Moreover, we prove dimension-explicit bounds for the Barron norm of this approximate solution, depending at most polynomially on the dimension $d$ of the PDE. As a direct consequence of the complexity estimates, the solution of the PDE can be approximated on any bounded domain by a two-layer neural network with respect to the $H^1$ norm with a dimension-explicit convergence rate."}}
{"id": "s70uVhsQrg", "cdate": 1609459200000, "mdate": 1682374715895, "content": {"title": "Solving multiscale steady radiative transfer equation using neural networks with uniform stability", "abstract": "This paper concerns solving the steady radiative transfer equation with diffusive scaling, using the physics informed neural networks (PINNs). The idea of PINNs is to minimize a least-square loss function, that consists of the residual from the governing equation, the mismatch from the boundary conditions, and other physical constraints such as conservation. It is advantageous of being flexible and easy to execute, and brings the potential for high dimensional problems. Nevertheless, due the presence of small scales, the vanilla PINNs can be extremely unstable for solving multiscale steady transfer equations. In this paper, we propose a new formulation of the loss based on the macro-micro decomposition. We prove that, the new loss function is uniformly stable with respect to the small Knudsen number in the sense that the $L^2$-error of the neural network solution is uniformly controlled by the loss. When the boundary condition is an-isotropic, a boundary layer emerges in the diffusion limit and therefore brings an additional difficulty in training the neural network. To resolve this issue, we include a boundary layer corrector that carries over the sharp transition part of the solution and leaves the rest easy to be approximated. The effectiveness of the new methodology is demonstrated in extensive numerical examples."}}
{"id": "mZkBkROG_r", "cdate": 1609459200000, "mdate": 1682374715918, "content": {"title": "A Priori Generalization Analysis of the Deep Ritz Method for Solving High Dimensional Elliptic Equations", "abstract": "This paper concerns the a priori generalization analysis of the Deep Ritz Method (DRM) [W. E and B. Yu, 2017], a popular neural-network-based method for solving high dimensional partial differential equations. We derive the generalization error bounds of two-layer neural networks in the framework of the DRM for solving two prototype elliptic PDEs: Poisson equation and static Schr\\\"odinger equation on the $d$-dimensional unit hypercube. Specifically, we prove that the convergence rates of generalization errors are independent of the dimension $d$, under the a priori assumption that the exact solutions of the PDEs lie in a suitable low-complexity space called spectral Barron space. Moreover, we give sufficient conditions on the forcing term and the potential function which guarantee that the solutions are spectral Barron functions. We achieve this by developing a new solution theory for the PDEs on the spectral Barron space, which can be viewed as an analog of the classical Sobolev regularity theory for PDEs."}}
{"id": "ZqnEIYE06KI", "cdate": 1609459200000, "mdate": 1682374715767, "content": {"title": "A Priori Generalization Error Analysis of Two-Layer Neural Networks for Solving High Dimensional Schr\u00f6dinger Eigenvalue Problems", "abstract": "This paper analyzes the generalization error of two-layer neural networks for computing the ground state of the Schr\\\"odinger operator on a $d$-dimensional hypercube. We prove that the convergence rate of the generalization error is independent of the dimension $d$, under the a priori assumption that the ground state lies in a spectral Barron space. We verify such assumption by proving a new regularity estimate for the ground state in the spectral Barron space. The later is achieved by a fixed point argument based on the Krein-Rutman theorem."}}
{"id": "UK3XUnn2AH", "cdate": 1609459200000, "mdate": 1682374716016, "content": {"title": "A Priori Generalization Analysis of the Deep Ritz Method for Solving High Dimensional Elliptic Partial Differential Equations", "abstract": "This paper concerns the a priori generalization analysis of the Deep Ritz Method (DRM) [W. E and B. Yu, 2017], a popular neural-network-based method for solving high dimensional partial differentia..."}}
{"id": "LQX3NQ9FTvf", "cdate": 1609459200000, "mdate": 1682374715777, "content": {"title": "On the Representation of Solutions to Elliptic PDEs in Barron Spaces", "abstract": "Numerical solutions to high-dimensional partial differential equations (PDEs) based on neural networks have seen exciting developments. This paper derives complexity estimates of the solutions of $d$-dimensional second-order elliptic PDEs in the Barron space, that is a set of functions admitting the integral of certain parametric ridge function against a probability measure on the parameters. We prove under some appropriate assumptions that if the coefficients and the source term of the elliptic PDE lie in Barron spaces, then the solution of the PDE is $\\epsilon$-close with respect to the $H^1$ norm to a Barron function. Moreover, we prove dimension-explicit bounds for the Barron norm of this approximate solution, depending at most polynomially on the dimension $d$ of the PDE. As a direct consequence of the complexity estimates, the solution of the PDE can be approximated on any bounded domain by a two-layer neural network with respect to the $H^1$ norm with a dimension-explicit convergence rate."}}
