{"id": "XConDmwIQ0I", "cdate": 1672531200000, "mdate": 1682346909225, "content": {"title": "AIM: Adapting Image Models for Efficient Video Action Recognition", "abstract": "Recent vision transformer based video models mostly follow the ``image pre-training then finetuning\" paradigm and have achieved great success on multiple video benchmarks. However, full finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is \\url{https://adapt-image-models.github.io/}."}}
{"id": "8RxbxTu6SK", "cdate": 1672531200000, "mdate": 1682346909157, "content": {"title": "A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition", "abstract": "The goal of building a benchmark (suite of datasets) is to provide a unified protocol for fair evaluation and thus facilitate the evolution of a specific area. Nonetheless, we point out that existing protocols of action recognition could yield partial evaluations due to several limitations. To comprehensively probe the effectiveness of spatiotemporal representation learning, we introduce BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18 video datasets grouped into 5 categories (anomaly, gesture, daily, sports, and instructional), which covers a diverse set of real-world applications. With BEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both supervised and self-supervised learning. We also report transfer performance via standard finetuning, few-shot finetuning, and unsupervised domain adaptation. Our observation suggests that current state-of-the-art cannot solidly guarantee high performance on datasets close to real-world applications, and we hope BEAR can serve as a fair and challenging evaluation benchmark to gain insights on building next-generation spatiotemporal learners. Our dataset, code, and models are released at: https://github.com/AndongDeng/BEAR"}}
{"id": "QoCjyyPR2X", "cdate": 1668775645628, "mdate": 1668775645628, "content": {"title": "MutualNet: Adaptive ConvNet via Mutual Learning from Different Model Configurations", "abstract": "Most existing deep neural networks are static, which means they can only perform inference at a fixed complexity. But the\nresource budget can vary substantially across different devices. Even on a single device, the affordable budget can change with\ndifferent scenarios, and repeatedly training networks for each required budget would be incredibly expensive. Therefore, in this work,\nwe propose a general method called MutualNet to train a single network that can run at a diverse set of resource constraints. Our\nmethod trains a cohort of model configurations with various network widths and input resolutions. This mutual learning scheme not only\nallows the model to run at different width-resolution configurations but also transfers the unique knowledge among these\nconfigurations, helping the model to learn stronger representations overall. MutualNet is a general training methodology that can be\napplied to various network structures (e.g., 2D networks: MobileNets, ResNet, 3D networks: SlowFast, X3D) and various tasks (e.g.,\nimage classification, object detection, segmentation, and action recognition), and is demonstrated to achieve consistent improvements\non a variety of datasets. Since we only train the model once, it also greatly reduces the training cost compared to independently\ntraining several models. Surprisingly, MutualNet can also be used to significantly boost the performance of a single network, if dynamic\nresource constraints are not a concern. In summary, MutualNet is a unified method for both static and adaptive, 2D and 3D networks.\nCode and pre-trained models are available at https://github.com/taoyang1122/MutualNet.\n"}}
{"id": "CIoSZ_HKHS7", "cdate": 1663850022827, "mdate": null, "content": {"title": "AIM: Adapting Image Models for Efficient Video Action Recognition", "abstract": "Recent vision transformer based video models mostly follow the ``image pre-training then finetuning\" paradigm and have achieved great success on multiple video benchmarks. However, fully finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is https://adapt-image-models.github.io/."}}
{"id": "EmH1WE1fRbt", "cdate": 1663849948109, "mdate": null, "content": {"title": "Exploring Parameter-Efficient Fine-tuning for Improving Communication Efficiency in Federated Learning", "abstract": "Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. However, this can quickly put a massive communication burden on the system, especially if more capable models beyond very small MLPs are employed.\nRecently, the use of pre-trained models has been shown effective in federated learning optimization and improving convergence. This opens the door for new research questions. Can we adjust the weight-sharing paradigm in federated learning, leveraging strong and readily-available pre-trained models, to significantly reduce the communication burden while simultaneously achieving excellent performance? To this end, we investigate the use of parameter-efficient fine-tuning in federated learning. Specifically, we systemically evaluate the performance of several parameter-efficient fine-tuning methods across a variety of client stability, data distribution, and differential privacy settings. By only locally tuning and globally sharing a small portion of the model weights, significant reductions in the total communication overhead can be achieved while maintaining competitive performance in a wide range of federated learning scenarios, providing insight into a new paradigm for practical and effective federated systems."}}
{"id": "sLPhtvZ9A7f", "cdate": 1663849881816, "mdate": null, "content": {"title": "FeatER: An Efficient Network for Human Reconstruction Feature map-based TransformER", "abstract": "Recently, vision transformers have shown great success in a set of human reconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks, feature map representations of the human structural information are often extracted first from the image by a CNN (such as HRNet), and then further processed by transformer to predict the heatmaps (encodes each joint's location into a feature map with a Gaussian distribution) for HPE or HMR. However, existing transformer architectures are not able to process these feature map inputs directly, forcing an unnatural flattening of the location-sensitive human structural information. Furthermore, much of the performance benefit in recent HPE and HMR methods has come at the cost of ever-increasing computation and memory needs. Therefore, to simultaneously address these problems, we propose FeatER, a novel transformer design which preserves the inherent structure of feature map representations when modeling attention while reducing the memory and computational costs. Taking advantage of FeatER, we build an efficient network for a set of human reconstruction tasks including 2D HPE, 3D HPE, and HMR. A feature map reconstruction module is applied to improve the performance of the estimated human pose and mesh. Extensive experiments demonstrate the effectiveness of FeatER on various human pose and mesh datasets. For instance, FeatER outperforms the SOTA method MeshGraphormer by requiring 5\\% of Params (total parameters) and 16\\% of MACs (the Multiply\u2013Accumulate Operations) on Human3.6M and 3DPW datasets. Code will be publicly available."}}
{"id": "wQDdEFPy6vi", "cdate": 1632875504657, "mdate": null, "content": {"title": "Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning", "abstract": "Federated learning (FL) is a promising strategy for performing privacy-preserving, distributed learning with a network of clients (i.e., edge devices). However, the data distribution among clients is often non-IID in nature, making efficient optimization difficult. To alleviate this issue, many FL algorithms focus on mitigating the effects of data heterogeneity across clients by introducing a variety of proximal terms, some incurring considerable compute and/or memory overheads, to restrain local updates with respect to the global model. Instead, we consider rethinking solutions to data heterogeneity in FL with a focus on local learning generality rather than proximal restriction. Inspired by findings from generalization literature, we employ second-order information to better understand algorithm effectiveness in FL, and find that in many cases standard regularization methods are surprisingly strong performers in mitigating data heterogeneity effects. Armed with key insights from our analysis, we propose a simple and effective method, FedAlign, to overcome data heterogeneity and the pitfalls of previous methods. FedAlign achieves comparable accuracy with state-of-the-art FL methods across a variety of settings while minimizing computation and memory overhead. "}}
{"id": "7tZdmLFs4T_", "cdate": 1609459200000, "mdate": null, "content": {"title": "Consistency-based Active Learning for Object Detection", "abstract": "Active learning aims to improve the performance of task model by selecting the most informative samples with a limited budget. Unlike most recent works that focused on applying active learning for image classification, we propose an effective Consistency-based Active Learning method for object Detection (CALD), which fully explores the consistency between original and augmented data. CALD has three appealing benefits. (i) CALD is systematically designed by investigating the weaknesses of existing active learning methods, which do not take the unique challenges of object detection into account. (ii) CALD unifies box regression and classification with a single metric, which is not concerned by active learning methods for classification. CALD also focuses on the most informative local region rather than the whole image, which is beneficial for object detection. (iii) CALD not only gauges individual information for sample selection, but also leverages mutual information to encourage a balanced data distribution. Extensive experiments show that CALD significantly outperforms existing state-of-the-art task-agnostic and detection-specific active learning methods on general object detection datasets. Based on the Faster R-CNN detector, CALD consistently surpasses the baseline method (random selection) by 2.9/2.8/0.8 mAP on average on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO. Code is available at \\url{https://github.com/we1pingyu/CALD}"}}
{"id": "a20PtfcdAna", "cdate": 1601067838027, "mdate": null, "content": {"title": "MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution", "abstract": "We propose the width-resolution mutual learning method (MutualNet) to train a network that is executable at dynamic resource constraints to achieve adaptive accuracy-efficiency trade-offs at runtime. Our method trains a cohort of sub-networks with different widths using different input resolutions to mutually learn multi-scale representations for each sub-network. It achieves consistently better ImageNet top-1 accuracy over the state-of-the-art adaptive network US-Net under different computation constraints, and outperforms the best compound scaled MobileNet in EfficientNet by 1.5%. The superiority of our method is also validated on COCO object detection and instance segmentation as well as transfer learning. Surprisingly, the training strategy of MutualNet can also boost the performance of a single network, which substantially outperforms the powerful AutoAugmentation in both efficiency (GPU search hours: 15000 vs. 0) and accuracy (ImageNet: 77.6% vs. 78.6%). "}}
{"id": "u9B92zeSLGN", "cdate": 1577836800000, "mdate": null, "content": {"title": "Density Map Guided Object Detection in Aerial Images", "abstract": "Object detection in high-resolution aerial images is a challenging task because of 1) the large variation in object size, and 2) non-uniform distribution of objects. A common solution is to divide the large aerial image into small (uniform) crops and then apply object detection on each small crop. In this paper, we investigate the image cropping strategy to address these challenges. Specifically, we propose a Density-Map guided object detection Network (DMNet), which is inspired from the observation that the object density map of an image presents how objects distribute in terms of the pixel intensity of the map. As pixel intensity varies, it is able to tell whether a region has objects or not, which in turn provides guidance for cropping images statistically. DMNet has three key components: a density map generation module, an image cropping module and an object detector. DMNet generates a density map and learns scale information based on density intensities to form cropping regions. Extensive experiments show that DMNet achieves state-of-the-art performance on two popular aerial image datasets, i.e. VisionDrone [30] and UAVDT [4]."}}
