{"id": "0QfOx3wiofn", "cdate": 1684114945707, "mdate": 1684114945707, "content": {"title": "Optimization of the dynamic transition in the continuous coloring problem", "abstract": "Random constraint satisfaction problems (CSPs) can exhibit a phase where the number of constraints per variable \u03b1 makes the system solvable in theory on the one hand, but also makes the search for a solution hard, meaning that common algorithms such as Monte Carlo (MC) method fail to find a solution. The onset of this hardness is deeply linked to the appearance of a dynamical phase transition where the phase space of the problem breaks into an exponential number of clusters. The exact position of this dynamical phase transition is not universal with respect to the details of the Hamiltonian one chooses to represent a given problem. In this paper, we develop some theoretical tools in order to find a systematic way to build a Hamiltonian that maximizes the dynamic \u03b1d threshold. To illustrate our techniques, we will concentrate on the problem of continuous coloring, where one tries to set an angle xi \u2208 [0; 2\u03c0] on each node of a network in such a way that no adjacent nodes are closer than some threshold angle \u03b8, that is cos(xi \u2212 xj)\u2a7d cos\u2009\u03b8. This problem can be both seen as a continuous version of the discrete graph coloring problem or as a one-dimensional version of the Mari\u2013Krzakala\u2013Kurchan model. The relevance of this model stems from the fact that continuous CSPs on sparse random graphs remain largely unexplored in statistical physics. We show that for sufficiently small angle \u03b8 this model presents a random first order transition and compute the dynamical, condensation and Kesten\u2013Stigum transitions; we also compare the analytical predictions with MC simulations for values of \u03b8 = 2\u03c0/q, $q\\in \\mathbb{N}$. Choosing such values of q allows us to easily compare our results with the renowned problem of discrete coloring."}}
{"id": "zltdOgH5nq", "cdate": 1640995200000, "mdate": 1683880682114, "content": {"title": "Cracking nuts with a sledgehammer: when modern graph neural networks do worse than classical greedy algorithms", "abstract": "The recent work ``Combinatorial Optimization with Physics-Inspired Graph Neural Networks'' [Nat Mach Intell 4 (2022) 367] introduces a physics-inspired unsupervised Graph Neural Network (GNN) to solve combinatorial optimization problems on sparse graphs. To test the performances of these GNNs, the authors of the work show numerical results for two fundamental problems: maximum cut and maximum independent set (MIS). They conclude that \"the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables.\" In this comment, we show that a simple greedy algorithm, running in almost linear time, can find solutions for the MIS problem of much better quality than the GNN. The greedy algorithm is faster by a factor of $10^4$ with respect to the GNN for problems with a million variables. We do not see any good reason for solving the MIS with these GNN, as well as for using a sledgehammer to crack nuts. In general, many claims of superiority of neural networks in solving combinatorial problems are at risk of being not solid enough, since we lack standard benchmarks based on really hard problems. We propose one of such hard benchmarks, and we hope to see future neural network optimizers tested on these problems before any claim of superiority is made."}}
{"id": "skUxGzGPGA", "cdate": 1640995200000, "mdate": 1683880682191, "content": {"title": "A theory explaining the limits and performances of algorithms based on simulated annealing in solving sparse hard inference problems", "abstract": "The planted coloring problem is a prototypical inference problem for which thresholds for Bayes optimal algorithms, like Belief Propagation (BP), can be computed analytically. In this paper, we analyze the limits and performances of the Simulated Annealing (SA), a Monte Carlo-based algorithm that is more general and robust than BP, and thus of broader applicability. We show that SA is sub-optimal in the recovery of the planted solution because it gets attracted by glassy states that, instead, do not influence the BP algorithm. At variance with previous conjectures, we propose an analytic estimation for the SA algorithmic threshold by comparing the spinodal point of the paramagnetic phase and the dynamical critical temperature. This is a fundamental connection between thermodynamical phase transitions and out of equilibrium behavior of Glauber dynamics. We also study an improved version of SA, called replicated SA (RSA), where several weakly coupled replicas are cooled down together. We show numerical evidence that the algorithmic threshold for the RSA coincides with the Bayes optimal one. Finally, we develop an approximated analytical theory explaining the optimal performances of RSA and predicting the location of the transition towards the planted solution in the limit of a very large number of replicas. Our results for RSA support the idea that mismatching the parameters in the prior with respect to those of the generative model may produce an algorithm that is optimal and very robust."}}
{"id": "Gvxt46JZV20", "cdate": 1640995200000, "mdate": 1683880682115, "content": {"title": "The closest vector problem and the zero-temperature p-spin landscape for lossy compression", "abstract": "We consider a high-dimensional random constrained optimization problem in which a set of binary variables is subjected to a linear system of equations. The cost function is a simple linear cost, measuring the Hamming distance with respect to a reference configuration. Despite its apparent simplicity, this problem exhibits a rich phenomenology. We show that different situations arise depending on the random ensemble of linear systems. When each variable is involved in at most two linear constraints, we show that the problem can be partially solved analytically, in particular we show that upon convergence, the zero-temperature limit of the cavity equations returns the optimal solution. We then study the geometrical properties of more general random ensembles. In particular we observe a range in the density of constraints at which the systems enters a glassy phase where the cost function has many minima. Interestingly, the algorithmic performances are only sensitive to another phase transition affecting the structure of configurations allowed by the linear constraints. We also extend our results to variables belonging to $\\text{GF}(q)$, the Galois Field of order $q$. We show that increasing the value of $q$ allows to achieve a better optimum, which is confirmed by the Replica Symmetric cavity method predictions."}}
{"id": "qLDGRwQcdd", "cdate": 1609459200000, "mdate": 1683880682088, "content": {"title": "SpaRTA Tracking Across Occlusions via Partitioning of 3D Clouds of Points", "abstract": "Any 3D tracking algorithm has to deal with occlusions: multiple targets get so close to each other that the loss of their identities becomes likely; hence, potentially affecting the very quality of the data with interrupted trajectories and identity switches. Here, we present a novel tracking method that addresses the problem of occlusions within large groups of featureless objects by means of three steps: i) it represents each target as a cloud of points in 3D; ii) once a 3D cluster corresponding to an occlusion occurs, it defines a partitioning problem by introducing a cost function that uses both attractive and repulsive spatio-temporal proximity links; and iii) it minimizes the cost function through a semi-definite optimization technique specifically designed to cope with the presence of multiminima landscapes. The algorithm is designed to work on 3D data regardless of the experimental method used: multicamera systems, lidars, radars, and RGB-D systems. By performing tests on public data-sets, we show that the new algorithm produces a significant improvement over the state-of-the-art tracking methods, both by reducing the number of identity switches and by increasing the accuracy of the estimated positions of the targets in real space."}}
{"id": "bYe3skLK_bw", "cdate": 1609459200000, "mdate": 1683880682121, "content": {"title": "Nonequilibrium Monte Carlo for unfreezing variables in hard combinatorial optimization", "abstract": "Optimizing highly complex cost/energy functions over discrete variables is at the heart of many open problems across different scientific disciplines and industries. A major obstacle is the emergence of many-body effects among certain subsets of variables in hard instances leading to critical slowing down or collective freezing for known stochastic local search strategies. An exponential computational effort is generally required to unfreeze such variables and explore other unseen regions of the configuration space. Here, we introduce a quantum-inspired family of nonlocal Nonequilibrium Monte Carlo (NMC) algorithms by developing an adaptive gradient-free strategy that can efficiently learn key instance-wise geometrical features of the cost function. That information is employed on-the-fly to construct spatially inhomogeneous thermal fluctuations for collectively unfreezing variables at various length scales, circumventing costly exploration versus exploitation trade-offs. We apply our algorithm to two of the most challenging combinatorial optimization problems: random k-satisfiability (k-SAT) near the computational phase transitions and Quadratic Assignment Problems (QAP). We observe significant speedup and robustness over both specialized deterministic solvers and generic stochastic solvers. In particular, for 90% of random 4-SAT instances we find solutions that are inaccessible for the best specialized deterministic algorithm known as Survey Propagation (SP) with an order of magnitude improvement in the quality of solutions for the hardest 10% instances. We also demonstrate two orders of magnitude improvement in time-to-solution over the state-of-the-art generic stochastic solver known as Adaptive Parallel Tempering (APT)."}}
{"id": "Jm5pbZoOTx", "cdate": 1609459200000, "mdate": 1683880682108, "content": {"title": "How we are leading a 3-XORSAT challenge: from the energy landscape to the algorithm and its efficient implementation on GPUs", "abstract": "A recent 3-XORSAT challenge required to minimize a very complex and rough energy function, typical of glassy models with a random first order transition and a golf course like energy landscape. We present the ideas beyond the quasi-greedy algorithm and its very efficient implementation on GPUs that are allowing us to rank first in such a competition. We suggest a better protocol to compare algorithmic performances and we also provide analytical predictions about the exponential growth of the times to find the solution in terms of free-energy barriers."}}
{"id": "h7xSLY-e2s6", "cdate": 1577836800000, "mdate": 1683880682041, "content": {"title": "Spin Glasses in a Field Show a Phase Transition Varying the Distance among Real Replicas (And How to Exploit It to Find the Critical Line in a Field)", "abstract": "We discuss a phase transition in spin glass models that have been rarely considered in the past, namely, the phase transition that may take place when two real replicas are forced to be at a larger distance (i.e., at a smaller overlap) than the typical one. In the first part of the work, by solving analytically the Sherrington-Kirkpatrick model in a field close to its critical point, we show that, even in a paramagnetic phase, the forcing of two real replicas to an overlap small enough leads the model to a phase transition where the symmetry between replicas is spontaneously broken. More importantly, this phase transition is related to the de Almeida-Thouless (dAT) critical line. In the second part of the work, we exploit the phase transition in the overlap between two real replicas to identify the critical line in a field in finite dimensional spin glasses. This is a notoriously difficult computational problem, because of considerable finite size corrections. We introduce a new method of analysis of Monte Carlo data for disordered systems, where the overlap between two real replicas is used as a conditioning variate. We apply this analysis to equilibrium measurements collected in the paramagnetic phase in a field, h > 0 and T c ( h ) < T < T c ( h = 0 ) , of the d = 1 spin glass model with long range interactions decaying fast enough to be outside the regime of validity of the mean field theory. We thus provide very reliable estimates for the thermodynamic critical temperature in a field."}}
{"id": "t_M8MYwKf7", "cdate": 1546300800000, "mdate": 1683880682044, "content": {"title": "How to iron out rough landscapes and get optimal performances: Replicated Gradient Descent and its application to tensor PCA", "abstract": "In many high-dimensional estimation problems the main task consists in minimizing a cost function, which is often strongly non-convex when scanned in the space of parameters to be estimated. A standard solution to flatten the corresponding rough landscape consists in summing the losses associated to different data points and obtain a smoother empirical risk. Here we propose a complementary method that works for a single data point. The main idea is that a large amount of the roughness is uncorrelated in different parts of the landscape. One can then substantially reduce the noise by evaluating an empirical average of the gradient obtained as a sum over many random independent positions in the space of parameters to be optimized. We present an algorithm, called Averaged Gradient Descent, based on this idea and we apply it to tensor PCA, which is a very hard estimation problem. We show that Averaged Gradient Descent over-performs physical algorithms such as gradient descent and approximate message passing and matches the best algorithmic thresholds known so far, obtained by tensor unfolding and methods based on sum-of-squares."}}
{"id": "L7J863GnU7", "cdate": 1546300800000, "mdate": 1683880682089, "content": {"title": "Monte Carlo algorithms are very effective in finding the largest independent set in sparse random graphs", "abstract": "The effectiveness of stochastic algorithms based on Monte Carlo dynamics in solving hard optimization problems is mostly unknown. Beyond the basic statement that at a dynamical phase transition the ergodicity breaks and a Monte Carlo dynamics cannot sample correctly the probability distribution in times linear in the system size, there are almost no predictions nor intuitions on the behavior of this class of stochastic dynamics. The situation is particularly intricate because, when using a Monte Carlo based algorithm as an optimization algorithm, one is usually interested in the out of equilibrium behavior which is very hard to analyse. Here we focus on the use of Parallel Tempering in the search for the largest independent set in a sparse random graph, showing that it can find solutions well beyond the dynamical threshold. Comparison with state-of-the-art message passing algorithms reveals that parallel tempering is definitely the algorithm performing best, although a theory explaining its behavior is still lacking."}}
