{"id": "HkxPtJh4YB", "cdate": 1571237759407, "mdate": null, "content": {"title": "Sinkhorn Permutation  Variational Marginal Inference", "abstract": "We address the problem of marginal inference for an exponential family defined over the set of permutation matrices. This problem is known to quickly become intractable as the size of the permutation increases, since its involves the computation of the permanent of a matrix, a #P-hard problem. We introduce Sinkhorn variational marginal inference as a scalable alternative, a method whose validity is ultimately justified by the so-called Sinkhorn approximation of the permanent. We demonstrate the efectiveness of our method in the problem of probabilistic identification of neurons in the worm C.elegans"}}
{"id": "SygJKVBgLH", "cdate": 1567802486676, "mdate": null, "content": {"title": "Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem", "abstract": "We prove several fundamental statistical bounds for entropic OT with the squared Euclidean cost between subgaussian probability measures in arbitrary dimension. First, through a new sample complexity result we establish the rate of convergence of entropic OT for empirical measures. Our analysis improves exponentially on the bound of Genevay et al. (2018) and extends their work to unbounded measures. Second, we establish a central limit theorem for entropic OT, based on techniques developed by Del Barrio et al. (2019). Previously, such a result was only known for finite metric spaces (Bigot et al., 2017; Klatt et al., 2018). As an application of our results, we develop and analyze a new technique for estimating the entropy of a random variable corrupted by gaussian noise."}}
{"id": "Byt3oJ-0W", "cdate": 1518730174776, "mdate": null, "content": {"title": "Learning Latent Permutations with Gumbel-Sinkhorn Networks", "abstract": "Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data. Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable. In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.  Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator. With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings. We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms. "}}
