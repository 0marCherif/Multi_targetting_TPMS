{"id": "m5VON5h-XM", "cdate": 1672531200000, "mdate": 1694954754142, "content": {"title": "From Tempered to Benign Overfitting in ReLU Neural Networks", "abstract": "Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. This phenomenon motivated a large body of work on \"benign overfitting\", where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and empirically observed that the behavior of NNs is often better described as \"tempered overfitting\", where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise level. However, a theoretical justification of this claim for non-linear NNs has been lacking so far. In this work, we provide several results that aim at bridging these complementing views. We study a simple classification setting with 2-layer ReLU NNs, and prove that under various assumptions, the type of overfitting transitions from tempered in the extreme case of one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a crucial role on the type of overfitting in this setting, which we also validate empirically for intermediate dimensions. Overall, our results shed light on the intricate connections between the dimension, sample size, architecture and training algorithm on the one hand, and the type of resulting overfitting on the other hand."}}
{"id": "bKwzc1H8v-z", "cdate": 1672531200000, "mdate": 1694954754224, "content": {"title": "An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization", "abstract": "We study the complexity of producing $(\\delta,\\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\\delta^{-1}\\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\\delta,\\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned convergence rate in expectation as well as with high probability. Our analysis is based on a simple yet powerful geometric lemma regarding the Goldstein-subdifferential set, which allows utilizing recent advancements in first-order nonsmooth nonconvex optimization."}}
{"id": "XJUJD_-hp3X", "cdate": 1672531200000, "mdate": 1681589808626, "content": {"title": "Near-optimal learning with average H\u00f6lder smoothness", "abstract": ""}}
{"id": "Vr6ja9JT_q", "cdate": 1672531200000, "mdate": 1683928108125, "content": {"title": "Deterministic Nonsmooth Nonconvex Optimization", "abstract": "We study the complexity of optimizing nonsmooth nonconvex Lipschitz functions by producing $(\\delta,\\epsilon)$-stationary points. Several recent works have presented randomized algorithms that produce such points using $\\tilde O(\\delta^{-1}\\epsilon^{-3})$ first-order oracle calls, independent of the dimension $d$. It has been an open problem as to whether a similar result can be obtained via a deterministic algorithm. We resolve this open problem, showing that randomization is necessary to obtain a dimension-free rate. In particular, we prove a lower bound of $\\Omega(d)$ for any deterministic algorithm. Moreover, we show that unlike smooth or convex optimization, access to function values is required for any deterministic algorithm to halt within any finite time. On the other hand, we prove that if the function is even slightly smooth, then the dimension-free rate of $\\tilde O(\\delta^{-1}\\epsilon^{-3})$ can be obtained by a deterministic algorithm with merely a logarithmic dependence on the smoothness parameter. Motivated by these findings, we turn to study the complexity of deterministically smoothing Lipschitz functions. Though there are efficient black-box randomized smoothings, we start by showing that no such deterministic procedure can smooth functions in a meaningful manner, resolving an open question. We then bypass this impossibility result for the structured case of ReLU neural networks. To that end, in a practical white-box setting in which the optimizer is granted access to the network's architecture, we propose a simple, dimension-free, deterministic smoothing that provably preserves $(\\delta,\\epsilon)$-stationary points. Our method applies to a variety of architectures of arbitrary depth, including ResNets and ConvNets. Combined with our algorithm, this yields the first deterministic dimension-free algorithm for optimizing ReLU networks, circumventing our lower bound."}}
{"id": "UNTDYNqoht", "cdate": 1672531200000, "mdate": 1694954754129, "content": {"title": "Deterministic Nonsmooth Nonconvex Optimization", "abstract": "We study the complexity of optimizing nonsmooth nonconvex Lipschitz functions by producing $(\\delta,\\epsilon)$-Goldstein stationary points. Several recent works have presented randomized algorithms..."}}
{"id": "SaRQ4oTqWbP", "cdate": 1664731443565, "mdate": null, "content": {"title": "On the Complexity of Finding Small Subgradients in Nonsmooth Optimization", "abstract": "We study the oracle complexity of producing $(\\delta,\\epsilon)$-stationary points of Lipschitz functions, in the sense proposed by Zhang et al. [2020]. While there exist dimension-free randomized algorithms for producing such points within $\\widetilde{O}(1/\\delta\\epsilon^3)$ first-order oracle calls, we show that no dimension-free rate can be achieved by a deterministic algorithm. On the other hand, we point out that this rate can be derandomized for smooth functions with merely a logarithmic dependence on the smoothness parameter. Moreover, we establish several lower bounds for this task which hold for any randomized algorithm, with or without convexity. Finally, we show how the convergence rate of finding $(\\delta,\\epsilon)$-stationary points can be improved in case the function is convex, a setting which we motivate by proving that in general no finite time algorithm can produce points with small subgradients even for convex functions."}}
{"id": "ri2_Y8vtRk", "cdate": 1640995200000, "mdate": 1694954754140, "content": {"title": "On the Complexity of Finding Small Subgradients in Nonsmooth Optimization", "abstract": "We study the oracle complexity of producing $(\\delta,\\epsilon)$-stationary points of Lipschitz functions, in the sense proposed by Zhang et al. [2020]. While there exist dimension-free randomized algorithms for producing such points within $\\widetilde{O}(1/\\delta\\epsilon^3)$ first-order oracle calls, we show that no dimension-free rate can be achieved by a deterministic algorithm. On the other hand, we point out that this rate can be derandomized for smooth functions with merely a logarithmic dependence on the smoothness parameter. Moreover, we establish several lower bounds for this task which hold for any randomized algorithm, with or without convexity. Finally, we show how the convergence rate of finding $(\\delta,\\epsilon)$-stationary points can be improved in case the function is convex, a setting which we motivate by proving that in general no finite time algorithm can produce points with small subgradients even for convex functions."}}
{"id": "Vd4qAGetBUY", "cdate": 1640995200000, "mdate": 1694954754221, "content": {"title": "Oracle Complexity in Nonsmooth Nonconvex Optimization", "abstract": "It is well-known that given a smooth, bounded-from-below, and possibly nonconvex function, standard gradient-based methods can find $\\epsilon$-stationary points (with gradient norm less than $\\epsilon$) in $\\mathcal{O}(1/\\epsilon^2)$ iterations. However, many important nonconvex optimization problems, such as those associated with training modern neural networks, are inherently not smooth, making these results inapplicable. In this paper, we study nonsmooth nonconvex optimization from an oracle complexity viewpoint, where the algorithm is assumed to be given access only to local information about the function at various points. We provide two main results: First, we consider the problem of getting near $\\epsilon$-stationary points. This is perhaps the most natural relaxation of finding $\\epsilon$-stationary points, which is impossible in the nonsmooth nonconvex case. We prove that this relaxed goal cannot be achieved efficiently, for any distance and $\\epsilon$ smaller than some constants. Our second result deals with the possibility of tackling nonsmooth nonconvex optimization by reduction to smooth optimization: Namely, applying smooth optimization methods on a smooth approximation of the objective function. For this approach, we prove under a mild assumption an inherent trade-off between oracle complexity and smoothness: On the one hand, smoothing a nonsmooth nonconvex function can be done very efficiently (e.g., by randomized smoothing), but with dimension-dependent factors in the smoothness parameter, which can strongly affect iteration complexity when plugging into standard smooth optimization methods. On the other hand, these dimension factors can be eliminated with suitable smoothing methods, but only by making the oracle complexity of the smoothing process exponentially large."}}
{"id": "aMZJBOiOOPg", "cdate": 1621629818360, "mdate": null, "content": {"title": "Oracle Complexity in Nonsmooth Nonconvex Optimization", "abstract": "It is well-known that given a smooth, bounded-from-below, and possibly nonconvex function, standard gradient-based methods can find $\\epsilon$-stationary points (with gradient norm less than $\\epsilon$) in $\\mathcal{O}(1/\\epsilon^2)$ iterations. However, many important nonconvex optimization problems, such as those associated with training modern neural networks, are inherently not smooth, making these results inapplicable. In this paper, we study nonsmooth nonconvex optimization from an oracle complexity viewpoint, where the algorithm is assumed to be given access only to local information about the function at various points. We provide two main results (under mild assumptions): First, we consider the problem of getting \\emph{near} $\\epsilon$-stationary points. This is perhaps the most natural relaxation of \\emph{finding} $\\epsilon$-stationary points, which is impossible in the nonsmooth nonconvex case. We prove that this relaxed goal cannot be achieved efficiently, for any distance and $\\epsilon$ smaller than some constants. Our second result deals with the possibility of tackling nonsmooth nonconvex optimization by reduction to smooth optimization: Namely, applying smooth optimization methods on a smooth approximation of the objective function. For this approach, we prove an inherent trade-off between oracle complexity and smoothness: On the one hand, smoothing a nonsmooth nonconvex function can be done very efficiently (e.g., by randomized smoothing), but with dimension-dependent factors in the smoothness parameter, which can strongly affect iteration complexity when plugging into standard smooth optimization methods. On the other hand, these dimension factors can be  eliminated with suitable smoothing methods, but only by making the oracle complexity of the smoothing process exponentially large."}}
{"id": "aBRv_VMnQf", "cdate": 1609459200000, "mdate": 1694954754142, "content": {"title": "Oracle Complexity in Nonsmooth Nonconvex Optimization", "abstract": "It is well-known that given a smooth, bounded-from-below, and possibly nonconvex function, standard gradient-based methods can find $\\epsilon$-stationary points (with gradient norm less than $\\epsilon$) in $\\mathcal{O}(1/\\epsilon^2)$ iterations. However, many important nonconvex optimization problems, such as those associated with training modern neural networks, are inherently not smooth, making these results inapplicable. In this paper, we study nonsmooth nonconvex optimization from an oracle complexity viewpoint, where the algorithm is assumed to be given access only to local information about the function at various points. We provide two main results (under mild assumptions): First, we consider the problem of getting \\emph{near} $\\epsilon$-stationary points. This is perhaps the most natural relaxation of \\emph{finding} $\\epsilon$-stationary points, which is impossible in the nonsmooth nonconvex case. We prove that this relaxed goal cannot be achieved efficiently, for any distance and $\\epsilon$ smaller than some constants. Our second result deals with the possibility of tackling nonsmooth nonconvex optimization by reduction to smooth optimization: Namely, applying smooth optimization methods on a smooth approximation of the objective function. For this approach, we prove an inherent trade-off between oracle complexity and smoothness: On the one hand, smoothing a nonsmooth nonconvex function can be done very efficiently (e.g., by randomized smoothing), but with dimension-dependent factors in the smoothness parameter, which can strongly affect iteration complexity when plugging into standard smooth optimization methods. On the other hand, these dimension factors can be eliminated with suitable smoothing methods, but only by making the oracle complexity of the smoothing process exponentially large."}}
