{"id": "z-KvBpvAOHw", "cdate": 1640995200000, "mdate": 1666621953120, "content": {"title": "Visual Localization via Few-Shot Scene Region Classification", "abstract": "Visual (re)localization addresses the problem of estimating the 6-DoF (Degree of Freedom) camera pose of a query image captured in a known scene, which is a key building block of many computer vision and robotics applications. Recent advances in structure-based localization solve this problem by memorizing the mapping from image pixels to scene coordinates with neural networks to build 2D-3D correspondences for camera pose optimization. However, such memorization requires training by amounts of posed images in each scene, which is heavy and inefficient. On the contrary, few-shot images are usually sufficient to cover the main regions of a scene for a human operator to perform visual localization. In this paper, we propose a scene region classification approach to achieve fast and effective scene memorization with few-shot images. Our insight is leveraging a) pre-learned feature extractor, b) scene region classifier, and c) meta-learning strategy to accelerate training while mitigating overfitting. We evaluate our method on both indoor and outdoor benchmarks. The experiments validate the effectiveness of our method in the few-shot setting, and the training time is significantly reduced to only a few minutes. Code available at: \\url{https://github.com/siyandong/SRC}"}}
{"id": "dN8K3s8YCbT", "cdate": 1640995200000, "mdate": 1666621952541, "content": {"title": "MDISN: Learning multiscale deformed implicit fields from single images", "abstract": ""}}
{"id": "b6dsTss_hW", "cdate": 1640995200000, "mdate": 1666621952543, "content": {"title": "Filtering In Implicit Neural Networks", "abstract": "Neural implicit functions are highly effective for data representation. However, the implicit functions learned by neural networks usually include unexpected noisy artifacts or lose fine details if the input data has many scales of detail or contains both low-frequency and high-frequency bandwidths. Removing artifacts while preserving fine-scale contents is challenging and usually comes out with over-smoothing or noisy issues. To solve this dilemma, we propose a new framework (FINN) that integrates a filtering module into the MLPs to perform data reconstruction while adapting regions containing different frequencies. The filtering module has a smoothing operator acting on intermediate results of the network that encourages the results to be smooth and a recovering operator bringing high frequencies to regions overly smooth. The two counteractive operators play consecutively in all MLP layers to adaptively influence the reconstruction. We demonstrate the advantage of FINN on several tasks and showcase significant improvement compared to state-of-the-art methods. In addition, FINN also yields better performance in both convergence speed and network stability."}}
{"id": "TCG2g9YPUFZ", "cdate": 1640995200000, "mdate": 1666621952542, "content": {"title": "Progressive Multimodal Shape Generation via Contextual Part Reasoning", "abstract": "We present a contextual generative network for 3D shapes based on a conditional variational autoencoder, which learns a subspace of plausible complementary parts in the context of a partial shape. With the learned part subspace prior, which encodes bi-part spatial relations and geometry descriptions, a shape is generated via iterative \u201cnext part reasoning\u201d, where a next part is sampled conditioned on a partial shape. Furthermore, our conditional subspace allows not just one, but a set of reasonable next parts to be generated, which adds controllability (e.g., via user selection) to the generative process. Our core idea of reasoning about next parts via conditional modeling offers a new way of understanding shape structures via part correlation modeling. Evaluations show the effectiveness of our approach and also the diversity of the generated shapes."}}
{"id": "3G1PdZz7Rc", "cdate": 1640995200000, "mdate": 1682324411568, "content": {"title": "Visual Localization via Few-Shot Scene Region Classification", "abstract": "Visual (re)localization addresses the problem of estimating the 6-DoF (Degree of Freedom) camera pose of a query image captured in a known scene, which is a key building block of many computer vision and robotics applications. Recent advances in structure-based localization solve this problem by memorizing the mapping from image pixels to scene coordinates with neural networks to build 2D-3D correspondences for camera pose optimization. However, such memorization requires training by amounts of posed images in each scene, which is heavy and inefficient. On the contrary, few-shot images are usually sufficient to cover the main regions of a scene for a human operator to perform visual localization. In this paper, we propose a scene region classification approach to achieve fast and effective scene memorization with few-shot images. Our insight is leveraging a) pre-learned feature extractor, b) scene region classifier, and c) meta-learning strategy to accelerate training while mitigating overfitting. We evaluate our method on both indoor and outdoor benchmarks. The experiments validate the effectiveness of our method in the few-shot setting, and the training time is significantly reduced to only a few minutes. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Code available at: https://github.com/siyandong/SRC"}}
{"id": "5ODDbWCQTw", "cdate": 1609459200000, "mdate": 1666621952544, "content": {"title": "Neural Implicit 3D Shapes from Single Images with Spatial Patterns", "abstract": "Neural implicit functions have achieved impressive results for reconstructing 3D shapes from single images. However, the image features for describing 3D point samplings of implicit functions are less effective when significant variations of occlusions, views, and appearances exist from the image. To better encode image features, we study a geometry-aware convolutional kernel to leverage geometric relationships of point samplings by the proposed \\emph{spatial pattern}, i.e., a structured point set. Specifically, the kernel operates at 2D projections of 3D points from the spatial pattern. Supported by the spatial pattern, the 2D kernel encodes geometric information that is crucial for 3D reconstruction tasks, while traditional ones mainly consider appearance information. Furthermore, to enable the network to discover more adaptive spatial patterns for further capturing non-local contextual information, the kernel is devised to be deformable manipulated by a spatial pattern generator. Experimental results on both synthetic and real datasets demonstrate the superiority of the proposed method. Pre-trained models, codes, and data are available at https://github.com/yixin26/SVR-SP."}}
{"id": "x0NjJ17xdfu", "cdate": 1577836800000, "mdate": 1666621952543, "content": {"title": "PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes", "abstract": "We introduce PQ-NET, a deep neural network which represents and generates 3D shapes via sequential part assembly. The input to our network is a 3D shape segmented into parts, where each part is first encoded into a feature representation using a part autoencoder. The core component of PQ-NET is a sequence-to-sequence or Seq2Seq autoencoder which encodes a sequence of part features into a latent vector of fixed size, and the decoder reconstructs the 3D shape, one part at a time, resulting in a sequential assembly. The latent space formed by the Seq2Seq encoder encodes both part structure and fine part geometry. The decoder can be adapted to perform several generative tasks including shape autoencoding, interpolation, novel shape generation, and single-view 3D reconstruction, where the generated shapes are all composed of meaningful parts."}}
{"id": "HKCC5jFM2ZM", "cdate": 1577836800000, "mdate": 1666621952547, "content": {"title": "Multimodal Shape Completion via Conditional Generative Adversarial Networks", "abstract": "Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality."}}
{"id": "2m9GF1fIh8C", "cdate": 1577836800000, "mdate": 1666621952854, "content": {"title": "Multimodal Shape Completion via Conditional Generative Adversarial Networks", "abstract": "Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality."}}
{"id": "54C1LgX1PD6", "cdate": 1546300800000, "mdate": 1666621953117, "content": {"title": "PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes", "abstract": "We introduce PQ-NET, a deep neural network which represents and generates 3D shapes via sequential part assembly. The input to our network is a 3D shape segmented into parts, where each part is first encoded into a feature representation using a part autoencoder. The core component of PQ-NET is a sequence-to-sequence or Seq2Seq autoencoder which encodes a sequence of part features into a latent vector of fixed size, and the decoder reconstructs the 3D shape, one part at a time, resulting in a sequential assembly. The latent space formed by the Seq2Seq encoder encodes both part structure and fine part geometry. The decoder can be adapted to perform several generative tasks including shape autoencoding, interpolation, novel shape generation, and single-view 3D reconstruction, where the generated shapes are all composed of meaningful parts."}}
