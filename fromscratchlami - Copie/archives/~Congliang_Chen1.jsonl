{"id": "xQXHkKI_7TG", "cdate": 1663849976052, "mdate": null, "content": {"title": "Local Coefficient Optimization in Federated Learning", "abstract": "Federated learning emerges as a promising approach to build a large-scale cooperative learning system among multiple clients without sharing their raw data. However, given a specific global objective, finding the optimal sampling weights for each client remains largely unexplored. This is particularly challenging when clients' data distributions are non-i.i.d. and clients partially participate.\n\nIn this paper, we model the above task as a bi-level optimization problem which takes the correlations among different clients into account. We present a double-loop primal-dual-based algorithm to solve the bi-level optimization problem. We further provide rigorous convergence analysis for our algorithm under mild assumptions. Finally, we perform extensive empirical studies under both toy examples and learning models from real datasets to verify the effectiveness of the proposed method. "}}
{"id": "l5UNyaHqFdO", "cdate": 1652737636403, "mdate": null, "content": {"title": "Adam Can Converge Without Any Modification On Update Rules", "abstract": "Ever since \\citet{reddi2019convergence} pointed out the divergence issue of Adam, many new variants have been designed to obtain convergence. However, vanilla Adam remains exceptionally popular and it works well in practice. Why is there a gap between theory and practice? We point out there is a mismatch between the settings of theory and practice: \\citet{reddi2019convergence} pick the problem after picking the hyperparameters of Adam, i.e., $(\\beta_1,\\beta_2)$; while practical applications often fix the problem first and then tune $(\\beta_1,\\beta_2)$.   Due to this observation, we conjecture that the empirical convergence can be theoretically justified, only if we change the order of picking the problem and hyperparameter.  In this work, we confirm this conjecture.  We prove that, when the 2nd-order momentum parameter $\\beta_2$ is large and 1st-order momentum parameter $\\beta_1 < \\sqrt{\\beta_2}<1$, Adam converges to the neighborhood of critical points. The size of the neighborhood is propositional to the variance of stochastic gradients. Under an extra condition (strong growth condition), Adam converges to critical points. It is worth mentioning that our results cover a wide range of hyperparameters: as $\\beta_2$  increases, our convergence result can cover any $\\beta_1 \\in [0,1)$ including $\\beta_1=0.9$, which is the default setting in deep learning libraries. To our knowledge, this is the first result showing that Adam can converge {\\it without any modification} on its update rules. Further, our analysis does not require assumptions of bounded gradients or bounded 2nd-order momentum. When $\\beta_2$ is small, we further point out a large region of  $(\\beta_1,\\beta_2)$ combinations where  Adam can diverge to infinity. Our divergence result considers the same setting (fixing the optimization problem ahead) as our convergence result, indicating that there is a phase transition from divergence to convergence when increasing $\\beta_2$. These positive and negative results provide suggestions on how to tune Adam hyperparameters: for instance,  when Adam does not work well, we suggest tuning up $\\beta_2$ and trying $\\beta_1< \\sqrt{\\beta_2}$."}}
{"id": "uf7FZWXJxYQ", "cdate": 1601308183566, "mdate": null, "content": {"title": "Communication Efficient Primal Dual Algorithm for Nonconvex Nonsmooth Distributed Optimization", "abstract": "Decentralized optimization problems frequently appear in the large scale machine learning problem. However, few works work on the nonconvex nonsmooth case. In this paper, we give a primal-dual algorithm to solve the nonconvex nonsmooth optimization problem. In addition, to reduce communication overhead, we introduce compression function. We analyze the convergence results of the algorithm and shows the algorithm meets the lower iteration complexity bound. Besides, we conduct two experiments, both of them shows the efficacy of our algorithm. "}}
{"id": "Y8hOBLJrJLU", "cdate": 1601308172672, "mdate": null, "content": {"title": "Efficient-Adam: Communication-Efficient Distributed Adam with Complexity Analysis", "abstract": "Distributed adaptive stochastic gradient methods have been widely used for large scale nonconvex optimization, such as training deep learning models. However, their iteration complexity on finding $\\varepsilon$-stationary points has rarely been analyzed in the nonconvex setting. In this work, we present a novel communication-efficient distributed Adam in the parameter-server model for stochastic nonconvex optimization, dubbed {\\em Efficient-Adam}. Specifically, we incorporate a two-way quantization scheme into Efficient-Adam to reduce the communication cost between the workers and the server. Simultaneously, we adopt a two-way error feedback strategy to reduce the biases caused by the two-way quantization on both the server and workers, respectively. In addition, we establish the iteration complexity for the proposed Efficient-Adam with a class of quantization operators and further characterize its communication complexity between the server and workers when an $\\varepsilon$-stationary point is achieved. \nFinally, we solve a toy stochastic convex optimization problem and train deep learning models on real-world vision and language tasks. Extensive experimental results together with a theoretical guarantee justify the merits of Efficient Adam.\n"}}
{"id": "SJWzwyf_-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Arbitrary Style Transfer With Deep Feature Reshuffle", "abstract": "This paper introduces a novel method by reshuffling deep features (i.e., permuting the spacial locations of a feature map) of the style image for arbitrary style transfer. We theoretically prove that our new style loss based on reshuffle connects both global and local style losses respectively used by most parametric and non-parametric neural style transfer methods. This simple idea can effectively address the challenging issues in existing style transfer methods. On one hand, it can avoid distortions in local style patterns, and allow semantic-level transfer, compared with neural parametric methods. On the other hand, it can preserve globally similar appearance to the style image, and avoid wash-out artifacts, compared with neural non-parametric methods. Based on the proposed loss, we also present a progressive feature-domain optimization approach. The experiments show that our method is widely applicable to various styles, and produces better quality than existing methods."}}
