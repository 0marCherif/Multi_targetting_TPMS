{"id": "o56X170U_H9", "cdate": 1676827104394, "mdate": null, "content": {"title": "Adaptivity Complexity for Causal Graph Discovery", "abstract": "Causal discovery from interventional data is an important problem, where the task is to design an interventional strategy that learns the hidden ground truth causal graph $G(V,E)$ on $|V| = n$ nodes while minimizing the number of performed interventions. Most prior interventional strategies broadly fall into two categories: non-adaptive and adaptive. Non-adaptive strategies decide on a single fixed set of interventions to be performed while adaptive strategies can decide on which nodes to intervene on sequentially based on past interventions. While adaptive algorithms may use exponentially fewer interventions than their non-adaptive counterparts, there are practical concerns that constrain the amount of adaptivity allowed. Motivated by this trade-off, we study the problem of $r$-adaptivity, where the algorithm designer recovers the causal graph under a total of $r$ sequential rounds whilst trying to minimize the total number of interventions. For this problem, we provide a $r$-adaptive algorithm that achieves $O(\\min\\{r,\\log n\\} \\cdot n^{1/\\min\\{r,\\log n\\}})$ approximation with respect to the verification number, a well-known lower bound for adaptive algorithms. Furthermore, for every $r$, we show that our approximation is tight. Our definition of $r$-adaptivity interpolates nicely between the non-adaptive ($r=1$) and fully adaptive ($r=n$) settings where our approximation simplifies to $O(n)$ and $O(\\log n)$ respectively, matching the best-known approximation guarantees for both extremes. Our results also extend naturally to the bounded size interventions."}}
{"id": "5wVS9_InKi", "cdate": 1672531200000, "mdate": 1681875917362, "content": {"title": "Subset verification and search algorithms for causal DAGs", "abstract": "Learning causal relationships between variables is a fundamental task in causal inference and directed acyclic graphs (DAGs) are a popular choice to represent the causal relationships. As one can recover a causal graph only up to its Markov equivalence class from observations, interventions are often used for the recovery task. Interventions are costly in general and it is important to design algorithms that minimize the number of interventions performed. In this work, we study the problem of identifying the smallest set of interventions required to learn the causal relationships between a subset of edges (target edges). Under the assumptions of faithfulness, causal sufficiency, and ideal interventions, we study this problem in two settings: when the underlying ground truth causal graph is known (subset verification) and when it is unknown (subset search). For the subset verification problem, we provide an efficient algorithm to compute a minimum sized interventional set; we further extend these results to bounded size non-atomic interventions and node-dependent interventional costs. For the subset search problem, in the worst case, we show that no algorithm (even with adaptivity or randomization) can achieve an approximation ratio that is asymptotically better than the vertex cover of the target edges when compared with the subset verification number. This result is surprising as there exists a logarithmic approximation algorithm for the search problem when we wish to recover the whole causal graph. To obtain our results, we prove several interesting structural properties of interventional causal graphs that we believe have applications beyond the subset verification/search problems studied here."}}
{"id": "4F7vp67j79I", "cdate": 1652737445385, "mdate": null, "content": {"title": "Verification and search algorithms for causal DAGs", "abstract": "We study two problems related to recovering causal graphs from interventional data: (i) $\\textit{verification}$, where the task is to check if a purported causal graph is correct, and (ii) $\\textit{search}$, where the task is to recover the correct causal graph. For both, we wish to minimize the number of interventions performed. For the first problem, we give a characterization of a minimal sized set of atomic interventions that is necessary and sufficient to check the correctness of a claimed causal graph. Our characterization uses the notion of $\\textit{covered edges}$, which enables us to obtain simple proofs and also easily reason about earlier known results. We also generalize our results to the settings of bounded size interventions and node-dependent interventional costs. For all the above settings, we provide the first known provable algorithms for efficiently computing (near)-optimal verifying sets on general graphs. For the second problem, we give a simple adaptive algorithm based on graph separators that produces an atomic intervention set which fully orients any essential graph while using $\\mathcal{O}(\\log n)$ times the optimal number of interventions needed to $\\textit{verify}$ (verifying size) the underlying DAG on $n$ vertices. This approximation is tight as $\\textit{any}$ search algorithm on an essential line graph has worst case approximation ratio of $\\Omega(\\log n)$ with respect to the verifying size. With bounded size interventions, each of size $\\leq k$, our algorithm gives an $\\mathcal{O}(\\log n \\cdot \\log k)$ factor approximation. Our result is the first known algorithm that gives a non-trivial approximation guarantee to the verifying size on general unweighted graphs and with bounded size interventions."}}
{"id": "w6LsItagj3", "cdate": 1640995200000, "mdate": 1681733005873, "content": {"title": "Learning Sparse Fixed-Structure Gaussian Bayesian Networks", "abstract": "Gaussian Bayesian networks are widely used to model causal interactions among continuous variables. In this work, we study the problem of learning a fixed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze the commonly used node-wise least squares regression LeastSquares and prove that it has the near-optimal sample complexity. We also study a couple of new algorithms for the problem: BatchAvgLeastSquares takes the average of several batches of least squares solutions at each node, so that one can interpolate between the batch size and the number of batches. We show that BatchAvgLeastSquares also has near-optimal sample complexity. CauchyEst takes the median of solutions to several batches of linear systems at each node. We show that the algorithm specialized to polytrees, CauchyEstTree, has near-optimal sample complexity. Experimentally, we show that for uncontaminated, realizable data, the LeastSquares algorithm performs best, but in the presence of contamination or DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares respectively perform better."}}
{"id": "EIoX8_uQYo", "cdate": 1640995200000, "mdate": 1681875917364, "content": {"title": "Learning and Testing Latent-Tree Ising Models Efficiently", "abstract": "We provide time- and sample-efficient algorithms for learning and testing latent-tree Ising models, i.e. Ising models that may only be observed at their leaf nodes. On the learning side, we obtain efficient algorithms for learning a tree-structured Ising model whose leaf node distribution is close in Total Variation Distance, improving on the results of prior work. On the testing side, we provide an efficient algorithm with fewer samples for testing whether two latent-tree Ising models have leaf-node distributions that are close or far in Total Variation distance. We obtain our algorithms by showing novel localization results for the total variation distance between the leaf-node distributions of tree-structured Ising models, in terms of their marginals on pairs of leaves."}}
{"id": "Bo1Dm_M3qca", "cdate": 1640995200000, "mdate": 1681875917377, "content": {"title": "Verification and search algorithms for causal DAGs", "abstract": "We study two problems related to recovering causal graphs from interventional data: (i) $\\textit{verification}$, where the task is to check if a purported causal graph is correct, and (ii) $\\textit{search}$, where the task is to recover the correct causal graph. For both, we wish to minimize the number of interventions performed. For the first problem, we give a characterization of a minimal sized set of atomic interventions that is necessary and sufficient to check the correctness of a claimed causal graph. Our characterization uses the notion of $\\textit{covered edges}$, which enables us to obtain simple proofs and also easily reason about earlier known results. We also generalize our results to the settings of bounded size interventions and node-dependent interventional costs. For all the above settings, we provide the first known provable algorithms for efficiently computing (near)-optimal verifying sets on general graphs. For the second problem, we give a simple adaptive algorithm based on graph separators that produces an atomic intervention set which fully orients any essential graph while using $\\mathcal{O}(\\log n)$ times the optimal number of interventions needed to $\\textit{verify}$ (verifying size) the underlying DAG on $n$ vertices. This approximation is tight as $\\textit{any}$ search algorithm on an essential line graph has worst case approximation ratio of $\\Omega(\\log n)$ with respect to the verifying size. With bounded size interventions, each of size $\\leq k$, our algorithm gives an $\\mathcal{O}(\\log n \\cdot \\log k)$ factor approximation. Our result is the first known algorithm that gives a non-trivial approximation guarantee to the verifying size on general unweighted graphs and with bounded size interventions."}}
{"id": "FKCTeO1fsvH", "cdate": 1621629824222, "mdate": null, "content": {"title": "The Complexity of Sparse Tensor PCA", "abstract": "We study the problem of sparse tensor principal component analysis: given a tensor $\\pmb Y = \\pmb W + \\lambda x^{\\otimes p}$ with $\\pmb W \\in \\otimes^p \\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover the $k$-sparse unit vector $x \\in \\mathbb{R}^n$. The model captures both sparse PCA (in its Wigner form)  and tensor PCA.\n\nFor the highly sparse regime of $k \\leq \\sqrt{n}$, we present a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For any $1 \\leq t \\leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio $\\lambda \\geq \\tilde{\\mathcal{O}} (\\sqrt{t} \\cdot (k/t)^{p/2})$ in time $\\tilde{\\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for the matrix settings (in both the polynomial-time and sub-exponential time regimes).\n\nOur results naturally extend to the case of $r$ distinct $k$-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes. Even in the restricted case of sparse PCA, known algorithms only recover the sparse vectors for $\\lambda \\geq \\tilde{\\mathcal{O}}(k \\cdot r)$ while our algorithms require $\\lambda \\geq \\tilde{\\mathcal{O}}(k)$.\n\nFinally, by analyzing the low-degree likelihood ratio, we complement these algorithmic results with rigorous evidence illustrating the trade-offs between signal-to-noise ratio and running time. This lower bound captures the known lower bounds for both sparse  PCA and tensor PCA. In this general model, we observe a more intricate three-way trade-off between the number of samples $n$, the sparsity $k$, and the tensor power $p$."}}
{"id": "iDomGIQF5a", "cdate": 1609459200000, "mdate": 1681875917377, "content": {"title": "Massively Parallel Correlation Clustering in Bounded Arboricity Graphs", "abstract": "Identifying clusters of similar elements in a set is a common task in data analysis. With the immense growth of data and physical limitations on single processor speed, it is necessary to find efficient parallel algorithms for clustering tasks. In this paper, we study the problem of correlation clustering in bounded arboricity graphs with respect to the Massively Parallel Computation (MPC) model. More specifically, we are given a complete graph where the edges are either positive or negative, indicating whether pairs of vertices are similar or dissimilar. The task is to partition the vertices into clusters with as few disagreements as possible. That is, we want to minimize the number of positive inter-cluster edges and negative intra-cluster edges. Consider an input graph G on n vertices such that the positive edges induce a \u03bb-arboric graph. Our main result is a 3-approximation (in expectation) algorithm to correlation clustering that runs in \ud835\udcaa(log \u03bb \u22c5 poly(log log n)) MPC rounds in the strongly sublinear memory regime. This is obtained by combining structural properties of correlation clustering on bounded arboricity graphs with the insights of Fischer and Noever (SODA '18) on randomized greedy MIS and the PIVOT algorithm of Ailon, Charikar, and Newman (STOC '05). Combined with known graph matching algorithms, our structural property also implies an exact algorithm and algorithms with worst case (1+\u03b5)-approximation guarantees in the special case of forests, where \u03bb = 1."}}
{"id": "OlDBol1XZU", "cdate": 1609459200000, "mdate": 1681875917377, "content": {"title": "The Complexity of Sparse Tensor PCA", "abstract": "We study the problem of sparse tensor principal component analysis: given a tensor $\\pmb Y = \\pmb W + \\lambda x^{\\otimes p}$ with $\\pmb W \\in \\otimes^p \\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover the $k$-sparse unit vector $x \\in \\mathbb{R}^n$. The model captures both sparse PCA (in its Wigner form) and tensor PCA.For the highly sparse regime of $k \\leq \\sqrt{n}$, we present a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For any $1 \\leq t \\leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio $\\lambda \\geq \\tilde{\\mathcal{O}} (\\sqrt{t} \\cdot (k/t)^{p/2})$ in time $\\tilde{\\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for the matrix settings (in both the polynomial-time and sub-exponential time regimes).Our results naturally extend to the case of $r$ distinct $k$-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes. Even in the restricted case of sparse PCA, known algorithms only recover the sparse vectors for $\\lambda \\geq \\tilde{\\mathcal{O}}(k \\cdot r)$ while our algorithms require $\\lambda \\geq \\tilde{\\mathcal{O}}(k)$.Finally, by analyzing the low-degree likelihood ratio, we complement these algorithmic results with rigorous evidence illustrating the trade-offs between signal-to-noise ratio and running time. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. In this general model, we observe a more intricate three-way trade-off between the number of samples $n$, the sparsity $k$, and the tensor power $p$."}}
{"id": "s4Aol1ZVmV", "cdate": 1577836800000, "mdate": 1681875917587, "content": {"title": "k-means++: few more steps yield constant approximation", "abstract": "The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k) approximation. Recently, ..."}}
