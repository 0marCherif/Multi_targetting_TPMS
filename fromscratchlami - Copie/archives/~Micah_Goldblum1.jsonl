{"id": "_QreMdMNIz-", "cdate": 1677713820932, "mdate": null, "content": {"title": "Seeing in Words: Learning to Classify through Language Bottlenecks", "abstract": "Neural networks for computer vision extract uninterpretable features despite achieving high accuracy on benchmarks.  In contrast, humans can explain their predictions using succinct and intuitive descriptions.  To incorporate explainability into neural networks, we train a vision model whose feature representations are text.  We show that such a model can effectively classify ImageNet images, and we discuss the challenges we encountered when training it."}}
{"id": "nWUvkpYG-aV", "cdate": 1672531200000, "mdate": 1682335263608, "content": {"title": "Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses", "abstract": "As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space."}}
{"id": "lydVJKD3QXi", "cdate": 1672531200000, "mdate": 1695381357885, "content": {"title": "Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent", "abstract": ""}}
{"id": "ke0CDZNX9x", "cdate": 1672531200000, "mdate": 1682335263623, "content": {"title": "Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness", "abstract": "The robustness of a deep classifier can be characterized by its margins: the decision boundary's distances to natural data points. However, it is unclear whether existing robust training methods effectively increase the margin for each vulnerable point during training. To understand this, we propose a continuous-time framework for quantifying the relative speed of the decision boundary with respect to each individual point. Through visualizing the moving speed of the decision boundary under Adversarial Training, one of the most effective robust training algorithms, a surprising moving-behavior is revealed: the decision boundary moves away from some vulnerable points but simultaneously moves closer to others, decreasing their margins. To alleviate these conflicting dynamics of the decision boundary, we propose Dynamics-aware Robust Training (DyART), which encourages the decision boundary to engage in movement that prioritizes increasing smaller margins. In contrast to prior works, DyART directly operates on the margins rather than their indirect approximations, allowing for more targeted and effective robustness improvement. Experiments on the CIFAR-10 and Tiny-ImageNet datasets verify that DyART alleviates the conflicting dynamics of the decision boundary and obtains improved robustness under various perturbation sizes compared to the state-of-the-art defenses. Our code is available at https://github.com/Yuancheng-Xu/Dynamics-Aware-Robust-Training."}}
{"id": "ejYMtn8oSaG", "cdate": 1672531200000, "mdate": 1695381357920, "content": {"title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models", "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision."}}
{"id": "d1XnR313GM", "cdate": 1672531200000, "mdate": 1695381357927, "content": {"title": "Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness", "abstract": ""}}
{"id": "UY5n8pfLMg", "cdate": 1672531200000, "mdate": 1695381357941, "content": {"title": "On the Reliability of Watermarks for Large Language Models", "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors."}}
{"id": "SUFbcd-sQh9", "cdate": 1672531200000, "mdate": 1695381357893, "content": {"title": "The Lie Derivative for Measuring Learned Equivariance", "abstract": ""}}
{"id": "Rw_y4XoViY", "cdate": 1672531200000, "mdate": 1695381357889, "content": {"title": "How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization", "abstract": ""}}
{"id": "Oy5qk3aSko_", "cdate": 1672531200000, "mdate": 1695381357928, "content": {"title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models", "abstract": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data."}}
