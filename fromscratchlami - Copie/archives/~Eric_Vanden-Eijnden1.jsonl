{"id": "-8hY-Ak-8l", "cdate": 1672531200000, "mdate": 1682339735086, "content": {"title": "Stochastic Interpolants: A Unifying Framework for Flows and Diffusions", "abstract": ""}}
{"id": "li7qeBbCR1t", "cdate": 1663850460284, "mdate": null, "content": {"title": "Building Normalizing Flows with Stochastic Interpolants", "abstract": "A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet $32 \\times 32$. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to $128\\times128$."}}
{"id": "x9tAJ3_N0k", "cdate": 1663850447394, "mdate": null, "content": {"title": "Probability flow solution of the Fokker-Planck equation", "abstract": "The method of choice for integrating the time-dependent Fokker-Planck equation in high-dimension is to generate samples from the solution via integration of the associated stochastic differential equation. Here, we introduce an alternative scheme based on integrating an ordinary differential equation that describes the flow of probability. Acting as a transport map, this equation deterministically pushes samples from the initial density onto samples from the solution at any later time. Unlike integration of the stochastic dynamics, the method has the advantage of giving direct access to quantities that are challenging to estimate from trajectories alone, such as the probability current, the density itself, and its entropy. The probability flow equation depends on the gradient of the logarithm of the solution (its \"score\"), and so is a-priori unknown. To resolve this dependence, we model the score with a deep neural network that is learned on-the-fly by propagating a set of samples according to the instantaneous probability current. We consider several high-dimensional examples from the physics of interacting particle systems to highlight the efficiency and precision of the approach; we find that the method accurately matches analytical solutions computed by hand and moments computed via Monte-Carlo."}}
{"id": "dZEZu7zxJBF", "cdate": 1652737813271, "mdate": null, "content": {"title": "Learning sparse features can lead to overfitting in neural networks", "abstract": "It is widely believed that the success of deep networks lies in their ability to learn a meaningful representation of the features of the data. Yet, understanding when and how this feature learning improves performance remains a challenge: for example, it is beneficial for modern architectures trained to classify images, whereas it is detrimental for fully-connected networks trained for the same task on the same data. Here we propose an explanation for this puzzle, by showing that feature learning can perform worse than lazy training (via random feature kernel or the NTK) as the former can lead to a sparser neural representation. Although sparsity is known to be essential for learning anisotropic data, it is detrimental when the target function is constant or smooth along certain directions of input space. We illustrate this phenomenon in two settings: (i) regression of Gaussian random functions on the $d$-dimensional unit sphere and  (ii) classification of benchmark datasets of images. For (i), we compute the scaling of the generalization error with number of training points, and show that methods that do not learn features generalize better, even when the dimension of the input space is large. For (ii), we show empirically that learning features can indeed lead to sparse and thereby less smooth representations of the image predictors. This fact is plausibly responsible for deteriorating the performance, which is known to be correlated with smoothness along diffeomorphisms."}}
{"id": "rYkGxHPnCIf", "cdate": 1652737800868, "mdate": null, "content": {"title": "Learning Optimal Flows for Non-Equilibrium Importance Sampling", "abstract": "Many applications in computational sciences and statistical inference require the computation of expectations with respect to complex high-dimensional distributions with unknown normalization constants, as well as the estimation of these constants. Here we develop a method to perform these calculations based on generating samples from a simple base distribution, transporting them by the flow generated by a velocity field, and performing averages along these flowlines. This non-equilibrium importance sampling (NEIS) strategy is straightforward to implement and can be used for calculations with arbitrary target distributions. On the theory side, we discuss how to tailor the velocity field to the target and establish general conditions under which the proposed estimator is a perfect estimator with zero-variance. We also draw connections between NEIS and approaches based on mapping a base distribution onto a target via a transport map. On the computational side, we show how to use deep learning to represent the velocity field by a neural network and train it towards the zero variance optimum. These results are illustrated numerically on benchmark examples (with dimension up to $10$), where after training the velocity field, the variance of the NEIS estimator is reduced by up to $6$ orders of magnitude than that of a vanilla estimator. We also compare the performances of NEIS with those of Neal's annealed importance sampling (AIS)."}}
{"id": "bWZl2zOzpXM", "cdate": 1640995200000, "mdate": 1682339734951, "content": {"title": "Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations", "abstract": ""}}
{"id": "VRrLSYdQ5Y_", "cdate": 1640995200000, "mdate": 1682339734984, "content": {"title": "Building Normalizing Flows with Stochastic Interpolants", "abstract": ""}}
{"id": "Sy4sprNUGSr", "cdate": 1640995200000, "mdate": 1682223185837, "content": {"title": "A Functional-Space Mean-Field Theory of Partially-Trained Three-Layer Neural Networks", "abstract": "To understand the training dynamics of neural networks (NNs), prior studies have considered the infinite-width mean-field (MF) limit of two-layer NN, establishing theoretical guarantees of its convergence under gradient flow training as well as its approximation and generalization capabilities. In this work, we study the infinite-width limit of a type of three-layer NN model whose first layer is random and fixed. To define the limiting model rigorously, we generalize the MF theory of two-layer NNs by treating the neurons as belonging to functional spaces. Then, by writing the MF training dynamics as a kernel gradient flow with a time-varying kernel that remains positive-definite, we prove that its training loss in $L_2$ regression decays to zero at a linear rate. Furthermore, we define function spaces that include the solutions obtainable through the MF training dynamics and prove Rademacher complexity bounds for these spaces. Our theory accommodates different scaling choices of the model, resulting in two regimes of the MF limit that demonstrate distinctive behaviors while both exhibiting feature learning."}}
{"id": "OgXM1Y-bg2", "cdate": 1640995200000, "mdate": 1681650247536, "content": {"title": "Learning sparse features can lead to overfitting in neural networks", "abstract": ""}}
{"id": "Kz2u-qNYQ0a", "cdate": 1640995200000, "mdate": 1682339734912, "content": {"title": "On feature learning in neural networks with global convergence guarantees", "abstract": ""}}
