{"id": "BEhFhsLKi3C", "cdate": 1677628800000, "mdate": 1680034885073, "content": {"title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning", "abstract": ""}}
{"id": "DM31JBxmKIN", "cdate": 1640995200000, "mdate": 1667331725725, "content": {"title": "Conceptual complexity of neural networks", "abstract": ""}}
{"id": "zwczNt3e6za", "cdate": 1609459200000, "mdate": null, "content": {"title": "Conceptual capacity and effective complexity of neural networks", "abstract": "We propose a complexity measure of a neural network mapping function based on the diversity of the set of tangent spaces from different inputs. Treating each tangent space as a linear PAC concept we use an entropy-based measure of the bundle of concepts in order to estimate the conceptual capacity of the network. The theoretical maximal capacity of a ReLU network is equivalent to the number of its neurons. In practice however, due to correlations between neuron activities within the network, the actual capacity can be remarkably small, even for very big networks. Empirical evaluations show that this new measure is correlated with the complexity of the mapping function and thus the generalisation capabilities of the corresponding network. It captures the effective, as oppose to the theoretical, complexity of the network function. We also showcase some uses of the proposed measure for analysis and comparison of trained neural network models."}}
{"id": "yGp7d-beuj", "cdate": 1609459200000, "mdate": null, "content": {"title": "Pseudo-rehearsal: Achieving deep reinforcement learning without catastrophic forgetting", "abstract": "Neural networks can achieve excellent results in a wide variety of applications. However, when they attempt to sequentially learn, they tend to learn the new task while catastrophically forgetting previous ones. We propose a model that overcomes catastrophic forgetting in sequential reinforcement learning by combining ideas from continual learning in both the image classification domain and the reinforcement learning domain. This model features a dual memory system which separates continual learning from reinforcement learning and a pseudo-rehearsal system that \u201crecalls\u201d items representative of previous tasks via a deep generative network. Our model sequentially learns Atari 2600 games without demonstrating catastrophic forgetting and continues to perform above human level on all three games. This result is achieved without: demanding additional storage requirements as the number of tasks increases, storing raw data or revisiting past tasks. In comparison, previous state-of-the-art solutions are substantially more vulnerable to forgetting on these complex deep reinforcement learning tasks."}}
{"id": "bi3ea1YvarD", "cdate": 1577836800000, "mdate": null, "content": {"title": "MIME: Mutual Information Minimisation Exploration", "abstract": "We show that reinforcement learning agents that learn by surprise (surprisal) get stuck at abrupt environmental transition boundaries because these transitions are difficult to learn. We propose a counter-intuitive solution that we call Mutual Information Minimising Exploration (MIME) where an agent learns a latent representation of the environment without trying to predict the future states. We show that our agent performs significantly better over sharp transition boundaries while matching the performance of surprisal driven agents elsewhere. In particular, we show state-of-the-art performance on difficult learning games such as Gravitar, Montezuma's Revenge and Doom."}}
{"id": "BZY7_rNGDWX", "cdate": 1577836800000, "mdate": null, "content": {"title": "Predicting Cherry Quality Using Siamese Networks", "abstract": "The cherry industry is a rapidly growing sector of New Zealand's export merchandise and, as such, the accuracy with which pack-houses can grade cherries during processing is becoming increasingly critical. Conventional computer vision systems are usually employed in this process, yet they fall short in many respects, still requiring humans to manually verify the grading. In this work, we investigate the use of deep learning to improve upon the traditional approach. The nature of the industry means that the grade standards are influenced by a range of factors and can change on a daily basis. This makes conventional classification approaches infeasible (as there are no fixed classes) so we construct a model to overcome this. We convert the problem from classification to regression, using a Siamese network trained with pairwise comparison labels. We extract the model embedded within to predict continuous quality values for the fruit. Our model is able to predict which of two similar quality fruit is better with over 88% accuracy, only 5% below the self-agreement of a human expert."}}
{"id": "0dK3voKc5hF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deep Sheep: kinship assignment in livestock from facial images", "abstract": "For the non-farmer folk all sheep might look the same, but they are in fact morphologically quite different; including when it comes to facial features. Image analysis has already demonstrated that computer-based facial recognition in livestock is very accurate. We investigate the viability of deep learning for assigning kinship in livestock for use in genetic evaluation- given two images of sheep faces, our proposed model predicts their genetic relationship. In this work we present two CNN models: one for face detection (reporting 80% accuracy) and one for kinship detection (reporting 68% balanced accuracy)."}}
{"id": "SyxjVRVKDB", "cdate": 1569439282870, "mdate": null, "content": {"title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability", "abstract": "We introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that completely determine the entire computation of the deep network for a given input instance. We also propose that for interpretability it is more instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network's computation. We introduce a novel interpretability method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is more robust (in the presence of noise), more complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks"}}
{"id": "r-2jO9hRJj", "cdate": 1546300800000, "mdate": 1667331725923, "content": {"title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability", "abstract": "We introduce switched linear projections for expressing the activity of a neuron in a deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that determine the entire computation of the network for a given input instance. With these projections we can decompose activity in any hidden layer into patterns detected in a given input instance. We also propose that in ReLU networks it is instructive and meaningful to examine patterns that deactivate the neurons in a hidden layer, something that is implicitly ignored by the existing interpretability methods tracking solely the active aspect of the network's computation."}}
{"id": "1D_zicdE97", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Convolutional Self-organizing Map for Visual Category Learning", "abstract": "In this paper we present a novel neural network architecture that aims to combine the highly popular and successful convolutional neural network architecture with the learning mechanism of an unsupervised self-organizing map. The convolutional self-organizing map (ConvSOM) is a hierarchical network consisting of several independent self-organizing maps. It incorporates features associated with convolutional networks, such as weight sharing, spatial pooling, and hierarchical abstraction, with the unsupervised, topographically organized self-organizing map. We will show that the resulting architecture performs poorly on the MNIST data set, but offers interesting avenues for further research."}}
