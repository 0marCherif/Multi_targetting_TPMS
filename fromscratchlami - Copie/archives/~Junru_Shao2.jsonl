{"id": "nyCr6-0hinG", "cdate": 1652737819886, "mdate": null, "content": {"title": "Tensor Program Optimization with Probabilistic Programs", "abstract": "Automatic optimization for tensor programs becomes increasingly important as we deploy deep learning in various environments, and efficient optimization relies on a rich search space and effective search. Most existing efforts adopt a search space which lacks the ability to efficiently enable domain experts to grow the search space. This paper introduces MetaSchedule, a domain-specific probabilistic programming language abstraction to construct a rich search space of tensor programs. Our abstraction allows domain experts to analyze the program, and easily propose stochastic choices in a modular way to compose program transformation accordingly. We also build an end-to-end learning-driven framework to find an optimized program for a given search space. Experimental results show that MetaSchedule can cover the search space used in the state-of-the-art tensor program optimization frameworks in a modular way. Additionally, it empowers domain experts to conveniently grow the search space and modularly enhance the system, which brings 48% speedup on end-to-end deep learning workloads."}}
{"id": "kRfOeQ9Ot9H", "cdate": 1640995200000, "mdate": 1672249819710, "content": {"title": "Tensor Program Optimization with Probabilistic Programs", "abstract": ""}}
{"id": "9FIx4ExM0M", "cdate": 1640995200000, "mdate": 1672249819768, "content": {"title": "SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning", "abstract": ""}}
{"id": "1ut1SfyRau", "cdate": 1640995200000, "mdate": 1672249819770, "content": {"title": "TensorIR: An Abstraction for Automatic Tensorized Program Optimization", "abstract": ""}}
{"id": "aIfp8kLuvc9", "cdate": 1622976572830, "mdate": null, "content": {"title": "TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers", "abstract": "Search-based tensor compilers can greatly accelerate the execution of machine learning models by generating high-performance tensor programs, such as matrix multiplications and convolutions. These compilers take a high-level mathematical expression as input and search for the fastest low-level implementations. At the core of the search procedure is a cost model which estimates the performance of different candidates to reduce the frequency of time-consuming on-device measurements. There has been a growing interest in using machine learning techniques to learn a cost model to ease the effort of building an analytical model. However, a standard dataset for pre-training and benchmarking learned cost models is lacking.\n\nWe introduce TenSet, a large-scale tensor program performance dataset. TenSet contains 52 million program performance records collected from 6 hardware platforms. We provide comprehensive studies on how to learn and evaluate the cost models, including data collection, model architectures, loss functions, transfer learning, and evaluation metrics. We also show that a cost model pre-trained on TenSet can accelerate the search time in the state-of-the-art tensor compiler by up to 10$\\times$. The dataset is available at https://github.com/tlc-pack/tenset."}}
{"id": "YWVVaMWDXG", "cdate": 1609459200000, "mdate": 1672249819770, "content": {"title": "TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers", "abstract": ""}}
{"id": "CSj1wIjdaI", "cdate": 1546300800000, "mdate": null, "content": {"title": "Deep Neural Networks with Multi-Branch Architectures Are Intrinsically Less Non-Convex", "abstract": "Several recently proposed architectures of neural networks such as ResNeXt, Inception, Xception, SqueezeNet and Wide ResNet are based on the designing idea of having multiple branches and have demo..."}}
{"id": "rkbqwneOWB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "abstract": "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer)."}}
{"id": "FQzfnl7T7JS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Generalized Residual Vector Quantization and Aggregating Tree for Large Scale Search", "abstract": "Vector quantization is an essential tool for tasks involving large scale data, for example, large scale similarity search, which is crucial for content-based information retrieval and analysis. In this paper, we propose a novel vector quantization framework that iteratively minimizes quantization error. First, we provide a detailed review on a relevant vector quantization method named residual vector quantization (RVQ). Next, we propose generalized residual vector quantization (GRVQ) to further improve over RVQ. Many vector quantization methods can be viewed as special cases of our proposed method. To enable GRVQ on billion scale data, we introduce a nonexhaustive search scheme named aggregating tree (A-Tree) for high dimensional data that uses GRVQ encodings to build a radix tree and perform the nearest neighbor search by beam search. To search accurately and efficiently, VQ-encodings should satisfy locally aggregating encoding criterion: For any node of the corresponding A-Tree, neighboring vectors should aggregate in fewer subtrees to make beam search efficient. We show that the proposed GRVQ encodings best satisfy the suggested criterion, and the joint use of GRVQ and A-Tree shows significantly better performances on billion scale datasets. Our methods are validated on several standard benchmark datasets. Experimental results and empirical analysis show the superior efficiency and effectiveness of our proposed methods compared to the state-of-the-art for large scale search."}}
{"id": "kBTXpjR8nU", "cdate": 1451606400000, "mdate": 1672249819771, "content": {"title": "Generalized residual vector quantization for large scale data", "abstract": ""}}
