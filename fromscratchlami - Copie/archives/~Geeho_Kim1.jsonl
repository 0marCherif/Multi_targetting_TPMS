{"id": "de-_FHXQ4--", "cdate": 1663849978088, "mdate": null, "content": {"title": "Communication-Efficient Federated Learning with Accelerated Client Gradient", "abstract": "Federated learning often suffers from slow and unstable convergence due to heterogeneous characteristics of participating client datasets.\nSuch a tendency is aggravated when the client participation ratio is low since the information collected from the clients is prone to have large variations.\nTo tackle this challenge, we propose a novel federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model.\nThis is achieved by making the server broadcast a global model with a gradient acceleration.\nBy adopting the strategy, the proposed algorithm conveys the projective global update information to participants effectively with no extra communication cost and relieves the clients from storing the previous models.\nWe also regularize local updates by aligning each of the clients with the overshot global model to reduce bias and improve the stability of our algorithm.\nWe perform comprehensive empirical studies on real data under various settings and demonstrate remarkable performance gains of the proposed method in terms of accuracy and communication efficiency compared to the state-of-the-art methods, especially with low client participation rates.\nWe will release our code to facilitate and disseminate our work."}}
{"id": "l4WlGSDt0XB", "cdate": 1640995200000, "mdate": 1667553681233, "content": {"title": "Multi-Level Branched Regularization for Federated Learning", "abstract": "A critical challenge of federated learning is data heterogeneity and imbalance across clients, which leads to inconsistency between local networks and unstable convergence of global models. To alle..."}}
{"id": "xEaJvbVKeT", "cdate": 1632875573818, "mdate": null, "content": {"title": "Open-Set Representation Learning through Combinatorial Embedding", "abstract": "Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on the examples in both labeled and unlabeled classes, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. We also introduce a metric learning strategy to estimate pairwise pseudo-labels for improving representations of unlabeled examples, which preserves semantic relations across known and novel classes effectively. The proposed algorithm discovers novel concepts via a joint optimization of enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable performance gains by the proposed approach in multiple image retrieval and novel class discovery benchmarks."}}
{"id": "Ov3ajFYKOv", "cdate": 1577836800000, "mdate": 1667553681338, "content": {"title": "Learning to Optimize Domain Specific Normalization for Domain Generalization", "abstract": "We propose a simple but effective multi-source domain generalization technique based on deep neural networks by incorporating optimized normalization layers that are specific to individual domains. Our approach employs multiple normalization methods while learning separate affine parameters per domain. For each domain, the activations are normalized by a weighted average of multiple normalization statistics. The normalization statistics are kept track of separately for each normalization type if necessary. Specifically, we employ batch and instance normalizations in our implementation to identify the best combination of these two normalization methods in each domain. The optimized normalization layers are effective to enhance the generalizability of the learned model. We demonstrate the state-of-the-art accuracy of our algorithm in the standard domain generalization benchmarks, as well as viability to further tasks such as multi-source domain adaptation and domain generalization in the presence of label noise."}}
{"id": "KrHwjZgB8J", "cdate": 1546300800000, "mdate": 1667553681337, "content": {"title": "Combinatorial Inference against Label Noise", "abstract": "Label noise is one of the critical sources that degrade generalization performance of deep neural networks significantly. To handle the label noise issue in a principled way, we propose a unique classification framework of constructing multiple models in heterogeneous coarse-grained meta-class spaces and making joint inference of the trained models for the final predictions in the original (base) class space. Our approach reduces noise level by simply constructing meta-classes and improves accuracy via combinatorial inferences over multiple constituent classifiers. Since the proposed framework has distinct and complementary properties for the given problem, we can even incorporate additional off-the-shelf learning algorithms to improve accuracy further. We also introduce techniques to organize multiple heterogeneous meta-class sets using $k$-means clustering and identify a desirable subset leading to learn compact models. Our extensive experiments demonstrate outstanding performance in terms of accuracy and efficiency compared to the state-of-the-art methods under various synthetic noise configurations and in a real-world noisy dataset."}}
