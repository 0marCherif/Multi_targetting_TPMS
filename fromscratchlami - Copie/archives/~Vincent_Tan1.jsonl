{"id": "pOCOv-RFhc9", "cdate": 1683882821482, "mdate": 1683882821482, "content": {"title": "Covert Identification Over Binary-Input Discrete Memoryless Channels", "abstract": "This paper considers the covert identification problem in which a sender aims to reliably convey an identification (ID) message to a set of receivers via a binary-input discrete memoryless channel (BDMC), and simultaneously to guarantee that the communication is covert with respect to a warden who monitors the communication via another independent BDMC. We prove a square-root law for the covert identification problem. This states that an ID message of size exp(exp(\u0398(\u221a{ n})) can be transmitted over n channel uses. We then characterize the exact pre-constant in the \u0398(\u00b7) notation. This constant is referred to as the covert identification capacity. We show that it equals the recently developed covert capacity in the standard covert communication problem, and somewhat surprisingly, the covert identification capacity can be achieved without any shared key between the sender and receivers. The achievability proof relies on a random coding argument with pulse-position modulation (PPM), coupled with a second stage which performs code refinements. The converse proof relies on an expurgation argument as well as results for channel resolvability with stringent input constraints."}}
{"id": "kE85Tt5dH1z", "cdate": 1683882684821, "mdate": 1683882684821, "content": {"title": "Optimal Change-Point Detection With Training Sequences in the Large and Moderate Deviations Regimes", "abstract": "This paper investigates a novel offline change-point detection problem from an information-theoretic perspective. In contrast to most related works, we assume that the knowledge of the underlying pre- and post-change distributions are not known and can only be learned from the training sequences which are available. We further require the probability of the estimation error to decay either exponentially or sub-exponentially fast (corresponding respectively to the large and moderate deviations regimes in information theory parlance). Based on the training sequences as well as the test sequence consisting of a single change-point, we design a change-point estimator and further show that this estimator is optimal by establishing matching (strong) converses. This leads to a full characterization of the optimal confidence width (i.e., half the width of the confidence interval within which the true change-point is located at with high probability) as a function of the undetected error, under both the large and moderate deviations regimes."}}
{"id": "_XBTjpNbamF", "cdate": 1683882652792, "mdate": 1683882652792, "content": {"title": "Community Detection and Matrix Completion With Social and Item Similarity Graphs", "abstract": "We consider the problem of recovering a binary rating matrix as well as clusters of users and items based on a partially observed matrix together with side-information in the form of social and item similarity graphs. These two graphs are both generated according to the celebrated stochastic block model (SBM). We develop lower and upper bounds on sample complexity that match for various scenarios. Our information-theoretic results quantify the benefits of the availability of the social and item similarity graphs. Further analysis reveals that under certain scenarios, the social and item similarity graphs produce an interesting synergistic effect. This means that observing two graphs is strictly better than observing just one in terms of reducing the sample complexity."}}
{"id": "gohuhIU6_lv", "cdate": 1683882618281, "mdate": 1683882618281, "content": {"title": "Mc2g: An Efficient Algorithm for Matrix Completion With Social and Item Similarity Graphs", "abstract": "In this paper, we design and analyze Mc2g ( M atrix C ompletion with 2 G raphs), an efficient algorithm that performs matrix completion in the presence of social and item similarity graphs. Mc2g runs in quasilinear time and is parameter free. It is based on spectral clustering and local refinement steps. For the matrix completion problem which possesses additional block structures in its rows and columns, we derive the expected number of sampled entries required for Mc2g to succeed, and further show that it matches an information-theoretic lower bound up to a constant factor for a wide range of parameters. We perform extensive experiments on both synthetic datasets and a semi-real dataset inspired by real graphs. The experimental results show that Mc2g outperforms other state-of-the-art matrix completion algorithms."}}
{"id": "5ktYrdbNLU", "cdate": 1683882364688, "mdate": 1683882364688, "content": {"title": "Covert Communication With Mismatched Decoders", "abstract": "This paper considers the problem of covert communication over binary-input discrete memoryless channels with mismatched decoding, in which a sender wishes to reliably communicate with a receiver whose decoder is fixed and possibly sub-optimal, and simultaneously to ensure that the communication is covert with respect to a warden. We present a single-letter lower bound and two single-letter upper bounds on the information-theoretically optimal throughput as a function of the given decoding metric, channel laws, and the desired level of covertness. These bounds match for a variety of scenarios of interest, such as (i) when the channel between the sender and receiver is a binary-input binary-output channel, and (ii) when the decoding metric is particularized to the so-called erasures-only metric. The lower bound is obtained based on a modified random coding union bound with pulse position modulation (PPM) codebooks, coupled with a non-standard expurgation argument. The upper bounds are based on the idea of translating the mismatched-decoding error of the original channel to the decoding error of an auxiliary channel."}}
{"id": "hfODpecGa3", "cdate": 1683882292496, "mdate": 1683882292496, "content": {"title": "Exact Recovery in the General Hypergraph Stochastic Block Model", "abstract": "This paper investigates fundamental limits of exact recovery in the general d -uniform hypergraph stochastic block model ( d -HSBM), wherein n nodes are partitioned into k disjoint communities with relative sizes (p1,\u2026,pk) . Each subset of nodes with cardinality d is generated independently as an order- d hyperedge with a certain probability that depends on the ground-truth communities that the d nodes belong to. The goal is to exactly recover the k hidden communities based on the observed hypergraph. We show that there exists a sharp threshold such that exact recovery is achievable above the threshold and impossible below the threshold (apart from a small regime of parameters that will be specified precisely). This threshold is represented in terms of a quantity which we term as the generalized Chernoff-Hellinger divergence between communities. Our result for this general model recovers prior results for the standard SBM and d -HSBM with two symmetric communities as special cases. En route to proving our achievability results, we develop a polynomial-time two-stage algorithm that meets the threshold. The first stage adopts a certain hypergraph spectral clustering method to obtain a coarse estimate of communities, and the second stage refines each node individually via local refinement steps to ensure exact recovery."}}
{"id": "YeN9IuZKAP", "cdate": 1680307200000, "mdate": 1681752873688, "content": {"title": "Active-LATHE: An Active Learning Algorithm for Boosting the Error Exponent for Learning Homogeneous Ising Trees", "abstract": "The Chow\u2013Liu algorithm (IEEE Trans. Inform. Theory, 1968) has been a mainstay for the learning of tree-structured graphical models from i.i.d. sampled data vectors. Its theoretical properties have been well-studied and are well-understood. In this paper, we focus on the class of trees that are arguably even more fundamental, namely <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">homogeneous</i> trees in which each pair of nodes that forms an edge has the same correlation <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\rho $ </tex-math></inline-formula> . We ask whether we are able to further reduce the error probability of learning the structure of the homogeneous tree model when <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">active learning</i> is allowed. Our figure of merit is the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">error exponent</i> , which quantifies the exponential rate of decay of the error probability with an increasing number of data samples. We design and analyze an algorithm <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Active</u> Learning <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</u> lgorithm for <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">T</u> rees with <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">H</u> omogeneous <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">E</u> dges (ACTIVE-LATHE), which surprisingly boosts the error exponent by at least 40% when <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\rho $ </tex-math></inline-formula> is at least 0.8. For all other values of <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\rho $ </tex-math></inline-formula> , we also observe commensurate, but more modest, improvements in the error exponent. Our analysis hinges on judiciously exploiting the minute but detectable statistical variation of the samples to allocate more data to parts of the graph in which we are less confident of being correct."}}
{"id": "F-DtvMyJzA7", "cdate": 1680307200000, "mdate": 1683879012335, "content": {"title": "Almost Optimal Variance-Constrained Best Arm Identification", "abstract": "We design and analyze Variance-Aware-Lower and Upper Confidence Bound (VA-LUCB), a parameter-free algorithm, for identifying the best arm under the fixed-confidence setup and under a stringent constraint that the variance of the chosen arm is strictly smaller than a given threshold. An upper bound on VA-LUCB\u2019s sample complexity is shown to be characterized by a fundamental variance-aware hardness quantity <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$H_{\\mathrm {VA}}$ </tex-math></inline-formula> . By proving an information-theoretic lower bound, we show that sample complexity of VA-LUCB is optimal up to a factor logarithmic in <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$H_{\\mathrm {VA}}$ </tex-math></inline-formula> . Extensive experiments corroborate the dependence of the sample complexity on the various terms in <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$H_{\\mathrm {VA}}$ </tex-math></inline-formula> . By comparing VA-LUCB\u2019s empirical performance to a close competitor RiskAverse-UCB-BAI by David et al. (2018) our experiments suggest that VA-LUCB has the lowest sample complexity for this class of risk-constrained best arm identification problems, especially for the riskiest instances."}}
{"id": "yRJLhPCJFyh", "cdate": 1672531200000, "mdate": 1683879012157, "content": {"title": "Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits", "abstract": "Motivated by concerns about making online decisions that incur undue amount of risk at each time step, in this paper, we formulate the probably anytime-safe stochastic combinatorial semi-bandits problem. In this problem, the agent is given the option to select a subset of size at most $K$ from a set of $L$ ground items. Each item is associated to a certain mean reward as well as a variance that represents its risk. To mitigate the risk that the agent incurs, we require that with probability at least $1-\\delta$, over the entire horizon of time $T$, each of the choices that the agent makes should contain items whose sum of variances does not exceed a certain variance budget. We call this probably anytime-safe constraint. Under this constraint, we design and analyze an algorithm {\\sc PASCombUCB} that minimizes the regret over the horizon of time $T$. By developing accompanying information-theoretic lower bounds, we show that under both the problem-dependent and problem-independent paradigms, {\\sc PASCombUCB} is almost asymptotically optimal. Experiments are conducted to corroborate our theoretical findings. Our problem setup, the proposed {\\sc PASCombUCB} algorithm, and novel analyses are applicable to domains such as recommendation systems and transportation in which an agent is allowed to choose multiple items at a single time step and wishes to control the risk over the whole time horizon."}}
{"id": "airA7wy8BM", "cdate": 1672531200000, "mdate": 1683879011730, "content": {"title": "Codes for Correcting t Limited-Magnitude Sticky Deletions", "abstract": "Codes for correcting sticky insertions/deletions and limited-magnitude errors have attracted significant attention due to their applications of flash memories, racetrack memories, and DNA data storage systems. In this paper, we first consider the error type of $t$-sticky deletions with $\\ell$-limited-magnitude and propose a non-systematic code for correcting this type of error with redundancy $2t(1-1/p)\\cdot\\log(n+1)+O(1)$, where $p$ is the smallest prime larger than $\\ell+1$. Next, we present a systematic code construction with an efficient encoding and decoding algorithm with redundancy $\\frac{\\lceil2t(1-1/p)\\rceil\\cdot\\lceil\\log p\\rceil}{\\log p} \\log(n+1)+O(\\log\\log n)$, where $p$ is the smallest prime larger than $\\ell+1$."}}
