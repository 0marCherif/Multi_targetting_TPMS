{"id": "r2MgyybnfO", "cdate": 1664928780754, "mdate": null, "content": {"title": "Tackling Distribution Shifts in Federated Learning with Superquantile Aggregation", "abstract": "Federated learning has emerged as the predominant framework for distributed machine learning over decentralized data, e.g. in mobile phones. The usual approaches suffer from a distribution shift: the model is trained to fit the average population distribution but is deployed on individual clients, whose data distributions can be quite different. We present a distributionally robust approach to federated learning based on a risk measure known as the superquantile and show how to optimize it by interleaving federated averaging steps with quantile computation. We demonstrate experimentally that our approach is competitive with usual ones in terms of average error and outperforms them in terms of tail statistics of the error. "}}
{"id": "832ufoAu9Hn", "cdate": 1663939407358, "mdate": null, "content": {"title": "Differentially Private Federated Quantiles with the Distributed Discrete Gaussian Mechanism", "abstract": " The computation of analytics in a federated environment plays an increasingly important role in data science and machine learning. We consider the differentially private computation of the quantiles of a distribution of values stored on a population of clients. We present two quantile estimation algorithms based on the distributed discrete Gaussian mechanism compatible with secure aggregation. Based on a privacy-utility analysis and numerical experiments, we delineate the regime under which each one is superior. We find that the algorithm with suboptimal asymptotic performance works the best on moderate problem sizes typical in federated learning with client sampling. We apply these algorithms to augment distributionally robust federated learning with differential privacy. "}}
{"id": "iFf26yMjRdN", "cdate": 1632875546842, "mdate": null, "content": {"title": "Federated Learning with Partial Model Personalization", "abstract": "We propose and analyze a general framework of federated learning with  partial model personalization. Compared with full model personalization, partial model personalization relies on domain knowledge to select a small portion of the model to personalize, thus imposing a much smaller on-device memory footprint. We propose two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. We give convergence analyses of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. Our experiments on real-world image and text datasets demonstrate that (a) partial model personalization can obtain most of the benefit of full model personalization with a small fraction of personalized parameters, and, (b) the alternating update algorithm often outperforms the simultaneous update algorithm."}}
{"id": "qWB9XX0Vmxr", "cdate": 1622243669763, "mdate": null, "content": {"title": "A Smoother Way to Train Structured Prediction Models", "abstract": "We present a framework to train a structured prediction model by performing smoothing on the inference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum margin structured prediction objective, and paves the way for the use of fast primal gradient-based optimization algorithms. We illustrate the proposed framework by developing a novel primal incremental optimization algorithm for the structural support vector machine. The proposed algorithm blends an extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study several practical variants, including extensions to deep structured prediction. We present experimental results on two real-world problems, namely named entity recognition and visual object localization. The experimental results show that the proposed framework allows us to build upon efficient inference algorithms to develop large-scale optimization algorithms for structured prediction which can achieve competitive performance on the two real-world problems. "}}
{"id": "gBCNItsYbx", "cdate": 1622243503258, "mdate": null, "content": {"title": "A Superquantile Approach to Federated Learningwith Heterogeneous Devices", "abstract": "We present a  federated learning framework that allows one to handle heterogeneous client devices that may not conform to the population data distribution.  The proposed approach hinges upon a  parameterized superquantile-based objective,  where the parameter ranges over levels of conformity. We introduce a stochastic optimization algorithm compatible with secure aggregation,  which interleaves device filtering steps with federated averaging steps. We conclude with numerical experiments with neural networks on computer vision and natural language processing data."}}
{"id": "Z_J5bCb4Rra", "cdate": 1621630133959, "mdate": null, "content": {"title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects, and Frontier Integrals", "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. We establish non-asymptotic bounds on the sample complexity of divergence frontiers. We also introduce frontier integrals which provide summary statistics of divergence frontiers. We show how smoothed estimators such as Good-Turing or Krichevsky-Trofimov can overcome the missing mass problem and lead to faster rates of convergence. We illustrate the theoretical results with numerical examples from natural language processing and computer vision."}}
{"id": "Tqx7nJp7PR", "cdate": 1621630126261, "mdate": null, "content": {"title": "MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers", "abstract": "As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce Mauve, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. Mauve scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that Mauve identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics."}}
{"id": "oiq92o1EFg1", "cdate": 1621629750815, "mdate": null, "content": {"title": "LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes", "abstract": "Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional neural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for $\\textbf{L}$earning $\\textbf{L}$ow-dimensional binary $\\textbf{C}$odes $(\\textbf{LLC})$ for instances as well as classes. Our method does ${\\textit{not}}$ require any side-information, like annotated attributes or label meta-data, and learns extremely low-dimensional binary codes ($\\approx 20$ bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring $\\textit{nearly optimal}$ classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further quantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 retrieval problem, our learnt binary codes outperform $16$ bit HashNet using only $10$ bits and also are as accurate as $10$ dimensional real representations. Finally, our learnt binary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs $\\approx3000$ samples to tune its threshold, while we require ${\\textit{none}}$. Code is open-sourced at https://github.com/RAIVNLab/LLC."}}
