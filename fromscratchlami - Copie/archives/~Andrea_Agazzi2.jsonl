{"id": "lee28D0UkJz", "cdate": 1672531200000, "mdate": 1682333198830, "content": {"title": "Global Optimality of Elman-type RNN in the Mean-Field Regime", "abstract": "We analyze Elman-type Recurrent Reural Networks (RNNs) and their training in the mean-field regime. Specifically, we show convergence of gradient descent training dynamics of the RNN to the corresponding mean-field formulation in the large width limit. We also show that the fixed points of the limiting infinite-width dynamics are globally optimal, under some assumptions on the initialization of the weights. Our results establish optimality for feature-learning with wide RNNs in the mean-field regime"}}
{"id": "UBR7UAUCwuh", "cdate": 1640995200000, "mdate": 1682333198703, "content": {"title": "A Homotopic Approach to Policy Gradients for Linear Quadratic Regulators with Nonlinear Controls", "abstract": "We study the convergence of deterministic policy gradient algorithms in continuous state and action space for the prototypical Linear Quadratic Regulator (LQR) problem when the search space is not limited to the family of linear policies. We first provide a counterexample showing that extending the policy class to piecewise linear functions results in local minima of the policy gradient algorithm. To solve this problem, we develop a new approach that involves sequentially increasing a discount factor between iterations of the original policy gradient algorithm. We finally prove that this homotopic variant of policy gradient methods converges to the global optimum of the undiscounted Linear Quadratic Regulator problem for a large class of Lipschitz, non-linear policies."}}
{"id": "rimqZx8XZ5", "cdate": 1609459200000, "mdate": 1682333198701, "content": {"title": "Urgency-aware optimal routing in repeated games through artificial currencies", "abstract": ""}}
{"id": "jm0qn5ALYLK", "cdate": 1609459200000, "mdate": 1682333198831, "content": {"title": "Temporal-difference learning with nonlinear function approximation: lazy training and mean field regimes", "abstract": "We discuss the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with wide neural networks trained with the Temporal-Difference (TD) learning algorithm. We first consider this problem under a certain scaling of the approximating function, leading to a regime called lazy training. In this regime, which arises naturally, implicit in the initialization of the neural network, the parameters of the model vary only slightly during the learning process, resulting in approximately linear behavior of the model. Both in the under- and over-parametrized frameworks, we prove exponential convergence to local, respectively global minimizers of the TD learning algorithm in the lazy training regime. We then compare the above scaling with the alternative mean-field scaling, where the approximately linear behavior of the model is lost. In this nonlinear, mean-field regime we prove that all fixed points of the dynamics in parameter space are global minimizers. We finally give examples of our convergence results in the case of models that diverge if trained with non-lazy TD learning."}}
{"id": "ggWg0aIsxt", "cdate": 1609459200000, "mdate": 1682333198701, "content": {"title": "Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime", "abstract": "We study the problem of policy optimization for infinite-horizon discounted Markov Decision Processes with softmax policy and nonlinear function approximation trained with policy gradient algorithms. We concentrate on the training dynamics in the mean-field regime, modeling e.g. the behavior of wide single hidden layer neural networks, when exploration is encouraged through entropy regularization. The dynamics of these models is established as a Wasserstein gradient flow of distributions in parameter space. We further prove global optimality of the fixed points of this dynamics under mild conditions on their initialization."}}
{"id": "bB2drc7DPuB", "cdate": 1601308239461, "mdate": null, "content": {"title": "Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime", "abstract": "We study the problem of policy optimization for infinite-horizon discounted Markov Decision Processes with softmax policy and nonlinear function approximation trained with policy gradient algorithms. We concentrate on the training dynamics in the mean-field regime, modeling e.g. the behavior of wide single hidden layer neural networks, when exploration is encouraged through entropy regularization. The dynamics of these models is established as a Wasserstein gradient flow of distributions in parameter space.  We further prove global optimality of the fixed points of this dynamics  under mild conditions on their initialization."}}
{"id": "sr3FQm5hG42", "cdate": 1577836800000, "mdate": 1682333198830, "content": {"title": "Urgency-aware Optimal Routing in Repeated Games through Artificial Currencies", "abstract": "When people choose routes minimizing their individual delay, the aggregate congestion can be much higher compared to that experienced by a centrally-imposed routing. Yet centralized routing is incompatible with the presence of self-interested agents. How can we reconcile the two? In this paper we address this question within a repeated game framework and propose a fair incentive mechanism based on artificial currencies that routes selfish agents in a system-optimal fashion, while accounting for their temporal preferences. We instantiate the framework in a parallel-network whereby agents commute repeatedly (e.g., daily) from a common start node to the end node. Thereafter, we focus on the specific two-arcs case whereby, based on an artificial currency, the agents are charged when traveling on the first, fast arc, whilst they are rewarded when traveling on the second, slower arc. We assume the agents to be rational and model their choices through a game where each agent aims at minimizing a combination of today's discomfort, weighted by their urgency, and the average discomfort encountered for the rest of the period (e.g., a week). We show that, if prices of artificial currencies are judiciously chosen, the routing pattern converges to a system-optimal solution, while accommodating the agents' urgency. We complement our study through numerical simulations. Our results show that it is possible to achieve a system-optimal solution whilst reducing the agents' perceived discomfort by 14-20% when compared to a centralized optimal but urgency-unaware policy."}}
{"id": "D_nDe_VVdd", "cdate": 1577836800000, "mdate": 1682333198701, "content": {"title": "Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime", "abstract": "We study the problem of policy optimization for infinite-horizon discounted Markov Decision Processes with softmax policy and nonlinear function approximation trained with policy gradient algorithms. We concentrate on the training dynamics in the mean-field regime, modeling e.g., the behavior of wide single hidden layer neural networks, when exploration is encouraged through entropy regularization. The dynamics of these models is established as a Wasserstein gradient flow of distributions in parameter space. We further prove global optimality of the fixed points of this dynamics under mild conditions on their initialization."}}
{"id": "HJghoa4YDB", "cdate": 1569439140081, "mdate": null, "content": {"title": "Temporal-difference learning for nonlinear value function approximation in the lazy training regime", "abstract": "We discuss the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm. We consider this problem under a certain scaling of the approximating function, leading to a regime called lazy training. In this regime the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling we study arises naturally, implicit in the initialization of their parameters.  Both in the under- and over-parametrized frameworks, we prove exponential convergence to local, respectively global minimizers of the above algorithm in the lazy training regime. We then give examples of such convergence results in the case of models that diverge if trained with non-lazy TD learning, and in the case of neural networks."}}
{"id": "wD0yoCljCP", "cdate": 1546300800000, "mdate": 1682333198831, "content": {"title": "Temporal-difference learning for nonlinear value function approximation in the lazy training regime", "abstract": "We discuss the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm. We first consider this problem under a certain scaling of the approximating function, leading to a regime called lazy training. In this regime, the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling we study arises naturally, implicit in the initialization of their parameters. Both in the under- and over-parametrized frameworks, we prove exponential convergence to local, respectively global minimizers of the above algorithm in the lazy training regime. We then compare this scaling of the parameters to the mean-field regime, where the approximately linear behavior of the model is lost. Under this alternative scaling we prove that all fixed points of the dynamics in parameter space are global minimizers. We finally give examples of our convergence results in the case of models that diverge if trained with non-lazy TD learning, and in the case of neural networks."}}
