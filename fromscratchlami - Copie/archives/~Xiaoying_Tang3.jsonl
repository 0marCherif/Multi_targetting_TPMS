{"id": "6Wbj3QCo4U4", "cdate": 1673287857907, "mdate": null, "content": {"title": "MProtoNet: A Case-Based Interpretable Model for Brain Tumor Classification with 3D Multi-parametric Magnetic Resonance Imaging", "abstract": "Recent applications of deep convolutional neural networks in medical imaging raise concerns about their interpretability. While most explainable deep learning applications use post hoc methods (such as GradCAM) to generate feature attribution maps, there is a new type of case-based reasoning models, namely ProtoPNet and its variants, which identify prototypes during training and compare input image patches with those prototypes. We propose the first medical prototype network (MProtoNet) to extend ProtoPNet to brain tumor classification with 3D multi-parametric magnetic resonance imaging (mpMRI) data. To address different requirements between 2D natural images and 3D mpMRIs especially in terms of localizing attention regions, a new attention module with soft masking and online-CAM loss is introduced. Soft masking helps sharpen attention maps, while online-CAM loss directly utilizes image-level labels when training the attention module. MProtoNet achieves statistically significant improvements in interpretability metrics of both correctness and localization coherence (with a best activation precision of $0.713\\pm0.058$) without human-annotated labels during training, when compared with GradCAM and several ProtoPNet variants. The source code is available at https://github.com/aywi/mprotonet."}}
{"id": "Ae1KAltzEd", "cdate": 1673287851359, "mdate": null, "content": {"title": "Prior Guided 3D Medical Image Landmark Localization", "abstract": "Accurate detection of 3D medical landmarks is critical for evaluating and characterizing anatomical features and performing preoperative planning. However, detecting 3D landmarks can be challenging due to the local structural homogeneity of medical images. In this study, we present a prior guided coarse-to-fine framework for efficient and accurate 3D medical landmark detection. Specifically, we utilize the prior knowledge that in specific settings, physicians often annotate multiple landmarks on a same slice. In the coarse stage, we perform coordinate regression on downsampled 3D images to maintain the structural relationships across different landmarks. In the fine stage, we categorize landmarks as independent and correlated landmarks based on their annotation prior. For each independent landmark, we train a single localization model to capture local features and deliver reliable local predictions. For correlated landmarks, we mimic the manual annotation process and propose a correlated landmark detection model that fuses information from various patches to query key slices and identify correlated landmarks. The proposed method is extensively evaluated on two datasets, exhibiting superior performance with an average detection error of respective 3.29 mm and 2.13 mm. "}}
{"id": "w0hnyLry__", "cdate": 1640995200000, "mdate": 1668090739957, "content": {"title": "Lesion2void: Unsupervised Anomaly Detection in Fundus Images", "abstract": "Anomalous data are usually rare in the field of medical imaging, in contrast to normal (healthy) data that account for the vast majority of the real-world medical image data, leading to challenges of developing image-based disease detection algorithms. In this work, we propose an unsupervised anomaly detection framework for diabetic retinopathy (DR) identification from fundus images, named Lesion2Void. Lesion2Void is capable of identifying anomalies in fundus images by only leveraging normal data without any additional annotation during training. We first randomly mask out multiple patches in normal fundus images. Then, a convolutional neural network is trained to reconstruct the corresponding complete images. We make a simple assumption that in a fundus image, lesion patches, if present, are independent of each other and are also independent of their neighboring pixels, whereas normal patches can be predicted based on the information from the neighborhood. Therefore, in the testing phase, an image can be identified as normal or abnormal by measuring the reconstruction errors of the erased patches. Extensive experiments are conducted on the publicly accessible dataset EyeQ, demonstrating the superiority of our proposed framework for DR-related anomaly detection in fundus images."}}
{"id": "ntJ4S1khmM2", "cdate": 1640995200000, "mdate": 1668090739959, "content": {"title": "DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal MRI Synthesis Network", "abstract": "Contrast-enhanced T1 (T1ce) is one of the most essential magnetic resonance imaging (MRI) modalities for diagnosing and analyzing brain tumors, especially gliomas. In clinical practice, common MRI modalities such as T1, T2, and fluid attenuation inversion recovery are relatively easy to access while T1ce is more challenging considering the additional cost and potential risk of allergies to the contrast agent. Therefore, it is of great clinical necessity to develop a method to synthesize T1ce from other common modalities. Current paired image translation methods typically have the issue of requiring a large amount of paired data and do not focus on specific regions of interest, e.g., the tumor region, in the synthesization process. To address these issues, we propose a Difficulty-perceived common-to-T1ce Semi-Supervised multimodal MRI Synthesis network (DS3-Net), involving both paired and unpaired data together with dual-level knowledge distillation. DS3-Net predicts a difficulty map to progressively promote the synthesis task. Specifically, a pixelwise constraint and a patchwise contrastive constraint are guided by the predicted difficulty map. Through extensive experiments on the publiclyavailable BraTS2020 dataset, DS3-Net outperforms its supervised counterpart in each respect. Furthermore, with only 5% paired data, the proposed DS3-Net achieves competitive performance with state-of-theart image translation methods utilizing 100% paired data, delivering an average SSIM of 0.8947 and an average PSNR of 23.60."}}
{"id": "mY2acJyW0ab", "cdate": 1640995200000, "mdate": 1668090739793, "content": {"title": "Superpixel Inpainting For Self-Supervised Skin Lesion Segmentation from Dermoscopic Images", "abstract": "Automated and accurate segmentation of skin lesions based on dermoscopic images is an important task in clinical practice. However, limited labeled images and noisy annotations make the skin lesion segmentation task challenging. In this work, we propose a superpixel inpainting based self-supervised pretraining method to enhance skin lesion segmentation, the effectiveness of which is identified both quantitatively and qualitatively on two public datasets. State-of-the-art performance on skin lesion segmentation is observed, with mean Jaccard indices of 76.5% and 84.3% being obtained respectively on the ISIC2017 and PH2 datasets."}}
{"id": "mXNAy4rwt_", "cdate": 1640995200000, "mdate": 1668090739794, "content": {"title": "AugPaste: One-Shot Anomaly Detection for Medical Images", "abstract": "Due to the high cost of manually annotating medical images, especially for large-scale datasets, anomaly detection has been explored through training models with only normal data. Lacking prior knowledge of true anomalies is the main reason for the limited application of previous anomaly detection methods, especially in the medical image analysis realm. In this work, we propose a one-shot anomaly detection framework, namely AugPaste, that utilizes true anomalies from a single annotated sample and synthesizes artificial anomalous samples for anomaly detection. First, a lesion bank is constructed by applying augmentation to randomly selected lesion patches. Then, MixUp is adopted to paste patches from the lesion bank at random positions in normal images to synthesize anomalous samples for training. Finally, a classification network is trained using the synthetic abnormal samples and the true normal data. Extensive experiments are conducted on two publicly-available medical image datasets with different types of abnormalities. On both datasets, our proposed AugPaste largely outperforms several state-of-the-art unsupervised and semi-supervised anomaly detection methods, and is on a par with the fully-supervised counterpart. To note, AugPaste is even better than the fully-supervised method in detecting early-stage diabetic retinopathy."}}
{"id": "lhNAuWFUoAn", "cdate": 1640995200000, "mdate": 1668090739805, "content": {"title": "Corolla: An Efficient Multi-Modality Fusion Framework with Supervised Contrastive Learning for Glaucoma Grading", "abstract": "Glaucoma is one of the ophthalmic diseases that may cause blindness, for which early detection and treatment are very important. Fundus images and optical coherence tomography (OCT) images are both widely-used modalities in diagnosing glaucoma. However, existing glaucoma grading approaches mainly utilize a single modality, ignoring the complementary information between fundus and OCT. In this paper, we propose an efficient multi-modality supervised contrastive learning framework, named COROLLA, for glaucoma grading. Through layer segmentation as well as thickness calculation and projection, retinal thickness maps are extracted from the original OCT volumes and used as a replacing modality, resulting in more efficient calculations with less memory usage. Given the high structure and distribution similarities across medical image samples, we employ supervised contrastive learning to increase our models\u2019 discriminative power with better convergence. Moreover, feature-level fusion of paired fundus image and thickness map is conducted for enhanced diagnosis accuracy. On the GAMMA dataset, our COROLLA framework achieves overwhelming glaucoma grading performance compared to state-of-the-art methods."}}
{"id": "jG8ZqvLq_L", "cdate": 1640995200000, "mdate": 1668090740050, "content": {"title": "LesionPaste: One-Shot Anomaly Detection for Medical Images", "abstract": "Due to the high cost of manually annotating medical images, especially for large-scale datasets, anomaly detection has been explored through training models with only normal data. Lacking prior knowledge of true anomalies is the main reason for the limited application of previous anomaly detection methods, especially in the medical image analysis realm. In this work, we propose a one-shot anomaly detection framework, namely LesionPaste, that utilizes true anomalies from a single annotated sample and synthesizes artificial anomalous samples for anomaly detection. First, a lesion bank is constructed by applying augmentation to randomly selected lesion patches. Then, MixUp is adopted to paste patches from the lesion bank at random positions in normal images to synthesize anomalous samples for training. Finally, a classification network is trained using the synthetic abnormal samples and the true normal data. Extensive experiments are conducted on two publicly-available medical image datasets with different types of abnormalities. On both datasets, our proposed LesionPaste largely outperforms several state-of-the-art unsupervised and semi-supervised anomaly detection methods, and is on a par with the fully-supervised counterpart. To note, LesionPaste is even better than the fully-supervised method in detecting early-stage diabetic retinopathy."}}
{"id": "hXX8cEQ4yn", "cdate": 1640995200000, "mdate": 1668090740051, "content": {"title": "AADG: Automatic Augmentation for Domain Generalization on Retinal Image Segmentation", "abstract": "Convolutional neural networks have been widely applied to medical image segmentation and have achieved considerable performance. However, the performance may be significantly affected by the domain gap between training data (source domain) and testing data (target domain). To address this issue, we propose a data manipulation based domain generalization method, called Automated Augmentation for Domain Generalization (AADG). Our AADG framework can effectively sample data augmentation policies that generate novel domains and diversify the training set from an appropriate search space. Specifically, we introduce a novel proxy task maximizing the diversity among multiple augmented novel domains as measured by the Sinkhorn distance in a unit sphere space, making automated augmentation tractable. Adversarial training and deep reinforcement learning are employed to efficiently search the objectives. Quantitative and qualitative experiments on 11 publicly-accessible fundus image datasets (four for retinal vessel segmentation, four for optic disc and cup (OD/OC) segmentation and three for retinal lesion segmentation) are comprehensively performed. Two OCTA datasets for retinal vasculature segmentation are further involved to validate cross-modality generalization. Our proposed AADG exhibits state-of-the-art generalization performance and outperforms existing approaches by considerable margins on retinal vessel, OD/OC and lesion segmentation tasks. The learned policies are empirically validated to be model-agnostic and can transfer well to other models. The source code is available at https://github.com/CRazorback/AADG."}}
{"id": "ebAOMmk6TjT", "cdate": 1640995200000, "mdate": 1668090739984, "content": {"title": "GAMMA Challenge: Glaucoma grAding from Multi-Modality imAges", "abstract": "Color fundus photography and Optical Coherence Tomography (OCT) are the two most cost-effective tools for glaucoma screening. Both two modalities of images have prominent biomarkers to indicate glaucoma suspected. Clinically, it is often recommended to take both of the screenings for a more accurate and reliable diagnosis. However, although numerous algorithms are proposed based on fundus images or OCT volumes in computer-aided diagnosis, there are still few methods leveraging both of the modalities for the glaucoma assessment. Inspired by the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held previously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA) Challenge to encourage the development of fundus \\& OCT-based glaucoma grading. The primary task of the challenge is to grade glaucoma from both the 2D fundus images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT volumes, which is the first multi-modality dataset for glaucoma grading. In addition, an evaluation framework is also established to evaluate the performance of the submitted methods. During the challenge, 1272 results were submitted, and finally, top-10 teams were selected to the final stage. We analysis their results and summarize their methods in the paper. Since all these teams submitted their source code in the challenge, a detailed ablation study is also conducted to verify the effectiveness of the particular modules proposed. We find many of the proposed techniques are practical for the clinical diagnosis of glaucoma. As the first in-depth study of fundus \\& OCT multi-modality glaucoma grading, we believe the GAMMA Challenge will be an essential starting point for future research."}}
