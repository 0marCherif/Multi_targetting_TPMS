{"id": "aWN9_ubqUK", "cdate": 1684123423351, "mdate": 1684123423351, "content": {"title": "Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering", "abstract": "Multi-view clustering, a long-standing and important research problem, focuses on mining complementary information from diverse views. However, existing works often fuse multiple views' representations or handle clustering in a common feature space, which may result in their entanglement especially for visual representations. To address this issue, we present a novel VAE-based multi-view clustering framework (Multi-VAE) by learning disentangled visual representations. Concretely, we define a view-common variable and multiple view-peculiar variables in the generative model. The prior of view-common variable obeys approximately discrete Gumbel Softmax distribution, which is introduced to extract the common cluster factor of multiple views. Meanwhile, the prior of view-peculiar variable follows continuous Gaussian distribution, which is used to represent each view's peculiar visual factors. By controlling the mutual information capacity to disentangle the view-common and view-peculiar representations, continuous visual information of multiple views can be separated so that their common discrete cluster information can be effectively mined. Experimental results demonstrate that Multi-VAE enjoys the disentangled and explainable visual representations, while obtaining superior clustering performance compared with state-of-the-art methods.\n"}}
{"id": "6C27usH9Tb", "cdate": 1675575953460, "mdate": 1675575953460, "content": {"title": "Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering", "abstract": "Multi-view clustering, a long-standing and important research problem, focuses on mining complementary information from diverse views. However, existing works often fuse multiple views\u2019 representations or handle clustering in a common feature space, which may result in their entanglement especially for visual representations. To address this issue, we present a novel VAE-based multi-view clustering framework (Multi-VAE) by learning disentangled visual representations. Concretely, we define a view-common variable and multiple view-peculiar variables in the generative model. The prior of view-common variable obeys approximately discrete Gumbel Softmax distribution, which is introduced to extract the common cluster factor of multiple views. Meanwhile, the prior of view-peculiar variable follows continuous Gaussian distribution, which is used to represent each view\u2019s peculiar visual factors. By controlling the mutual information capacity to disentangle the view-common and view-peculiar representations, continuous visual information of multiple views can be separated so that their common discrete cluster information can be effectively mined. Experimental results demonstrate that Multi-VAE enjoys the disentangled and explainable visual representations, while obtaining superior clustering performance compared with state-of-the-art methods."}}
{"id": "L9SfWuvAzh", "cdate": 1667354900813, "mdate": 1667354900813, "content": {"title": "Multi-level Feature Learning for Contrastive Multi-view Clustering", "abstract": "Multi-view clustering can explore common semantics from multiple views and has attracted increasing attention. However, existing works punish multiple objectives in the same feature space, where they ignore the conflict between learning consistent common semantics and reconstructing inconsistent view-private information. In this paper, we propose a new framework of multi-level feature learning for contrastive multi-view clustering to address the aforementioned issue. Our method learns different levels of features from the raw features, including low-level features, high-level features, and semantic labels/features in a fusion-free manner, so that it can effectively achieve the reconstruction objective and the consistency objectives in different feature spaces. Specifically, the reconstruction objective is conducted on the low-level features. Two consistency objectives based on contrastive learning are conducted on the high-level features and the semantic labels, respectively. They make the high-level features effectively explore the common semantics and the semantic labels achieve the multi-view clustering. As a result, the proposed framework can reduce the adverse influence of view-private information. Extensive experiments on public datasets demonstrate that our method achieves state-of-the-art clustering effectiveness."}}
{"id": "_636OzRD2zN", "cdate": 1649407753143, "mdate": 1649407753143, "content": {"title": "Interpretable learning based Dynamic Graph Convolutional Networks for Alzheimer\u2019s Disease analysis", "abstract": "Graph Convolutional Networks (GCNs) are widely applied in classification tasks by aggregating the neighborhood information of each sample to output robust node embedding. However, conventional GCN methods do not update the graph during the training process so that their effectiveness is always influenced by the quality of the input graph. Moreover, previous GCN methods lack the interpretability to limit their real applications. In this paper, a novel personalized diagnosis technique is proposed for early Alzheimer\u2019s Disease (AD) diagnosis via coupling interpretable feature learning with dynamic graph learning into the GCN architecture. Specifically, the module of interpretable feature learning selects informative features to provide interpretability for disease diagnosis and abandons redundant features to capture inherent correlation of data points. The module of dynamic graph learning adjusts the neighborhood relationship of every data point to output robust node embedding as well as the correlations of all data points to refine the classifier. The GCN module outputs diagnosis results based on the learned inherent graph structure. All three modules are jointly optimized to perform reliable disease diagnosis at an individual level. Experiments demonstrate that our method outputs competitive diagnosis performance as well as provide interpretability for personalized disease diagnosis."}}
{"id": "LVdZToP2TON", "cdate": 1640995200000, "mdate": 1649409931806, "content": {"title": "Interpretable learning based Dynamic Graph Convolutional Networks for Alzheimer's Disease analysis", "abstract": "Highlights \u2022 Interpretable learning is used to output interpretable diagnosis results. \u2022 Dynamic graph learning coupled with GCN to optimize graph structure. \u2022 GCN coupled with above learning are used to output personalized diagnosis. Abstract Graph Convolutional Networks (GCNs) are widely applied in classification tasks by aggregating the neighborhood information of each sample to output robust node embedding. However, conventional GCN methods do not update the graph during the training process so that their effectiveness is always influenced by the quality of the input graph. Moreover, previous GCN methods lack the interpretability to limit their real applications. In this paper, a novel personalized diagnosis technique is proposed for early Alzheimer\u2019s Disease (AD) diagnosis via coupling interpretable feature learning with dynamic graph learning into the GCN architecture. Specifically, the module of interpretable feature learning selects informative features to provide interpretability for disease diagnosis and abandons redundant features to capture inherent correlation of data points. The module of dynamic graph learning adjusts the neighborhood relationship of every data point to output robust node embedding as well as the correlations of all data points to refine the classifier. The GCN module outputs diagnosis results based on the learned inherent graph structure. All three modules are jointly optimized to perform reliable disease diagnosis at an individual level. Experiments demonstrate that our method outputs competitive diagnosis performance as well as provide interpretability for personalized disease diagnosis."}}
{"id": "so_j9sH40N", "cdate": 1609459200000, "mdate": 1649409931768, "content": {"title": "Video Representation Learning with Graph Contrastive Augmentation", "abstract": "Contrastive-based self-supervised learning for image representations has significantly closed the gap with supervised learning. A natural extension of image-based contrastive learning methods to the video domain is to fully exploit the temporal structure presented in videos. We propose a novel contrastive self-supervised video representation learning framework, termed Graph Contrastive Augmentation (GCA), by constructing a video temporal graph and devising a graph augmentation that is designed to enhance the correlation across frames of videos and developing a new view for exploring temporal structure in videos. Specifically, we construct the temporal graph in the video by leveraging the relational knowledge behind the correlated sequence video features. Afterwards, we apply the proposed graph augmentation to generate another graph view by cooperating random corruption of the original graph to enhance the diversity of the intrinsic structure of the temporal graph. To this end, we provide two different kinds of contrastive learning methods to train our framework using temporal relationships concealed in videos as self-supervised signals. We perform empirical experiments on downstream tasks, action recognition and video retrieval, using the learned video representation, and the results demonstrate that with the graph view of temporal structure, our proposed GCA remarkably improves performance against or on par with the recent methods."}}
{"id": "nBPOmWsmEk", "cdate": 1609459200000, "mdate": 1649409931623, "content": {"title": "Fusing functional connectivity with network nodal information for sparse network pattern learning of functional brain networks", "abstract": "Highlights \u2022 Learning network patterns with connectivity and node information. \u2022 Adopting different models to characterize different information. \u2022 Fusing two heterogeneous information in a unified framework. Abstract Sparse learning methods have been powerful tools for learning compact representations of functional brain networks consisting of a set of brain network nodes and a connectivity matrix measuring functional coherence between the nodes. However, these tools typically focus on the functional connectivity measures alone, ignoring the brain network nodal information that is complementary to the functional connectivity measures for comprehensively characterizing the functional brain networks. In order to provide a comprehensive delineation of the functional brain networks, we develop a new data fusion method for heterogeneous data, aiming at learning sparse network patterns to characterize both the functional connectivity measures and their complementary network nodal information within a unified framework. Experimental results have demonstrated that our method outperforms the best alternative method under comparison in terms of accuracy on simulated data as well as both reproducibility and prediction performance of brain age on real resting state functional magnetic resonance imaging data."}}
{"id": "ga-mh-kmuLB", "cdate": 1609459200000, "mdate": 1649409931631, "content": {"title": "Multi-Band Brain Network Analysis for Functional Neuroimaging Biomarker Identification", "abstract": "The functional connectomic profile is one of the non-invasive imaging biomarkers in the computer-assisted diagnostic system for many neuro-diseases. However, the diagnostic power of functional connectivity is challenged by mixed frequency-specific neuronal oscillations in the brain, which makes the single Functional Connectivity Network (FCN) often underpowered to capture the disease-related functional patterns. To address this challenge, we propose a novel functional connectivity analysis framework to conduct joint feature learning and personalized disease diagnosis, in a semi-supervised manner, aiming at focusing on putative multi-band functional connectivity biomarkers from functional neuroimaging data. Specifically, we first decompose the Blood Oxygenation Level Dependent (BOLD) signals into multiple frequency bands by the discrete wavelet transform, and then cast the alignment of all fully-connected FCNs derived from multiple frequency bands into a parameter-free multi-band fusion model. The proposed fusion model fuses all fully-connected FCNs to obtain a sparsely-connected FCN (sparse FCN for short) for each individual subject, as well as lets each sparse FCN be close to its neighbored sparse FCNs and be far away from its furthest sparse FCNs. Furthermore, we employ the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _{{1}}$ </tex-math></inline-formula> -SVM to conduct joint brain region selection and disease diagnosis. Finally, we evaluate the effectiveness of our proposed framework on various neuro-diseases, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i.e.,</i> Fronto-Temporal Dementia (FTD), Obsessive-Compulsive Disorder (OCD), and Alzheimer\u2019s Disease (AD), and the experimental results demonstrate that our framework shows more reasonable results, compared to state-of-the-art methods, in terms of classification performance and the selected brain regions. The source code can be visited by the url <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/reynard-hu/mbbna</uri> ."}}
{"id": "Ux4GwcfKBy", "cdate": 1609459200000, "mdate": 1649409931672, "content": {"title": "Joint hub identification for brain networks by multivariate graph inference", "abstract": "Highlights \u2022 A novel multivariate hub identification method to jointly find a set of critical connector hub nodes in the network. \u2022 An extension of population-wise hub identification was also proposed to identify a set of common hubs from a group of networks across different populations or longitudinal time points. \u2022 Experiments based on both structural and functional data demonstrated the population-wise hub identification has a more power on distinguishing network alterations related to disorders such as Alzheimer's disease and obsessive-compulsive disorder. Abstract Recent developments in neuroimaging allow us to investigate the structural and functional connectivity between brain regions in vivo. Mounting evidence suggests that hub nodes play a central role in brain communication and neural integration. Such high centrality, however, makes hub nodes particularly susceptible to pathological network alterations and the identification of hub nodes from brain networks has attracted much attention in neuroimaging. Current popular hub identification methods often work in a univariate manner, i.e., selecting the hub nodes one after another based on either heuristic of the connectivity profile at each node or predefined settings of network modules. Since the topological information of the entire network (such as network modules) is not fully utilized, current methods have limited power to identify hubs that link multiple modules (connector hubs) and are biased toward identifying hubs having many connections within the same module (provincial hubs). To address this challenge, we propose a novel multivariate hub identification method. Our method identifies connector hubs as those that partition the network into disconnected components when they are removed from the network. Furthermore, we extend our hub identification method to find the population-based hub nodes from a group of network data. We have compared our hub identification method with existing methods on both simulated and human brain network data. Our proposed method achieves more accurate and replicable discovery of hub nodes and exhibits enhanced statistical power in identifying network alterations related to neurological disorders such as Alzheimer's disease and obsessive-compulsive disorder."}}
{"id": "TncYsmeOnik", "cdate": 1609459200000, "mdate": 1649409931805, "content": {"title": "Vision-guided Music Source Separation via a Fine-grained Cycle-Separation Network", "abstract": "Music source separation from a sound mixture remains a big challenge because there often exist heavy overlaps and interactions among similar music signals. In order to correctly separate mixed sources, we propose a novel Fine-grained Cycle-Separation Network (FCSN) for vision-guided music source separation. With the guidance of visual features, the proposed FCSN approach preliminarily separated music sources by minimizing the residual spectrogram which is calculated by removing preliminarily separated music spectrograms from the original music mixture. The separation is repeated several times until the residual spectrogram becomes empty or leaves only noise. Extensive experiments are performed on three large-scale datasets, the MUSIC (MUSIC-21), the AudioSet, and the VGGSound. Our approach outperforms state-of-the-art approaches in all datasets, and both separation accuracies and visualization results demonstrate its effectiveness for solving the problem of overlap and interaction in music source separation."}}
