{"id": "fLOU5jXlJZV", "cdate": 1652737460242, "mdate": null, "content": {"title": "Differentially Private Online-to-batch for Smooth Losses", "abstract": "  We develop a new reduction that converts any online convex optimization algorithm suffering $O(\\sqrt{T})$ regret into an $\\epsilon$-differentially private stochastic convex optimization algorithm with the optimal convergence rate $\\tilde O(1/\\sqrt{T} + 1/\\epsilon T)$ on smooth losses in linear time, forming a direct analogy to the classical non-private ``online-to-batch'' conversion. By applying our techniques to more advanced adaptive online algorithms, we produce adaptive differentially private counterparts whose convergence rates depend on apriori unknown variances or parameter norms."}}
{"id": "skiUX7b8IP", "cdate": 1640995200000, "mdate": 1682353750678, "content": {"title": "CRACAU: Byzantine Machine Learning Meets Industrial Edge Computing in Industry 5.0", "abstract": "Industry 5.0 is emerging as a result of the advancement in networking and communication technologies, artificial intelligence, distributed computing, and beyond 5G. Among the important enabling technologies, federated learning, industrial edge computing, and Byzantine-tolerant machine learning (ML) are key accelerators in Industry 5.0. We propose a framework to integrate these key components. Recent works have designed various Byzantine-tolerant ML algorithms for a datacenter or a cluster. However, these algorithms are difficult to be applied to industrial edge computing paradigms. In this article, a novel Byzantine-tolerant federated learning algorithm, CRACAU, is designed for the popular three-level edge computing architecture. In this algorithm, edge devices jointly learn an ML model using the data collected at each device, and their private data are never shared with others. Under standard assumptions, we formally prove that CRACAU converges to the optimal point, i.e., CRACAU finds the optimal parameters of the ML model. We also implement CRACAU in the MXNet framework and evaluate it on the popular benchmark MNIST and CIFAR-10 image classification datasets. Experimental results show that CRACAU achieves satisfying accuracy."}}
{"id": "TbCXcu7uyC", "cdate": 1640995200000, "mdate": 1682353750709, "content": {"title": "Brief Announcement: Computability and Anonymous Storage-Efficient Consensus with an Abstract MAC Layer", "abstract": "This paper explores fault-tolerant algorithms in the abstract MAC layer [7] in a single-hop network. The model captures the basic properties of modern wireless MAC protocols. Newport [11] proves that it is impossible to achieve deterministic fault-tolerant consensus, and Newport and Robinson [10] present randomized crash-tolerant consensus algorithms. We are not aware of any study on the computability in this model. This paper presents a straightforward construction of the store-collect object, which then allows us to apply prior algorithms to solve many well-known distributed problems, such as multi-writer atomic registers, counters, atomic snapshot objects, and approximate and randomized consensus. Our construction does not require a priori information on the participating nodes. Our insight also leads to anonymous approximate consensus and randomized consensus algorithms that use a constant number of variables (or values). All of the algorithms are wait-free."}}
{"id": "4cYAUFTELa", "cdate": 1640995200000, "mdate": 1682353750742, "content": {"title": "Differentially Private Online-to-Batch for Smooth Losses", "abstract": "We develop a new reduction that converts any online convex optimization algorithm suffering $O(\\sqrt{T})$ regret into an $\\epsilon$-differentially private stochastic convex optimization algorithm with the optimal convergence rate $\\tilde O(1/\\sqrt{T} + \\sqrt{d}/\\epsilon T)$ on smooth losses in linear time, forming a direct analogy to the classical non-private \"online-to-batch\" conversion. By applying our techniques to more advanced adaptive online algorithms, we produce adaptive differentially private counterparts whose convergence rates depend on apriori unknown variances or parameter norms."}}
{"id": "zjVAI5xYUa3", "cdate": 1609459200000, "mdate": 1682353750674, "content": {"title": "Practical approximate consensus algorithms for small devices in lossy networks", "abstract": "This paper studies a fundamental distributed primitive - approximate consensus - in connected things using wireless networks. It has been extensively studied in different disciplines, such as fault-tolerant computing, distributed computing, control, and robotics communities. To our surprise, we have not found any practical algorithm that is appropriate for our target scenario - a system of small things that have limited computation and storage capability, and use lossy wireless links to communicate with each other. This work first identifies the limitations of prior algorithms. Then we present two fault-tolerant approximate consensus algorithms, which minimizes storage complexity. In an n-node system, the first algorithm tolerates crash faults and only needs to store 4 values and an n-bit bit-vector, whereas the second algorithm tolerates up to f Byzantine faults, and needs to store 2f + 4 values and an n-bit bit-vector. In practice, these values are implemented using either integer or float data type. Compared to our algorithms, prior works may need to store O(nD) more values, where D is the number of rounds needed for convergence."}}
{"id": "Pn5MA8Fj7vw", "cdate": 1577836800000, "mdate": 1682353750675, "content": {"title": "Echo-CGC: A Communication-Efficient Byzantine-tolerant Distributed Machine Learning Algorithm in Single-Hop Radio Network", "abstract": "In this paper, we focus on a popular DML framework -- the parameter server computation paradigm and iterative learning algorithms that proceed in rounds. We aim to reduce the communication complexity of Byzantine-tolerant DML algorithms in the single-hop radio network. Inspired by the CGC filter developed by Gupta and Vaidya, PODC 2020, we propose a gradient descent-based algorithm, Echo-CGC. Our main novelty is a mechanism to utilize the broadcast properties of the radio network to avoid transmitting the raw gradients (full $d$-dimensional vectors). In the radio network, each worker is able to overhear previous gradients that were transmitted to the parameter server. Roughly speaking, in Echo-CGC, if a worker \"agrees\" with a combination of prior gradients, it will broadcast the \"echo message\" instead of the its raw local gradient. The echo message contains a vector of coefficients (of size at most $n$) and the ratio of the magnitude between two gradients (a float). In comparison, the traditional approaches need to send $n$ local gradients in each round, where each gradient is typically a vector in an ultra-high dimensional space ($d\\gg n$). The improvement on communication complexity of our algorithm depends on multiple factors, including number of nodes, number of faulty workers in an execution, and the cost function. We numerically analyze the improvement, and show that with a large number of nodes, Echo-CGC reduces $80\\%$ of the communication under standard assumptions."}}
{"id": "KwvD0aQ2jH", "cdate": 1577836800000, "mdate": 1682353750689, "content": {"title": "Brief Announcement: Reaching Approximate Consensus When Everyone May Crash", "abstract": "Fault-tolerant consensus is of great importance in distributed systems. This paper studies the asynchronous approximate consensus problem in the crash-recovery model with fair-loss links. In our model, up to f nodes may crash forever, while the rest may crash intermittently. Each node is equipped with a limited-size persistent storage that does not lose data when crashed. We present an algorithm that only stores three values in persistent storage - state, phase index, and a counter."}}
{"id": "JP3m56f1Wei", "cdate": 1577836800000, "mdate": 1682353750703, "content": {"title": "Echo-CGC: A Communication-Efficient Byzantine-Tolerant Distributed Machine Learning Algorithm in Single-Hop Radio Network", "abstract": "In the past few years, many Byzantine-tolerant distributed machine learning (DML) algorithms have been proposed in the point-to-point communication model. In this paper, we focus on a popular DML framework - the parameter server computation paradigm and iterative learning algorithms that proceed in rounds, e.g., [Gupta and Vaidya, 2020; El-Mhamdi et al., 2020; Chen et al., 2017]. One limitation of prior algorithms in this domain is the high communication complexity. All the Byzantine-tolerant DML algorithms that we are aware of need to send n d-dimensional vectors from worker nodes to the parameter server in each round, where n is the number of workers and d is the number of dimensions of the feature space (which may be in the order of millions). In a wireless network, power consumption is proportional to the number of bits transmitted. Consequently, it is extremely difficult, if not impossible, to deploy these algorithms in power-limited wireless devices. Motivated by this observation, we aim to reduce the communication complexity of Byzantine-tolerant DML algorithms in the single-hop radio network [Alistarh et al., 2010; Bhandari and Vaidya, 2005; Koo, 2004]. Inspired by the CGC filter developed by Gupta and Vaidya, PODC 2020 [Gupta and Vaidya, 2020], we propose a gradient descent-based algorithm, Echo-CGC. Our main novelty is a mechanism to utilize the broadcast properties of the radio network to avoid transmitting the raw gradients (full d-dimensional vectors). In the radio network, each worker is able to overhear previous gradients that were transmitted to the parameter server. Roughly speaking, in Echo-CGC, if a worker \"agrees\" with a combination of prior gradients, it will broadcast the \"echo message\" instead of the its raw local gradient. The echo message contains a vector of coefficients (of size at most n) and the ratio of the magnitude between two gradients (a float). In comparison, the traditional approaches need to send n local gradients in each round, where each gradient is typically a vector in a ultra-high dimensional space (d \u226b n). The improvement on communication complexity of our algorithm depends on multiple factors, including number of nodes, number of faulty workers in an execution, and the cost function. We numerically analyze the improvement, and show that with a large number of nodes, Echo-CGC reduces 80% of the communication under standard assumptions."}}
{"id": "8NiGYKORVu", "cdate": 1577836800000, "mdate": 1682353750674, "content": {"title": "Exact Consensus under Global Asymmetric Byzantine Links", "abstract": "Fault-tolerant distributed consensus is an important primitive in many large-scale distributed systems and applications. The consensus problem has been investigated under various fault models in the literature since the seminal work by Lamport et al. in 1982. In this paper, we study the exact consensus problem in a new faulty link model, namely global asymmetric Byzantine (GAB) link model. Our link-fault model is simple, yet to our surprise, not studied before. In our system, all the nodes are fault-free and each pair of nodes can communicate directly with each other. In the GAB link model, up to f directed links may become Byzantine, and have arbitrary behavior. Non-faulty links deliver messages reliably. In our model, it is possible that the link from node a to node b is faulty, but the link from node b to node a is fault-free. Unlike some prior models with a local constraint, which enforced a local upper bound on the number of failure links attached to each node, we adopt the global constraint, which allows any link to be corrupted in the GAB model. These global and asymmetric features distinguish our model from all prior faulty link models. In our GAB model, we study the consensus problem in both synchronous and asynchronous systems. We show that 2f + 1 nodes is both necessary and sufficient for solving synchronous consensus, whereas 2f+2 nodes is the tight condition on resilience for solving asynchronous consensus. We also study the models where faulty links are mobile (or transient), i.e., the set of faulty links might change from round to round. We show that 2f + 3 nodes is necessary and sufficient for a family of algorithms that update local state in an iterative fashion."}}
