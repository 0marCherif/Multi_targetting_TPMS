{"id": "HP-k7dfqo9G", "cdate": 1675209600000, "mdate": 1680005370607, "content": {"title": "DS-Net: A dedicated approach for collapsed building detection from post-event airborne point clouds", "abstract": ""}}
{"id": "lOqBsFVgOs", "cdate": 1672531200000, "mdate": 1682319261630, "content": {"title": "Optimizing Local Feature Representations of 3D Point Clouds with Anisotropic Edge Modeling", "abstract": "An edge between two points describes rich information about the underlying surface. However, recent works merely use edge information as an ad hoc feature, which may undermine its effectiveness. In this study, we propose the Anisotropic Edge Modeling (AEM) block by which edges are modeled adaptively. As a result, the local feature representation is optimized where edges (e.g., object boundaries defined by ground truth) are appropriately enhanced. By stacking AEM blocks, AEM-Nets are constructed to tackle various point cloud understanding tasks. Extensive experiments demonstrate that AEM-Nets compare favorably to recent strong networks. In particular, AEM-Nets achieve state-of-the-art performance in object classification on ScanObjectNN, object segmentation on ShapeNet Part, and scene segmentation on S3DIS. Moreover, it is verified that AEM-Net outperforms the strong transformer-based method with significantly fewer parameters and FLOPs, achieving efficient learning. Qualitatively, the intuitive visualization of learned features successfully validates the effect of the AEM block."}}
{"id": "uhzZUQSqvOi", "cdate": 1640995200000, "mdate": 1680005370636, "content": {"title": "Efficient stereo matching on embedded GPUs with zero-means cross correlation", "abstract": ""}}
{"id": "OiZ9qjsDepC", "cdate": 1640995200000, "mdate": 1668599581837, "content": {"title": "Spatio-Temporal Perturbations for Video Attribution", "abstract": "The attribution method provides a direction for interpreting opaque neural networks in a visual way by identifying and visualizing the input regions/pixels that dominate the output of a network. Regarding the attribution method for visually explaining video understanding networks, it is challenging because of the unique spatiotemporal dependencies existing in video inputs and the special 3D convolutional or recurrent structures of video understanding networks. However, most existing attribution methods focus on explaining networks taking a single image as input and a few works specifically devised for video attribution come short of dealing with diversified structures of video understanding networks. In this paper, we investigate a generic perturbation-based attribution method that is compatible with diversified video understanding networks. Besides, we propose a novel regularization term to enhance the method by constraining the smoothness of its attribution results in both spatial and temporal dimensions. In order to assess the effectiveness of different video attribution methods without relying on manual judgement, we introduce reliable objective metrics which are checked by a newly proposed reliability measurement. We verified the effectiveness of our method by both subjective and objective evaluation and comparison with multiple significant attribution methods."}}
{"id": "Mpswyk-2TA6", "cdate": 1640995200000, "mdate": 1680005370742, "content": {"title": "Weakly Supervised Point Cloud Upsampling VIA Optimal Transport", "abstract": ""}}
{"id": "J3k-Gzpj2vh", "cdate": 1609459200000, "mdate": 1648712701348, "content": {"title": "Delivery of omnidirectional video using saliency prediction and optimal bitrate allocation", "abstract": "In this work, we propose and investigate a user-centric framework for the delivery of omnidirectional video (ODV) on VR systems by taking advantage of visual attention (saliency) models for bitrate allocation module. For this purpose, we formulate a new bitrate allocation algorithm that takes saliency map and nonlinear sphere-to-plane mapping into account for each ODV and solve the formulated problem using linear integer programming. For visual attention models, we use both image- and video-based saliency prediction results; moreover, we explore two types of attention model approaches: (i) salient object detection with transfer learning using pre-trained networks, (ii) saliency prediction with supervised networks trained on eye-fixation dataset. Experimental evaluations on saliency integration of models are discussed with interesting findings on transfer learning and supervised saliency approaches."}}
{"id": "HD7wEyJAgW_", "cdate": 1609459200000, "mdate": 1648712701351, "content": {"title": "Towards Visually Explaining Video Understanding Networks with Perturbation", "abstract": "Making black box models explainable \" is a vital problem that accompanies the development of deep learning networks. For networks taking visual information as input, one basic but challenging explanation method is to identify and visualize the input pixels/regions that dominate the network's prediction. However, most existing works focus on explaining networks taking a single image as input and do not consider the temporal relationship that exists in videos. Providing an easy-to-use visual explanation method that is applicable to diversified structures of video understanding networks still remains an open challenge. In this paper, we investigate a generic perturbation-based method for visually explaining video understanding networks. Besides, we propose a novel loss function to enhance the method by constraining the smoothness of its results in both spatial and temporal dimensions. The method enables the comparison of explanation results between different network structures to become possible and can also avoid generating the pathological adversarial explanations for video inputs. Experimental comparison results verified the effectiveness of our method."}}
{"id": "p8L2iOg5CWv", "cdate": 1577836800000, "mdate": 1648712701321, "content": {"title": "Z2-ZNCC: ZigZag Scanning based Zero-means Normalized Cross Correlation for Fast and Accurate Stereo Matching on Embedded GPU", "abstract": "Mobile stereo matching systems are becoming more important in many applications such as auto-driving and autonomous robots. However, to maintain its low power consumption, mobile platforms have only limited hardware resources. Accurate stereo matching methods require a high computational complexity, and it is difficult to maintain both acceptable accuracy and processing speed on the mobile platforms. To solve this trade-off, in this paper, we propose a novel acceleration approach for a well-known matching algorithm Zero-means Normalized Cross Correlation (ZNCC), and show its effectiveness on a Jetson TX2 embedded GPU. By combining our new approach, Z <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> - ZNCC, with the Semi-Global Matching (SGM) algorithm, our system achieves a low error rate of 7.76% while keeping 28 fps for 1242\u00d7375 pixels images with the maximum disparity of 128 on the KITTI 2015 dataset. This performance is higher than previous state-of-the-art system on the same hardware platform."}}
{"id": "bDezyt5lPg4", "cdate": 1577836800000, "mdate": 1648712701558, "content": {"title": "SOIC: Semantic Online Initialization and Calibration for LiDAR and Camera", "abstract": "This paper presents a novel semantic-based online extrinsic calibration approach, SOIC (so, I see), for Light Detection and Ranging (LiDAR) and camera sensors. Previous online calibration methods usually need prior knowledge of rough initial values for optimization. The proposed approach removes this limitation by converting the initialization problem to a Perspective-n-Point (PnP) problem with the introduction of semantic centroids (SCs). The closed-form solution of this PnP problem has been well researched and can be found with existing PnP methods. Since the semantic centroid of the point cloud usually does not accurately match with that of the corresponding image, the accuracy of parameters are not improved even after a nonlinear refinement process. Thus, a cost function based on the constraint of the correspondence between semantic elements from both point cloud and image data is formulated. Subsequently, optimal extrinsic parameters are estimated by minimizing the cost function. We evaluate the proposed method either with GT or predicted semantics on KITTI dataset. Experimental results and comparisons with the baseline method verify the feasibility of the initialization strategy and the accuracy of the calibration approach. In addition, we release the source code at https://github.com/--/SOIC."}}
{"id": "Pp-GI0xdy8w", "cdate": 1577836800000, "mdate": 1693150866385, "content": {"title": "Densification of Airborne Lidar Point Cloud with Fused Encoder-Decoder Networks", "abstract": "This paper presents a density enhancement method for airborne LiDAR point cloud with the corresponding image based on a fused Encoder-Decoder network. Different from terrestrial indoor or outdoor scenes, the variance of objects and depth ranges in the large scale airborne data is challenging. To address the problem of objects at different scales, we propose a RGB and depth fused Encoder-Decoder structure inspired by UNet. In addition, we propose a heuristic method for refining the result if instance segmentation labels are available. Both quantitative and qualitative evaluations are performed on a dataset covering 24km <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> area of Osaka in Japan validates the feasibility of the proposed method for densification of point cloud in large scale environment."}}
