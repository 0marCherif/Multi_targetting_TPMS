{"id": "zevIvF092Dc", "cdate": 1672531200000, "mdate": 1681415841031, "content": {"title": "Can AI-Generated Text be Reliably Detected?", "abstract": ""}}
{"id": "NA9sv1iku8", "cdate": 1672531200000, "mdate": 1681415841029, "content": {"title": "Temporal Robustness against Data Poisoning", "abstract": ""}}
{"id": "PYnSpt3jAz", "cdate": 1652737265117, "mdate": null, "content": {"title": "Lethal Dose Conjecture on Data Poisoning", "abstract": "Data poisoning considers an adversary that distorts the training set of machine learning algorithms for malicious purposes. In this work, we bring to light one conjecture regarding the fundamentals of data poisoning, which we call the Lethal Dose Conjecture. The conjecture states: If $n$ clean training samples are needed for accurate predictions, then in a size-$N$ training set, only $\\Theta(N/n)$ poisoned samples can be tolerated while ensuring accuracy. Theoretically, we verify this conjecture in multiple cases. We also offer a more general perspective of this conjecture through distribution discrimination. Deep Partition Aggregation (DPA) and its extension, Finite Aggregation (FA) are recent approaches for provable defenses against data poisoning, where they predict through the majority vote of many base models trained from different subsets of training set using a given learner. The conjecture implies that both DPA and FA are (asymptotically) optimal---if we have the most data-efficient learner, they can turn it into one of the most robust defenses against data poisoning. This outlines a practical approach to developing stronger defenses against poisoning via finding data-efficient learners. Empirically, as a proof of concept, we show that by simply using different data augmentations for base learners, we can respectively double and triple the certified robustness of DPA on CIFAR-10 and GTSRB without sacrificing accuracy. "}}
{"id": "sonBnCOori7", "cdate": 1640995200000, "mdate": 1681415841032, "content": {"title": "Lethal Dose Conjecture on Data Poisoning", "abstract": ""}}
{"id": "Sf0hghjC2F7", "cdate": 1640995200000, "mdate": 1648674888985, "content": {"title": "Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation", "abstract": "Data poisoning attacks aim at manipulating model behaviors through distorting training data. Previously, an aggregation-based certified defense, Deep Partition Aggregation (DPA), was proposed to mitigate this threat. DPA predicts through an aggregation of base classifiers trained on disjoint subsets of data, thus restricting its sensitivity to dataset distortions. In this work, we propose an improved certified defense against general poisoning attacks, namely Finite Aggregation. In contrast to DPA, which directly splits the training set into disjoint subsets, our method first splits the training set into smaller disjoint subsets and then combines duplicates of them to build larger (but not disjoint) subsets for training base classifiers. This reduces the worst-case impacts of poison samples and thus improves certified robustness bounds. In addition, we offer an alternative view of our method, bridging the designs of deterministic and stochastic aggregation-based certified defenses. Empirically, our proposed Finite Aggregation consistently improves certificates on MNIST, CIFAR-10, and GTSRB, boosting certified fractions by up to 3.05%, 3.87% and 4.77%, respectively, while keeping the same clean accuracies as DPA's, effectively establishing a new state of the art in (pointwise) certified robustness against data poisoning."}}
{"id": "IPRD0QyCvJ", "cdate": 1640995200000, "mdate": 1681415841031, "content": {"title": "Spuriosity Rankings: Sorting Data for Spurious Correlation Robustness", "abstract": ""}}
{"id": "3b436mlA2P", "cdate": 1640995200000, "mdate": 1681415841031, "content": {"title": "Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation", "abstract": ""}}
{"id": "sPVj1X1KiQ1", "cdate": 1609459200000, "mdate": 1648674888984, "content": {"title": "On Feature Decorrelation in Self-Supervised Learning", "abstract": "In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight."}}
{"id": "s0qOL_Koe6q", "cdate": 1609459200000, "mdate": 1635126471985, "content": {"title": "DPlis: Boosting Utility of Differentially Private Deep Learning via Randomized Smoothing", "abstract": "Deep learning techniques have achieved remarkable performance in wide-ranging tasks. However, when trained on privacy-sensitive datasets, the model parameters may expose private information in training data. Prior attempts for differentially private training, although offering rigorous privacy guarantees, lead to much lower model performance than the non-private ones. Besides, different runs of the same training algorithm produce models with large performance variance. To address these issues, we propose DPlis--Differentially Private Learning wIth Smoothing. The core idea of DPlis is to construct a smooth loss function that favors noise-resilient models lying in large flat regions of the loss landscape. We provide theoretical justification for the utility improvements of DPlis. Extensive experiments also demonstrate that DPlis can effectively boost model quality and training stability under a given privacy budget."}}
{"id": "m7TQJU1Fi2S", "cdate": 1609459200000, "mdate": 1635126471973, "content": {"title": "On Feature Decorrelation in Self-Supervised Learning", "abstract": "In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight."}}
