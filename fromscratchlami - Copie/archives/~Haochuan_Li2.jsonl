{"id": "JLweqJeqhSq", "cdate": 1652737827635, "mdate": null, "content": {"title": "Tight Analysis of Extra-gradient and Optimistic Gradient Methods For Nonconvex Minimax Problems", "abstract": "Despite the established convergence theory of Optimistic Gradient Descent Ascent (OGDA) and Extragradient (EG) methods for the convex-concave minimax problems, little is known about the theoretical guarantees of these methods in nonconvex settings. To bridge this gap, for the first time, this paper establishes the convergence of OGDA and EG methods under the nonconvex-strongly-concave (NC-SC) and nonconvex-concave (NC-C) settings by providing a unified analysis through the lens of single-call extra-gradient methods. We further establish lower bounds on the convergence of GDA/OGDA/EG, shedding light on the tightness of our analysis. We also conduct experiments supporting our theoretical results. We believe our results will advance the theoretical understanding of OGDA and EG methods for solving complicated nonconvex minimax real-world problems, e.g., Generative Adversarial Networks (GANs) or robust neural networks training."}}
{"id": "Kug2s3rHiG3", "cdate": 1621630326728, "mdate": null, "content": {"title": "Complexity Lower Bounds for Nonconvex-Strongly-Concave Min-Max Optimization", "abstract": "We provide a first-order oracle complexity lower bound for finding stationary points of min-max optimization problems where the objective function is smooth, nonconvex in the minimization variable, and strongly concave in the maximization variable. We establish a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2}\\right)$ for deterministic oracles, where $\\epsilon$ defines the level of approximate stationarity and $\\kappa$ is the condition number. Our lower bound matches the best existing upper bound in the $\\epsilon$ and $\\kappa$ dependence up to logarithmic factors. For stochastic oracles, we provide a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2} + \\kappa^{1/3}\\epsilon^{-4}\\right)$. It suggests that there is a gap between the best existing upper bound $\\mathcal{O}(\\kappa^3 \\epsilon^{-4})$ and our lower bound in the condition number dependence. "}}
{"id": "Hk45JiWuZB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Gradient Descent Finds Global Minima of Deep Neural Networks", "abstract": "Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in po..."}}
