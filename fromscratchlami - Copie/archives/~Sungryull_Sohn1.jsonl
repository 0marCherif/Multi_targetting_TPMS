{"id": "dYjH8Nv81K", "cdate": 1676591081113, "mdate": null, "content": {"title": "Unsupervised Object Interaction Learning with Counterfactual Dynamics Models", "abstract": "We present COIL (Counterfactual Object Interaction Learning), a novel way of learning skills of object interactions on entity-centric environments. The goal is to learn primitive behaviors that can control objects and induce their interactions without external reward or supervision being used. Existing skill discovery methods are limited to locomotion, simple navigation tasks, or single-object manipulation tasks, mostly not inducing useful behaviors of inducing interaction between objects. Unlike a monolithic representation usually used in prior skill learning methods, we propose to use a structured goal representation that can query and scope which objects to interact with, which can serve a basis for solving more complex downstream tasks. We design a novel counterfactual intrinsic reward through an use of either forward model or successor features that can learn an interaction skill between a pair of objects given as a goal. Through experiments on continuous control environments such as Magnetic Block and 2.5-D Stacking Box, we demonstrate that an agent can learn object interaction behaviors (e.g., attaching or stacking one block to another) without any external rewards or domain-specific knowledge."}}
{"id": "3hTpRkGohE", "cdate": 1675715128384, "mdate": null, "content": {"title": "A Picture is Worth a Thousand Words: Language Models Plan from Pixels", "abstract": "Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text by a captioning model, reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that the PLM can accurately plan even when observations are directly encoded as input prompts for the PLM. We show this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks."}}
{"id": "zJhWyDcmgNs", "cdate": 1675715127817, "mdate": null, "content": {"title": "Multimodal Subtask Graph Generation from Instructional Videos", "abstract": "Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty pan needs to be washed before cooking). In this work, we aim to model the causal dependencies between such subtasks from instructional videos describing the task. This is a challenging problem since complete information about the world is often inaccessible from videos, which demands robust learning mechanisms to understand the causal structure of events. We present Multimodal Subtask Graph Generation (MSG$^2$), an approach that constructs a Subtask Graph defining the dependency between a task\u2019s subtasks relevant to a task from noisy web videos. Graphs generated by our multimodal approach are closer to human-annotated graphs compared to prior approaches. MSG$^2$ further performs the downstream task of next subtask prediction 85% and 30% more accurately than recent video transformer models in the ProceL and CrossTask datasets, respectively."}}
{"id": "y1mN9HyvKmy", "cdate": 1672531200000, "mdate": 1682342241252, "content": {"title": "Unsupervised Task Graph Generation from Instructional Video Transcripts", "abstract": "This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner. We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets."}}
{"id": "s6jg-jdmnh", "cdate": 1672531200000, "mdate": 1682342241259, "content": {"title": "Multimodal Subtask Graph Generation from Instructional Videos", "abstract": "Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty pan needs to be washed before it can be used for cooking). In this work, we aim to model the causal dependencies between such subtasks from instructional videos describing the task. This is a challenging problem since complete information about the world is often inaccessible from videos, which demands robust learning mechanisms to understand the causal structure of events. We present Multimodal Subtask Graph Generation (MSG2), an approach that constructs a Subtask Graph defining the dependency between a task's subtasks relevant to a task from noisy web videos. Graphs generated by our multimodal approach are closer to human-annotated graphs compared to prior approaches. MSG2 further performs the downstream task of next subtask prediction 85% and 30% more accurately than recent video transformer models in the ProceL and CrossTask datasets, respectively."}}
{"id": "BbYhcj7yMo", "cdate": 1672531200000, "mdate": 1682342241171, "content": {"title": "A Picture is Worth a Thousand Words: Language Models Plan from Pixels", "abstract": "Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text (e.g., provided by a captioning model), reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM. We show that this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks."}}
{"id": "TpYf4EumAi", "cdate": 1664816295267, "mdate": null, "content": {"title": "ReSPack: A Large-Scale Rectilinear Steiner Tree Packing Data Generator and Benchmark", "abstract": "Combinatorial optimization (CO) has been studied as a useful tool for modeling industrial problems, but it still remains a challenge in complex domains because of the NP-hardness. With recent advances in machine learning, the field of CO is shifting to the study of neural combinatorial optimization using a large amount of data, showing promising results in some CO problems. Rectilinear Steiner tree packing problem (RSTPP) is a well-known CO problem and is widely used in modeling wiring problem among components in a printed circuit board and an integrated circuit design. Despite the importance of its application, the lack of available data has restricted to fully leverage machine learning approaches. In this paper, we present ReSPack, a large-scale synthetic RSTPP data generator and a benchmark. ReSPack includes a source code for generating RSTPP instances of various types with different sizes, test instances generated for the benchmark evaluation, and implementations of several baseline algorithms."}}
{"id": "BKZIivLs9xc", "cdate": 1646077539372, "mdate": null, "content": {"title": "Fast Inference and Transfer of Compositional Task Structures for Few-shot Task Generalization", "abstract": "We tackle real-world problems with complex structures beyond the pixel-based game or simulator. We formulate it as a few-shot reinforcement learning problem where a task is characterized by a subtask graph that defines a set of subtasks and their dependencies that are unknown to the agent. Different from the previous meta-RL methods trying to directly infer the unstructured task embedding, our multi-task subtask graph inferencer (MTSGI) first infers the common high-level task structure in terms of the subtask graph from the training tasks, and use it as a prior to improve the task inference in testing. Our experiment results on 2D grid-world and complex web navigation domains show that the proposed method can learn and leverage the common underlying structure of the tasks for faster adaptation to the unseen tasks than various existing algorithms such as meta reinforcement learning, hierarchical reinforcement learning, and other heuristic agents."}}
{"id": "zWBXBsKiAZL", "cdate": 1640995200000, "mdate": 1684364730208, "content": {"title": "Fast inference and transfer of compositional task structures for few-shot task generalization", "abstract": "We tackle real-world problems with complex structures beyond the pixel-based game or simulator. We formulate it as a few-shot reinforcement learning problem where a task is characterized by a subta..."}}
{"id": "MbY-ySNb3E", "cdate": 1640995200000, "mdate": 1684370639781, "content": {"title": "Learning Parameterized Task Structure for Generalization to Unseen Entities", "abstract": "Real world tasks are hierarchical and compositional. Tasks can be composed of multiple subtasks (or sub-goals) that are dependent on each other. These subtasks are defined in terms of entities (e.g., \"apple\", \"pear\") that can be recombined to form new subtasks (e.g., \"pickup apple\", and \"pickup pear\"). To solve these tasks efficiently, an agent must infer subtask dependencies (e.g. an agent must execute \"pickup apple\" before \"place apple in pot\"), and generalize the inferred dependencies to new subtasks (e.g. \"place apple in pot\" is similar to \"place apple in pan\"). Moreover, an agent may also need to solve unseen tasks, which can involve unseen entities. To this end, we formulate parameterized subtask graph inference (PSGI), a method for modeling subtask dependencies using first-order logic with factored entities. To facilitate this, we learn parameter attributes in a zero-shot manner, which are used as quantifiers (e.g. is_pickable(X)) for the factored subtask graph. We show this approach accurately learns the latent structure on hierarchical and compositional tasks more efficiently than prior work, and show PSGI can generalize by modelling structure on subtasks unseen during adaptation."}}
