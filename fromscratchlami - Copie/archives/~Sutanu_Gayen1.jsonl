{"id": "kOc95CcpkbP", "cdate": 1609459200000, "mdate": null, "content": {"title": "Testing Product Distributions: A Closer Look", "abstract": "We study the problems of {\\em identity} and {\\em closeness testing} of $n$-dimensional product distributions. Prior works of Canonne, Diakonikolas, Kane and Stewart (COLT 2017) and Daskalakis and P..."}}
{"id": "iEAzQKuHzyS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Efficient Distance Approximation for Structured High-Dimensional Distributions via Learning.", "abstract": "We design efficient distance approximation algorithms for several classes of structured high-dimensional distributions. Specifically, we show algorithms for the following problems: - Given sample access to two Bayesian networks $P_1$ and $P_2$ over known directed acyclic graphs $G_1$ and $G_2$ having $n$ nodes and bounded in-degree, approximate $d_{tv}(P_1,P_2)$ to within additive error $\\epsilon$ using $poly(n,\\epsilon)$ samples and time - Given sample access to two ferromagnetic Ising models $P_1$ and $P_2$ on $n$ variables with bounded width, approximate $d_{tv}(P_1, P_2)$ to within additive error $\\epsilon$ using $poly(n,\\epsilon)$ samples and time - Given sample access to two $n$-dimensional Gaussians $P_1$ and $P_2$, approximate $d_{tv}(P_1, P_2)$ to within additive error $\\epsilon$ using $poly(n,\\epsilon)$ samples and time - Given access to observations from two causal models $P$ and $Q$ on $n$ variables that are defined over known causal graphs, approximate $d_{tv}(P_a, Q_a)$ to within additive error $\\epsilon$ using $poly(n,\\epsilon)$ samples, where $P_a$ and $Q_a$ are the interventional distributions obtained by the intervention $do(A=a)$ on $P$ and $Q$ respectively for a particular variable $A$. Our results are the first efficient distance approximation algorithms for these well-studied problems. They are derived using a simple and general connection to distribution learning algorithms. The distance approximation algorithms imply new efficient algorithms for {\\em tolerant} testing of closeness of the above-mentioned structured high-dimensional distributions."}}
{"id": "hfe03emXVih", "cdate": 1577836800000, "mdate": null, "content": {"title": "Near-Optimal Learning of Tree-Structured Distributions by Chow-Liu", "abstract": "We provide finite sample guarantees for the classical Chow-Liu algorithm (IEEE Trans.~Inform.~Theory, 1968) to learn a tree-structured graphical model of a distribution. For a distribution $P$ on $\\Sigma^n$ and a tree $T$ on $n$ nodes, we say $T$ is an $\\varepsilon$-approximate tree for $P$ if there is a $T$-structured distribution $Q$ such that $D(P\\;||\\;Q)$ is at most $\\varepsilon$ more than the best possible tree-structured distribution for $P$. We show that if $P$ itself is tree-structured, then the Chow-Liu algorithm with the plug-in estimator for mutual information with $\\widetilde{O}(|\\Sigma|^3 n\\varepsilon^{-1})$ i.i.d.~samples outputs an $\\varepsilon$-approximate tree for $P$ with constant probability. In contrast, for a general $P$ (which may not be tree-structured), $\\Omega(n^2\\varepsilon^{-2})$ samples are necessary to find an $\\varepsilon$-approximate tree. Our upper bound is based on a new conditional independence tester that addresses an open problem posed by Canonne, Diakonikolas, Kane, and Stewart~(STOC, 2018): we prove that for three random variables $X,Y,Z$ each over $\\Sigma$, testing if $I(X; Y \\mid Z)$ is $0$ or $\\geq \\varepsilon$ is possible with $\\widetilde{O}(|\\Sigma|^3/\\varepsilon)$ samples. Finally, we show that for a specific tree $T$, with $\\widetilde{O} (|\\Sigma|^2n\\varepsilon^{-1})$ samples from a distribution $P$ over $\\Sigma^n$, one can efficiently learn the closest $T$-structured distribution in KL divergence by applying the add-1 estimator at each node."}}
{"id": "Lz_F3smBal_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning and Sampling of Atomic Interventions from Observations", "abstract": "We study the problem of efficiently estimating the effect of an intervention on a single variable using observational samples. Our goal is to give algorithms with polynomial time and sample complex..."}}
{"id": "6vd0GmTAtCH", "cdate": 1577836800000, "mdate": null, "content": {"title": "Testing Product Distributions: A Closer Look", "abstract": "We study the problems of identity and closeness testing of $n$-dimensional product distributions. Prior works by Canonne, Diakonikolas, Kane and Stewart (COLT 2017) and Daskalakis and Pan (COLT 2017) have established tight sample complexity bounds for non-tolerant testing over a binary alphabet: given two product distributions $P$ and $Q$ over a binary alphabet, distinguish between the cases $P = Q$ and $d_{\\mathrm{TV}}(P, Q) > \\epsilon$. We build on this prior work to give a more comprehensive map of the complexity of testing of product distributions by investigating tolerant testing with respect to several natural distance measures and over an arbitrary alphabet. Our study gives a fine-grained understanding of how the sample complexity of tolerant testing varies with the distance measures for product distributions. In addition, we also extend one of our upper bounds on product distributions to bounded-degree Bayes nets."}}
