{"id": "jZnMZBJ-Ka", "cdate": 1640995200000, "mdate": 1682350658062, "content": {"title": "Mutual Information Regularized Feature-Level Frankenstein for Discriminative Recognition", "abstract": "Deep learning recognition approaches can potentially perform better if we can extract a discriminative representation that controllably separates nuisance factors. In this paper, we propose a novel approach to explicitly enforce the extracted discriminative representation <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{d}$</tex-math></inline-formula> , extracted latent variation <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{l}$</tex-math></inline-formula> (e,g., background, unlabeled nuisance attributes), and semantic variation label vector <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{s}$</tex-math></inline-formula> (e.g., labeled expressions/pose) to be independent and complementary to each other. We can cast this problem as an adversarial game in the latent space of an auto-encoder. Specifically, with the to-be-disentangled <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{s}$</tex-math></inline-formula> , we propose to equip an end-to-end conditional adversarial network with the ability to decompose an input sample into <inline-formula><tex-math notation=\"LaTeX\">${\\boldsymbol{d}}$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{l}$</tex-math></inline-formula> . However, we argue that maximizing the cross-entropy loss of semantic variation prediction from <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{d}$</tex-math></inline-formula> is not sufficient to remove the impact of <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{s}$</tex-math></inline-formula> from <inline-formula><tex-math notation=\"LaTeX\">$\\boldsymbol{d}$</tex-math></inline-formula> , and that the uniform-target and entropy regularization are necessary. A collaborative mutual information regularization framework is further proposed to avoid unstable adversarial training. It is able to minimize the differentiable mutual information between the variables to enforce independence. The proposed discriminative representation inherits the desired tolerance property guided by prior knowledge of the task. Our proposed framework achieves top performance on diverse recognition tasks, including digits classification, large-scale face recognition on LFW and IJB-A datasets, and face recognition tolerant to changes in lighting, makeup, disguise, etc."}}
{"id": "Fn9LDO-6Xm", "cdate": 1640995200000, "mdate": 1682350657318, "content": {"title": "Fundus GAN - GAN-based Fundus Image Synthesis for Training Retinal Image Classifiers", "abstract": "Two major challenges in applying deep learning to develop a computer-aided diagnosis of fundus images are the lack of enough labeled data and legal issues with patient privacy. Various efforts are being made to increase the amount of data either by augmenting training images or by synthesizing realistic-looking fundus images. However, augmentation is limited by the amount of available data and it does not address the patient privacy concern. In this paper, we propose a Generative Adversarial Network-based (GAN-based) fundus image synthesis method (Fundus GAN) that generates synthetic training images to solve the above problems. Fundus GAN is an improved way of generating retinal images by following a two-step generation process which involves first training a segmentation network to extract the vessel tree followed by vessel tree to fundus image-to-image translation using unsupervised generative attention networks. Our results show that the proposed Fundus GAN outperforms state of the art methods in different evaluation metrics. Our results also validate that generated retinal images can be used to train retinal image classifiers for eye diseases diagnosis. Clinical Relevance\u2014 Our proposed method Fundus GAN helps in solving the shortage of patient privacy-preserving training data in developing algorithms for automating image- based eye disease diagnosis. The proposed two-step GAN- based image synthesis can be used to improve the classification accuracy of retinal image classifiers without compromising the privacy of the patient"}}
{"id": "079FH9rneJ", "cdate": 1609459200000, "mdate": 1682350656918, "content": {"title": "A Map Inference Approach Using Signal Processing from Crowd-sourced GPS Data", "abstract": "The amount of GPS data that can be collected is increasing tremendously, thanks to the increased popularity of Global Position System (GPS) devices (e.g., smartphones). This article aims to develop novel methods of converting crowd-sourced GPS traces into road topology maps. We explore map inference using a three-stage approach, which incorporates a novel Multi-source Variable Rate (MSVR) signal reconstruction mechanism. Unlike conventional map inference methods based on map graph theory, our approach, to the best of our knowledge, is the first use of estimation theory for map inference. In particular, our approach addresses the unique challenges of vehicular GPS data. This data is plentiful but suffers from noise in location and variable coverage of regions. This makes it difficult to differentiate between noise and sparsely covered regions when increasing coverage and reducing noise. Due to the asynchronous, variable sampling rate, and often under-sampled nature of the data, our MSVR approach can better handle inherent GPS errors, reconstruct road shapes more accurately, and better deal with variable GPS data density in empirical environments. We evaluated our method for map inference by comparing to Open Street Map maps as ground truth. We use the F-Measure, Precision, and Recall metrics to evaluate our method on Tsinghua University\u2019s Beijing Taxi Dataset and Shanghai Jiao Tong University\u2019s SUVnet Dataset. On these datasets, we obtained a mean<?brk?> F-Measure, Precision, and Recall of 0.7212, 0.9165, and 0.6021, respectively, outperforming a well-known method based on Kernel Density Estimation in terms of these evaluation metrics."}}
{"id": "_pAlWi4FT", "cdate": 1581700273270, "mdate": null, "content": {"title": "Confidence regularized self-training", "abstract": "Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance."}}
{"id": "pahglkKS6gh", "cdate": 1577836800000, "mdate": 1682350655938, "content": {"title": "Towards Occlusion-Aware Multifocal Displays", "abstract": "The human visual system uses numerous cues for depth perception, including disparity, accommodation, motion parallax and occlusion. It is incumbent upon virtual-reality displays to satisfy these cues to provide an immersive user experience. Multifocal displays, one of the classic approaches to satisfy the accommodation cue, place virtual content at multiple focal planes, each at a di erent depth. However, the content on focal planes close to the eye do not occlude those farther away; this deteriorates the occlusion cue as well as reduces contrast at depth discontinuities due to leakage of the defocus blur. This paper enables occlusion-aware multifocal displays using a novel ConeTilt operator that provides an additional degree of freedom -- tilting the light cone emitted at each pixel of the display panel. We show that, for scenes with relatively simple occlusion con gurations, tilting the light cones provides the same e ect as physical occlusion. We demonstrate that ConeTilt can be easily implemented by a phase-only spatial light modulator. Using a lab prototype, we show results that demonstrate the presence of occlusion cues and the increased contrast of the display at depth edges."}}
{"id": "bIn9duLj5aF", "cdate": 1577836800000, "mdate": 1682350658559, "content": {"title": "Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification", "abstract": "Although a significant progress has been witnessed in supervised person re-identification (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation efficacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneficial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins."}}
{"id": "_jqYwhW6prY", "cdate": 1577836800000, "mdate": 1682350655912, "content": {"title": "Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification", "abstract": "Although a significant progress has been witnessed in supervised person re-identification (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation efficacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneficial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins."}}
{"id": "UfKSeZRQC-s", "cdate": 1577836800000, "mdate": 1668695308085, "content": {"title": "Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection", "abstract": "Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts. Moreover, the image-level category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a Comprehensive Attention Self-Distillation (CASD) training approach for WSOD. To balance feature learning among all object instances, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO."}}
{"id": "LIm29x7lHbz", "cdate": 1577836800000, "mdate": 1682350658226, "content": {"title": "Towards occlusion-aware multifocal displays", "abstract": "The human visual system uses numerous cues for depth perception, including disparity, accommodation, motion parallax and occlusion. It is incumbent upon virtual-reality displays to satisfy these cues to provide an immersive user experience. Multifocal displays, one of the classic approaches to satisfy the accommodation cue, place virtual content at multiple focal planes, each at a different depth. However, the content on focal planes close to the eye do not occlude those farther away; this deteriorates the occlusion cue as well as reduces contrast at depth discontinuities due to leakage of the defocus blur. This paper enables occlusion-aware multifocal displays using a novel ConeTilt operator that provides an additional degree of freedom --- tilting the light cone emitted at each pixel of the display panel. We show that, for scenes with relatively simple occlusion configurations, tilting the light cones provides the same effect as physical occlusion. We demonstrate that ConeTilt can be easily implemented by a phase-only spatial light modulator. Using a lab prototype, we show results that demonstrate the presence of occlusion cues and the increased contrast of the display at depth edges."}}
{"id": "HJm-dnF1Ra", "cdate": 1577836800000, "mdate": 1682350656777, "content": {"title": "Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection", "abstract": "Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts. Moreover, the image-level category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a Comprehensive Attention Self-Distillation (CASD) training approach for WSOD. To balance feature learning among all object instances, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO."}}
