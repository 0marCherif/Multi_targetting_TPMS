{"id": "kuKqd3qZuk", "cdate": 1701830092656, "mdate": 1701830092656, "content": {"title": "Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective", "abstract": "We aim at advancing blind image quality assessment (BIQA), which predicts the human perception of image quality without any reference information. We develop a general and automated multitask learning scheme for BIQA to exploit auxiliary knowledge from other tasks, in a way that the model parameter sharing and the loss weighting are determined automatically. Specifically, we first describe all candidate label combinations (from multiple tasks) using a textual template, and compute the joint probability from the cosine similarities of the visual-textual embeddings. Predictions of each task can be inferred from the joint distribution, and optimized by carefully designed loss functions. Through comprehensive experiments on learning three tasks - BIQA, scene classification, and distortion type identification, we verify that the proposed BIQA method 1) benefits from the scene classification and distortion type identification tasks and outperforms the stateof-the-art on multiple IQA datasets, 2) is more robust in the group maximum differentiation competition, and 3) realigns the quality annotations from different IQA datasets more effectively. The source code is available at https://github.com/zwx8981/LIQE."}}
{"id": "SRKzfufgOmv", "cdate": 1677628800000, "mdate": 1681436361891, "content": {"title": "Continual Learning for Blind Image Quality Assessment", "abstract": ""}}
{"id": "3AV_53iRfTi", "cdate": 1652737495909, "mdate": null, "content": {"title": "Perceptual Attacks of No-Reference Image Quality Models with Human-in-the-Loop", "abstract": "No-reference image quality assessment (NR-IQA) aims to quantify how humans perceive visual distortions of digital images without access to their undistorted references. NR-IQA models are extensively studied in computational vision, and are widely used for performance evaluation and perceptual optimization of man-made vision systems. Here we make one of the first attempts to examine the perceptual robustness of NR-IQA models. Under a Lagrangian formulation, we identify insightful connections of the proposed perceptual attack to previous beautiful ideas in computer vision and machine learning. We test one knowledge-driven and three data-driven NR-IQA methods under four full-reference IQA models (as approximations to human perception of just-noticeable differences). Through carefully designed psychophysical experiments, we find that all four NR-IQA models are vulnerable to the proposed perceptual attack. More interestingly, we observe that the generated counterexamples are not transferable, manifesting themselves as distinct design flows of respective NR-IQA methods. Source code are available at https://github.com/zwx8981/PerceptualAttack_BIQA."}}
{"id": "r_Ox3T1MuAF", "cdate": 1643810755558, "mdate": 1643810755558, "content": {"title": "Continual Learning for Blind Image Quality Assessment", "abstract": "The explosive growth of image data facilitates the fast development of image processing and computer vision methods for emerging visual applications, meanwhile introducing novel distortions to the processed images. This poses a grand challenge to existing blind image quality assessment (BIQA) models, failing to continually adapt to such subpopulation shift. Recent work suggests training BIQA methods on the combination of all available human-rated IQA datasets. However, this type of approach is not scalable to a large number of datasets, and is cumbersome to incorporate a newly created dataset as well. In this paper, we formulate continual learning for BIQA, where a model learns continually from a stream of IQA datasets, building on what was learned from previously seen data. We first identify five desiderata in the new setting with a measure to quantify the plasticity-stability trade-off. We then propose a simple yet effective method for learning BIQA models continually. Specifically, based on a shared backbone network, we add a prediction head for a new dataset, and enforce a regularizer to allow all prediction heads to evolve with new data while being resistant to catastrophic forgetting of old data. We compute the quality score by an adaptive weighted summation of estimates from all prediction heads. Extensive experiments demonstrate the promise of the proposed continual learning method in comparison to standard training techniques for BIQA."}}
{"id": "HtLeBDkfO0K", "cdate": 1643810652748, "mdate": 1643810652748, "content": {"title": "Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception", "abstract": "Perceptual quality assessment of the videos acquired in the wilds is of vital importance for quality assurance of video services. The inaccessibility of reference videos with pristine quality and the complexity of authentic distortions pose great challenges for this kind of blind video quality assessment (BVQA) task. Although model-based transfer learning is an effective and efficient paradigm for the BVQA task, it remains to be a challenge to explore what and how to bridge the domain shifts for better video representation. In this work, we propose to transfer knowledge from image quality assessment (IQA) databases with authentic distortions and large-scale action recognition with rich motion patterns. We rely on both groups of data to learn the feature extractor. We train the proposed model on the target VQA databases using a mixed list-wise ranking loss function. Extensive experiments on six databases demonstrate that our method performs very competitively under both individual database and mixed database training settings. We also verify the rationality of each component of the proposed method and explore a simple manner for further improvement."}}
{"id": "Hgg2jRWO0F", "cdate": 1643810468457, "mdate": 1643810468457, "content": {"title": "Hierarchical features fusion for image aesthetics assessment", "abstract": "Image aesthetics assessment is an interesting yet challenging topic which can be applied on numerous scenarios such as high quality image retrieval or recommendation systems. We propose a hierarchical features fusion aesthetic assessment (HFFAA) model for this task. HFFAA is a two-stream convolutional neural network (CNN) which is composed of two branches with heterogeneous and complementary aesthetic perceptual abilities. HFFAA learns the mapping from deep image representation into their ground-truth aesthetic labels (good or bad) in an end-to-end fashion. Extensive experiments demonstrate that the proposed model achieves superior performance on two widely evaluated public benchmark databases, i.e., CUHKPQ and AVA. We also validate the rationality of the designs of HFFAA through a series of ablation experiments."}}
{"id": "SnGxc36bOAY", "cdate": 1643810226300, "mdate": 1643810226300, "content": {"title": "Uncertainty-aware blind image quality assessment in the laboratory and wild", "abstract": "Performance of blind image quality assessment (BIQA) models has been significantly boosted by end-to-end optimization of feature engineering and quality regression. Nevertheless, due to the distributional shift between images simulated in the laboratory and captured in the wild, models trained on databases with synthetic distortions remain particularly weak at handling realistic distortions (and vice versa). To confront the cross-distortion-scenario challenge, we develop a unified BIQA model and an approach of training it for both synthetic and realistic distortions. We first sample pairs of images from individual IQA databases, and compute a probability that the first image of each pair is of higher quality. We then employ the fidelity loss to optimize a deep neural network for BIQA over a large number of such image pairs. We also explicitly enforce a hinge constraint to regularize uncertainty estimation during optimization. Extensive experiments on six IQA databases show the promise of the learned method in blindly assessing image quality in the laboratory and wild. In addition, we demonstrate the universality of the proposed training strategy by using it to improve existing BIQA models."}}
{"id": "S_ZeQIab_RY", "cdate": 1643810122848, "mdate": 1643810122848, "content": {"title": "Blind image quality assessment using a deep bilinear convolutional neural network", "abstract": "We propose a deep bilinear model for blind image quality assessment that works for both synthetically and authentically distorted images. Our model constitutes two streams of deep convolutional neural networks (CNNs), specializing in two distortion scenarios separately. For synthetic distortions, we first pre-train a CNN to classify the distortion type and the level of an input image, whose ground truth label is readily available at a large scale. For authentic distortions, we make use of a pre-train CNN (VGG-16) for the image classification task. The two feature sets are bilinearly pooled into one representation for a final quality prediction. We fine-tune the whole network on the target databases using a variant of stochastic gradient descent. The extensive experimental results show that the proposed model achieves state-of-the-art performance on both synthetic and authentic IQA databases. Furthermore, we verify the generalizability of our method on the large-scale Waterloo Exploration Database, and demonstrate its competitiveness using the group maximum differentiation competition methodology."}}
{"id": "sSMqN188AnZ", "cdate": 1640995200000, "mdate": 1668090088496, "content": {"title": "Learning a Blind Quality Evaluator for UGC Videos in Perceptually Relevant Domains", "abstract": "The absence of reference videos with pristine quality and the complexity of authentic distortions pose great challenges for blind video quality assessment (BVQA) towards user-generated content (UGC) videos. Although it is straightfor-ward to leverage transfer learning techniques to learn effective BVQA models, it is nontrivial to explore how to bridge the domain shifts for better video representation learning. In this work, we propose to transfer meaningful knowledge from perceptually relevant domains, i.e., image quality assessment (IQA) with authentic distortions and video classification with rich motion patterns. We develop a promising strategy to use both groups of data to learn the feature extractors. We train the proposed model on the target VQA databases using a mixed list-wise ranking loss function. Extensive experiments on six VQA databases demonstrate that our method performs very competitively under both individual database and mixed database training settings. Codes and models are available at https://github.com/zwx8981/BVQA-2021."}}
{"id": "gBKctBs2zA", "cdate": 1640995200000, "mdate": 1668090088491, "content": {"title": "Blindly Assess Quality of In-the-Wild Videos via Quality-Aware Pre-Training and Motion Perception", "abstract": "Perceptual quality assessment of the videos acquired in the wilds is of vital importance for quality assurance of video services. The inaccessibility of reference videos with pristine quality and the complexity of authentic distortions pose great challenges for this kind of blind video quality assessment (BVQA) task. Although model-based transfer learning is an effective and efficient paradigm for the BVQA task, it remains to be a challenge to explore <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">what</i> and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">how</i> to bridge the domain shifts for better video representation. In this work, we propose to transfer knowledge from image quality assessment (IQA) databases with authentic distortions and large-scale action recognition with rich motion patterns. We rely on both groups of data to learn the feature extractor and use a mixed list-wise ranking loss function to train the entire model on the target VQA databases. Extensive experiments on six benchmarking databases demonstrate that our method performs very competitively under both individual database and mixed databases training settings. We also verify the rationality of each component of the proposed method and explore a simple ensemble trick for further improvement."}}
