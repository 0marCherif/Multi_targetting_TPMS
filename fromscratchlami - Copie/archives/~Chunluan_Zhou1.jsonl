{"id": "acm7J7wWe9Z", "cdate": 1668511235341, "mdate": null, "content": {"title": "AiATrack: Attention in Attention for Transformer Visual Tracking", "abstract": "Transformer trackers have achieved impressive advancements\nrecently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate\ncorrelations and suppresses erroneous ones by seeking consensus among\nall correlation vectors. Our AiA module can be readily applied to both\nself-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we\npropose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that\nour tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed. Code and models are publicly\navailable at https://github.com/Little-Podi/AiATrack.\n"}}
{"id": "xO8HQnLoYCY", "cdate": 1609459200000, "mdate": 1631937590567, "content": {"title": "Pyramidal Multiple Instance Detection Network With Mask Guided Self-Correction for Weakly Supervised Object Detection", "abstract": "Weakly supervised object detection has attracted more and more attention as it only needs image-level annotations for training object detectors. A popular solution to this task is to train a multiple instance detection network (MIDN) which integrates multiple instance learning into a deep convolutional neural network. One major issue of the MIDN is that it is prone to be stuck at local discriminative regions. To address this local optimum issue, we propose a pyramidal MIDN (P-MIDN) comprised of a sequence of multiple MIDNs. In particular, one MIDN performs proposal removal for its subsequent MIDN to reduce the exposure of local discriminative proposal regions to the latter during training. In this manner, it allows our MIDNs to focus on proposals which cover objects more completely. Furthermore, we integrate the P-MIDN into an online instance classifier refinement (OICR) framework. Combined with the P-MIDN, a mask guided self-correction (MGSC) method is proposed to generate high-quality pseudo ground-truths for training the OICR. Experimental results on PASCAL VOC 2007, PASCAL VOC 2010, PASCAL VOC 2012, ILSVRC 2013 DET and MS-COCO benchmarks demonstrate that our approach achieves state-of-the-art performance."}}
{"id": "kF0nfqVFTr", "cdate": 1609459200000, "mdate": 1631937590567, "content": {"title": "Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking", "abstract": "Multi-person pose estimation and tracking serve as crucial steps for video understanding. Most state-of-the-art approaches rely on first estimating poses in each frame and only then implementing data association and refinement. Despite the promising results achieved, such a strategy is inevitably prone to missed detections especially in heavily-cluttered scenes, since this tracking-by-detection paradigm is, by nature, largely dependent on visual evidences that are absent in the case of occlusion. In this paper, we propose a novel online approach to learning the pose dynamics, which are independent of pose detections in current fame, and hence may serve as a robust estimation even in challenging scenarios including occlusion. Specifically, we derive this prediction of dynamics through a graph neural network~(GNN) that explicitly accounts for both spatial-temporal and visual information. It takes as input the historical pose tracklets and directly predicts the corresponding poses in the following frame for each tracklet. The predicted poses will then be aggregated with the detected poses, if any, at the same frame so as to produce the final pose, potentially recovering the occluded joints missed by the estimator. Experiments on PoseTrack 2017 and PoseTrack 2018 datasets demonstrate that the proposed method achieves results superior to the state of the art on both human pose estimation and tracking tasks."}}
{"id": "75XWXPsz29t", "cdate": 1609459200000, "mdate": 1631937590566, "content": {"title": "Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking", "abstract": "Multi-person pose estimation and tracking serve as crucial steps for video understanding. Most state-of-the-art approaches rely on first estimating poses in each frame and only then implementing data association and refinement. Despite the promising results achieved, such a strategy is inevitably prone to missed detections especially in heavily-cluttered scenes, since this tracking-by-detection paradigm is, by nature, largely dependent on visual evidences that are absent in the case of occlusion. In this paper, we propose a novel online approach to learning the pose dynamics, which are independent of pose detections in current fame, and hence may serve as a robust estimation even in challenging scenarios including occlusion. Specifically, we derive this prediction of dynamics through a graph neural network (GNN) that explicitly accounts for both spatial-temporal and visual information. It takes as input the historical pose tracklets and directly predicts the corresponding poses in the following frame for each tracklet. The predicted poses will then be aggregated with the detected poses, if any, at the same frame so as to produce the final pose, potentially recovering the occluded joints missed by the estimator. Experiments on PoseTrack 2017 and PoseTrack 2018 datasets demonstrate that the proposed method achieves results superior to the state of the art on both human pose estimation and tracking tasks."}}
{"id": "vXfGhqXQS7Z", "cdate": 1577836800000, "mdate": 1631937590566, "content": {"title": "Detecting spatiotemporal irregularities in videos via a 3D convolutional autoencoder", "abstract": "Spatiotemporal irregularities (i.e., the uncommon appearance and motion patterns) in videos are difficult to detect, as they are usually not well defined and appear rarely in videos. We tackle this problem by learning normal patterns from regular videos, while treating irregularities as deviations from normal patterns. To this end, we introduce a 3D fully convolutional autoencoder (3D-FCAE) that is trainable in an end-to-end manner to detect both temporal and spatiotemporal irregularities in videos using limited training data. Subsequently, temporal irregularities can be detected as frames with high reconstruction errors, and irregular spatiotemporal patterns can be detected as blurry regions that are not well reconstructed. Our approach can accurately locate temporal and spatiotemporal irregularities thanks to the 3D fully convolutional autoencoder and the explored effective architecture. We evaluate the proposed autoencoder for detecting irregular patterns on benchmark video datasets with weak supervision. Comparisons with state-of-the-art approaches demonstrate the effectiveness of our approach. Moreover, the learned autoencoder shows good generalizability across multiple datasets."}}
{"id": "f8cnWKpRr0", "cdate": 1577836800000, "mdate": 1631937590299, "content": {"title": "Occlusion Pattern Discovery for Object Detection and Occlusion Reasoning", "abstract": "Despite recent progress of object category detection in real scenes, detecting objects that are partially or heavily occluded remains a challenging problem due to the uncertainty and diversity of occlusion situations which could cause large intra-category appearance variance. To learn these occlusion situations, we propose a novel approach to discover occlusion patterns that cannot only boost occluded object detection but also provide occlusion reasoning. Our approach is based on a classic deformable part model (DPM) trained on fully observed object examples. Each occlusion pattern contains only a subset of visible parts, thus the total number of occlusion patterns are exponential to the number of parts, i.e., m parts will generate 2 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">m</sup> occlusion patterns to compose an occlusion pattern pool. From this occlusion pattern pool, we look for a small group of occlusion patterns that are: (1) representative patterns that can well explain training examples and (2) discriminative patterns that have high detection performance individually. To select such occlusion patterns, we formulate occlusion pattern discovery as a facility location problem, which can be solved effectively by greedy search. The discovered occlusion patterns are themselves DPMs and can be used as object detectors when properly tuned. They can also be combined with the state-of-the-art detectors (e.g. Faster R-CNN) for improving detection performance and achieving part-level occlusion reasoning. The effectiveness of the proposed approach is validated on Pascal VOC2007 and VOC2010 datasets."}}
{"id": "NJJl2lUs112", "cdate": 1577836800000, "mdate": 1631937590217, "content": {"title": "Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians", "abstract": "State-of-the-art pedestrian detectors have performed promisingly on non-occluded pedestrians, yet they are still confronted by heavy occlusions. Although many previous works have attempted to alleviate the pedestrian occlusion issue, most of them rest on still images. In this paper, we exploit the local temporal context of pedestrians in videos and propose a tube feature aggregation network (TFAN) aiming at enhancing pedestrian detectors against severe occlusions. Specifically, for an occluded pedestrian in the current frame, we iteratively search for its relevant counterparts along temporal axis to form a tube. Then, features from the tube are aggregated according to an adaptive weight to enhance the feature representations of the occluded pedestrian. Furthermore, we devise a temporally discriminative embedding module (TDEM) and a part-based relation module (PRM), respectively, which adapts our approach to better handle tube drifting and heavy occlusions. Extensive experiments are conducted on three datasets, Caltech, NightOwls and KAIST, showing that our proposed method is significantly effective for heavily occluded pedestrian detection. Moreover, we achieve the state-of-the-art performance on the Caltech and NightOwls datasets."}}
{"id": "AHZk45b7dmm", "cdate": 1577836800000, "mdate": 1631937590238, "content": {"title": "Self-Mimic Learning for Small-scale Pedestrian Detection", "abstract": "Detecting small-scale pedestrians is one of the most challenging problems in pedestrian detection. Due to the lack of visual details, the representations of small-scale pedestrians tend to be weak to be distinguished from background clutters. In this paper, we conduct an in-depth analysis of the small-scale pedestrian detection problem, which reveals that weak representations of small-scale pedestrians are the main cause for a classifier to miss them. To address this issue, we propose a novel Self-Mimic Learning (SML) method to improve the detection performance on small-scale pedestrians. We enhance the representations of small-scale pedestrians by mimicking the rich representations from large-scale pedestrians. Specifically, we design a mimic loss to force the feature representations of small-scale pedestrians to approach those of large-scale pedestrians. The proposed SML is a general component that can be readily incorporated into both one-stage and two-stage detectors, with no additional network layers and incurring no extra computational cost during inference. Extensive experiments on both the CityPersons and Caltech datasets show that the detector trained with the mimic loss is significantly effective for small-scale pedestrian detection and achieves state-of-the-art results on CityPersons and Caltech, respectively."}}
{"id": "pQ8CRgCr-A", "cdate": 1546300800000, "mdate": 1631937590567, "content": {"title": "Multi-resolution attention convolutional neural network for crowd counting", "abstract": "Estimating crowd counts remains a challenging task due to the problems of scale variations, non-uniform distribution and complex backgrounds. In this paper, we propose a multi-resolution attention convolutional neural network (MRA-CNN) to address this challenging task. Except for the counting task, we exploit an additional density-level classification task during training and combine features learned for the two tasks, thus forming multi-scale, multi-contextual features to cope with the scale variation and non-uniform distribution. Besides, we utilize a multi-resolution attention (MRA) model to generate score maps, where head locations are with higher scores to guide the network to focus on head regions and suppress non-head regions regardless of the complex backgrounds. During the generation of score maps, atrous convolution layers are used to expand the receptive field with fewer parameters, thus getting higher-level features and providing the MRA model more comprehensive information. Experiments on ShanghaiTech, WorldExpo\u201910 and UCF datasets demonstrate the effectiveness of our method."}}
{"id": "LXXV9B3u2R3", "cdate": 1546300800000, "mdate": 1631937590625, "content": {"title": "A scale adaptive network for crowd counting", "abstract": "Scale variations occur frequently and present a great challenge for crowd counting in practical applications. In this paper, we propose a scale adaptive network to address the scale variation problem for crowd counting. We design a scale expansion unit which uses normal and dilated convolution to expand the receptive field size range of its input and connect several such units densely to cover a large range of densely distributed receptive field sizes so as to fit objects of different sizes in images. To alleviate competition among different scales, especially the negative effect of inappropriate scales, we also design a residual channel-wise re-weighting unit which is inserted after each scale expansion unit to enhance informative feature channels. We evaluate the effectiveness of the proposed scale adaptive network on ShanghaiTech-B and WorldExpo\u201910 datasets."}}
