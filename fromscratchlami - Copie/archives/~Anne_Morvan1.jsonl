{"id": "7bvWopYY1H", "cdate": 1663849871093, "mdate": null, "content": {"title": "GeoVeX: Geospatial Vectors with Hexagonal Convolutional Autoencoders", "abstract": "We introduce a new geospatial representation model called GeoVeX to learn global vectors for all geographical locations on Earth land cover (200+ million embeddings). GeoVeX is built on a novel model architecture named Hexagonal Convolutional Autoencoders (HCAE) combined with a Zero-Inflated Poisson (ZIP) reconstruction layer, applied to a grid of Uber's H3 hexagons, each one described by the histogram of OpenStreetMap (OSM) geographical tags occurrences. GeoVeX is novel on three aspects: 1) it produces the first geospatial vectors trained on worldwide open data, enabling wide adoption on every downstream tasks which may benefit from enriched geographical information, requiring only location coordinates; 2) it represents the first use of hexagonal convolutions within autoencoder architectures, to learn latent representations of an hexagonal grid; and 3) it introduces a spatial-contextual Poisson reconstruction loss function for autoencoder architectures suitable for training on sparse geographical count data. Experiments demonstrate that GeoVeX embeddings can improve upon state-of-the-art geospatial location representations on two different downstream tasks: price prediction in the travel industry and hyperlocal interpolation of climate data from weather stations. "}}
{"id": "jwhW6oCes5E", "cdate": 1663230833315, "mdate": 1663230833315, "content": {"title": "Hotel2vec: Learning Hotel Embeddings from User Click Sessions with Side Information", "abstract": "We propose a new neural network architecture for learning vector representations of items with attributes, specifically hotels. Unlike\nprevious works, which typically only rely on modeling of user-item interactions for learning item embeddings, we propose a framework\nthat combines several sources of data, including user clicks, hotel attributes (e.g., property type, star rating, average user rating),\namenity information (e.g., if the hotel has free Wi-Fi or free breakfast), and geographic information that leverages an hexagonal\ngeospatial system as well as spatial encoders. During model training, a joint embedding is learned from all of the above information.\nWe show that including structured attributes about hotels enables us to make better predictions in a downstream task than when we\nrely exclusively on click data. We train our embedding model on more than 60 million user click sessions from a leading online travel\nplatform, and learn embeddings for more than one million hotels. Our final learned embeddings integrate distinct sub-embeddings\nfor user clicks, hotel attributes, and geographic information, providing a representation that can be used flexibly depending on the\napplication. An important advantage of the proposed neural model is that it addresses the cold-start problem for hotels with insufficient\nhistorical click information by incorporating additional hotel attributes, which are available for all hotels. We show through the results\nof an online A/B test that our model generates high-quality representations that boost the performance of a hotel recommendation\nsystem on a large online travel platform"}}
{"id": "U7YDo8UnGU", "cdate": 1514764800000, "mdate": null, "content": {"title": "Streaming Binary Sketching Based on Subspace Tracking and Diagonal Uniformization.", "abstract": "In this paper, we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search. We present a new online method for computing binary compressed representations -sketches- of high-dimensional real feature vectors. Given an expected code length c and high-dimensional input data points, our algorithm provides a c-bits binary code for preserving the distance between the points from the original high-dimensional space. Our algorithm does not require neither the storage of the whole dataset nor a chunk, thus it is fully adaptable to the streaming setting. It also provides low time complexity and convergence guarantees. We demonstrate the quality of our binary sketches through experiments on real data for the nearest neighbors search task in the online setting."}}
{"id": "LLs1DgoEpMu", "cdate": 1514764800000, "mdate": null, "content": {"title": "Contributions to unsupervised learning from massive high-dimensional data streams: structuring, hashing and clustering. (Contributions \u00e0 l'apprentissage non supervis\u00e9 \u00e0 partir de flux de donn\u00e9es massives en grande dimension: structuration, hashing et clustering).", "abstract": "This thesis focuses on how to perform efficiently unsupervised machine learning such as the fundamentally linked nearest neighbor search and clustering task, under time and space constraints for high-dimensional datasets. First, a new theoretical framework reduces the space cost and increases the rate of flow of data-independent Cross-polytope LSH for the approximative nearest neighbor search with almost no loss of accuracy.Second, a novel streaming data-dependent method is designed to learn compact binary codes from high-dimensional data points in only one pass. Besides some theoretical guarantees, the quality of the obtained embeddings are accessed on the approximate nearest neighbors search task.Finally, a space-efficient parameter-free clustering algorithm is conceived, based on the recovery of an approximate Minimum Spanning Tree of the sketched data dissimilarity graph on which suitable cuts are performed."}}
{"id": "C6FWyU8TvTh", "cdate": 1514764800000, "mdate": null, "content": {"title": "On the Needs for Rotations in Hypercubic Quantization Hashing.", "abstract": "The aim of this paper is to endow the well-known family of hypercubic quantization hashing methods with theoretical guarantees. In hypercubic quantization, applying a suitable (random or learned) rotation after dimensionality reduction has been experimentally shown to improve the results accuracy in the nearest neighbors search problem. We prove in this paper that the use of these rotations is optimal under some mild assumptions: getting optimal binary sketches is equivalent to applying a rotation uniformizing the diagonal of the covariance matrix between data points. Moreover, for two closed points, the probability to have dissimilar binary sketches is upper bounded by a factor of the initial distance between the data points. Relaxing these assumptions, we obtain a general concentration result for random matrices. We also provide some experiments illustrating these theoretical points and compare a set of algorithms in both the batch and online settings."}}
{"id": "GpeSwT4811B", "cdate": 1483228800000, "mdate": null, "content": {"title": "Graph sketching-based Massive Data Clustering.", "abstract": "In this paper, we address the problem of recovering arbitrary-shaped data clusters from datasets while facing \\emph{high space constraints}, as this is for instance the case in many real-world applications when analysis algorithms are directly deployed on resources-limited mobile devices collecting the data. We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a \\emph{sketched} version of the dissimilarity graph $\\mathcal{G}$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular forms thanks to the property of the MST for expressing the underlying structure of a graph. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. An approximate MST is retrieved by following the dynamic \\emph{semi-streaming} model in handling the dissimilarity graph $\\mathcal{G}$ as a stream of edge weight updates which is sketched in one pass over the data into a compact structure requiring $O(N \\operatorname{polylog}(N))$ space, far better than the theoretical memory cost $O(N^2)$ of $\\mathcal{G}$. The recovered approximate MST $\\mathcal{T}$ as input, DBMSTClu then successfully detects the right number of nonconvex clusters by performing relevant cuts on $\\mathcal{T}$ in a time linear in $N$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets."}}
{"id": "EUhE-z-JIqO", "cdate": 1483228800000, "mdate": null, "content": {"title": "Streaming Binary Sketching based on Subspace Tracking and Diagonal Uniformization.", "abstract": "In this paper, we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search. We present a new online method for computing binary compressed representations -sketches- of high-dimensional real feature vectors. Given an expected code length $c$ and high-dimensional input data points, our algorithm provides a $c$-bits binary code for preserving the distance between the points from the original high-dimensional space. Our algorithm does not require neither the storage of the whole dataset nor a chunk, thus it is fully adaptable to the streaming setting. It also provides low time complexity and convergence guarantees. We demonstrate the quality of our binary sketches through experiments on real data for the nearest neighbors search task in the online setting."}}
