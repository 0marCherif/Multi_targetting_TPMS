{"id": "a70BHshi2qY", "cdate": 1664814072847, "mdate": null, "content": {"title": "Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays", "abstract": "We use a dataset of eye-tracking data from five radiologists to compare the regions used by deep learning models for their decisions and the heatmaps representing where radiologists looked. We conduct a class-independent analysis of the saliency maps generated by two methods selected from the literature: Grad-CAM and attention maps from an attention-gated model. For the comparison, we use shuffled metrics, avoiding biases from fixation locations. We achieve scores comparable to an interobserver baseline in one metric, highlighting the potential of saliency maps from Grad-CAM to mimic a radiologist's attention over an image. We also divide the dataset into subsets to evaluate in which cases similarities are higher."}}
{"id": "jSSJeMyB8qq", "cdate": 1639017971039, "mdate": null, "content": {"title": "Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays", "abstract": "The interpretability of medical image analysis models is considered a key research field. We use a dataset of eye-tracking data from five radiologists to compare the outputs of interpretability methods against the heatmaps representing where radiologists looked. We conduct a class-independent analysis of the saliency maps generated by two methods selected from the literature: Grad-CAM and attention maps from an attention-gated model. For the comparison, we use shuffled metrics, which avoid biases from fixation locations. We achieve scores comparable to an interobserver baseline in one shuffled metric, highlighting the potential of saliency maps from Grad-CAM to mimic a radiologist's attention over an image. We also divide the dataset into subsets to evaluate in which cases similarities are higher."}}
{"id": "xddPSSpHP8", "cdate": 1609459200000, "mdate": 1632928395018, "content": {"title": "Image-Based Multiresolution Topology Optimization Using Deep Disjunctive Normal Shape Model", "abstract": "Highlights \u2022 Generic framework for multiresolution topology optimization with varying parameters using deep neural networks. \u2022 A systematic algorithm to choose important high resolution designs for training based on a quantitative image-based measure. \u2022 Generation of near optimal high resolution designs using mainly inexpensive low resolution designs. \u2022 Application to 3D topology optimization. Abstract We present a machine learning framework for predicting the optimized structural topology designs using multiresolution data. Our approach primarily uses optimized designs from inexpensive coarse mesh finite element simulations for model training and generates high resolution images associated with simulation parameters that are not previously used. Our cost-efficient approach enables the designers to effectively search through possible candidate designs in situations where the design requirements rapidly change. The underlying neural network framework is based on a deep disjunctive normal shape model (DDNSM) which learns the mapping between the simulation parameters and segments of multi resolution images. Using this image-based analysis we provide a practical algorithm which enhances the predictability of the learning machine by determining a limited number of important parametric samples (i.e. samples of the simulation parameters) on which the high resolution training data is generated. We demonstrate our approach on benchmark compliance minimization problems including the 3D topology optimization where we show that the high-fidelity designs from the learning machine are close to optimal designs and can be used as effective initial guesses for the large-scale optimization problem."}}
{"id": "H2sakKoO0Hz", "cdate": 1596568247911, "mdate": null, "content": {"title": "Domain adaptation for biomedical image segmentation using adversarial training", "abstract": "Many biomedical image analysis applications require segmentation. Convolutional neural networks (CNN) have become a promising approach to segment biomedical images; however, the accuracy of these methods is highly dependent on the training data. We focus on biomedical image segmentation in the context where there is variation between source and target datasets and ground truth for the target dataset is very limited or non-existent. We use an adversarial based training approach to train CNNs to achieve good accuracy on the target domain. We use the DRIVE and STARE eye vasculture segmentation datasets and show that our approach can significantly improve results where we only use labels of one domain in training and test on the other domain. We also show improvements on membrane detection between MIC-CAI 2016 CREMI challenge and ISBI2013 EM segmentation challenge datasets.\n"}}
{"id": "hAhjslAzuvw", "cdate": 1596568145364, "mdate": null, "content": {"title": "Interpretation of Disease Evidence for Medical Images Using Adversarial Deformation Fields", "abstract": "The high complexity of deep learning models is associated with the difficulty of explaining what evidence they recognize as correlating with specific disease labels. This information is critical for building trust in models and finding their biases. Until now, automated deep learning visualization solutions have identified regions of images used by classifiers, but these solutions are too coarse, too noisy, or have a limited representation of the way images can change. We propose a novel method for formulating and presenting spatial explanations of disease evidence, called deformation field interpretation with generative adversarial networks (DeFI-GAN). An adversarially trained generator produces deformation fields that modify images of diseased patients to resemble images of healthy patients. We validate the method studying chronic obstructive pulmonary disease (COPD) evidence in chest x-rays (CXRs) and Alzheimer's disease (AD) evidence in brain MRIs. When extracting disease evidence in longitudinal data, we show compelling results against a baseline producing difference maps. DeFI-GAN also highlights disease biomarkers not found by previous methods and potential biases that may help in investigations of the dataset and of the adopted learning methods.\n"}}
{"id": "LFDhySxTDam", "cdate": 1596568078431, "mdate": null, "content": {"title": "Adversarial regression training for visualizing the progression of chronic obstructive pulmonary disease with chest x-rays", "abstract": "Knowledge of what spatial elements of medical images deep learning methods use as evidence is important for model interpretability, trustiness, and validation. There is a lack of such techniques for models in regression tasks. We propose a method, called visualization for regression with a generative adversarial network (VR-GAN), for formulating adversarial training specifically for datasets containing regression target values characterizing disease severity. We use a conditional generative adversarial network where the generator attempts to learn to shift the output of a regressor through creating disease effect maps that are added to the original images. Meanwhile, the regressor is trained to predict the original regression value for the modified images. A model trained with this technique learns to provide visualization for how the image would appear at different stages of the disease. We analyze our \u2026\n"}}
{"id": "v9ayDOr93xN", "cdate": 1577836800000, "mdate": 1632928395147, "content": {"title": "Interpretation of Disease Evidence for Medical Images Using Adversarial Deformation Fields", "abstract": "The high complexity of deep learning models is associated with the difficulty of explaining what evidence they recognize as correlating with specific disease labels. This information is critical for building trust in models and finding their biases. Until now, automated deep learning visualization solutions have identified regions of images used by classifiers, but these solutions are too coarse, too noisy, or have a limited representation of the way images can change. We propose a novel method for formulating and presenting spatial explanations of disease evidence, called deformation field interpretation with generative adversarial networks (DeFI-GAN). An adversarially trained generator produces deformation fields that modify images of diseased patients to resemble images of healthy patients. We validate the method studying chronic obstructive pulmonary disease (COPD) evidence in chest x-rays (CXRs) and Alzheimer\u2019s disease (AD) evidence in brain MRIs. When extracting disease evidence in longitudinal data, we show compelling results against a baseline producing difference maps. DeFI-GAN also highlights disease biomarkers not found by previous methods and potential biases that may help in investigations of the dataset and of the adopted learning methods."}}
{"id": "lKr27vRI0I", "cdate": 1577836800000, "mdate": 1632928395315, "content": {"title": "Inter-Slice Image Augmentation Based on Frame Interpolation for Boosting Medical Image Segmentation Accuracy", "abstract": "We introduce the idea of inter-slice image augmentation whereby the numbers of the medical images and the corresponding segmentation labels are increased between two consecutive images in order to boost medical image segmentation accuracy. Unlike conventional data augmentation methods in medical imaging, which only increase the number of training samples directly by adding new virtual samples using simple parameterized transformations such as rotation, flipping, scaling, etc., we aim to augment data based on the relationship between two consecutive images, which increases not only the number but also the information of training samples. For this purpose, we propose a frame-interpolation-based data augmentation method to generate intermediate medical images and the corresponding segmentation labels between two consecutive images. We train and test a supervised U-Net liver segmentation network on SLIVER07 and CHAOS2019, respectively, with the augmented training samples, and obtain segmentation scores exhibiting significant improvement compared to the conventional augmentation methods."}}
{"id": "XZWTOPP1Wq", "cdate": 1577836800000, "mdate": 1632928395145, "content": {"title": "Interpretation of Disease Evidence for Medical Images Using Adversarial Deformation Fields", "abstract": "The high complexity of deep learning models is associated with the difficulty of explaining what evidence they recognize as correlating with specific disease labels. This information is critical for building trust in models and finding their biases. Until now, automated deep learning visualization solutions have identified regions of images used by classifiers, but these solutions are too coarse, too noisy, or have a limited representation of the way images can change. We propose a novel method for formulating and presenting spatial explanations of disease evidence, called deformation field interpretation with generative adversarial networks (DeFI-GAN). An adversarially trained generator produces deformation fields that modify images of diseased patients to resemble images of healthy patients. We validate the method studying chronic obstructive pulmonary disease (COPD) evidence in chest x-rays (CXRs) and Alzheimer's disease (AD) evidence in brain MRIs. When extracting disease evidence in longitudinal data, we show compelling results against a baseline producing difference maps. DeFI-GAN also highlights disease biomarkers not found by previous methods and potential biases that may help in investigations of the dataset and of the adopted learning methods."}}
{"id": "T7YSWalwUji", "cdate": 1577836800000, "mdate": 1632928395150, "content": {"title": "Graph constraint-based robust latent space low-rank and sparse subspace clustering", "abstract": "Recently, low-rank and sparse representation-based methods have achieved great success in subspace clustering, which aims to cluster data lying in a union of subspaces. However, most methods fail if the data samples are corrupted by noise and outliers. To solve this problem, we propose a novel robust method that uses the F-norm for dealing with universal noise and the $$l_1$$ l1 norm or the $$l_{2,1}$$ l2,1 norm for capturing outliers. The proposed method can find a low-dimensional latent space and a low-rank and sparse representation simultaneously. To preserve the local manifold structure of the data, we have adopted a graph constraint in our model to obtain a discriminative latent space. Extensive experiments on several face benchmark datasets show that our proposed method performs better than state-of-the-art subspace clustering methods."}}
