{"id": "lGQ4MoHXyW5", "cdate": 1672531200000, "mdate": 1695918077017, "content": {"title": "Online Certification of Preference-Based Fairness for Personalized Recommender Systems (Extended Abstract)", "abstract": "Recommender systems are facing scrutiny because of their growing impact on the opportunities we have access to. Current audits for fairness are limited to coarse-grained parity assessments at the level of sensitive groups. We propose to audit for envy-freeness, a more granular criterion aligned with individual preferences: every user should prefer their recommendations to those of other users. Since auditing for envy requires to estimate the preferences of users beyond their existing recommendations, we cast the audit as a new pure exploration problem in multi-armed bandits. We propose a sample-efficient algorithm with theoretical guarantees that it does not deteriorate user experience. We also study the trade-offs achieved on real-world recommendation datasets."}}
{"id": "2ZaXz09U1X", "cdate": 1672531200000, "mdate": 1695918077013, "content": {"title": "Matched Pair Calibration for Ranking Fairness", "abstract": "We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias."}}
{"id": "hH3XGpUfGvH", "cdate": 1640995200000, "mdate": 1695918076965, "content": {"title": "Adaptive Sampling Strategies to Construct Equitable Training Datasets", "abstract": "In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data -- an application domain that often suffers from non-representative data collection. We find that our adaptive sampling strategy outperforms several common data collection heuristics, including equal and proportional sampling, demonstrating the value of strategic dataset design for building equitable models."}}
{"id": "FpMJY1IqjIk", "cdate": 1640995200000, "mdate": 1678121110218, "content": {"title": "Online Certification of Preference-Based Fairness for Personalized Recommender Systems", "abstract": ""}}
{"id": "1wRAMsUXOlt", "cdate": 1640995200000, "mdate": 1695918077029, "content": {"title": "Adaptive Sampling Strategies to Construct Equitable Training Datasets", "abstract": ""}}
{"id": "uPWdkoZHgba", "cdate": 1621630238594, "mdate": null, "content": {"title": "Two-sided fairness in rankings via Lorenz dominance", "abstract": "We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efficiency. It guarantees that rankings are Pareto efficient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efficient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility."}}
{"id": "zzTbAVfprs", "cdate": 1609459200000, "mdate": 1678121109733, "content": {"title": "Two-sided fairness in rankings via Lorenz dominance", "abstract": ""}}
{"id": "WdI5xumvJkm", "cdate": 1609459200000, "mdate": 1678121110211, "content": {"title": "Two-sided fairness in rankings via Lorenz dominance", "abstract": ""}}
{"id": "C-gYgWBYURe", "cdate": 1609459200000, "mdate": 1678121109729, "content": {"title": "Online certification of preference-based fairness for personalized recommender systems", "abstract": ""}}
{"id": "1AevxB3t9Pv", "cdate": 1609459200000, "mdate": 1684344264066, "content": {"title": "Fairness On The Ground: Applying Algorithmic Fairness Approaches to Production Systems", "abstract": "Many technical approaches have been proposed for ensuring that decisions made by machine learning systems are fair, but few of these proposals have been stress-tested in real-world systems. This paper presents an example of one team's approach to the challenge of applying algorithmic fairness approaches to complex production systems within the context of a large technology company. We discuss how we disentangle normative questions of product and policy design (like, \"how should the system trade off between different stakeholders' interests and needs?\") from empirical questions of system implementation (like, \"is the system achieving the desired tradeoff in practice?\"). We also present an approach for answering questions of the latter sort, which allows us to measure how machine learning systems and human labelers are making these tradeoffs across different relevant groups. We hope our experience integrating fairness tools and approaches into large-scale and complex production systems will be useful to other practitioners facing similar challenges, and illuminating to academics and researchers looking to better address the needs of practitioners."}}
