{"id": "DJgHzXv61b", "cdate": 1686250303304, "mdate": null, "content": {"title": "Contextualize Me \u2013 The Case for Context in Reinforcement Learning", "abstract": "While Reinforcement Learning (RL) has shown successes in a variety of domains, including game playing, robot manipulation and nuclear fusion, modern RL algorithms are not designed with generalization in mind, making them brittle when faced with even slight variations of their environment. \nTo address this limitation, recent research has increasingly focused on the generalization capabilities of RL agents.\nIdeally, general agents should be capable of zero-shot transfer to previously unseen environments and robust to changes in the problem setting while interacting with an environment.\nSteps in this direction have been taken by proposing new problem settings where agents can test their transfer performance, e.g.~the Arcade Learning Environment's flavors or benchmarks utilizing Procedural Content Generation (PCG) to increase task variation, e.g. ProcGen, NetHack or Alchemy.\n\nWhile these extended problem settings in RL have expanded the possibilities for benchmarking agents in diverse environments, the degree of task variation is often either unknown or cannot be controlled precisely.\nWe believe that generalization in RL is held back by these factors, stemming in part from a lack of problem formalization.\nIn order to facilitate generalization in RL, contextual RL (cRL) proposes to explicitly take environment characteristics, the so-called context into account.\nThis inclusion enables precise design of train and test distributions with respect to this context.\nThus, cRL allows us to reason about the generalization capabilities of RL agents and to quantify their generalization performance.\nOverall, cRL provides a framework for both theoretical analysis and practical improvements.\n\nIn order to empirically study cRL, we introduce our benchmark library CARL, short for Context-Adaptive Reinforcement Learning.\nCARL collects well-established environments from the RL community and extends them with the notion of context.\nWe use our benchmark library to empirically show how different context variations can drastically increase the difficulty of training RL agents, even in simple environments.\nWe further verify the intuition that allowing RL agents access to context information is beneficial for generalization tasks in theory and practice."}}
{"id": "btdRY4lftF6", "cdate": 1686211287185, "mdate": 1686211287185, "content": {"title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning", "abstract": " Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging. To improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPaCE). Based on self-paced learning, \\spc automatically generates \\task curricula online with little computational overhead. To this end, SPaCE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new instances from the same problem domain. Nevertheless, SPaCE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. We demonstrate SPaCE's ability to speed up learning of different value-based RL agents on two environments, showing better generalization capabilities and up to 10x faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach. "}}
{"id": "oJp7uTL7ox-", "cdate": 1664924969919, "mdate": null, "content": {"title": "Gray-Box Gaussian Processes for  Automated Reinforcement Learning", "abstract": "Despite having achieved spectacular milestones in an array of important real-world applications, most Reinforcement Learning (RL) methods are very brittle concerning their hyperparameters. Notwithstanding the crucial importance of setting the hyperparameters in training state-of-the-art agents, the task of hyperparameter optimization (HPO) in RL is understudied. In this paper, we propose a novel gray-box Bayesian Optimization technique for HPO in RL, that enriches Gaussian Processes with reward curve estimations based on generalized logistic functions. We thus about the performance of learning algorithms, transferring information across configurations and about epochs of the learning algorithm. In a very large-scale experimental protocol, comprising 5 popular RL methods (DDPG, A2C, PPO, SAC, TD3), 22 environments (OpenAI Gym: Mujoco, Atari, Classic Control), and 7 HPO baselines, we demonstrate that our method significantly outperforms current HPO practices in RL."}}
{"id": "RyAl60VhTcG", "cdate": 1664924969681, "mdate": null, "content": {"title": "AutoRL-Bench 1.0", "abstract": "It is well established that Reinforcement Learning (RL) \nis very brittle and sensitive to the choice of hyperparameters. This prevents RL methods from being usable out of the box.\nThe field of automated RL (AutoRL) aims at automatically configuring the RL pipeline, to both make RL usable by a broader audience, as well as reveal its full potential. \nStill, there has been little progress towards this goal as new AutoRL methods often are evaluated with incompatible experimental protocols.\nFurthermore, the typically high cost of experimentation prevents a thorough and meaningful comparison of different AutoRL methods or established hyperparameter optimization (HPO) methods from the automated Machine Learning (AutoML) community.\nTo alleviate these issues, we propose the first tabular AutoRL Benchmark for studying the hyperparameters of RL algorithms. We consider the hyperparameter search spaces of five well established RL methods (PPO, DDPG, A2C, SAC, TD3) across 22 environments for which we compute and provide the reward curves. This enables HPO methods to simply query our benchmark as a lookup table, instead of actually training agents. Thus, our benchmark offers a testbed for very fast, fair, and reproducible experimental protocols for comparing future black-box, gray-box, and online HPO methods for RL."}}
{"id": "rmoMvptXK7M", "cdate": 1663850543251, "mdate": null, "content": {"title": "Gray-Box Gaussian Processes for Automated Reinforcement Learning", "abstract": "Despite having achieved spectacular milestones in an array of important real-world applications, most Reinforcement Learning (RL) methods are very brittle concerning their hyperparameters. Notwithstanding the crucial importance of setting the hyperparameters in training state-of-the-art agents, the task of hyperparameter optimization (HPO) in RL is understudied. In this paper, we propose a novel gray-box Bayesian Optimization technique for HPO in RL, that enriches Gaussian Processes with reward curve estimations based on generalized logistic functions. In a very large-scale experimental protocol, comprising 5 popular RL methods (DDPG, A2C, PPO, SAC, TD3), dozens of environments (Atari, Mujoco), and 7 HPO baselines, we demonstrate that our method significantly outperforms current HPO practices in RL."}}
{"id": "qQghx7f6Qp", "cdate": 1661361914738, "mdate": 1661361914738, "content": {"title": "Automated Reinforcement Learning (AutoRL): A Survey and Open Problems", "abstract": "The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems, while also limits its full potential. In many other areas of machine learning, AutoML has shown it is possible to automate such design choices and has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey we seek to unify the field of AutoRL, we provide a common taxonomy, discuss each area in detail and pose open problems which would be of interest to researchers going forward."}}
{"id": "g_2W9quM4U9", "cdate": 1642089007813, "mdate": 1642089007813, "content": {"title": "DACBench: A Benchmark Library for Dynamic Algorithm Configuration", "abstract": "Dynamic Algorithm Configuration (DAC) aims to\ndynamically control a target algorithm\u2019s hyperpa-\nrameters in order to improve its performance. Sev-\neral theoretical and empirical results have demon-\nstrated the benefits of dynamically controlling hy-\nperparameters in domains like evolutionary com-\nputation, AI Planning or deep learning. Replicat-\ning these results, as well as studying new methods\nfor DAC, however, is difficult since existing bench-\nmarks are often specialized and incompatible with\nthe same interfaces. To facilitate benchmarking and\nthus research on DAC, we propose DACBench, a\nbenchmark library that seeks to collect and stan-\ndardize existing DAC benchmarks from different\nAI domains, as well as provide a template for new\nones. For the design of DACBench, we focused on\nimportant desiderata, such as (i) flexibility, (ii) re-\nproducibility, (iii) extensibility and (iv) automatic\ndocumentation and visualization. To show the po-\ntential, broad applicability and challenges of DAC,\nwe explore how a set of six initial benchmarks com-\npare in several dimensions of difficulty."}}
{"id": "yLemUubApLa", "cdate": 1640995200000, "mdate": 1681712725245, "content": {"title": "Automated Dynamic Algorithm Configuration", "abstract": ""}}
{"id": "tgPK5XXmkWD", "cdate": 1640995200000, "mdate": 1681712725238, "content": {"title": "DeepCAVE: An Interactive Analysis Tool for Automated Machine Learning", "abstract": "Automated Machine Learning (AutoML) is used more than ever before to support users in determining efficient hyperparameters, neural architectures, or even full machine learning pipelines. However, users tend to mistrust the optimization process and its results due to a lack of transparency, making manual tuning still widespread. We introduce DeepCAVE, an interactive framework to analyze and monitor state-of-the-art optimization procedures for AutoML easily and ad hoc. By aiming for full and accessible transparency, DeepCAVE builds a bridge between users and AutoML and contributes to establishing trust. Our framework's modular and easy-to-extend nature provides users with automatically generated text, tables, and graphic visualizations. We show the value of DeepCAVE in an exemplary use-case of outlier detection, in which our framework makes it easy to identify problems, compare multiple runs and interpret optimization processes. The package is freely available on GitHub https://github.com/automl/DeepCAVE."}}
{"id": "iLDAxPKW1jO", "cdate": 1640995200000, "mdate": 1681712724898, "content": {"title": "Theory-inspired parameter control benchmarks for dynamic algorithm configuration", "abstract": "It has long been observed that the performance of evolutionary algorithms and other randomized search heuristics can benefit from a non-static choice of the parameters that steer their optimization behavior. Mechanisms that identify suitable configurations on the fly (\"parameter control\") or via a dedicated training process (\"dynamic algorithm configuration\") are thus an important component of modern evolutionary computation frameworks. Several approaches to address the dynamic parameter setting problem exist, but we barely understand which ones to prefer for which applications. As in classical benchmarking, problem collections with a known ground truth can offer very meaningful insights in this context. Unfortunately, settings with well-understood control policies are very rare. One of the few exceptions for which we know which parameter settings minimize the expected runtime is the LeadingOnes problem. We extend this benchmark by analyzing optimal control policies that can select the parameters only from a given portfolio of possible values. This also allows us to compute optimal parameter portfolios of a given size. We demonstrate the usefulness of our benchmarks by analyzing the behavior of the DDQN reinforcement learning approach for dynamic algorithm configuration."}}
