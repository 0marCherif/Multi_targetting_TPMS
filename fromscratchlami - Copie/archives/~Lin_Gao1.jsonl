{"id": "KZRXTrwIKn", "cdate": 1697631241249, "mdate": 1697631241249, "content": {"title": "Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields", "abstract": "Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF presents fine-detailed and anti-aliased renderings but takes days for training, while Instant-ngp can accomplish the reconstruction in a few minutes but suffers from blurring or aliasing when rendering at various distances or resolutions due to ignoring the sampling area. To this end, we propose a novel Tri-Mip encoding (\u00e0 la \u201cmipmap\u201d) that enables both instant reconstruction and anti-aliased high-fidelity rendering for neural radiance fields. The key is to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can efficiently perform 3D area sampling by taking advantage of 2D pre-filtered feature maps, which significantly elevates the rendering quality without sacrificing efficiency. To cope with the novel Tri-Mip representation, we propose a cone-casting rendering technique to efficiently sample antialiased 3D features with the Tri-Mip encoding considering both pixel imaging and observing distance. Extensive experiments on both synthetic and real-world datasets demonstrate our method achieves state-of-the-art rendering quality and reconstruction speed while maintaining a compact representation that reduces 25% model size compared against Instantngp. Code is available at the project webpage: https: //wbhu.github.io/projects/Tri-MipRF"}}
{"id": "0PRhvgTy0Rv", "cdate": 1696309597591, "mdate": 1696309597591, "content": {"title": "SAC-GAN: Structure-Aware Image Composition", "abstract": "We introduce an end-to-end learning framework for image-to-image composition, aiming to plausibly compose an object represented as a cropped patch from an object image into a background scene image. As our approach emphasizes more on semantic and structural coherence of the composed images, rather than their pixel-level RGB accuracies, we tailor the input and output of our network with structure-aware features and design our network losses accordingly, with ground truth established in a self-supervised setting through the object cropping. Specifically, our network takes the semantic layout features from the input scene image, features encoded from the edges and silhouette in the input object patch, as well as a latent code as inputs, and generates a 2D spatial affine transform defining the translation and scaling of the object patch. The learned parameters are further fed into a differentiable spatial transformer network to transform the object patch into the target image, where our model is trained adversarially using an affine transform discriminator and a layout discriminator. We evaluate our network, coined SAC-GAN, for various image composition scenarios in terms of quality, composability, and generalizability of the composite images. Comparisons are made to state-of-the-art alternatives, including Instance Insertion, ST-GAN, CompGAN and PlaceNet, confirming superiority of our method."}}
{"id": "ZtPEZjrLEXz", "cdate": 1668781327693, "mdate": 1668781327693, "content": {"title": "StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning", "abstract": "3D scene stylization aims at generating stylized images of the scene from arbitrary novel views following a given set of style examples, while ensuring consistency when rendered from different views. Directly applying methods for image or video stylization to 3D scenes cannot achieve such consistency. Thanks to recently proposed neural radiance fields (NeRF), we are able to represent a 3D scene in a consistent way. Consistent 3D scene stylization can be effectively achieved by stylizing the corresponding NeRF. However, there is a  significant domain gap between style examples which are 2D images and NeRF which is an implicit volumetric representation. To address this problem, we propose a novel mutual learning framework for 3D scene stylization that combines a 2D image stylization network and NeRF to fuse the stylization ability of 2D stylization network with the 3D consistency of NeRF. We first pre-train a standard NeRF of the 3D scene to be stylized and replace its color prediction module with a style network to obtain a stylized NeRF. It is followed by distilling the prior knowledge of spatial consistency from NeRF to the 2D stylization network through an introduced consistency loss. We also introduce a mimic loss to supervise the mutual learning of the NeRF style module and fine-tune the 2D stylization decoder. In order to further make our model handle ambiguities of 2D stylization results, we introduce learnable latent codes that obey the probability distributions  conditioned on the style. They are attached to training samples as conditional inputs to better learn the style module in our novel stylized NeRF. Experimental results demonstrate that our method is superior to existing approaches in both visual quality and long-range  consistency."}}
{"id": "Soadfc-JMeX", "cdate": 1652737328346, "mdate": null, "content": {"title": "HSDF: Hybrid Sign and Distance Field for Modeling Surfaces with Arbitrary Topologies", "abstract": "Neural implicit function based on signed distance field (SDF) has achieved impressive progress in reconstructing 3D models with high fidelity. However, such approaches can only represent closed shapes. \nRecent works based on unsigned distance function (UDF) are proposed to handle both watertight and open surfaces. \nNonetheless, as UDF is signless, its direct output is limited to point cloud, which imposes an additional challenge on extracting high-quality meshes from discrete points.\nTo address this issue, we present a new learnable implicit representation, coded HSDF, that connects the good ends of SDF and UDF. In particular, HSDF is able to represent arbitrary topologies containing both closed and open surfaces while being compatible with existing iso-surface extraction techniques for easy field-to-mesh conversion. In addition to predicting a UDF, we propose to learn an additional sign field via a simple classifier. Unlike traditional SDF, HSDF is able to locate the surface of interest before level surface extraction by generating surface points following NDF~\\cite{chibane2020ndf}. We are then able to obtain open surfaces via an adaptive meshing approach that only instantiates regions containing surface into a polygon mesh. We also propose HSDF-Net, a dedicated learning framework that factorizes the learning of HSDF into two easier problems. \nExperiments on multiple datasets show that HSDF outperforms state-of-the-art techniques both qualitatively and quantitatively."}}
{"id": "zvTBIFQ43Sd", "cdate": 1621629671926, "mdate": null, "content": {"title": "OctField: Hierarchical Implicit Functions for 3D Modeling", "abstract": "Recent advances in localized implicit functions have enabled neural implicit representation to be scalable to large scenes.\nHowever, the regular subdivision of 3D space employed by these approaches fails to take into account the sparsity of the surface occupancy and the varying granularities of geometric details. As a result, its memory footprint grows cubically with the input volume, leading to a prohibitive computational cost even at a moderately dense decomposition. In this work, we present a learnable hierarchical implicit representation for 3D surfaces, coded OctField, that allows high-precision encoding of intricate surfaces with low memory and computational budget. The key to our approach is an adaptive decomposition of 3D scenes that only distributes local implicit functions around the surface of interest. We achieve this goal by introducing a hierarchical octree structure to adaptively subdivide the 3D space according to the surface occupancy and the richness of part geometry. As octree is discrete and non-differentiable, we further propose a novel hierarchical network that models the subdivision of octree cells as a probabilistic process and recursively encodes and decodes both octree structure and surface geometry in a differentiable manner. We demonstrate the value of OctField for a range of shape modeling and reconstruction tasks, showing superiority over alternative approaches."}}
{"id": "Bj-ZNJfluaH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Graph CNNs with Motif and Variable Temporal Block for Skeleton-Based Action Recognition.", "abstract": "Hierarchical structure and different semantic roles of joints in human skeleton convey important information for action recognition. Conventional graph convolution methods for modeling skeleton structure consider only physically connected neighbors of each joint, and the joints of the same type, thus failing to capture highorder information. In this work, we propose a novel model with motif-based graph convolution to encode hierarchical spatial structure, and a variable temporal dense block to exploit local temporal information over different ranges of human skeleton sequences. Moreover, we employ a non-local block to capture global dependencies of temporal domain in an attention mechanism. Our model achieves improvements over the stateof-the-art methods on two large-scale datasets."}}
{"id": "rkZjXxMu-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Variational Autoencoders for Deforming 3D Mesh Models", "abstract": "3D geometric contents are becoming increasingly popular. In this paper, we study the problem of analyzing deforming 3D meshes using deep neural networks. Deforming 3D meshes are \ufb02exible to represent 3D animation sequences as well as collections of objects of the same category, allowing diverse shapes with large-scale non-linear deformations. We propose a novel framework which we call mesh variational autoencoders (mesh VAE), to explore the probabilistic latent space of 3D surfaces. The framework is easy to train, and requires very few training examples. We also propose an extended model which allows \ufb02exibly adjusting the signi\ufb01cance of different latent variables by altering the prior distribution. Extensive experiments demonstrate that our general framework is able to learn a reasonable representation for a collection of deformable shapes, and produce competitive results for a variety of applications, including shape generation, shape interpolation, shape space embedding and shape exploration, outperforming state-of-the-art methods."}}
{"id": "HybI7RgO-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Mesh-Based Autoencoders for Localized Deformation Component Analysis", "abstract": "Spatially localized deformation components are very useful\r\nfor shape analysis and synthesis in 3D geometry processing.\r\nSeveral methods have recently been developed, with an aim to\r\nextract intuitive and interpretable deformation components.\r\nHowever, these techniques suffer from fundamental limitations\r\nespecially for meshes with noise or large-scale deformations,\r\nand may not always be able to identify important\r\ndeformation components. In this paper we propose a novel\r\nmesh-based autoencoder architecture that is able to cope with\r\nmeshes with irregular topology. We introduce sparse regularization\r\nin this framework, which along with convolutional operations,\r\nhelps localize deformations. Our framework is capable\r\nof extracting localized deformation components from\r\nmesh data sets with large-scale deformations and is robust to\r\nnoise. It also provides a nonlinear approach to reconstruction\r\nof meshes using the extracted basis, which is more effective\r\nthan the current linear combination approach. Extensive experiments\r\nshow that our method outperforms state-of-the-art\r\nmethods in both qualitative and quantitative evaluations."}}
