{"id": "ojbbv_eQ6U", "cdate": 1683903538757, "mdate": 1683903538757, "content": {"title": "A physics-informed diffusion model for high-fidelity flow field reconstruction", "abstract": "Machine learning models are gaining increasing popularity in the domain of fluid dynamics for their potential to accelerate the production of high-fidelity computational fluid dynamics data. However, many recently proposed machine learning models for high-fidelity data reconstruction require low-fidelity data for model training. Such requirement restrains the application performance of these models, since their data reconstruction accuracy would drop significantly if the low-fidelity input data used in model test has a large deviation from the training data. To overcome this restraint, we propose a diffusion model which only uses high-fidelity data at training. With different configurations, our model is able to reconstruct high-fidelity data from either a regular low-fidelity sample or a randomly measured sample, and is also able to gain an accuracy increase by using physics-informed conditioning information from a known partial differential equation when that is available. Experimental results demonstrate that our model can produce accurate reconstruction results for 2d turbulent flows based on different input sources without retraining."}}
{"id": "Z8m-FLeJehn", "cdate": 1683903453582, "mdate": 1683903453582, "content": {"title": "Graph neural networks accelerated molecular dynamics", "abstract": "Molecular Dynamics (MD) simulation is a powerful tool for understanding the dynamics and structure of matter. Since the resolution of MD is atomic-scale, achieving long timescale simulations with femtosecond integration is very expensive. In each MD step, numerous iterative computations are performed to calculate energy based on different types of interaction and their corresponding spatial gradients. These repetitive computations can be learned and surrogated by a deep learning model, such as a Graph Neural Network (GNN). In this work, we developed a GNN Accelerated MD (GAMD) model that directly predicts forces, given the state of the system (atom positions, atom types), bypassing the evaluation of potential energy. By training the GNN on a variety of data sources (simulation data derived from classical MD and density functional theory), we show that GAMD can predict the dynamics of two typical molecular systems, Lennard-Jones system and water system, in the NVT ensemble with velocities regulated by a thermostat. We further show that GAMD\u2019s learning and inference are agnostic to the scale, where it can scale to much larger systems at test time. We also perform a comprehensive benchmark test comparing our implementation of GAMD to production-level MD software, showing GAMD\u2019s competitive performance on the large-scale simulation."}}
{"id": "ckDca_-ttwg", "cdate": 1683903261550, "mdate": 1683903261550, "content": {"title": "Graph neural network-accelerated Lagrangian fluid simulation", "abstract": "We present a data-driven model for fluid simulation under Lagrangian representation. Our model, Fluid Graph Networks (FGN), uses graphs to represent the fluid field. In FGN, fluid particles are represented as nodes and their interactions are represented as edges. Instead of directly predicting the acceleration or position correction given the current state, FGN decomposes the simulation scheme into separate parts \u2014 advection, collision, and pressure projection. For these different predictions tasks, we propose two kinds of graph neural network structures, node-focused networks and edge-focused networks. We show that the learned model can produce accurate results and remain stable in scenarios with different geometries. In addition, FGN is able to retain many important physical properties of incompressible fluids, such as low velocity divergence, and adapt to time step sizes beyond the one used in the training set. FGN is also computationally efficient compared to classical simulation methods as it operates on a smaller neighborhood and does not require iteration at each timestep during the inference."}}
{"id": "lY0-7bj0Vfz", "cdate": 1632875713104, "mdate": null, "content": {"title": "Prototype memory and attention mechanisms for few shot image generation", "abstract": "Recent discoveries indicate that the neural codes in the primary visual cortex (V1) of macaque monkeys are complex, diverse and sparse. This leads us to ponder the computational advantages and functional role of these \u201cgrandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing within the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized via a memory-based attention operation, which we define as Memory Concept Attention (MoCA). To test our proposal, we show in a few-shot image generation task, that having a prototype memory during attention can improve image synthesis quality, learn interpretable visual concept clusters, as well as improve the robustness of the model. Interestingly, we also find that our attentional memory mechanism can implicitly modify the horizontal connections by updating the transformation into the prototype embedding space for self-attention. Insofar as GANs can be seen as plausible models for reasoning about the top-down synthesis in the analysis-by-synthesis loop of the hierarchical visual cortex, our findings demonstrate a plausible computational role for these \u201cprototype concept\" neurons in visual processing in the brain."}}
{"id": "FEBFJ98FKx", "cdate": 1632875712623, "mdate": null, "content": {"title": "TPU-GAN: Learning temporal coherence from dynamic point cloud sequences", "abstract": "Point cloud sequence is an important data representation that provides flexible shape and motion information. Prior work demonstrates that incorporating scene flow information into loss can make model learn temporally coherent feature spaces. However, it is prohibitively expensive to acquire point correspondence information across frames in real-world environments. In this work, we propose a super-resolution generative adversarial network (GAN) for upsampling dynamic point cloud sequences, which does not require point correspondence annotation.  Our model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, we propose a learnable masking module to adapt upsampling ratio according to the point distribution. We conduct extensive experiments on point cloud sequences from two different domains: particles in the fluid dynamical system and human action scanned data. The quantitative and qualitative evaluation demonstrates the effectiveness of our method on upsampling tasks as well as learning temporal coherence from irregular point cloud sequences."}}
{"id": "7WwYBADS3E_", "cdate": 1601308249028, "mdate": null, "content": {"title": "Learning Lagrangian Fluid Dynamics with Graph Neural Networks", "abstract": "We present a data-driven model for fluid simulation under Lagrangian representation. Our model uses graphs to describe the fluid field, where physical quantities are encoded as node and edge features. Instead of directly predicting the acceleration or position correction given the current state, we decompose the simulation scheme into separate parts - advection, collision, and pressure projection. For these different reasoning tasks, we propose two kinds of graph neural network structures, node-focused networks, and edge-focused networks. By introducing physics prior knowledge, our model can be efficient in terms of training and inference. Our tests show that the learned model can produce accurate results and remain stable in scenarios with a large amount of particles and different geometries. Unlike many previous works, further tests demonstrate that our model is able to retain many important physical properties of incompressible fluids, such as minor divergence and reasonable pressure distribution. Additionally, our model can adopt a range of time step sizes different from ones using in the training set, which indicates its robust generalization capability."}}
