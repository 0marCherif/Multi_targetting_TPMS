{"id": "0ISChqjlrq", "cdate": 1652737574806, "mdate": null, "content": {"title": "Knowledge Distillation: Bad Models Can Be Good Role Models", "abstract": "Large neural networks trained in the overparameterized regime are able to fit noise to zero train error. Recent work of Nakkiran and Bansal has empirically observed that such networks behave as \u201cconditional samplers\u201d from the noisy distribution. That is, they replicate the noise in the train data to unseen examples. We give a theoretical framework for studying this conditional sampling behavior in the context of learning theory. We relate the notion of such samplers to knowledge distillation, where a student network imitates the outputs of a teacher on unlabeled data. We show that samplers, while being bad classifiers, can be good teachers. Concretely, we prove that distillation from samplers is guaranteed to produce a student which approximates the Bayes optimal classifier. Finally, we show that some common learning algorithms (e.g., Nearest-Neighbours and Kernel Machines) can often generate samplers when applied in the overparameterized regime."}}
{"id": "8XWP2ewX-im", "cdate": 1652737567070, "mdate": null, "content": {"title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit", "abstract": "There is mounting evidence of emergent phenomena in the capabilities of deep learning methods as we scale up datasets, model sizes, and training times. While there are some accounts of how these resources modulate statistical capacity, far less is known about their effect on the computational problem of model training. This work conducts such an exploration through the lens of learning a $k$-sparse parity of $n$ bits, a canonical discrete search problem which is statistically easy but computationally hard. Empirically, we find that a variety of neural networks successfully learn sparse parities, with discontinuous phase transitions in the training curves. On small instances, learning abruptly occurs at approximately $n^{O(k)}$ iterations; this nearly matches SQ lower bounds, despite the apparent lack of a sparse prior. Our theoretical analysis shows that these observations are not explained by a Langevin-like mechanism, whereby SGD \"stumbles in the dark\" until it finds the hidden set of features (a natural algorithm which also runs in $n^{O(k)}$ time). Instead, we show that SGD gradually amplifies the sparse solution via a Fourier gap in the population gradient, making continual progress that is invisible to loss and error metrics."}}
{"id": "xsHxlb3ymRt", "cdate": 1640995200000, "mdate": 1684048332755, "content": {"title": "Efficient Learning of CNNs using Patch Based Features", "abstract": "Recent work has demonstrated the effectiveness of using patch based representations when learning from image data. Here we provide theoretical support for this observation, by showing that a simple..."}}
{"id": "HwjFlowtOAF", "cdate": 1640995200000, "mdate": 1684048333494, "content": {"title": "Knowledge Distillation: Bad Models Can Be Good Role Models", "abstract": "Large neural networks trained in the overparameterized regime are able to fit noise to zero train error. Recent work of Nakkiran and Bansal has empirically observed that such networks behave as \u201cconditional samplers\u201d from the noisy distribution. That is, they replicate the noise in the train data to unseen examples. We give a theoretical framework for studying this conditional sampling behavior in the context of learning theory. We relate the notion of such samplers to knowledge distillation, where a student network imitates the outputs of a teacher on unlabeled data. We show that samplers, while being bad classifiers, can be good teachers. Concretely, we prove that distillation from samplers is guaranteed to produce a student which approximates the Bayes optimal classifier. Finally, we show that some common learning algorithms (e.g., Nearest-Neighbours and Kernel Machines) can often generate samplers when applied in the overparameterized regime."}}
{"id": "5hRrF1HM0", "cdate": 1640995200000, "mdate": 1684048333027, "content": {"title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit", "abstract": "There is mounting evidence of emergent phenomena in the capabilities of deep learning methods as we scale up datasets, model sizes, and training times. While there are some accounts of how these resources modulate statistical capacity, far less is known about their effect on the computational problem of model training. This work conducts such an exploration through the lens of learning a $k$-sparse parity of $n$ bits, a canonical discrete search problem which is statistically easy but computationally hard. Empirically, we find that a variety of neural networks successfully learn sparse parities, with discontinuous phase transitions in the training curves. On small instances, learning abruptly occurs at approximately $n^{O(k)}$ iterations; this nearly matches SQ lower bounds, despite the apparent lack of a sparse prior. Our theoretical analysis shows that these observations are not explained by a Langevin-like mechanism, whereby SGD \"stumbles in the dark\" until it finds the hidden set of features (a natural algorithm which also runs in $n^{O(k)}$ time). Instead, we show that SGD gradually amplifies the sparse solution via a Fourier gap in the population gradient, making continual progress that is invisible to loss and error metrics."}}
{"id": "4iQWdZMf32", "cdate": 1640995200000, "mdate": 1684048333044, "content": {"title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit", "abstract": "There is mounting evidence of emergent phenomena in the capabilities of deep learning methods as we scale up datasets, model sizes, and training times. While there are some accounts of how these resources modulate statistical capacity, far less is known about their effect on the computational problem of model training. This work conducts such an exploration through the lens of learning a $k$-sparse parity of $n$ bits, a canonical discrete search problem which is statistically easy but computationally hard. Empirically, we find that a variety of neural networks successfully learn sparse parities, with discontinuous phase transitions in the training curves. On small instances, learning abruptly occurs at approximately $n^{O(k)}$ iterations; this nearly matches SQ lower bounds, despite the apparent lack of a sparse prior. Our theoretical analysis shows that these observations are not explained by a Langevin-like mechanism, whereby SGD \"stumbles in the dark\" until it finds the hidden set of features (a natural algorithm which also runs in $n^{O(k)}$ time). Instead, we show that SGD gradually amplifies the sparse solution via a Fourier gap in the population gradient, making continual progress that is invisible to loss and error metrics."}}
{"id": "3Li0OPkhQU", "cdate": 1632875676258, "mdate": null, "content": {"title": "Provable Learning of Convolutional Neural Networks with Data Driven Features", "abstract": "Convolutional networks (CNN) are computationally hard to learn. In practice, however, CNNs are learned successfully on natural image data. In this work, we study a semi-supervised algorithm, that learns a linear classifier over data-dependent features which were obtained from unlabeled data. We show that the algorithm provably learns CNNs, under some natural distributional assumptions. Specifically, it efficiently learns CNNs, assuming the distribution of patches in the input images has low-dimensional structure (e.g., when the patches are sampled from a low-dimensional manifold).  We complement our result with a lower bound, showing that the dependence of our algorithm on the dimension of the patch distribution is essentially optimal."}}
{"id": "WYrC0Aentah", "cdate": 1621630133175, "mdate": null, "content": {"title": "On the Power of Differentiable Learning versus PAC and SQ Learning", "abstract": "We study the power of learning via mini-batch stochastic gradient descent (SGD) on the loss of a differentiable model or neural network, and ask what learning problems can be learnt using this paradigm. We show that SGD can always simulate\u00a0learning with statistical queries (SQ), but its ability to go beyond that depends on the precision $\\rho$ of the gradients and the minibatch size $b$. With fine enough precision relative to minibatch size, namely when $b \\rho$ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning;\u00a0this extends prior work that achieved this result for $b=1$.\u00a0Moreover,\u00a0with polynomially many bits of precision (i.e. when $\\rho$ is exponentially small), SGD can simulate PAC learning regardless of the batch size. On the other hand, when $b \\rho^2$ is large enough, the power of SGD is equivalent to that of SQ learning."}}
{"id": "TZPidZS3r_z", "cdate": 1621630133175, "mdate": null, "content": {"title": "On the Power of Differentiable Learning versus PAC and SQ Learning", "abstract": "We study the power of learning via mini-batch stochastic gradient descent (SGD) on the loss of a differentiable model or neural network, and ask what learning problems can be learnt using this paradigm. We show that SGD can always simulate\u00a0learning with statistical queries (SQ), but its ability to go beyond that depends on the precision $\\rho$ of the gradients and the minibatch size $b$. With fine enough precision relative to minibatch size, namely when $b \\rho$ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning;\u00a0this extends prior work that achieved this result for $b=1$.\u00a0Moreover,\u00a0with polynomially many bits of precision (i.e. when $\\rho$ is exponentially small), SGD can simulate PAC learning regardless of the batch size. On the other hand, when $b \\rho^2$ is large enough, the power of SGD is equivalent to that of SQ learning."}}
{"id": "olSVLiCXuUm", "cdate": 1609459200000, "mdate": 1684048333016, "content": {"title": "Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels", "abstract": "We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain condition..."}}
