{"id": "mooV4uFZQX", "cdate": 1680307200000, "mdate": 1681703438488, "content": {"title": "Generative Text Convolutional Neural Network for Hierarchical Document Representation Learning", "abstract": "For document analysis, existing methods often resort to the document representation that either discards the word order information or projects each word into a low-dimensional dense embedding vector. However, confined by the data's sparsity and high-dimensionality, limited effort has been made to explore the semantic structures underlying the document representation that formulates each document as a sequence of one-hot vectors, especially in the probabilistic modeling literature. To construct a probabilistic generative model for this type of document representation, we first develop convolutional Poisson factor analysis (CPFA) that not only utilizes the sparse property of data but also enables model parallelism. Through interleaving probabilistic Dirichlet-gamma pooling layers with learnable parameters, we extend the shallow CPFA into a generative text convolutional neural network (GTCNN), which captures richer semantic information with multiple probabilistic convolutional layers and can be coupled with existing deep topic models to alleviate their loss of word order. For efficient and scalable model inference, we not only develop both a parallel upward-downward Gibbs sampler and SG-MCMC based algorithm for training GTCNN, but also construct a hierarchical Weibull convolutional inference network for fast out-of-sample prediction. Experimental results on document representation learning tasks demonstrate the effectiveness of the proposed methods."}}
{"id": "r3-aLHxn2nB", "cdate": 1663850560734, "mdate": null, "content": {"title": "CLEP: Exploiting Edge Partitioning for Graph Contrastive Learning", "abstract": "Generative and contrastive are two fundamental unsupervised approaches to model graph information. The graph generative models extract intra-graph information whereas the graph contrastive learning methods focus on inter-graph information. Combining these complementary sources of information can potentially enhance the expressiveness of graph representations, which, nevertheless, is underinvestigated by existing methods. In this work, we introduce a probabilistic framework called contrastive learning with edge partitioning (CLEP) that integrates generative modeling and graph contrastive learning. CLEP models edge generation by cumulative latent node interactions over multiple mutually independent hidden communities. Inspired by the ``assembly'' behavior of communities in graph generation, CEGCL learns community-specific graph embeddings and assemble them together to represent the entire graph, which are further used to predict the graph's identity via a contrastive objective. To relate each embedding to one hidden community, we define a set of community-specific weighted edges for node feature aggregation by partitioning the observed edges according to the latent node interactions associated with the corresponding hidden community. With these unique designs, CLEP is able to model the statistical dependency among hidden communities, graph structures as well as the identity of each graph; it can also be trained end-to-end via variational inference. We evaluate CLEP on real-world benchmarks under self-supervised and semi-supervised settings and achieve promising results, which demostrate the effectiveness of our method. Various exploratory studies are also conducted to highlight the characteristics of the inferred hidden communities and the potential benefits they bring to representation learning."}}
{"id": "N2AGw9s-wvX", "cdate": 1652737673206, "mdate": null, "content": {"title": "Knowledge-Aware Bayesian Deep Topic Model", "abstract": "We propose a Bayesian generative model for incorporating prior domain knowledge into hierarchical topic modeling. Although embedded topic models (ETMs) and its variants have gained promising performance in text analysis, they mainly focus on mining word co-occurrence patterns, ignoring potentially easy-to-obtain prior topic hierarchies that could help enhance topic coherence. While several knowledge-based topic models have recently been proposed, they are either only applicable to shallow hierarchies or sensitive to the quality of the provided prior knowledge. To this end, we develop a novel deep ETM that jointly models the documents and the given prior knowledge by embedding the words and topics into the same space. Guided by the provided domain knowledge, the proposed model tends to discover topic hierarchies that are organized into interpretable taxonomies. Moreover, with a technique for adapting a given graph, our extended version allows the structure of the prior knowledge to be fine-tuned to match the target corpus. Extensive experiments show that our proposed model efficiently integrates the prior knowledge and improves both hierarchical topic discovery and document representation."}}
{"id": "PfStAhJ2t1g", "cdate": 1652737641605, "mdate": null, "content": {"title": "A Variational Edge Partition Model for Supervised Graph Representation Learning", "abstract": "Graph neural networks (GNNs), which propagate the node features through the edges and learn how to transform the aggregated features under label supervision, have achieved great success in supervised feature extraction for both node-level and graph-level  classification tasks. However, GNNs typically treat the graph structure as given and ignore how the edges are formed. This paper introduces a graph generative process to model how the observed edges are generated by aggregating the node interactions over a set of overlapping node communities, each of which contributes to the edges via a logical OR mechanism. Based on this generative model, we partition each edge into the summation of multiple community-specific weighted edges and use them to define community-specific GNNs. A variational inference framework is proposed to jointly learn a GNN-based inference network  that partitions the edges into different communities, these community-specific GNNs, and a GNN-based predictor that combines community-specific GNNs for the end classification task. Extensive evaluations on real-world graph datasets have verified the effectiveness of the proposed method in learning discriminative representations for both node-level and graph-level classification tasks."}}
{"id": "LKPtAaJcuLx", "cdate": 1652737526024, "mdate": null, "content": {"title": "Alleviating \"Posterior Collapse'' in Deep Topic Models via Policy Gradient", "abstract": "Deep topic models have been proven as a promising way to extract hierarchical latent representations from documents represented as high-dimensional bag-of-words vectors.\nHowever, the representation capability of existing deep topic models is still limited by the phenomenon of \"posterior collapse\", which has been widely criticized in deep generative models, resulting in the higher-level latent representations exhibiting similar or meaningless patterns.\nTo this end, in this paper, we first develop a novel deep-coupling generative process for existing deep topic models, which incorporates skip connections into the generation of documents, enforcing strong links between the document and its multi-layer latent representations.\nAfter that, utilizing data augmentation techniques, we reformulate the deep-coupling generative process as a Markov decision process and develop a corresponding Policy Gradient (PG) based training algorithm, which can further alleviate the information reduction at higher layers.\nExtensive experiments demonstrate that our developed methods can effectively alleviate \"posterior collapse\" in deep topic models, contributing to providing higher-quality latent document representations."}}
{"id": "vMQ1V_z0TxU", "cdate": 1652737385805, "mdate": null, "content": {"title": "Out-of-Distribution Detection with An Adaptive Likelihood Ratio on Informative Hierarchical VAE", "abstract": "Unsupervised out-of-distribution (OOD) detection is essential for the reliability of machine learning. In the literature, existing work has shown that higher-level semantics captured by hierarchical VAEs can be used to detect OOD instances.\nHowever, we empirically show that, the inherent issue of hierarchical VAEs, i.e., ``posterior collapse'', would seriously limit their capacity for OOD detection.\nBased on a thorough analysis for `posterior collapse'', we propose a novel informative hierarchical VAE to alleviate this issue through enhancing the connections between the data sample and its multi-layer stochastic latent representations during training.\nFurthermore, we propose a novel score function for unsupervised OOD detection, referred to as Adaptive Likelihood Ratio. With this score function, one can selectively aggregate the semantic information on multiple hidden layers of hierarchical VAEs, leading to a strong separability between in-distribution and OOD samples. \nExperimental results demonstrate that our method can significantly outperform existing state-of-the-art unsupervised OOD detection approaches."}}
{"id": "jcozJOu5GN1", "cdate": 1640995200000, "mdate": 1681703438488, "content": {"title": "Bayesian Deep Embedding Topic Meta-Learner", "abstract": "Existing deep topic models are effective in capturing the latent semantic structures in textual data but usually rely on a plethora of documents. This is less than satisfactory in practical applica..."}}
{"id": "hTkeGIw-2n", "cdate": 1640995200000, "mdate": 1668430403631, "content": {"title": "Ordinal Graph Gamma Belief Network for Social Recommender Systems", "abstract": "To build recommender systems that not only consider user-item interactions represented as ordinal variables, but also exploit the social network describing the relationships between the users, we develop a hierarchical Bayesian model termed ordinal graph factor analysis (OGFA), which jointly models user-item and user-user interactions. OGFA not only achieves good recommendation performance, but also extracts interpretable latent factors corresponding to representative user preferences. We further extend OGFA to ordinal graph gamma belief network, which is a multi-stochastic-layer deep probabilistic model that captures the user preferences and social communities at multiple semantic levels. For efficient inference, we develop a parallel hybrid Gibbs-EM algorithm, which exploits the sparsity of the graphs and is scalable to large datasets. Our experimental results show that the proposed models not only outperform recent baselines on recommendation datasets with explicit or implicit feedback, but also provide interpretable latent representations."}}
{"id": "UeMA1OfW0x-", "cdate": 1640995200000, "mdate": 1682562910042, "content": {"title": "Infinite Switching Dynamic Probabilistic Network With Bayesian Nonparametric Learning", "abstract": "To model sequentially observed multivariate nonstationary count data, we propose a switching Poisson-gamma dynamical systems (SPGDS), a dynamic probabilistic network with switching mechanism. Different from previous models, SPGDS assigns its latent variables into mixture of gamma distributed parameters to model complex sequences and describe the nonlinear dynamics, meanwhile, capture various temporal dependencies. Moreover, SPGDS can model all discrete and nonnegative real data by linking them to latent counts. To take advantage of Bayesian nonparametrics in handling the unknown number of mixture components, we integrate Dirichlet process (DP) mixture into SPGDS and develop an infinite switching Poisson-gamma dynamical systems (iSPGDS). For efficient and nonparametric inference, we develop a infinite switching recurrent variational inference network, combined with a scalable hybrid stochastic gradient-MCMC and variational inference method, which is scalable to large scale sequences and fast in out-of-sample prediction. Besides, to handle the time-series categorization task, we further propose an supervised attention iSPGDS (attn-iSPGDS), which combines the representation power of iSPGDS, discriminative power of deep neural networks, and selection power of the attention mechanism under a principled probabilistic framework. Experiments on both unsupervised and supervised tasks demonstrate that the proposed model not only has excellent fitting and prediction performance on complex sequences, but also separates different dynamical patterns within them."}}
{"id": "Lycxj59xqLt", "cdate": 1640995200000, "mdate": 1682562910037, "content": {"title": "Multimodal Weibull Variational Autoencoder for Jointly Modeling Image-Text Data", "abstract": "For multimodal representation learning, traditional black-box approaches often fall short of extracting interpretable multilayer hidden structures, which contribute to visualize the connections between different modalities at multiple semantic levels. To extract interpretable multimodal latent representations and visualize the hierarchial semantic relationships between different modalities, based on deep topic models, we develop a novel multimodal Poisson gamma belief network (mPGBN) that tightly couples the observations of different modalities via imposing sparse connections between their modality-specific hidden layers. To alleviate the time-consuming Gibbs sampler adopted by traditional topic models in the testing stage, we construct a Weibull-based variational inference network (encoder) to directly map the observations to their latent representations, and further combine it with the mPGBN (decoder), resulting in a novel multimodal Weibull variational autoencoder (MWVAE), which is fast in out-of-sample prediction and can handle large-scale multimodal datasets. Qualitative evaluations on bimodal data consisting of image-text pairs show that the developed MWVAE can successfully extract expressive multimodal latent representations for downstream tasks like missing modality imputation and multimodal retrieval. Further extensive quantitative results demonstrate that both MWVAE and its supervised extension sMWVAE achieve state-of-the-art performance on various multimodal benchmarks."}}
