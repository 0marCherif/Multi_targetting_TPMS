{"id": "-BJrW413hte", "cdate": 1685532023799, "mdate": null, "content": {"title": "Kernelized Reinforcement Learning with Order Optimal Regret Bounds", "abstract": "Modern reinforcement learning has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modelled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose \n$\\pi$-KRVI, an optimistic modification of least-squares value iteration, when the action-value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\u00e9rn kernels) the existing results lead to trivial (superlinear in the number of episodes) regret bounds. We show a sublinear regret bound that is order optimal in the cases where a lower bound on regret is known (which includes the kernels mentioned above)."}}
{"id": "q6CtXrZQ4k5", "cdate": 1685532017241, "mdate": null, "content": {"title": "First- and Second-Order Bounds for Adversarial Linear Contextual Bandits", "abstract": "We consider the adversarial linear contextual bandit setting, which allows for the loss functions associated with each of $K$ arms to change over time without restriction. Assuming the $d$-dimensional contexts are drawn from a fixed known distribution, the worst-case expected regret over the course of $T$ rounds is known to scale as $\\tilde O(\\sqrt{KdT})$. Under the additional assumption that the density of the contexts is log-concave, we obtain a second-order bound of order $\\tilde O(K\\sqrt{d V_T})$ in terms of the cumulative second moment of the learner's losses $V_T$, and a closely related first-order bound of order $\\tilde O(K\\sqrt{d L_T^*})$ in terms of the cumulative loss of the best policy $L_T^*$. Since $V_T$ or $L_T^*$ may be significantly smaller than $T$, these improve over the worst-case regret whenever the environment is relatively benign. Our results are obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, which we analyse by exploiting a novel connection to the linear bandit setting without contexts."}}
{"id": "rJjJda5q0E", "cdate": 1652737805097, "mdate": null, "content": {"title": "Lifting the Information Ratio: An Information-Theoretic Analysis of Thompson Sampling for Contextual  Bandits", "abstract": "We study the Bayesian regret of the renowned Thompson Sampling algorithm in contextual bandits with binary losses and adversarially-selected contexts. We adapt the information-theoretic perspective of Russo and Van Roy [2016] to the contextual setting by considering a lifted version of the information ratio defined in terms of the unknown model parameter instead of the optimal action or optimal policy as done in previous works on the same setting. This allows us to bound the regret in terms of the entropy of the prior distribution through a remarkably simple proof, and with no structural assumptions on the likelihood or the prior. The extension to priors with infinite entropy only requires a Lipschitz assumption on the log-likelihood. An interesting special case is that of logistic bandits with $d$-dimensional parameters, $K$ actions, and Lipschitz logits, for which we provide a $\\tilde{O}(\\sqrt{dKT})$ regret upper-bound that does not depend on the smallest slope of the sigmoid link function."}}
{"id": "gviX23L1bqw", "cdate": 1621630293778, "mdate": null, "content": {"title": " Online learning in MDPs with linear function approximation and bandit feedback. ", "abstract": "We consider the problem of online learning in an episodic Markov decision process, where the reward function is allowed to change between episodes in an adversarial manner and the learner only observes the rewards associated with its actions. We assume that rewards and the transition function can be represented as linear functions in terms of a known low-dimensional feature map, which allows us to consider the setting where  the state space is arbitrarily large. We also assume that the learner has a perfect knowledge of the MDP dynamics. Our main contribution is developing an algorithm whose expected regret after $T$ episodes is bounded by $\\widetilde{\\mathcal{O}}(\\sqrt{dHT})$, where $H$ is the number of steps in each episode and $d$ is the dimensionality of the feature map."}}
