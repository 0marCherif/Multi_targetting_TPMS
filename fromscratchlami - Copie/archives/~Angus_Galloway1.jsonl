{"id": "Lcj_q7yQF-q", "cdate": 1653750182218, "mdate": null, "content": {"title": "Monitoring Shortcut Learning using Mutual Information", "abstract": "The failure of deep neural networks to generalize to out-of-distribution data is a well-known problem and raises concerns about the deployment of trained networks in safety-critical domains such as healthcare, finance, and autonomous vehicles. We study a particular kind of distribution shift \u2014 shortcuts or spurious correlations in the training data. Shortcut learning is often only exposed when models are evaluated on real-world data that does not contain the same spurious correlations, posing a serious dilemma for AI practitioners to properly assess the effectiveness of a trained model for real-world applications. In this work, we propose to use the mutual information (MI) between the learned representation and the input as a metric to find where in training the network latches onto shortcuts. Experiments demonstrate that MI can be used as a domain-agnostic metric for detecting shortcut learning."}}
{"id": "H1x-3xSKDr", "cdate": 1569439912521, "mdate": null, "content": {"title": "Batch Normalization is a Cause of Adversarial Vulnerability", "abstract": "Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models."}}
{"id": "BkxOwVShhE", "cdate": 1558103135793, "mdate": null, "content": {"title": "Batch Normalization is a Cause of Adversarial Vulnerability", "abstract": "Batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages, as we show on five standard datasets. Furthermore, substituting weight decay for batch norm is sufficient to nullify the relationship between adversarial vulnerability and the input dimension. Our work is consistent with a mean-field analysis that found that batch norm causes exploding gradients."}}
{"id": "HkezfhA5Y7", "cdate": 1538087945618, "mdate": null, "content": {"title": "A Rate-Distortion Theory of Adversarial Examples", "abstract": "The generalization ability of deep neural networks (DNNs) is intertwined with model complexity, robustness, and capacity. Through establishing an equivalence between a DNN and a noisy communication channel, we characterize generalization and fault tolerance for unbounded adversarial attacks in terms of information-theoretic quantities. Invoking rate-distortion theory, we suggest that excess capacity is a significant cause of vulnerability to adversarial examples."}}
{"id": "HkTEFfZRb", "cdate": 1518730161772, "mdate": null, "content": {"title": "Attacking Binarized Neural Networks", "abstract": "Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients."}}
