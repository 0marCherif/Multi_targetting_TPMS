{"id": "bcPPERh6nQ", "cdate": 1672531200000, "mdate": 1677180458786, "content": {"title": "Retiring $\u0394$DP: New Distribution-Level Metrics for Demographic Parity", "abstract": ""}}
{"id": "VtOrFw5C-O", "cdate": 1672531200000, "mdate": 1684177540828, "content": {"title": "Data-centric Artificial Intelligence: A Survey", "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI"}}
{"id": "KiIpaCSyvn", "cdate": 1672531200000, "mdate": 1681137530131, "content": {"title": "Weight Perturbation Can Help Fairness under Distribution Shift", "abstract": ""}}
{"id": "-9H-tWTz58", "cdate": 1672531200000, "mdate": 1684177540784, "content": {"title": "Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs", "abstract": ""}}
{"id": "TVMjn0RpLHf", "cdate": 1663850517487, "mdate": null, "content": {"title": "Topology Matters in Fair Graph Learning: a Theoretical Pilot Study", "abstract": "Recent advances in fair graph learning observe that graph neural networks (GNNs) further amplify prediction bias compared with multilayer perception (MLP), while the reason behind this is unknown. In this paper, we conduct a theoretical analysis of the bias amplification mechanism in GNNs. This is a challenging task since GNNs are difficult to be interpreted, and real-world networks are complex. To bridge the gap, we theoretically and experimentally demonstrate that aggregation operation in representative GNNs accumulates bias in node representation due to topology bias induced by graph topology. We provide a sufficient condition identifying the statistical information of graph data, so that graph aggregation enhances prediction bias in GNNs. \n Motivated by this data-centric finding, we propose a fair graph refinement algorithm, named \\textit{FairGR}, to rewire graph topology to reduce sensitive homophily coefficient while preserving useful graph topology. Experiments on node classification tasks demonstrate that \\textit{FairGR} can mitigate the prediction bias with comparable performance on three real-world datasets. Additionally, \\textit{FairGR} is compatible with many state-of-the-art methods, such as adding regularization, adversarial debiasing, and Fair mixup via refining graph topology. Therefore, \\textit{FairGR} is a plug-in fairness method and can be adapted to improve existing fair graph learning strategies. "}}
{"id": "UntYZBCdypc", "cdate": 1663850440260, "mdate": null, "content": {"title": "Graph Mixup with Soft Alignments", "abstract": "We study graph data augmentation by mixup, which has been used successfully on images. A key operation of mixup is to compute a convex combination of a pair of inputs. This operation is straightforward for grid-like data, such as images, but challenging for graph data. The key difficulty lies in the fact that different graphs typically have different numbers of nodes, and thus there lacks a node-level correspondence between graphs. In this work, we propose a simple yet effective mixup method for graph classification by soft alignments. Specifically, given a pair of graphs, we explicitly obtain node-level correspondence via computing a soft assignment matrix to match the nodes between two graphs. Based on the soft assignments, we transform the adjacency and node feature matrices of one graph, so that the transformed graph is aligned with the other graph. In this way, any pair of graphs can be mixed directly to generate an augmented graph. We conduct systematic experiments to show that our method can improve the performance and generalization of graph neural networks (GNNs) on various graph classification tasks. In addition, we show that our method can increase the robustness of GNNs against noisy labels."}}
{"id": "NGv_ui-1wz", "cdate": 1663850430481, "mdate": null, "content": {"title": "Fair Graph Message Passing with Transparency", "abstract": "Recent advanced works achieve fair representations and predictions through regularization, adversarial debiasing, and contrastive learning in graph neural networks (GNNs). These methods \\textit{implicitly} encode the sensitive attribute information in the well-trained model weight via \\textit{backward propagation}. In practice, we not only pursue a fair machine learning model but also lend such fairness perception to the public. For current fairness methods,\nhow the sensitive attribute information usage makes the model achieve fair prediction still remains a black box. In this work, we first propose the concept \\textit{transparency} to describe \\textit{whether} the model embraces the ability of lending fairness perception to the public \\textit{or not}. Motivated by the fact that current fairness models lack of transparency, we aim to pursue a fair machine learning model with transparency via \\textit{explicitly} rendering sensitive attribute usage for fair prediction in \\textit{forward propagation} . Specifically, we develop an effective and transparent \\textsf{F}air \\textsf{M}essage \\textsf{P}assing (FMP) scheme adopting sensitive attribute information in forward propagation. In this way, FMP explicitly uncovers how sensitive attributes influence final prediction. Additionally, FMP scheme can aggregate useful information from neighbors and mitigate bias in a unified framework to simultaneously achieve graph smoothness and fairness objectives. An acceleration approach is also adopted to improve the efficiency of FMP. Experiments on node classification tasks demonstrate that the proposed FMP outperforms the state-of-the-art baselines in terms of fairness and accuracy on three real-world datasets. The code is available in {\\color{blue}\\url{https://anonymous.4open.science/r/FMP-AD84}}."}}
{"id": "1_OGWcP1s9w", "cdate": 1663850341688, "mdate": null, "content": {"title": "Learning Fair Graph Representations via Automated Data Augmentations", "abstract": "We consider fair graph representation learning via data augmentations. While this direction has been explored previously, existing methods invariably rely on certain assumptions on the properties of fair graph data in order to design fixed strategies on data augmentations. Nevertheless, the exact properties of fair graph data may vary significantly in different scenarios. Hence, heuristically designed augmentations may not always generate fair graph data in different application scenarios. In this work, we propose a method, known as Graphair, to learn fair representations based on automated graph data augmentations. Such fairness-aware augmentations are themselves learned from data. Our Graphair is designed to automatically discover fairness-aware augmentations from input graphs in order to circumvent sensitive information while preserving other useful information. Experimental results demonstrate that our Graphair consistently outperforms many baselines on multiple node classification datasets in terms of fairness-accuracy trade-off performance. In addition, results indicate that Graphair can automatically learn to generate fair graph data without prior knowledge on fairness-relevant graph properties."}}
{"id": "SJO188Y53lk", "cdate": 1663849887259, "mdate": null, "content": {"title": "Do We Really Achieve Fairness with Explicit Sensitive Attributes? ", "abstract": "Recently the wide usage of machine learning models for high-stake decision-making raises the concerns about the fairness and discrimination issue. Existing works found that sensitive information of a sample could be leaked completely by sensitive attributes or partially by non-sensitive attributes, thus removing the sensitive attributes directly from the original features can not achieve fairness. The current fairness practice is to leverage the explicit sensitive attributes (i.e., as regularization) to debias the prediction, based on a strong assumption that non-sensitive attributes of all samples leak the sensitive information totally. However, we investigate the distribution of leaked sensitive information from non-sensitive attributes and make interesting findings that 1) the sensitive information distinctly varies across different samples. 2) the violation of demographic parity for samples prone to leak sensitive information (high-sensitive) are worse than that for low-sensitive samples, indicating the failure of current demographic parity measurements. To this end, we propose a new group fairness ($\\alpha$-Demographic Parity) to measure the demographic parity for samples with different levels of sensitive information leakage. Furthermore, we move one step forward and propose to achieve $\\alpha$-demographic parity by encouraging the independence of the distribution of the sensitive information in non-sensitive attributes and that of  downstream task prediction, which is formulated as a cross-task knowledge distillation framework. Specifically, the sensitive teacher models the distribution of the sensitive information and the fair student models the distribution of the downstream task prediction. Then we encourage the independence between them by minimizing the Hilbert-Schmidt Independence Criterion. Our model can naturally tackle the limited sensitive attribution scenario since the teacher models can be trained with partial samples with sensitive attributes. Extensive experiments show the superior performance of our proposed method on the $\\alpha$-demographic parity and performs well on limited sensitive attribute scenarios."}}
{"id": "6Pv8AMSylux", "cdate": 1663849844472, "mdate": null, "content": {"title": "DIVISION: Memory Efficient Training via Dual Activation Precision", "abstract": "Activation compressed training (ACT) has been shown to be a promising way to reduce the memory cost of training deep neural networks (DNNs). However, existing work of ACT relies on searching for optimal bit-width during DNN training to reduce the quantization noise, which makes the procedure complicated and less transparent. To this end, we propose a simple and effective method to compress DNN training. Our method is motivated by an instructive observation: DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps, while the majority of memory is for caching the high-frequency component (HFC) during the training. This indicates the HFC of activation maps is highly redundant and compressible during DNN training, which inspires our proposed Dual Activation Precision (DIVISION). During the training, DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision. This can significantly reduce the memory cost without negatively affecting the precision of backward propagation such that DIVISION maintains competitive model accuracy. Experimental results show DIVISION achieves over 10\u00d7 compression of activation maps, and significantly higher training throughput than state-of-the-art ACT methods, without loss of model accuracy. The code is available at https://anonymous.4open.science/r/division-5CC0/\n"}}
