{"id": "qbX5VVvjAQ", "cdate": 1672531200000, "mdate": 1681491092139, "content": {"title": "Harnessing Simulation for Molecular Embeddings", "abstract": ""}}
{"id": "dRHyU056E3f", "cdate": 1672531200000, "mdate": 1681491091706, "content": {"title": "To Aggregate or Not? Learning with Separate Noisy Labels", "abstract": ""}}
{"id": "cScb-RrBQC", "cdate": 1664872118483, "mdate": null, "content": {"title": "Fishy: Layerwise Fisher Approximation for Higher-order Neural Network Optimization", "abstract": "We introduce Fishy, a local approximation of the Fisher information matrix at each layer for natural gradient descent training of deep neural networks. The true Fisher approximation for deep networks involves sampling labels from the model's predictive distribution at the output layer and performing a full backward pass -- Fishy defines a Bregman exponential family distribution at each layer, performing the sampling locally. Local sampling allows for model parallelism when forming the preconditioner, removing the need for the extra backward pass. We demonstrate our approach through the Shampoo optimizer, replacing its preconditioner gradients with our locally sampled gradients. Our training results on deep autoencoder and VGG16 image classification models indicate the efficacy of our construction."}}
{"id": "u8kFmPafOc", "cdate": 1664872118365, "mdate": null, "content": {"title": "Fast Implicit Constrained Optimization of Non-decomposable Objectives for Deep Networks", "abstract": "We consider a popular family of constrained optimization problems in machine learning that involve optimizing a non-decomposable objective while constraining another. Different from the previous approach that expresses the classifier thresholds as a function of all model parameters, we consider an alternative strategy where the thresholds are expressed as a function of only a subset of the model parameters, i.e., the last layer of the neural network. We propose new training procedures that optimize for the bottom and last layers separately, and solve them using standard gradient-based methods. Experiments on a benchmark dataset demonstrate our proposed method achieves performance comparable to the existing approach while being computationally efficient."}}
{"id": "3KUfbI9_DQE", "cdate": 1663850022458, "mdate": null, "content": {"title": "Distributionally Robust Post-hoc Classifiers under Prior Shifts", "abstract": "The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization objective is inspired from a natural notion of robustness to controlled distribution shifts. Our method comes with provable guarantees and empirically makes a strong case for distributional robust post-hoc classifiers. An empirical implementation is available at https://github.com/weijiaheng/Drops.\n"}}
{"id": "zC1inM9_ZG2", "cdate": 1640995200000, "mdate": 1681491092210, "content": {"title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate It", "abstract": ""}}
{"id": "fImtcsHezq9", "cdate": 1640995200000, "mdate": 1681491091960, "content": {"title": "Learning from Randomly Initialized Neural Network Features", "abstract": ""}}
{"id": "YLvpb5htEUw", "cdate": 1640995200000, "mdate": 1681491092188, "content": {"title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate It", "abstract": ""}}
{"id": "YBJd-YMSJq", "cdate": 1640995200000, "mdate": 1681491092357, "content": {"title": "Layerwise Bregman Representation Learning with Applications to Knowledge Distillation", "abstract": ""}}
{"id": "VePo1Lidm5", "cdate": 1640995200000, "mdate": 1681491092344, "content": {"title": "Clustering above Exponential Families with Tempered Exponential Measures", "abstract": ""}}
