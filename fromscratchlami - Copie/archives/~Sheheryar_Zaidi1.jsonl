{"id": "KmoOwyzCX_", "cdate": 1664725482310, "mdate": null, "content": {"title": "When Does Re-initialization Work?", "abstract": "Re-initializing a neural network during training has been observed to improve generalization in recent works. Yet it is neither widely adopted in deep learning practice nor is it often used in state-of-the-art training protocols. This raises the question of when re-initialization works, and whether it should be used together with regularization techniques such as data augmentation, weight decay and learning rate schedules. In this work, we conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less sensitive to the choice of learning rate and weight decay hyperparameters. To investigate the impact of re-initialization methods on noisy data, we also consider learning under label noise. Surprisingly, in this case, re-initialization significantly improves upon standard training, even in the presence of other carefully tuned regularization techniques."}}
{"id": "YDXRqKLvfvz", "cdate": 1664248842699, "mdate": null, "content": {"title": "Pre-training via Denoising for Molecular Property Prediction", "abstract": "Many important problems involving molecular property prediction from 3D structures have limited data, posing a generalization challenge for neural networks. In this paper, we describe a pre-training technique based on denoising that achieves a new state-of-the-art in molecular property prediction by utilizing large datasets of 3D molecular structures at equilibrium to learn meaningful representations for downstream tasks. Relying on the well-known link between denoising autoencoders and score-matching, we show that the denoising objective corresponds to learning a molecular force field -- arising from approximating the Boltzmann distribution with a mixture of Gaussians -- directly from equilibrium structures. Our experiments demonstrate that using this pre-training objective significantly improves performance on multiple benchmarks, achieving a new state-of-the-art on the majority of targets in the widely used QM9 dataset. Our analysis then provides practical insights into the effects of different factors -- dataset sizes, model size and architecture, and the choice of upstream/downstream datasets -- on pre-training."}}
{"id": "tYIMtogyee", "cdate": 1663850118786, "mdate": null, "content": {"title": "Pre-training via Denoising for Molecular Property Prediction", "abstract": "Many important problems involving molecular property prediction from 3D structures have limited data, posing a generalization challenge for neural networks. In this paper, we describe a pre-training technique based on denoising that achieves a new state-of-the-art in molecular property prediction by utilizing large datasets of 3D molecular structures at equilibrium to learn meaningful representations for downstream tasks. Relying on the well-known link between denoising autoencoders and score-matching, we show that the denoising objective corresponds to learning a molecular force field -- arising from approximating the Boltzmann distribution with a mixture of Gaussians -- directly from equilibrium structures. Our experiments demonstrate that using this pre-training objective significantly improves performance on multiple benchmarks, achieving a new state-of-the-art on the majority of targets in the widely used QM9 dataset. Our analysis then provides practical insights into the effects of different factors -- dataset sizes, model size and architecture, and the choice of upstream and downstream datasets -- on pre-training."}}
{"id": "va3b82vZPa", "cdate": 1640995200000, "mdate": 1681651557787, "content": {"title": "When Does Re-initialization Work?", "abstract": ""}}
{"id": "jeBKYt3t4Xb", "cdate": 1640995200000, "mdate": 1681651557816, "content": {"title": "Pre-training via Denoising for Molecular Property Prediction", "abstract": ""}}
{"id": "HiYDAwAGWud", "cdate": 1621629980305, "mdate": null, "content": {"title": "Neural Ensemble Search for Uncertainty Estimation and Dataset Shift", "abstract": "Ensembles of neural networks achieve superior performance compared to standalone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. Deep ensembles, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a fixed architecture. Instead, we propose two methods for automatically constructing ensembles with varying architectures, which implicitly trade-off individual architectures\u2019 strengths against the ensemble\u2019s diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: https://github.com/automl/nes"}}
{"id": "wZnZwv8pX3", "cdate": 1609459200000, "mdate": 1681651557937, "content": {"title": "Provably Strict Generalisation Benefit for Equivariant Models", "abstract": ""}}
{"id": "hQnnFgVQKbw", "cdate": 1609459200000, "mdate": 1632955958962, "content": {"title": "LieTransformer: Equivariant Self-Attention for Lie Groups", "abstract": "Group equivariant neural networks are used as building blocks of group invariant neural networks, which have been shown to improve generalisation performance and data efficiency through principled ..."}}
{"id": "b0OfKmrlyY", "cdate": 1609459200000, "mdate": 1681651557867, "content": {"title": "Provably Strict Generalisation Benefit for Equivariant Models", "abstract": ""}}
{"id": "Uu2RBCnDTyp", "cdate": 1609459200000, "mdate": 1681651557868, "content": {"title": "Neural Ensemble Search for Uncertainty Estimation and Dataset Shift", "abstract": ""}}
