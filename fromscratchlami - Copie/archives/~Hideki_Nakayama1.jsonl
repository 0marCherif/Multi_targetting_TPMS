{"id": "ij0-YYq4JjL", "cdate": 1672531200000, "mdate": 1697293807255, "content": {"title": "Indirect Adversarial Losses via an Intermediate Distribution for Training GANs", "abstract": "In this study, we consider the weak convergence characteristics of the Integral Probability Metrics (IPM) methods in training Generative Adversarial Networks (GANs). We first concentrate on a successful IPM-based GAN method that employs a repulsive version of the Maximum Mean Discrepancy (MMD) as the discriminator loss (called repulsive MMD-GAN). We reinterpret its repulsive metrics as an indirect discriminator loss function toward an intermediate distribution. This allows us to propose a novel generator loss via such an intermediate distribution based on our reinterpretation. Our indirect adversarial losses use a simple known distribution (i.e., the Normal or Uniform distribution in our experiments) to simulate indirect adversarial learning between three parts \u2013 real, fake, and intermediate distributions. Furthermore, we found the Kernelized Stein Discrepancy (KSD) from the IPM family as the adversarial loss function to avoid randomness from intermediate distribution samples because the target side (intermediate one) is sample-free in KSD. Experiments on several real-world datasets show that our methods can successfully train GANs with the intermediate-distribution-based KSD and MMD and can outperform previous loss metrics."}}
{"id": "bGGGDivlBb", "cdate": 1672531200000, "mdate": 1697293807231, "content": {"title": "LED: A Dataset for Life Event Extraction from Dialogs", "abstract": ""}}
{"id": "WGtjE3ME_Y", "cdate": 1672531200000, "mdate": 1697293807248, "content": {"title": "Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding", "abstract": ""}}
{"id": "BqJbEjaJzP", "cdate": 1672531200000, "mdate": 1697293807258, "content": {"title": "Handwritten Text Generation with Character-Specific Encoding for Style Imitation", "abstract": "In this paper, we propose a novel method for handwritten text generation that uses a style encoder based on a vision transformer network that encodes handwriting style from reference images and allows the generator to imitate it. The encoder learns to disentangle style information from the content by learning to recognize who wrote the text, and the self-attention mechanism in the encoder allows us to produce character-specific encodings by using characters in the target sequence as queries. Our method can also generate handwritten text images in random styles by sampling random latent vectors instead of encoding style vectors from reference images. We demonstrate through experiments that our proposed method outperforms existing methods for handwritten text generation in terms of the quality of generated images and their fidelity with respect to the distribution of real images. Furthermore, it achieves significantly better performance at imitating handwriting styles defined by reference images. Our model generalizes well to unseen data and can generate handwritten images of words and character sequences as well as imitate handwriting styles not included in the training data."}}
{"id": "oslh1NP4C3", "cdate": 1667462060147, "mdate": 1667462060147, "content": {"title": "PPCD-GAN: Progressive Pruning and Class-aware Distillation for Large-Scale Conditional GANs Compression", "abstract": "We push forward neural network compression research by exploiting a novel challenging task of large-scale conditional generative adversarial networks (GANs) compression. To this end, we propose a gradually shrinking GAN (PPCD-GAN) by introducing progressive pruning residual block (PP-Res) and class-aware distillation. The PP-Res is an extension of the conventional residual block where each convolutional layer is followed by a learnable mask layer to progressively prune network parameters as training proceeds. The class-aware distillation, on the other hand, enhances the stability of training by transferring immense knowledge from a well-trained teacher model through instructive attention maps. We train the pruning and distillation processes simultaneously on a well-known GAN architecture in an end-to-end manner. After training, all redundant parameters as well as the mask layers are discarded, yielding a lighter network while retaining the performance. We comprehensively illustrate, on ImageNet 128 x 128 dataset, PPCD-GAN reduces up to 5.2x (81%) parameters against state-of-the-arts while keeping better performance."}}
{"id": "n5uFDNYNCPR", "cdate": 1667461480954, "mdate": 1667461480954, "content": {"title": "NOC-REK: Novel Object Captioning with Retrieved Vocabulary from External Knowledge ", "abstract": "Novel object captioning aims at describing objects absent from training data, with the key ingredient being the provision of object vocabulary to the model.\nAlthough existing methods heavily rely on an object detection model, we view the detection step as vocabulary retrieval from an external knowledge in the form of embeddings for any object's definition from Wiktionary, where we use in the retrieval image region features learned from a transformers model.\nWe propose an end-to-end Novel Object Captioning with Retrieved vocabulary from External Knowledge method, which simultaneously learns vocabulary retrieval and caption generation, successfully describing novel objects outside of the training dataset.\nFurthermore, our model eliminates the requirement for model retraining by simply updating the external knowledge whenever a novel object appears. \nOur comprehensive experiments on held-out COCO and Nocaps datasets show that our \\ours{} is considerably effective against SOTAs."}}
{"id": "nQtcJ24_45K", "cdate": 1663850480182, "mdate": null, "content": {"title": "MILE: Memory-Interactive Learning Engine for Solving Mathematical Problems", "abstract": "Mathematical problem solving is a task that examines the capacity of machine learning models for performing logical reasoning. Existing work employed formulas as intermediate labels in this task to formulate the computing and reasoning processes and achieved remarkable performance. However, we are questioning the limitations of existing methods from two perspectives: the expressive capacity of formulas and the learning capacity of existing models. In this work, we proposed Memory-Interactive Learning Engine (MILE), a new framework for solving mathematical problems. Our main contribution in this work includes a new formula representing technique and a new decoding method. In our experiment on Math23K dataset, MILE outperformed existing methods on not only question answering accuracy but also robustness and generalization capacity."}}
{"id": "kV0cA81Vau", "cdate": 1663850277551, "mdate": null, "content": {"title": "Server Aggregation as Linear Regression: Reformulation for Federated Learning", "abstract": "We propose a conceptually novel framework for Federated Learning (FL) called FedFit to mitigate issues of FL. FedFit is a reformulation of the server aggregation in FL, where the global model is updated by linear regression. This reformulation naturally enables us to utilize the established linear regression techniques for several FL issues. For example, we apply robust regression to alleviate the vulnerability issue against attacks on the global model from collapsed clients, and we apply LASSO regression to introduce sparsity into the model to reduce the communication cost in FL. Moreover, FedFit enables clients to upload compressed model parameters to the server, significantly reducing the data traffic. In experiments, we demonstrate that FedFit successfully improves robustness against attacks on a global model by robust regression and reduces the global model size by LASSO regression."}}
{"id": "UABiFnzAU67", "cdate": 1661990400000, "mdate": 1697293807224, "content": {"title": "Improving Noised Gradient Penalty with Synchronized Activation Function for Generative Adversarial Networks", "abstract": ""}}
{"id": "wJG2bN6vi18", "cdate": 1640995200000, "mdate": 1668781709063, "content": {"title": "Weakly Supervised Formula Learner for Solving Mathematical Problems", "abstract": ""}}
