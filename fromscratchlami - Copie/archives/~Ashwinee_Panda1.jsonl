{"id": "GkabCYqcpNn", "cdate": 1681058418738, "mdate": 1681058418738, "content": {"title": "Neurotoxin: durable backdoors in federated learning", "abstract": "Due to their decentralized nature, federated learning (FL) systems have an inherent vulnerability during their training to adversarial backdoor attacks. In this type of attack, the goal of the attacker is to use poisoned updates to implant so-called backdoors into the learned model such that, at test time, the model\u2019s outputs can be fixed to a given target for certain inputs. (As a simple toy example, if a user types \u201cpeople from New York\u201d into a mobile keyboard app that uses a backdoored next word prediction model, then the model could autocomplete the sentence to \u201cpeople from New York are rude\u201d). Prior work has shown that backdoors can be inserted into FL models, but these backdoors are often not durable, i.e., they do not remain in the model after the attacker stops uploading poisoned updates. Thus, since training typically continues progressively in production FL systems, an inserted backdoor may not survive until deployment. Here, we propose Neurotoxin, a simple one-line modification to existing backdoor attacks that acts by attacking parameters that are changed less in magnitude during training. We conduct an exhaustive evaluation across ten natural language processing and computer vision tasks, and we find that we can double the durability of state-of-the-art backdoors."}}
{"id": "12IA_9gvFO", "cdate": 1672531200000, "mdate": 1684098450076, "content": {"title": "Differentially Private In-Context Learning", "abstract": "An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (<2\\% degradation) with non-private ICL."}}
{"id": "dN4Y2f_theL", "cdate": 1640995200000, "mdate": 1681496646370, "content": {"title": "Neurotoxin: Durable Backdoors in Federated Learning", "abstract": ""}}
{"id": "NX5C9bsvfN", "cdate": 1640995200000, "mdate": 1684098450078, "content": {"title": "SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification", "abstract": "Federated learning is inherently vulnerable to model poisoning attacks because its decentralized nature allows attackers to participate with compromised devices. In model poisoning attacks, the attacker reduces the model\u2019s performance on targeted sub-tasks (e.g. classifying planes as birds) by uploading \"poisoned\" updates. In this paper we introduce SparseFed, a novel defense that uses global top-k update sparsification and device-level gradient clipping to mitigate model poisoning attacks. We propose a theoretical framework for analyzing the robustness of defenses against poisoning attacks, and provide robustness and convergence analysis of our algorithm. To validate its empirical efficacy we conduct an open-source evaluation at scale across multiple benchmark datasets for computer vision and federated learning."}}
{"id": "M25ieDl-trn", "cdate": 1640995200000, "mdate": 1681669984923, "content": {"title": "DP-RAFT: A Differentially Private Recipe for Accelerated Fine-Tuning", "abstract": "A major direction in differentially private machine learning is differentially private fine-tuning: pretraining a model on a source of \"public data\" and transferring the extracted features to downstream tasks. This is an important setting because many industry deployments fine-tune publicly available feature extractors on proprietary data for downstream tasks. In this paper, we carefully integrate techniques, both new and from prior work, to solve benchmark tasks in computer vision and natural language processing using differentially private fine-tuning. Our key insight is that by accelerating training with the choice of key hyperparameters, we can quickly drive the model parameters to regions in parameter space where the impact of noise is minimized. We obtain new state-of-the art performance on CIFAR10, CIFAR100, FashionMNIST, STL10, and PersonaChat, including $99 \\%$ on CIFAR10 for $\\varepsilon=1, \\delta=1e-5$-DP."}}
{"id": "y2qNb62Vuux", "cdate": 1577836800000, "mdate": null, "content": {"title": "FetchSGD: Communication-Efficient Federated Learning with Sketching", "abstract": "Existing approaches to federated learning suffer from a communication bottleneck as well as convergence issues due to sparse client participation. In this paper we introduce a novel algorithm,calle..."}}
