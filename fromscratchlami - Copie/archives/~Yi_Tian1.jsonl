{"id": "_J4PZTk5L_", "cdate": 1640995200000, "mdate": 1681862088935, "content": {"title": "Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?", "abstract": "We study the task of learning state representations from potentially high-dimensional observations, with the goal of controlling an unknown partially observable system. We pursue a direct latent model learning approach, where a dynamic model in some latent state space is learned by predicting quantities directly related to planning (e.g., costs) without reconstructing the observations. In particular, we focus on an intuitive cost-driven state representation learning method for solving Linear Quadratic Gaussian (LQG) control, one of the most fundamental partially observable control problems. As our main results, we establish finite-sample guarantees of finding a near-optimal state representation function and a near-optimal controller using the directly learned latent model. To the best of our knowledge, despite various empirical successes, prior to this work it was unclear if such a cost-driven latent model learner enjoys finite-sample guarantees. Our work underscores the value of predicting multi-step costs, an idea that is key to our theory, and notably also an idea that is known to be empirically valuable for learning state representations."}}
{"id": "HKLOg2bHVX", "cdate": 1640995200000, "mdate": 1681862088937, "content": {"title": "Byzantine-Robust Federated Linear Bandits", "abstract": "In this paper, we study a linear bandit optimization problem in a federated setting where a large collection of distributed agents collaboratively learn a common linear bandit model. Standard federated learning algorithms applied to this setting are vulnerable to Byzantine attacks on even a small fraction of agents. We propose a novel algorithm with a robust aggregation oracle that utilizes the geometric median. We prove that our proposed algorithm is robust to Byzantine attacks on fewer than half of agents and achieves a sublinear $\\tilde{\\mathcal{O}}({T^{3/4}})$ regret with $\\mathcal{O}(\\sqrt{T})$ steps of communication in $T$ steps. Moreover, we make our algorithm differentially private via a tree-based mechanism. Finally, if the level of corruption is known to be small, we show that using the geometric median of mean oracle for robust aggregation further improves the regret bound."}}
{"id": "Kug2s3rHiG3", "cdate": 1621630326728, "mdate": null, "content": {"title": "Complexity Lower Bounds for Nonconvex-Strongly-Concave Min-Max Optimization", "abstract": "We provide a first-order oracle complexity lower bound for finding stationary points of min-max optimization problems where the objective function is smooth, nonconvex in the minimization variable, and strongly concave in the maximization variable. We establish a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2}\\right)$ for deterministic oracles, where $\\epsilon$ defines the level of approximate stationarity and $\\kappa$ is the condition number. Our lower bound matches the best existing upper bound in the $\\epsilon$ and $\\kappa$ dependence up to logarithmic factors. For stochastic oracles, we provide a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2} + \\kappa^{1/3}\\epsilon^{-4}\\right)$. It suggests that there is a gap between the best existing upper bound $\\mathcal{O}(\\kappa^3 \\epsilon^{-4})$ and our lower bound in the condition number dependence. "}}
{"id": "L_zkHurU6w", "cdate": 1609459200000, "mdate": 1681862088937, "content": {"title": "Provably Efficient Algorithms for Multi-Objective Competitive RL", "abstract": "We study multi-objective reinforcement learning (RL) where an agent\u2019s reward is represented as a vector. In settings where an agent competes against opponents, its performance is measured by the di..."}}
{"id": "K9vYus0xNI", "cdate": 1609459200000, "mdate": 1681862088937, "content": {"title": "Online Learning in Unknown Markov Games", "abstract": "We study online learning in unknown Markov games, a problem that arises in episodic multi-agent reinforcement learning where the actions of the opponents are unobservable. We show that in this chal..."}}
{"id": "HCtQQ88s3_b", "cdate": 1609459200000, "mdate": 1681862088938, "content": {"title": "Complexity Lower Bounds for Nonconvex-Strongly-Concave Min-Max Optimization", "abstract": "We provide a first-order oracle complexity lower bound for finding stationary points of min-max optimization problems where the objective function is smooth, nonconvex in the minimization variable, and strongly concave in the maximization variable. We establish a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2}\\right)$ for deterministic oracles, where $\\epsilon$ defines the level of approximate stationarity and $\\kappa$ is the condition number. Our analysis shows that the upper bound achieved in (Lin et al., 2020b) is optimal in the $\\epsilon$ and $\\kappa$ dependence up to logarithmic factors. For stochastic oracles, we provide a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2} + \\kappa^{1/3}\\epsilon^{-4}\\right)$. It suggests that there is a significant gap between the upper bound $\\mathcal{O}(\\kappa^3 \\epsilon^{-4})$ in (Lin et al., 2020a) and our lower bound in the condition number dependence."}}
{"id": "baKtB72oSzp", "cdate": 1577836800000, "mdate": 1681862088936, "content": {"title": "Towards Minimax Optimal Reinforcement Learning in Factored Markov Decision Processes", "abstract": "We study minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs with conditionally independent transition components. Assuming the factorization is known, we propose two model-based algorithms. The first one achieves minimax optimal regret guarantees for a rich class of factored structures, while the second one enjoys better computational complexity with a slightly worse regret. A key new ingredient of our algorithms is the design of a bonus term to guide exploration. We complement our algorithms by presenting several structure dependent lower bounds on regret for FMDPs that reveal the difficulty hiding in the intricacy of the structures."}}
{"id": "BJbK9CZdWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition", "abstract": "In this paper, we propose a deep progressive reinforcement learning (DPRL) method for action recognition in skeleton-based videos, which aims to distil the most informative frames and discard ambiguous frames in sequences for recognizing actions. Since the choices of selecting representative frames are multitudinous for each video, we model the frame selection as a progressive process through deep reinforcement learning, during which we progressively adjust the chosen frames by taking two important factors into account: (1) the quality of the selected frames and (2) the relationship between the selected frames to the whole video. Moreover, considering the topology of human body inherently lies in a graph-based structure, where the vertices and edges represent the hinged joints and rigid bones respectively, we employ the graph-based convolutional neural network to capture the dependency between the joints for action recognition. Our approach achieves very competitive performance on three widely used benchmarks."}}
{"id": "oOsHngjxCIJD", "cdate": 1483228800000, "mdate": 1668693317851, "content": {"title": "Action recognition in RGB-D egocentric videos", "abstract": ""}}
