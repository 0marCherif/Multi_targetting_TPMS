{"id": "QwjfhENnXc", "cdate": 1697616252015, "mdate": 1697616252015, "content": {"title": "Small data well-posedness for derivative nonlinear Schr\u00f6dinger equations", "abstract": "We study the generalized derivative nonlinear Schr\u00f6dinger equation i\u2202tu+\u0394u=P(u,u\u00af\u00af\u00af,\u2202xu,\u2202xu\u00af\u00af\u00af), where P is a polynomial, in Sobolev spaces. It turns out that when deg P\u22653, the equation is locally well-posed in H12 when each term in P contains only one derivative, otherwise we have a local well-posedness in H32. If deg P\u22655, the solution can be extended globally. By restricting to equations of the form i\u2202tu+\u0394u=\u2202xP(u,u\u00af\u00af\u00af) with deg P\u22655, we were able to obtain the global well-posedness in the critical Sobolev space."}}
{"id": "fHEjBHnJEP", "cdate": 1697615955899, "mdate": 1697615955899, "content": {"title": "Universal consistency of Wasserstein k-NN classifier: a negative and some positive results", "abstract": "We study the k-nearest neighbour classifier (\u2060k-NN) of probability measures under the Wasserstein distance. We show that the k-NN classifier is not universally consistent on the space of measures supported in (0,1)\u2060. As any Euclidean ball contains a copy of (0,1)\u2060, one should not expect to obtain universal consistency without some restriction on the base metric space, or the Wasserstein space itself. To this end, via the notion of \u03c3-finite metric dimension, we show that the k-NN classifier is universally consistent on spaces of discrete measures (and more generally, \u03c3-finite uniformly discrete measures) with rational mass. In addition, by studying the geodesic structures of the Wasserstein spaces for p=1 and p=2\u2060, we show that the k-NN classifier is universally consistent on spaces of measures supported on a finite set, the space of Gaussian measures and spaces of measures with finite wavelet series densities."}}
{"id": "SL6J18F-lsT", "cdate": 1697615846555, "mdate": null, "content": {"title": "Uniform Confidence Band for Optimal Transport Map on One-Dimensional Data", "abstract": "We develop a statistical inference method for an optimal transport map between distributions on real numbers with uniform confidence bands. The concept of optimal transport (OT) is used to measure distances between distributions, and OT maps are used to construct the distance. OT has been applied in many fields in recent years, and its statistical properties have attracted much interest. In particular, since the OT map is a function, a uniform norm-based statistical inference is significant for visualization and interpretation. In this study, we derive a limit distribution of a uniform norm of an estimation error for the OT map, and then develop a uniform confidence band based on it. In addition to our limit theorem, we develop a smoothed bootstrap method with its validation and guarantee on an asymptotic coverage probability of the confidence band. Our proof is based on the functional delta method and the representation of OT maps on the reals."}}
{"id": "tM7jZWwM5A", "cdate": 1672531200000, "mdate": 1696062164421, "content": {"title": "Dirichlet Mechanism for Differentially Private KL Divergence Minimization", "abstract": "Given an empirical distribution $f(x)$ of sensitive data $x$, we consider the task of minimizing $F(y) = D_{\\text{KL}} (f(x)\\Vert y)$ over a probability simplex, while protecting the privacy of $x$. We observe that, if we take the exponential mechanism and use the KL divergence as the loss function, then the resulting algorithm is the $Dirichlet\\text{ }mechanism$ that outputs a single draw from a Dirichlet distribution. Motivated by this, we propose a R\u00e9nyi differentially private (RDP) algorithm that employs the Dirichlet mechanism to solve the KL divergence minimization task. In addition, given $f(x)$ as above and $\\hat{y}$ an output of the Dirichlet mechanism, we prove a probability tail bound on $D_{\\text{KL}} (f(x)\\Vert \\hat{y})$, which is then used to derive a lower bound for the sample complexity of our RDP algorithm. Experiments on real-world datasets demonstrate advantages of our algorithm over Gaussian and Laplace mechanisms in supervised classification and maximum likelihood estimation."}}
{"id": "fgBxcKcz9ea", "cdate": 1621630226470, "mdate": null, "content": {"title": "Differential Privacy of Dirichlet Posterior Sampling", "abstract": "We study the inherent privacy of releasing a single sample from a Dirichlet posterior distribution. As a complement to the previous study that provides general theories on the differential privacy of posterior sampling from exponential families, this study focuses specifically on the Dirichlet posterior sampling and its privacy guarantees. With the notion of truncated concentrated differential privacy (tCDP), we are able to derive a simple privacy guarantee of the Dirichlet posterior sampling, which effectively allows us to analyze its utility in various settings. Specifically, we provide accuracy guarantees of the Dirichlet posterior sampling in Multinomial-Dirichlet sampling and private normalized histogram publishing."}}
{"id": "Pk972F7J8Um", "cdate": 1621630226470, "mdate": null, "content": {"title": "Differential Privacy of Dirichlet Posterior Sampling", "abstract": "We study the inherent privacy of releasing a single sample from a Dirichlet posterior distribution. As a complement to the previous study that provides general theories on the differential privacy of posterior sampling from exponential families, this study focuses specifically on the Dirichlet posterior sampling and its privacy guarantees. With the notion of truncated concentrated differential privacy (tCDP), we are able to derive a simple privacy guarantee of the Dirichlet posterior sampling, which effectively allows us to analyze its utility in various settings. Specifically, we provide accuracy guarantees of the Dirichlet posterior sampling in Multinomial-Dirichlet sampling and private normalized histogram publishing."}}
