{"id": "rMcZ0-PUbDe", "cdate": 1668681213873, "mdate": 1668681213873, "content": {"title": "BABEL: Bodies, Action and Behavior With English Labels", "abstract": "Understanding the semantics of human movement -- the what, how and why of the movement -- is an important problem that requires datasets of human actions with semantic labels. Existing datasets take one of two approaches. Large-scale video datasets contain many action labels but do not contain ground-truth 3D human motion. Alternatively, motion-capture (mocap) datasets have precise body motions but are limited to a small number of actions. To address this, we present BABEL, a large dataset with language labels describing the actions being performed in mocap sequences. BABEL consists of action labels for about 43.5 hours of mocap sequences from AMASS. Action labels are at two levels of abstraction -- sequence labels which describe the overall action in the sequence, and frame labels which describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. There are over 28k sequence labels, and 63k frame labels in BABEL, which belong to over 250 unique action categories. Labels from BABEL can be leveraged for tasks like action recognition, temporal action localization, motion synthesis, etc. To demonstrate the value of BABEL as a benchmark, we evaluate the performance of models on 3D action recognition. We demonstrate that BABEL poses interesting learning challenges that are applicable to real-world scenarios, and can serve as a useful benchmark of progress in 3D action recognition. The dataset, baseline method, and evaluation code is made available, and supported for academic research purposes at https://babel.is.tue.mpg.de/."}}
{"id": "ouGgEnlMI8V", "cdate": 1640995200000, "mdate": 1654092016218, "content": {"title": "LocATe: End-to-end Localization of Actions in 3D with Transformers", "abstract": "Understanding a person's behavior from their 3D motion is a fundamental problem in computer vision with many applications. An important component of this problem is 3D Temporal Action Localization (3D-TAL), which involves recognizing what actions a person is performing, and when. State-of-the-art 3D-TAL methods employ a two-stage approach in which the action span detection task and the action recognition task are implemented as a cascade. This approach, however, limits the possibility of error-correction. In contrast, we propose LocATe, an end-to-end approach that jointly localizes and recognizes actions in a 3D sequence. Further, unlike existing autoregressive models that focus on modeling the local context in a sequence, LocATe's transformer model is capable of capturing long-term correlations between actions in a sequence. Unlike transformer-based object-detection and classification models which consider image or patch features as input, the input in 3D-TAL is a long sequence of highly correlated frames. To handle the high-dimensional input, we implement an effective input representation, and overcome the diffuse attention across long time horizons by introducing sparse attention in the model. LocATe outperforms previous approaches on the existing PKU-MMD 3D-TAL benchmark (mAP=93.2%). Finally, we argue that benchmark datasets are most useful where there is clear room for performance improvement. To that end, we introduce a new, challenging, and more realistic benchmark dataset, BABEL-TAL-20 (BT20), where the performance of state-of-the-art methods is significantly worse. The dataset and code for the method will be available for research purposes."}}
{"id": "tEn_W-5S6TK", "cdate": 1609459200000, "mdate": 1654092016220, "content": {"title": "How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI", "abstract": "Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, Peter Clark. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "tC3ajAasbdc", "cdate": 1609459200000, "mdate": 1654092016220, "content": {"title": "BABEL: Bodies, Action and Behavior With English Labels", "abstract": "Understanding the semantics of human movement -- the what, how and why of the movement -- is an important problem that requires datasets of human actions with semantic labels. Existing datasets take one of two approaches. Large-scale video datasets contain many action labels but do not contain ground-truth 3D human motion. Alternatively, motion-capture (mocap) datasets have precise body motions but are limited to a small number of actions. To address this, we present BABEL, a large dataset with language labels describing the actions being performed in mocap sequences. BABEL consists of language labels for over 43 hours of mocap sequences from AMASS, containing over 250 unique actions. Each action label in BABEL is precisely aligned with the duration of the corresponding action in the mocap sequence. BABEL also allows overlap of multiple actions, that may each span different durations. This results in a total of over 66000 action segments. The dense annotations can be leveraged for tasks like action recognition, temporal localization, motion synthesis, etc. To demonstrate the value of BABEL as a benchmark, we evaluate the performance of models on 3D action recognition. We demonstrate that BABEL poses interesting learning challenges that are applicable to real-world scenarios, and can serve as a useful benchmark for progress in 3D action recognition. The dataset, baseline methods, and evaluation code are available and supported for academic research purposes at https://babel.is.tue.mpg.de/."}}
{"id": "gzhlqmjpih", "cdate": 1601908503774, "mdate": null, "content": {"title": "Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings", "abstract": "Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a subset that is maximally-informative via active learning (AL). In this work, we study the problem of AL under a domain shift. We empirically demonstrate how existing AL approaches based solely on model uncertainty or representative sampling are suboptimal for active domain adaptation.\n\nOur algorithm, Active Domain Adaptation via CLustering Uncertainty-weighted Embeddings (ADA-CLUE), i) identifies diverse datapoints for labeling that are both uncertain under the model and representative of unlabeled target data, and ii) leverages the available source and target data for adaptation by optimizing a semisupervised adversarial entropy loss that is complimentary to our active sampling objective. On standard image classification benchmarks for domain adaptation, ADA-CLUE consistently performs as well or better than competing active adaptation, active learning, and domain adaptation methods across shift severities, model\ninitializations, and labeling budgets."}}
{"id": "wt_JnQnRmU3", "cdate": 1601908329200, "mdate": null, "content": {"title": "Evaluating visual conversational agents via cooperative human-ai games", "abstract": "As AI continues to advance, human-AI teams are inevitable.\nHowever, progress in AI is routinely measured in isolation,\nwithout a human in the loop. It is crucial to benchmark\nprogress in AI, not just in isolation, but also in terms of how\nit translates to helping humans perform certain tasks, i.e., the\nperformance of human-AI teams.\nIn this work, we design a cooperative game \u2013 GuessWhich \u2013\nto measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the\nAI. The AI, which we call ALICE, is provided an image which\nis unseen by the human. Following a brief description of the\nimage, the human questions ALICE about this secret image to\nidentify it from a fixed pool of images.\nWe measure performance of the human-ALICE team by the\nnumber of guesses it takes the human to correctly identify the\nsecret image after a fixed number of dialog rounds with ALICE. We compare performance of the human-ALICE teams for\ntwo versions of ALICE. Our human studies suggest a counterintuitive trend \u2013 that while AI literature shows that one version outperforms the other when paired with an AI questioner\nbot, we find that this improvement in AI-AI performance does\nnot translate to improved human-AI performance. This suggests a mismatch between benchmarking of AI in isolation\nand in the context of human-AI teams."}}
{"id": "ryVmuzMuWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Do explanations make VQA models more predictable to a human?", "abstract": ""}}
{"id": "r1W9FXbu-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Punny Captions: Witty Wordplay in Image Descriptions", "abstract": "Wit is a quintessential form of rich inter-human interaction, and is often grounded in a specific situation (e.g., a comment in response to an event). In this work, we attempt to build computational models that can produce witty descriptions for a given image. Inspired by a cognitive account of humor appreciation, we employ linguistic wordplay, specifically puns. We compare our approach against meaningful baseline approaches via human studies. In a Turing test style evaluation, people find our model's description for an image to be wittier than a human's witty description 55% of the time!"}}
{"id": "ry4nFMG_bS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Sort Story: Sorting Jumbled Images and Captions into Stories", "abstract": "Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense."}}
{"id": "HyZuJ6bdWr", "cdate": 1451606400000, "mdate": null, "content": {"title": "We are Humor Beings: Understanding and Predicting Visual Humor", "abstract": "Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question \u2013 what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available."}}
