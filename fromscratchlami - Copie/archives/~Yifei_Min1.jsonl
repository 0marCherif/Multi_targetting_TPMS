{"id": "N229V-3-Ln", "cdate": 1683904724428, "mdate": 1683904724428, "content": {"title": "Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation", "abstract": "Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a more important question: Can we really handle imbalanced samples to yield better performance? Hence, the key innovation in ACTION is to learn global semantic relationship across the entire dataset and local anatomical features among the neighbouring pixels with minimal additional memory footprint. During the training, we introduce anatomical contrast by actively sampling a sparse set of hard negative pixels, which can generate smoother segmentation boundaries and more accurate predictions. Extensive experiments across two benchmark datasets and different unlabeled settings show that ACTION significantly outperforms the current state-of-the-art semi-supervised methods."}}
{"id": "QkSNcO9Vfo", "cdate": 1678895513001, "mdate": 1678895513001, "content": {"title": "Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation", "abstract": "Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a more important question: Can we really handle imbalanced samples to yield better performance? Hence, the key innovation in ACTION is to learn global semantic relationship across the entire dataset and local anatomical features among the neighbouring pixels with minimal additional memory footprint. During the training, we introduce anatomical contrast by actively sampling a sparse set of hard negative pixels, which can generate smoother segmentation boundaries and more accurate predictions. Extensive experiments across two benchmark datasets and different unlabeled settings show that ACTION significantly outperforms the current state-of-the-art semi-supervised methods."}}
{"id": "PPbRFR9ENek", "cdate": 1668544246639, "mdate": 1668544246639, "content": {"title": "Learn to Match with No Regret: Reinforcement Learning in Markov Matching Markets", "abstract": "We study a Markov matching market involving a planner and a set of strategic agents on the two sides of the market.\nAt each step, the agents are presented with a dynamical context, where the contexts determine the utilities. \nThe planner controls the transition of the contexts to maximize the cumulative social welfare, while the agents aim to find a myopic stable matching at each step. Such a setting captures a range of applications including ridesharing platforms. We formalize the problem by proposing a reinforcement learning framework that integrates optimistic value iteration with maximum weight matching. \nThe proposed algorithm addresses the coupled challenges of sequential exploration, matching stability, and function approximation. We prove that the algorithm achieves sublinear regret. "}}
{"id": "PbkBDQ5_UbV", "cdate": 1663849945386, "mdate": null, "content": {"title": "Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes", "abstract": "We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \\underline{P}roxy variable \\underline{P}essimistic \\underline{P}olicy \\underline{O}ptimization (\\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \\texttt{P3O} is a coupled sequence of pessimistic confidence regions  constructed via proximal causal inference, which is  formulated as  minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \\texttt{P3O} achieves a $n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the dataset. To our best knowledge, \\texttt{P3O} is the first provably efficient offline RL algorithm for POMDPs with a confounded dataset."}}
{"id": "R3JMyR4MvoU", "cdate": 1652737487025, "mdate": null, "content": {"title": "Learn to Match with No Regret: Reinforcement Learning in Markov Matching Markets", "abstract": "We study a Markov matching market involving a planner and a set of strategic agents on the two sides of the market.\nAt each step, the agents are presented with a dynamical context, where the contexts determine the utilities. \nThe planner controls the transition of the contexts to maximize the cumulative social welfare, while the agents aim to find a myopic stable matching at each step. Such a setting captures a range of applications including ridesharing platforms. We formalize the problem by proposing a reinforcement learning framework that integrates optimistic value iteration with maximum weight matching. \nThe proposed algorithm addresses the coupled challenges of sequential exploration, matching stability, and function approximation. We prove that the algorithm achieves sublinear regret. "}}
{"id": "Fx7oXUVEPW", "cdate": 1652737483227, "mdate": null, "content": {"title": "A Simple and Provably Efficient Algorithm for Asynchronous Federated Contextual Linear Bandits", "abstract": "We study federated contextual linear bandits, where $M$ agents cooperate with each other to solve a global contextual linear bandit problem with the help of a central server. We consider the asynchronous setting, where all agents work independently and the communication between one agent and the server will not trigger other agents' communication. We propose a simple algorithm named FedLinUCB based on the principle of optimism. We prove that the regret of FedLinUCB is bounded by $\\widetilde{\\mathcal{O}}(d\\sqrt{\\sum_{m=1}^M T_m})$ and the communication complexity is $\\widetilde{O}(dM^2)$, where $d$ is the dimension of the contextual vector and $T_m$ is the total number of interactions with the environment by agent $m$. To the best of our knowledge, this is the first provably efficient algorithm that allows fully asynchronous communication for federated linear bandits, while achieving the same regret guarantee as in the single-agent setting. "}}
{"id": "HtUeCI4Vf75", "cdate": 1648669781543, "mdate": 1648669781543, "content": {"title": "Learn to Match with No Regret: Reinforcement Learning in Markov Matching Markets", "abstract": "We study a Markov matching market involving a planner and a set of strategic agents on the two sides of the market. At each step, the agents are presented with a dynamical context, where the contexts determine the utilities. The planner controls the transition of the contexts to maximize the cumulative social welfare, while the agents aim to find a myopic stable matching at each step. Such a setting captures a range of applications including ridesharing platforms. We formalize the problem by proposing a reinforcement learning framework that integrates optimistic value iteration with maximum weight matching. The proposed algorithm addresses the coupled challenges of sequential exploration, matching stability, and function approximation. We prove that the algorithm achieves sublinear regret."}}
{"id": "rETYiFiaQm", "cdate": 1640995200000, "mdate": 1668618172813, "content": {"title": "A Simple and Provably Efficient Algorithm for Asynchronous Federated Contextual Linear Bandits", "abstract": "We study federated contextual linear bandits, where $M$ agents cooperate with each other to solve a global contextual linear bandit problem with the help of a central server. We consider the asynchronous setting, where all agents work independently and the communication between one agent and the server will not trigger other agents' communication. We propose a simple algorithm named \\texttt{FedLinUCB} based on the principle of optimism. We prove that the regret of \\texttt{FedLinUCB} is bounded by $\\tilde{O}(d\\sqrt{\\sum_{m=1}^M T_m})$ and the communication complexity is $\\tilde{O}(dM^2)$, where $d$ is the dimension of the contextual vector and $T_m$ is the total number of interactions with the environment by $m$-th agent. To the best of our knowledge, this is the first provably efficient algorithm that allows fully asynchronous communication for federated contextual linear bandits, while achieving the same regret guarantee as in the single-agent setting."}}
{"id": "qY_grr3r6l", "cdate": 1640995200000, "mdate": 1668618172592, "content": {"title": "Learn to Match with No Regret: Reinforcement Learning in Markov Matching Markets", "abstract": "We study a Markov matching market involving a planner and a set of strategic agents on the two sides of the market. At each step, the agents are presented with a dynamical context, where the contexts determine the utilities. The planner controls the transition of the contexts to maximize the cumulative social welfare, while the agents aim to find a myopic stable matching at each step. Such a setting captures a range of applications including ridesharing platforms. We formalize the problem by proposing a reinforcement learning framework that integrates optimistic value iteration with maximum weight matching. The proposed algorithm addresses the coupled challenges of sequential exploration, matching stability, and function approximation. We prove that the algorithm achieves sublinear regret."}}
{"id": "bjL-Ag1xyP7", "cdate": 1640995200000, "mdate": 1668618173031, "content": {"title": "Learning Stochastic Shortest Path with Linear Function Approximation", "abstract": "We study the stochastic shortest path (SSP) problem in reinforcement learning with linear function approximation, where the transition kernel is represented as a linear mixture of unknown models. W..."}}
