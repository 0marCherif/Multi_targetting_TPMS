{"id": "vrYUWBETYxS", "cdate": 1640995200000, "mdate": 1666172515661, "content": {"title": "Fine Tuning of Deep Contexts Toward Improved Perceptual Quality of In-Paintings", "abstract": "Over the recent years, a number of deep learning approaches are successfully introduced to tackle the problem of image in-painting for achieving better perceptual effects. However, there still exist obvious hole-edge artifacts in these deep learning-based approaches, which need to be rectified before they become useful for practical applications. In this article, we propose an iteration-driven in-painting approach, which combines the deep context model with the backpropagation mechanism to fine-tune the learning-based in-painting process and hence, achieves further improvement over the existing state of the arts. Our iterative approach fine tunes the image generated by a pretrained deep context model via backpropagation using a weighted context loss. Extensive experiments on public available test sets, including the CelebA, Paris Streets, and PASCAL VOC 2012 dataset, show that our proposed method achieves better visual perceptual quality in terms of hole-edge artifacts compared with the state-of-the-art in-painting methods using various context models."}}
{"id": "WCLsecSX3Oa", "cdate": 1640995200000, "mdate": 1666172515663, "content": {"title": "Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning", "abstract": "Thanks to the efficient retrieval speed and low storage consumption, learning to hash has been widely used in visual retrieval tasks. However, existing hashing methods assume that the query and retrieval samples lie in homogeneous feature space within the same domain. As a result, they cannot be directly applied to heterogeneous cross-domain retrieval. In this paper, we propose a Generalized Image Transfer Retrieval (GITR) problem, which encounters two crucial bottlenecks: 1) the query and retrieval samples may come from different domains, leading to an inevitable {domain distribution gap}; 2) the features of the two domains may be heterogeneous or misaligned, bringing up an additional {feature gap}. To address the GITR problem, we propose an Asymmetric Transfer Hashing (ATH) framework with its unsupervised/semi-supervised/supervised realizations. Specifically, ATH characterizes the domain distribution gap by the discrepancy between two asymmetric hash functions, and minimizes the feature gap with the help of a novel adaptive bipartite graph constructed on cross-domain data. By jointly optimizing asymmetric hash functions and the bipartite graph, not only can knowledge transfer be achieved but information loss caused by feature alignment can also be avoided. Meanwhile, to alleviate negative transfer, the intrinsic geometrical structure of single-domain data is preserved by involving a domain affinity graph. Extensive experiments on both single-domain and cross-domain benchmarks under different GITR subtasks indicate the superiority of our ATH method in comparison with the state-of-the-art hashing methods."}}
{"id": "Oy58qtUmn_", "cdate": 1609459200000, "mdate": null, "content": {"title": "Real-time video super resolution network using recurrent multi-branch dilated convolutions", "abstract": "Highlights \u2022 A new multi-branch dilated module to effectively improve network receptive field. \u2022 Extracting spatial-temporal features at different scales in parallel. \u2022 Resulting in superior performance with minimal computational costs. \u2022 A new recurrent architecture to process a consecutive multi-frame sequence. \u2022 Extracting temporal and spatial features simultaneously for super-resolution. \u2022 The proposed network can reconstruct high definition video clip up to 50 fps. Abstract Recent developments of video super-resolution reconstruction often exploit spatial and temporal contexts from input frame sequence by making use of explicit motion estimation, e.g., optical flow, which may introduce accumulated errors and requires huge computations to obtain an accurate estimation. In this paper, we propose a novel multi-branch dilated convolution module for real-time frame alignment without explicit motion estimation, which is incorporated with the depthwise separable up-sampling module to formulate a sophisticated real-time video super-resolution network. Specifically, the proposed video super-resolution framework can efficiently acquire a larger receptive field and learn spatial\u2013temporal features of multiple scales with minimal computational operations and memory requirements. Extensive experiments show that the proposed super-resolution network outperforms current state-of-the-art real-time video super-resolution networks, e.g., VESPCN and 3DVSRnet, in terms of PSNR values (0.49\u00a0dB and 0.17\u00a0dB) on average in various datasets, but requires less multiplication operations."}}
{"id": "3_3ya3QsVt", "cdate": 1609459200000, "mdate": null, "content": {"title": "Real-time video super-resolution using lightweight depthwise separable group convolutions with channel shuffling", "abstract": "Highlights \u2022 This work proposes a new network architecture for video super-resolution. \u2022 Depthwise and pointwise group convolution for frame alignment and generation. \u2022 Frames alignment without explicit motion compensation to save computations. \u2022 Extensive ablation studies to verify the benefits of the proposed framework. \u2022 Outperform real-time SOTA in terms of calculations, running time and parameters. Abstract In recent years, convolutional neural networks (CNNs) have accelerated the developments of video super resolution (SR) for achieving higher image quality. However, the computational cost of existing CNN-based video super-resolution is too heavy for real-time applications. In this paper, we propose a new video super-resolution framework using lightweight frame alignment module and well-designed up-sampling module for real-time processing. Specifically, our framework, which is called as Lightweight Shuffle Video Super-Resolution Network (LSVSR), combines channel shuffling, depthwise convolution and pointwise group convolution to significantly reduce the computational burden during frame alignment and high-resolution frame reconstruction. On the public benchmark datasets, our proposed network outperforms the state-of-the-art lightweight video SR networks in terms of objective (PSNR and SSIM) and subjective evaluations, number of network parameters and floating-point operations. Our network can achieve real-time 540P to 2160P 4 \u00d7 super-resolution for more than 60fps using desktop GPUs or mobile phones with neural processing unit."}}
{"id": "z2k-RHmWsp", "cdate": 1546300800000, "mdate": null, "content": {"title": "Robust Image Completion via Deep Feature Transformations", "abstract": "For many practical applications, it is essential to address both geometric corrections and missing information reconstruction of face images and natural images. However, it is unfavorable to separate the problem into two sub-tasks due to error accumulations of sequential tasks. In this paper, we propose a novel robust missing information reconstruction framework via deep feature transformations to simultaneously address both geometric corrections and image completion. Specifically, our proposed framework realizes multiple channel spatial transformations to tackle geometric corrections, and address image completion through non-linear features projections. The flow of our framework includes deep feature extraction, feature enhancement, feature projection, and feature refinement, where deep features are extracted and learnt to achieve robust image completion. Experimental results show the superior performance of our framework for both face images and natural images in various databases. Compared with the conventional approaches approach to split the problem into two sub-tasks, including image inpainting and spatial transformation, our proposed framework achieves a number of advantages, including i) an unified framework to automatically correct the geometric distortions and to reconstruct the missing information simultaneously and ii) achieving much better visual quality for those recovered images."}}
{"id": "sNCiuqBDARh", "cdate": 1546300800000, "mdate": null, "content": {"title": "Video Super Resolution via Deep Global-Aware Network", "abstract": "Video super-resolution aims to increase the resolution of videos by exploiting the intra-frame and inter-frame dependencies of the low-resolution video sequences. There are usually two dependent steps in the video super-resolution: the motion compensation and the super-resolution reconstruction. In this paper, we propose a new deep learning framework without the explicit motion estimation by utilizing the self-attention model to exploit the full receptive field of the input video frames. In other words, the proposed deep neural network extracts the local features at all spatial-temporal locations for combining into global features using the self-attention networks in order to reconstruct the high-resolution video frame. The proposed global-aware network outperforms the state-of-the-art deep learning-based image and video super-resolution algorithms in terms of subjective and objective quality with less computational operations, as verified by extensive experiments on public image and video datasets, including Set5, Set14, B100, Urban100, and Vid4."}}
{"id": "_YV6WD3sYVj", "cdate": 1546300800000, "mdate": null, "content": {"title": "Real-Time Image Super-Resolution Using Recursive Depthwise Separable Convolution Network", "abstract": "In recent years, deep convolutional neural networks (CNNs) have been widely used for image super-resolution (SR) to achieve a range of sophisticated performances. Despite the significant advancement made in CNNs, it is still difficult to apply CNNs to practical SR applications due to enormous computations of deep convolutions. In this paper, we propose two lightweight deep neural networks using depthwise separable convolution for the real-time image SR. Specifically, depthwise separable convolution divides the standard convolution into depthwise convolution and pointwise convolution to significantly reduce the number of model parameters and multiplication operations. Moreover, recursive learning is adopted to increase the depth and receptive field of the network in order to improve the SR quality without increasing the model parameters. Finally, we propose a novel technique called Super-Sampling (SS) to learn more abundant high-resolution information by over-sampling the output image followed by adaptive down-sampling. The proposed two models, named SSNet-M and SSNet, outperform the existing state-of-the-art real-time image SR networks, including SRCNN, FSRCNN, ESPCN, and VDSR, in terms of model complexity, and subjective and PSNR/SSIM evaluations on Set5, Set14, B100, Urban100, and Manga109."}}
{"id": "SGg-vNN0VvT", "cdate": 1546300800000, "mdate": null, "content": {"title": "Spatial Transformer Generative Adversarial Network for Robust Image Super-Resolution", "abstract": "Recently, there have been significant advances in image super-resolution based on generative adversarial networks (GANs) to achieve breakthroughs in generating more images with high subjective quality. However, there are remaining challenges needs to be met, such as simultaneously recovering the finer texture details for large upscaling factors and mitigating the geometric transformation effects. In this paper, we propose a novel robust super-resolution GAN (i.e. namely RSR-GAN) which can simultaneously perform both the geometric transformation and recovering the finer texture details. Specifically, since the performance of the generator depends on the discreminator, we propose a novel discriminator design by incorporating the spatial transformer module with residual learning to improve the discrimination of fake and true images through removing the geometric noise, in order to enhance the super-resolution of geometric corrected images. Finally, to further improve the perceptual quality, we introduce an additional DCT loss term into the existing loss function. Extensive experiments, measured by both PSNR and SSIM measurements, show that our proposed method achieves a high level of robustness against a number of geometric transformations, including rotation, translation, a combination of rotation and scaling effects, and a cobmination of rotaion, transalation and scaling effects. Benchmarked by the existing state-of-the-arts SR methods, our proposed delivers superior performances on a wide range of datasets which are publicly available and widely adopted across research communities."}}
{"id": "FRy612UwYVr", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution", "abstract": "In general, existing research on single image super-resolution does not consider the practical application that, when image transmission is over noisy channels, the effect of any possible geometric transformations could incur significant quality loss and distortions. To address this problem, we present a new and robust super-resolution method in this paper, where a robust spatially-transformed deep learning framework is established to simultaneously perform both the geometric transformation and the single image super-resolution. The proposed seamlessly integrates deep residual learning based spatial transform module with a very deep super-resolution module to achieve a robust and improved single image super-resolution. In comparison with the existing state of the arts, our proposed robust single image super-resolution has a number of novel features, including 1) content-characterized deep features are extracted out of the input LR images to identify the incurred geometric transformations, and hence transformation parameters can be optimized to influence and control the super-resolution process; 2) the effects of any geometric transformations can be automatically corrected at the output without compromise on the quality of final super-resolved images; and 3) compared with the existing research reported in the literature, our proposed achieves the advantage that HR images can be recovered from those down-sampled LR images corrupted by a number of different geometric transformations. The extensive experiments, measured by both the peak-signal-to-noise-ratio and the similar structure index measurement, show that our proposed method achieves a high level of robustness against a number of geometric transformations, including scaling, translations, and rotations. Benchmarked by the existing state-of-the-arts SR methods, our proposed delivers superior performances on a wide range of datasets which are publicly available and widely adopted across relevant research communities."}}
{"id": "FH7u5AtWekK", "cdate": 1546300800000, "mdate": null, "content": {"title": "Image interpolation using convolutional neural networks with deep recursive residual learning", "abstract": "Recent developments of image super-resolution often utilize the deep convolutional neural network (CNN) and residual learning to relate the observed low-resolution pixels and unknown high-resolution pixels. However, image interpolation assumes that the observed image was directly down-sampled without low-pass filtering, such that the aliased down-sampled low-resolution image exhibits jags and chaos that cannot be easily modeled by conventional residual learning in super-resolution. In this paper, we propose a new framework to exploit the residual dense network using hierarchical levels of recursive residual learning and densely connected convolutional layers for image interpolation. The proposed deep recursive network iteratively reconstructs hierarchical levels of image details for aliased and discontinuous residual of interpolated pixels. Experimental results on popular Set16, Set18, and Urban12 image datasets show that the proposed method outperforms state-of-the-art image interpolation methods using local and nonlocal autoregressive models, random forests and deep CNN, in terms of PSNR (0.27\u20131.57 dB gain), SSIM and subjective evaluations. More importantly, model parameters of the proposed method are significantly less than that of existing deep CNN for image interpolation."}}
