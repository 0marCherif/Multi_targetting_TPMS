{"id": "oXAoI3RPea", "cdate": 1672531200000, "mdate": 1681841668135, "content": {"title": "Hidet: Task-Mapping Programming Paradigm for Deep Learning Tensor Programs", "abstract": "As deep learning models nowadays are widely adopted by both cloud services and edge devices, reducing the latency of deep learning model inferences becomes crucial to provide efficient model serving. However, it is challenging to develop efficient tensor programs for deep learning operators due to the high complexity of modern accelerators (e.g., NVIDIA GPUs and Google TPUs) and the rapidly growing number of operators. Deep learning compilers, such as Apache TVM, adopt declarative scheduling primitives to lower the bar of developing tensor programs. However, we show that this approach is insufficient to cover state-of-the-art tensor program optimizations (e.g., double buffering). In this paper, we propose to embed the scheduling process into tensor programs and use dedicated mappings, called task mappings, to define the computation assignment and ordering directly in the tensor programs. This new approach greatly enriches the expressible optimizations by allowing developers to manipulate tensor programs at a much finer granularity (e.g., allowing program-statement-level optimizations). We call the proposed method the task-mapping programming paradigm. In addition, we propose a new post-scheduling fusion optimization that allows developers to focus on scheduling every single operator and automates the fusion after scheduling. It greatly reduces the engineering efforts for operator fusion. Our proposed paradigm also constructs an efficient hardware-centric schedule space, which is agnostic to the program input size and greatly reduces the tuning time. With the proposed paradigm, we implement a deep learning compiler Hidet. Extensive experiments on modern convolution and transformer models show that Hidet outperforms state-of-the-art DNN inference framework, ONNX Runtime, and compiler, TVM equipped with scheduler AutoTVM and Ansor, by up to 1.48x (1.22x on average). It also reduces the tuning time by 20x and 11x compared with AutoTVM and Ansor, respectively. We open-sourced hidet at https://www.github.com/hidet-org/hidet."}}
{"id": "RhzgiLaer-", "cdate": 1672531200000, "mdate": 1682343099242, "content": {"title": "TiLT: A Time-Centric Approach for Stream Query Optimization and Parallelization", "abstract": "Stream processing engines (SPEs) are widely used for large scale streaming analytics over unbounded time-ordered data streams. Modern day streaming analytics applications exhibit diverse compute characteristics and demand strict latency and throughput requirements. Over the years, there has been significant attention in building hardware-efficient stream processing engines (SPEs) that support several query optimization, parallelization, and execution strategies to meet the performance requirements of large scale streaming analytics applications. However, in this work, we observe that these strategies often fail to generalize well on many real-world streaming analytics applications due to several inherent design limitations of current SPEs. We further argue that these limitations stem from the shortcomings of the fundamental design choices and the query representation model followed in modern SPEs. To address these challenges, we first propose TiLT, a novel intermediate representation (IR) that offers a highly expressive temporal query language amenable to effective query optimization and parallelization strategies. We subsequently build a compiler backend for TiLT that applies such optimizations on streaming queries and generates hardware-efficient code to achieve high performance on multi-core stream query executions. We demonstrate that TiLT achieves up to 326x (20.49x on average) higher throughput compared to state-of-the-art SPEs (e.g., Trill) across eight real-world streaming analytics applications. TiLT source code is available at https://github.com/ampersand-projects/tilt.git."}}
{"id": "OXHSRjSz2e", "cdate": 1672531200000, "mdate": 1682343098413, "content": {"title": "TiLT: A Time-Centric Approach for Stream Query Optimization and Parallelization", "abstract": "Stream processing engines (SPEs) are widely used for large scale streaming analytics over unbounded time-ordered data streams. Modern day streaming analytics applications exhibit diverse compute characteristics and demand strict latency and throughput requirements. Over the years, there has been significant attention in building hardware-efficient stream processing engines (SPEs) that support several query optimization, parallelization, and execution strategies to meet the performance requirements of large scale streaming analytics applications. However, in this work, we observe that these strategies often fail to generalize well on many real-world streaming analytics applications due to several inherent design limitations of current SPEs. We further argue that these limitations stem from the shortcomings of the fundamental design choices and the query representation model followed in modern SPEs. To address these challenges, we first propose TiLT, a novel intermediate representation (IR) that offers a highly expressive temporal query language amenable to effective query optimization and parallelization strategies. We subsequently build a compiler backend for TiLT that applies such optimizations on streaming queries and generates hardware-efficient code to achieve high performance on multi-core stream query executions. We demonstrate that TiLT achieves up to 326\u00d7 (20.49\u00d7 on average) higher throughput compared to state-of-the-art SPEs (e.g., Trill) across eight real-world streaming analytics applications. TiLT source code is available at https://github.com/ampersand-projects/tilt.git."}}
{"id": "xqyEG7EhTZ", "cdate": 1652737776382, "mdate": null, "content": {"title": "Tempo: Accelerating Transformer-Based Model Training through Memory Footprint Reduction", "abstract": "Training deep learning models can be computationally expensive. Prior works have shown that increasing the batch size can potentially lead to better overall throughput. However, the batch size is frequently limited by the accelerator memory capacity due to the activations/feature maps stored for the training backward pass, as larger batch sizes require larger feature maps to be stored. Transformer-based models, which have recently seen a surge in popularity due to their good performance and applicability to a variety of tasks, have a similar problem. To remedy this issue, we propose Tempo, a new approach to efficiently use accelerator (e.g., GPU) memory resources for training Transformer-based models. Our approach provides drop-in replacements for the GELU, LayerNorm, and Attention layers, reducing the memory usage and ultimately leading to more efficient training. We implement Tempo and evaluate the throughput, memory usage, and accuracy/loss on the BERT Large pre-training task. We demonstrate that Tempo enables up to 2\u00d7 higher batch sizes and 16% higher training throughput over the state-of-the-art baseline. We also evaluate Tempo on GPT2 and RoBERTa models, showing 19% and 26% speedup over the baseline."}}
{"id": "nLfikpyFypt", "cdate": 1640995200000, "mdate": 1682343099289, "content": {"title": "Pavise: Integrating Fault Tolerance Support for Persistent Memory Applications", "abstract": "Persistent memory (PM) allows programmers to bypass the file system and efficiently manage persistent data directly. As a consequence, the application is now responsible for a non-trivial task---maintaining data crash consistency. In addition, it is highly desirable for today's production-grade storage systems to have fault tolerance to restore from data corruptions. Systems may provide fault tolerance through data redundancy. However, direct PM accesses bypass the system and make the data vulnerable to corruption. Without system-level support, it is the application's responsibility to maintain both crash consistency and fault tolerance, creating a demand for software tools to alleviate the burden from the application programmer. Providing fault tolerance is challenging in the absence of system support because it is difficult to track data updates and efficiently update data along with its redundancy in a crash-consistent manner. Existing fault-tolerant mechanisms for PM applications either impose significant programming restrictions to the programmer or compromise on the level of protection they provide. This paper designs and implements Pavise1, a software framework that provides protection for data within PM applications. Pavise uses a compiler pass to automatically track accesses to persistent data. It co-designs fault tolerance operations with the crash consistency mechanism to efficiently update data and its redundancy while maintaining the crash consistency guarantee. Pavise can be easily applied to existing PM applications with minimal manual effort and modest overheads. Our evaluation of common PM applications shows that Pavise achieves 83.2% (with ignore-list) and 70.9% (with conservative tracking) performance of the current state-of-the-art fault-tolerance software system, Pangolin. Because Pavise provides both application and library data with equally strong protection, Pavise can sustain a much higher error rate of 10\u22125 as compared to Pangolin's 10\u22127."}}
{"id": "d5BIYuZOag", "cdate": 1640995200000, "mdate": 1682343099186, "content": {"title": "Optimizing Data Collection in Deep Reinforcement Learning", "abstract": "Reinforcement learning (RL) workloads take a notoriously long time to train due to the large number of samples collected at run-time from simulators. Unfortunately, cluster scale-up approaches remain expensive, and commonly used CPU implementations of simulators induce high overhead when switching back and forth between GPU computations. We explore two optimizations that increase RL data collection efficiency by increasing GPU utilization: (1) GPU vectorization: parallelizing simulation on the GPU for increased hardware parallelism, and (2) simulator kernel fusion: fusing multiple simulation steps to run in a single GPU kernel launch to reduce global memory bandwidth requirements. We find that GPU vectorization can achieve up to $1024\\times$ speedup over commonly used CPU simulators. We profile the performance of different implementations and show that for a simple simulator, ML compiler implementations (XLA) of GPU vectorization outperform a DNN framework (PyTorch) by $13.4\\times$ by reducing CPU overhead from repeated Python to DL backend API calls. We show that simulator kernel fusion speedups with a simple simulator are $11.3\\times$ and increase by up to $1024\\times$ as simulator complexity increases in terms of memory bandwidth requirements. We show that the speedups from simulator kernel fusion are orthogonal and combinable with GPU vectorization, leading to a multiplicative speedup."}}
{"id": "buqEpibqz0", "cdate": 1640995200000, "mdate": 1682343098498, "content": {"title": "Hidet: Task Mapping Programming Paradigm for Deep Learning Tensor Programs", "abstract": "As deep learning models nowadays are widely adopted by both cloud services and edge devices, reducing the latency of deep learning model inferences becomes crucial to provide efficient model serving. However, it is challenging to develop efficient tensor programs for deep learning operators due to the high complexity of modern accelerators and the rapidly growing number of operators. Deep learning compilers, such as Apache TVM, adopt declarative scheduling primitives to lower the bar of developing tensor programs. However, we show that this approach is insufficient to cover state-of-the-art tensor program optimizations. In this paper, we propose to embed the scheduling process into tensor programs and use dedicated mappings, called task mappings, to define the computation assignment and ordering. This new approach greatly enriches the expressible optimizations by allowing developers to manipulate tensor programs at a much finer granularity. We call the proposed method the task-mapping programming paradigm. In addition, we propose a new post-scheduling fusion optimization that allows developers to focus on scheduling every single operator and automates the fusion after scheduling. It greatly reduces the engineering efforts for operator fusion. Our proposed paradigm also constructs an efficient hardware-centric schedule space, which is agnostic to the program input size and greatly reduces the tuning time. With the proposed paradigm, we implement a deep learning compiler Hidet. Extensive experiments on modern convolution and transformer models show that Hidet outperforms state-of-the-art DNN inference framework, ONNX Runtime, and compiler, TVM equipped with scheduler AutoTVM and Ansor, by up to 1.48x (1.22x on average). It also reduces the tuning time by 20x and 11x compared with AutoTVM and Ansor, respectively. We open-sourced hidet at https://www.github.com/hidet-org/hidet."}}
{"id": "_sIYwJEKL2", "cdate": 1640995200000, "mdate": 1682343098343, "content": {"title": "DietCode: Automatic Optimization for Dynamic Tensor Programs", "abstract": ""}}
{"id": "Rm2qKD8MmTe", "cdate": 1640995200000, "mdate": 1682343099190, "content": {"title": "Tempo: Accelerating Transformer-Based Model Training through Memory Footprint Reduction", "abstract": "Training deep learning models can be computationally expensive. Prior works have shown that increasing the batch size can potentially lead to better overall throughput. However, the batch size is frequently limited by the accelerator memory capacity due to the activations/feature maps stored for the training backward pass, as larger batch sizes require larger feature maps to be stored. Transformer-based models, which have recently seen a surge in popularity due to their good performance and applicability to a variety of tasks, have a similar problem. To remedy this issue, we propose Tempo, a new approach to efficiently use accelerator (e.g., GPU) memory resources for training Transformer-based models. Our approach provides drop-in replacements for the GELU, LayerNorm, and Attention layers, reducing the memory usage and ultimately leading to more efficient training. We implement Tempo and evaluate the throughput, memory usage, and accuracy/loss on the BERT Large pre-training task. We demonstrate that Tempo enables up to 2x higher batch sizes and 16% higher training throughput over the state-of-the-art baseline. We also evaluate Tempo on GPT2 and RoBERTa models, showing 19% and 26% speedup over the baseline."}}
{"id": "RcO4tU3KvR5", "cdate": 1640995200000, "mdate": 1682343098431, "content": {"title": "GPUPool: A Holistic Approach to Fine-Grained GPU Sharing in the Cloud", "abstract": "As Graphics Processing Units (GPUs) evolved into popular hardware accelerators for many compute-hungry applications in the cloud, GPU virtualization has become a highly desirable feature to lower operating costs of cloud infrastructures. In this work, we explore fine-grained GPU sharing, which has a larger potential in improving hardware utilization compared to a coarse-grained approach adopted in Nvidia's latest virtualization solution. However, fine-grained sharing introduces workload-dependent interference among concurrent applications, which leads to performance unpredictability and thus makes it extremely challenging to provide quality-of-service (QoS) guarantees in a cloud environment. To this end, we present GPUPool, which (i) unlocks the performance potential of fine-grained GPU sharing through mitigating interference and (ii) provides QoS guarantees to GPU workloads at both the systems and hardware levels. By leveraging our key insight that co-execution performance of GPU jobs is highly correlated with hardware utilization of individual workloads, GPUPool predicts performance of co-running incoming jobs and assigns compatible GPU jobs onto the same hardware. Given this performance estimation, GPUPool is able to satisfy job-specific QoS requirements while maximizing hardware sharing across a GPU cluster. Our experimental results show GPUPool reduces the number of GPUs required to meet QoS requirements of incoming jobs by 31.2% and 21.1%, compared to a system with no sharing and one with coarse-grained sharing respectively."}}
