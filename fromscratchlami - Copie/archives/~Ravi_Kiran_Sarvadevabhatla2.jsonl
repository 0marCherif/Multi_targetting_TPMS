{"id": "YTmVh6j6N0J", "cdate": 1700282223494, "mdate": 1700282223494, "content": {"title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization", "abstract": "State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB\nand Flow modalities, leaving the information-rich audio\nmodality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of\ntrimmed (clip-level) action recognition. However, TAL\nposes a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL.\nTo the best of our knowledge, our work is the first to jointly\nconsider audio and video modalities for supervised TAL. We\nexperimentally show that our schemes consistently improve\nperformance for state of the art video-only TAL approaches.\nSpecifically, they help achieve new state of the art performance on large-scale benchmark datasets - ActivityNet1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5).\nOur experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data are available\nat https://github.com/skelemoa/tal-hmo"}}
{"id": "aTV4oSdolPj", "cdate": 1672531200000, "mdate": 1700836079492, "content": {"title": "Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation", "abstract": "We introduce Action-GPT, a plug-and-play framework for incorporating Large Language Models (LLMs) into text-based action generation models. Action phrases in current motion capture datasets contain minimal and to-the-point information. By carefully crafting prompts for LLMs, we generate richer and fine-grained descriptions of the action. We show that utilizing these detailed descriptions instead of the original action phrases leads to better alignment of text and motion spaces. We introduce a generic approach compatible with stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion models. In addition, the approach enables multiple text descriptions to be utilized. Our experiments show (i) noticeable qualitative and quantitative improvement in the quality of synthesized motions, (ii) benefits of utilizing multiple LLM-generated descriptions, (iii) suitability of the prompt function, and (iv) zero-shot generation capabilities of the proposed approach. Code and pretrained models are available at https://actiongpt.github.io."}}
{"id": "_KSWL3yNM9", "cdate": 1672531200000, "mdate": 1700836079507, "content": {"title": "SeamFormer: High Precision Text Line Segmentation for Handwritten Documents", "abstract": "Historical manuscripts often contain dense unstructured text lines. The large diversity in sizes, scripts and appearance makes precise text line segmentation extremely challenging. Existing line segmentation approaches often associate diacritic elements incorrectly to text lines and also address above mentioned challenges inadequately. To tackle these issues, we introduce SeamFormer, a novel approach for high precision text line segmentation in handwritten manuscripts. In the first stage of our approach, a multi-task Transformer deep network outputs coarse line identifiers which we term \u2018scribbles\u2019 and the binarized manuscript image. In the second stage, a scribble-conditioned seam generation procedure utilizes outputs from first stage and feature maps derived from manuscript image to generate tight-fitting line segmentation polygons. In the process, we incorporate a novel diacritic feature map which enables improved diacritic and text line associations. Via experiments and evaluations on new and existing challenging palm leaf manuscript datasets, we show that SeamFormer outperforms competing approaches and generates precise text line segmentations."}}
{"id": "PP9dVsBhDwe", "cdate": 1672531200000, "mdate": 1700836079501, "content": {"title": "\"Draw Fast, Guess Slow\": Characterizing Interactions in Cooperative Partially Observable Settings with Online Pictionary as a Case Study", "abstract": "Cooperative human-human communication becomes challenging when restrictions such as difference in communication modality and limited time are imposed. We use the popular cooperative social game Pictionary as an online multimodal test bed to explore the dynamics of human-human interactions in such settings. As a part of our study, we identify attributes of player interactions that characterize cooperative gameplay. We found stable and role-specific playing style components that are independent of game difficulty. In terms of gameplay and the larger context of cooperative partially observable communication, our results suggest that too much interaction or unbalanced interaction negatively impacts game success. Additionally, the playing style components discovered via our analysis align with select player personality types proposed in existing frameworks for multiplayer games."}}
{"id": "N15kdGmucCq", "cdate": 1672531200000, "mdate": 1700836079517, "content": {"title": "A Cloud-Fog Architecture for Video Analytics on Large Scale Camera Networks Using Semantic Scene Analysis", "abstract": "This paper proposes a scalable distributed video analytics framework that can process thousands of video streams from sources such as CCTV cameras using semantic scene analysis. The main idea is to deploy deep learning pipelines on the fog nodes and generate semantic scene description records (SDRs) of video feeds from the associated CCTV cameras. These SDRs are transmitted to the cloud instead of video frames saving on network bandwidth. Using these SDRs stored on the cloud database, we can answer many complex queries and perform rich video analytics, within extremely low latencies. There is no need to scan and process the video streams again on a per query basis. The software architecture on the fog nodes allows for integrating new deep learning pipelines dynamically into the existing system, thereby supporting novel analytics and queries. We demonstrate the effectiveness of the system by proposing a novel distributed algorithm for real-time vehicle pursuit. The proposed algorithm involves asking multiple spatio-temporal queries in an adaptive fashion to reduce the query processing time and is robust to inaccuracies in the deployed deep learning pipelines and camera failures."}}
{"id": "6O-9yU2kq7W", "cdate": 1672531200000, "mdate": 1682384750216, "content": {"title": "DSAG: A Scalable Deep Framework for Action-Conditioned Multi-Actor Full Body Motion Synthesis", "abstract": "We introduce DSAG, a controllable deep neural framework for action-conditioned generation of full body multiactor variable duration actions. To compensate for incompletely detailed finger joints in existing large-scale datasets, we introduce full body dataset variants with detailed finger joints. To overcome shortcomings in existing generative approaches, we introduce dedicated representations for encoding finger joints. We also introduce novel spatiotemporal transformation blocks with multi-head self attention and specialized temporal processing. The design choices enable generations for a large range in body joint counts (24 - 52), frame rates (13 - 50), global body movement (inplace, locomotion) and action categories (12 - 120), across multiple datasets (NTU-120, HumanAct12, UESTC, Human3.6M). Our experimental results demonstrate DSAG\u2019s significant improvements over state-of-the-art, its suitability for action-conditioned generation at scale."}}
{"id": "GbQy6uBaUh", "cdate": 1668798127387, "mdate": 1668798127387, "content": {"title": "NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions", "abstract": "The lack of fine-grained joints (facial joints, hand fingers) is a\nfundamental performance bottleneck for state of the art skeleton\naction recognition models. Despite this bottleneck, community\u2019s\nefforts seem to be invested only in coming up with novel architectures. To specifically address this bottleneck, we introduce two\nnew pose based human action datasets - NTU60-X and NTU120-X.\nOur datasets extend the largest existing action recognition dataset,\nNTU-RGBD. In addition to the 25 body joints for each skeleton as in\nNTU-RGBD, NTU60-X and NTU120-X dataset includes finger and\nfacial joints, enabling a richer skeleton representation. We appropriately modify the state of the art approaches to enable training using\nthe introduced datasets. Our results demonstrate the effectiveness\nof these NTU-X datasets in overcoming the aforementioned bottleneck and improve state of the art performance, overall and on\npreviously worst performing action categories. Code and pretrained\nmodels can be found at https://github.com/skelemoa/ntu-x."}}
{"id": "rryzihJDck", "cdate": 1668797895369, "mdate": 1668797895369, "content": {"title": "Quo Vadis, Skeleton Action Recognition?", "abstract": "In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition.\nTo begin with, we benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results. To\nexamine skeleton action recognition 'in the wild', we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos\nsourced from Kinetics-700, a large-scale action dataset. The results from benchmarking the top performers of NTU-120 on\nSkeletics-152 reveal the challenges and domain gap induced by actions 'in the wild'. We extend our study to include out-of-context\nactions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. Finally, as a new frontier for\naction recognition, we introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb\nCharades and interpretative dance performances. Overall, our work characterizes the strengths and limitations of existing approaches\nand datasets. It also provides an assessment of top-performing approaches across a spectrum of activity settings and via the\nintroduced datasets, proposes new frontiers for human action recognition."}}
{"id": "ux5V6T2UlJ", "cdate": 1640995200000, "mdate": 1682384750394, "content": {"title": "PSUMNet: Unified Modality Part Streams Are All You Need for Efficient Pose-Based Action Recognition", "abstract": "Pose-based action recognition is predominantly tackled by approaches which treat the input skeleton in a monolithic fashion, i.e. joints in the pose tree are processed as a whole. However, such approaches ignore the fact that action categories are often characterized by localized action dynamics involving only small subsets of part joint groups involving hands (e.g. \u2018Thumbs up\u2019) or legs (e.g. \u2018Kicking\u2019). Although part-grouping based approaches exist, each part group is not considered within the global pose frame, causing such methods to fall short. Further, conventional approaches employ independent modality streams (e.g. joint, bone, joint velocity, bone velocity) and train their network multiple times on these streams, which massively increases the number of training parameters. To address these issues, we introduce PSUMNet, a novel approach for scalable and efficient pose-based action recognition. At the representation level, we propose a global frame based part stream approach as opposed to conventional modality based streams. Within each part stream, the associated data from multiple modalities is unified and consumed by the processing pipeline. Experimentally, PSUMNet achieves state of the art performance on the widely used NTURGB+D 60/120 dataset and dense joint skeleton dataset NTU 60-X/120-X. PSUMNet is highly efficient and outperforms competing methods which use 100%\u2013400% more parameters. PSUMNet also generalizes to the SHREC hand gesture dataset with competitive performance. Overall, PSUMNet\u2019s scalability, performance and efficiency makes it an attractive choice for action recognition and for deployment on compute-restricted embedded and edge devices. Code and pretrained models can be accessed at https://github.com/skelemoa/psumnet ."}}
{"id": "p-aslbm1gj", "cdate": 1640995200000, "mdate": 1682384750415, "content": {"title": "Detecting, Tracking and Counting Motorcycle Rider Traffic Violations on Unconstrained Roads", "abstract": "In many Asian countries with unconstrained road traffic conditions, driving violations such as not wearing helmets and triple-riding are a significant source of fatalities involving motorcycles. Identifying and penalizing such riders is vital in curbing road accidents and improving citizens' safety. With this motivation, we propose an approach for detecting, tracking, and counting motorcycle riding violations in videos taken from a vehicle-mounted dashboard camera. We employ a curriculum learning-based object detector to better tackle challenging scenarios such as occlusions. We introduce a novel trapezium-shaped object boundary representation to increase robustness and tackle the rider-motorcycle association. We also introduce an amodal regressor that generates bounding boxes for the occluded riders. Experimental results on a large-scale unconstrained driving dataset demonstrate the superiority of our approach compared to existing approaches and other ablative variants."}}
