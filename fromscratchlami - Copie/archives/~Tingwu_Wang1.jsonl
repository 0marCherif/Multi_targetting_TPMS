{"id": "CMpYEzD8OTB", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning to Generate Diverse Dance Motions with Transformer", "abstract": "With the ongoing pandemic, virtual concerts and live events using digitized performances of musicians are getting traction on massive multiplayer online worlds. However, well choreographed dance movements are extremely complex to animate and would involve an expensive and tedious production process. In addition to the use of complex motion capture systems, it typically requires a collaborative effort between animators, dancers, and choreographers. We introduce a complete system for dance motion synthesis, which can generate complex and highly diverse dance sequences given an input music sequence. As motion capture data is limited for the range of dance motions and styles, we introduce a massive dance motion data set that is created from YouTube videos. We also present a novel two-stream motion transformer generative model, which can generate motion sequences with high flexibility. We also introduce new evaluation metrics for the quality of synthesized dance motions, and demonstrate that our system can outperform state-of-the-art methods. Our system provides high-quality animations suitable for large crowds for virtual concerts and can also be used as reference for professional animation pipelines. Most importantly, we show that vast online videos can be effective in training dance motion models."}}
{"id": "H1exf64KwH", "cdate": 1569438984299, "mdate": null, "content": {"title": "Exploring Model-based Planning with Policy Networks", "abstract": "Model-based reinforcement learning (MBRL) with model-predictive control or\nonline planning has shown great potential for locomotion control tasks in both\nsample efficiency and asymptotic performance. Despite the successes, the existing\nplanning methods search from candidate sequences randomly generated in the\naction space, which is inefficient in complex high-dimensional environments. In\nthis paper, we propose a novel MBRL algorithm, model-based policy planning\n(POPLIN), that combines policy networks with online planning. More specifically,\nwe formulate action planning at each time-step as an optimization problem using\nneural networks. We experiment with both optimization w.r.t. the action sequences\ninitialized from the policy network, and also online optimization directly w.r.t. the\nparameters of the policy network. We show that POPLIN obtains state-of-the-art\nperformance in the MuJoCo benchmarking environments, being about 3x more\nsample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC.\nTo explain the effectiveness of our algorithm, we show that the optimization surface\nin parameter space is smoother than in action space. Further more, we found the\ndistilled policy network can be effectively applied without the expansive model\npredictive control during test time for some environments such as Cheetah. Code\nis released."}}
{"id": "H1lefTEKDS", "cdate": 1569438983892, "mdate": null, "content": {"title": "Benchmarking Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) is widely seen as having the potential\nto be significantly more sample efficient than model-free RL. However, research in\nmodel-based RL has not been very standardized. It is fairly common for authors to\nexperiment with self-designed environments, and there are several separate lines of\nresearch, which are sometimes closed-sourced or not reproducible. Accordingly, it\nis an open question how these various existing algorithms perform relative to each\nother. To facilitate research in MBRL, in this paper we gather a wide collection\nof MBRL algorithms and propose over 18 benchmarking environments specially\ndesigned for MBRL. We benchmark these algorithms with unified problem settings,\nincluding noisy environments. Beyond cataloguing performance, we explore\nand unify the underlying algorithmic differences across MBRL algorithms. We\ncharacterize three key research challenges for future MBRL research: the dynamics\nbottleneck, the planning horizon dilemma, and the early-termination dilemma.\nFinally, to facilitate future research on MBRL, we open-source our benchmark."}}
{"id": "BkgWHnR5tm", "cdate": 1538087993407, "mdate": null, "content": {"title": "Neural Graph Evolution: Towards Efficient Automatic Robot Design", "abstract": "Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering. Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates. To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space. We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively. Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods by an order of magnitude. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs. Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2\nmachine.\n"}}
{"id": "S1sqHMZCb", "cdate": 1518730163282, "mdate": null, "content": {"title": "NerveNet: Learning Structured Policy with Graph Neural Networks", "abstract": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n"}}
{"id": "rJVVLJGd-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "VirtualHome: Simulating Household Activities via Programs", "abstract": "In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to \"drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language"}}
