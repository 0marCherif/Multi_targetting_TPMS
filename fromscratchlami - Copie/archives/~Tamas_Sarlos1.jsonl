{"id": "cImlS4oD58F", "cdate": 1680225655651, "mdate": 1680225655651, "content": {"title": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers", "abstract": "In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention\n(Choromanski et al., 2021) and log-linear RPEattention (Luo et al., 2021) are special cases of this\ngeneral mechanism. However by casting the problem as a topological (graph-based) modulation of\nunmasked attention, we obtain several results unknown before, including efficient d-dimensional\nRPE-masking and graph-kernel masking. We\nleverage many mathematical techniques ranging\nfrom spectral analysis through dynamic programming and random walks to new algorithms for\nsolving Markov processes on graphs. We provide\na corresponding empirical evaluation."}}
{"id": "mbdzhPMrDB", "cdate": 1672531200000, "mdate": 1681677773072, "content": {"title": "Efficient Graph Field Integrators Meet Point Clouds", "abstract": "We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant."}}
{"id": "mbJ_aTT8NK", "cdate": 1672531200000, "mdate": 1681677773069, "content": {"title": "Generalized Private Selection and Testing with High Confidence", "abstract": "Composition theorems are general and powerful tools that facilitate privacy accounting across multiple data accesses from per-access privacy bounds. However they often result in weaker bounds compared with end-to-end analysis. Two popular tools that mitigate that are the exponential mechanism (or report noisy max) and the sparse vector technique, generalized in a recent private selection framework by Liu and Talwar (STOC 2019). In this work, we propose a flexible framework of private selection and testing that generalizes the one proposed by Liu and Talwar, supporting a wide range of applications. We apply our framework to solve several fundamental tasks, including query releasing, top-k selection, and stable selection, with improved confidence-accuracy tradeoffs. Additionally, for online settings, we apply our private testing to design a mechanism for adaptive query releasing, which improves the sample complexity dependence on the confidence parameter for the celebrated private multiplicative weights algorithm of Hardt and Rothblum (FOCS 2010)."}}
{"id": "lkDMLSexDLG", "cdate": 1672531200000, "mdate": 1681677773066, "content": {"title": "FAVOR#: Sharp Attention Kernel Approximations via New Classes of Positive Random Features", "abstract": "The problem of efficient approximation of a linear operator induced by the Gaussian or softmax kernel is often addressed using random features (RFs) which yield an unbiased approximation of the operator's result. Such operators emerge in important applications ranging from kernel methods to efficient Transformers. We propose parameterized, positive, non-trigonometric RFs which approximate Gaussian and softmax-kernels. In contrast to traditional RF approximations, parameters of these new methods can be optimized to reduce the variance of the approximation, and the optimum can be expressed in closed form. We show that our methods lead to variance reduction in practice ($e^{10}$-times smaller variance and beyond) and outperform previous methods in a kernel regression task. Using our proposed mechanism, we also present FAVOR#, a method for self-attention approximation in Transformers. We show that FAVOR# outperforms other random feature methods in speech modelling and natural language processing."}}
{"id": "SSPlfbcrhV", "cdate": 1672531200000, "mdate": 1681677773071, "content": {"title": "Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers", "abstract": "We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention."}}
{"id": "vRwCvlvd8eA", "cdate": 1652737406733, "mdate": null, "content": {"title": "Chefs' Random Tables: Non-Trigonometric Random Features", "abstract": "We introduce chefs' random tables (CRTs), a new class of non-trigonometric random features (RFs) to approximate Gaussian and softmax kernels. CRTs are an alternative to standard random kitchen sink (RKS) methods, which inherently rely on the trigonometric maps. We present variants of CRTs where RFs are positive, a key requirement for applications in recent low-rank Transformers. Further variance reduction is possible by leveraging statistics which are simple to compute. One instantiation of CRTs, the optimal positive random features (OPRFs), is to our knowledge the first RF method for unbiased softmax kernel estimation with positive and bounded RFs, resulting in exponentially small tails and much lower variance than its counterparts. As we show, orthogonal random features applied in OPRFs provide additional variance reduction for any dimensionality $d$ (not only asymptotically for sufficiently large $d$, as for RKS). We test CRTs on many tasks ranging from non-parametric classification to training Transformers for text, speech and image data, obtaining new state-of-the-art results for low-rank text Transformers, while providing linear space and time complexity."}}
{"id": "x8CNWQeO5o", "cdate": 1640995200000, "mdate": 1681677773071, "content": {"title": "On the Robustness of CountSketch to Adaptive Inputs", "abstract": "The last decade saw impressive progress towards understanding the performance of algorithms in <em>adaptive</em> settings, where subsequent inputs may depend on the output from prior inputs. Adapti..."}}
{"id": "ww1TF13cPw", "cdate": 1640995200000, "mdate": 1681677773082, "content": {"title": "On the Robustness of CountSketch to Adaptive Inputs", "abstract": "CountSketch is a popular dimensionality reduction technique that maps vectors to a lower dimension using randomized linear measurements. The sketch supports recovering $\\ell_2$-heavy hitters of a vector (entries with $v[i]^2 \\geq \\frac{1}{k}\\|\\boldsymbol{v}\\|^2_2$). We study the robustness of the sketch in adaptive settings where input vectors may depend on the output from prior inputs. Adaptive settings arise in processes with feedback or with adversarial attacks. We show that the classic estimator is not robust, and can be attacked with a number of queries of the order of the sketch size. We propose a robust estimator (for a slightly modified sketch) that allows for quadratic number of queries in the sketch size, which is an improvement factor of $\\sqrt{k}$ (for $k$ heavy hitters) over prior work."}}
{"id": "iFp_ZwzypjZ", "cdate": 1640995200000, "mdate": 1681677773074, "content": {"title": "Tricking the Hashing Trick: A Tight Lower Bound on the Robustness of CountSketch to Adaptive Inputs", "abstract": "CountSketch and Feature Hashing (the \"hashing trick\") are popular randomized dimensionality reduction methods that support recovery of $\\ell_2$-heavy hitters (keys $i$ where $v_i^2 > \\epsilon \\|\\boldsymbol{v}\\|_2^2$) and approximate inner products. When the inputs are {\\em not adaptive} (do not depend on prior outputs), classic estimators applied to a sketch of size $O(\\ell/\\epsilon)$ are accurate for a number of queries that is exponential in $\\ell$. When inputs are adaptive, however, an adversarial input can be constructed after $O(\\ell)$ queries with the classic estimator and the best known robust estimator only supports $\\tilde{O}(\\ell^2)$ queries. In this work we show that this quadratic dependence is in a sense inherent: We design an attack that after $O(\\ell^2)$ queries produces an adversarial input vector whose sketch is highly biased. Our attack uses \"natural\" non-adaptive inputs (only the final adversarial input is chosen adaptively) and universally applies with any correct estimator, including one that is unknown to the attacker. In that, we expose inherent vulnerability of this fundamental method."}}
{"id": "VSDj5bTe7ZI", "cdate": 1640995200000, "mdate": 1681677773071, "content": {"title": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers", "abstract": "In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that..."}}
