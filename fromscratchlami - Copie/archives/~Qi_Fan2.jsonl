{"id": "98mksfFJROk", "cdate": 1667472488876, "mdate": 1667472488876, "content": {"title": "Few-Shot Object Detection with Model Calibration", "abstract": "Few-shot object detection (FSOD) targets at transferring knowledge from known to unknown classes to detect objects of novel classes. However, previous works ignore the model bias problem inherent in the transfer learning paradigm. Such model bias causes overfitting toward the training classes and destructs the well-learned transferable knowledge. In this paper, we pinpoint and comprehensively investigate the model bias problem in FSOD models and propose a simple yet effective method to address the model bias problem with the facilitation of model calibrations in three levels: 1) Backbone calibration to preserve the well-learned prior knowledge and relieve the model bias toward base classes, 2) RPN calibration to rescue unlabeled objects of novel classes and, 3) Detector calibration to prevent the model bias toward a few training samples for novel classes. Specifically, we leverage the overlooked classification dataset to facilitate our model calibration procedure, which has only been used for pre-training in other related works. We validate the effectiveness of our model calibration method on the popular Pascal VOC and MS COCO datasets, where our method achieves very promising performance."}}
{"id": "rrxDSvp-rWn", "cdate": 1667472453635, "mdate": 1667472453635, "content": {"title": "Self-Support Few-Shot Semantic Segmentation", "abstract": "Existing few-shot segmentation methods have achieved great progress based on the support-query matching framework. But they still heavily suffer from the limited coverage of intra-class variations from the few-shot supports provided. Motivated by the simple Gestalt principle that pixels belonging to the same object are more similar than those to different objects of same class, we propose a novel self-support matching strategy to alleviate this problem, which uses query prototypes to match query features, where the query prototypes are collected from high-confidence query predictions. This strategy can effectively capture the consistent underlying characteristics of the query objects, and thus fittingly match query features. We also propose an adaptive self-support background prototype generation module and self-support loss to further facilitate the self-support matching procedure. Our self-support network substantially improves the prototype quality, benefits more improvement from stronger backbones and more supports, and achieves SOTA on multiple datasets. Codes are at \\url{https://github.com/fanq15/SSP}."}}
{"id": "odVhX3QDtO", "cdate": 1667472415741, "mdate": 1667472415741, "content": {"title": "Few-shot video object detection", "abstract": "We introduce Few-Shot Video Object Detection (FSVOD)\nwith three contributions to real-world visual learning challenge in our\nhighly diverse and dynamic world: 1) a large-scale video dataset FSVOD500 comprising of 500 classes with class-balanced videos in each category for few-shot learning; 2) a novel Tube Proposal Network (TPN) to\ngenerate high-quality video tube proposals for aggregating feature representation for the target video object which can be highly dynamic;\n3) a strategically improved Temporal Matching Network (TMN+) for\nmatching representative query tube features with better discriminative\nability thus achieving higher diversity. Our TPN and TMN+ are jointly\nand end-to-end trained. Extensive experiments demonstrate that our\nmethod produces significantly better detection results on two few-shot\nvideo object detection datasets compared to image-based methods and\nother naive video-based extensions. "}}
{"id": "k5XML9l9aR", "cdate": 1667472355008, "mdate": 1667472355008, "content": {"title": "Commonality-parsing network across shape and appearance for partially supervised instance segmentation", "abstract": "Partially supervised instance segmentation aims to perform\nlearning on limited mask-annotated categories of data thus eliminating\nexpensive and exhaustive mask annotation. The learned models are expected to be generalizable to novel categories. Existing methods either\nlearn a transfer function from detection to segmentation, or cluster shape\npriors for segmenting novel categories. We propose to learn the underlying class-agnostic commonalities that can be generalized from maskannotated categories to novel categories. Specifically, we parse two types\nof commonalities: 1) shape commonalities which are learned by performing supervised learning on instance boundary prediction; and 2) appearance commonalities which are captured by modeling pairwise affinities\namong pixels of feature maps to optimize the separability between instance and the background. Incorporating both the shape and appearance commonalities, our model significantly outperforms the state-ofthe-art methods on both partially supervised setting and few-shot setting for instance segmentation on COCO dataset."}}
{"id": "zwTt7b1nsl", "cdate": 1667472182935, "mdate": 1667472182935, "content": {"title": "Group collaborative learning for co-salient object detection", "abstract": "We present a novel group collaborative learning framework (GCNet) capable of detecting co-salient objects in real time (16ms), by simultaneously mining consensus representations at group level based on the two necessary criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module; 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module conditioning the inconsistent consensus. To learn a better embedding space without extra computational overhead, we explicitly employ auxiliary classification supervision. Extensive experiments on three challenging benchmarks, ie, CoCA, CoSOD3k, and Cosal2015, demonstrate that our simple GCNet outperforms 10 cutting-edge models and achieves the new state-of-the-art. We demonstrate this paper's new technical contributions on a number of important downstream computer vision applications including content aware co-segmentation, co-localization based automatic thumbnails, etc. Our research code with two applications will be released."}}
{"id": "0somR4DXJ3", "cdate": 1667472115559, "mdate": 1667472115559, "content": {"title": "Few-shot object detection with attention-RPN and multi-relation detector", "abstract": "Conventional methods for object detection typically require a substantial amount of training data and preparing such high-quality training data is very labor-intensive. In this paper, we propose a novel few-shot object detection network that aims at detecting objects of unseen categories with only a few annotated examples. Central to our method are our Attention-RPN, Multi-Relation Detector and Contrastive Training strategy, which exploit the similarity between the few shot support set and query set to detect novel objects while suppressing false detection in the background. To train our network, we contribute a new dataset that contains 1000 categories of various objects with high-quality annotations. To the best of our knowledge, this is one of the first datasets specifically designed for few-shot object detection. Once our few-shot network is trained, it can detect objects of unseen categories without further training or fine-tuning. Our method is general and has a wide range of potential applications. We produce a new state-of-the-art performance on different datasets in the few-shot setting. The dataset link is https://github. com/fanq15/Few-Shot-Object-Detection-Dataset."}}
{"id": "vqSyt8D3ny", "cdate": 1663849932016, "mdate": null, "content": {"title": "Towards Robust Object Detection Invariant to Real-World Domain Shifts", "abstract": "Safety-critical applications such as autonomous driving require robust object detection invariant to real-world domain shifts. Such shifts can be regarded as different domain styles, which can vary substantially due to environment changes and sensor noises, but deep models only know the training domain style. Such domain style gap impedes object detection generalization on diverse real-world domains. Existing classification domain generalization (DG) methods cannot effectively solve the robust object detection problem, because they either rely on multiple source domains with large style variance or destroy the content structures of the original images. In this paper, we analyze and investigate effective solutions to overcome domain style overfitting for robust object detection without the above shortcomings. Our method, dubbed as Normalization Perturbation (NP), perturbs the channel statistics of source domain low-level features to synthesize various latent styles, so that the trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training. This approach is motivated by the observation that feature channel statistics of the target domain images deviate around the source domain statistics. We further explore the style-sensitive channels for effective style synthesis. Normalization Perturbation only relies on a single source domain and is surprisingly simple and effective, contributing a practical solution by effectively adapting or generalizing classification DG methods to robust object detection. Extensive experiments demonstrate the effectiveness of our method for generalizing object detectors under real-world domain shifts."}}
{"id": "uylnzi15feI", "cdate": 1640995200000, "mdate": 1668588617080, "content": {"title": "Self-support Few-Shot Semantic Segmentation", "abstract": "Existing few-shot segmentation methods have achieved great progress based on the support-query matching framework. But they still heavily suffer from the limited coverage of intra-class variations from the few-shot supports provided. Motivated by the simple Gestalt principle that pixels belonging to the same object are more similar than those to different objects of same class, we propose a novel self-support matching strategy to alleviate this problem, which uses query prototypes to match query features, where the query prototypes are collected from high-confidence query predictions. This strategy can effectively capture the consistent underlying characteristics of the query objects, and thus fittingly match query features. We also propose an adaptive self-support background prototype generation module and self-support loss to further facilitate the self-support matching procedure. Our self-support network substantially improves the prototype quality, benefits more improvement from stronger backbones and more supports, and achieves SOTA on multiple datasets. Codes are at https://github.com/fanq15/SSP ."}}
{"id": "u2valrC0B0_", "cdate": 1640995200000, "mdate": 1668588617324, "content": {"title": "GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector", "abstract": "In this paper, we present a novel end-to-end group collaborative learning network, termed GCoNet+, which can effectively and efficiently (250 fps) identify co-salient objects in natural scenes. The proposed GCoNet+ achieves the new state-of-the-art performance for co-salient object detection (CoSOD) through mining consensus representations based on the following two essential criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module (GAM); 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module (GCM) conditioning on the inconsistent consensus. To further improve the accuracy, we design a series of simple yet effective components as follows: i) a recurrent auxiliary classification module (RACM) promoting model learning at the semantic level; ii) a confidence enhancement module (CEM) assisting the model in improving the quality of the final predictions; and iii) a group-based symmetric triplet (GST) loss guiding the model to learn more discriminative features. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that our GCoNet+ outperforms the existing 12 cutting-edge models. Code has been released at https://github.com/ZhengPeng7/GCoNet_plus."}}
{"id": "_dDwZ1de1Xr", "cdate": 1640995200000, "mdate": 1668588617322, "content": {"title": "Self-Support Few-Shot Semantic Segmentation", "abstract": "Existing few-shot segmentation methods have achieved great progress based on the support-query matching framework. But they still heavily suffer from the limited coverage of intra-class variations from the few-shot supports provided. Motivated by the simple Gestalt principle that pixels belonging to the same object are more similar than those to different objects of same class, we propose a novel self-support matching strategy to alleviate this problem, which uses query prototypes to match query features, where the query prototypes are collected from high-confidence query predictions. This strategy can effectively capture the consistent underlying characteristics of the query objects, and thus fittingly match query features. We also propose an adaptive self-support background prototype generation module and self-support loss to further facilitate the self-support matching procedure. Our self-support network substantially improves the prototype quality, benefits more improvement from stronger backbones and more supports, and achieves SOTA on multiple datasets. Codes are at \\url{https://github.com/fanq15/SSP}."}}
