{"id": "lZtf-UloplT", "cdate": 1698737041987, "mdate": null, "content": {"title": "Temporal-agnostic change region proposal for semantic change detection", "abstract": "Remote sensing imagery allows temporal and large-scale observation of the Earth, and advanced techniques such as deep learning have been developed to deal with the massive data. As a result, by combining remote sensing with deep learning, it is possible to realize change detection, which can reveal the change in land cover and land use. Recent work has focused on semantic change detection (SCD), which simultaneously locates where the changes took place while identifying what objects changed between the different periods. Deep learning based SCD methods are typically based on a multi-task structure, which combines the classification and binary change detection (BCD). In this structure, the change region proposals are derived from the transformation (e.g., concatenation, differencing) of multi-temporal images, but after being processed by a convolutional neural network, there can be an incomplete response in the spatio-temporal features. Because of the high similarity of the color, texture, and geometric information of the ground objects in temporal high-resolution remote sensing imagery, this reduces the distinguishability of features and makes it difficult to identify changed areas. Meanwhile, there is less interaction between the BCD and classification, resulting in semantic inconsistency in changed areas. In this paper, the temporal-agnostic change region proposal network (TCRPN) is proposed, which is based on exploring the saliency of the changed regions in single-temporal images to highlight the changed areas in the spatio-temporal features and improve the response to change. As the changed areas can be the salient regions in the single-temporal images, a single-image salient area extractor (SSAE) is proposed to extract the salient areas from the different temporal images, obtaining salient change region candidates. As the change-related salient regions are temporally agnostic, temporal saliency fusion is used to integrate the candidates extracted from the single-temporal images to obtain a more reliable salient area map, where the salient area map is used to enhance the spatio-temporal features. Meanwhile, a shortcut is proposed to further make the change representation more distinguishable and suppress incorrectly highlighted regions. Directly utilizing multi-temporal images for change region proposal can reduce the change information loss and increase the interaction between the classification and BCD tasks to obtain better spatiotemporal features. The TCRPN is simple and flexible, and can be plugged into any multi-task SCD network. We selected four advanced multi-task SCD methods as the baseline, and added the TCRPN in their encoder or decoder part to conduct comprehensive experiments on three large-scale SCD datasets. The experimental results confirmed the effectiveness of the proposed method."}}
{"id": "gyT6bg5YOvD", "cdate": 1640995200000, "mdate": 1667287247453, "content": {"title": "Cascaded Multi-Task Road Extraction Network for Road Surface, Centerline, and Edge Extraction", "abstract": "Road extraction from very high-resolution (VHR) remote sensing imagery remains a huge challenge, due to the shadows and occlusions of trees and buildings. Such complex backgrounds result in deep networks often producing fragmented roads with poor connectivity. Road extraction has three typical tasks: road surface segmentation (SS), centerline extraction (CE), and edge detection (ED), which are conducted in a wide range of real applications. Also, the three tasks have a symbiotic relationship, i.e., the road SS determines the location of the centerline and edges, and the CE and ED can allow the generation of more continuous road surfaces. However, most of the previous works have completed these three tasks separately, without exploiting the symbiotic relationship between them to boost the road connectivity. In this article, in order to improve road connectivity, a cascaded multitask (CasMT) road extraction framework for simultaneously extracting the road surface, centerline, and edges is proposed. In the proposed framework, topology-aware learning is applied to capture the long-distance topological relationships, and hard example mining (HEM) loss is employed to focus more on hard samples, to further enhance the road completeness. Extensive experiments were conducted on the DeepGlobe road dataset and a large-scale road dataset (called the LSCC dataset) from the three Chinese cities of Beijing, Shanghai, and Wuhan. The experimental results obtained on the public DeepGlobe dataset demonstrate that the proposed CasMT framework can significantly outperform the current state-of-the-art method. Moreover, the generalization capability of the model was verified on the LSCC dataset, where the proposed CasMT framework achieved the best performance in the average path length similarity (APLS) road topology metric, which further confirms the superiority of the proposed framework."}}
{"id": "g2Rlyp6BZl", "cdate": 1640995200000, "mdate": 1667287247422, "content": {"title": "FactSeg: Foreground Activation-Driven Small Object Semantic Segmentation in Large-Scale Remote Sensing Imagery", "abstract": "The small object semantic segmentation task is aimed at automatically extracting key objects from high-resolution remote sensing (HRS) imagery. Compared with the large-scale coverage areas for remote sensing imagery, the key objects, such as cars and ships, in HRS imagery often contain only a few pixels. In this article, to tackle this problem, the foreground activation (FA)-driven small object semantic segmentation (FactSeg) framework is proposed from perspectives of structure and optimization. In the structure design, FA object representation is proposed to enhance the awareness of the weak features in small objects. The FA object representation framework is made up of a dual-branch decoder and collaborative probability (CP) loss. In the dual-branch decoder, the FA branch is designed to activate the small object features (activation) and suppress the large-scale background, and the semantic refinement (SR) branch is designed to further distinguish small objects (refinement). The CP loss is proposed to effectively combine the activation and refinement outputs of the decoder under the CP hypothesis. During the collaboration, the weak features of the small objects are enhanced with the activation output, and the refined output can be viewed as the refinement of the binary outputs. In the optimization stage, small object mining (SOM)-based network optimization is applied to automatically select effective samples and refine the direction of the optimization while addressing the imbalanced sample problem between the small objects and the large-scale background. The experimental results obtained with two benchmark HRS imagery segmentation datasets demonstrate that the proposed framework outperforms the state-of-the-art semantic segmentation methods and achieves a good tradeoff between accuracy and efficiency. Code will be available at: <uri>http://rsidea.whu.edu.cn/FactSeg.htm</uri>"}}
{"id": "D86mB6SMpTG", "cdate": 1640995200000, "mdate": 1667287247447, "content": {"title": "GRE and Beyond: A Global Road Extraction Dataset", "abstract": "Accurate and timely road mapping that describes the road network geometry and topology is the key element of intelligent transport systems and smart city management. However, current global road maps like OpenStreetMap (OSM) are typically outdated and spatially incomplete with uneven accuracies. Although the development of remote sensing satellite technology and the advance of computer vision technology have made it possible to quickly extract road networks from massive very-high-resolution (VHR) remote sensing imagery, existing road extraction methods are limited by the problem: lacking of an accurate and diverse training dataset for global-scale road extraction, and manually labelling millions of road samples for training a global model is labor intensive. To address this problem, we utilized VHR satellite imagery and open-source crowdsourcing geospatial big data to build a robust global-scale road training dataset, termed GlobalRoadNet, for global road extraction (GRE) and beyond. The proposed GlobalRoadNet contains 47210 samples from 121 capital cities of six continents in Europe, Africa, Asia, South America, Oceania, and North America. Experimental results show that GlobalRoadNet can significantly improve model performance, not only can be applied for road extraction, but also has the potential to update OSM road data."}}
{"id": "9PRGthLKBv2", "cdate": 1640995200000, "mdate": 1667287247391, "content": {"title": "A Supervised Progressive Growing Generative Adversarial Network for Remote Sensing Image Scene Classification", "abstract": "Remote sensing image scene classification is a challenging task. With the development of deep learning, methods based on convolutional neural networks (CNNs) have made great achievements in remote sensing image scene classification. Since the training of a CNN requires a large number of labeled samples, a generative adversarial network (GAN) for sample generation represents a new opportunity to solve the problem of the limited samples. However, most of the existing GAN-based sample generation methods can only generate unlabeled samples, instead of samples labeled with the corresponding scene category. In this article, to solve the problem, a supervised progressive growing generative adversarial network (SPG-GAN) is proposed for remote sensing image scene classification. The proposed method can generate labeled samples for the remote sensing image scene classification, significantly improving the classification accuracy in the case of limited samples. The SPG-GAN method has two main improvements. First, a conditional generative framework for labeled samples is proposed, in which the label information is added in the channel dimension as the input. By considering the constraints of the label information in the loss function, the network can be trained in the direction of a specific category. As a result, the network can generate remote sensing image scene classification samples with label categories. Second, a progressive growing sample generation method is introduced. In order to ensure that the generated samples have more spatial details, they are generated by progressively adding modules to the generator and discriminator, thereby ensuring that the generated sample is of better quality. After testing on two benchmark datasets and carrying out a large-scale experiment in the central area of the city of Wuhan in China, it was found that the proposed method can obtain a superior scene classification accuracy in the case of limited samples."}}
{"id": "2PgT7SVw6-M", "cdate": 1640995200000, "mdate": 1667287247498, "content": {"title": "A Spectral-Spatial-Dependent Global Learning Framework for Insufficient and Imbalanced Hyperspectral Image Classification", "abstract": "Deep learning techniques have been widely applied to hyperspectral image (HSI) classification and have achieved great success. However, the deep neural network model has a large parameter space and requires a large number of labeled data. Deep learning methods for HSI classification usually follow a patchwise learning framework. Recently, a fast patch-free global learning (FPGA) architecture was proposed for HSI classification according to global spatial context information. However, FPGA has difficulty in extracting the most discriminative features when the sample data are imbalanced. In this article, a spectral-spatial-dependent global learning (SSDGL) framework based on the global convolutional long short-term memory (GCL) and global joint attention mechanism (GJAM) is proposed for insufficient and imbalanced HSI classification. In SSDGL, the hierarchically balanced (H-B) sampling strategy and the weighted softmax loss are proposed to address the imbalanced sample problem. To effectively distinguish similar spectral characteristics of land cover types, the GCL module is introduced to extract the long short-term dependency of spectral features. To learn the most discriminative feature representations, the GJAM module is proposed to extract attention areas. The experimental results obtained with three public HSI datasets show that the SSDGL has powerful performance in insufficient and imbalanced sample problems and is superior to other state-of-the-art methods."}}
{"id": "bLBIbVaGDu", "cdate": 1628987765398, "mdate": null, "content": {"title": "LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation", "abstract": "Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets mainly promote the research of learning semantic representation, thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) dataset to advance semantic and transferable learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated objects from three different cities. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the:  1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on eleven semantic segmentation methods and eight UDA methods. Some exploratory studies including multi-scale architectures and strategies, additional background supervision, and pseudo-label analysis were also carried out to address these challenges. The code and data are available at https://github.com/Junjue-Wang/LoveDA."}}
{"id": "_-O9SefMb99", "cdate": 1621427613208, "mdate": null, "content": {"title": "LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptation Semantic Segmentation", "abstract": "Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets only focus on improvement of the semantic segmentation in one domain (urban or rural), thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptation semantic segmentation (LoveDA) dataset to promote large-scale land-cover mapping. The LoveDA dataset contains 3338 aerial images with 86,516 annotated objects for seven common land-cover categories. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the: 1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on nine semantic segmentation methods and eight UDA methods. Some exploratory studies were also carried out to find alternative ways to address these challenges. The code and data will be available at: https://github.com/Junjue-Wang/LoveDA."}}
{"id": "am2bjHY0CK", "cdate": 1609459200000, "mdate": 1667287247473, "content": {"title": "RSNet: The Search for Remote Sensing Deep Neural Networks in Recognition Tasks", "abstract": "Deep learning algorithms, especially convolutional neural networks (CNNs), have recently emerged as a dominant paradigm for high spatial resolution remote sensing (HRS) image recognition. A large amount of CNNs have already been successfully applied to various HRS recognition tasks, such as land-cover classification and scene classification. However, they are often modifications of the existing CNNs derived from natural image processing, in which the network architecture is inherited without consideration of the complexity and specificity of HRS images. In this article, the remote sensing deep neural network (RSNet) framework is proposed using an automatically search strategy to find the appropriate network architecture for HRS image recognition tasks. In RSNet, the hierarchical search space is first designed to include module- and transition-level spaces. The module-level space defines the basic structure block, where a series of lightweight operations as candidates, including depthwise separable convolutions, is proposed to ensure the efficiency. The transition-level space controls the spatial resolution transformations of the features. In the hierarchical search space, a gradient-based search strategy is used to find the appropriate architecture. In RSNet, the task-driven architecture training process can acquire the optimal model parameters of the switchable recognition module for HRS image recognition tasks. The experimental results obtained using four benchmark data sets for land-cover classification and scene classification tasks demonstrate that the searched RSNet can achieve a satisfactory accuracy with a high computational efficiency and, hence, provides an effective option for the processing of HRS imagery."}}
{"id": "V6GSyjQpWvJ", "cdate": 1609459200000, "mdate": 1667287247392, "content": {"title": "LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation", "abstract": "Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets mainly promote the research of learning semantic representation, thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) dataset to advance semantic and transferable learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated objects from three different cities. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the: 1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on eleven semantic segmentation methods and eight UDA methods. Some exploratory studies including multi-scale architectures and strategies, additional background supervision, and pseudo-label analysis were also carried out to address these challenges. The code and data are available at https://github.com/Junjue-Wang/LoveDA."}}
