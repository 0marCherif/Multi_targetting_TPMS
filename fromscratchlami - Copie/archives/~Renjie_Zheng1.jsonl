{"id": "ry-YhGZObr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning to Stop in Structured Prediction for Neural Machine Translation", "abstract": "Mingbo Ma, Renjie Zheng, Liang Huang. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "SkVSNQGdZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks", "abstract": "Distributed representation plays an important role in deep learning based natural language processing. However, the representation of a sentence often varies in different tasks, which is usually learned from scratch and suffers from the limited amounts of training data. In this paper, we claim that a good sentence representation should be invariant and can benefit the various subsequent tasks. To achieve this purpose, we propose a new scheme of information sharing for multi-task learning. More specifically, all tasks share the same sentence representation and each task can select the task-specific information from the shared sentence representation with attention mechanisms. The query vector of each task's attention could be either static parameters or generated dynamically. We conduct extensive experiments on 16 different text classification tasks, which demonstrate the benefits of our architecture. Source codes of this paper are available on Github."}}
{"id": "SJWeA-Md-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation", "abstract": "Neural text generation, including neural machine translation, image captioning, and summarization, has been quite successful recently. However, during training time, typically only one reference is considered for each example, even though there are often multiple references available, e.g., 4 references in NIST MT evaluations, and 5 references in image captioning data. We first investigate several different ways of utilizing multiple human references during training. But more importantly, we then propose an algorithm to generate exponentially many pseudo-references by first compressing existing human references into lattices and then traversing them to generate new pseudo-references. These approaches lead to substantial improvements over strong baselines in both machine translation (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr)."}}
