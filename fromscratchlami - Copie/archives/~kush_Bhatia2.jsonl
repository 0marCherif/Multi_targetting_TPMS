{"id": "Io71X_zEce", "cdate": 1696661496911, "mdate": 1696661496911, "content": {"title": "SuperHF: Supervised Iterative Learning from Human Feedback", "abstract": "While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training. Here, we focus on two prevalent methods used to align these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). SFT is simple and robust, powering a host of open-source models, while RLHF is a more sophisticated method used in top-tier models like ChatGPT but also suffers from instability and susceptibility to reward hacking. We propose a novel approach, Supervised Iterative Learning from Human Feedback (SuperHF), which seeks to leverage the strengths of both methods. Our hypothesis is two-fold: we posit that the reward model used in RLHF is critical for efficient data use and model generalization and that the use of Proximal Policy Optimization (PPO) in RLHF may not be necessary and could contribute to instability issues. SuperHF replaces PPO with a simple supervised loss and a Kullback-Leibler (KL) divergence prior. It creates its own training data by repeatedly sampling a batch of model outputs and filtering them through the reward model in an online learning regime. We then break down the reward optimization problem into three components: robustly optimizing the training rewards themselves, preventing reward hacking\u2014or exploitation of the reward model that can degrade model performance\u2014as measured by a novel METEOR similarity metric, and maintaining good performance on downstream evaluations. Our experimental results show SuperHF exceeds PPO-based RLHF on the training objective, easily and favorably trades off high reward with low reward hacking, improves downstream calibration, and performs the same on our GPT-4 based qualitative evaluation scheme all the while being significantly simpler to implement, highlighting SuperHF's potential as a competitive language model alignment technique."}}
{"id": "bhUPJnS2g0X", "cdate": 1663850505275, "mdate": null, "content": {"title": "Ask Me Anything: A simple strategy for prompting language models", "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly crafted \"perfect prompt\" for a task. To mitigate the high degree of effort, we instead ask whether collecting multiple decent, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed method, Ask Me Anything (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. True or False?\"). AMA recursively uses the LLM to transform task inputs to the effective QA format. AM generates multiple questions per input and applies these prompts to collect several noisy \"votes\" for the input's true label. We find the prompts have varying accuracies and dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance lift of 10.2\\% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B  on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting."}}
{"id": "hJqGbUpDGV", "cdate": 1663850447630, "mdate": null, "content": {"title": "On the Sensitivity of Reward Inference to Misspecified Human Models", "abstract": "Inferring reward functions from human behavior is at the center of value alignment \u2013 aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data."}}
{"id": "mp1AstNFvQ5", "cdate": 1634067446498, "mdate": null, "content": {"title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models", "abstract": "Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \\emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, we propose an anomaly detection task for aberrant policies and offer several baseline detectors."}}
{"id": "JYtwGwIL7ye", "cdate": 1632875700120, "mdate": null, "content": {"title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models", "abstract": "Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \\emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors."}}
{"id": "L2V-VQ7Npl0", "cdate": 1632875694407, "mdate": null, "content": {"title": "Reward Learning as Doubly Nonparametric Bandits:  Optimal Design and Scaling Laws", "abstract": "Specifying reward functions for complex tasks like object manipulation or driving is challenging to do by hand. Reward learning seeks to address this by learning a reward model using human feedback on selected query policies. This shifts the burden of reward specification to the optimal design of the queries. We propose a theoretical framework for studying reward learning and the associated optimal experiment design  problem. Our framework models rewards and policies as nonparametric functions belonging to subsets of Reproducing Kernel Hilbert Spaces (RKHSs). The learner receives (noisy) oracle access to a true reward  and must output a policy  that performs well under the true reward.  For this setting, we first derive non-asymptotic excess risk bounds for a simple plug-in estimator based on ridge regression.  We then solve the query design problem by optimizing these risk bounds with respect to the choice of query set and obtain a finite sample statistical rate, which depends primarily on the eigenvalue spectrum of a certain linear operator on the RKHSs. Despite the generality of these results, our bounds are stronger than previous bounds developed for more specialized problems. We specifically show that the well-studied problem of Gaussian process (GP) bandit optimization is a special case of our framework, and that our bounds either improve or are competitive with known regret guarantees for the Mat\\'ern kernel."}}
{"id": "syzTg1vyBtL", "cdate": 1632875693611, "mdate": null, "content": {"title": "Congested bandits: Optimal routing via short-term resets", "abstract": "For traffic routing platforms, the choice of which route to recommend to a user depends on the congestion on these routes -- indeed, an individual's utility depends on the number of people using the recommended route at that instance. Motivated by this, we introduce the problem of Congested Bandits where each arm's reward is allowed to depend on the number of times it was played in the past $\\Delta$ timesteps. This dependence on past history of actions leads to a dynamical system where an algorithm's present choices also affect its future pay-offs, and requires an algorithm to plan for this. We study the congestion aware formulation in the  multi-armed bandit (MAB) setup and in the  contextual bandit setup with linear rewards. For the multi-armed setup, we propose a UCB style algorithm  and show that its policy regret scales as $\\tilde{O}(\\sqrt{K \\Delta T})$. For the linear contextual bandit setup, our algorithm, based on an iterative least squares planner, achieves policy regret $\\tilde{O}(\\sqrt{dT} + \\Delta)$. From an experimental standpoint, we corroborate the no-regret properties of our algorithms via a simulation study."}}
{"id": "WT2mBOgwcu4", "cdate": 1621566578904, "mdate": null, "content": {"title": "Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems", "abstract": "We study derivative-free methods for policy optimization over the class of linear policies. We\nfocus on characterizing the convergence rate of these methods when applied to linear-quadratic\nsystems, and study various settings of driving noise and reward feedback. We show that these\nmethods provably converge to within any pre-specified tolerance of the optimal policy with a\nnumber of zero-order evaluations that is an explicit polynomial of the error tolerance, dimension,\nand curvature properties of the problem. Our analysis reveals some interesting differences\nbetween the settings of additive driving noise and random initialization, as well as the settings of\none-point and two-point reward feedback. Our theory is corroborated by extensive simulations\nof derivative-free methods on these systems. Along the way, we derive convergence rates for\nstochastic zero-order optimization algorithms when applied to a certain class of non-convex\nproblems."}}
{"id": "JXCtc1mTqLz", "cdate": 1621280357531, "mdate": null, "content": {"title": "Politex: Regret bounds for policy iteration using expert prediction", "abstract": "We present POLITEX (POLicy ITeration with EXpert advice), a variant of policy iteration where\neach policy is a Boltzmann distribution over the sum of action-value function estimates of the previous policies, and analyze its regret in continuing RL problems. We assume that the value function error after running a policy for \u03c4 time steps scales as \u03b5(\u03c4 ) = \u03b50 + O(sqrt(d/\u03c4)), where \u03b50 is the\nworst-case approximation error and d is the number of features in a compressed representation of\nthe state-action space. We establish that this condition is satisfied by the LSPE algorithm under\ncertain assumptions on the MDP and policies. Under the error assumption, we show that the regret\nof POLITEX in uniformly mixing MDPs scales as O(d^{1/2}T^{3/4 + \u03b50T), where O(\u00b7) hides logarithmic terms and problem-dependent constants. Thus, we provide the first regret bound for a fully practical model-free method which only scales in the number of features, and not in the size of the underlying MDP. Experiments on a queuing problem confirm that POLITEX is competitive with some of its alternatives, while preliminary results on Ms Pacman (one of the standard Atari benchmark problems) confirm the viability of POLITEX beyond linear function approximation."}}
{"id": "l_BZBBaaf65", "cdate": 1609459200000, "mdate": null, "content": {"title": "Agnostic Learning with Unknown Utilities", "abstract": "Traditional learning approaches for classification implicitly assume that each mistake has the same cost. In many real-world problems though, the utility of a decision depends on the underlying context x and decision y; for instance, misclassifying a stop sign is worse than misclassifying a road-side postbox. However, directly incorporating these utilities into the learning objective is often infeasible since these can be quite complex and difficult for humans to specify. We formally study this as agnostic learning with unknown utilities: given a dataset S = {x_1, \u2026, x_n} where each data point x_i \u223c \ud835\udc9f_x from some unknown distribution \ud835\udc9f_x, the objective of the learner is to output a function f in some class of decision functions \u2131 with small excess risk. This risk measures the performance of the output predictor f with respect to the best predictor in the class \u2131 on the unknown underlying utility u^*:\ud835\udcb3\u00d7\ud835\udcb4\u21a6 [0,1]. This utility u^* is not assumed to have any specific structure and is allowed to be any bounded function. This raises an interesting question whether learning is even possible in our setup, given that obtaining a generalizable estimate of utility u^* might not be possible from finitely many samples. Surprisingly, we show that estimating the utilities of only the sampled points S suffices to learn a decision function which generalizes well. With this insight, we study mechanisms for eliciting information from human experts which allow a learner to estimate the utilities u^* on the set S. While humans find it difficult to directly provide utility values reliably, it is often easier for them to provide comparison feedback based on these utilities. We show that, unlike in the realizable setup, the vanilla comparison queries where humans compare a pair of decisions for a single input x are insufficient. We introduce a family of elicitation mechanisms by generalizing comparisons, called the k-comparison oracle, which enables the learner to ask for comparisons across k different inputs x at once. We show that the excess risk in our agnostic learning framework decreases at a rate of O (1/k) with such queries. This result brings out an interesting accuracy-elicitation trade-off - as the order k of the oracle increases, the comparative queries become harder to elicit from humans but allow for more accurate learning."}}
