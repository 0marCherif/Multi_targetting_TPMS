{"id": "y_AdUtOyRcc", "cdate": 1577836800000, "mdate": null, "content": {"title": "Design of a Host Interface Logic for GC-Free SSDs", "abstract": "Garbage collection (GC) and resource contention on I/O buses (channels) are among the critical bottlenecks in solid-state drives (SSDs) that cannot be easily hidden. Most existing I/O scheduling algorithms in the host interface logic (HIL) of state-of-the-art SSDs are oblivious to such low-level performance bottlenecks in SSDs. As a result, SSDs may violate quality of service (QoS) requirements by not being able to meet the deadlines of I/O requests. In this paper, we propose a novel host interface I/O scheduler that is both GC aware and QoS aware. The proposed scheduler redistributes the GC overheads across noncritical I/O requests and reduces channel resource contention. Our experiments with workloads from various application domains revealed that the proposed client-level SSD scheduler reduces the standard deviation for latency by 52.5% and the worst-case latency by 86.6%, compared to the state-of-the-art I/O schedulers used for the HIL. In addition, for I/O requests smaller than a superpage, the proposed scheduler avoids channel resource conflicts and reduces latency by 29.2% in comparison to the state-of-the-art I/O schedulers. Furthermore, we present an extension of the proposed I/O scheduler for enterprise SSDs based on the NVMe protocol."}}
{"id": "9RpA6D42AN", "cdate": 1451606400000, "mdate": null, "content": {"title": "Enhanced dynamic background updating MOD algorithm for harsh environmental conditions", "abstract": "This paper presents an algorithm which is capable of detecting moving objects in challenging environmental conditions. This work is mainly based on the recently published Dynamic Background Updating (DBU) for lightweight moving object detection. In the DBU algorithm, a combined temporal difference and background subtraction technique that exploits both frame-based and pixel-based background updates is used to detect some moving objects in the application scenario with a static camera. Even though its complexity can be significantly reduced, the main drawback of DBU is its lower performance compared to the GMM-based algorithms at harsh environmental conditions which have a non-uniform illumination distribution or extremely high/low light. By presenting a pre-pixel processing of adaptive histogram equalization and a post-pixel processing of bounding box interpolation, this paper proposes an Enhanced DBU (EDBU) algorithm to accurately detect moving objects even in harsh environmental conditions. Experimental results show that the proposed algorithm does not only increase the detection rate of the previous DBU algorithm but also exceeds the performance of GMM-based algorithms by at least 15%."}}
{"id": "cJWeMXVAiUR", "cdate": 1420070400000, "mdate": null, "content": {"title": "Experimental Research Testbeds for Large-Scale WSNs: A Survey from the Architectural Perspective", "abstract": "Wireless sensor networks (WSNs) have a significant potential in diverse applications. In contrast to WSNs in a small-scale setting, the real-world adoption of large-scale WSNs is quite slow particularly due to the lack of robustness of protocols at all levels. Upon the demanding need for their experimental verification and evaluation, researchers have developed numerous WSN testbeds. While each individual WSN testbed contributes to the progress with its own unique innovation, still a missing element is an analysis on the overall system architecture and methodologies that can lead to systematic advances. This paper seeks to provide a framework to reason about the evolving WSN testbeds from the architectural perspective. We define three core requirements for WSN testbeds, which are scalability, flexibility, and efficiency. Then, we establish a taxonomy of WSN testbeds that represents the architectural design space by a hierarchy of design domains and associated design approaches. Through a comprehensive literature survey of existing prominent WSN testbeds, we examine their best practices for each design approach in our taxonomy. Finally, we qualitatively evaluate WSN testbeds for their responsiveness to the aforementioned core requirements by assessing the influence by each design approach on the core requirements and suggest future directions of research."}}
{"id": "SC7jjPzMW4_", "cdate": 1388534400000, "mdate": null, "content": {"title": "Multi-Channel Packet-Analysis System Based on IEEE 802.15.4 Packet-Capturing Modules", "abstract": "There have been increasing demands for research into multi-channel-based wireless sensor network protocols and applications to support requirements such as increased throughput and real-time or reliable transmission. Researchers or developers of these protocols and applications have to simultaneously analyze the exchanged packets for correctness of both their contents and message exchange timelines. However, if developers were to use multiple conventional single-channel packet sniffers for this purpose, debugging during development and the verification process becomes extremely tedious and difficult because of the need to check the correctness of the protocols over multiple channels individually. Therefore, we present a multi-channel packet-analysis system (MPAS) that helps in debugging and verification for multi-channel protocols or applications. Wireless packets are detected and timestamped by each sniffer module in the MPAS for each channel, and packets are preprocessed and transmitted to a GUI-based analyzer, which then parses the received packets and shows them in order. We present the design and implementation results of the MPAS and evaluate its performance by comparing it against a widely used packet sniffer."}}
{"id": "5SMAV94D8nA", "cdate": 1388534400000, "mdate": null, "content": {"title": "HIOS: A host interface I/O scheduler for Solid State Disks", "abstract": "Garbage collection (GC) and resource contention on I/O buses (channels) are among the critical bottlenecks in Solid State Disks (SSDs) that cannot be easily hidden. Most existing I/O scheduling algorithms in the host interface logic (HIL) of state-of-the-art SSDs are oblivious to such low-level performance bottlenecks in SSDs. As a result, SSDs may violate quality of service (QoS) requirements by not being able to meet the deadlines of I/O requests. In this paper, we propose a novel host interface I/O scheduler that is both GC-aware and QoS-aware. The proposed scheduler redistributes the GC overheads across non-critical I/O requests and reduces channel resource contention. Our experiments with workloads from various application domains reveal that the proposed scheduler reduces the standard deviation for latency over state-of-the-art I/O schedulers used in the HIL by 52.5%, and the worst-case latency by 86.6%. In addition, for I/O requests with sizes smaller than a superpage, our proposed scheduler avoids channel resource conflicts and reduces latency by 29.2% compared to the state-of-the-art."}}
{"id": "XZL998M1Cn4", "cdate": 1325376000000, "mdate": null, "content": {"title": "WiP Abstract: Packet Loss Compensation for Cyber-Physical Control Systems", "abstract": "Packet loss over wireless network incurs a great challenge over the system stability as well as its control performance in WSAN-based cyber-physical systems. This paper proposes a packet loss compensation technique, called by EWMA-based value estimator, for cyber-physical control over wireless network. Evaluation results simulated by Truetime/Simulink show the validity of the proposed scheme."}}
{"id": "JlboF4LiR5W", "cdate": 1325376000000, "mdate": null, "content": {"title": "Poster Abstract: Exploiting Virtually Constant Property for Time-Varying Delay Compensation", "abstract": "Delay compensation is becoming increasingly important due to time-varying delay characteristics over wireless network employed in WSAN-based cyber-physical control systems. Time-varying delay is hard to measure and compensate for it accurately in real-time. This paper presents a delay compensation technique with smith predictor by exploiting virtually constant delay property achieved by playback buffer. Simulation results show that the proposed scheme improves the control performance."}}
{"id": "9XISIz2Ivv4", "cdate": 1293840000000, "mdate": null, "content": {"title": "Probabilistic Soft Error Detection Based on Anomaly Speculation", "abstract": "Microprocessors are becoming increasingly vulnerable to soft errors due to the current trends of semiconductor technology scaling. Traditional redundant multi-threading architectures provide perfect fault tolerance by re-executing all the computations. However, such a full re-execution technique significantly increases the verification workload on the processor resources, resulting in severe performance degradation. This paper presents a pro-active verification management approach to mitigate the verification workload to increase its performance with a minimal effect on overall reliability. An anomaly-speculation-based filter checker is proposed to guide a verification priority before the re-execution process starts. This technique is accomplished by exploiting a value similarity property, which is defined by a frequent occurrence of partially identical values. Based on the biased distribution of similarity distance measure, this paper investigates further application to exploit similar values for soft error tolerance with anomaly speculation. Extensive measurements prove that the majority of instructions produce values, which are different from the previous result value, only in a few bits. Experimental results show that the proposed scheme accelerates the processor to be 180% faster than traditional fully-fault-tolerant processor with a minimal impact on overall soft error rate."}}
{"id": "mrntL-KhhRB", "cdate": 1199145600000, "mdate": null, "content": {"title": "Hierarchical Verification for Increasing Performance in Reliable Processors", "abstract": "Dynamic verification using the checker processor introduces severe degradation in performance unless the checker is as fast as the main processor core. Without widening the checker\u2019s bandwidth, we propose an active verification management (AVM) approach that utilizes a checker hierarchy. Before an instruction is verified at the checker processor, a filter checker marks a correctness non-criticality indicator (CNI) bit to indicate how likely its result is to be unimportant for reliability. AVM uses the CNI information to realize a congestion avoidance policy. Both reactive and proactive congestion avoidance policies are proposed to mitigate the performance degradation caused by the checker\u2019s congestion. Based on a simplified queueing model, we evaluate the proposed AVM analytically. Our experimental results show that AVM has the potential to solve the verification congestion problem when perfect fault coverage is not needed. With no AVM, congestion at the checker badly affects performance, to the tune of 57%, when compared to that of a non-fault-tolerant processor. With good marking by AVM, the performance of a reliable processor approaches 95% of that of a processor with no verification. Although instructions can be skipped on a random basis, such an approach reduces the fault coverage. A filter checker with a marking policy correlated with the correctness non-criticality metric, on the other hand, significantly reduces the soft error rate. Finally, we also present results showing the trade-off between performance and reliability."}}
{"id": "yriW7ht3HT4", "cdate": 1167609600000, "mdate": null, "content": {"title": "Harnessing Checker Hierarchy for Reliable Microprocessors", "abstract": "Traditional fault-tolerant multi-threading architectures provide good fault tolerance by re-executing all the computations. However, such a full re-execution significantly increases the demand on the processor resources, resulting in severe performance degradation. To address this problem, this dissertation presents Active Verification Management (AVM) approaches that utilize a checker hierarchy to increase its performance with a minimal effect on the overall reliability. Based on a simplified queueing model, AVM employs a filter checker which prioritizes the verification candidates to selectively do verification. This dissertation proposes three filter checkers - based on (1) result usage, (2) result bitwidth, and (3) result anomaly - that exploit correctness-criticality metrics and anomaly speculation. Binary Correctness Criticality (BCC) and Likelihood of Correctness Criticality (LoCC) are metrics that quantify whether an instruction is important for reliability or how likely an instruction is correctness-critical, respectively. Based on the BCC, a result-usage-based filter checker mitigates the verification workload by bypassing instructions that are unnecessary for correct execution. Computing the LoCC is accomplished by exploiting information redundancy of compressing computationally useful data bits. Numerical significance hints let the result-bitwidth-based filter checker guide a verification priority effectively before the re-execution process starts. A result-anomaly-based filter checker exploits a value similarity property, which is defined by a frequent occurrence of partially identical values. Based on the biased distribution of similarity distance measure, this dissertation further investigates another application to exploit similar values for soft error tolerance with anomaly speculation. Extensive measurements show that the majority of instructions produce values that are different from the previous result value only in a few bits. Experimental results show that the proposed schemes accelerate the processor to be 180% faster than traditional fully-fault-tolerant processor, with a minimal impact on the overall soft error rate. With no AVM, congestion at the checker badly affects performance, to the tune of 57%, when compared to that of a non-fault-tolerant processor. These results explain that the proposed AVM has the potential to solve the verification congestion problem when perfect fault coverage is not needed."}}
