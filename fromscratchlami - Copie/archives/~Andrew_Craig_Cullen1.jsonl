{"id": "fBqzk3doP9m", "cdate": 1672531200000, "mdate": 1683885949807, "content": {"title": "Exploiting Certified Defences to Attack Randomised Smoothing", "abstract": "In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \\emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \\%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certification mechanisms. We end the paper with a discussion of how these risks could potentially be mitigated."}}
{"id": "X7JZwYvrzZw", "cdate": 1672531200000, "mdate": 1683885949758, "content": {"title": "Failure-tolerant Distributed Learning for Anomaly Detection in Wireless Networks", "abstract": "The analysis of distributed techniques is often focused upon their efficiency, without considering their robustness (or lack thereof). Such a consideration is particularly important when devices or central servers can fail, which can potentially cripple distributed systems. When such failures arise in wireless communications networks, important services that they use/provide (like anomaly detection) can be left inoperable and can result in a cascade of security problems. In this paper, we present a novel method to address these risks by combining both flat- and star-topologies, combining the performance and reliability benefits of both. We refer to this method as \"Tol-FL\", due to its increased failure-tolerance as compared to the technique of Federated Learning. Our approach both limits device failure risks while outperforming prior methods by up to 8% in terms of anomaly detection AUROC in a range of realistic settings that consider client as well as server failure, all while reducing communication costs. This performance demonstrates that Tol-FL is a highly suitable method for distributed model training for anomaly detection, especially in the domain of wireless networks."}}
{"id": "AeTl9sbF-VT", "cdate": 1663850104097, "mdate": null, "content": {"title": "Exploiting Certified Defences to Attack Randomised Smoothing", "abstract": "Certified guarantees of adversarial robustness play an important role in providing assurances regarding a models output, irrespective of the behaviour of an attacker. However, while the development of such guarantees has drawn upon an improved understanding of attacker behaviour, so too can certified guarantees be exploited in order to generate more efficient adversarial attacks. Within this work, we explore this heretofore undiscovered additional attack surface, while also considering how previously discovered attacks could be applied to models defended by randomised smoothing. In all bar one experiment our approach generates smaller adversarial perturbations for more than $70 \\%$ of tested samples, reducing the average magnitude of the adversarial perturbation by $13 \\%$."}}
{"id": "WbnvmtD9N1g", "cdate": 1652737648793, "mdate": null, "content": {"title": "Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity", "abstract": "In response to subtle adversarial examples flipping classifications of neural network models, recent research has promoted certified robustness as a solution. There, invariance of predictions to all norm-bounded attacks is achieved through randomised smoothing of network inputs. Today's state-of-the-art certifications make optimal use of the class output scores at the input instance under test: no better radius of certification (under the $L_2$ norm) is possible given only these score. However, it is an open question as to whether such lower bounds can be improved using local information around the instance under test.  In this work, we demonstrate how today's ``optimal'' certificates can be improved by exploiting both the transitivity of certifications, and the geometry of the input space, giving rise to what we term Geometrically-Informed Certified Robustness. By considering the smallest distance to points on the boundary of a set of certifications this approach improves certifications for more than $80 \\%$ of Tiny-Imagenet instances, yielding an on average $5\\%$ increase in the associated certification. When incorporating training time processes that enhance the certified radius, our technique shows even more promising results, with a uniform $4$ percentage point increase in the achieved certified radius."}}
{"id": "zVLO029l6y", "cdate": 1640995200000, "mdate": 1683885949810, "content": {"title": "Generative Adversarial Networks for anomaly detection on decentralised data", "abstract": ""}}
{"id": "p6n8kQ67wfc", "cdate": 1640995200000, "mdate": 1683885949800, "content": {"title": "Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity", "abstract": "In response to subtle adversarial examples flipping classifications of neural network models, recent research has promoted certified robustness as a solution. There, invariance of predictions to all norm-bounded attacks is achieved through randomised smoothing of network inputs. Today's state-of-the-art certifications make optimal use of the class output scores at the input instance under test: no better radius of certification (under the $L_2$ norm) is possible given only these score. However, it is an open question as to whether such lower bounds can be improved using local information around the instance under test. In this work, we demonstrate how today's \"optimal\" certificates can be improved by exploiting both the transitivity of certifications, and the geometry of the input space, giving rise to what we term Geometrically-Informed Certified Robustness. By considering the smallest distance to points on the boundary of a set of certifications this approach improves certifications for more than $80\\%$ of Tiny-Imagenet instances, yielding an on average $5 \\%$ increase in the associated certification. When incorporating training time processes that enhance the certified radius, our technique shows even more promising results, with a uniform $4$ percentage point increase in the achieved certified radius."}}
{"id": "MkEBxzhNYL", "cdate": 1640995200000, "mdate": 1683880071216, "content": {"title": "Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity", "abstract": "In response to subtle adversarial examples flipping classifications of neural network models, recent research has promoted certified robustness as a solution. There, invariance of predictions to all norm-bounded attacks is achieved through randomised smoothing of network inputs. Today's state-of-the-art certifications make optimal use of the class output scores at the input instance under test: no better radius of certification (under the $L_2$ norm) is possible given only these score. However, it is an open question as to whether such lower bounds can be improved using local information around the instance under test. In this work, we demonstrate how today's ``optimal'' certificates can be improved by exploiting both the transitivity of certifications, and the geometry of the input space, giving rise to what we term Geometrically-Informed Certified Robustness. By considering the smallest distance to points on the boundary of a set of certifications this approach improves certifications for more than $80 \\%$ of Tiny-Imagenet instances, yielding an on average $5\\%$ increase in the associated certification. When incorporating training time processes that enhance the certified radius, our technique shows even more promising results, with a uniform $4$ percentage point increase in the achieved certified radius."}}
{"id": "KvxvK3ccEX", "cdate": 1640995200000, "mdate": 1683885949737, "content": {"title": "Wireless Network Simulation to Create Machine Learning Benchmark Data", "abstract": "While several wireless network simulators exist, the absence of modern, standardised network datasets may adversely affect the application of machine learning methods to problems involving wireless networks. Due to the difficulty of acquiring and sharing more modern network datasets, many of the rapidly evolving techniques in machine learning are only ever ported to network analysis through archaic network datasets such as KDD'99-creating a divide between communication networks and machine learning. To address this divide, this paper presents a new network simulation framework that brings existing network and machine learning tools together to conveniently generate data consisting of PCAP or raw physical layer data and derived statistics in a format that is directly consumable by machine learning algorithms. The proposed simulation frame-work allows the user to design custom networks through a simple configuration file-based scheme instead of learning a sophisticated network simulator. Measuring our framework's performance on consumer-grade devices, raw data is efficiently generated at up to 1, 200 Ethernet frames per second, and processed into feature vectors of 5.6 samples per second (where each sample uses 1 second of simulation time) or 0.29 samples per second if physical layer signals are used."}}
{"id": "7ojLpZ_9Ex", "cdate": 1640995200000, "mdate": 1683885949719, "content": {"title": "Adversarial Decisions on Complex Dynamical Systems using Game Theory", "abstract": "We apply computational Game Theory to a unification of physics-based models that represent decision-making across a number of agents within both cooperative and competitive processes. Here the competitors try to both positively influence their own returns, while negatively affecting those of their competitors. Modelling these interactions with the so-called Boyd-Kuramoto-Lanchester (BKL) complex dynamical system model yields results that can be applied to business, gaming and security contexts. This paper studies a class of decision problems on the BKL model, where a large set of coupled, switching dynamical systems are analysed using game-theoretic methods. Due to their size, the computational cost of solving these BKL games becomes the dominant factor in the solution process. To resolve this, we introduce a novel Nash Dominant solver, which is both numerically efficient and exact. The performance of this new solution technique is compared to traditional exact solvers, which traverse the entire game tree, as well as to approximate solvers such as Myopic and Monte Carlo Tree Search (MCTS). These techniques are assessed, and used to gain insights into both nonlinear dynamical systems and strategic decision making in adversarial environments."}}
{"id": "iDJ6mC-9Cs", "cdate": 1609459200000, "mdate": 1683885949756, "content": {"title": "Privacy-Preserving Collaborative SDR Networks for Anomaly Detection", "abstract": "By offering unprecedented flexibility for wireless communications, Software-Defined Radio (SDR) is an enabling technology for distributed platforms such as Internet-of-Things (IoT) and cognitive radio. We propose a network of SDR devices with sensing and computation capabilities without relying on a centralized architecture to monitor and develop a detailed representation of network behavior in the radio frequency spectrum. This network representation provides a foundation for the task of anomaly detection for security purposes, in which SDRs collect the data needed to identify adversarial actors in a selected environment. We present and evaluate a novel distributed anomaly detection scheme for SDRs that applies Round Robin Learning (RRL) and Random Exchange Learning (REL) strategies to a type of machine learning model known as an autoencoder. In doing so, participating devices fill in gaps in a single model\u2019s data representation, in order to build a model closer to that achieved through centralized training, while maintaining the advantages of distributed systems. The distributed architecture developed improves network robustness, decreases the amount of intranetwork communication, and preserves privacy of individual SDR subnetworks. Furthermore, it was found to consistently raise the average anomaly detection performance for groups of participating devices without exchanging any training data."}}
