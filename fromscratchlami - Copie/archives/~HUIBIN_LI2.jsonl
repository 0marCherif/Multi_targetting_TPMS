{"id": "J516fE6WO8", "cdate": 1672531200000, "mdate": 1672149897813, "content": {"title": "Meta-learning-based adversarial training for deep 3D face recognition on point clouds", "abstract": ""}}
{"id": "FFpdPaJINCX", "cdate": 1640995200000, "mdate": 1682321197842, "content": {"title": "Learning Optimal Transport Mapping of Joint Distribution for Cross-scenario Face Anti-spoofing", "abstract": "Face anti-spoofing (FAS) under different scenarios is a challenging and indispensable task for a real face recognition system. In this paper, we propose a novel cross-scenario FAS method by learning the optimal transport mapping of joint distributions under the unsupervised domain adaption framework, namely OTJD-FAS. In particular, given the training and testing real or fake face samples from different scenarios (i.e., source and target domains), their deep CNN features are firstly extracted and the labels of the test samples are firstly predicted by an initial binary classifier. Then, the gap of joint distributions (i.e., in the product space of deep features and their corresponding labels) between training and testing sets is measured by the Wasserstein distance and their optimal transport mapping is learned. Finally, an adaptive cross-entropy loss for classification and cross-entropy loss of testing labels are employed for the final FAS. Extensive experimental results demonstrated on the MSU-MFSD, CASIA-FASD and Idiap REPLAY-ATTACK databases under cross-scenario setting show that aligning joint distributions is more effective than the widely used only aligning marginal distributions based methods and the proposed method can achieve competitive performance for cross-scenario FAS."}}
{"id": "BYE1E4d96R", "cdate": 1640995200000, "mdate": 1682321197706, "content": {"title": "Dense Feature Learning and Compact Cost Aggregation for Deep Stereo Matching", "abstract": "Recently, Convolutional Neural Networks (CNN) based deep models have been successfully applied to the task of stereo matching. In this paper, we propose a novel deep stereo matching network based on the strategies of dense feature learning and compact cost aggregation, namely DFL-CCA-Net. It consists of three modules: Dense Feature Learning (DFL), Compact Cost Aggregation (CCA) and the disparity regression module. In DFL module, the CNN backbone with Dense Atrous Spatial Pyramid Pooling (DenseASPP) is employed to extract multi-scale deep feature maps of the given left and right images respectively. Then an initial 4D cost volume is obtained by concatenating left feature maps with their corresponding right feature maps across each disparity level. In the following CCA module, each initial 3D cost volume component (i.e., the component across the left or right image feature channel dimension) is aggregated into a more compact one by using the atrous convolution operation with different expansion rates. These updated 3D cost volume components are then fed into the disparity regression module, which consisting of a 3D CNN network with a stacked hourglass structure, to estimate the final disparity map. Comprehensive experimental results demonstrated on the Scene Flow, KITTI 2012 and KITTI 2015 datasets show that the 3D cost volume components obtained by the proposed DFL and CCA modules generally containing more multi-scale semantic information and thus can largely improve the final disparity regression accuracies. Compared with other deep stereo matching methods, DFL-CCA-Net achieves very competitive prediction accuracies especially in the reflective regions and regions containing detail information."}}
{"id": "01-Y94h_LJG", "cdate": 1640995200000, "mdate": 1682321197837, "content": {"title": "An Unsupervised Monocular Visual Odometry Based on Multi-Scale Modeling", "abstract": "Unsupervised deep learning methods have shown great success in jointly estimating camera pose and depth from monocular videos. However, previous methods mostly ignore the importance of multi-scale information, which is crucial for pose estimation and depth estimation, especially when the motion pattern is changed. This article proposes an unsupervised framework for monocular visual odometry (VO) that can model multi-scale information. The proposed method utilizes densely linked atrous convolutions to increase the receptive field size without losing image information, and adopts a non-local self-attention mechanism to effectively model the long-range dependency. Both of them can model objects of different scales in the image, thereby improving the accuracy of VO, especially in rotating scenes. Extensive experiments on the KITTI dataset have shown that our approach is competitive with other state-of-the-art unsupervised learning-based monocular methods and is comparable to supervised or model-based methods. In particular, we have achieved state-of-the-art results on rotation estimation."}}
{"id": "R9nGEllVBx", "cdate": 1609459200000, "mdate": 1667559589848, "content": {"title": "Learning Flexibly Distributional Representation for Low-quality 3D Face Recognition", "abstract": "Due to the superiority of using geometric information, 3D Face Recognition (FR) has achieved great successes. Existing methods focus on high-quality 3D FR which is unpractical in real scenarios. Low-quality 3D FR is a more realistic scenario but the low-quality data are born with heavy noises. Therefore, exploring noise-robust low-quality 3D FR methods becomes an urgent and challenging problem. To solve this issue, in this paper, we propose to learn flexibly distributional representation for low-quality 3D FR. Firstly, we introduce the distributional representation for low-quality 3D faces due to that it can weaken the impact of noises. Generally, the distributional representation of a given 3D face is restricted to a specific distribution such as Gaussian distribution. However, the specific distribution may be not up to describing the complex low-quality face. Therefore, we propose to transform this specific distribution to a flexible one via Continuous Normalizing Flow (CNF), which can get rid of the form limitation. This kind of flexible distribution can approximate the latent distribution of the given noisy face more accurately, which further improves accuracy of low-quality 3D FR. Comprehensive experiments show that our proposed method improves both low-quality and cross-quality 3D FR performances on low-quality benchmarks. Furthermore, the improvements are more remarkable on low-quality 3D faces when the intensity of noise increases which indicate the robustness"}}
{"id": "L18b3TYmN7", "cdate": 1609459200000, "mdate": 1668926628655, "content": {"title": "A distribution independence based method for 3D face shape decomposition", "abstract": ""}}
{"id": "CJXBWBEw91", "cdate": 1581727028533, "mdate": null, "content": {"title": "Neural Multi-Atlas Label Fusion: Application to Cardiac MR Images", "abstract": "Multi-atlas segmentation approach is one of the most widely-used image segmentation techniques in biomedical applications. There are two major challenges in this category of methods, i.e., atlas selection and label fusion. In this paper, we propose a novel multi-atlas segmentation method that formulates multi-atlas segmentation in a deep learning framework for better solving these challenges. The proposed method, dubbed deep fusion net (DFN), is a deep architecture that integrates a feature extraction subnet and a non-local patch-based label fusion (NL-PLF) subnet in a single network. The network parameters are learned by end-to-end training for automatically learning deep features that enable optimal performance in a NL-PLF framework. The learned deep features are further utilized in defining a similarity measure for atlas selection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09 for left ventricle segmentation, our approach achieved 0.833 in averaged Dice metric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on LV-09 dataset, comparing favorably with the other automatic left ventricle segmentation methods. We also tested our approach on Cardiac Atlas Project (CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method achieved 0.815 in ADM, ranking highest at the time of writing."}}
{"id": "Iz9b7I5z5w", "cdate": 1581726727961, "mdate": null, "content": {"title": "ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing", "abstract": "Compressive sensing (CS) is an effective technique for reconstructing image from a small amount of sampled data. It has been widely applied in medical imaging, remote sensing, image compression, etc. In this paper, we propose two versions of a novel deep learning architecture, dubbed as ADMM-CSNet, by combining the traditional model-based CS method and data-driven deep learning method for image reconstruction from sparsely sampled measurements. We first consider a generalized CS model for image reconstruction with undetermined regularizations in undetermined transform domains, and then two efficient solvers using Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the model are proposed. We further unroll and generalize the ADMM algorithm to be two deep architectures, in which all parameters of the CS model and the ADMM algorithm are discriminatively learned by end-to-end training. For both applications of fast CS complex-valued MR imaging and CS imaging of real-valued natural images, the proposed ADMM-CSNet achieved favorable reconstruction accuracy in fast computational speed compared with the traditional and the other deep learning methods."}}
{"id": "w7i3-_DFdS", "cdate": 1577836800000, "mdate": 1668700418455, "content": {"title": "Discovering influential factors in variational autoencoders", "abstract": ""}}
{"id": "sFfi_68gfK", "cdate": 1577836800000, "mdate": 1682321198151, "content": {"title": "ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing", "abstract": "Compressive sensing (CS) is an effective technique for reconstructing image from a small amount of sampled data. It has been widely applied in medical imaging, remote sensing, image compression, etc. In this paper, we propose two versions of a novel deep learning architecture, dubbed as ADMM-CSNet, by combining the traditional model-based CS method and data-driven deep learning method for image reconstruction from sparsely sampled measurements. We first consider a generalized CS model for image reconstruction with undetermined regularizations in undetermined transform domains, and then two efficient solvers using Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the model are proposed. We further unroll and generalize the ADMM algorithm to be two deep architectures, in which all parameters of the CS model and the ADMM algorithm are discriminatively learned by end-to-end training. For both applications of fast CS complex-valued MR imaging and CS imaging of real-valued natural images, the proposed ADMM-CSNet achieved favorable reconstruction accuracy in fast computational speed compared with the traditional and the other deep learning methods."}}
