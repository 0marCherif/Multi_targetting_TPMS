{"id": "4UT_hm1qQ-D", "cdate": 1682899200000, "mdate": 1695953386463, "content": {"title": "Comparative validation of machine learning algorithms for surgical workflow and skill analysis with the HeiChole benchmark", "abstract": ""}}
{"id": "Uax7o53PHo", "cdate": 1675209600000, "mdate": 1695953386459, "content": {"title": "Toward Robust Partial-Image Based Template Matching Techniques for MRI-Guided Interventions", "abstract": "We have developed an MRI-safe needle guidance toolkit for MRI-guided interventions intended to enable accurate positioning for needle-based procedures. The toolkit allows intuitive and accurate needle angulation and entry point positioning according to an MRI-based plan, using a flexible, patterned silicone 2D grid. The toolkit automatically matches the grid on MRI planning images with a physical silicon grid placed conformally on the patient\u2019s skin and provides the Interventional Radiologist an easy-to-use guide showing the needle entry point on the silicon grid as well as needle angle information. The radiologist can use this guide along with a 2-degree-of-freedom (rotation and angulation relative to the entry point) hand-held needle guide to place the needle into the anatomy of interest. The initial application that we are considering for this toolkit is arthrography, a diagnostic procedure to evaluate the joint space condition. However, this toolkit could be used for any needle-based and percutaneous procedures such as MRI-guided biopsy and facet joint injection. For matching the images, we adopt a transformation parameter estimation technique using the phase-only correlation method in the frequency domain. We investigated the robustness of this method against rotation, displacement, and Rician noise. The algorithm was able to successfully match all the dataset images. We also investigated the accuracy of identifying the entry point from registered template images as a prerequisite for a future targeting study. Application of the template matching algorithm to locate the needle entry points within the MRI dataset resulted in an average entry point location estimation accuracy of 0.12 \u00b10.2 mm. This promising result motivates a more detailed assessment of this algorithm in the future including a targeting study on a silicon phantom with embedded plastic targets to investigate the end-to-end accuracy of this automatic template matching algorithm in the interventional MRI room."}}
{"id": "phkldSdrUK", "cdate": 1640995200000, "mdate": 1695953386478, "content": {"title": "EADTC: An Approach to Interpretable and Accurate Crime Prediction", "abstract": "Machine learning applications related to high-stakes decisions are often surrounded by significant amounts of controversy. This has led to increasing interest in interpretable machine learning models. A well-known class of interpretable models is that of decision trees (DTs), which mirror a common strategy used by humans to arrive at solutions through a series of well-defined decisions. However, much of previous research on DTs for criminal justice predictions has focused primarily on collections (ensembles) of DTs whose results are aggregated together. Such DT ensembles are used to help improve accuracy; however, their increased complexity and deviation from human decision-making processes makes them much less interpretable compared to single-DT approaches. In this paper, we present a new DT model for criminal recidivism prediction that is designed with high interpretability, accuracy, and fairness as core objectives. The interpretability of the model stems from its formulation in terms of a single DT structure, while accuracy is achieved through an intensive optimization process of DT parameters that is carried out using a novel evolutionary algorithm. Through extensive experiments, we analyze the performance of our proposed EADTC (Evolutionary Algorithm Decision Tree for Crime prediction) method on relevant datasets. Our experiments show that the EADTC approach achieves competitive accuracy and fairness with respect to state-of-the-art ensemble DT models, while achieving higher interpretability due to the simpler, single-DT structure."}}
{"id": "Wgsi9QngJke", "cdate": 1640995200000, "mdate": 1692983594408, "content": {"title": "Development of a Modular Real-time Shared-control System for a Smart Wheelchair", "abstract": "In this paper, we propose a modular navigation system that can be mounted on a regular powered wheelchair to assist disabled children and the elderly with autonomous mobility and shared-control features. The lack of independent mobility drastically affects an individual's mental and physical health making them feel less self-reliant, especially children with Cerebral Palsy and limited cognitive skills. To address this problem, we propose a comparatively inexpensive and modular system that uses a stereo camera to perform tasks such as path planning, obstacle avoidance, and collision detection in environments with narrow corridors. We avoid any major changes to the hardware of the wheelchair for an easy installation by replacing wheel encoders with a stereo camera for visual odometry. An open source software package, the Real-Time Appearance Based Mapping package, running on top of the Robot Operating System (ROS) allows us to perform visual SLAM that allows mapping and localizing itself in the environment. The path planning is performed by the move base package provided by ROS, which quickly and efficiently computes the path trajectory for the wheelchair. In this work, we present the design and development of the system along with its significant functionalities. Further, we report experimental results from a Gazebo simulation and real-world scenarios to prove the effectiveness of our proposed system with a compact form factor and a single stereo camera."}}
{"id": "IEPdemqRfG5", "cdate": 1640995200000, "mdate": 1695953386458, "content": {"title": "Exploiting Simplified Depth Estimation for Stereo-based 2D Object Detection", "abstract": "Stereo image inputs provide higher object detection accuracy than monocular images by enabling the detection of objects that are missed from one view while being detectable from another view. To take advantage of additional information from the secondary image, it is necessary to search for the corresponding region in the images of different views by projecting with depth information of the target object. However, most existing studies utilize highly complex computations to estimate the depth for simple 2D object detection. This complexity limits the potential for deploying the methods on platforms, such as unmanned aerial vehicles, that involve significant resource constraints. In this paper, we introduce a simplified depth approximation to obtain depth information by quantizing the depth values into a small number of representative values. With these values, the regions of interest are projected to the secondary image to concatenate the information from the additional image. We validate our method with the KITTI dataset. Our results show that while having very low complexity, our approximation method leads to greatly improved object detection performance in two out of three difficulty groups of the dataset, and comparable performance in the other difficulty group compared to use of monocular image input."}}
{"id": "5dm63IoC6s", "cdate": 1640995200000, "mdate": 1695953386465, "content": {"title": "Validation of object detection in UAV-based images using synthetic data", "abstract": "Object detection is increasingly used onboard Unmanned Aerial Vehicles (UAV) for various applications; however, the machine learning (ML) models for UAV-based detection are often validated using data curated for tasks unrelated to the UAV application. This is a concern because training neural networks on large-scale benchmarks have shown excellent capability in generic object detection tasks, yet conventional training approaches can lead to large inference errors for UAV-based images. Such errors arise due to differences in imaging conditions between images from UAVs and images in training. To overcome this problem, we characterize boundary conditions of ML models, beyond which the models exhibit rapid degradation in detection accuracy. Our work is focused on understanding the impact of different UAV-based imaging conditions on detection performance by using synthetic data generated using a game engine. Properties of the game engine are exploited to populate the synthetic datasets with realistic and annotated images. Specifically, it enables the fine control of various parameters, such as camera position, view angle, illumination conditions, and object pose. Using the synthetic datasets, we analyze detection accuracy in different imaging conditions as a function of the above parameters. We use three well-known neural network models with different model complexity in our work. In our experiment, we observe and quantify the following: 1) how detection accuracy drops as the camera moves toward the nadir-view region; 2) how detection accuracy varies depending on different object poses, and 3) the degree to which the robustness of the models changes as illumination conditions vary."}}
{"id": "3WW9skCc0G", "cdate": 1640995200000, "mdate": 1695953386486, "content": {"title": "Real-time Face Tracking Based on a Configurable Ensemble of Detectors", "abstract": "The deployment of face tracking capabilities at the network edge requires (near) real-time performance under strict computational and energy constraints. Existing approaches often use object detectors with low complexity for tracking to satisfy limited resource constraints. An obvious problem with limiting complexity is that it has a direct impact on performance (e.g., ability to detect faces), especially at lower resolutions. In this study, we present a novel face tracking system for edge computing devices, which combines a tracking-by-detection algorithm with an ensemble of detectors. This system utilizes an online decision-making strategy based on extracting scene information from a density map to inform the active configuration of object detectors and image resolution. Using this system, we enhance real-time processing capability and reduce energy consumption by adaptively making trade-offs in resolution and active detectors to maintain tracking performance while minimizing resource costs. The proposed face tracking system is coupled with a multi-frame bounding box matching algorithm to provide multi-facial tracking functionality. We demonstrate the effectiveness of our system through experiments using the Multiple Object Tracking (MOT) Head Tracking 21 dataset."}}
{"id": "xqWBp2so_VO", "cdate": 1609459200000, "mdate": 1631926962074, "content": {"title": "Deep Super-Resolution Imaging Technology: Toward Optical Super-Vision", "abstract": "Spatially variant optical blur is inevitable in real optical imaging systems with imperfect camera lenses since each object has a different depth in a real scene. Unfortunately, existing super-resolution imaging techniques may not achieve satisfactory performance on images acquired from practical optical imaging systems, where the optical blur is unknown. This is because a predefined global point spread function is adopted for every spatial coordinate without any optical blur estimation in the existing models. To address the challenging issue in recent video technologies, in this article, we propose an optimal super-resolution imaging network based on object-adaptive optical point spread function estimation. In technical, a new object sharpness consistency cost is suggested in the dark channel domain to estimate the point spread function, which is optimal to the super-resolution imaging. In addition, an object image sharpening network is proposed and adaptively combined with the point spread function estimation. The performance of the proposed network is also validated via experiments in terms of various quantitative metrics such as the peak signal-to-noise ratio, S3 index, and BRISQUE values."}}
{"id": "tvMf5SEbgq", "cdate": 1609459200000, "mdate": 1695953386474, "content": {"title": "Deep-Learning Based Image Analysis on Resource-Constrained Systems", "abstract": "In recent years, deep learning has led to high-end performance on a very wide variety of computer vision tasks. Among different types of deep neural networks, convolutional neural networks (CNNs) are extensively studied and utilized for image analysis purposes, as CNNs have the capability to effectively capture spatial and temporal dependencies in images. The growth in the amount of annotated image data and improvements in graphics processing units are factors in the rapid gain in popularity of CNN-based image analysis systems. This growth in turn motivates investigation into the application of CNN-based deep learning to increasingly complex tasks, including an increasing variety applications at the network edge. The application of deep CNNs to novel edge applications involves two major challenges. First, in many of the emerging edge-based application areas, there is a lack of sufficient training data or an uneven class balance within the datasets. Second, stringent implementation constraints --- including constraints on real-time performance, memory requirements, and energy consumption --- must be satisfied to enable practical deployment. In this thesis, we address these challenges in developing deep-CNN-based image analysis systems for deployment on resource-constrained devices at the network edge. To tackle the challenges for medical image analysis, we first propose a methodology and tool for semi-automated training dataset generation in support of robust segmentation. The framework is developed to provide robust segmentation of surgical instruments using deep learning. We then address the problem of training dataset generation for real-time object tracking using a weakly supervised learning method. In particular, we present a weakly supervised method for surgical tool tracking based on a class of hybrid sensor systems. The targeted class of systems combines electromagnetic (EM) and vision-based modalities. Furthermore, we present a new framework for assessing the quality of nonrigid multimodality image registration in real-time. With the augmented dataset, we construct a solution using various registration quality metrics that are integrated to form a single binary assessment of image registration effectiveness as either high quality or low quality. To address challenges in practical deployment, we present a deep-learning-based hyperspectral image (HSI) classification method that is designed for deployment on resource-constrained devices at the network edge. Due to the large volumes of data produced by HSI sensors, and the complexity of deep neural network (DNN) architectures, developing DNN solutions for HSI classification on resource-constrained platforms is a challenging problem. In this part of the thesis, we introduce a novel approach that integrates DNN-based image analysis with discrete cosine transform (DCT) analysis for HSI classification. In addition to medical image processing and HSI classification, a third application area that we investigate in this thesis is on-board object detection from Unmanned Aerial Vehicles (UAVs), which represents another important domain of interest for the edge-based deployment of CNN methods. In this part of the thesis, we present a novel framework for object detection using images captured from UAVs. The framework is optimized using synthetic datasets that are generated from a game engine to capture imaging scenarios that are specific to the UAV-based operating environment. Using the generated synthetic dataset, we develop new insight on the impact of different UAV-based imaging conditions on object detection performance."}}
{"id": "jgi9BgXBn20", "cdate": 1609459200000, "mdate": 1631926962083, "content": {"title": "DCT-based Hyperspectral Image Classification on Resource-Constrained Platforms", "abstract": "Deep learning based approaches to hyperspectral image analysis have attracted large attention and exhibited high performance in image classification tasks. However, deployment of deep learning based hyperspectral image analysis systems is challenging due to the computational complexity of deep learning and the large amount of data involved in hyperspectral images. To address this problem, this paper introduces a novel framework that integrates deep neural network (DNN) based image analysis by learning the network from discrete cosine transform (DCT) coefficients for hyperspectral image classification. The framework allows designers to derive diverse implementation configurations using a variable number of DCT coefficients for training. These configurations can be used to flexibly trade off classification accuracy and computational cost (e.g., based on specific characteristics of the device being used or based on time-varying operating requirements). Through experiments using a publicly available remote sensing dataset and a resource constrained Android platform, we demonstrate that our proposed approach enables the streamlined deployment of DNN-based hyperspectral image classification."}}
