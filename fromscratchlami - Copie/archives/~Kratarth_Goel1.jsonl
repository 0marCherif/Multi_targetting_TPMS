{"id": "jBhli8TMY1", "cdate": 1640995200000, "mdate": 1668218594213, "content": {"title": "Wayformer: Motion Forecasting via Simple & Efficient Attention Networks", "abstract": "Motion forecasting for autonomous driving is a challenging task because complex driving scenarios result in a heterogeneous mix of static and dynamic inputs. It is an open problem how best to represent and fuse information about road geometry, lane connectivity, time-varying traffic light state, and history of a dynamic set of agents and their interactions into an effective encoding. To model this diverse set of input features, many approaches proposed to design an equally complex system with a diverse set of modality specific modules. This results in systems that are difficult to scale, extend, or tune in rigorous ways to trade off quality and efficiency. In this paper, we present Wayformer, a family of attention based architectures for motion forecasting that are simple and homogeneous. Wayformer offers a compact model description consisting of an attention based scene encoder and a decoder. In the scene encoder we study the choice of early, late and hierarchical fusion of the input modalities. For each fusion type we explore strategies to tradeoff efficiency and quality via factorized attention or latent query attention. We show that early fusion, despite its simplicity of construction, is not only modality agnostic but also achieves state-of-the-art results on both Waymo Open MotionDataset (WOMD) and Argoverse leaderboards, demonstrating the effectiveness of our design philosophy"}}
{"id": "o7ny_MRPXE2", "cdate": 1609459200000, "mdate": 1668218594286, "content": {"title": "QuadroNet: Multi-Task Learning for Real-Time Semantic Depth Aware Instance Segmentation", "abstract": "Vision for autonomous driving is a uniquely challenging problem: the number of tasks required for full scene understanding is large and diverse; the quality requirements on each task are stringent due to the safety-critical nature of the application; and the latency budget is limited, requiring real-time solutions. In this work we address these challenges with QuadroNet, a one-shot network that jointly produces four outputs: 2D detections, instance segmentation, semantic segmentation, and monocular depth estimates in real-time (>60fps) on consumer-grade GPU hardware. On a challenging real-world autonomous driving dataset, we demonstrate an increase of+2.4% mAP for detection, +3.15% mIoU for semantic segmentation, +5.05% mAP@0.5 for instance segmentation and +1.36% in \u03b4 <; 1.25 for depth prediction over a baseline approach. We also compare our work against other multi-task learning approaches on Cityscapes and demonstrate state-of-the-art results."}}
{"id": "eIUAU9QHQ1", "cdate": 1483228800000, "mdate": 1668218594216, "content": {"title": "Learning to Predict Human Behavior in Crowded Scenes", "abstract": ""}}
{"id": "BkbP0yz_WB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Social LSTM: Human Trajectory Prediction in Crowded Spaces", "abstract": "Pedestrians follow different trajectories to avoid obstacles and accommodate fellow pedestrians. Any autonomous vehicle navigating such a scene should be able to foresee the future positions of pedestrians and accordingly adjust its path to avoid collisions. This problem of trajectory prediction can be viewed as a sequence generation task, where we are interested in predicting the future trajectory of people based on their past positions. Following the recent success of Recurrent Neural Network (RNN) models for sequence prediction tasks, we propose an LSTM model which can learn general human movement and predict their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We demonstrate the performance of our method on several public datasets. Our model outperforms state-of-the-art methods on some of these datasets. We also analyze the trajectories predicted by our model to demonstrate the motion behaviour learned by our model."}}
{"id": "oC0Ed_2S1jG", "cdate": 1420070400000, "mdate": 1668218594159, "content": {"title": "Modeling temporal dependencies in data using a DBN-LSTM", "abstract": ""}}
{"id": "lb6pNa63i-", "cdate": 1420070400000, "mdate": 1668218594094, "content": {"title": "A Recurrent Latent Variable Model for Sequential Data", "abstract": "In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state."}}
{"id": "SkEaPd-u-H", "cdate": 1420070400000, "mdate": null, "content": {"title": "A Recurrent Latent Variable Model for Sequential Data", "abstract": "In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN) can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics."}}
{"id": "kFhjqoh2Sw", "cdate": 1388534400000, "mdate": 1668218594216, "content": {"title": "Learning Temporal Dependencies in Data Using a DBN-BLSTM", "abstract": "Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results."}}
{"id": "ScwaQneE-0p", "cdate": 1388534400000, "mdate": 1668218594329, "content": {"title": "A Novel Feature Selection and Extraction Technique for Classification", "abstract": "This paper presents a versatile technique for the purpose of feature selection and extraction - Class Dependent Features (CDFs). We use CDFs to improve the accuracy of classification and at the same time control computational expense by tackling the curse of dimensionality. In order to demonstrate the generality of this technique, it is applied to handwritten digit recognition and text categorization."}}
{"id": "RPgxLJbkk7", "cdate": 1388534400000, "mdate": 1668218594152, "content": {"title": "Polyphonic Music Generation by Modeling Temporal Dependencies Using a RNN-DBN", "abstract": ""}}
