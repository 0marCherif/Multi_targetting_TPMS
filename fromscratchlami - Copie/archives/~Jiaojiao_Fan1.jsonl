{"id": "GbCtGloSdJ", "cdate": 1676827078703, "mdate": null, "content": {"title": "Generating Synthetic Datasets by Interpolating along Generalized Geodesics", "abstract": "  Data for pretraining machine learning models often consists of collections of heterogeneous datasets. Although training on their union is reasonable in agnostic settings, it might be suboptimal when the target domain ---where the model will ultimately be used--- is known in advance. In that case, one would ideally pretrain only on the dataset(s) most similar to the target one. Instead of limiting this choice to those datasets already present in the pretraining collection, here we explore extending this search to all datasets that can be synthesized as `combinations' of them. We define such combinations as multi-dataset interpolations, formalized through the notion of generalized geodesics from optimal transport (OT) theory. We compute these geodesics using a recent notion of distance between labeled datasets, and derive alternative interpolation schemes based on it: using either barycentric projections or optimal transport maps, the latter computed using recent neural OT methods. These methods are scalable, efficient, and ---notably--- can be used to interpolate even between datasets with distinct and unrelated label sets. Through various experiments in transfer learning in computer vision, we demonstrate this is a promising new approach for targeted on-demand dataset synthesis. "}}
{"id": "NNvpIQqsjtD", "cdate": 1664816293969, "mdate": null, "content": {"title": "Generating Synthetic Datasets by Interpolating along Generalized Geodesics", "abstract": "Data for pretraining machine learning models often consists of collections of heterogeneous datasets. Although training on their union is reasonable in agnostic settings, it might be suboptimal when the target domain ---where the model will ultimately be used--- is known in advance. In that case, one would ideally pretrain only the dataset(s) most similar to the target one. Instead of limiting this choice to those datasets already present in the pretraining collection, here we explore extending this search to all datasets that can be synthesized as `combinations' of them. We define such combinations as multi-dataset interpolations, formalized through the notion of generalized geodesics from optimal transport (OT) theory. We compute these geodesics using a recent notion of distance between labeled datasets, and derive alternative interpolation schemes based on it: using either barycentric projections or optimal transport maps, the latter computed using recent neural OT methods. These methods are scalable, efficient, and ---notably--- can be used to interpolate even between datasets with distinct and unrelated label sets. Through various experiments in transfer learning, we demonstrate this promising new approach to targeted on-demand dataset synthesis."}}
{"id": "rEnGR3VdDW5", "cdate": 1646916789827, "mdate": null, "content": {"title": "Scalable Computation of Monge Maps with General Costs", "abstract": "Monge map refers to the optimal transport map between two probability distributions and provides a principled approach to transform one distribution to another. In spite of the rapid developments of the numerical methods for optimal transport problems, computing the Monge maps remains challenging, especially for high dimensional problems. In this paper, we present a scalable algorithm for computing the Monge map between two probability distributions. Our algorithm is based on a weak form of the optimal transport problem, thus it only requires samples from the marginals instead of their analytic expressions, and can accommodate optimal transport between two distributions with different dimensions. Our algorithm is suitable for general cost functions, compared with other existing methods for estimating Monge maps using samples, which are usually for quadratic costs. The performance of our algorithms is demonstrated through a series of experiments with both synthetic and realistic data."}}
{"id": "zQkvkKKNHVY", "cdate": 1640995200000, "mdate": 1683647029614, "content": {"title": "Variational Wasserstein gradient flow", "abstract": "Wasserstein gradient flow has emerged as a promising approach to solve optimization problems over the space of probability distributions. A recent trend is to use the well-known JKO scheme in combi..."}}
{"id": "6lLFgqWAzI", "cdate": 1640995200000, "mdate": 1683647029643, "content": {"title": "On the complexity of the optimal transport problem with graph-structured cost", "abstract": "Multi-marginal optimal transport (MOT) is a generalization of optimal transport to multiple marginals. Optimal transport has evolved into an important tool in many machine learning applications, and its multi-marginal extension opens up for addressing new challenges in the field of machine learning. However, the usage of MOT has been largely impeded by its computational complexity which scales exponentially in the number of marginals. Fortunately, in many applications, such as barycenter or interpolation problems, the cost function adheres to structures, which has recently been exploited for developing efficient computational methods. In this work we derive computational bounds for these methods. In particular, with $m$ marginal distributions supported on $n$ points, we provide a $ \\mathcal{\\tilde O}(d(\\mathcal{T})m n^{w(G)+1}\\epsilon^{-2})$ bound for a $\\epsilon$-accuracy when the problem is associated with a graph that can be factored as a junction tree with diameter $d(\\mathcal{T})$ and tree-width $w(G)$. For the special case of the Wasserstein barycenter problem, which corresponds to a star-shaped tree, our bound is in alignment with the existing complexity bound for it."}}
{"id": "WZR7ckBkzPY", "cdate": 1632875442271, "mdate": null, "content": {"title": "Variational Wasserstein gradient flow", "abstract": "The gradient flow of a function over the space of probability densities with respect to the Wasserstein metric often exhibits nice properties and has been utilized in several machine learning applications. The standard approach to compute the Wasserstein gradient flow is the finite difference which discretizes the underlying space over a grid, and is not scalable. In this work, we propose a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key of our method is a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. This primal-dual problem can be efficiently solved by alternatively updating the parameters in the inner and outer loops. Our framework covers all the classical Wasserstein gradient flows including the heat equation and the porous medium equation. We demonstrate the performance and scalability of our algorithm with several numerical examples."}}
{"id": "j6r7BQf3e9f", "cdate": 1609459200000, "mdate": 1683647029609, "content": {"title": "Scalable Computations of Wasserstein Barycenter via Input Convex Neural Networks", "abstract": ""}}
{"id": "hiWPohSU9ex", "cdate": 1609459200000, "mdate": 1683647029631, "content": {"title": "Stein particle filtering", "abstract": "We present a new particle filtering algorithm for nonlinear systems in the discrete-time setting. Our algorithm is based on the Stein variational gradient descent (SVGD) framework, which is a general approach to sample from a target distribution. We merge the standard two-step paradigm in particle filtering into one step so that SVGD can be used. A distinguishing feature of the proposed algorithm is that, unlike most particle filtering methods, all the particles at any time step are equally weighted and thus no update on the weights is needed. We further extended our algorithm to allow for updating previous particles within a sliding window. This strategy may improve the reliability of the algorithm with respect to unexpected disturbance in the dynamics or outlier-measurements. The efficacy of the proposed algorithms is illustrated through several numerical examples in comparison with a standard particle filtering method."}}
