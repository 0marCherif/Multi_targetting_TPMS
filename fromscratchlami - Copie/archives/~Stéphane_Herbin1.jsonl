{"id": "VLizLVyFdae", "cdate": 1640995200000, "mdate": 1667397335976, "content": {"title": "Leveraging generative models to characterize the failure conditions of image classifiers", "abstract": ""}}
{"id": "Qq9Q6L0k6Jk", "cdate": 1640995200000, "mdate": 1667397335924, "content": {"title": "Benchmarking and deeper analysis of adversarial patch attack on object detectors", "abstract": ""}}
{"id": "wPb8N83rPTt", "cdate": 1609459200000, "mdate": 1667397335957, "content": {"title": "Pseudo-Labeling for Class Incremental Learning", "abstract": ""}}
{"id": "dmpzDBus3Wi", "cdate": 1577836800000, "mdate": 1667397335907, "content": {"title": "Semi-Supervised Class Incremental Learning", "abstract": "This paper makes a contribution to the problem of incremental class learning, the principle of which is to sequentially introduce batches of samples annotated with new classes during the learning phase. The main objective is to reduce the drop in classification performance on old classes, a phenomenon commonly called catastrophic forgetting. We propose in this paper a new method which exploits the availability of a large quantity of non-annotated images in addition to the annotated batches. These images are used to regularize the classifier and give the feature space a more stable structure. We demonstrate on two image data sets, MNIST and STL-10, that our approach is able to improve the global performance of classifiers learned using an incremental learning protocol, even with annotated batches of small size."}}
{"id": "oU0rpEHVYWf", "cdate": 1514764800000, "mdate": 1667397335894, "content": {"title": "Semantic Bottleneck for Computer Vision Tasks", "abstract": "This paper introduces a novel method for the representation of images that is semantic by nature, addressing the question of computation intelligibility in computer vision tasks. More specifically, our proposition is to introduce what we call a semantic bottleneck in the processing pipeline, which is a crossing point in which the representation of the image is entirely expressed with natural language, while retaining the efficiency of numerical representations. We show that our approach is able to generate semantic representations that give state-of-the-art results on semantic content-based image retrieval and also perform very well on image classification tasks. Intelligibility is evaluated through user centered experiments for failure detection."}}
{"id": "SOofysIPE-", "cdate": 1483228800000, "mdate": 1667397335901, "content": {"title": "Generating Visual Representations for Zero-Shot Classification", "abstract": "This paper addresses the task of learning an image classifier when some categories are defined by semantic descriptions only (e.g. visual attributes) while the others are defined by exemplar images as well. This task is often referred to as the Zero-Shot classification task (ZSC). Most of the previous methods rely on learning a common embedding space allowing to compare visual features of unknown categories with semantic descriptions. This paper argues that these approaches are limited as i) efficient discriminative classifiers can't be used ii) classification tasks with seen and unseen categories (Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. In contrast, this paper suggests to address ZSC and GZSC by i) learning a conditional generator using seen classes ii) generate artificial training examples for the categories without exemplars. ZSC is then turned into a standard supervised learning problem. Experiments with 4 generative models and 5 datasets experimentally validate the approach, giving state-of-the-art results on both ZSC and GZSC."}}
{"id": "AtqhNkp6K1", "cdate": 1483228800000, "mdate": 1667397335887, "content": {"title": "Generating Visual Representations for Zero-Shot Classification", "abstract": "This paper addresses the task of learning an image clas-sifier when some categories are defined by semantic descriptions only (e.g. visual attributes) while the others are defined by exemplar images as well. This task is often referred to as the Zero-Shot classification task (ZSC). Most of the previous methods rely on learning a common embedding space allowing to compare visual features of unknown categories with semantic descriptions. This paper argues that these approaches are limited as i) efficient discrimi-native classifiers can't be used ii) classification tasks with seen and unseen categories (Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. In contrast , this paper suggests to address ZSC and GZSC by i) learning a conditional generator using seen classes ii) generate artificial training examples for the categories without exemplars. ZSC is then turned into a standard supervised learning problem. Experiments with 4 generative models and 5 datasets experimentally validate the approach, giving state-of-the-art results on both ZSC and GZSC."}}
{"id": "ryEcJc-dWH", "cdate": 1451606400000, "mdate": null, "content": {"title": "Hard Negative Mining for Metric Learning Based Zero-Shot Classification", "abstract": "Zero-Shot learning has been shown to be an efficient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve Zero-Shot classification problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a significant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets."}}
{"id": "HkNJ-9ZubB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Improving Semantic Embedding Consistency by Metric Learning for Zero-Shot Classiffication", "abstract": "This paper addresses the task of zero-shot image classification. The key contribution of the proposed approach is to control the semantic embedding of images \u2013 one of the main ingredients of zero-shot learning \u2013 by formulating it as a metric learning problem. The optimized empirical criterion associates two types of sub-task constraints: metric discriminating capacity and accurate attribute prediction. This results in a novel expression of zero-shot learning not requiring the notion of class in the training phase: only pairs of image/attributes, augmented with a consistency indicator, are given as ground truth. At test time, the learned model can predict the consistency of a test image with a given set of attributes, allowing flexible ways to produce recognition inferences. Despite its simplicity, the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation."}}
{"id": "6Hsn-5tcGY", "cdate": 1451606400000, "mdate": 1667397335903, "content": {"title": "Processing of Extremely High-Resolution LiDAR and RGB Data: Outcome of the 2015 IEEE GRSS Data Fusion Contest-Part A: 2-D Contest", "abstract": "In this paper, we discuss the scientific outcomes of the 2015 data fusion contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource studies. The 2015 edition of the contest proposed a multiresolution and multisensorial challenge involving extremely high-resolution RGB images and a three-dimensional (3-D) LiDAR point cloud. The competition was framed in two parallel tracks, considering 2-D and 3-D products, respectively. In this paper, we discuss the scientific results obtained by the winners of the 2-D contest, which studied either the complementarity of RGB and LiDAR with deep neural networks (winning team) or provided a comprehensive benchmarking evaluation of new classification strategies for extremely high-resolution multimodal data (runner-up team). The data and the previously undisclosed ground truth will remain available for the community and can be obtained at http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/. The 3-D part of the contest is discussed in the Part-B paper [1]."}}
