{"id": "Lk5aix-SrSw", "cdate": 1680333505893, "mdate": 1680333505893, "content": {"title": "Guiding Online Reinforcement Learning with Action-Free Offline Pretraining", "abstract": "Offline RL methods have been shown to reduce the need for environment interaction by training agents using offline collected episodes. However, these methods typically require action information to be logged during data collection, which can be difficult or even impossible in some practical cases. In this paper, we investigate the potential of using action-free offline datasets to improve online reinforcement learning, name this problem Reinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We introduce Action-Free Guide (AF-Guide), a method that guides online training by extracting knowledge from action-free offline datasets. AF-Guide consists of an Action-Free Decision Transformer (AFDT) implementing a variant of Upside-Down Reinforcement Learning. It learns to plan the next states from the offline dataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with guidance from AFDT. Experimental results show that AF-Guide can improve sample efficiency and performance in online training thanks to the knowledge from the action-free offline dataset."}}
{"id": "ngDIbawd6Q", "cdate": 1672531200000, "mdate": 1682319480965, "content": {"title": "Guiding Online Reinforcement Learning with Action-Free Offline Pretraining", "abstract": "Offline RL methods have been shown to reduce the need for environment interaction by training agents using offline collected episodes. However, these methods typically require action information to be logged during data collection, which can be difficult or even impossible in some practical cases. In this paper, we investigate the potential of using action-free offline datasets to improve online reinforcement learning, name this problem Reinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We introduce Action-Free Guide (AF-Guide), a method that guides online training by extracting knowledge from action-free offline datasets. AF-Guide consists of an Action-Free Decision Transformer (AFDT) implementing a variant of Upside-Down Reinforcement Learning. It learns to plan the next states from the offline dataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with guidance from AFDT. Experimental results show that AF-Guide can improve sample efficiency and performance in online training thanks to the knowledge from the action-free offline dataset. Code is available at https://github.com/Vision-CAIR/AF-Guide."}}
{"id": "kzwRAn3LQwX", "cdate": 1672531200000, "mdate": 1682319481079, "content": {"title": "SMIX(\u03bb): Enhancing Centralized Value Functions for Cooperative Multiagent Reinforcement Learning", "abstract": "Learning a stable and generalizable centralized value function (CVF) is a crucial but challenging task in multiagent reinforcement learning (MARL), as it has to deal with the issue that the joint action space increases exponentially with the number of agents in such scenarios. This article proposes an approach, named SMIX( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\lambda }$ </tex-math></inline-formula> ), that uses an OFF-policy training to achieve this by avoiding the greedy assumption commonly made in CVF learning. As importance sampling for such OFF-policy training is both computationally costly and numerically unstable, we proposed to use the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\lambda }$ </tex-math></inline-formula> -return as a proxy to compute the temporal difference (TD) error. With this new loss function objective, we adopt a modified QMIX network structure as the base to train our model. By further connecting it with the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${Q(\\lambda)}$ </tex-math></inline-formula> approach from a unified expectation correction viewpoint, we show that the proposed SMIX( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\lambda }$ </tex-math></inline-formula> ) is equivalent to <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${Q(\\lambda)}$ </tex-math></inline-formula> and hence shares its convergence properties, while without being suffered from the aforementioned curse of dimensionality problem inherent in MARL. Experiments on the StarCraft Multiagent Challenge (SMAC) benchmark demonstrate that our approach not only outperforms several state-of-the-art MARL methods by a large margin but also can be used as a general tool to improve the overall performance of other centralized training with decentralized execution (CTDE)-type algorithms by enhancing their CVFs."}}
{"id": "NFcRC4aYSWf", "cdate": 1663849935023, "mdate": null, "content": {"title": "Highway Reinforcement Learning", "abstract": "Traditional Dynamic Programming (DP) approaches suffer from slow backward credit-assignment (CA): only a one-step search is performed at each update. A popular solution for multi-step CA is to use multi-step Bellman operators. Unfortunately, in the control settings, existing methods typically suffer from the large variance of multi-step off-policy corrections or are biased, preventing convergence. To overcome these problems, we introduce a novel multi-step Bellman optimality equation with adaptive lookahead steps. We first derive a new multi-step Value Iteration (VI) method that converges to the optimal Value Function (VF) with an exponential contraction rate but linear computational complexity. Given some trial, our so-called Highway RL performs rapid CA, by picking a policy and a possible lookahead (up to the trial end) that maximize the near-term reward during lookahead plus a DP-based estimate of the cumulative reward for the remaining part of the trial. Highway RL does not require off-policy corrections. Under mild assumptions, it achieves better convergence rates than the traditional one-step Bellman Optimality Operator. We then derive Highway Q-Learning, a convergent multi-step off-policy variant of Q-learning. We show that our Highway algorithms significantly outperform DP approaches on toy tasks. Finally, we propose a deep function approximation variant called Highway DQN. We evaluate it on visual MinAtar Games, outperforming similar multi-step methods."}}
{"id": "JeqkmCTlQmT", "cdate": 1609459200000, "mdate": 1682319481175, "content": {"title": "Deep Recurrent Belief Propagation Network for POMDPs", "abstract": "In many real-world sequential decision-making tasks, especially in continuous control like robotic control, it is rare that the observations are perfect, that is, the sensory data could be incomplete, noisy or even dynamically polluted due to the unexpected malfunctions or intrinsic low quality of the sensors. Previous methods handle these issues in the framework of POMDPs and are either deterministic by feature memorization or stochastic by belief inference. In this paper, we present a new method that lies somewhere in the middle of the spectrum of research methodology identified above and combines the strength of both approaches. In particular, the proposed method, named Deep Recurrent Belief Propagation Network (DRBPN), takes a hybrid style belief updating procedure \u2212 an RNN-type feature extraction step followed by an analytical belief inference, significantly reducing the computational cost while faithfully capturing the complex dynamics and maintaining the necessary uncertainty for generalization. The effectiveness of the proposed method is verified on a collection of benchmark tasks, showing that our approach outperforms several state-of-the-art methods under various challenging scenarios."}}
{"id": "rAIkhjUK0Tx", "cdate": 1601308029008, "mdate": null, "content": {"title": "Greedy Multi-Step Off-Policy Reinforcement Learning", "abstract": "This paper presents a novel multi-step reinforcement learning algorithms, named Greedy Multi-Step Value Iteration (GM-VI), under off-policy setting. GM-VI iteratively approximates the optimal value function of a given environment using a newly proposed multi-step bootstrapping technique, in which the step size is adaptively adjusted along each trajectory according to a greedy principle. With the improved multi-step information propagation mechanism, we show that the resulted VI process is capable of safely learning from arbitrary behavior policy without additional off-policy correction. We further analyze the theoretical properties of the corresponding operator, showing that it is able to converge to globally optimal value function, with a rate faster than traditional Bellman Optimality Operator. Experiments reveal that the proposed methods is reliable, easy to implement and achieves state-of-the-art performance on a series of standard benchmark datasets."}}
{"id": "FxNSsIToTuX", "cdate": 1598779827974, "mdate": null, "content": {"title": " Trust Region-Guided Proximal Policy Optimization", "abstract": "Proximal policy optimization (PPO) is one of the most popular deep reinforcement\nlearning (RL) methods, achieving state-of-the-art performance across a wide range\nof challenging tasks. However, as a model-free RL method, the success of PPO\nrelies heavily on the effectiveness of its exploratory policy search. In this paper, we\ngive an in-depth analysis on the exploration behavior of PPO, and show that PPO\nis prone to suffer from the risk of lack of exploration especially under the case of\nbad initialization, which may lead to the failure of training or being trapped in bad\nlocal optima. To address these issues, we proposed a novel policy optimization\nmethod, named Trust Region-Guided PPO (TRGPPO), which adaptively adjusts\nthe clipping range within the trust region. We formally show that this method not\nonly improves the exploration ability within the trust region but enjoys a better\nperformance bound compared to the original PPO as well. Extensive experiments\nverify the advantage of the proposed method.\n"}}
{"id": "IWZdJ9PYZY9", "cdate": 1598779602533, "mdate": null, "content": {"title": "SMIX (\u03bb): Enhancing Centralized Value Functions for Cooperative Multi-Agent Reinforcement Learning", "abstract": "This work presents a sample efficient and effective valuebased method, named SMIX(\u03bb), for reinforcement learning\nin multi-agent environments (MARL) within the paradigm of\ncentralized training with decentralized execution (CTDE), in\nwhich learning a stable and generalizable centralized value\nfunction (CVF) is crucial. To achieve this, our method carefully combines different elements, including 1) removing the\nunrealistic centralized greedy assumption during the learning\nphase, 2) using the \u03bb-return to balance the trade-off between\nbias and variance and to deal with the environment\u2019s nonMarkovian property, and 3) adopting an experience-replay\nstyle off-policy training. Interestingly, it is revealed that\nthere exists inherent connection between SMIX(\u03bb) and previous off-policy Q(\u03bb) approach for single-agent learning. Experiments on the StarCraft Multi-Agent Challenge (SMAC)\nbenchmark show that the proposed SMIX(\u03bb) algorithm outperforms several state-of-the-art MARL methods by a large\nmargin, and that it can be used as a general tool to improve\nthe overall performance of a CTDE-type method by enhancing the evaluation quality of its CVF. We open-source our\ncode at: https://github.com/chaovven/SMIX.\n"}}
{"id": "bxHu_x7xzqE", "cdate": 1577836800000, "mdate": 1682319480987, "content": {"title": "SMIX(\u03bb): Enhancing Centralized Value Functions for Cooperative Multi-Agent Reinforcement Learning", "abstract": "This work presents a sample efficient and effective value-based method, named SMIX(\u03bb), for reinforcement learning in multi-agent environments (MARL) within the paradigm of centralized training with decentralized execution (CTDE), in which learning a stable and generalizable centralized value function (CVF) is crucial. To achieve this, our method carefully combines different elements, including 1) removing the unrealistic centralized greedy assumption during the learning phase, 2) using the \u03bb-return to balance the trade-off between bias and variance and to deal with the environment's non-Markovian property, and 3) adopting an experience-replay style off-policy training. Interestingly, it is revealed that there exists inherent connection between SMIX(\u03bb) and previous off-policy Q(\u03bb) approach for single-agent learning. Experiments on the StarCraft Multi-Agent Challenge (SMAC) benchmark show that the proposed SMIX(\u03bb) algorithm outperforms several state-of-the-art MARL methods by a large margin, and that it can be used as a general tool to improve the overall performance of a CTDE-type method by enhancing the evaluation quality of its CVF. We open-source our code at: https://github.com/chaovven/SMIX."}}
{"id": "r1eVEVrgUr", "cdate": 1567802411683, "mdate": null, "content": {"title": "Trust Region-Guided Proximal Policy Optimization", "abstract": "Proximal policy optimization (PPO) is one of the most popular deep reinforcement learning (RL) methods, achieving state-of-the-art performance across a wide range of challenging tasks. However, as a model-free RL method, the success of PPO relies heavily on the effectiveness of its exploratory policy search. In this paper, we give an in-depth analysis on the exploration behavior of PPO, and show that PPO is prone to suffer from the risk of lack of exploration especially under the case of bad initialization, which may lead to the failure of training or being trapped in bad local optima. To address these issues, we proposed a novel policy optimization method, named Trust Region-Guided PPO (TRGPPO), which adaptively adjusts the clipping range within the trust region. We formally show that this method not only improves the exploration ability within the trust region but enjoys a better performance bound compared to the original PPO as well. Extensive experiments verify the advantage of the proposed method."}}
