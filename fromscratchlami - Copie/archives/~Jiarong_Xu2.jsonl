{"id": "fIk5U77XUH6", "cdate": 1683899089812, "mdate": 1683899089812, "content": {"title": "Robust Network Enhancement from Flawed Networks", "abstract": "Network data in real-world tends to be error-prone due to\nincomplete sampling, imperfect measurements, etc.; this in turn results\nin inaccurate results when performing network analysis or modeling,\nsuch as node classification and link prediction, on these flawed networks. In this paper, we aim to reconstruct a reliable network from a\nflawed, undirected, unweighted network, a process referred to network\nenhancement. More specifically, network enhancement aims to detect\nthe noisy links that are observed in the network but should not exist in\nthe real world, as well as to predict the missing links that do indeed\nexist in the real world yet remain unobserved. While some attempts\nhave been made to detect either noisy links or missing links, few of\nthese works have considered unifying these two tasks, even though\nthey are inter-dependent and capable of mutually boosting each others\u2019\nperformance. In this paper, we therefore propose E-Net, an end-toend graph neural network model, to leverage the mutual influence of\nthese two tasks in order to achieve both goals more effectively. On one\nhand, detecting noisy links can benefit the performance of missing link\nprediction, while on the other hand, predicting missing links can provide\nindirect supervision for detecting noisy link detection when the labels of\nthese noisy links are unavailable. Moreover, by proposing a subgraph\nextraction mechanism based on random walk with restart, the model\ncan be scaled up to large networks and is able to preserve the local\nand global structural characteristics. The experimental results on several\ntypes of large networks demonstrate that the proposed model obtains an\nimprovement of 10.7% on average in terms of F1 for predicting missing\nlinks, along with an average of 3.7% improvement in terms of precision\nfor detecting noisy links compared with the state-of-the-art baselines."}}
{"id": "KyR0jw-6YhC", "cdate": 1683898224896, "mdate": null, "content": {"title": " Beyond Homophily: Structure-aware Path Aggregation Graph Neural Network", "abstract": "Graph neural networks (GNNs) have been intensively studied in various real-world tasks. However, the homophily assumption of GNNs\u2019 aggregation function limits their representation learning\nability in heterophily graphs. In this paper, we shed\nlight on the path level patterns in graphs that can explicitly reflect rich semantic and structural information. We therefore propose a novel Structure-aware\nPath Aggregation Graph Neural Network (PathNet)\naiming to generalize GNNs for both homophily\nand heterophily graphs. Specifically, we first introduce a maximal entropy path sampler, which\nhelps us sample a number of paths containing structural context. Then, we introduce a structure-aware\nrecurrent cell consisting of order-preserving and\ndistance-aware components to learn the semantic\ninformation of neighborhoods. Finally, we model\nthe preference of different paths to target node after\npath encoding. Experimental results demonstrate\nthat our model obtains significant improvements\nin node classification on both heterophily and homophily graphs."}}
{"id": "b5akM_1H-f", "cdate": 1672531200000, "mdate": 1683879598001, "content": {"title": "Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning", "abstract": "Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four datasets demonstrate that the proposed model achieves significant improvements in complex reasoning tasks involving diverse structures, and shows transferability to downstream tasks with limited training data and effectiveness for complex reasoning of KGs modality."}}
{"id": "663Cl-KetJ", "cdate": 1663850271687, "mdate": null, "content": {"title": "Better with Less: Data-Active Pre-training of Graph Neural Networks", "abstract": "Recently, pre-training on graph neural networks (GNNs) has become an active research area and is used to learn transferable knowledge for downstream tasks with unlabeled data. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training samples and graph datasets do not necessarily lead to better performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: few, but carefully chosen data are fed into a GNN model to enhance pre-training. This novel pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as the predictive uncertainty. The proposed uncertainty, as feedback from the pre-training model, measures the confidence level of the model to the data. When fed with the chosen data, on the other hand, the pre-training model grasps an initial understanding of the new, unseen data, and at the same time attempts to remember the knowledge learnt from the previous data. Therefore, the integration and interaction between these two components form a unified framework, in which graph pre-training is performed in a progressive way. Experiment results show that the proposed APT framework is able to obtain an efficient pre-training model with fewer training data and better downstream performance."}}
{"id": "2rQPxsmjKF", "cdate": 1654519376012, "mdate": null, "content": {"title": "DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection", "abstract": "Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that 2M background nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes. "}}
{"id": "mKNV4NTx57", "cdate": 1640995200000, "mdate": 1682414005878, "content": {"title": "A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training", "abstract": "Multi-modal pre-training and knowledge discovery are two important research topics in multi-modal machine learning. Nevertheless, none of existing works make attempts to link knowledge discovery with knowledge guided multi-modal pre-training. In this paper, we propose to unify them into a continuous learning framework for mutual improvement. Taking the open-domain uni-modal datasets of images and texts as input, we maintain a knowledge graph as the foundation to support these two tasks. For knowledge discovery, a pre-trained model is used to identify cross-modal links on the graph. For model pre-training, the knowledge graph is used as the external knowledge to guide the model updating. These two steps are iteratively performed in our framework for continuous learning. The experimental results on MS-COCO and Flickr30K with respect to both knowledge discovery and the pre-trained model validate the effectiveness of our framework."}}
{"id": "lCFt6U_yztq", "cdate": 1640995200000, "mdate": 1683879597901, "content": {"title": "DropMessage: Unifying Random Dropping for Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph representation learning. Despite their rapid development, GNNs also face some challenges, such as over-fitting, over-smoothing, and non-robustness. Previous works indicate that these problems can be alleviated by random dropping methods, which integrate augmented data into models by randomly masking parts of the input. However, some open problems of random dropping on GNNs remain to be solved. First, it is challenging to find a universal method that are suitable for all cases considering the divergence of different datasets and models. Second, augmented data introduced to GNNs causes the incomplete coverage of parameters and unstable training process. Third, there is no theoretical analysis on the effectiveness of random dropping methods on GNNs. In this paper, we propose a novel random dropping method called DropMessage, which performs dropping operations directly on the propagated messages during the message-passing process. More importantly, we find that DropMessage provides a unified framework for most existing random dropping methods, based on which we give theoretical analysis of their effectiveness. Furthermore, we elaborate the superiority of DropMessage: it stabilizes the training process by reducing sample variance; it keeps information diversity from the perspective of information theory, enabling it become a theoretical upper bound of other methods. To evaluate our proposed method, we conduct experiments that aims for multiple tasks on five public datasets and two industrial datasets with various backbone models. The experimental results show that DropMessage has the advantages of both effectiveness and generalization, and can significantly alleviate the problems mentioned above."}}
{"id": "hPM3iVgBQ1e", "cdate": 1640995200000, "mdate": 1683879598098, "content": {"title": "Unsupervised Adversarially Robust Representation Learning on Graphs", "abstract": "Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require pre-specified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, graph representation vulnerability (GRV), to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (i.e., GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods."}}
{"id": "Z007DyGB6l", "cdate": 1640995200000, "mdate": 1683879598108, "content": {"title": "DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection", "abstract": "Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental work. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes."}}
{"id": "Wgvbi-b9Jfb", "cdate": 1640995200000, "mdate": 1678364593250, "content": {"title": "Can Abnormality be Detected by Graph Neural Networks?", "abstract": ""}}
