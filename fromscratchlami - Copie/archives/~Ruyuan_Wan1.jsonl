{"id": "wXSIrPXdete", "cdate": 1672531200000, "mdate": 1695954829613, "content": {"title": "Dragonfly_captain at SemEval-2023 Task 11: Unpacking Disagreement with Investigation of Annotator Demographics and Task Difficulty", "abstract": ""}}
{"id": "skSf77oAhI5", "cdate": 1672531200000, "mdate": 1695954829615, "content": {"title": "Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions", "abstract": "Annotating data via crowdsourcing is time-consuming and expensive. Due to these costs, dataset creators often have each annotator label only a small subset of the data. This leads to sparse datasets with examples that are marked by few annotators. The downside of this process is that if an annotator doesn't get to label a particular example, their perspective on it is missed. This is especially concerning for subjective NLP datasets where there is no single correct label: people may have different valid opinions. Thus, we propose using imputation methods to generate the opinions of all annotators for all examples, creating a dataset that does not leave out any annotator's view. We then train and prompt models, using data from the imputed dataset, to make predictions about the distribution of responses and individual annotations. In our analysis of the results, we found that the choice of imputation method significantly impacts soft label changes and distribution. While the imputation introduces noise in the prediction of the original dataset, it has shown potential in enhancing shots for prompts, particularly for low-response-rate annotators. We have made all of our code and data publicly available."}}
{"id": "_D2ikBx4mAX", "cdate": 1672531200000, "mdate": 1695954829619, "content": {"title": "Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information", "abstract": "In NLP annotation, it is common to have multiple annotators label the text and then obtain the ground truth labels based on major annotators\u2019 agreement. However, annotators are individuals with different backgrounds and various voices. When annotation tasks become subjective, such as detecting politeness, offense, and social norms, annotators\u2019 voices differ and vary. Their diverse voices may represent the true distribution of people\u2019s opinions on subjective matters. Therefore, it is crucial to study the disagreement from annotation to understand which content is controversial from the annotators. In our research, we extract disagreement labels from five subjective datasets, then fine-tune language models to predict annotators\u2019 disagreement. Our results show that knowing annotators\u2019 demographic information (e.g., gender, ethnicity, education level), in addition to the task text, helps predict the disagreement. To investigate the effect of annotators\u2019 demographics on their disagreement level, we simulate different combinations of their artificial demographics and explore the variance of the prediction to distinguish the disagreement from the inherent controversy from text content and the disagreement in the annotators\u2019 perspective. Overall, we propose an innovative disagreement prediction mechanism for better design of the annotation process that will achieve more accurate and inclusive results for NLP systems. Our code and dataset are publicly available."}}
{"id": "VxFQ10IEYb", "cdate": 1672531200000, "mdate": 1681707736686, "content": {"title": "Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information", "abstract": "In NLP annotation, it is common to have multiple annotators label the text and then obtain the ground truth labels based on the agreement of major annotators. However, annotators are individuals with different backgrounds, and minors' opinions should not be simply ignored. As annotation tasks become subjective and topics are controversial in modern NLP tasks, we need NLP systems that can represent people's diverse voices on subjective matters and predict the level of diversity. This paper examines whether the text of the task and annotators' demographic background information can be used to estimate the level of disagreement among annotators. Particularly, we extract disagreement labels from the annotators' voting histories in the five subjective datasets, and then fine-tune language models to predict annotators' disagreement. Our results show that knowing annotators' demographic information, like gender, ethnicity, and education level, helps predict disagreements. In order to distinguish the disagreement from the inherent controversy from text content and the disagreement in the annotators' different perspectives, we simulate everyone's voices with different combinations of annotators' artificial demographics and examine its variance of the finetuned disagreement predictor. Our paper aims to improve the annotation process for more efficient and inclusive NLP systems through a novel disagreement prediction mechanism. Our code and dataset are publicly available."}}
{"id": "bWpr2sRkXPS", "cdate": 1640995200000, "mdate": 1673436507568, "content": {"title": "A conversational agent system for dietary supplements use", "abstract": ""}}
{"id": "04W4lx7b_S", "cdate": 1640995200000, "mdate": 1673436507566, "content": {"title": "User or Labor: An Interaction Framework for Human-Machine Relationships in NLP", "abstract": ""}}
{"id": "ru_Uzus2x9", "cdate": 1609459200000, "mdate": 1673436507568, "content": {"title": "A Conversational Agent System for Dietary Supplements Use", "abstract": ""}}
{"id": "n-HmXTLNUB", "cdate": 1609459200000, "mdate": 1673436507566, "content": {"title": "Social determinants of health in the era of artificial intelligence with electronic health records: A systematic review", "abstract": ""}}
{"id": "XFRxpS1mOxX", "cdate": 1577836800000, "mdate": 1673436507570, "content": {"title": "How much is a \"like\" worth? Engagement and Retention in an Online Health Community", "abstract": ""}}
