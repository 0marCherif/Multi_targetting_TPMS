{"id": "vjTqkVDPGq", "cdate": 1640995200000, "mdate": 1668522800548, "content": {"title": "RV-GAN: Recurrent GAN for Unconditional Video Generation", "abstract": "Generative models aiming to generate content from noise have achieved high-fidelity synthesis for image data. However, obtaining comparable performance in the field of unconditional video generation still remains challenging. In this work, we propose a recurrent GAN architecture to model the high-dimensional video data distribution. Recurrent networks by design are able to generate complex, long sequences in an autoregressive fashion. However, the standard LSTM unit for videos (ConvLSTM) is not ideally suited for the task of unconditional video generation. Therefore, we propose a simple yet effective LSTM variant called as TransConv LSTM (TC-LSTM) by modulating the conventional ConvLSTM to have a transpose convolutional structure in input-to-state transitions. This enables the network to model both spatial and temporal relationships across layers simultaneously inside the TC-LSTM unit. TC-LSTM unit acts as a building block of our generator. Extensive quantitative and qualitative analysis shows that RV-GAN outperforms state-of-the-art methods by a significant margin on Moving MNIST, MUG, Weizmann and UCF101 datasets. Additionally, owing to the recurrent structure, our method is able to generate high-quality videos, up to 2 times longer (32 frames) than training videos at inference time. Further analysis confirms that the proposed architecture is generic and can be easily adapted to other tasks like class-conditional video synthesis and text-to-video synthesis."}}
{"id": "tdhU4o8FoOu", "cdate": 1640995200000, "mdate": 1668522800729, "content": {"title": "V3GAN: Decomposing Background, Foreground and Motion for Video Generation", "abstract": "Video generation is a challenging task that requires modeling plausible spatial and temporal dynamics in a video. Inspired by how humans perceive a video by grouping a scene into moving and stationary components, we propose a method that decomposes the task of video generation into the synthesis of foreground, background and motion. Foreground and background together describe the appearance, whereas motion specifies how the foreground moves in a video over time. We propose V3GAN, a novel three-branch generative adversarial network where two branches model foreground and background information, while the third branch models the temporal information without any supervision. The foreground branch is augmented with our novel feature-level masking layer that aids in learning an accurate mask for foreground and background separation. To encourage motion consistency, we further propose a shuffling loss for the video discriminator. Extensive quantitative and qualitative analysis on synthetic as well as real-world benchmark datasets demonstrates that V3GAN outperforms the state-of-the-art methods by a significant margin."}}
{"id": "ek0y8xQPeT3", "cdate": 1640995200000, "mdate": 1652800326762, "content": {"title": "Catch Me if You Can: A Novel Task for Detection of Covert Geo-Locations (CGL)", "abstract": "Most visual scene understanding tasks in the field of computer vision involve identification of the objects present in the scene. Image regions like hideouts, turns, & other obscured regions of the scene also contain crucial information, for specific surveillance tasks. Task proposed in this paper involves the design of an intelligent visual aid for identification of such locations in an image, which has either the potential to create an imminent threat from an adversary or appear as the target zones needing further investigation. Covert places (CGL) for hiding behind an occluding object are concealed 3D locations, not detectable from the viewpoint (camera). Hence this involves delineating specific image regions around the projections of outer boundary of the occluding objects, as places to be accessed around the potential hideouts. CGL detection finds applications in military counter-insurgency operations, surveillance with path planning for an exploratory robot. Given an RGB image, the goal is to identify all CGLs in the 2D scene. Identification of such regions would require knowledge about the 3D boundaries of obscuring items (pillars, furniture), their spatial location with respect to the neighboring regions of the scene. We propose this as a novel task, termed Covert Geo-Location (CGL) Detection. Classification of any region of an image as a CGL (as boundary sub-segments of an occluding object that conceals the hideout) requires examining the 3D relation between boundaries of occluding objects and their neighborhoods & surroundings. Our method successfully extracts relevant depth features from a single RGB image and quantitatively yields significant improvement over existing object detection and segmentation models adapted and trained for CGL detection. We also introduce a novel hand-annotated CGL detection dataset containing 1.5K real-world images for experimentation."}}
{"id": "UlJWl9DF4cd", "cdate": 1640995200000, "mdate": 1668522800577, "content": {"title": "Navigational Aid for Open-Ended Surveillance, by Fusing Estimated Depth and Scene Segmentation Maps, Using RGB Images of Indoor Scenes", "abstract": "Open-ended surveillance task for a robot in an unspecified environment using only an RGB camera, has not been addressed at length in literature. This is unlike the popular scenario of path planning where both the target and environments are often known. We focus on the task of a robot which needs to estimate a realistic depiction of the surrounding 3D environment, including the location of obstacles and free space to navigate in the scene within the view field. In this paper, we propose an unsupervised algorithm to iteratively compute an optimal direction for maximal unhindered movement in the scene. This task is challenging when presented with only a single RGB view of the scene, without the use of any online depth sensor. Our process combines cues from two deep-learning processes - semantic segmentation and depth map estimation, to automatically decide plausible robot movement paths while avoiding hindrance posed by objects in the scene. We make assumptions of the use of a low-end RGB USB camera, pre-set camera view direction (angle) and field of view, incremental movement of the robot in the view field, and iterative analysis of the scene, all catering to any open-ended (target-free) surveillance/patrolling applications. Inverse perspective geometry has been used to map the optimal direction estimated in the view field, to that on the floor of the scene for navigation. Results of evaluation using a dataset of videos of scenes captured from indoor (office, labs, meeting/class-rooms, corridors, lounge) environments, reveal the success of the proposed approach."}}
{"id": "VkbOCrZkLg", "cdate": 1609459200000, "mdate": 1668522800932, "content": {"title": "V3GAN: Decomposing Background, Foreground and Motion for Video Generation", "abstract": ""}}
{"id": "6ncfDGYaHhR", "cdate": 1609459200000, "mdate": 1652800327402, "content": {"title": "Where to Look?: Mining Complementary Image Regions for Weakly Supervised Object Localization", "abstract": "Humans possess an innate capability of recognizing objects and their corresponding parts and confine their attention to that location in a visual scene where the object is spatially present. Recently, efforts to train machines to mimic this ability of humans in the form of weakly supervised object localization, using training labels only at the image-level, have garnered a lot of attention. Nonetheless, one of the well-known problems that most of the existing methods suffer from is localizing only the most discriminative part of an object. Such methods provide very little or no focus on other pertinent parts of the object. In this paper, we propose a novel way of scrupulously localizing objects using training with labels as for the entire image by mining information from complementary regions in an image. Primarily, we adapt to regional dropout at complementary spatial locations to create two intermediate images. With the help of a novel Channel-wise Assisted Attention Module (CAAM) coupled with a Spatial Self-Attention Module (SSAM), we parallely train our model to leverage the information from complementary image regions for excellent localization. Finally, we fuse the attention maps generated by the two classifiers using our Attention-based Fusion Loss. Several experimental studies manifest the superior performance of our proposed approach. Our method demonstrates a significant increase in localization performance over the existing state-of-the-art methods on CUB-200-2011 and ILSVRC 2016 datasets."}}
{"id": "1prcq-BVMeY", "cdate": 1609459200000, "mdate": 1652800327444, "content": {"title": "G3AN++: exploring wide GANs with complementary feature learning for video generation", "abstract": "Video generation task is a challenging problem which involves the modelling of complex real-world dynamics. Most of the existing methods have designed deep networks to tackle high-dimensional video data distributions. However, the utilization of wide networks is still under explored. Inspired by the success of wide networks in the image recognition literature, we present G3AN++, a three-stream generative adversarial network for video. The three streams are spatial, temporal and spatio-temporal processing branches. In pursuit of improving the quality of video generation, we make our network wider by splitting the spatial stream into two parallel identical branches learning complementary feature representations. We further introduce a novel adaptive masking layer to impose the complementary constraint. The masking layer encourages the parallel branches to learn distinct and richer visual features. Extensive quantitative and qualitative analysis demonstrates that our model outperforms the existing state-of-the-art methods by a significant margin on Weizmann Action, UvA-Nemo Smile and UCF101 Action datasets. Additional exploration reveals that G3AN++ is capable of disentangling the appearance and motion. We also show that the proposed method can be easily extended to solve the hard task of text-to-video generation."}}
{"id": "an555hRLCvo", "cdate": 1577836800000, "mdate": 1623650241165, "content": {"title": "SD-GAN: Structural and Denoising GAN reveals facial parts under occlusion", "abstract": "Certain facial parts are salient (unique) in appearance, which substantially contribute to the holistic recognition of a subject. Occlusion of these salient parts deteriorates the performance of face recognition algorithms. In this paper, we propose a generative model to reconstruct the missing parts of the face which are under occlusion. The proposed generative model (SD-GAN) reconstructs a face preserving the illumination variation and identity of the face. A novel adversarial training algorithm has been designed for a bimodal mutually exclusive Generative Adversarial Network (GAN) model, for faster convergence. A novel adversarial \"structural\" loss function is also proposed, comprising of two components: a holistic and a local loss, characterized by SSIM and patch-wise MSE. Ablation studies on real and synthetically occluded face datasets reveal that our proposed technique outperforms the competing methods by a considerable margin, even for boosting the performance of Face Recognition."}}
{"id": "E0a-nvu9kO", "cdate": 1577836800000, "mdate": 1652800327362, "content": {"title": "Future Frame Prediction of a Video Sequence", "abstract": "Predicting future frames of a video sequence has been a problem of high interest in the field of Computer Vision as it caters to a multitude of applications. The ability to predict, anticipate and reason about future events is the essence of intelligence and one of the main goals of decision-making systems such as human-machine interaction, robot navigation and autonomous driving. However, the challenge lies in the ambiguous nature of the problem as there may be multiple future sequences possible for the same input video shot. A naively designed model averages multiple possible futures into a single blurry prediction. Recently, two distinct approaches have attempted to address this problem as: (a) use of latent variable models that represent underlying stochasticity and (b) adversarially trained models that aim to produce sharper images. A latent variable model often struggles to produce realistic results, while an adversarially trained model underutilizes latent variables and thus fails to produce diverse predictions. These methods have revealed complementary strengths and weaknesses. Combining the two approaches produces predictions that appear more realistic and better cover the range of plausible futures. This forms the basis and objective of study in this project work. In this paper, we proposed a novel multi-scale architecture combining both approaches. We validate our proposed model through a series of experiments and empirical evaluations on Moving MNIST, UCF101, and Penn Action datasets. Our method outperforms the results obtained using the baseline methods."}}
{"id": "4JOL8l-V8w8", "cdate": 1577836800000, "mdate": 1623650241254, "content": {"title": "D2SC-GAN: Dual Deep-Shallow Channeled Generative Adversarial Network, for Resolving Low-Resolution Faces for Recognition in Classroom Scenarios", "abstract": "Face Recognition using convolutional neural networks have achieved considerable success in constrained environments in the recent past. However, the performance of these methods deteriorates in case of mismatch of training and test distributions, under classroom/surveillance scenarios. These test (probe) samples suffer from degradations such as noise, poor illumination, pose variations, occlusion, low-resolution (LR), blur as well as aliasing, when compared to the crisp, rich training (gallery) set, comprising mostly of high-resolution (HR) mugshot images captured in laboratory settings. To cope with this scenario, we propose a novel dual deep-shallow channeled generative adversarial network (D2SC-GAN) which performs supervised domain adaptation (DA) by mapping LR degraded probe samples to their corresponding HR gallery-like counterparts to perform closed-set face recognition. D2SC-GAN uses a multi-component loss function comprising of multi-resolution patchwise MSE and normalized chi-squared distance loss functions, along with a Kullback-Leibler divergence based loss function. Moreover, we propose a novel classroom face dataset called the Indian Classroom Face Dataset (ICFD), which, to the best of our knowledge, is a first of its kind and will be helpful to explore the challenges of face recognition when used for automatically recording the attendance in classroom conditions. The proposed network achieves superior results on five real-world face datasets when compared with recent state-of-the-art deep as well as shallow supervised domain adaptation (DA), super-resolution (SR), and degraded face recognition (DFR) methods, which show the effectiveness of our proposed method."}}
