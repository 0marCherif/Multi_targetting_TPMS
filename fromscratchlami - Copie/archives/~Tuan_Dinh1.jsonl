{"id": "hu02dZMKp7", "cdate": 1672531200000, "mdate": 1681703417259, "content": {"title": "Detection of Microsleep Events With a Behind-the-Ear Wearable System", "abstract": "Every year, the U.S. economy loses more than  <inline-formula><tex-math notation=\"LaTeX\">${\\$}$</tex-math></inline-formula> 411 billion because of work performance reduction, injuries, and traffic accidents caused by microsleep. To mitigate microsleep's consequences, an unobtrusive, reliable, and socially acceptable microsleep detection solution throughout the day, every day is required. Unfortunately, existing solutions do not meet these requirements. In this paper, we propose WAKE, a novel behind-the-ear wearable device for microsleep detection. By monitoring biosignals from the brain, eye movements, facial muscle contractions, and sweat gland activities from behind the user's ears, WAKE can detect microsleep with a high temporal resolution. We introduce a Three-fold Cascaded Amplifying (3CA) technique to tame the motion artifacts and environmental noises for capturing high fidelity signals. Through our prototyping, we show that WAKE can suppress motion and environmental noise in real-time by 9.74-19.47 dB while walking, driving, or staying in different environments, ensuring that the biosignals are captured reliably. We evaluated WAKE using gold-standard devices on 19 sleep-deprived and narcoleptic subjects. The Leave-One-Subject-Out Cross-Validation results show the feasibility of WAKE in microsleep detection on an unseen subject with average precision and recall of 76 and 85 percent, respectively."}}
{"id": "s_PJMEGIUfa", "cdate": 1652737465651, "mdate": null, "content": {"title": "LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks", "abstract": "Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling \"no-code machine learning with LMs.\"  We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning."}}
{"id": "kjwOIPEdZb-", "cdate": 1640995200000, "mdate": 1681682630206, "content": {"title": "Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment", "abstract": "Word translation without parallel corpora has become feasible, rivaling the performance of supervised methods. Recent findings have shown that the accuracy and robustness of unsupervised word translation (UWT) can be improved by making use of visual observations, which are universal representations across languages. In this work, we investigate the potential of using not only visual observations but also pretrained language-image models for enabling a more efficient and robust UWT. Specifically, we develop a novel UWT method dubbed Word Alignment using Language-Image Pretraining (WALIP), which leverages visual observations via the shared embedding space of images and texts provided by CLIP models (Radford et al., 2021). WALIP has a two-step procedure. First, we retrieve word pairs with high confidences of similarity, computed using our proposed image-based fingerprints, which define the initial pivot for the word alignment. Second, we apply our robust Procrustes algorithm to estimate the linear mapping between two embedding spaces, which iteratively corrects and refines the estimated alignment. Our extensive experiments show that WALIP improves upon the state-of-the-art performance of bilingual word alignment for a few language pairs across different word embeddings and displays great robustness to the dissimilarity of language pairs or training corpora for two word embeddings."}}
{"id": "bPVawVP1WQP", "cdate": 1640995200000, "mdate": 1681682630201, "content": {"title": "Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment", "abstract": ""}}
{"id": "K1Mi7Mf1dj", "cdate": 1640995200000, "mdate": 1681703417226, "content": {"title": "PROS: an efficient pattern-driven compressive sensing framework for low-power biopotential-based wearables with on-chip intelligence", "abstract": "While the global healthcare market of wearable devices has been growing significantly in recent years and is predicted to reach $60 billion by 2028, many important healthcare applications such as seizure monitoring, drowsiness detection, etc. have not been deployed due to the limited battery lifetime, slow response rate, and inadequate biosignal quality. This study proposes PROS, an efficient pattern-driven compressive sensing framework for low-power biopotential-based wearables. PROS eliminates the conventional trade-off between signal quality, response time, and power consumption by introducing tiny pattern recognition primitives and a pattern-driven compressive sensing technique that exploits the sparsity of biosignals. Specifically, we (i) develop tiny machine learning models to eliminate irrelevant biosignal patterns, (ii) efficiently perform compressive sampling of relevant biosignals with appropriate sparse wavelet domains, and (iii) optimize hardware and OS operations to push processing efficiency. PROS also provides an abstraction layer, so the application only needs to care about detected relevant biosignal patterns without knowing the optimizations underneath. We have implemented and evaluated PROS on two open biosignal datasets with 120 subjects and six biosignal patterns. The experimental results on unknown subjects of a practical use case such as epileptic seizure monitoring are very encouraging. PROS can reduce the streaming data rate by 24X while maintaining high fidelity signal. It boosts the power efficiency of the wearable device by more than 1200% and enables the ability to react to critical events immediately on the device. The memory and runtime overheads of PROS are minimal, with a few KBs and 10s of milliseconds for each biosignal pattern, respectively. PROS is currently adopted in research projects in multiple universities and hospitals."}}
{"id": "IRwuWfF6rz", "cdate": 1640995200000, "mdate": 1681682630214, "content": {"title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks", "abstract": "Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling \"no-code machine learning with LMs.\" We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning."}}
{"id": "HFbZG9zZve5", "cdate": 1640995200000, "mdate": 1645838986273, "content": {"title": "Improved Input Reprogramming for GAN Conditioning", "abstract": "We study the GAN conditioning problem, whose goal is to convert a pretrained unconditional GAN into a conditional GAN using labeled data. We first identify and analyze three approaches to this problem -- conditional GAN training from scratch, fine-tuning, and input reprogramming. Our analysis reveals that when the amount of labeled data is small, input reprogramming performs the best. Motivated by real-world scenarios with scarce labeled data, we focus on the input reprogramming approach and carefully analyze the existing algorithm. After identifying a few critical issues of the previous input reprogramming approach, we propose a new algorithm called InRep+. Our algorithm InRep+ addresses the existing issues with the novel uses of invertible neural networks and Positive-Unlabeled (PU) learning. Via extensive experiments, we show that InRep+ outperforms all existing methods, particularly when label information is scarce, noisy, and/or imbalanced. For instance, for the task of conditioning a CIFAR10 GAN with 1% labeled data, InRep+ achieves an average Intra-FID of 76.24, whereas the second-best method achieves 114.51."}}
{"id": "SZN-z9fZPx9", "cdate": 1609459200000, "mdate": 1645838986208, "content": {"title": "Coded-InvNet for Resilient Prediction Serving Systems", "abstract": "Inspired by a new coded computation algorithm for invertible functions, we propose Coded-InvNet a new approach to design resilient prediction serving systems that can gracefully handle stragglers o..."}}
{"id": "M6_LEmfpj0", "cdate": 1609459200000, "mdate": 1681682630203, "content": {"title": "Coded-InvNet for Resilient Prediction Serving Systems", "abstract": "Inspired by a new coded computation algorithm for invertible functions, we propose Coded-InvNet a new approach to design resilient prediction serving systems that can gracefully handle stragglers or node failures. Coded-InvNet leverages recent findings in the deep learning literature such as invertible neural networks, Manifold Mixup, and domain translation algorithms, identifying interesting research directions that span across machine learning and systems. Our experimental results show that Coded-InvNet can outperform existing approaches, especially when the compute resource overhead is as low as 10%. For instance, without knowing which of the ten workers is going to fail, our algorithm can design a backup task so that it can correctly recover the missing prediction result with an accuracy of 85.9%, significantly outperforming the previous SOTA by 32.5%."}}
{"id": "1WYwranXxw", "cdate": 1577836800000, "mdate": 1681703417267, "content": {"title": "WAKE: a behind-the-ear wearable system for microsleep detection", "abstract": "Microsleep, caused by sleep deprivation, sleep apnea, and narcolepsy, costs the U.S.'s economy more than $411 billion/year because of work performance reduction, injuries, and traffic accidents. Mitigating microsleep's consequences require an unobtrusive, reliable, and socially acceptable microsleep detection solution throughout the day, every day. Unfortunately, existing solutions do not meet these requirements. In this paper, we propose a novel behind-the-ear wearable device for microsleep detection, called WAKE. WAKE detects microsleep by monitoring biosignals from the brain, eye movements, facial muscle contractions, and sweat gland activities from behind the user's ears. In particular, we introduce a Three-fold Cascaded Amplifying (3CA) technique to tame the motion artifacts and environmental noises for capturing high fidelity signals. The behind-the-ear form factor is motivated by the fact that bone-conductance headphones, which are worn around the ear, are becoming widely used. This technology trend gives us an opportunity to enable a wide range of cognitive monitoring and improvement applications by integrating more sensing and actuating functionality into the ear-phone, making it a smarter one. Through our prototyping, we show that WAKE can suppress motion and environmental noise in real-time by 9.74-19.47 dB while walking, driving, or staying in different environments ensuring that the biosignals are captured reliably. We evaluated WAKE against gold-standard devices on 19 sleep-deprived and narcoleptic subjects. The Leave-One-Subject-Out Cross-Validation results show the feasibility of WAKE in microsleep detection on an unseen subject with average precision and recall of 76% and 85%, respectively."}}
