{"id": "FMqPswLZ66A", "cdate": 1667475041793, "mdate": 1667475041793, "content": {"title": "STRIVE: Scene Text Replacement In Videos", "abstract": "We propose replacing scene text in videos using deep style transfer and learned photometric transformations.Building on recent progress on still image text replacement,we present extensions that alter text while preserving the appearance and motion characteristics of the original video.Compared to the problem of still image text replacement,our method addresses additional challenges introduced by video, namely effects induced by changing lighting, motion blur, diverse variations in camera-object pose over time,and preservation of temporal consistency. We parse the problem into three steps. First, the text in all frames is normalized to a frontal pose using a spatio-temporal trans-former network. Second, the text is replaced in a single reference frame using a state-of-art still-image text replacement method. Finally, the new text is transferred from the reference to remaining frames using a novel learned image transformation network that captures lighting and blur effects in a temporally consistent manner. Results on synthetic and challenging real videos show realistic text trans-fer, competitive quantitative and qualitative performance,and superior inference speed relative to alternatives. We introduce new synthetic and real-world datasets with paired text objects. To the best of our knowledge this is the first attempt at deep video text replacement."}}
{"id": "tTrrgst6eJK", "cdate": 1648781488716, "mdate": 1648781488716, "content": {"title": "A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning", "abstract": "We propose a method that substantially improves the efficiency of deep distance metric learning based on the optimization of the triplet loss function. One epoch of such training process based on a na\"ive optimization of the triplet loss function has a run-time complexity O(N^3), where N is the number of training samples. Such optimization scales poorly, and the most common approach proposed to address this high complexity issue is based on sub-sampling the set of triplets needed for the training process. Another approach explored in the field relies on an ad-hoc linearization (in terms of N) of the triplet loss that introduces class centroids, which must be optimized using the whole training set for each mini-batch - this means that a na\"ive implementation of this approach has run-time complexity O(N^2). This complexity issue is usually mitigated with poor, but computationally cheap, approximate centroid optimization methods. In this paper, we first propose a solid theory on the linearization of the triplet loss with the use of class centroids, where the main conclusion is that our new linear loss represents a tight upper-bound to the triplet loss. Furthermore, based on the theory above, we propose a training algorithm that no longer requires the centroid optimization step, which means that our approach is the first in the field with a guaranteed linear run-time complexity. We show that the training of deep distance metric learning methods using the proposed upper-bound is substantially faster than triplet-based methods, while producing competitive retrieval accuracy results on benchmark datasets (CUB-200-2011 and CAR196)."}}
{"id": "8LKjO3U3q6p", "cdate": 1648781342230, "mdate": 1648781342230, "content": {"title": "Single-Stream Multi-Level Alignment for Vision-Language Pretraining", "abstract": "Recent progress in large-scale vision-language pre-training has shown the importance of aligning the visual and text modalities for downstream vision-language tasks. Many methods use a dual-stream architecture that fuses visual tokens and language tokens after representation learning, which aligns only at a global level and cannot extract finer-scale semantics. In contrast, we propose a single stream model that aligns the modalities at multiple levels: i) instance level, ii) fine-grained patch level, iii) conceptual semantic level. We achieve this using two novel tasks: symmetric cross-modality reconstruction and a pseudo-labeled key word prediction. In the former part, we mask the input tokens from one of the modalities and use the cross-modal information to reconstruct the masked token, thus improving fine-grained alignment between the two modalities. In the latter part, we parse the caption to select a few key words and feed it together with the momentum encoder pseudo signal to self-supervise the visual encoder, enforcing it to learn rich semantic concepts that are essential for grounding a textual token to an image region. We demonstrate top performance on a set of Vision-Language downstream tasks such as zero-shot/fine-tuned image/text retrieval, referring expression, and VQA. We also demonstrate how the proposed models can align the modalities at multi"}}
{"id": "PG9C1I2PwN", "cdate": 1648781266064, "mdate": 1648781266064, "content": {"title": "Smart mining for Deep metric learning", "abstract": "To solve deep metric learning problems and producing\nfeature embeddings, current methodologies will commonly\nuse a triplet model to minimise the relative distance between\nsamples from the same class and maximise the relative dis-\ntance between samples from different classes. Though suc-\ncessful, the training convergence of this triplet model can be\ncompromised by the fact that the vast majority of the train-\ning samples will produce gradients with magnitudes that are\nclose to zero. This issue has motivated the development of\nmethods that explore the global structure of the embedding\nand other methods that explore hard negative/positive min-\ning. The effectiveness of such mining methods is often asso-\nciated with intractable computational requirements. In this\npaper, we propose a novel deep metric learning method that\ncombines the triplet model and the global structure of the\nembedding space. We rely on a smart mining procedure that\nproduces effective training samples for a low computational\ncost. In addition, we propose an adaptive controller that\nautomatically adjusts the smart mining hyper-parameters\nand speeds up the convergence of the training process. We\nshow empirically that our proposed method allows for fast\nand more accurate training of triplet ConvNets than other\ncompeting mining methods. Additionally, we show that our\nmethod achieves new state-of-the-art embedding results for\nCUB-200-2011 and Cars196 datasets."}}
{"id": "TFLFLV-n0zy", "cdate": 1648780861899, "mdate": 1648780861899, "content": {"title": "Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimizing Global Loss Functions", "abstract": "Recent innovations in training deep convolutional neu-\nral network (ConvNet) models have motivated the design\nof new methods to automatically learn local image descrip-\ntors. The latest deep ConvNets proposed for this task consist\nof a siamese network that is trained by penalising misclas-\nsification of pairs of local image patches. Current results\nfrom machine learning show that replacing this siamese by\na triplet network can improve the classification accuracy in\nseveral problems, but this has yet to be demonstrated for lo-\ncal image descriptor learning. Moreover, current siamese\nand triplet networks have been trained with stochastic gra-\ndient descent that computes the gradient from individual\npairs or triplets of local image patches, which can make\nthem prone to overfitting. In this paper, we first propose the\nuse of triplet networks for the problem of local image de-\nscriptor learning. Furthermore, we also propose the use of\na global loss that minimises the overall classification error\nin the training set, which can improve the generalisation\ncapability of the model. Using the UBC benchmark dataset\nfor comparing local image descriptors, we show that the\ntriplet network produces a more accurate embedding than\nthe siamese network in terms of the UBC dataset errors.\nMoreover, we also demonstrate that a combination of the\ntriplet and global losses produces the best embedding in\nthe field, using this triplet network. Finally, we also show\nthat the use of the central-surround siamese network trained\nwith the global loss produces the best result of the field on\nthe UBC dataset."}}
{"id": "rk-K95b_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Multi-modal Cycle-Consistent Generalized Zero-Shot Learning", "abstract": "In generalized zero shot learning (GZSL), the set of classes are split into seen and unseen classes, where training relies on the semantic features of the seen and unseen classes and the visual representations of only the seen classes, while testing uses the visual representations of the seen and unseen classes. Current methods address GZSL by learning a transformation from the visual to the semantic space, exploring the assumption that the distribution of classes in the semantic and visual spaces is relatively similar. Such methods tend to transform unseen testing visual representations into one of the seen classes\u2019 semantic features instead of the semantic features of the correct unseen class, resulting in low accuracy GZSL classification. Recently, generative adversarial networks (GAN) have been explored to synthesize visual representations of the unseen classes from their semantic features - the synthesized representations of the seen and unseen classes are then used to train the GZSL classifier. This approach has been shown to boost GZSL classification accuracy, but there is one important missing constraint: there is no guarantee that synthetic visual representations can generate back their semantic feature in a multi-modal cycle-consistent manner. This missing constraint can result in synthetic visual representations that do not represent well their semantic features, which means that the use of this constraint can improve GAN-based approaches. In this paper, we propose the use of such constraint based on a new regularization for the GAN training that forces the generated visual features to reconstruct their original semantic features. Once our model is trained with this multi-modal cycle-consistent semantic compatibility, we can then synthesize more representative visual representations for the seen and, more importantly, for the unseen classes. Our proposed approach shows the best GZSL classification results in the field in several publicly available datasets."}}
{"id": "H1baRF-_-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Bayesian Semantic Instance Segmentation in Open Set World", "abstract": "This paper addresses the semantic instance segmentation task in the open-set conditions, where input images can contain known and unknown object classes. The training process of existing semantic instance segmentation methods requires annotation masks for all object instances, which is expensive to acquire or even infeasible in some realistic scenarios, where the number of categories may increase boundlessly. In this paper, we present a novel open-set semantic instance segmentation approach capable of segmenting all known and unknown object classes in images, based on the output of an object detector trained on known object classes. We formulate the problem using a Bayesian framework, where the posterior distribution is approximated with a simulated annealing optimization equipped with an efficient image partition sampler. We show empirically that our method is competitive with state-of-the-art supervised methods on known classes, but also performs well on unknown classes when compared with unsupervised methods."}}
{"id": "rk-BDq-u-r", "cdate": 1451606400000, "mdate": null, "content": {"title": "Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue", "abstract": "A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manually labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth prediction, without requiring a pre-training stage or annotated ground-truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photometric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset gives comparable performance to that of the state-of-the-art supervised methods for single view depth estimation."}}
{"id": "HkNcrgzO-H", "cdate": 1325376000000, "mdate": null, "content": {"title": "Learning codebook weights for action detection", "abstract": "In this work we present a discriminative codebook weighting approach for action detection. We learn global and local weights for the codewords by considering the spatio-temporal Hough voting space of the training sequences. In contrast to the Implicit Shape Model (ISM) where all the codewords that are matched to a local descriptor cast votes with uniform weights, we learn local weights for the matched codewords. In order to learn the local weights we employ Locality-constrained Linear Coding (LLC). Further, we formulate the learning of the global weights as a convex quadratic programming and use alternating optimization to solve for the weights. We demonstrate the performance of the algorithm on KTH action dataset where we compare with the Hough detector using kmeans codebook."}}
