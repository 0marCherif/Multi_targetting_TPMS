{"id": "oDWIsQpkqOR", "cdate": 1672531200000, "mdate": 1683883172388, "content": {"title": "DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises", "abstract": "While diffusion models have achieved great success in generating continuous signals such as images and audio, it remains elusive for diffusion models in learning discrete sequence data like natural languages. Although recent advances circumvent this challenge of discreteness by embedding discrete tokens as continuous surrogates, they still fall short of satisfactory generation quality. To understand this, we first dive deep into the denoised training protocol of diffusion-based sequence generative models and determine their three severe problems, i.e., 1) failing to learn, 2) lack of scalability, and 3) neglecting source conditions. We argue that these problems can be boiled down to the pitfall of the not completely eliminated discreteness in the embedding space, and the scale of noises is decisive herein. In this paper, we introduce DINOISER to facilitate diffusion models for sequence generation by manipulating noises. We propose to adaptively determine the range of sampled noise scales for counter-discreteness training; and encourage the proposed diffused sequence learner to leverage source conditions with amplified noise scales during inference. Experiments show that DINOISER enables consistent improvement over the baselines of previous diffusion-based sequence generative models on several conditional sequence modeling benchmarks thanks to both effective training and inference strategies. Analyses further verify that DINOISER can make better use of source conditions to govern its generative process."}}
{"id": "t0kDJ2h6PL", "cdate": 1640995200000, "mdate": 1681667752348, "content": {"title": "switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder", "abstract": "Multilingual machine translation aims to develop a single model for multiple language directions. However, existing multilingual models based on Transformer are limited in terms of both translation..."}}
{"id": "Te4G8vIRf2b", "cdate": 1640995200000, "mdate": 1683883172493, "content": {"title": "Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning", "abstract": "For sequence generation, both autoregressive models and non-autoregressive models have been developed in recent years. Autoregressive models can achieve high generation quality, but the sequential decoding scheme causes slow decoding speed. Non-autoregressive models accelerate the inference speed with parallel decoding, while their generation quality still needs to be improved due to the difficulty of modeling multi-modalities in data. To address the multi-modality issue, we propose Diff-Glat, a non-autoregressive model featured with a modality diffusion process and residual glancing training. The modality diffusion process decomposes the modalities and reduces the modalities to learn for each transition. And the residual glancing sampling further smooths the modality learning procedures. Experiments demonstrate that, without using knowledge distillation data, Diff-Glat can achieve superior performance in both decoding efficiency and accuracy compared with the autoregressive Transformer."}}
{"id": "LrTuXvKVlT", "cdate": 1640995200000, "mdate": 1682319128722, "content": {"title": "latent-GLAT: Glancing at Latent Variables for Parallel Text Generation", "abstract": "Yu Bao, Hao Zhou, Shujian Huang, Dongqi Wang, Lihua Qian, Xinyu Dai, Jiajun Chen, Lei Li. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "5HvpvYd68b", "cdate": 1632875596238, "mdate": null, "content": {"title": "switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder", "abstract": "Multilingual machine translation aims to develop a single model for multiple language directions. However, existing multilingual models based on Transformer are limited in terms of both translation performance and inference speed. In this paper, we propose switch-GLAT, a non-autoregressive multilingual machine translation model with a code-switch decoder. It can generate contextual code-switched translations for a given source sentence, and perform code-switch back-translation, greatly boosting multilingual translation performance. In addition, its inference is highly efficient thanks to its parallel decoder. Experiments show that our proposed switch-GLAT outperform the multilingual Transformer with as much as 0.74 BLEU improvement and 6.2x faster decoding speed in inference.\n"}}
{"id": "OlPBoNR59D", "cdate": 1609459200000, "mdate": 1651067687458, "content": {"title": "The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21", "abstract": "This paper describes the Volctrans' submission to the WMT21 news translation shared task for German->English translation. We build a parallel (i.e., non-autoregressive) translation system using the Glancing Transformer, which enables fast and accurate parallel decoding in contrast to the currently prevailing autoregressive models. To the best of our knowledge, this is the first parallel translation system that can be scaled to such a practical scenario like WMT competition. More importantly, our parallel translation system achieves the best BLEU score (35.0) on German->English translation task, outperforming all strong autoregressive counterparts."}}
{"id": "LHRD1bHfSER", "cdate": 1609459200000, "mdate": 1651067685594, "content": {"title": "The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21", "abstract": "Lihua Qian, Yi Zhou, Zaixiang Zheng, Yaoming Zhu, Zehui Lin, Jiangtao Feng, Shanbo Cheng, Lei Li, Mingxuan Wang, Hao Zhou. Proceedings of the Sixth Conference on Machine Translation. 2021."}}
{"id": "BnsvjePe19V", "cdate": 1609459200000, "mdate": 1634226519353, "content": {"title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation", "abstract": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "ZaYZfu8pT_N", "cdate": 1601308330901, "mdate": null, "content": {"title": "Non-iterative Parallel Text Generation via Glancing Transformer", "abstract": "Although non-autoregressive models with one-iteration generation achieve remarkable inference speed-up, they still fall behind their autoregressive counterparts in prediction accuracy. The non-autoregressive models with the best accuracy currently rely on multiple decoding iterations, which largely sacrifice the inference speed of non-autoregressive models.  Inspired by the way of learning word dependencies in autoregressive and iterative-decoding models, we propose Glancing Transformer (GLAT) with a glancing language model (GLM), which learns to capture the word dependency gradually. Experiments on three benchmarks demonstrate that our approach can significantly improve the accuracy of non-autoregressive models without multiple decoding iterations. In particular, GLAT achieves state-of-the-art results among non-iterative models and even outperforms top iterative counterparts in some specific benchmarks."}}
{"id": "TUVlGSPcC7d", "cdate": 1546300800000, "mdate": 1631854586414, "content": {"title": "Exploring Diverse Expressions for Paraphrase Generation", "abstract": "Lihua Qian, Lin Qiu, Weinan Zhang, Xin Jiang, Yong Yu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
