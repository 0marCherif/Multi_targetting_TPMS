{"id": "Swc1CCoq_C", "cdate": 1672531200000, "mdate": 1683891163519, "content": {"title": "Pricing Query Complexity of Revenue Maximization", "abstract": "The common way to optimize auction and pricing systems is to set aside a small fraction of the traffic to run experiments. This leads to the question: how can we learn the most with the smallest amount of data? For truthful auctions, this is the sample complexity problem. For posted price auctions, we no longer have access to samples. Instead, the algorithm is allowed to choose a price pt; then for a fresh sample vt ~ D we learn the sign st = sign(pt \u2014 vt) \u2208 {-1, +1}. How many pricing queries are needed to estimate a given parameter of the underlying distribution? We give tight upper and lower bounds on the number of pricing queries required to find an approximately optimal reserve price for general, regular and MHR distributions. Interestingly, for regular distributions, the pricing query and sample complexities match. But for general and MHR distributions, we show a strict separation between them. All known results on sample complexity for revenue optimization follow from a variant of using the optimal reserve price of the empirical distribution. In the pricing query complexity setting, we show that learning the entire distribution within an error of \u03b5 in Levy distance requires strictly more pricing queries than to estimate the reserve. Instead, our algorithm uses a new property we identify called relative flatness to quickly zoom into the right region of the distribution to get the optimal pricing query complexity. * The full version of the paper can be accessed at https://arxiv.org/abs/2111.03158"}}
{"id": "O3ii4T5uWb4", "cdate": 1672531200000, "mdate": 1681657222990, "content": {"title": "Learning Rate Schedules in the Presence of Distribution Shift", "abstract": "We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift, and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedules and their cumulative regret."}}
{"id": "UhhOQ_o4qM_P", "cdate": 1640995200000, "mdate": 1683886500473, "content": {"title": "Limiting Behaviors of Nonconvex-Nonconcave Minimax Optimization via Continuous-Time Systems", "abstract": "Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solut..."}}
{"id": "OtYMs1GkYR", "cdate": 1609459200000, "mdate": 1683891163676, "content": {"title": "Pricing Query Complexity of Revenue Maximization", "abstract": "The common way to optimize auction and pricing systems is to set aside a small fraction of the traffic to run experiments. This leads to the question: how can we learn the most with the smallest amount of data? For truthful auctions, this is the \\emph{sample complexity} problem. For posted price auctions, we no longer have access to samples. Instead, the algorithm is allowed to choose a price $p_t$; then for a fresh sample $v_t \\sim \\mathcal{D}$ we learn the sign $s_t = sign(p_t - v_t) \\in \\{-1,+1\\}$. How many pricing queries are needed to estimate a given parameter of the underlying distribution? We give tight upper and lower bounds on the number of pricing queries required to find an approximately optimal reserve price for general, regular and MHR distributions. Interestingly, for regular distributions, the pricing query and sample complexities match. But for general and MHR distributions, we show a strict separation between them. All known results on sample complexity for revenue optimization follow from a variant of using the optimal reserve price of the empirical distribution. In the pricing query complexity setting, we show that learning the entire distribution within an error of $\\epsilon$ in Levy distance requires strictly more pricing queries than to estimate the reserve. Instead, our algorithm uses a new property we identify called \\emph{relative flatness} to quickly zoom into the right region of the distribution to get the optimal pricing query complexity."}}
{"id": "LVF6LIF3gp-", "cdate": 1609459200000, "mdate": 1683891163489, "content": {"title": "Learning to Price Against a Moving Target", "abstract": "In the Learning to Price setting, a seller posts prices over time with the goal of maximizing revenue while learning the buyer\u2019s valuation. This problem is very well understood when values are stat..."}}
{"id": "-_uChHoQiIo", "cdate": 1609459200000, "mdate": 1683891163616, "content": {"title": "Learning to Price Against a Moving Target", "abstract": "In the Learning to Price setting, a seller posts prices over time with the goal of maximizing revenue while learning the buyer's valuation. This problem is very well understood when values are stationary (fixed or iid). Here we study the problem where the buyer's value is a moving target, i.e., they change over time either by a stochastic process or adversarially with bounded variation. In either case, we provide matching upper and lower bounds on the optimal revenue loss. Since the target is moving, any information learned soon becomes out-dated, which forces the algorithms to keep switching between exploring and exploiting phases."}}
{"id": "ZgAZMvNOICmM", "cdate": 1577836800000, "mdate": 1683886499467, "content": {"title": "Limiting Behaviors of Nonconvex-Nonconcave Minimax Optimization via Continuous-Time Systems", "abstract": "Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solution paths: sometimes converging to a solution, sometimes never converging and instead following a limit cycle, and sometimes diverging. In this paper, we study the limiting behaviors of three classic minimax algorithms: gradient descent ascent (GDA), alternating gradient descent ascent (AGDA), and the extragradient method (EGM). Numerically, we observe that all of these limiting behaviors can arise in Generative Adversarial Networks (GAN) training and are easily demonstrated for a range of GAN problems. To explain these different behaviors, we study the high-order resolution continuous-time dynamics that correspond to each algorithm, which results in the sufficient (and almost necessary) conditions for the local convergence by each method. Moreover, this ODE perspective allows us to characterize the phase transition between these different limiting behaviors caused by introducing regularization as Hopf Bifurcations."}}
{"id": "M2nfZLmd-c", "cdate": 1577836800000, "mdate": 1683886498889, "content": {"title": "The Landscape of Nonconvex-Nonconcave Minimax Optimization", "abstract": "Minimax optimization has become a central tool in machine learning with applications in robust optimization, reinforcement learning, GANs, etc. These applications are often nonconvex-nonconcave, but the existing theory is unable to identify and deal with the fundamental difficulties this poses. In this paper, we study the classic proximal point method (PPM) applied to nonconvex-nonconcave minimax problems. We find that a classic generalization of the Moreau envelope by Attouch and Wets provides key insights. Critically, we show this envelope not only smooths the objective but can convexify and concavify it based on the level of interaction present between the minimizing and maximizing variables. From this, we identify three distinct regions of nonconvex-nonconcave problems. When interaction is sufficiently strong, we derive global linear convergence guarantees. Conversely when the interaction is fairly weak, we derive local linear convergence guarantees with a proper initialization. Between these two settings, we show that PPM may diverge or converge to a limit cycle."}}
{"id": "V8xTeGVwKC", "cdate": 1514764800000, "mdate": 1683891163505, "content": {"title": "A Dynamical System on Bipartite Graphs", "abstract": ""}}
{"id": "SJW_kvZ_WB", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network", "abstract": "An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods."}}
