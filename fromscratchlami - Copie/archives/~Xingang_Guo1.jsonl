{"id": "NtwEUZE6VcL", "cdate": 1652737573436, "mdate": null, "content": {"title": "Global Convergence of Direct Policy Search for State-Feedback $\\mathcal{H}_\\infty$ Robust Control: A Revisit of Nonsmooth Synthesis with Goldstein Subdifferential", "abstract": "Direct policy search has been widely applied in modern reinforcement learning and continuous control. However, the theoretical properties of direct policy search on nonsmooth robust control synthesis have not been fully understood. The optimal $\\mathcal{H}_\\infty$ control framework aims at designing a policy to minimize the closed-loop $\\mathcal{H}_\\infty$ norm, and is arguably the most fundamental robust control paradigm. In this work, we show that direct policy search is guaranteed to find the global solution of the robust $\\mathcal{H}_\\infty$ state-feedback control design problem. Notice that policy search for optimal $\\mathcal{H}_\\infty$ control leads to a constrained nonconvex nonsmooth optimization problem, where the nonconvex feasible set consists of all the policies stabilizing the closed-loop dynamics. We show that for this nonsmooth optimization problem, all Clarke stationary points are global minimum. Next, we identify the coerciveness of the closed-loop $\\mathcal{H}_\\infty$ objective function, and prove that all the sublevel sets of the resultant policy search problem are compact. Based on these properties, we show that Goldstein's subgradient method and its implementable variants can be guaranteed to stay in the nonconvex feasible set and eventually find the global optimal solution of the $\\mathcal{H}_\\infty$ state-feedback synthesis problem. Our work builds a new connection between nonconvex nonsmooth optimization theory and robust control, leading to an interesting global convergence result for direct policy search on optimal $\\mathcal{H}_\\infty$ synthesis."}}
{"id": "jRpYXfwses9", "cdate": 1640995200000, "mdate": 1696002647643, "content": {"title": "Convex Programs and Lyapunov Functions for Reinforcement Learning: A Unified Perspective on the Analysis of Value-Based Methods", "abstract": "Value-based methods play a fundamental role in Markov decision processes (MDPs) and reinforcement learning (RL). In this paper, we present a unified control-theoretic framework for analyzing valued-based methods such as value computation (VC), value iteration (VI), and temporal difference (TD) learning (with linear function approximation). Built upon an intrinsic connection between value-based methods and dynamic systems, we can directly use existing convex testing conditions in control theory to derive various convergence results for the aforementioned value-based methods. These testing conditions are convex programs in form of either linear programming (LP) or semidefinite programming (SDP), and can be solved to construct Lyapunov functions in a straightforward manner. Our analysis reveals some intriguing connections between feedback control systems and RL algorithms. It is our hope that such connections can inspire more work at the intersection of system/control theory and RL."}}
{"id": "_sZfebds5s", "cdate": 1640995200000, "mdate": 1696002647634, "content": {"title": "Global Convergence of Direct Policy Search for State-Feedback $\\mathcal{H}_\\infty$ Robust Control: A Revisit of Nonsmooth Synthesis with Goldstein Subdifferential", "abstract": "Direct policy search has been widely applied in modern reinforcement learning and continuous control. However, the theoretical properties of direct policy search on nonsmooth robust control synthesis have not been fully understood. The optimal $\\mathcal{H}_\\infty$ control framework aims at designing a policy to minimize the closed-loop $\\mathcal{H}_\\infty$ norm, and is arguably the most fundamental robust control paradigm. In this work, we show that direct policy search is guaranteed to find the global solution of the robust $\\mathcal{H}_\\infty$ state-feedback control design problem. Notice that policy search for optimal $\\mathcal{H}_\\infty$ control leads to a constrained nonconvex nonsmooth optimization problem, where the nonconvex feasible set consists of all the policies stabilizing the closed-loop dynamics. We show that for this nonsmooth optimization problem, all Clarke stationary points are global minimum. Next, we identify the coerciveness of the closed-loop $\\mathcal{H}_\\infty$ objective function, and prove that all the sublevel sets of the resultant policy search problem are compact. Based on these properties, we show that Goldstein's subgradient method and its implementable variants can be guaranteed to stay in the nonconvex feasible set and eventually find the global optimal solution of the $\\mathcal{H}_\\infty$ state-feedback synthesis problem. Our work builds a new connection between nonconvex nonsmooth optimization theory and robust control, leading to an interesting global convergence result for direct policy search on optimal $\\mathcal{H}_\\infty$ synthesis."}}
{"id": "TapkT7Qw3dG", "cdate": 1640995200000, "mdate": 1696002647633, "content": {"title": "Exact Formulas for Finite-Time Estimation Errors of Decentralized Temporal Difference Learning with Linear Function Approximation", "abstract": "In this paper, we consider the policy evaluation problem in multi-agent reinforcement learning (MARL) and derive exact closed-form formulas for the finite-time mean-squared estimation errors of decentralized temporal difference (TD) learning with linear function approximation. Our analysis hinges upon the fact that the decentralized TD learning method can be viewed as a Markov jump linear system (MJLS). Then standard MJLS theory can be applied to quantify the mean and covariance matrix of the estimation error of the decentralized TD method at every time step. Various implications of our exact formulas on the algorithm performance are also discussed. An interesting finding is that under a necessary and sufficient stability condition, the mean-squared TD estimation error will converge to an exact limit at a specific exponential rate."}}
{"id": "RTrshHk2uDM", "cdate": 1640995200000, "mdate": 1696002647654, "content": {"title": "Global Convergence of Direct Policy Search for State-Feedback H\u221e Robust Control: A Revisit of Nonsmooth Synthesis with Goldstein Subdifferential", "abstract": "Direct policy search has been widely applied in modern reinforcement learning and continuous control. However, the theoretical properties of direct policy search on nonsmooth robust control synthesis have not been fully understood. The optimal $\\mathcal{H}_\\infty$ control framework aims at designing a policy to minimize the closed-loop $\\mathcal{H}_\\infty$ norm, and is arguably the most fundamental robust control paradigm. In this work, we show that direct policy search is guaranteed to find the global solution of the robust $\\mathcal{H}_\\infty$ state-feedback control design problem. Notice that policy search for optimal $\\mathcal{H}_\\infty$ control leads to a constrained nonconvex nonsmooth optimization problem, where the nonconvex feasible set consists of all the policies stabilizing the closed-loop dynamics. We show that for this nonsmooth optimization problem, all Clarke stationary points are global minimum. Next, we identify the coerciveness of the closed-loop $\\mathcal{H}_\\infty$ objective function, and prove that all the sublevel sets of the resultant policy search problem are compact. Based on these properties, we show that Goldstein's subgradient method and its implementable variants can be guaranteed to stay in the nonconvex feasible set and eventually find the global optimal solution of the $\\mathcal{H}_\\infty$ state-feedback synthesis problem. Our work builds a new connection between nonconvex nonsmooth optimization theory and robust control, leading to an interesting global convergence result for direct policy search on optimal $\\mathcal{H}_\\infty$ synthesis."}}
{"id": "5RuEZnwIlX", "cdate": 1640995200000, "mdate": 1696002647650, "content": {"title": "Convex Programs and Lyapunov Functions for Reinforcement Learning: A Unified Perspective on the Analysis of Value-Based Methods", "abstract": "Value-based methods play a fundamental role in Markov decision processes (MDPs) and reinforcement learning (RL). In this paper, we present a unified control-theoretic framework for analyzing valued-based methods such as value computation (VC), value iteration (VI), and temporal difference (TD) learning (with linear function approximation). Built upon an intrinsic connection between value-based methods and dynamic systems, we can directly use existing convex testing conditions in control theory to derive various convergence results for the aforementioned value-based methods. These testing conditions are convex programs in form of either linear programming (LP) or semidefinite programming (SDP), and can be solved to construct Lyapunov functions in a straightforward manner. Our analysis reveals some intriguing connections between feedback control systems and RL algorithms. It is our hope that such connections can inspire more work at the intersection of system/control theory and RL."}}
{"id": "GhOMggU4xQ", "cdate": 1546300800000, "mdate": 1696002647636, "content": {"title": "Model Predictive Control Paradigms for Direct Contact Membrane Desalination Modeled by Differential Algebraic Equations", "abstract": "Direct Contact Membrane Distillation (DCMD) is an emerging sustainable desalination technology that can utilize solar energy to desalinate seawater. The low water production rate associated with this technology prevents it from becoming commercially feasible. To overcome this challenge, advanced control strategies may be utilized. An optimization-based control scheme termed Model Predictive Control (MPC) provides a natural framework to optimally operate DCMD processes due to its unique control advantages. Among these advantages are the flexibility provided in formulating the objective function, the capability to directly handle process constraints, and the ability to work with various classes of nonlinear systems. Motivated by the above considerations, this paper proposes two MPC schemes that can maximize the water production rate of DCMD systems. The first MPC scheme is formulated to track an optimal set-point while taking input and stability constraints into account. The second MPC scheme termed Economic Model Predictive Control (EMPC) is formulated to maximize the distilled water flux while meeting input, stability and other process operational constraints. To illustrate the effectiveness of the two proposed control paradigms, the total water production under both control designs is compared. Simulation results show that the DCMD process produces more distilled water when it is operated by EMPC than when it is operated by MPC."}}
{"id": "Kn1c1pjNr7", "cdate": 1483228800000, "mdate": 1696002647642, "content": {"title": "Laser beam pointing and stabilization by fractional-order PID control: Tuning rule and experiments", "abstract": "This paper studies the problem of high-precision positioning of laser beams by using a robust Fractional-Order Proportional-Integral-Derivative (FOPID) controller. The control problem addressed in laser beams aims to maintain the position of the laser beam on a Position Sensing Device (PSD) despite the effects of noise and active disturbances. The FOPID controller is well known for its simplicity with better tuning flexibility along with robustness to noise and output disturbance rejections. Thus, a control strategy based on FOPID to achieve the control objectives has been proposed. The FOPID gains and differentiation orders are optimally tuned in order to fulfill the robustness design specifications by solving a nonlinear optimization problem. A comparison to the conventional Proportional-Integral-Derivative (PID) and robust PID is also provided from simulation and experiment set-up. Due to sensor noise, practical PID controllers that filter the position signal before taking the derivative have been also proposed. Experimental results show that the requirements are totally met for the laser beam platform to be stabilized."}}
