{"id": "y3g35MAuIPE", "cdate": 1684288245335, "mdate": 1684288245335, "content": {"title": "Augmenting Anchors by the Detector Itself", "abstract": "Usually, it is diffcult to determine the scale and aspect ratio of anchors for anchor-based object detection methods. Current state-of-the-art object detectors either determine anchor parameters according to objects\u2019 shape and scale in a dataset, or avoid this problem by utilizing anchor-free methods, however, the former scheme is dataset-specifc and the latter methods could not get better performance than the former ones. In this paper, we propose a novel anchor augmentation method named AADI, which means Augmenting Anchors by the Detector Itself. AADI is not an anchorfree method, instead, it can convert the scale and aspect ratio of anchors from a continuous space\nto a discrete space, which greatly alleviates the problem of anchors\u2019 designation. Furthermore, AADI is a learning-based anchor augmentation method, but it does not add any parameters or hyper-parameters, which is benefcial for research and downstream tasks. Extensive experiments on COCO dataset demonstrate the effectiveness of AADI, specifcally, AADI achieves signifcant performance boosts on many state-of-the-art object detectors (eg. at least +2.4 box AP on Faster R-CNN, +2.2 box AP on Mask R-CNN, and +0.9 box AP on Cascade Mask R-CNN). We hope that this simple and cost-effcient method can be widely used in object detection. Code and models are available at https://github.com/WanXiaopei/aadi."}}
{"id": "ZZbtMBea1W", "cdate": 1684150122535, "mdate": 1684150122535, "content": {"title": "Augmenting Anchors by the Detector Itself", "abstract": "Usually, it is diffcult to determine the scale and aspect ratio of anchors for anchor-based object detection methods. Current state-of-the-art object detectors either determine anchor parameters according to objects\u2019 shape and scale in a dataset, or avoid this problem by utilizing anchor-free methods, however, the former scheme is dataset-specifc and the latter methods could not get better performance than the former ones. In this paper, we propose a novel anchor augmentation method named AADI, which means Augmenting Anchors by the Detector Itself. AADI is not an anchorfree method, instead, it can convert the scale and aspect ratio of anchors from a continuous space to a discrete space, which greatly alleviates the problem of anchors\u2019 designation. Furthermore, AADI is a learning-based anchor augmentation\nmethod, but it does not add any parameters or hyper-parameters, which is benefcial for research and downstream tasks. Extensive experiments on COCO dataset demonstrate the effectiveness of AADI, specifcally, AADI achieves signifcant performance boosts on many state-of-the-art object detectors (eg. at least +2.4 box AP on Faster R-CNN, +2.2 box AP on Mask R-CNN, and +0.9 box AP on Cascade Mask R-CNN). We hope that this simple and cost-effcient method can be widely used in object detection. Code and models are available at\nhttps://github.com/WanXiaopei/aadi."}}
{"id": "MJo20lOK2u", "cdate": 1683882065629, "mdate": 1683882065629, "content": {"title": "A Novel 2D Contactless Fingerprint Matching Method", "abstract": "Fingerprint recognition is an important biometric technology. According to the data collection methods, fingerprints can be divided into contact-based and contactless schemes. In recent years, 2D contactless fingerprints have constantly received attention and research attention because of their safety and hygiene advantages and low cost. However, because of its collection mode, contactless fingerprints usually have low resolution and are prone to posture inconsistency. This issue leads to the poor matching effect of most current fingerprint algorithms on 2D contactless fingerprints, especially for mainstream minutiae-based methods. To address these problems, we propose a novel 2D contactless fingerprint matching method named fingerprint triplet-GAN (FTG). It is an end-to-end method consisting of a triplet network and a GAN, and it bypasses the conventional extraction of fingerprint minutiae. First, we use a triplet network to perform data balancing and augmentation and improve the robustness at low resolution. Furthermore, we design a GAN to help extract posture-independent features to improve the rotation robustness of the model. The effectiveness of the proposed method is evaluated on two databases: PolyU and FVC2004DB1. The experimental results show that it can achieve state-of-the-art performance on 2D contactless fingerprints."}}
{"id": "U7U6O1JNIB", "cdate": 1667358730901, "mdate": null, "content": {"title": "StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN", "abstract": "One-shot talking face generation aims at synthesizing a high-quality talking face video from an arbitrary portrait image, driven by a video or an audio segment. In this work, we provide a solution from a novel perspective that differs from existing frameworks. We first investigate the latent feature space of a pre-trained StyleGAN and discover some excellent spatial transformation properties. Upon the observation, we propose a novel unified framework based on a pre-trained StyleGAN that enables a set of powerful functionalities, i.e., high-resolution video generation, disentangled control by driving video or audio, and flexible face editing. Our framework elevates the resolution of the synthesized talking face to 1024\u00d71024 for the first time, even though the training dataset has a lower resolution. Moreover, our framework allows two types of facial editing, i.e., global editing via GAN inversion and intuitive editing via 3D morphable models. Comprehensive experiments show superior video quality and flexible controllability over state-of-the-art methods. Code is available at https://github.com/FeiiYin/StyleHEAT."}}
{"id": "bvwZ43dY2xj", "cdate": 1663850205078, "mdate": null, "content": {"title": "Rethinking Identity in Knowledge Graph  Embedding", "abstract": "Knowledge Graph Embedding (KGE) is a common method to complete real-world Knowledge Graphs (KGs) by learning the embeddings of entities and relations.\nBeyond specific KGE models, previous work proposes a general framework based on group. A group has a special element identity that uniquely corresponds to the relation identity in KGs, which implies that identity should be represented uniquely. However, we find that this uniqueness cannot be modeled by bilinear based models, revealing the crack between the framework and models. To this end, we study the required conditions and propose a solution named Unit Ball Bilinear Model (UniBi). In addition to the theoretical superiority, UniBi is more robust and interpretable. Experiments demonstrate that UniBi models the uniqueness without the cost of performance and verify its a robustness and interpretability. "}}
{"id": "Ih0fKoIUyEh", "cdate": 1663850157998, "mdate": null, "content": {"title": "Wide Graph Neural Network", "abstract": "Usually, graph neural networks (GNNs) suffer from several problems, e.g., over-smoothing (in the spatial domain), poor flexibility (in the spectral domain), and low performance on heterophily (in both domains). In this paper, we provide a new GNN framework, called Wide Graph Neural Networks (WGNN) to solve these problems. It is motivated by our proposed unified view of GNNs from the perspective of dictionary learning. In light of this view, we formulate the graph learning in GNNs as learning representations from the dictionaries, where the fixed graph information is regarded as the dictionary and the trainable parameters are representations. Then, the dictionaries of spatial GNNs encode the adjacency matrix multiplication, while spectral ones sum its polynomials. Differently, WGNN directly concatenates all polynomials as the dictionary, where each polynomial is a sub-dictionary. Beyond polynomials, WGNN allows sub-dictionaries with an arbitrary size, for instance, the principal components of the adjacency matrix. This wide concatenation structure enjoys the great capability of avoiding over-smoothing and promoting flexibility, while the supplement of principal components can significantly improve the representation of heterophilic graphs. We provide a detailed theoretical analysis and conduct extensive experiments on eight datasets to demonstrate the superiority of the proposed WGNN. "}}
{"id": "Om_QvnjjBL2", "cdate": 1663849861159, "mdate": null, "content": {"title": "D2Match: Leveraging Deep Learning and Degeneracy for  Subgraph Matching", "abstract": "Subgraph matching is a fundamental building block for many graph-based applications and is challenging due to its high-order combinatorial nature.  However, previous methods usually tackle it by combinatorial optimization or representation learning and suffer from exponential computational cost or matching without theoretical guarantees.  In this paper, we develop D2Match by leveraging the efficiency of Deep learning and Degeneracy for subgraph matching. More specifically, we prove that subgraph matching can degenerate to subtree matching, and subsequently is equivalent to finding a perfect matching on a bipartite graph.  This matching procedure can be implemented by the built-in tree-structured aggregation mechanism on graph neural networks, which yields linear time complexity.  Moreover, circle structures, abstracted as {\\em supernodes}, and node attributes can be easily incorporated in D2Match to boost the matching. Finally, we conduct extensive experiments to show the superior performance of our D2Match and confirm that our D2Match indeed tries to exploit the subtrees and differs from existing learning-based subgraph matching methods that depend on memorizing the data distribution divergence."}}
{"id": "NgIf3FpcHie", "cdate": 1652737324222, "mdate": null, "content": {"title": "Rethinking Alignment in Video Super-Resolution Transformers", "abstract": "The alignment of adjacent frames is considered an essential operation in video super-resolution (VSR). Advanced VSR models, including the latest VSR Transformers, are generally equipped with well-designed alignment modules. However, the progress of the self-attention mechanism may violate this common sense. In this paper, we rethink the role of alignment in VSR Transformers and make several counter-intuitive observations. Our experiments show that: (i) VSR Transformers can directly utilize multi-frame information from unaligned videos, and (ii) existing alignment methods are sometimes harmful to VSR Transformers. These observations indicate that we can further improve the performance of VSR Transformers simply by removing the alignment module and adopting a larger attention window. Nevertheless, such designs will dramatically increase the computational burden, and cannot deal with large motions. Therefore, we propose a new and efficient alignment method called patch alignment, which aligns image patches instead of pixels. VSR Transformers equipped with patch alignment could demonstrate state-of-the-art performance on multiple benchmarks. Our work provides valuable insights on how multi-frame information is used in VSR and how to select alignment methods for different networks/datasets. Codes and models will be released at https://github.com/XPixelGroup/RethinkVSRAlignment."}}
{"id": "5Ld5bRB9jzY", "cdate": 1621629826628, "mdate": null, "content": {"title": "Adder Attention for Vision Transformer", "abstract": "Transformer is a new kind of calculation paradigm for deep learning which has shown strong performance on a large variety of computer vision tasks. However, compared with conventional deep models (e.g., convolutional neural networks), vision transformers require more computational resources which cannot be easily deployed on mobile devices. To this end, we present to reduce the energy consumptions using adder neural network (AdderNet). We first theoretically analyze the mechanism of self-attention and the difficulty for applying adder operation into this module. Specifically, the feature diversity, i.e., the rank of attention map using only additions cannot be well preserved. Thus, we develop an adder attention layer that includes an additional identity mapping. With the new operation, vision transformers constructed using additions can also provide powerful feature representations. Experimental results on several benchmarks demonstrate that the proposed approach can achieve highly competitive performance to that of the baselines while achieving an about 2~3\u00d7 reduction on the energy consumption. "}}
{"id": "z-5yUFuQ6Sk", "cdate": 1609459200000, "mdate": 1624337153150, "content": {"title": "Domain Fingerprints for No-Reference Image Quality Assessment", "abstract": "Human fingerprints are detailed and nearly unique markers of human identity. Such a unique and stable fingerprint is also left on each acquired image. It can reveal how an image was degraded during the image acquisition procedure and thus is closely related to the quality of an image. In this work, we propose a new no-reference image quality assessment (NR-IQA) approach called domain-aware IQA (DA-IQA), which for the first time introduces the concept of domain fingerprint to the NR-IQA field. The domain fingerprint of an image is learned from image collections of different degradations and then used as the unique characteristics to identify the degradation sources and assess the quality of the image. To this end, we design a new domain-aware architecture, which enables simultaneous determination of both the distortion sources and the quality of an image. With the distortion in an image better characterized, the image quality can be more accurately assessed, as verified by extensive experiments, which show that the proposed DA-IQA performs better than almost all the compared state-of-the-art NR-IQA methods."}}
