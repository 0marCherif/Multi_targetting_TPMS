{"id": "HAl3g3i5DLy", "cdate": 1698649592395, "mdate": 1698649592395, "content": {"title": "Latent Graph Attention for Enhanced Spatial Context", "abstract": "Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also takes into account the influence of the intermediate pixels. Moreover, the depth of the graph network can be used to adapt the extent of contextual spread to the target dataset, thereby being able to explicitly control the added computational cost. To enhance the learning mechanism of LGA, we also introduce a novel contrastive loss term that helps our LGA module to couple well with the original architecture at the expense of minimal additional computational load. We show that incorporating LGA improves the performance on three challenging applications, namely transparent object segmentation, image restoration for dehazing and optical flow estimation."}}
{"id": "5vmmrNp_-Lt", "cdate": 1698649472375, "mdate": 1698649472375, "content": {"title": "pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems", "abstract": "Nearest neighbor (NN) sampling provides more semantic variations than pre-defined transformations for self-supervised learning (SSL) based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline (pNNCLR) to the nearest neighbor based SSL approach (NNCLR). To this end, we introduce pseudo nearest neighbors (pNN) to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods."}}
{"id": "whw851IQgQ", "cdate": 1668764624235, "mdate": 1668764624235, "content": {"title": "Training-based spectral reconstruction from a single RGB image", "abstract": "This paper focuses on a training-based method to recon- struct a scene\u2019s spectral reflectance from a single RGB image captured by a camera with known spectral response. In particular, we explore a new strategy to use training images to model the mapping between camera- specific RGB values and scene reflectance spectra. Our method is based on a radial basis function network that leverages RGB white-balancing to normalize the scene illumination to recover the scene reflectance. We show that our method provides the best result against three state-of-art methods, especially when the tested illumination is not included in the training stage. In addition, we also show an effective approach to recover the spectral illumination from the reconstructed spectral reflectance and RGB image. As a part of this work, we present a newly captured, publicly available, data set of hyperspectral images that are useful for addressing problems pertaining to spectral imaging, analysis and processing.\n"}}
{"id": "pRUW8BuTEFI", "cdate": 1663850504919, "mdate": null, "content": {"title": "MixBin: Towards Budgeted Binarization", "abstract": "Binarization has proven to be amongst the most effective ways of neural network compression, reducing the FLOPs of the original model by a large extent. However, such levels of compression are often accompanied by a significant drop in the performance of the model. There exist some approaches that reduce this performance drop by facilitating partial binarization of the network, however, a systematic approach to mix binary and full-precision parameters in a single network is still missing. In this paper, we propose a paradigm to perform partial binarization of neural networks in a controlled sense, thereby constructing budgeted binary neural network (B2NN). We present $\\texttt{MixBin}$, an iterative search-based strategy that constructs B2NN through optimized mixing of the binary and full-precision components. $\\texttt{MixBin}$ allows to explicitly choose the approximate fraction of the network to be kept as binary, thereby presenting the flexibility to adapt the inference cost at a prescribed budget. We demonstrate through numerical experiments that B2NNs obtained from our $\\texttt{MixBin}$ strategy are significantly better than those obtained from random selection of the network layers. To perform partial binarization in an effective manner, it is important that both the full-precision as well as the binary components of the B2NN are appropriately optimized. We also demonstrate that the choice of the activation function can have a significant effect on this process, and to circumvent this issue, we present BinReLU, an integral component of $\\texttt{MixBin}$, that can be used as an effective activation function for the full-precision as well as the binary components of any B2NN. Experimental investigations reveal that BinReLU outperforms the other activation functions in all possible scenarios of B2NN: zero-, partial- as well as full binarization. Finally, we demonstrate the efficacy of $\\texttt{MixBin}$ on the tasks of classification and object tracking using benchmark datasets."}}
{"id": "kquoMjdoyR", "cdate": 1655063407566, "mdate": 1655063407566, "content": {"title": "Learning Nanoscale Motion Patterns of Vesicles in Living Cells", "abstract": "Detecting and analyzing nanoscale motion patterns of vesicles, smaller than the microscope resolution ( 250 nm), inside living biological cells is a challenging problem. State-of-the-art CV approaches based on detection, tracking, optical flow or deep learning perform poorly for this problem. We propose an integrative approach, built upon physics based simulations, nanoscopy algorithms, and shallow residual attention network to make it possible for the first time to analysis sub-resolution motion patterns in vesicles that may also be of sub-resolution diameter. Our results show state-of-the-art performance, 89% validation accuracy on simulated dataset and 82% testing accuracy on an experimental dataset of living heart muscle cells imaged under three different pathological conditions. We demonstrate automated analysis of the motion states and changed in them for over 9000 vesicles. Such analysis will enable large scale biological studies of vesicle transport and interaction in living cells in the future."}}
{"id": "4EAli5VcRag", "cdate": 1655063200142, "mdate": 1655063200142, "content": {"title": "Physics-based machine learning for subcellular segmentation in living cells", "abstract": "Segmenting subcellular structures in living cells from fluorescence microscope images is a ground truth (GT)-deficient problem. The microscopes\u2019 three-dimensional blurring function, finite optical resolution due to light diffraction, finite pixel resolution and the complex morphological manifestations of the structures all contribute to GT-hardness. Unsupervised segmentation approaches are quite inaccurate. Therefore, manual segmentation relying on heuristics and experience remains the preferred approach. However, this process is tedious, given the countless structures present inside a single cell, and generating analytics across a large population of cells or performing advanced artificial intelligence tasks such as tracking are greatly limited. Here we bring modelling and deep learning to a nexus for solving this GT-hard problem, improving both the accuracy and speed of subcellular segmentation. We introduce a simulation-supervision approach empowered by physics-based GT, which presents two advantages. First, the physics-based GT resolves the GT-hardness. Second, computational modelling of all the relevant physical aspects assists the deep learning models in learning to compensate, to a great extent, for the limitations of physics and the instrument. We show extensive results on the segmentation of small vesicles and mitochondria in diverse and independent living- and fixed-cell datasets. We demonstrate the adaptability of the approach across diverse microscopes through transfer learning, and illustrate biologically relevant applications of automated analytics and motion analysis."}}
{"id": "jJLZ3d4i93Q", "cdate": 1640995200000, "mdate": 1663788917316, "content": {"title": "UltraMNIST Classification: A Benchmark to Train CNNs for Very Large Images", "abstract": "Convolutional neural network (CNN) approaches available in the current literature are designed to work primarily with low-resolution images. When applied on very large images, challenges related to GPU memory, smaller receptive field than needed for semantic correspondence and the need to incorporate multi-scale features arise. The resolution of input images can be reduced, however, with significant loss of critical information. Based on the outlined issues, we introduce a novel research problem of training CNN models for very large images, and present 'UltraMNIST dataset', a simple yet representative benchmark dataset for this task. UltraMNIST has been designed using the popular MNIST digits with additional levels of complexity added to replicate well the challenges of real-world problems. We present two variants of the problem: 'UltraMNIST classification' and 'Budget-aware UltraMNIST classification'. The standard UltraMNIST classification benchmark is intended to facilitate the development of novel CNN training methods that make the effective use of the best available GPU resources. The budget-aware variant is intended to promote development of methods that work under constrained GPU memory. For the development of competitive solutions, we present several baseline models for the standard benchmark and its budget-aware variant. We study the effect of reducing resolution on the performance and present results for baseline models involving pretrained backbones from among the popular state-of-the-art models. Finally, with the presented benchmark dataset and the baselines, we hope to pave the ground for a new generation of CNN methods suitable for handling large images in an efficient and resource-light manner."}}
{"id": "dv3ZVQcl5B", "cdate": 1640995200000, "mdate": 1663788916960, "content": {"title": "Emotionally charged text classification with deep learning and sentiment semantic", "abstract": "Text classification is one of the widely used phenomena in different natural language processing tasks. State-of-the-art text classifiers use the vector space model for extracting features. Recent progress in deep models, recurrent neural networks those preserve the positional relationship among words achieve a higher accuracy. To push text classification accuracy even higher, multi-dimensional document representation, such as vector sequences or matrices combined with document sentiment, should be explored. In this paper, we show that documents can be represented as a sequence of vectors carrying semantic meaning and classified using a recurrent neural network that recognizes long-range relationships. We show that in this representation, additional sentiment vectors can be easily attached as a fully connected layer to the word vectors to further improve classification accuracy. On the UCI sentiment labelled dataset, using the sequence of vectors alone achieved an accuracy of 85.6%, which is better than 80.7% from ridge regression classifier\u2014the best among the classical technique we tested. Additional sentiment information further increases accuracy to 86.3%. On our suicide notes dataset, the best classical technique\u2014the Na\u00edve Bayes Bernoulli classifier, achieves accuracy of 71.3%, while our classifier, incorporating semantic and sentiment information, exceeds that at 75% accuracy."}}
{"id": "JVczIAlIja", "cdate": 1640995200000, "mdate": 1663788916897, "content": {"title": "Learning-based Ellipse Detection for Robotic Grasps of Cylinders and Ellipsoids", "abstract": "In our daily life, there are many objects represented by cylindrical shapes and ellipsoids. The tops of these objects are formed by elliptic shape primitives. Thus, it is available for a robot to manipulate these objects by ellipse detection. In this work, we propose a novel approach to generating ground truth for training the model based on domain randomization. Using synthetic data generated in this manner, we build an end-to-end deep neural network with a detection backbone and then, combine multiple branches archived from the backbone for sharing the multiple-scale features; further, after employing active rotation filters, the features pass through the region proposal net to form the prediction branches of the box, orientation regression, and object classification; finally, these branches are fused to do ellipse detection, allowing robotic manipulations of cylinders and ellipsoids. To demonstrate the capabilities of the proposed detector, we show the comparison results with the state-of-the-art detector on synthetic and public datasets. The proposed model for ellipse detection and data generation pipeline based on domain randomization in a simulation are evaluated by a series of robotic manipulations implemented in real application scenarios. The results illustrate a high success rate on real-world grasp attempts despite having only been trained on a synthetic dataset. (A video of some robotic experiments is available on YouTube: https://youtu.be/Ueg1XSI2S98)."}}
{"id": "3w8A7ZalOgt", "cdate": 1640995200000, "mdate": 1663788917319, "content": {"title": "Counterfactual Explainable Gastrointestinal and Colonoscopy Image Segmentation", "abstract": "Segmenting medical images accurately and reliably is crucial for disease diagnosis and treatment. Due to the wide assortment of objects\u2019 sizes, shapes, and scanning modalities, it has become more challenging. Many convolutional neural networks (CNN) have recently been designed for segmentation tasks and achieved great success. This paper presents an optimized deep learning solution using DeepLabv3+ with ResNet-101 as its backbone. The proposed approach allows capturing variabilities of diverse objects. It provides improved and reliable quantitative and qualitative results in comparison to other state-of-the-art (SOTA) methods on two publicly available gastrointestinal and colonoscopy datasets. Few studies show the inadequacy of stable performance in varying object segmentation tasks, notwithstanding the sizes of objects. Our method has stable performance in the segmentation of large and small medical objects. The explainability of our robust model with benchmarking on SOTA approaches for both datasets will be fruitful for further research on biomedical image segmentation."}}
