{"id": "O6aTlY9UdF", "cdate": 1675209600000, "mdate": 1695971799480, "content": {"title": "Unsupervised learning reveals interpretable latent representations for translucency perception", "abstract": "Author summary Translucency is an essential visual phenomenon, facilitating our interactions with the environment. Perception of translucent materials (i.e., materials that transmit light) is challenging to study due to the high perceptual variability of their appearance across different scenes. We present the first image-computable model that can predict human translucency judgments based on unsupervised learning from natural photographs of translucent objects. We train a deep image generation network to synthesize realistic translucent appearances from unlabeled data and learn a layer-wise latent representation that captures the statistical structure of images at multiple spatial scales. By manipulating specific layers of latent representation, we can independently modify certain visual attributes of the generated object, such as its shape, material, and color, without affecting the others. Particularly, we find the middle-layers of the latent space, which represent mid-to-low spatial scale features, can predict human perception. In contrast, the pixel-based embeddings from dimensionality reduction methods (e.g., t-SNE) do not correlate with perception. Our results suggest that scale-specific representation of visual information might be crucial for humans to perceive materials. We provide a systematic framework to discover perceptually relevant image features from natural stimuli for perceptual inference tasks and therefore valuable for understanding both human and computer vision."}}
{"id": "ZfOgSmcedqI", "cdate": 1672531200000, "mdate": 1695971799516, "content": {"title": "Large Language Models as Superpositions of Cultural Perspectives", "abstract": "Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrating their context-dependent nature). We then conduct quantitative experiments to study the controllability of different models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the effectiveness of various methods for inducing perspectives, and the smoothness of the models' drivability. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions. The project website is available at https://sites.google.com/view/llm-superpositions ."}}
{"id": "k3KBiq9yqtm", "cdate": 1640995200000, "mdate": 1682318695724, "content": {"title": "Language-biased image classification: evaluation based on semantic representations", "abstract": "Humans show language-biased image recognition for a word-embedded image, known as picture-word interference. Such interference depends on hierarchical semantic categories and reflects that human language processing highly interacts with visual processing. Similar to humans, recent artificial models jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased image classification. Exploring whether the bias leads to interference similar to those observed in humans can contribute to understanding how much the model acquires hierarchical semantic representations from joint learning of language and vision. The present study introduces methodological tools from the cognitive science literature to assess the biases of artificial models. Specifically, we introduce a benchmark task to test whether words superimposed on images can distort the image classification across different category levels and, if it can, whether the perturbation is due to the shared semantic representation between language and vision. Our dataset is a set of word-embedded images and consists of a mixture of natural image datasets and hierarchical word labels with superordinate/basic category levels. Using this benchmark test, we evaluate the CLIP model. We show that presenting words distorts the image classification by the model across different category levels, but the effect does not depend on the semantic relationship between images and embedded words. This suggests that the semantic word representation in the CLIP visual processing is not shared with the image representation, although the word representation strongly dominates for word-embedded images."}}
{"id": "UIwuXLw1wY", "cdate": 1640995200000, "mdate": 1682319295495, "content": {"title": "Evaluating language-biased image classification based on semantic representations", "abstract": "Humans show language-biased image recognition for a word-embedded image, known as picture-word interference. Such interference depends on hierarchical semantic categories and reflects that human language processing highly interacts with visual processing. Similar to humans, recent artificial models jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased image classification. Exploring whether the bias leads to interference similar to those observed in humans can contribute to understanding how much the model acquires hierarchical semantic representations from joint learning of language and vision. The present study introduces methodological tools from the cognitive science literature to assess the biases of artificial models. Specifically, we introduce a benchmark task to test whether words superimposed on images can distort the image classification across different category levels and, if it can, whether the perturbation is due to the shared semantic representation between language and vision. Our dataset is a set of word-embedded images and consists of a mixture of natural image datasets and hierarchical word labels with superordinate/basic category levels. Using this benchmark test, we evaluate the CLIP model. We show that presenting words distorts the image classification by the model across different category levels, but the effect does not depend on the semantic relationship between images and embedded words. This suggests that the semantic word representation in the CLIP visual processing is not shared with the image representation, although the word representation strongly dominates for word-embedded images."}}
{"id": "xNO7OEIcJc6", "cdate": 1632875694606, "mdate": null, "content": {"title": "Language-biased image classification: evaluation based on semantic representations", "abstract": "Humans show language-biased image recognition for a word-embedded image, known as picture-word interference. Such interference depends on hierarchical semantic categories and reflects that human language processing highly interacts with visual processing. Similar to humans, recent artificial models jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased image classification. Exploring whether the bias leads to interference similar to those observed in humans can contribute to understanding how much the model acquires hierarchical semantic representations from joint learning of language and vision. The present study introduces methodological tools from the cognitive science literature to assess the biases of artificial models. Specifically, we introduce a benchmark task to test whether words superimposed on images can distort the image classification across different category levels and, if it can, whether the perturbation is due to the shared semantic representation between language and vision. Our dataset is a set of word-embedded images and consists of a mixture of natural image datasets and hierarchical word labels with superordinate/basic category levels. Using this benchmark test, we evaluate the CLIP model. We show that presenting words distorts the image classification by the model across different category levels, but the effect does not depend on the semantic relationship between images and embedded words. This suggests that the semantic word representation in the CLIP visual processing is not shared with the image representation, although the word representation strongly dominates for word-embedded images."}}
{"id": "DDz3lbuRhP", "cdate": 1546300800000, "mdate": 1682318695709, "content": {"title": "Image-based translucency transfer through correlation analysis over multi-scale spatial color distribution", "abstract": ""}}
{"id": "jByuNxTzHqa", "cdate": 1514764800000, "mdate": 1682318695730, "content": {"title": "Material and shape perception based on two types of intensity gradient information", "abstract": "Author summary Objects in our visual world contain a variety of material information. Although such information enables us to experience rich material impressions, it can be a distraction for the estimation of other physical properties such as shapes, albedos, and illuminations. The coupled estimation of these properties we humans perform in daily situations is one of the fundamental mysteries in visual neuroscience. Here, we show that material and shape perception depend on two different types of intensity gradient information. Specifically, our image analyses and psychophysical experiments show that a human\u2019s material perception relies mainly on the intensity gradient magnitude information, while shape perception relies mainly on the intensity gradient order information. In addition, we show that the intensity order information can be utilized for discriminating albedo changes on an object surface from other physical properties including specular highlights."}}
{"id": "BlG3cQrENF", "cdate": 1514764800000, "mdate": 1682318695821, "content": {"title": "Haptic Texture Perception on 3D-Printed Surfaces Transcribed from Visual Natural Textures", "abstract": ""}}
{"id": "bp_g6ntwBOZ", "cdate": 1451606400000, "mdate": 1682318695709, "content": {"title": "Deformation Lamps: A Projection Technique to Make Static Objects Perceptually Dynamic", "abstract": "Light projection is a powerful technique that can be used to edit the appearance of objects in the real world. Based on pixel-wise modification of light transport, previous techniques have successfully modified static surface properties such as surface color, dynamic range, gloss, and shading. Here, we propose an alternative light projection technique that adds a variety of illusory yet realistic distortions to a wide range of static 2D and 3D projection targets. The key idea of our technique, referred to as (Deformation Lamps), is to project only dynamic luminance information, which effectively activates the motion (and shape) processing in the visual system while preserving the color and texture of the original object. Although the projected dynamic luminance information is spatially inconsistent with the color and texture of the target object, the observer's brain automatically combines these sensory signals in such a way as to correct the inconsistency across visual attributes. We conducted a psychophysical experiment to investigate the characteristics of the inconsistency correction and found that the correction was critically dependent on the retinal magnitude of the inconsistency. Another experiment showed that the perceived magnitude of image deformation produced by our techniques was underestimated. The results ruled out the possibility that the effect obtained by our technique stemmed simply from the physical change in an object's appearance by light projection. Finally, we discuss how our techniques can make the observers perceive a vivid and natural movement, deformation, or oscillation of a variety of static objects, including drawn pictures, printed photographs, sculptures with 3D shading, and objects with natural textures including human bodies."}}
{"id": "fnZ4BMKTOdm", "cdate": 1420070400000, "mdate": 1682318695740, "content": {"title": "Deformation lamps: a projection technique to make a static picture dynamic", "abstract": "We propose a new light projection technique named 'Deformation Lamps', which adds a variety of realistic movement impressions to a static projection target. While static pictures are good at expressing spatial information about objects and events, they are not good at expressing temporal information. To overcome the deficit of a static picture, Deformation Lamps superimposes luminance motion signals onto a colorful static picture and produces an illusory, but realistic movement of the picture by fully utilizing the processing characteristics of the human visual system."}}
