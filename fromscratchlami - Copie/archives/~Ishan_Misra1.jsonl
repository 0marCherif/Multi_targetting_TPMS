{"id": "7W28OhXtPDK", "cdate": 1680673747547, "mdate": 1680673747547, "content": {"title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture", "abstract": "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction."}}
{"id": "vzuOFWPEsk", "cdate": 1668603895623, "mdate": 1668603895623, "content": {"title": "Detecting Twenty-thousand Classes using Image-level Supervision", "abstract": "Abstract. Current object detectors are limited in vocabulary size due to\nthe small scale of detection datasets. Image classi\fers, on the other hand,\nreason about much larger vocabularies, as their datasets are larger and\neasier to collect. We propose Detic, which simply trains the classi\fers of a\ndetector on image classi\fcation data and thus expands the vocabulary of\ndetectors to tens of thousands of concepts. Unlike prior work, Detic does\nnot need complex assignment schemes to assign image labels to boxes\nbased on model predictions, making it much easier to implement and\ncompatible with a range of detection architectures and backbones. Our\nresults show that Detic yields excellent detectors even for classes without\nbox annotations. It outperforms prior work on both open-vocabulary and\nlong-tail detection benchmarks. Detic provides a gain of 2.4 mAP for\nall classes and 8.3 mAP for novel classes on the open-vocabulary LVIS\nbenchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP\nwhen evaluated on all classes, or only rare classes, hence closing the gap in\nperformance for object categories with few samples. For the \frst time, we\ntrain a detector with all the twenty-one-thousand classes of the ImageNet\ndataset and show that it generalizes to new datasets without \fnetuning.\nCode is available at https://github.com/facebookresearch/Detic."}}
{"id": "FWxpHAKftz", "cdate": 1667482778444, "mdate": 1667482778444, "content": {"title": "Masked Siamese Networks for Label-Efficient Learning", "abstract": "We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our approach matches the representation of an image view containing randomly masked patches to the representation of the original unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures, while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance, on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4% top-1 accuracy, and with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark. Our code is publicly available."}}
{"id": "04K3PMtMckp", "cdate": 1663850239269, "mdate": null, "content": {"title": "The hidden uniform cluster prior in self-supervised learning", "abstract": "A successful paradigm in representation learning is to perform self-supervised pretraining using tasks based on mini-batch statistics; (e.g., SimCLR, VICReg, SwAV, MSN). We show that in the formulation of all these methods is an overlooked prior to learn features that enable uniform clustering of the data. While this prior has led to remarkably semantic representations when pretraining on class-balanced data, such as ImageNet, we demonstrate that it can hamper performance when pretraining on class-imbalanced data. By moving away from conventional uniformity priors and instead preferring power-law distributed feature clusters, we show that one can improve the quality of the learned representations on real-world class-imbalanced datasets. To demonstrate this, we develop an extension of the Masked Siamese Networks (MSN) method to support the use of arbitrary features priors."}}
{"id": "G1H4NSATlr", "cdate": 1663850065725, "mdate": null, "content": {"title": "RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data", "abstract": "Semi-supervised learning aims to train a model using limited labels. State-of-the-art semi-supervised methods for image classification such as PAWS rely on self-supervised representations learned with large-scale unlabeled but curated data. However, PAWS is often less effective when using real-world unlabeled data that is uncurated, e.g., contains out-of-class data. We propose RoPAWS, a robust extension of PAWS that can work with real-world unlabeled data. We first reinterpret PAWS as a generative classifier that models densities using kernel density estimation. From this probabilistic perspective, we calibrate its prediction based on the densities of labeled and unlabeled data, which leads to a simple closed-form solution from the Bayes' rule. We demonstrate that RoPAWS significantly improves PAWS for uncurated Semi-iNat by +5.3% and curated ImageNet by +0.4%."}}
{"id": "wralRReGNHi", "cdate": 1663849843016, "mdate": null, "content": {"title": "Multiplane NeRF-Supervised Disentanglement of Depth and Camera Pose from Videos", "abstract": "We propose to perform self-supervised disentanglement of depth and camera pose from large-scale videos. We introduce an Autoencoder-based method to reconstruct the input video frames for training, without using any ground-truth annotations of depth and camera. The encoders for our model will estimate the monocular depth and camera pose as the disentangled representations. The decoder will then construct a Multiplane NeRF representation based on the depth encoder feature, and perform rendering to reconstruct the input frames with the estimated camera. The disentanglement is learned with the reconstruction error, based on the assumption that the scene structure does not change in short periods of time in videos. Once the model is learned, it can be applied to multiple applications including depth estimation, camera pose estimation, and single image novel view synthesis. We show substantial improvements over previous self-supervised approaches on all tasks and even better results than counterparts trained with camera ground-truths in some applications. Our code will be made publicly available. "}}
{"id": "ekQ_xrVWwQp", "cdate": 1652737737100, "mdate": null, "content": {"title": "A Data-Augmentation Is Worth A Thousand Samples: Analytical Moments And Sampling-Free Training", "abstract": "Data-Augmentation (DA) is known to improve performance across tasks and datasets. We propose a method to theoretically analyze the effect of DA and study questions such as: how many augmented samples are needed to correctly estimate the information encoded by that DA? How does the augmentation policy impact the final parameters of a model? We derive several quantities in close-form, such as the expectation and variance of an image, loss, and model's output under a given DA distribution. Up to our knowledge, we obtain the first explicit regularizer that corresponds to using DA during training for non-trivial transformations such as affine transformations, color jittering, or Gaussian blur. Those derivations open new avenues to quantify the benefits and limitations of DA. For example, given a loss at hand, we find that common DAs require tens of thousands of samples for the loss to be correctly estimated and for the model training to converge. We then show that for a training loss to have reduced variance under DA sampling, the model's saliency map (gradient of the loss with respect to the model's input) must align with the smallest eigenvector of the sample's covariance matrix under the considered DA augmentation; this is exactly the quantity estimated and regularized by TangentProp. Those findings also hint at a possible explanation on why models tend to shift their focus from edges to textures when specific DAs are employed."}}
{"id": "BHlep_HNGX5", "cdate": 1648670068860, "mdate": 1648670068860, "content": {"title": "Omnivore: A Single Model for Many Visual Modalities", "abstract": "Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'Omnivore' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. Omnivore's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together."}}
{"id": "r1r9v_HxHQ", "cdate": 1640995200000, "mdate": 1663094077449, "content": {"title": "Detecting Twenty-thousand Classes using Image-level Supervision", "abstract": "Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at \\url{https://github.com/facebookresearch/Detic}."}}
{"id": "qsdyGjRlrQh", "cdate": 1640995200000, "mdate": 1663094077438, "content": {"title": "Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision", "abstract": "Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to object centric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, robustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on object centric datasets such as ImageNet."}}
