{"id": "oW4rUGjbMYg", "cdate": 1689650519523, "mdate": null, "content": {"title": "The KCL-SAIR team's entry to the GENEA Challenge 2023 Exploring Role-based Gesture Generation in Dyadic Interactions: Listener vs. Speaker", "abstract": "This paper presents the KCL-SAIR team's contribution to the GENEA Challenge 2023. As this year's challenge addressed gesture generation in a dyadic context instead of a monadic one, our aim was to investigate how the previous state-of-the-art approach can be improved to be more applicable for the generation of both speaker and listener behaviours. The presented solution investigates how taking into account the conversational role of the target agent during training and inference time can influence the overall social appropriateness of the resulting gesture generation system. Our system is evaluated qualitatively based on three factors, including human likeness, appropriateness for agent speech, and appropriateness for interlocutor speech. Our results show that having separate models for listener and speaker behaviours could have potential, especially to generate better listener behaviour. However, the underlying model structures between the speaker and listener behaviour should be different, building on previous state-of-the-art monadic and dyadic solutions."}}
{"id": "f2VocjxnCKR", "cdate": 1682338092335, "mdate": null, "content": {"title": "A Survey of Evaluation Methods and Metrics for Explanations in Human\u2013Robot Interaction (HRI)", "abstract": "The crucial role of explanations in making AI safe and trustworthy was not only recognized by the machine learning community but also by roboticists and human\u2013robot interaction researchers. A robot that can explain its actions is supposed to be better perceived by the user, be more reliable, and seem more trustworthy. In collaborative scenarios, explanations are often expected to even improve the team's performance. To test whether a developed explanation-related ability meets these promises, it is essential to rigorously evaluate them. Due to the many aspects of explanations that can be evaluated, and their varying importance in different circumstances, a plethora of evaluation methods are available. In this survey, we provide a comprehensive overview of such methods while discussing features and considerations unique to explanations given during human\u2013robot interactions."}}
{"id": "iljxJmj5ge", "cdate": 1672531200000, "mdate": 1681826244954, "content": {"title": "Neural Weight Search for Scalable Task Incremental Learning", "abstract": "Task incremental learning aims to enable a system to maintain its performance on previously learned tasks while learning new tasks, solving the problem of catastrophic forgetting. One promising approach is to build an individual network or sub-network for future tasks. However, this leads to an ever-growing memory due to saving extra weights for new tasks and how to address this issue has remained an open problem in task incremental learning. In this paper, we introduce a novel Neural Weight Search technique that designs a fixed search space where the optimal combinations of frozen weights can be searched to build new models for novel tasks in an end-to-end manner, resulting in a scalable and controllable memory growth. Extensive experiments on two benchmarks, i.e., Split-CIFAR-100 and CUB-to-Sketches, show our method achieves state-of-the-art performance with respect to both average inference accuracy and total memory cost. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>"}}
{"id": "Jk5jsT-YRpD", "cdate": 1672531200000, "mdate": 1681826244942, "content": {"title": "A Multimodal Dataset for Robot Learning to Imitate Social Human-Human Interaction", "abstract": "Humans tend to use various nonverbal signals to communicate their messages to their interaction partners. Previous studies utilised this channel as an essential clue to develop automatic approaches for understanding, modelling and synthesizing individual behaviours in human-human interaction and human-robot interaction settings. On the other hand, in small-group interactions, an essential aspect of communication is the dynamic exchange of social signals among interlocutors. This paper introduces LISI-HHI - Learning to Imitate Social Human-Human Interaction, a dataset of dyadic human inter- actions recorded in a wide range of communication scenarios. The dataset contains multiple modalities simultaneously captured by high-accuracy sensors, including motion capture, RGB-D cameras, eye trackers, and microphones. LISI-HHI is designed to be a benchmark for HRI and multimodal learning research for modelling intra- and interpersonal nonverbal signals in social interaction contexts and investigating how to transfer such models to social robots."}}
{"id": "GUQdq5mXH1", "cdate": 1672531200000, "mdate": 1681826244974, "content": {"title": "Automatic Detection of Cognitive Impairment with Virtual Reality", "abstract": "Cognitive impairment features in neuropsychiatric conditions and when undiagnosed can have a severe impact on the affected individual\u2019s safety and ability to perform daily tasks. Virtual Reality (VR) systems are increasingly being explored for the recognition, diagnosis and treatment of cognitive impairment. In this paper, we describe novel VR-derived measures of cognitive performance and show their correspondence with clinically-validated cognitive performance measures. We use an immersive VR environment called VStore where participants complete a simulated supermarket shopping task. People with psychosis (k=26) and non-patient controls (k=128) participated in the study, spanning ages 20\u201379 years. The individuals were split into two cohorts, a homogeneous non-patient cohort (k=99 non-patient participants) and a heterogeneous cohort (k=26 patients, k=29 non-patient participants). Participants\u2019 spatio-temporal behaviour in VStore is used to extract four features, namely, route optimality score, proportional distance score, execution error score, and hesitation score using the Traveling Salesman Problem and explore-exploit decision mathematics. These extracted features are mapped to seven validated cognitive performance scores, via linear regression models. The most statistically important feature is found to be the hesitation score. When combined with the remaining extracted features, the multiple linear regression model resulted in statistically significant results with R2 = 0.369, F-Stat = 7.158, p(F-Stat) = 0.000128."}}
{"id": "A0KK-aLHDx", "cdate": 1672531200000, "mdate": 1681826244942, "content": {"title": "The Impact of Robot's Body Language on Customer Experience: An Analysis in a Cafe Setting", "abstract": "Nonverbal communication plays a crucial role in human-robot interaction (HRI) and have been widely used for robots in service environments. While few studies have addressed the understanding customer's acceptance of robots under many different interaction conditions, the impact of robots' nonverbal interaction modalities (i.e., a combination of body language, voice, and touch) on customers' experience has not been investigated truly. To this end, in this paper, we introduce an HRI framework that aims to assist customers in their food and beverage choices in a real-world cafe setting. With this framework, the contribution of this paper are two folds. We introduce a time-synchronised multisensory HRI dataset comprising the interactions between a social robot and customers in a real-world environment. We conduct a user study to evaluate the configuration of multimodal HRI framework, particularly nonverbal gestures, and its contribution to customers' interaction experience in this specific marketing setting."}}
{"id": "5VHK0q6Oo4M", "cdate": 1652737452114, "mdate": null, "content": {"title": "Policy Gradient With Serial Markov Chain Reasoning", "abstract": "We introduce a new framework that performs decision-making in reinforcement learning (RL) as an iterative reasoning process. We model agent behavior as the steady-state distribution of a parameterized reasoning Markov chain (RMC), optimized with a new tractable estimate of the policy gradient. We perform action selection by simulating the RMC for enough reasoning steps to approach its steady-state distribution. We show our framework has several useful properties that are inherently missing from traditional RL. For instance, it allows agent behavior to approximate any continuous distribution over actions by parameterizing the RMC with a simple Gaussian transition function. Moreover, the number of reasoning steps to reach convergence can scale adaptively with the difficulty of each action selection decision and can be accelerated by re-using past solutions. Our resulting algorithm achieves state-of-the-art performance in popular Mujoco and DeepMind Control benchmarks, both for proprioceptive and pixel-based tasks.\n"}}
{"id": "wo3WY0Fbnun", "cdate": 1640995200000, "mdate": 1667841158162, "content": {"title": "What Does Shared Understanding in Students' Face-to-Face Collaborative Learning Gaze Behaviours \"Look Like\"?", "abstract": "Several studies have shown a positive relationship between measures of gaze behaviours and the quality of student group collaboration over the past decade. Gaze behaviours, however, are frequently employed to investigate i) students\u2019 online interactions and ii) calculated as cumulative measures of collaboration, rarely providing insights into the actual process of collaborative learning in real-world settings. To address these two limitations, we explored the sequences of students\u2019 gaze behaviours as a process and its relationship to collaborative learning in a face-to-face environment. Twenty-five collaborative learning session videos were included from five groups in a 10-week post-graduate module. Four types of gaze behaviours (i.e., gazing at peers, their laptops, tutors, and undefined objects) were used to label student gaze behaviours and the resulting sequences were analyzed using the Optimal Matching (OM) algorithm and Ward\u2019s Clustering. Two distinct types of gaze patterns with different levels of shared understanding and collaboration satisfaction were identified, i) peer-interaction focused (PIF), which prioritise social interaction dimensions of collaboration and ii) resource-interaction focused (RIF) which prioritise resource management and task execution. The implications of the findings for automated detection of students\u2019 gaze behaviours with computer vision and adaptive support are discussed."}}
{"id": "gRT-mJDNFR", "cdate": 1640995200000, "mdate": 1668186669920, "content": {"title": "Stabilizing Off-Policy Deep Reinforcement Learning from Pixels", "abstract": ""}}
{"id": "ehT42Z8FcZ1", "cdate": 1640995200000, "mdate": 1668186669858, "content": {"title": "Personalized Productive Engagement Recognition in Robot-Mediated Collaborative Learning", "abstract": ""}}
