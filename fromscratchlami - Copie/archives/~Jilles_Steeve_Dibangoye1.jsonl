{"id": "S1gfu3EtDr", "cdate": 1569438825656, "mdate": null, "content": {"title": "EgoMap: Projective mapping and structured egocentric memory for Deep RL", "abstract": "Tasks involving localization, memorization and planning in partially observable 3D environments are an ongoing challenge in Deep Reinforcement Learning. We present EgoMap, a spatially structured neural memory architecture. EgoMap augments a deep reinforcement learning agent\u2019s performance in 3D environments on challenging tasks with multi-step objectives. The EgoMap architecture incorporates several inductive biases including a differentiable inverse projection of CNN feature vectors onto a top-down spatially structured map. The map is updated with ego-motion measurements through a differentiable affine transform. We show this architecture outperforms both standard recurrent agents and state of the art agents with structured memory. We demonstrate that incorporating these inductive biases into an agent\u2019s architecture allows for stable training with reward alone, circumventing the expense of acquiring and labelling expert trajectories. A detailed ablation study demonstrates the impact of key aspects of the architecture and through extensive qualitative analysis, we show how the agent exploits its structured internal memory to achieve higher performance. "}}
{"id": "SkEA3cb_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning to Act in Decentralized Partially Observable MDPs", "abstract": "We address a long-standing open problem of reinforcement learning in decentralized partially observable Markov decision processes. Previous attempts focussed on different forms of generalized polic..."}}
{"id": "rJZ_i4fd-H", "cdate": 1420070400000, "mdate": null, "content": {"title": "Structural Results for Cooperative Decentralized Control Models", "abstract": "The intractability in cooperative, decentralized control models is mainly due to prohibitive memory requirements in both optimal policies and value functions. The complexity analysis has emerged as the standard method to estimating the memory needed for solving a given computational problem, but complexity results may be somewhat limited. This paper introduces a general methodology-- structural analysis--for the design of optimality-preserving concise policies and value functions, which will eventually lead to the development of efficient theory and algorithms. For the first time, we show that memory requirements for policies and value functions may be asymmetric, resulting in cooperative, decentralized control models with exponential reductions in memory requirements."}}
{"id": "B1W6W4Gu-B", "cdate": 1420070400000, "mdate": null, "content": {"title": "Exploiting Separability in Multiagent Planning with Continuous-State MDPs (Extended Abstract)", "abstract": "Decentralized partially observable Markov deci- sion processes (Dec-POMDPs) provide a general model for decision-making under uncertainty in co- operative decentralized settings, but are difficult to solve optimally (NEXP-Complete). As a new way of solving these problems, we recently intro- duced a method for transforming a Dec-POMDP into a continuous-state deterministic MDP with a piecewise-linear and convex value function. This new Dec-POMDP formulation, which we call an occupancy MDP, allows powerful POMDP and continuous-state MDP methods to be used for the first time. However, scalability remains limited when the number of agents or problem variables becomes large. In this paper, we show that, un- der certain separability conditions of the optimal value function, the scalability of this approach can increase considerably. This separability is present when there is locality of interaction be- tween agents, which can be exploited to improve performance. Unlike most previous methods, the novel continuous-state MDP algorithm retains op- timality and convergence guarantees. Results show that the extension using separability can scale to a large number of agents and domain variables while maintaining optimality."}}
{"id": "B1VxH4z_bS", "cdate": 1356998400000, "mdate": null, "content": {"title": "Optimally Solving Dec-POMDPs as Continuous-State MDPs", "abstract": "Decentralized partially observable Markov decision processes (Dec-POMDPs) provide a general model for decision-making under uncertainty in decentralized settings, but are difficult to solve optimally (NEXP-Complete). As a new way of solving these problems, we introduce the idea of transforming a Dec-POMDP into a continuous-state deterministic MDP with a piecewise-linear and convex value function. This approach makes use of the fact that planning can be accomplished in a centralized offline manner, while execution can still be decentralized. This new Dec-POMDP formulation, which we call an occupancy MDP, allows powerful POMDP and continuous-state MDP methods to be used for the first time. To provide scalability, we refine this approach by combining heuristic search and compact representations that exploit the structure present in multi-agent domains, without losing the ability to converge to an optimal solution. In particular, we introduce a feature-based heuristic search value iteration (FB-HSVI) algorithm that relies on feature-based compact representations, point-based updates and efficient action selection. A theoretical analysis demonstrates that FB-HSVI terminates in finite time with an optimal solution. We include an extensive empirical analysis using well-known benchmarks, thereby demonstrating that our approach provides significant scalability improvements compared to the state of the art."}}
{"id": "SJ-fRMf_-S", "cdate": 1230768000000, "mdate": null, "content": {"title": "Topological Order Planner for POMDPs", "abstract": "Over the past few years, point-based POMDP solvers scaled up to produce approximate solutions to mid-sized domains. However, to solve real world problems, solvers must exploit the structure of the domain. In this paper we focus on the topological structure of the problem, where the state space contains layers of states. We present here the Topological Order Planner (TOP) that utilizes the topological structure of the domain to compute belief space trajectories. TOP rapidly produces trajectories focused on the solveable regions of the belief space, thus reducing the number of redundant backups considerably. We demonstrate TOP to produce good quality policies faster than any other pointbased algorithm on domains with sufficient structure."}}
