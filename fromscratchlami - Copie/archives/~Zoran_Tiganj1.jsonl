{"id": "g05Epey82Ft", "cdate": 1663850172043, "mdate": null, "content": {"title": "Characterizing neural representation of cognitively-inspired deep RL agents during an evidence accumulation task", "abstract": "Evidence accumulation is thought to be fundamental for decision-making in humans and other mammals. It has been extensively studied in neuroscience and cognitive science with the goal of explaining how sensory information is sequentially sampled until sufficient evidence has accumulated to favor one decision over others. Neuroscience studies suggest that the hippocampus encodes a low-dimensional ordered representation of evidence through sequential neural activity. Cognitive modelers have proposed a mechanism by which such sequential activity could emerge through the modulation of recurrent weights with a change in the amount of evidence. This gives rise to neurons tuned to a specific magnitude of evidence which resemble neurons recorded in the hippocampus. Here we integrated a cognitive science model inside a deep Reinforcement Learning (RL) agent and trained the agent to perform a simple evidence accumulation task inspired by the behavioral experiments on animals. We compared the agent's performance with the performance of agents equipped with GRUs and RNNs. We found that the agent based on a cognitive model was able to learn much faster and generalize better while having significantly fewer parameters. We also compared the emergent neural activity across agents and found that in some cases, GRU-based agents developed similar neural representations to agents based on a cognitive model. This study illustrates how integrating cognitive models and deep learning systems can lead to brain-like neural representations that can improve learning."}}
{"id": "KJ8iuccbPB", "cdate": 1663850171316, "mdate": null, "content": {"title": "Representing Latent Dimensions Using Compressed Number Lines", "abstract": "Humans use log-compressed number lines to represent different quantities, including elapsed time, traveled distance, numerosity, sound frequency, etc. Inspired by recent cognitive science and computational neuroscience work, we developed a neural network that learns to construct log-compressed number lines. The network computes a discrete approximation of a real-domain Laplace transform using an RNN with analytically derived weights giving rise to a log-compressed timeline of the past. The network learns to extract latent variables from the input and uses them for global modulation of the recurrent weights turning a timeline into a number line over relevant dimensions. The number line representation greatly simplifies learning on a set of problems that require learning associations in different spaces - problems that humans can typically solve easily. This approach illustrates how combining deep learning with cognitive models can result in systems that learn to represent latent variables in a brain-like manner and exhibit human-like behavior manifested through Weber-Fechner law."}}
{"id": "wukduxR2k1", "cdate": 1640995200000, "mdate": 1682358997186, "content": {"title": "A deep convolutional neural network that is invariant to time rescaling", "abstract": "Human learners can readily understand speech, or a melody, when it is presented slower or faster than usual. This paper presents a deep CNN (SITHCon) that uses a logarithmically compressed temporal..."}}
{"id": "tn6vqNUJaEW", "cdate": 1621630021509, "mdate": null, "content": {"title": "DeepSITH: Efficient Learning via Decomposition of What and When Across Time Scales", "abstract": "Extracting temporal relationships over a range of scales is a hallmark of\nhuman perception and cognition---and thus it is a critical feature of machine\nlearning applied to real-world problems.  Neural networks are either plagued\nby the exploding/vanishing gradient problem in recurrent neural networks\n(RNNs) or must adjust their parameters to learn the relevant time scales\n(e.g., in LSTMs). This paper introduces DeepSITH, a deep network comprising\nbiologically-inspired Scale-Invariant Temporal History (SITH) modules in\nseries with dense connections between layers. Each SITH module is simply a\nset of time cells coding what happened when with a geometrically-spaced set of\ntime lags.  The dense connections between layers change the definition of what\nfrom one layer to the next.  The geometric series of time lags implies that\nthe network codes time on a logarithmic scale, enabling DeepSITH network to\nlearn problems requiring memory over a wide range of time scales. We compare\nDeepSITH to LSTMs and other recent RNNs on several time series prediction and\ndecoding tasks. DeepSITH achieves results comparable to state-of-the-art\nperformance on these problems and continues to perform well even as the delays\nare increased.\n"}}
{"id": "cdrEjT88kh-", "cdate": 1609459200000, "mdate": 1682358997295, "content": {"title": "DeepSITH: Efficient Learning via Decomposition of What and When Across Time Scales", "abstract": "Extracting temporal relationships over a range of scales is a hallmark ofhuman perception and cognition---and thus it is a critical feature of machinelearning applied to real-world problems. Neural networks are either plaguedby the exploding/vanishing gradient problem in recurrent neural networks(RNNs) or must adjust their parameters to learn the relevant time scales(e.g., in LSTMs). This paper introduces DeepSITH, a deep network comprisingbiologically-inspired Scale-Invariant Temporal History (SITH) modules inseries with dense connections between layers. Each SITH module is simply aset of time cells coding what happened when with a geometrically-spaced set oftime lags. The dense connections between layers change the definition of whatfrom one layer to the next. The geometric series of time lags implies thatthe network codes time on a logarithmic scale, enabling DeepSITH network tolearn problems requiring memory over a wide range of time scales. We compareDeepSITH to LSTMs and other recent RNNs on several time series prediction anddecoding tasks. DeepSITH achieves results comparable to state-of-the-artperformance on these problems and continues to perform well even as the delaysare increased."}}
{"id": "F-ChIPifK6", "cdate": 1609459200000, "mdate": 1682358997176, "content": {"title": "SITHCon: A neural network robust to variations in input scaling on the time dimension", "abstract": "Human learners can readily understand speech, or a melody, when it is presented slower or faster than usual. Although deep convolutional neural networks (CNNs) are extremely powerful in extracting information from time series, they require explicit training to generalize to different time scales. This paper presents a deep CNN that incorporates a temporal representation inspired by recent findings from neuroscience. In the mammalian brain, time is represented by populations of neurons with temporal receptive fields. Critically, the peaks of the receptive fields form a geometric series, such that the population codes a set of temporal basis functions over log time. Because memory for the recent past is a function of log time, rescaling the input results in translation of the memory. The Scale-Invariant Temporal History Convolution network (SITHCon) builds a convolutional layer over this logarithmically-distributed temporal memory. A max-pool operation results in a network that is invariant to rescalings of time modulo edge effects. We compare performance of SITHCon to a Temporal Convolution Network (TCN). Although both networks can learn classification and regression problems on both univariate and multivariate time series f(t), only SITHCon generalizes to rescalings f(at). This property, inspired by findings from contemporary neuroscience and consistent with findings from cognitive psychology, may enable networks that learn with fewer training examples, fewer weights and that generalize more robustly to out of sample data."}}
{"id": "D-HdSp8F6Y", "cdate": 1609459200000, "mdate": 1682358997204, "content": {"title": "DeepSITH: Efficient Learning via Decomposition of What and When Across Time Scales", "abstract": "Extracting temporal relationships over a range of scales is a hallmark of human perception and cognition -- and thus it is a critical feature of machine learning applied to real-world problems. Neural networks are either plagued by the exploding/vanishing gradient problem in recurrent neural networks (RNNs) or must adjust their parameters to learn the relevant time scales (e.g., in LSTMs). This paper introduces DeepSITH, a network comprising biologically-inspired Scale-Invariant Temporal History (SITH) modules in series with dense connections between layers. SITH modules respond to their inputs with a geometrically-spaced set of time constants, enabling the DeepSITH network to learn problems along a continuum of time-scales. We compare DeepSITH to LSTMs and other recent RNNs on several time series prediction and decoding tasks. DeepSITH achieves state-of-the-art performance on these problems."}}
{"id": "kTIeGgeGZF", "cdate": 1546300800000, "mdate": null, "content": {"title": "Estimating Scale-Invariant Future in Continuous Time", "abstract": "Natural learners must compute an estimate of future outcomes that follow from a stimulus in continuous time. Widely used reinforcement learning algorithms discretize continuous time and estimate either transition functions from one step to the next (model-based algorithms) or a scalar value of exponentially discounted future reward using the Bellman equation (model-free algorithms). An important drawback of model-based algorithms is that computational cost grows linearly with the amount of time to be simulated. An important drawback of model-free algorithms is the need to select a timescale required for exponential discounting. We present a computational mechanism, developed based on work in psychology and neuroscience, for computing a scale-invariant timeline of future outcomes. This mechanism efficiently computes an estimate of inputs as a function of future time on a logarithmically compressed scale and can be used to generate a scale-invariant power-law-discounted estimate of expected future reward. The representation of future time retains information about what will happen when. The entire timeline can be constructed in a single parallel operation that generates concrete behavioral and neural predictions. This computational mechanism could be incorporated into future reinforcement learning algorithms."}}
{"id": "ejhECuAheU1", "cdate": 1546300800000, "mdate": 1682358997275, "content": {"title": "Towards a neural-level cognitive architecture: modeling behavior in working memory tasks with neurons", "abstract": "Constrained by results from classic behavioral experiments we provide a neural-level cognitive architecture for modeling behavior in working memory tasks. We propose a canonical microcircuit that can be used as a building block for working memory, decision making and cognitive control. The controller controls gates to route the flow of information between the working memory and the evidence accumulator and sets parameters of the circuits. We show that this type of cognitive architecture can account for results in behavioral experiments such as judgment of recency, probe recognition and delayed-match-to-sample. In addition, the neural dynamics generated by the cognitive architecture provides a good match with neurophysiological data from rodents and monkeys. For instance, it generates cells tuned to a particular amount of elapsed time (time cells), to a particular position in space (place cells) and to a particular amount of accumulated evidence."}}
{"id": "ZGD09yfhBs", "cdate": 1514764800000, "mdate": 1682358997148, "content": {"title": "Compressed Timeline of Recent Experience in Monkey Lateral Prefrontal Cortex", "abstract": "Cognitive theories suggest that working memory maintains not only the identity of recently presented stimuli but also a sense of the elapsed time since the stimuli were presented. Previous studies of the neural underpinnings of working memory have focused on sustained firing, which can account for maintenance of the stimulus identity, but not for representation of the elapsed time. We analyzed single-unit recordings from the lateral prefrontal cortex of macaque monkeys during performance of a delayed match-to-category task. Each sample stimulus triggered a consistent sequence of neurons, with each neuron in the sequence firing during a circumscribed period. These sequences of neurons encoded both stimulus identity and elapsed time. The encoding of elapsed time became less precise as the sample stimulus receded into the past. These findings suggest that working memory includes a compressed timeline of what happened when, consistent with long-standing cognitive theories of human memory."}}
