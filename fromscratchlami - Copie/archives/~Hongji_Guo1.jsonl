{"id": "oOg-gWxFUCc", "cdate": 1698939363481, "mdate": 1698939363481, "content": {"title": "Skeleton-based human action recognition with a physics-augmented encoder-decoder network", "abstract": "Human action recognition is important for many applications such as surveillance monitoring, safety, and healthcare.\nAs 3D body skeletons can accurately characterize body actions and are robust to camera views, we propose\na 3D skeleton-based human action method. Different from the existing skeleton-based methods that use only\ngeometric features for action recognition, we propose a physics-augmented encoder and decoder model that produces\nphysically plausible geometric features for human action recognition. Specifically, given the input skeleton\nsequence, the encoder performs a spatiotemporal graph convolution to produce spatiotemporal features for both\npredicting human actions and estimating the generalized positions and forces of body joints. The decoder, implemented\nas an ODE solver, takes the joint forces and solves the Euler-Lagrangian equation to reconstruct the\nskeletons in the next frame. By training the model to simultaneously minimize the action classification and the\n3D skeleton reconstruction errors, the encoder is ensured to produce features that are consistent with both body\nskeletons and the underlying body dynamics as well as being discriminative. The physics-augmented spatiotemporal\nfeatures are used for human action classification. We evaluate the proposed method on NTU-RGB+D, a\nlarge-scale dataset for skeleton-based action recognition. Compared with existing methods, our method achieves\nhigher accuracy and better generalization ability."}}
{"id": "IvEcX17rDTK", "cdate": 1698939316029, "mdate": 1698939316029, "content": {"title": "Video-based complex human event recognition with a probabilistic transformer", "abstract": "Complex human events are high-level human activities that are composed of a set of interacting primitive human\nactions over time. Complex human event recognition is important for many applications, including security\nsurveillance, healthcare, sports and games. Complex human event recognition requires recognizing not only\nthe constituent primitive actions but also, more importantly, their long range spatiotemporal interactions. To\nmeet this requirement, we propose to exploit the self-attention mechanism in the Transformer to model and\ncapture the long-range interactions among primitive actions. We further extend the conventional Transformer\nto a probabilistic Transformer in order to quantify the event recognition confidence and to detect anomaly\nevents. Specifically, given a sequence of human 3D skeletons, the proposed model first performs primitive action\nlocalization and recognition. The recognized primitive human actions and their features are then fed into the\nprobabilistic Transformer for complex human event recognition. By using a probabilistic attention score, the\nprobabilistic Transformer can not only recognize complex events but also quantify its prediction uncertainty.\nUsing the prediction uncertainty, we further propose to detect anomaly events in an unsupervised manner. We\nevaluate the proposed probabilistic Transformer on FineDiving dataset and Olympics Sports dataset for both\ncomplex event recognition and abnormal event detection. The dataset consists of complex events composed of\nprimitive diving actions. The experimental results demonstrate the effectiveness and superiority of our method\nagainst baseline methods."}}
{"id": "M7iL-LbPi0x", "cdate": 1698938883178, "mdate": 1698938883178, "content": {"title": "Physics-Augmented Autoencoder for 3D Skeleton-Based Gait Recognition", "abstract": "In this paper, we introduce physics-augmented autoencoder (PAA) framework for 3D skeleton-based human gait\nrecognition. Specifically, we construct the autoencoder with\na graph-convolution-based encoder and a physics-based\ndecoder. The encoder takes the skeleton sequence as input and produces the generalized positions and forces of\neach joint, which are taken by the decoder to reconstruct the\ninput skeleton based on the Lagrangian dynamics. In this\nway, the intermediate representations are physically plausible and discriminative. During the inference, the decoder is\ndiscared and a RNN-based classifier takes the output of the\nencoder for gait recognition. We evaluated our proposed\nmethod on three benchmark datasets including Gait3D,\nGREW, and KinectGait. Our method achieves state-ofthe-art performance for 3D skeleton-based gait recognition. Furthermore, extensive ablation studies show that our\nmethod generalizes better and is more robust with smallscale training data by incorporating the physics knowledge.\nWe also validated the physical plausibility of the intermediate representations by making force predictions on real data\nwith physical annotations."}}
{"id": "GiJflrjxcZ", "cdate": 1668526856451, "mdate": 1668526856451, "content": {"title": "Uncertainty-Guided Probabilistic Transformer for Complex Action Recognition", "abstract": "A complex action consists of a sequence of atomic\nactions that interact with each other over a relatively\nlong period of time. This paper introduces a probabilistic model named Uncertainty-Guided Probabilistic Transformer (UGPT) for complex action recognition. The self-attention mechanism of a Transformer is used to capture the\ncomplex and long-term dynamics of the complex actions. By\nexplicitly modeling the distribution of the attention scores,\nwe extend the deterministic Transformer to a probabilistic\nTransformer in order to quantify the uncertainty of the prediction. The model prediction uncertainty is used to improve both training and inference. Specifically, we propose\na novel training strategy by introducing a majority model\nand a minority model based on the epistemic uncertainty.\nDuring the inference, the prediction is jointly made by both\nmodels through a dynamic fusion strategy. Our method is\nvalidated on the benchmark datasets, including Breakfast\nActions, MultiTHUMOS, and Charades. The experiment results show that our model achieves the state-of-the-art performance under both sufficient and insufficient data.\n"}}
{"id": "et5UZG__9Nv", "cdate": 1640995200000, "mdate": 1667365774584, "content": {"title": "Uncertainty-Guided Probabilistic Transformer for Complex Action Recognition", "abstract": "A complex action consists of a sequence of atomic actions that interact with each other over a relatively long period of time. This paper introduces a probabilistic model named Uncertainty-Guided Probabilistic Transformer (UGPT) for complex action recognition. The self-attention mechanism of a Transformer is used to capture the complex and long-term dynamics of the complex actions. By explicitly modeling the distribution of the attention scores, we extend the deterministic Transformer to a probabilistic Transformer in order to quantify the uncertainty of the pre-diction. The model prediction uncertainty is used to improve both training and inference. Specifically, we propose a novel training strategy by introducing a majority model and a minority model based on the epistemic uncertainty. During the inference, the prediction is jointly made by both models through a dynamic fusion strategy. Our method is validated on the benchmark datasets, including Breakfast Actions, MultiTHUMOS, and Charades. The experiment re-sults show that our model achieves the state-of-the-art per-formance under both sufficient and insufficient data."}}
{"id": "bcKbe4MICt4", "cdate": 1640995200000, "mdate": 1681782533256, "content": {"title": "Uncertainty-Based Spatial-Temporal Attention for Online Action Detection", "abstract": "Online action detection aims at detecting the ongoing action in a streaming video. In this paper, we proposed an uncertainty-based spatial-temporal attention for online action detection. By explicitly modeling the distribution of model parameters, we extend the baseline models in a probabilistic manner. Then we quantify the predictive uncertainty and use it to generate spatial-temporal attention that focus on large mutual information regions and frames. For inference, we introduce a two-stream framework that combines the baseline model and the probabilistic model based on the input uncertainty. We validate the effectiveness of our method on three benchmark datasets: THUMOS-14, TVSeries, and HDD. Furthermore, we demonstrate that our method generalizes better under different views and occlusions, and is more robust when training with small-scale data."}}
