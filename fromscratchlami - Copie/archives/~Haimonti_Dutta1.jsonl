{"id": "GfGPFP5DKzP", "cdate": 1640995200000, "mdate": 1682686891350, "content": {"title": "PNRank: Unsupervised ranking of person name entities from noisy OCR text", "abstract": ""}}
{"id": "19UpOxc-JOX", "cdate": 1640995200000, "mdate": 1682686891350, "content": {"title": "A Consensus Algorithm for Linear Support Vector Machines", "abstract": "In the era of big data, an important weapon in a machine learning researcher\u2019s arsenal is a scalable support vector machine (SVM) algorithm. Traditional algorithms for learning SVMs scale superline..."}}
{"id": "wL3JTy5-C8", "cdate": 1609459200000, "mdate": 1682686892327, "content": {"title": "Consensus Based Vertically Partitioned Multi-layer Perceptrons for Edge Computing", "abstract": "Storing large volumes of data on distributed devices has become commonplace in recent years. Applications involving sensors, for example, capture data in different modalities including image, video, audio, GPS and others. Novel distributed algorithms are required to learn from this rich, multi-modal data. In this paper, we present an algorithm for learning consensus based multi-layer perceptrons on resource-constrained devices. Assuming nodes (devices) in the distributed system are arranged in a graph and contain vertically partitioned data and labels, the goal is to learn a global function that minimizes the loss. Each node learns a feed-forward multi-layer perceptron and obtains a loss on data stored locally. It then gossips with a neighbor, chosen uniformly at random, and exchanges information about the loss. The updated loss is used to run a back propagation algorithm and adjust local weights appropriately. This method enables nodes to learn the global function without exchange of data in the network. Empirical results reveal that the consensus algorithm converges to the centralized model and has performance comparable to centralized multi-layer perceptrons and tree-based algorithms including random forests and gradient boosted decision trees. Since it is completely decentralized, scalable with network size, can be used for binary and multi-class problems, not affected by feature overlap, and has good empirical convergence properties, it can be used for on-device machine learning."}}
{"id": "QUZvF8Vztld", "cdate": 1609459200000, "mdate": 1682686892064, "content": {"title": "Consensus Based Multi-Layer Perceptrons for Edge Computing", "abstract": "In recent years, storing large volumes of data on distributed devices has become commonplace. Applications involving sensors, for example, capture data in different modalities including image, video, audio, GPS and others. Novel algorithms are required to learn from this rich distributed data. In this paper, we present consensus based multi-layer perceptrons for resource-constrained devices. Assuming nodes (devices) in the distributed system are arranged in a graph and contain vertically partitioned data, the goal is to learn a global function that minimizes the loss. Each node learns a feed-forward multi-layer perceptron and obtains a loss on data stored locally. It then gossips with a neighbor, chosen uniformly at random, and exchanges information about the loss. The updated loss is used to run a back propagation algorithm and adjust weights appropriately. This method enables nodes to learn the global function without exchange of data in the network. Empirical results reveal that the consensus algorithm converges to the centralized model and has performance comparable to centralized multi-layer perceptrons and tree-based algorithms including random forests and gradient boosted decision trees."}}
{"id": "mBntLnkeJlO", "cdate": 1577836800000, "mdate": 1682686892035, "content": {"title": "Impact of Community Structure on Consensus Machine Learning", "abstract": "Consensus dynamics support decentralized machine learning for data that is distributed across a cloud compute cluster or across the internet of things. In these and other settings, one seeks to minimize the time $\\tau_\\epsilon$ required to obtain consensus within some $\\epsilon>0$ margin of error. $\\tau_\\epsilon$ typically depends on the topology of the underlying communication network, and for many algorithms $\\tau_\\epsilon$ depends on the second-smallest eigenvalue $\\lambda_2\\in[0,1]$ of the network's normalized Laplacian matrix: $\\tau_\\epsilon\\sim\\mathcal{O}(\\lambda_2^{-1})$. Here, we analyze the effect on $\\tau_\\epsilon$ of network community structure, which can arise when compute nodes/sensors are spatially clustered, for example. We study consensus machine learning over networks drawn from stochastic block models, which yield random networks that can contain heterogeneous communities with different sizes and densities. Using random matrix theory, we analyze the effects of communities on $\\lambda_2$ and consensus, finding that $\\lambda_2$ generally increases (i.e., $\\tau_\\epsilon$ decreases) as one decreases the extent of community structure. We further observe that there exists a critical level of community structure at which $\\tau_\\epsilon$ reaches a lower bound and is no longer limited by the presence of communities. We support our findings with empirical experiments for decentralized support vector machines."}}
{"id": "uo2Jjposjf", "cdate": 1514764800000, "mdate": 1682686891011, "content": {"title": "Consensus-based modeling using distributed feature construction with ILP", "abstract": "A particularly successful role for Inductive Logic Programming (ILP) is as a tool for discovering useful relational features for subsequent use in a predictive model. Conceptually, the case for using ILP to construct relational features rests on treating these features as functions, the automated discovery of which necessarily requires some form of first-order learning. Practically, there are now several reports in the literature that suggest that augmenting any existing feature with ILP-discovered relational features can substantially improve the predictive power of a model. While the approach is straightforward enough, much still needs to be done to scale it up to explore more fully the space of possible features that can be constructed by an ILP system. This is in principle, infinite and in practice, extremely large. Applications have been confined to heuristic or random selections from this space. In this paper, we address this computational difficulty by allowing features and models to be constructed in a distributed manner. That is, there is a network of computational units, each of which employs an ILP engine to construct some small number of features and then builds a (local) model. We then employ an asynchronous consensus-based algorithm, in which neighboring nodes share information and update local models. This gossip-based information exchange results in the formation of non-stationary Markov chains. For a category of models (those with convex loss functions), it can be shown (using the Supermartingale Convergence Theorem) that the algorithm will result in all nodes converging to a consensus model. In practice, it may be slow to achieve this convergence. Nevertheless, our results on synthetic and real datasets suggest that in relatively short time the \u201cbest\u201d node in the network reaches a model whose predictive accuracy is comparable to that obtained using more computational effort in a non-distributed setting (the best node is identified as the one whose weights converge first)."}}
{"id": "_xJrhCUnRO", "cdate": 1514764800000, "mdate": 1682686891096, "content": {"title": "A Machine Learning Approach to Quantitative Prosopography", "abstract": "Prosopography is an investigation of the common characteristics of a group of people in history, by a collective study of their lives. It involves a study of biographies to solve historical problems. If such biographies are unavailable, surviving documents and secondary biographical data are used. Quantitative prosopography involves analysis of information from a wide variety of sources about \"ordinary people\". In this paper, we present a machine learning framework for automatically designing a people gazetteer which forms the basis of quantitative prosopographical research. The gazetteer is learnt from the noisy text of newspapers using a Named Entity Recognizer (NER). It is capable of identifying influential people from it by making use of a custom designed Influential Person Index (IPI). Our corpus comprises of 14020 articles from a local newspaper, \"The Sun\", published from New York in 1896. Some influential people identified by our algorithm include Captain Donald Hankey (an English soldier), Dame Nellie Melba (an Australian operatic soprano), Hugh Allan (a Canadian shipping magnate) and Sir Hugh John McDonald (the first Prime Minister of Canada)."}}
{"id": "W6E6i37-Dk", "cdate": 1514764800000, "mdate": 1682686892271, "content": {"title": "A system for intergroup prejudice detection: The case of microblogging under terrorist attacks", "abstract": ""}}
{"id": "5DE3oddndTQ", "cdate": 1514764800000, "mdate": 1682686892055, "content": {"title": "GADGET SVM: A Gossip-bAseD sub-GradiEnT Solver for Linear SVMs", "abstract": "In the era of big data, an important weapon in a machine learning researcher's arsenal is a scalable Support Vector Machine (SVM) algorithm. SVMs are extensively used for solving classification problems. Traditional algorithms for learning SVMs often scale super linearly with training set size which becomes infeasible very quickly for large data sets. In recent years, scalable algorithms have been designed which study the primal or dual formulations of the problem. This often suggests a way to decompose the problem and facilitate development of distributed algorithms. In this paper, we present a distributed algorithm for learning linear Support Vector Machines in the primal form for binary classification called Gossip-bAseD sub-GradiEnT (GADGET) SVM. The algorithm is designed such that it can be executed locally on nodes of a distributed system. Each node processes its local homogeneously partitioned data and learns a primal SVM model. It then gossips with random neighbors about the classifier learnt and uses this information to update the model. Extensive theoretical and empirical results suggest that this anytime algorithm has performance comparable to its centralized and online counterparts."}}
{"id": "5OwIJ1PYNN", "cdate": 1483228800000, "mdate": 1682686891387, "content": {"title": "Ranking Clinical Trials Using Elasticsearch", "abstract": ""}}
