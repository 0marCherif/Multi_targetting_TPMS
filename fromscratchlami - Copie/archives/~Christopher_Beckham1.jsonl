{"id": "aYbCpFNnHdh", "cdate": 1601308376454, "mdate": null, "content": {"title": "Visual Question Answering From Another Perspective: CLEVR Mental Rotation Tests", "abstract": "Different types of \\emph{mental rotation tests} have been used extensively in psychology to understand human visual reasoning and perception. Understanding what an object or visual scene would look like from another viewpoint is a challenging problem that is made even harder if it must be performed from a single image. 3D computer vision has a long history of examining related problems. However, often what one is most interested in is the answer to a relatively simple question posed in another visual frame of reference -- as opposed to creating a full 3D reconstruction. \nMental rotations tests can also manifest as consequential questions in the real world such as: does the pedestrian that I see, see the car that I am driving?\nWe explore a controlled setting whereby questions are posed about the properties of a scene if the scene were observed from another viewpoint. To do this we have created a new version of the CLEVR VQA problem setup and dataset that we call CLEVR Mental Rotation Tests or CLEVR-MRT, where the goal is to answer questions about the original CLEVR viewpoint given a single image obtained from a different viewpoint of the same scene. Using CLEVR Mental Rotation Tests we examine standard state of the art methods, show how they fall short, then explore novel neural architectures that involve inferring representations encoded as feature volumes describing a scene. Our new methods use rigid transformations of feature volumes conditioned on the viewpoint camera. We examine the efficacy of different model variants through performing a rigorous ablation study. Furthermore, we examine the use of contrastive learning to infer a volumetric encoder in a self-supervised manner and find that this approach yields the best results of our study using CLEVR-MRT."}}
{"id": "ge3sG7jIhB", "cdate": 1581944955189, "mdate": null, "content": {"title": "On Adversarial Mixup Resynthesis", "abstract": "In this paper, we explore new approaches to combining information encoded within\nthe learned representations of auto-encoders. We explore models that are capable\nof combining the attributes of multiple inputs such that a resynthesised output\nis trained to fool an adversarial discriminator for real versus synthesised data.\nFurthermore, we explore the use of such an architecture in the context of semisupervised learning, where we learn a mixing function whose objective is to produce\ninterpolations of hidden states, or masked combinations of latent representations\nthat are consistent with a conditioned class label. We show quantitative and\nqualitative evidence that such a formulation is an interesting avenue of research."}}
{"id": "xxod8zoYkr", "cdate": 1581944830428, "mdate": null, "content": {"title": "On adversarial mixup resynthesis", "abstract": "In this paper, we explore new approaches to combining information encoded within\nthe learned representations of auto-encoders. We explore models that are capable\nof combining the attributes of multiple inputs such that a resynthesised output\nis trained to fool an adversarial discriminator for real versus synthesised data.\nFurthermore, we explore the use of such an architecture in the context of semisupervised learning, where we learn a mixing function whose objective is to produce\ninterpolations of hidden states, or masked combinations of latent representations\nthat are consistent with a conditioned class label. We show quantitative and\nqualitative evidence that such a formulation is an interesting avenue of research."}}
{"id": "ryMsu4BlIr", "cdate": 1567802483078, "mdate": null, "content": {"title": "On Adversarial Mixup Resynthesis", "abstract": "In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research."}}
{"id": "Syx9EIIKdN", "cdate": 1553716786001, "mdate": null, "content": {"title": "Adversarial Mixup Resynthesizers", "abstract": "In this paper, we explore new approaches to combining information encoded within the learned representations of autoencoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research."}}
{"id": "HJZDQh-OZB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Manifold Mixup: Better Representations by Interpolating Hidden States", "abstract": "Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts..."}}
{"id": "rJlRKjActQ", "cdate": 1538087813638, "mdate": null, "content": {"title": "Manifold Mixup: Learning Better Representations by Interpolating Hidden States", "abstract": "Deep networks often perform well on the data distribution on which they are trained, yet give incorrect (and often very confident) answers when evaluated on points from off of the training distribution. This is exemplified by the adversarial examples phenomenon but can also be seen in terms of model generalization and domain shift.  Ideally, a model would assign lower confidence to points unlike those from the training distribution.  We propose a regularizer which addresses this issue by training with interpolated hidden states and encouraging the classifier to be less confident at these points.  Because the hidden states are learned, this has an important effect of encouraging the hidden states for a class to be concentrated in such a way so that interpolations within the same class or between two different classes do not intersect with the real data points from other classes.  This has a major advantage in that it avoids the underfitting which can result from interpolating in the input space.  We prove that the exact condition for this problem of underfitting to be avoided by Manifold Mixup is that the dimensionality of the hidden states exceeds the number of classes, which is often the case in practice.  Additionally, this concentration can be seen as making the features in earlier layers more discriminative.  We show that despite requiring no significant additional computation, Manifold Mixup achieves large improvements over strong baselines in supervised learning, robustness to single-step adversarial attacks, semi-supervised learning, and Negative Log-Likelihood on held out samples."}}
{"id": "r1VJTU-OZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Unsupervised Depth Estimation, 3D Face Rotation and Replacement", "abstract": "We present an unsupervised approach for learning to estimate three dimensional (3D) facial structure from a single image while also predicting 3D viewpoint transformations that match a desired pose and facial geometry. We achieve this by inferring the depth of facial keypoints of an input image in an unsupervised manner, without using any form of ground-truth depth information. We show how it is possible to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3D affine transformation matrix that maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on a desired target facial geometry or pose. Our resulting approach, called DepthNets, can therefore be used to infer plausible 3D transformations from one face pose to another, allowing faces to be frontalized, transformed into 3D models or even warped to another pose and facial geometry. Lastly, we identify certain shortcomings with our formulation, and explore adversarial image translation techniques as a post-processing step to re-synthesize complete head shots for faces re-targeted to different poses or identities."}}
{"id": "ryEM6O-OWr", "cdate": 1483228800000, "mdate": null, "content": {"title": "ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events", "abstract": "Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect."}}
{"id": "B1-GCcZ_bS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Unimodal Probability Distributions for Deep Ordinal Classification", "abstract": "Probability distributions produced by the cross-entropy loss for ordinal classification problems can possess undesired properties. We propose a straightforward technique to constrain discrete ordin..."}}
