{"id": "enoU_Kp7Dz", "cdate": 1664731446037, "mdate": null, "content": {"title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability", "abstract": "Traditional analyses of gradient descent with learning rate $\\eta$ show that when the largest eigenvalue of the Hessian of the loss, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is \"stable\" and the training loss decreases monotonically. However, Cohen et al. (2021) recently observed two important phenomena. The first, \\emph{progressive sharpening}, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, \\emph{edge of stability}, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss non-monotonically decreases.\nWe demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call \\emph{self-stabilization}, is a general property of gradient descent and explains its behavior at the edge of stability.\nA key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows \\emph{projected} gradient descent (PGD) under the constraint $S(\\theta) \\le 2/\\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability."}}
{"id": "nhKHA59gXz", "cdate": 1663850180296, "mdate": null, "content": {"title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability", "abstract": "Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is \"stable\" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen at al. (2021) detailed two important phenomena. The first, dubbed \\emph{progressive sharpening}, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, dubbed \\emph{edge of stability}, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call \\emph{self-stabilization}, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows \\emph{projected} gradient descent (PGD) under the constraint $S(\\theta) \\le 2/\\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability."}}
{"id": "052QkenIdSI", "cdate": 1652737639609, "mdate": null, "content": {"title": "Identifying good directions to escape the NTK regime and efficiently learn low-degree plus sparse polynomials ", "abstract": "A recent goal in the theory of deep learning is to identify how neural networks can escape the \u201clazy training,\u201d or Neural Tangent Kernel (NTK) regime, where the network is coupled with its first order Taylor expansion at initialization. While the NTK is minimax optimal for learning dense polynomials (Ghorbani et al, 2021), it cannot learn features, and hence has poor sample complexity for learning many classes of functions including sparse polynomials. Recent works have thus aimed to identify settings where gradient based algorithms provably generalize better than the NTK. One such example is the \u201cQuadNTK\u201d approach of Bai & Lee (2020), which analyzes the second-order term in the Taylor expansion. Bai & Lee (2020) show that the second-order term can learn sparse polynomials efficiently; however, it sacrifices the ability to learn general dense polynomials.\n\nIn this paper, we analyze how gradient descent on a two-layer neural network can escape the NTK regime by utilizing a spectral characterization of the NTK (Montanari & Zhong, 2020) and building on the QuadNTK approach. We first expand upon the spectral analysis to identify \u201cgood\u201d directions in parameter space in which we can move without harming generalization. Next, we show that a wide two-layer neural network can jointly use the NTK and QuadNTK to fit target functions consisting of a dense low-degree term and a sparse high-degree term -- something neither the NTK nor the QuadNTK can do on their own. Finally, we construct a regularizer which encourages the parameter vector to move in the \u201cgood\" directions, and show that gradient descent on the regularized loss will converge to a global minimizer, which also has low test error. This yields an end to end convergence and generalization guarantee with provable sample complexity improvement over both the NTK and QuadNTK on their own."}}
{"id": "f-qA4D0l32", "cdate": 1640995200000, "mdate": 1681497516096, "content": {"title": "Identifying good directions to escape the NTK regime and efficiently learn low-degree plus sparse polynomials", "abstract": ""}}
{"id": "_opWg4ys0-n", "cdate": 1640995200000, "mdate": 1683759386243, "content": {"title": "Identifying good directions to escape the NTK regime and efficiently learn low-degree plus sparse polynomials", "abstract": "A recent goal in the theory of deep learning is to identify how neural networks can escape the \u201clazy training,\u201d or Neural Tangent Kernel (NTK) regime, where the network is coupled with its first order Taylor expansion at initialization. While the NTK is minimax optimal for learning dense polynomials (Ghorbani et al, 2021), it cannot learn features, and hence has poor sample complexity for learning many classes of functions including sparse polynomials. Recent works have thus aimed to identify settings where gradient based algorithms provably generalize better than the NTK. One such example is the \u201cQuadNTK\u201d approach of Bai &amp; Lee (2020), which analyzes the second-order term in the Taylor expansion. Bai &amp; Lee (2020) show that the second-order term can learn sparse polynomials efficiently; however, it sacrifices the ability to learn general dense polynomials.In this paper, we analyze how gradient descent on a two-layer neural network can escape the NTK regime by utilizing a spectral characterization of the NTK (Montanari &amp; Zhong, 2020) and building on the QuadNTK approach. We first expand upon the spectral analysis to identify \u201cgood\u201d directions in parameter space in which we can move without harming generalization. Next, we show that a wide two-layer neural network can jointly use the NTK and QuadNTK to fit target functions consisting of a dense low-degree term and a sparse high-degree term -- something neither the NTK nor the QuadNTK can do on their own. Finally, we construct a regularizer which encourages the parameter vector to move in the \u201cgood\" directions, and show that gradient descent on the regularized loss will converge to a global minimizer, which also has low test error. This yields an end to end convergence and generalization guarantee with provable sample complexity improvement over both the NTK and QuadNTK on their own."}}
{"id": "XqDfLLoMAg", "cdate": 1640995200000, "mdate": 1683759386557, "content": {"title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability", "abstract": "Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is \"stable\" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, dubbed edge of stability, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call self-stabilization, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows projected gradient descent (PGD) under the constraint $S(\\theta) \\le 2/\\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability."}}
{"id": "MtKZYolfCTS", "cdate": 1640995200000, "mdate": 1683759386200, "content": {"title": "Causal Structure Discovery between Clusters of Nodes Induced by Latent Factors", "abstract": "We consider the problem of learning the structure of a causal directed acyclic graph (DAG) model in the presence of latent variables. We define \"latent factor causal models\" (LFCMs) as a restriction on causal DAG models with latent variables, which are composed of clusters of observed variables that share the same latent parent and connections between these clusters given by edges pointing from the observed variables to latent variables. LFCMs are motivated by gene regulatory networks, where regulatory edges, corresponding to transcription factors, connect spatially clustered genes. We show identifiability results on this model and design a consistent three-stage algorithm that discovers clusters of observed nodes, a partial ordering over clusters, and finally, the entire structure over both observed and latent nodes. We evaluate our method in a synthetic setting, demonstrating its ability to almost perfectly recover the ground truth clustering even at relatively low sample sizes, as well as the ability to recover a significant number of the edges from observed variables to latent factors. Finally, we apply our method in a semi-synthetic setting to protein mass spectrometry data with a known ground truth network, and achieve almost perfect recovery of the ground truth variable clusters."}}
{"id": "D3wmPdXOvK", "cdate": 1635261626791, "mdate": null, "content": {"title": "Causal Structure Discovery between Clusters of Nodes Induced by Latent Factors", "abstract": "We consider the problem of learning the structure of a causal directed acyclic graph (DAG) model in the presence of latent variables. We define \"latent factor causal models\" (LFCMs) as a restriction on causal DAG models with latent variables, which are composed of clusters of observed variables that share the same latent parent and connections between these clusters given by edges pointing from the observed variables to latent variables. LFCMs are motivated by gene regulatory networks, where regulatory edges, corresponding to transcription factors, connect spatially clustered genes. We show identifiability results on this model and design a consistent three-stage algorithm that discovers clusters of observed nodes, a partial ordering over clusters, and finally, the entire structure over both observed and latent nodes. We evaluate our method in a synthetic setting, demonstrating its ability to almost perfectly recover the ground truth clustering even at relatively low sample sizes, as well as the ability to recover a significant number of the edges from observed variables to latent factors. Finally, we apply our method in a semi-synthetic setting to protein mass spectrometry data with a known ground truth network, and achieve almost perfect recovery of the ground truth variable clusters."}}
{"id": "ZwZ3sc0qad", "cdate": 1601308233004, "mdate": null, "content": {"title": "On Alignment in Deep Linear Neural Networks", "abstract": "    We study the properties of alignment, a form of implicit regularization, in linear neural networks under gradient descent.  We define alignment for fully connected networks with multidimensional outputs and show that it is a natural extension of alignment in networks with 1-dimensional outputs as defined by Ji and Telgarsky, 2018.  While in fully connected networks, there always exists a global minimum corresponding to an aligned solution, we analyze alignment as it relates to the training process.  Namely, we characterize when alignment is an invariant of training under gradient descent by providing necessary and sufficient conditions for this invariant to hold. In such settings, the dynamics of gradient descent simplify, thereby allowing us to provide an explicit learning rate under which the network converges linearly to a global minimum.  We then analyze networks with layer constraints such as convolutional networks. In this setting, we prove that gradient descent is equivalent to projected gradient descent, and that alignment is impossible with sufficiently large datasets.  \n"}}
{"id": "rYt0p0Um9r", "cdate": 1601308232323, "mdate": null, "content": {"title": "Do Deeper Convolutional Networks Perform Better?", "abstract": "    Over-parameterization is a recent topic of much interest in the machine learning community.  While over-parameterized neural networks are capable of perfectly fitting (interpolating) training data, these networks often perform well on test data, thereby contradicting classical learning theory.  Recent work provided an explanation for this phenomenon by introducing the double descent curve, showing that increasing model capacity past the interpolation threshold leads to a decrease in test error.  In line with this, it was recently shown empirically and theoretically that increasing neural network capacity through width leads to double descent.  In this work, we analyze the effect of increasing depth on test performance.  In contrast to what is observed for increasing width, we demonstrate through a variety of classification experiments on CIFAR10 and ImageNet-32 using ResNets and fully-convolutional networks that test performance worsens beyond a critical depth.  We posit an explanation for this phenomenon by drawing intuition from the principle of minimum norm solutions in linear networks.  \n"}}
