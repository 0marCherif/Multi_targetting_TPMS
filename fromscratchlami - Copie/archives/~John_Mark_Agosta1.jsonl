{"id": "IAd423ETwS1", "cdate": 1600196349304, "mdate": null, "content": {"title": "Mixture Models of Endhost Network Traffic  ( IEEE INFOCOM 2013, Torino, IT. April 2013. )", "abstract": "In this work we focus on modeling a little studied\ntype of traffic, namely the network traffic generated from\nendhosts. We introduce a parsimonious parametric model of\nthe marginal distribution for connection arrivals. We employ\nmixture models based on a convex combination of component\ndistributions with both heavy and light-tails. These models can be\nfitted with high accuracy using maximum likelihood techniques.\nOur methodology assumes that the underlying user data can be\nfitted to one of many modeling options, and we apply Bayesian\nmodel selection criteria as a rigorous way to choose the preferred\ncombination of components. Our experiments show that a simple\nPareto-exponential mixture model is preferred for a wide range\nof users, over both simpler and more complex alternatives.\nThis model has the desirable property of modeling the entire\ndistribution, effectively segmenting the traffic into the heavy \ntailed as well as the non-heavy-tailed components. We illustrate\nthat this technique has the flexibility to capture the wide diversity \nof user behaviors.\n"}}
{"id": "r1ZwRwbO-S", "cdate": 1293840000000, "mdate": null, "content": {"title": "Automated Refinement of Bayes Networks' Parameters based on Test Ordering Constraints", "abstract": "In this paper, we derive a method to refine a Bayes network diagnostic model by exploiting constraints implied by expert decisions on test ordering. At each step, the expert executes an evidence gathering test, which suggests the test's relative diagnostic value. We demonstrate that consistency with an expert's test selection leads to non-convex constraints on the model parameters. We incorporate these constraints by augmenting the network with nodes that represent the constraint likelihoods. Gibbs sampling, stochastic hill climbing and greedy search algorithms are proposed to find a MAP estimate that takes into account test ordering constraints and any data available. We demonstrate our approach on diagnostic sessions from a manufacturing scenario."}}
{"id": "HJgaLGZdZS", "cdate": 1262304000000, "mdate": null, "content": {"title": "What is disputed on the web?", "abstract": "We present a method for automatically acquiring of a corpus of disputed claims from the web. We consider a factual claim to be disputed if a page on the web suggests both that the claim is false and also that other people say it is true. Our tool extracts disputed claims by searching the web for patterns such as \"falsely claimed that X\" and then using a statistical classifier to select text that appears to be making a disputed claim. We argue that such a corpus of disputed claims is useful for a wide range of applications related to information credibility on the web, and we report what our current corpus reveals about what is being disputed on the web."}}
{"id": "By7tP-b_WH", "cdate": 1262304000000, "mdate": null, "content": {"title": "Highlighting disputed claims on the web", "abstract": "We describe Dispute Finder, a browser extension that alerts a user when information they read online is disputed by a source that they might trust. Dispute Finder examines the text on the page that the user is browsing and highlights any phrases that resemble known disputed claims. If a user clicks on a highlighted phrase then Dispute Finder shows them a list of articles that support other points of view. Dispute Finder builds a database of known disputed claims by crawling web sites that already maintain lists of disputed claims, and by allowing users to enter claims that they believe are disputed. Dispute Finder identifies snippets that make known disputed claims by running a simple textual entailment algorithm inside the browser extension, referring to a cached local copy of the claim database. In this paper, we explain the design of Dispute Finder, and the trade-offs between the various design decisions that we explored."}}
{"id": "B1V2ynZ_bB", "cdate": 1230768000000, "mdate": null, "content": {"title": "Workshop summary: Seventh annual workshop on Bayes applications", "abstract": "No abstract available."}}
{"id": "rkbWk1bO-r", "cdate": 1136073600000, "mdate": null, "content": {"title": "When Gossip is Good: Distributed Probabilistic Inference for Detection of Slow Network Intrusions", "abstract": "Intrusion attempts due to self-propagating code are becoming an increasingly urgent problem, in part due to the homogeneous makeup of the internet. Recent advances in anomaly-based intrusion detection systems (IDSs) have made use of the quickly spreading nature of these attacks to identify them with high sensitivity and at low false positive (FP) rates. However, slowly propagating attacks are much more difficult to detect because they are cloaked under the veil of normal network traffic, yet can be just as dangerous due to their exponential spread pattern. We extend the idea of using collaborative IDSs to corroborate the likelihood of attack by imbuing end hosts with probabilistic graphical models and using random messaging to gossip state among peer detectors. We show that such a system is able to boost a weak anomaly detector D to detect an order-of-magnitude slower worm, at false positive rates less than a few per week, than would be possible using D alone at the end-host or on a network aggregation point. We show that this general architecture is scalable in the sense that a fixed absolute false positive rate can be achieved as the network size grows, spreads communication bandwidth uniformly throughout the network, and makes use of the increased computation power of a distributed system. We argue that using probabilistic models provides more robust detections than previous collaborative counting schemes and allows the system to account for heterogeneous detectors in a principled fashion."}}
