{"id": "jgLZ75dw6V", "cdate": 1672531200000, "mdate": 1693826387429, "content": {"title": "Discovering Generalizable Multi-agent Coordination Skills from Multi-task Offline Data", "abstract": ""}}
{"id": "aN0rlSINMn", "cdate": 1672531200000, "mdate": 1695976442441, "content": {"title": "Learning Generalizable Batch Active Learning Strategies via Deep Q-networks (Student Abstract)", "abstract": "To handle a large amount of unlabeled data, batch active learning (BAL) queries humans for the labels of a batch of the most valuable data points at every round. Most current BAL strategies are based on human-designed heuristics, such as uncertainty sampling or mutual information maximization. However, there exists a disagreement between these heuristics and the ultimate goal of BAL, i.e., optimizing the model's final performance within the query budgets. This disagreement leads to a limited generality of these heuristics. To this end, we formulate BAL as an MDP and propose a data-driven approach based on deep reinforcement learning. Our method learns the BAL strategy by maximizing the model's final performance. Experiments on the UCI benchmark show that our method can achieve competitive performance compared to existing heuristics-based approaches."}}
{"id": "Ehzo36ifsE", "cdate": 1672531200000, "mdate": 1693826387344, "content": {"title": "Policy Regularization with Dataset Constraint for Offline Reinforcement Learning", "abstract": "We consider the problem of learning the best possible policy from a fixed dataset, known as offline Reinforcement Learning (RL). A common taxonomy of existing offline RL works is policy regularizat..."}}
{"id": "53FyUAdP7d", "cdate": 1663850263011, "mdate": null, "content": {"title": "Discovering Generalizable Multi-agent Coordination Skills from Multi-task Offline Data", "abstract": "Cooperative multi-agent reinforcement learning (MARL) faces the challenge of adapting to multiple tasks with varying agents and targets. Previous multi-task MARL approaches require costly interactions to simultaneously learn or fine-tune policies in different tasks. However, the situation that an agent should generalize to multiple tasks with only offline data from limited tasks is more in line with the needs of real-world applications. Since offline multi-task data contains a variety of behaviors, an effective data-driven approach is to extract informative latent variables that can represent universal skills for realizing coordination across tasks. In this paper, we propose a novel Offline MARL algorithm to Discover coordInation Skills (ODIS) from multi-task data. ODIS first extracts task-invariant coordination skills from offline multi-task data and learns to delineate different agent behaviors with the discovered coordination skills. Then we train a coordination policy to choose optimal coordination skills with the centralized training and decentralized execution paradigm. We further demonstrate that the discovered coordination skills can assign effective coordinative behaviors, thus significantly enhancing generalization to unseen tasks. Empirical results in cooperative MARL benchmarks, including the StarCraft multi-agent challenge, show that ODIS obtains superior performance in a wide range of tasks only with offline data from limited sources."}}
