{"id": "GGpDOveXWca", "cdate": 1685577600000, "mdate": 1687863525902, "content": {"title": "Sparsing and Smoothing for the seq2seq Models", "abstract": "Current neural language models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over the target. While this setup provides solid results in several natural language processing (NLP) tasks, one unsatisfying aspect is its dense output. This density is wasteful, making models hard to interpret and assigning probability mass to many implausible outputs. To overcome this problem, we propose T-softmax, a simple but effective method to draw considerably sparse probability out of neural language models than softmax. Our method avoids dense output by truncating the unreliable tail of the probability distribution to improve the model's performance. In addition, we generalize logits with temperature, a critical regularization technique, from the softmax to T-softmax. To show our approach as a drop-in replacement for softmax, we evaluate them on three NLP tasks: summary generation, question answer, and math word problem. Experimental results show that our proposed model significantly improves performance without sacrificing speed; notably, in all experiments, our method outperforms the softmax."}}
{"id": "Wm-K_9NKIE", "cdate": 1672531200000, "mdate": 1687863525909, "content": {"title": "Sparse summary generation", "abstract": "The state-of-the-art summary generators build on powerful language models, such as BERT, which achieves impressive performance. However, most models employ softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful since it assigns probability mass to many implausible outputs. In this paper, we propose a sparse summary generation model with a new gp-entmax transformation, which includes 1.5-entmax and gradient penalty. The 1.5-entmax has the great effect of filtering noise, retaining important information and improving model performance. Experimental results show that the generated summary has improved in both ROUGE and BLEU metrics, and when tested on the CSL summarization dataset, our method outperforms the softmax model by more than 3 ROUGE-L points. For the purpose of measuring the level of important information in model-generated summaries, we propose a new metric called M2I. Simulation tests on human evaluation showed that the summary generated by the sparse model is more fluent and closer to the text\u2019s main idea."}}
{"id": "ucTUsM0Y3A", "cdate": 1640995200000, "mdate": 1687863525901, "content": {"title": "Augment BERT with average pooling layer for Chinese summary generation", "abstract": "The BERT pre-trained language model has achieved good results in various subtasks of natural language processing, but its performance in generating Chinese summaries is not ideal. The most intuitive reason is that the BERT model is based on character-level composition, while the Chinese language is mostly in the form of phrases. Directly fine-tuning the BERT model cannot achieve the expected effect. This paper proposes a novel summary generation model with BERT augmented by the pooling layer. In our model, we perform an average pooling operation on token embedding to improve the model\u2019s ability to capture phrase-level semantic information. We use LCSTS and NLPCC2017 to verify our proposed method. Experimental data shows that the average pooling model\u2019s introduction can effectively improve the generated summary quality. Furthermore, different data needs to be set with varying pooling kernel sizes to achieve the best results through comparative analysis. In addition, our proposed method has strong generalizability. It can be applied not only to the task of generating summaries, but also to other natural language processing tasks."}}
{"id": "53FYdOM2su", "cdate": 1640995200000, "mdate": 1687863525914, "content": {"title": "AP-BERT: enhanced pre-trained model through average pooling", "abstract": "BERT, a pre-trained language model on the large-scale corpus, has made breakthrough progress in NLP tasks. However, the experimental data shows that the BERT model\u2019s application effect in Chinese tasks is not ideal. The reason is that we believe that only character-level embedding can be obtained through BERT. However, a single Chinese character often cannot express their comprehensive meaning. To improve the model\u2019s ability to understand phrase-level semantic information, this paper proposes an enhanced BERT based on the average pooling(AP-BERT). Our model uses an average pooling layer to act on token embedding and reconstructs the model\u2019s input embedding, which can effectively improve BERT\u2019s application effect in Chinese natural language processing. Experimental data show that our proposed method has been enhanced in the four tasks of Chinese text classification, named entity recognition, reading comprehension, and summary generation. This method can not only improve the application effect of the BERT model in Chinese tasks but also can be well applied to other pre-trained language models."}}
