{"id": "qQz1UKDCiy7", "cdate": 1663849845122, "mdate": null, "content": {"title": "Momentum in Momentum for Adaptive Optimization", "abstract": "Adaptive gradient methods, e.g., Adam, have achieved tremendous success in machine learning. Employing adaptive learning rates according to the gradients, such methods are able to attain rapid training of modern deep neural networks. Nevertheless, they are observed to suffer from compromised generalization capacity compared with stochastic gradient descent (SGD) and tend to be trapped in local minima at an early stage during the training process. Intriguingly, we discover that the issue can be resolved by substituting the gradient in the second raw moment estimate term with its momentumized version in Adam. The intuition is that the gradient with momentum contains more accurate directional information, and therefore its second moment estimation is a more preferable option for learning rate scaling than that of the raw gradient. Thereby we propose AdaM$^3$ as a new optimizer reaching the goal of training quickly while generalizing much better. We further develop a theory to back up the improvement in generalization and provide novel convergence guarantees for our designed optimizer. Extensive experiments on a variety of tasks and models demonstrate that AdaM$^3$ exhibits state-of-the-art performance and superior training stability consistently. Considering the simplicity and effectiveness of AdaM$^3$, we believe it has the potential to become a new standard method in deep learning. Code will be publicly available."}}
{"id": "fJ924S1j5xh", "cdate": 1652737606166, "mdate": null, "content": {"title": "Syndicated Bandits: A Framework for Auto Tuning Hyper-parameters in Contextual Bandit Algorithms", "abstract": "The stochastic contextual bandit problem, which models the trade-off between exploration and exploitation, has many real applications, including recommender systems, online advertising and clinical trials. As many other machine learning algorithms, contextual bandit algorithms often have one or more hyper-parameters. As an example, in most optimal stochastic contextual bandit algorithms, there is an unknown exploration parameter which controls the trade-off between exploration and exploitation. A proper choice of the hyper-parameters is essential for contextual bandit algorithms to perform well. However, it is infeasible to use offline tuning methods to select hyper-parameters in contextual bandit environment since there is no pre-collected dataset and the decisions have to be made in real time. To tackle this problem, we first propose a two-layer bandit structure for auto tuning the exploration parameter and further generalize it to the Syndicated Bandits framework which can learn multiple hyper-parameters dynamically in contextual bandit environment. We derive the regret bounds of our proposed Syndicated Bandits framework and show it can avoid its regret dependent exponentially in the number of hyper-parameters to be tuned. Moreover, it achieves optimal regret bounds under certain scenarios. Syndicated Bandits framework is general enough to handle the tuning tasks in many popular contextual bandit algorithms, such as LinUCB, LinTS, UCB-GLM, etc. Experiments on both synthetic and real datasets validate the effectiveness of our proposed framework."}}
{"id": "6V4vRCbVA3J", "cdate": 1652737589936, "mdate": null, "content": {"title": "Efficient Frameworks for Generalized Low-Rank Matrix Bandit Problems", "abstract": "In the stochastic contextual low-rank matrix bandit problem, the expected reward of an action is given by the inner product between the action's feature matrix and some fixed, but initially unknown $d_1$ by $d_2$ matrix $\\Theta^*$ with rank $r \\ll \\{d_1, d_2\\}$, and an agent sequentially takes actions based on past experience to maximize the cumulative reward. In this paper, we study the generalized low-rank matrix bandit problem, which has been recently proposed in \\cite{lu2021low} under the Generalized Linear Model (GLM) framework. To overcome the computational infeasibility and theoretical restrain of existing algorithms on this problem, we first propose the G-ESTT framework that modifies the idea from \\cite{jun2019bilinear} by using Stein's method on the subspace estimation and then leverage the estimated subspaces via a regularization idea. Furthermore, we remarkably improve the efficiency of G-ESTT by using a novel exclusion idea on the estimated subspace instead, and propose the G-ESTS framework. We also show that both of our methods are the first algorithm to achieve the optimal $\\tilde{O}((d_1+d_2)r\\sqrt{T})$ bound of regret presented in \\cite{lu2021low} up to logarithm terms under some mild conditions, which improves upon the current regret of $\\tilde{O}((d_1+d_2)^{3/2} \\sqrt{rT})$~\\citep{lu2021low}. For completeness, we conduct experiments to illustrate that our proposed algorithms, especially G-ESTS, are also computationally tractable and consistently outperform other state-of-the-art (generalized) linear matrix bandit methods based on a suite of simulations."}}
{"id": "R6hvtDTQmb", "cdate": 1632875444971, "mdate": null, "content": {"title": "Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization", "abstract": "Adaptive gradient methods, such as Adam, have achieved tremendous success in machine learning. Scaling gradients by square roots of the running averages of squared past gradients, such methods are able to attain rapid training of modern deep neural networks. Nevertheless, they are observed to generalize worse than stochastic gradient descent (SGD) and tend to be trapped in local minima at an early stage during training. Intriguingly, we discover that substituting the gradient in the second moment estimation term with the momentumized version in Adam can well solve the issues. The intuition is that gradient with momentum contains more accurate directional information and therefore its second moment estimation is a better choice for scaling than that of the raw gradient. Thereby we propose AdaMomentum as a new optimizer reaching the goal of training fast while generalizing better. We further develop a theory to back up the improvement in optimization and generalization and provide convergence guarantees under both convex and nonconvex settings. Extensive experiments on a wide range of tasks and models demonstrate that AdaMomentum exhibits state-of-the-art performance consistently. The source code is available at https://anonymous.4open.science/r/AdaMomentum_experiments-6D9B."}}
{"id": "gm6KP9HqpMu", "cdate": 1577836800000, "mdate": 1681665991559, "content": {"title": "Estimation of the Number of Endmembers via Thresholding Ridge Ratio Criterion", "abstract": "Endmember is defined as the spectral signature of pure material present in hyperspectral imagery. Estimation of the number of endmembers (NOE) present in a scene is an important preprocessing step and plays a crucial role in hyperspectral image processing, since over- or under-estimation of the NOE will lead to heavily incorrect results. In this article, we develop a thresholding ridge ratio (TRR) criterion based on eigendecomposition for NOE determination. Different from the widely used eigenvalue difference analysis methods, the TRR seeks an adaptive thresholding operation to the ridge ratio of eigenvalue differences, and ridge ratio combined with adaptive thresholding can theoretically guarantee a consistent estimate even when there are several local minima. Based on the TRR criterion, an algorithm is introduced to perform the estimation of NOE. Experimental results on both the simulated and real hyperspectral data sets have demonstrated that the proposed TRR-based algorithm has comparable and even better performances to several benchmark algorithms in the estimation accuracy of the NOE."}}
