{"id": "vxyHAA1rntR", "cdate": 1672531200000, "mdate": 1681803631105, "content": {"title": "Proximal boosting: Aggregating weak learners to minimize non-differentiable losses", "abstract": ""}}
{"id": "M6EhEd-JFu", "cdate": 1668589148842, "mdate": 1668589148842, "content": {"title": "Some Theoretical Insights into Wasserstein GANs", "abstract": "Generative Adversarial Networks (GANs) have been successful in producing outstanding results in areas as diverse as image, video, and text generation. Building on these successes, a large number of empirical studies have validated the benefits of the cousin approach called Wasserstein GANs (WGANs), which brings stabilization in the training process. In the present paper, we add a new stone to the edifice by proposing some theoretical advances in the properties of WGANs. First, we properly define the architecture of WGANs in the context of integral probability metrics parameterized by neural networks and highlight some of their basic mathematical features. We stress in particular interesting optimization properties arising from the use of a parametric 1-Lipschitz discriminator. Then, in a statistically-driven approach, we study the convergence of empirical WGANs as the sample size tends to infinity, and clarify the adversarial effects of the generator and the discriminator by underlining some trade-off properties. These features are finally illustrated with experiments using both synthetic and real-world datasets."}}
{"id": "5z4KtiuThr", "cdate": 1622458010956, "mdate": null, "content": {"title": "Infinite Task Learning in RKHSs", "abstract": "Machine learning has witnessed tremendous success in solving tasks depending on a single hyperparameter. When considering simultaneously a finite number of tasks, multitask learning enables one to account for the similarities of the tasks via appropriate regularizers. A step further consists of learning a continuum of tasks for various loss functions. A promising approach, called Parametric Task Learning, has paved the way in the continuum setting for affine models and piecewise-linear loss functions. In this\nwork, we introduce a novel approach called Infinite Task Learning: its goal is to learn a function whose output is a function over\nthe hyperparameter space. We leverage tools from operator-valued kernels and the associated Vector-Valued Reproducing Kernel Hilbert Space that provide an explicit control over the role of the hyperparameters, and also allows us to consider new type of constraints. We provide generalization guarantees to the suggested scheme and illustrate its efficiency in cost-sensitive classification, quantile regression and density level set estimation."}}
{"id": "sILmQjfc26", "cdate": 1546300800000, "mdate": 1669224088570, "content": {"title": "Infinite Task Learning in RKHSs", "abstract": "Machine learning has witnessed tremendous success in solving tasks depending on a single hyperparameter. When considering simultaneously a finite number of tasks, multi-task learning enables one to..."}}
{"id": "p1paqlfUers", "cdate": 1514764800000, "mdate": null, "content": {"title": "Output Fisher embedding regression", "abstract": "We investigate the use of Fisher vector representations in the output space in the context of structured and multiple output prediction. A novel, general and versatile method called output Fisher embedding regression is introduced. Based on a probabilistic modeling of training output data and the minimization of a Fisher loss, it requires to solve a pre-image problem in the prediction phase. For Gaussian Mixture Models and State-Space Models, we show that the pre-image problem enjoys a closed-form solution with an appropriate choice of the embedding. Numerical experiments on a wide variety of tasks (time series prediction, multi-output regression and multi-class classification) highlight the relevance of the approach for learning under limited supervision like learning with a handful of data per label and weakly supervised learning."}}
{"id": "M8zaIZJHkBw", "cdate": 1514764800000, "mdate": null, "content": {"title": "Infinite-Task Learning with Vector-Valued RKHSs", "abstract": "Machine learning has witnessed tremendous success in solving tasks depending on a single hyperparameter. When considering simultaneously a finite number of tasks, multi-task learning enables one to account for the similarities of the tasks via appropriate regularizers. A step further consists of learning a continuum of tasks for various loss functions. A promising approach, called \\emph{Parametric Task Learning}, has paved the way in the continuum setting for affine models and piecewise-linear loss functions. In this work, we introduce a novel approach called \\emph{Infinite Task Learning} whose goal is to learn a function whose output is a function over the hyperparameter space. We leverage tools from operator-valued kernels and the associated vector-valued RKHSs that provide an explicit control over the role of the hyperparameters, and also allows us to consider new type of constraints. We provide generalization guarantees to the suggested scheme and illustrate its efficiency in cost-sensitive classification, quantile regression and density level set estimation."}}
{"id": "pjwZssBuFcn", "cdate": 1483228800000, "mdate": null, "content": {"title": "Data sparse nonparametric regression with \u03b5-insensitive losses", "abstract": "Leveraging the celebrated support vector regression (SVR) method, we propose a unifying framework in order to deliver regression machines in reproducing kernel Hilbert spaces (RKHSs) with data spar..."}}
{"id": "SybN5obdWr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Early and Reliable Event Detection Using Proximity Space Representation", "abstract": "Let us consider a specific action or situation (called event) that takes place within a time series. The objective in early detection is to build a decision function that is able to go off as soon ..."}}
{"id": "BJNtRPW_ZB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Joint quantile regression in vector-valued RKHSs", "abstract": "Addressing the will to give a more complete picture than an average relationship provided by standard regression, a novel framework for estimating and predicting simultaneously several conditional quantiles is introduced. The proposed methodology leverages kernel-based multi-task learning to curb the embarrassing phenomenon of quantile crossing, with a one-step estimation procedure and no post-processing. Moreover, this framework comes along with theoretical guarantees and an efficient coordinate descent learning algorithm. Numerical experiments on benchmark and real datasets highlight the enhancements of our approach regarding the prediction error, the crossing occurrences and the training time."}}
{"id": "NtdGnF_FJpr", "cdate": 1420070400000, "mdate": 1681803631141, "content": {"title": "Filter bank learning for signal classification", "abstract": ""}}
