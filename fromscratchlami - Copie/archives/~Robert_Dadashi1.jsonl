{"id": "CmD5z_2DVuM", "cdate": 1652737382934, "mdate": null, "content": {"title": "Learning Energy Networks with Generalized Fenchel-Young Losses", "abstract": "Energy-based models, a.k.a. energy networks, perform inference by optimizing \nan energy function, typically parametrized by a neural network. \nThis allows one to capture potentially complex relationships between inputs and\noutputs.\nTo learn the parameters of the energy function, the solution to that\noptimization problem is typically fed into a loss function.\nThe key challenge for training energy networks lies in computing loss gradients,\nas this typically requires argmin/argmax differentiation.\nIn this paper, building upon a generalized notion of conjugate function,\nwhich replaces the usual bilinear pairing with a general energy function,\nwe propose generalized Fenchel-Young losses, a natural loss construction for\nlearning energy networks. Our losses enjoy many desirable properties and their\ngradients can be computed efficiently without argmin/argmax differentiation.\nWe also prove the calibration of their excess risk in the case of linear-concave\nenergies. We demonstrate our losses on multilabel classification and \nimitation learning tasks."}}
{"id": "zabVCGfJvbr", "cdate": 1634067441257, "mdate": null, "content": {"title": "Continuous Control with Action Quantization from Demonstrations", "abstract": "In Reinforcement Learning (RL), discrete actions, as opposed to continuous actions, result in less complex exploration problems and the immediate computation of the maximum of the action-value function which is central to dynamic programming-based methods. In this paper, we propose a novel method: Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces by leveraging the priors of  demonstrations. This dramatically reduces the exploration problem, since the actions faced by the agent not only are in a finite number but also are plausible in light of the demonstrator\u2019s behavior. By discretizing the action space we can apply any discrete action deep RL algorithm to the continuous control problem. We evaluate the proposed method on three different setups: RL with demonstrations, RL with play data --demonstrations of a human playing in an environment but not solving any specific task-- and Imitation Learning. For all three setups, we only consider human data, which is more challenging than synthetic data. We found that AQuaDem consistently outperforms state-of-the-art continuous control methods, both in terms of performance and sample efficiency."}}
{"id": "i2baoZMYZ3", "cdate": 1632875521932, "mdate": null, "content": {"title": "Continuous Control with Action Quantization from Demonstrations", "abstract": "In Reinforcement Learning (RL), discrete actions, as opposed to continuous actions, result in less complex exploration problem and the immediate derivation of the maximum of the action-value function which is central to dynamic programming-based methods. In this paper, we propose a novel method: Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces by leveraging the priors of  demonstrations. This dramatically reduces the exploration problem, since the actions faced by the agent not only are in a finite number but also are plausible in light of the demonstrator\u2019s behavior. By discretizing the action space we can apply any discrete action deep RL algorithm to the continuous control problem. We evaluate the proposed method on three different setups: RL with demonstrations, RL with play data --demonstrations of a human playing in an environment but not solving any specific task-- and Imitation Learning. For all three setups, we only consider human data, thus most challenging than synthetic data. We found that AQuaDem consistently outperforms state-of-the-art continuous control methods, both in terms of performance and sample efficiency."}}
{"id": "-OrwaD3bG91", "cdate": 1621630193054, "mdate": null, "content": {"title": "What Matters for Adversarial Imitation Learning?", "abstract": "Adversarial imitation learning has become a popular framework for imitation in continuous control. Over the years, several variations of its components were proposed to enhance the performance of the learned policies as well as the sample complexity of the algorithm. In practice, these choices are rarely tested all together in rigorous empirical studies.\nIt is therefore difficult to discuss and understand what choices, among the high-level algorithmic options as well as  low-level implementation details, matter. To tackle this issue, we implement more than 50 of these choices in a generic adversarial imitation learning framework\nand investigate their impacts in a large-scale study (>500k trained agents) with both synthetic and human-generated demonstrations. We analyze the key results and highlight the most surprising findings."}}
{"id": "uQzdlSVonNT", "cdate": 1620754887827, "mdate": null, "content": {"title": "The value-improvement path: Towards better representations for reinforcement learning", "abstract": "In value-based reinforcement learning (RL), unlike in supervised learning, the agent faces not a single, stationary, approximation problem, but a sequence of value prediction problems. Each time the policy improves, the nature of the problem changes, shifting both the distribution of states and their values. In this paper we take a novel perspective, arguing that the value prediction problems faced by an RL agent should not be addressed in isolation, but rather as a single, holistic, prediction problem. An RL algorithm generates a sequence of policies that, at least approximately, improve towards the optimal policy. We explicitly characterize the associated sequence of value functions and call it the value-improvement path. Our main idea is to approximate the value-improvement path holistically, rather than to solely track the value function of the current policy. Specifically, we discuss the impact that this holistic view of RL has on representation learning. We demonstrate that a representation that spans the past value-improvement path will also provide an accurate value approximation for future policy improvements. We use this insight to better understand existing approaches to auxiliary tasks and to propose new ones. To test our hypothesis empirically, we augmented a standard deep RL agent with an auxiliary task of learning the value-improvement path. In a study of Atari 2600 games, the augmented agent achieved approximately double the mean and median performance of the baseline agent.\n"}}
{"id": "TtYSU29zgR", "cdate": 1601308173483, "mdate": null, "content": {"title": "Primal Wasserstein Imitation Learning", "abstract": "Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance."}}
{"id": "ryZmisZOZr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Statistics and Samples in Distributional Reinforcement Learning", "abstract": "We present a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key ..."}}
{"id": "BJWRkj-ObS", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Value Function Polytope in Reinforcement Learning", "abstract": "We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of it..."}}
