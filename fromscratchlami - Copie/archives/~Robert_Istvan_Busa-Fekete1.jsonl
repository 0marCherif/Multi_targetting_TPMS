{"id": "gkp8mKO6Gu", "cdate": 1672531200000, "mdate": 1682341606923, "content": {"title": "Easy Learning from Label Proportions", "abstract": "We consider the problem of Learning from Label Proportions (LLP), a weakly supervised classification setup where instances are grouped into \"bags\", and only the frequency of class labels at each bag is available. Albeit, the objective of the learner is to achieve low task loss at an individual instance level. Here we propose Easyllp: a flexible and simple-to-implement debiasing approach based on aggregate labels, which operates on arbitrary loss functions. Our technique allows us to accurately estimate the expected loss of an arbitrary model at an individual level. We showcase the flexibility of our approach by applying it to popular learning frameworks, like Empirical Risk Minimization (ERM) and Stochastic Gradient Descent (SGD) with provable guarantees on instance level performance. More concretely, we exhibit a variance reduction technique that makes the quality of LLP learning deteriorate only by a factor of k (k being bag size) in both ERM and SGD setups, as compared to full supervision. Finally, we validate our theoretical results on multiple datasets demonstrating our algorithm performs as well or better than previous LLP approaches in spite of its simplicity."}}
{"id": "mamv07NQWk", "cdate": 1652737839719, "mdate": null, "content": {"title": "Regret Bounds for Multilabel Classification in Sparse Label Regimes", "abstract": "Multi-label classification (MLC) has wide practical importance, but the theoretical understanding of its statistical properties is still limited. As an attempt to fill this gap, we thoroughly study upper and lower regret bounds for two canonical MLC performance measures, Hamming loss and Precision@$\\kappa$. We consider two different statistical and algorithmic settings, a non-parametric setting tackled by plug-in classifiers \\`a la $k$-nearest neighbors, and a parametric one tackled by empirical risk minimization operating on surrogate loss functions. For both, we analyze the interplay between a natural MLC variant of the low noise assumption, widely studied in binary classification, and the label sparsity, the latter being a natural property of large-scale MLC problems. We show that those conditions are crucial in improving the bounds, but the way they are tangled is not obvious, and also different across the two settings."}}
{"id": "OZEmgSbRQW", "cdate": 1652737780518, "mdate": null, "content": {"title": "Private and Communication-Efficient Algorithms for Entropy Estimation", "abstract": "Modern statistical estimation is often performed in a distributed setting where each sample belongs to single user who shares their data with a central server. Users are typically concerned with preserving the privacy of their sample, and also with minimizing the amount of data they must transmit to the server. We give improved private and communication-efficient algorithms for estimating several popular measures of the entropy of a distribution. All of our algorithms have constant communication cost and satisfy local differential privacy. For a joint distribution on many variables whose conditional independence graph is a tree, we describe algorithms for estimating Shannon entropy that require a number of samples that is linear in the number of variables, compared to the quadratic sample complexity of prior work. We also describe an algorithm for estimating Gini entropy whose sample complexity has no dependence on the support size of the distribution and can be implemented using a single round of concurrent communication between the users and the server, while the previously best-known algorithm has high communication cost and requires the server to facilitate interaction between the users. Finally, we describe an algorithm for estimating collision entropy that matches the space and sample complexity of the best known algorithm but generalizes it to the private and communication-efficient setting."}}
{"id": "y6mwzauQDk5", "cdate": 1640995200000, "mdate": 1682341607047, "content": {"title": "On the Use of Ensemble X-Vector Embeddings for Improved Sleepiness Detection", "abstract": "The state-of-the-art in speaker recognition, called x-vectors, has been adopted in several computational paralinguistic tasks, as they were shown to extract embeddings that could be efficiently utilized as features in the subsequent classification or regression step. Nevertheless, similarly to all neural networks, x-vectors might also prove to be sensitive to several training meta-parameters such as the number of hidden layers and neurons, or the number of training epochs. In this study we experimentally demonstrate that the performance of x-vector embeddings is also affected by the random seed of the initial weight initialization step before training. We also show that, by training an ensemble learning method by repeating x-vector DNN training, we can make the utterance-level predictions more robust, leading to notable improvements in the performance on the test set. We perform our experiments on the publicly available Dusseldorf Sleepy Language Corpus, for estimating the degree of sleepiness. Improving upon our previous results, we present the highest Spearman\u2019s correlation coefficient on this dataset that was achieved by a single method."}}
{"id": "iavKjJChtMT", "cdate": 1640995200000, "mdate": 1683893325615, "content": {"title": "Private and Communication-Efficient Algorithms for Entropy Estimation", "abstract": "Modern statistical estimation is often performed in a distributed setting where each sample belongs to single user who shares their data with a central server. Users are typically concerned with preserving the privacy of their sample, and also with minimizing the amount of data they must transmit to the server. We give improved private and communication-efficient algorithms for estimating several popular measures of the entropy of a distribution. All of our algorithms have constant communication cost and satisfy local differential privacy. For a joint distribution on many variables whose conditional independence graph is a tree, we describe algorithms for estimating Shannon entropy that require a number of samples that is linear in the number of variables, compared to the quadratic sample complexity of prior work. We also describe an algorithm for estimating Gini entropy whose sample complexity has no dependence on the support size of the distribution and can be implemented using a single round of concurrent communication between the users and the server, while the previously best-known algorithm has high communication cost and requires the server to facilitate interaction between the users. Finally, we describe an algorithm for estimating collision entropy that matches the space and sample complexity of the best known algorithm but generalizes it to the private and communication-efficient setting."}}
{"id": "RNxc9qLrHCZ", "cdate": 1640995200000, "mdate": 1683893325662, "content": {"title": "Regret Bounds for Multilabel Classification in Sparse Label Regimes", "abstract": "Multi-label classification (MLC) has wide practical importance, but the theoretical understanding of its statistical properties is still limited. As an attempt to fill this gap, we thoroughly study upper and lower regret bounds for two canonical MLC performance measures, Hamming loss and Precision@$\\kappa$. We consider two different statistical and algorithmic settings, a non-parametric setting tackled by plug-in classifiers \\`a la $k$-nearest neighbors, and a parametric one tackled by empirical risk minimization operating on surrogate loss functions. For both, we analyze the interplay between a natural MLC variant of the low noise assumption, widely studied in binary classification, and the label sparsity, the latter being a natural property of large-scale MLC problems. We show that those conditions are crucial in improving the bounds, but the way they are tangled is not obvious, and also different across the two settings."}}
{"id": "I7uwvZeP4i", "cdate": 1640995200000, "mdate": 1682341606968, "content": {"title": "Statistical anonymity: Quantifying reidentification risks without reidentifying users", "abstract": "Data anonymization is an approach to privacy-preserving data release aimed at preventing participants reidentification, and it is an important alternative to differential privacy in applications that cannot tolerate noisy data. Existing algorithms for enforcing $k$-anonymity in the released data assume that the curator performing the anonymization has complete access to the original data. Reasons for limiting this access range from undesirability to complete infeasibility. This paper explores ideas -- objectives, metrics, protocols, and extensions -- for reducing the trust that must be placed in the curator, while still maintaining a statistical notion of $k$-anonymity. We suggest trust (amount of information provided to the curator) and privacy (anonymity of the participants) as the primary objectives of such a framework. We describe a class of protocols aimed at achieving these goals, proposing new metrics of privacy in the process, and proving related bounds. We conclude by discussing a natural extension of this work that completely removes the need for a central curator."}}
{"id": "2sWidqliCDG", "cdate": 1634343930088, "mdate": null, "content": {"title": "On the Pitfalls of Label Differential Privacy", "abstract": "We study the privacy limitations of label differential privacy, which has emerged as an intermediate trust model between local and central differential privacy, where only the label of each training example is protected (and the features are assumed to be public). We show that the guarantees provided by label DP are significantly weaker than they appear, as an adversary can \"un-noise\" the perturbed labels. Formally we show that the privacy loss has a close connection with Jeffreys' divergence of the conditional distribution between positive and negative labels, which allows explicit formulation of the trade-off between utility and privacy in this setting. Our results suggest how to select public features that optimize this trade-off. But we still show that there is no free lunch---instances where label differential privacy guarantees are strong are exactly those where a good classifier does not exist. We complement the negative results with a non-parametric estimator for the true privacy loss, and apply our techniques on large-scale benchmark data to demonstrate how to\nachieve a desired privacy protection. "}}
{"id": "Gf2EuAB9Xj", "cdate": 1631818982170, "mdate": null, "content": {"title": "Population Level Privacy Leakage in Binary Classification wtih Label Noise", "abstract": "We study the privacy limitations of label differential privacy. Label differential privacy has emerged as an intermediate trust model between local and central differential privacy, where only the label of each training example is protected (and the features are assumed to be public). We show that the guarantees provided by label DP are significantly weaker than they appear, as an adversary can \"un-noise\" the perturbed labels. Formally we show that the privacy loss has a close connection with Jeffreys' divergence of the conditional distribution between positive and negative labels, which allows explicit formulation of the trade-off between utility and privacy in this setting. Our results suggest how to select public features that optimize this trade-off. But we still show that there is no free lunch --- instances where label differential privacy guarantees are strong are exactly those where a good classifier does not exist. We complement the negative results with a non-parametric estimator for the true privacy loss, and apply our techniques on large-scale benchmark data to demonstrate how to\nachieve a desired privacy protection. \n"}}
{"id": "lMrwT4C93eT", "cdate": 1621630050395, "mdate": null, "content": {"title": "Private and Non-private Uniformity Testing for Ranking Data", "abstract": "We study the problem of uniformity testing for statistical data that consists of rankings over $m$ items where the alternative class is restricted to Mallows models with single parameter. Testing ranking data is challenging because of the size of the large domain that is factorial in $m$, therefore the tester needs to take advantage of some structure of the alternative class. We show that uniform distribution can be distinguished from Mallows model with $O(m^{-1/2})$ samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if $m$ is large enough. We also consider uniformity testing with central and locally differential private (DP) constraints. We present a central DP algorithm that requires $O\\left(\\max \\{ 1/\\epsilon_0, 1/\\sqrt{m} \\} \\right)$ where $\\epsilon_0$ is the privacy budget parameter. Interestingly, our uniformity testing algorithm is  straightforward to apply in the local DP scenario by its nature, since it works with binary statistics that is extracted from the ranking data. We carry out large-scale experiments, including $m=10000$, to show that these testing algorithms scales very gracefully with the number of items."}}
