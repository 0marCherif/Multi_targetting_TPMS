{"id": "GUSf17i8RMZ", "cdate": 1663850051764, "mdate": null, "content": {"title": "CircuitNet: A Generic Neural Network to Realize Universal Circuit Motif Modeling", "abstract": "The successes of artificial neural networks (ANNs) are largely attributed to mimicking the human brain structures. Recent advances in neuroscience revealed that neurons interact with each other through various kinds of connectivity patterns to process information, in which the common connectivity patterns are also called circuit motifs. However, many existing ANNs can only model one or two circuit motifs in their architectures, so that their performance may drastically vary among different types of machine learning tasks. \nIn this paper, we propose a new type of neural network inspired by the architectures of neuronal circuits, namely Circuit Neural Network (CircuitNet). In CircuitNet, a group of densely connected neurons, namely circuit motif unit (CMU), form the basic unit of the network, which is capable of modeling universal circuit motifs by adjusting the weights within the CMUs. Compared with traditional feed-forward networks, CircuitNet has the ability to model more types of neuron connections such as feed-back and lateral motifs.\nInspired by the locally dense and globally sparse structure of the human brain, several iterations of signal transmission among different CMUs are achieved by sparse connections through the input ports and output ports of different CMUs. \nExperiments have demonstrated that CircuitNet can outperform popular neural network architectures in function approximation, reinforcement learning, image classification, and time series forecasting tasks."}}
{"id": "c6GYV-EVSZX", "cdate": 1640995200000, "mdate": 1668739492320, "content": {"title": "RendNet: Unified 2D/3D Recognizer with Latent Space Rendering", "abstract": "Vector graphics (VG) have been ubiquitous in our daily life with vast applications in engineering, architecture, designs, etc. The VG recognition process of most existing methods is to first render the VG into raster graphics (RG) and then conduct recognition based on RG formats. However, this procedure discards the structure of geometries and loses the high resolution of VG. Recently, another category of algorithms is proposed to recognize directly from the original VG format. But it is affected by the topological errors that can be filtered out by RG rendering. Instead of looking at one format, it is a good solution to utilize the formats of VG and RG together to avoid these shortcomings. Besides, we argue that the VG-to-RG rendering process is essential to effectively combine VG and RG information. By specifying the rules on how to transfer VG primitives to RG pixels, the rendering process depicts the interaction and correlation between VG and RG. As a result, we propose RendNet, a unified architecture for recognition on both 2D and 3D scenarios, which considers both VG/RG representations and exploits their interaction by incorporating the VG-to-RG rasterization process. Experiments show that Rend-Net can achieve state-of-the-art performance on 2D and 3D object recognition tasks on various VG datasets."}}
{"id": "yWenMRY-3Nh", "cdate": 1609459200000, "mdate": 1668739492313, "content": {"title": "Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling", "abstract": "As an important task in multimodal context understanding, Text-VQA (Visual Question Answering) aims at question answering through reading text information in images. It differentiates from the original VQA task as Text-VQA requires large amounts of scene-text relationship understanding, in addition to the cross-modal grounding capability. In this paper, we propose Localize, Group, and Select (LOGOS), a novel model which attempts to tackle this problem from multiple aspects. LOGOS leverages two grounding tasks to better localize the key information of the image, utilizes scene text clustering to group individual OCR tokens, and learns to select the best answer from different sources of OCR (Optical Character Recognition) texts. Experiments show that LOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks without using additional OCR annotation data. Ablation studies and analysis demonstrate the capability of LOGOS to bridge different modalities and better understand scene text."}}
{"id": "xu0eFZM04W", "cdate": 1577836800000, "mdate": 1668739492317, "content": {"title": "Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction", "abstract": ""}}
{"id": "ifRPiDWXsi", "cdate": 1577836800000, "mdate": 1668739492305, "content": {"title": "Agent-Based Dynamic Collaboration Support in a Smart Office Space", "abstract": ""}}
{"id": "LYmGK9_V9f", "cdate": 1577836800000, "mdate": 1668739492316, "content": {"title": "Phans, Stans and Cishets: Self-Presentation Effects on Content Propagation in Tumblr", "abstract": "Research on content propagation in social media has largely focused on features from the content of posts and the network structure of users. However, social media platforms are also spaces where users present their identities in particular ways. How do the ways users present themselves affect how content they produce is propagated? In this paper, we address this question with an empirical study of interaction and self-presentation data from Tumblr. We use a pairwise learning-to-rank framework to predict whether a given user will reblog (share) another user\u2019s post from features comparing self-presented textual and visual identity information. We find evidence that alignment in identity presentation is associated with content propagation, as these features increase performance over a baseline of content features. Interpreting learned feature weights on self-presented text identity labels, we find that users who present labels that match or indicate shared interests and values are generally more likely to propagate each other\u2019s content."}}
{"id": "n1zHMGXKh7-", "cdate": 1546300800000, "mdate": 1668739492305, "content": {"title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge", "abstract": "Generating a reasonable ending for a given story context, i.e., story ending generation, is a strong indication of story comprehension. This task requires not only to understand the context clues which play an important role in planning the plot, but also to handle implicit knowledge to make a reasonable, coherent story. In this paper, we devise a novel model for story ending generation. The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings. Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines1."}}
{"id": "SoWZIMGluaH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge.", "abstract": "Generating a reasonable ending for a given story context, i.e., story ending generation, is a strong indication of story comprehension. This task requires not only to understand the context clues which play an important role in planning the plot, but also to handle implicit knowledge to make a reasonable, coherent story. In this paper, we devise a novel model for story ending generation. The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings. Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines1."}}
{"id": "HixlNsR-e_TS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors.", "abstract": "Humans convey their intentions through the usage of both verbal and nonverbal behaviors during face-to-face communication. Speaker intentions often vary dynamically depending on different nonverbal contexts, such as vocal patterns and facial expressions. As a result, when modeling human language, it is essential to not only consider the literal meaning of the words but also the nonverbal contexts in which these words appear. To better model human language, we first model expressive nonverbal representations by analyzing the fine-grained visual and acoustic patterns that occur during word segments. In addition, we seek to capture the dynamic nature of nonverbal intents by shifting word representations based on the accompanying nonverbal behaviors. To this end, we propose the Recurrent Attended Variation Embedding Network (RAVEN) that models the fine-grained structure of nonverbal subword sequences and dynamically shifts word representations based on nonverbal cues. Our proposed model achieves competitive performance on two publicly available datasets for multimodal sentiment analysis and emotion recognition. We also visualize the shifted word representations in different nonverbal contexts and summarize common patterns regarding multimodal variations of word representations."}}
{"id": "BJNAJ6x_ZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders", "abstract": "Asking good questions in large-scale, open-domain conversational systems is quite significant yet rather untouched. This task, substantially different from traditional question generation, requires to question not only with various patterns but also on diverse and relevant topics. We observe that a good question is a natural composition of {\\it interrogatives}, {\\it topic words}, and {\\it ordinary words}. Interrogatives lexicalize the pattern of questioning, topic words address the key information for topic transition in dialogue, and ordinary words play syntactical and grammatical roles in making a natural sentence. We devise two typed decoders (\\textit{soft typed decoder} and \\textit{hard typed decoder}) in which a type distribution over the three types is estimated and used to modulate the final generation distribution. Extensive experiments show that the typed decoders outperform state-of-the-art baselines and can generate more meaningful questions."}}
