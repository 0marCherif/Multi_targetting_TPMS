{"id": "AKLovigmZ_F", "cdate": 1675209600000, "mdate": 1682385330238, "content": {"title": "Activation to Saliency: Forming High-Quality Labels for Unsupervised Salient Object Detection", "abstract": "This paper focuses on the Unsupervised Salient Object Detection (USOD) issue. We come up with a two-stage Activation-to-Saliency (A2S) framework that effectively excavates saliency cues to train a robust saliency detector. It is worth noting that our method does not require any manual annotation in the whole process. In the first stage, we transform an unsupervisedly pre-trained network to aggregate multi-level features into a single activation map, where an Adaptive Decision Boundary (ADB) is proposed to assist the training of the transformed network. Moreover, a new loss function is proposed to facilitate the generation of high-quality pseudo labels. In the second stage, a self-rectification learning strategy is developed to train a saliency detector and refine the pseudo labels online. In addition, we construct a lightweight saliency detector using two Residual Attention Modules (RAMs) to learn robust saliency information. Extensive experiments on several SOD benchmarks prove that our framework reports significant performance compared with existing USOD methods. Moreover, training our framework on 3,000 images consumes about 1 hour, which is over 10 times faster than previous state-of-the-art methods. Code has been published at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/moothes/A2S-USOD</uri> ."}}
{"id": "_kBDiGckU8a", "cdate": 1672531200000, "mdate": 1682385330256, "content": {"title": "Adversarial Attack and Defense for Medical Image Analysis: Methods and Applications", "abstract": "Deep learning techniques have achieved superior performance in computer-aided medical image analysis, yet they are still vulnerable to imperceptible adversarial attacks, resulting in potential misdiagnosis in clinical practice. Oppositely, recent years have also witnessed remarkable progress in defense against these tailored adversarial examples in deep medical diagnosis systems. In this exposition, we present a comprehensive survey on recent advances in adversarial attack and defense for medical image analysis with a novel taxonomy in terms of the application scenario. We also provide a unified theoretical framework for different types of adversarial attack and defense methods for medical image analysis. For a fair comparison, we establish a new benchmark for adversarially robust medical diagnosis models obtained by adversarial training under various scenarios. To the best of our knowledge, this is the first survey paper that provides a thorough evaluation of adversarially robust medical diagnosis models. By analyzing qualitative and quantitative results, we conclude this survey with a detailed discussion of current challenges for adversarial attack and defense in medical image analysis systems to shed light on future research directions."}}
{"id": "QFWTnOlGpQx", "cdate": 1672531200000, "mdate": 1682385330422, "content": {"title": "CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution", "abstract": "Medical image arbitrary-scale super-resolution (MIASSR) has recently gained widespread attention, aiming to super sample medical volumes at arbitrary scales via a single model. However, existing MIASSR methods face two major limitations: (i) reliance on high-resolution (HR) volumes and (ii) limited generalization ability, which restricts their application in various scenarios. To overcome these limitations, we propose Cube-based Neural Radiance Field (CuNeRF), a zero-shot MIASSR framework that can yield medical images at arbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSR methods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRF focuses on building a coordinate-intensity continuous representation from LR volumes without the need for HR references. This is achieved by the proposed differentiable modules: including cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering. Through extensive experiments on magnetic resource imaging (MRI) and computed tomography (CT) modalities, we demonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRF yields better visual verisimilitude and reduces aliasing artifacts at various upsampling factors. Moreover, our CuNeRF does not need any LR-HR training pairs, which is more flexible and easier to be used than others. Our code will be publicly available soon."}}
{"id": "Q0xFoXYwU14", "cdate": 1672531200000, "mdate": 1682385330516, "content": {"title": "Hard Nominal Example-aware Template Mutual Matching for Industrial Anomaly Detection", "abstract": "Anomaly detectors are widely used in industrial production to detect and localize unknown defects in query images. These detectors are trained on nominal images and have shown success in distinguishing anomalies from most normal samples. However, hard-nominal examples are scattered and far apart from most normalities, they are often mistaken for anomalies by existing anomaly detectors. To address this problem, we propose a simple yet efficient method: \\textbf{H}ard Nominal \\textbf{E}xample-aware \\textbf{T}emplate \\textbf{M}utual \\textbf{M}atching (HETMM). Specifically, \\textit{HETMM} aims to construct a robust prototype-based decision boundary, which can precisely distinguish between hard-nominal examples and anomalies, yielding fewer false-positive and missed-detection rates. Moreover, \\textit{HETMM} mutually explores the anomalies in two directions between queries and the template set, and thus it is capable to capture the logical anomalies. This is a significant advantage over most anomaly detectors that frequently fail to detect logical anomalies. Additionally, to meet the speed-accuracy demands, we further propose \\textbf{P}ixel-level \\textbf{T}emplate \\textbf{S}election (PTS) to streamline the original template set. \\textit{PTS} selects cluster centres and hard-nominal examples to form a tiny set, maintaining the original decision boundaries. Comprehensive experiments on five real-world datasets demonstrate that our methods yield outperformance than existing advances under the real-time inference speed. Furthermore, \\textit{HETMM} can be hot-updated by inserting novel samples, which may promptly address some incremental learning issues."}}
{"id": "yrYfF7KpG1", "cdate": 1640995200000, "mdate": 1682385330321, "content": {"title": "Uncertainty Modeling with Second-Order Transformer for Group Re-identification", "abstract": "Group re-identification (G-ReID) focuses on associating the group images containing the same persons under different cameras. The key challenge of G-ReID is that all the cases of the intra-group member and layout variations are hard to exhaust. To this end, we propose a novel uncertainty modeling, which treats each image as a distribution depending on the current member and layout, then digs out potential group features by random samplings. Based on potential and original group features, uncertainty modeling can learn better decision boundaries, which is implemented by two modules, member variation module (MVM) and layout variation module (LVM). Furthermore, we propose a novel second-order transformer framework (SOT), which is inspired by the fact that the position modeling in the transformer is coped with the G-ReID task. SOT is composed of the intra-member module and inter-member module. Specifically, the intra-member module extracts the first-order token for each member, and then the inter-member module learns a second-order token as a group feature by the above first-order tokens, which can be regarded as the token of tokens. A large number of experiments have been conducted on three available datasets, including CSG, DukeGroup and RoadGroup. The experimental results show that the proposed SOT outperforms all previous state-of-the-art methods."}}
{"id": "q9gHEkX2i9I", "cdate": 1640995200000, "mdate": 1682385330264, "content": {"title": "Decoupled Contrastive Learning for Intra-Camera Supervised Person Re-identification", "abstract": "Intra-camera supervised (ICS) person re-identification (Re-ID) assumes that people are annotated independently in each camera, which is a newly proposed setting to reduce the cost of manual annotation. Most existing methods are developed on generating global pseudo labels to learn camera-agnostic features by classification loss. However, they do not utilize intra-camera labels well for cross-camera association. In this paper, we propose a Decoupled Contrastive Learning (DCL) strategy to tackle the issue. Concretely, we first conduct an intra-camera pre-train stage to reduce the intra-identity variance. Then an inter-identity association and an inter-camera learning step are alternatively iterated to improve the feature representation gradually. We propose a decoupled identity contrastive loss in the inter-camera learning stage to make the training more effective. In order to improve the compactness of the associated cluster, we further adopt a hard-aware contrastive loss. Extensive experiments on two large-scale Re-ID datasets demonstrate that the proposed method outperforms most ICS methods and performs comparably to fully supervised methods."}}
{"id": "iWYzN9q4HR", "cdate": 1640995200000, "mdate": 1668606814745, "content": {"title": "Self-supervised Image-specific Prototype Exploration for Weakly Supervised Semantic Segmentation", "abstract": "Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels has attracted much attention due to low annotation costs. Existing methods often rely on Class Activation Mapping (CAM) that measures the correlation between image pixels and classifier weight. However, the classifier focuses only on the discriminative regions while ignoring other useful information in each image, resulting in incomplete localization maps. To address this issue, we propose a Self-supervised Image-specific Prototype Exploration (SIPE) that consists of an Image-specific Prototype Exploration (IPE) and a General-Specific Consistency (GSC) loss. Specifically, IPE tailors prototypes for every image to capture complete regions, formed our Image-Specific CAM (IS-CAM), which is realized by two sequential steps. In addition, GSC is proposed to construct the consistency of general CAM and our specific IS-CAM, which further optimizes the feature representation and empowers a self-correction ability of prototype exploration. Extensive experiments are conducted on PASCAL VOC 2012 and MS COCO 2014 segmentation benchmark and results show our SIPE achieves new state-of-the-art performance using only image-level labels. The code is available at https://github.com/chenqi1126/SIPE."}}
{"id": "eVcifUgeP9", "cdate": 1640995200000, "mdate": 1682385330548, "content": {"title": "Learning Bi-directional Feature Propagation with Latent Layout Modeling for Group Re-identification", "abstract": "Group re-identification (G-ReID) aims to identify the same group of persons across the disjoint cameras. The key challenge of G-ReID is the robust feature extraction against the potential group layout and membership varitions. However, previous works focus more on the appearance modeling and less on the importance of group layout. In this paper, we propose a bi-directional feature propagation framework, which propagates information between group layout and member appearance. In addition, we propose the spatial generation framework, which analyses the group image and generates new images with different group layouts to simulate various layouts in the real world. Moreover, we propose a network that learns latent layout representations and propagates the layout representations with the member appearance representations. The proposed network achieves SOTA performance on two widely used G-ReID datasets, i.e., 87.9% mAP and 89.2% Rank-1 on CSG, 92.7% mAP and 90.1% Rank-1 on RoadGroup."}}
{"id": "cAj96xwHqz", "cdate": 1640995200000, "mdate": 1682385330719, "content": {"title": "Cross-level Attention and Ratio Consistency Network for Ship Detection", "abstract": "In ship detection task, target objects with extreme aspect ratios are common in practical applications. However, existing ship detection methods seldom make efforts to tackle this issue. In this paper, we present a novel Cross-level Attention and Ratio Consistency (CARC) Network for ship detection. First, we propose a Cross-Level Attention (CLA) module to generate attention signals by integrating information from both higher and lower level features. Specifically, for each feature, we calculate its similarity with features from adjacent levels. These similarities are utilized as weights to enhance the channels that consist of different-level information. By fusing multi-level information, a channel-wise attention vector is generated to enhance the learned representations in the base feature. Second, we propose a Ratio Consistency loss that promotes the networks to localize the ships with more accurate aspect ratios. Existing ship detection methods have different sensitiveness to width and height predictions, significantly increasing the learning difficulty for localizing target ships. We append an auxiliary supervision signal to the detection head in our method. This supervision signal measures the error between the predicted and the ground truth aspect ratios of target ships. Experiment results show that our model achieves significant performance gains compared to existing methods."}}
{"id": "_mhH1_ra4Vo", "cdate": 1640995200000, "mdate": 1682385330323, "content": {"title": "Global Trajectory Helps Person Retrieval in a Camera Network", "abstract": "We are concerned with retrieving a query person from multiple videos captured by a non-overlapping camera network. Existing methods often rely on purely visual matching or consider temporal constraints but ignore the spatial information of the camera network. To address this issue, we propose a pedestrian retrieval framework based on cross-camera trajectory generation, which integrates both temporal and spatial information. To obtain pedestrian trajectories, we propose a novel cross-camera spatio-temporal model that integrates pedestrians' walking habits and the path layout between cameras to form a joint probability distribution. Such a spatio-temporal model among a camera network can be specified using sparsely sampled pedestrian data. Based on the spatio-temporal model, cross-camera trajectories can be extracted by the conditional random field model and further optimized by restricted non-negative matrix factorization. Finally, a trajectory re-ranking technique is proposed to improve the pedestrian retrieval results. To verify the effectiveness of our method, we construct the first cross-camera pedestrian trajectory dataset, the Person Trajectory Dataset, in real surveillance scenarios. Extensive experiments verify the effectiveness and robustness of the proposed method."}}
