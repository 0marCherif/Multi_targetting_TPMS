{"id": "ccYOWWNa5v2", "cdate": 1652737568676, "mdate": null, "content": {"title": "Lifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting", "abstract": "In lifelong learning systems based on artificial neural networks, one of the biggest obstacles is the inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we propose a new kind of connectionist architecture, the Sequential Neural Coding Network, that is robust to forgetting when learning from streams of data points and, unlike networks of today, does not learn via the popular back-propagation of errors. Grounded in the neurocognitive theory of predictive coding, our model adapts its synapses in a biologically-plausible fashion while another neural system learns to direct and control this cortex-like structure, mimicking some of the task-executive control functionality of the basal ganglia. In our experiments, we demonstrate that our self-organizing system experiences significantly less forgetting compared to standard neural models, outperforming a swath of previously proposed methods, including rehearsal/data buffer-based methods, on both standard (SplitMNIST, Split Fashion MNIST, etc.) and custom benchmarks even though it is trained in a stream-like fashion. Our work offers evidence that emulating mechanisms in real neuronal systems, e.g., local learning, lateral competition, can yield new directions and possibilities for tackling the grand challenge of lifelong machine learning."}}
{"id": "tPtrfX0xBQX", "cdate": 1640995200000, "mdate": 1668657716356, "content": {"title": "ThreshNet: Segmentation Refinement Inspired by Region-Specific Thresholding", "abstract": "We present ThreshNet, a post-processing method to refine the output of neural networks designed for binary segmentation tasks. ThreshNet uses the confidence map produced by a base network along with global and local patch information to significantly improve the performance of even state-of-the-art methods. Binary segmentation models typically convert confidence maps into predictions by thresholding the confidence scores at 0.5 (or some other fixed number). However, we observe that the best threshold is image-dependent and often even region-specific -- different parts of the image benefit from using different thresholds. Thus ThreshNet takes a trained segmentation model and learns to correct its predictions by using a memory-efficient post-processing architecture that incorporates region-specific thresholds as part of the training mechanism. Our experiments show that ThreshNet consistently improves over current the state-of-the-art methods in binary segmentation and saliency detection, typically by 3 to 5% in mIoU and mBA."}}
{"id": "qnenYmVVitk", "cdate": 1640995200000, "mdate": 1668657716051, "content": {"title": "An Empirical Analysis of Recurrent Learning Algorithms In Neural Lossy Image Compression Systems", "abstract": "Recent advances in deep learning have resulted in image compression algorithms that outperform JPEG and JPEG 2000 on the standard Kodak benchmark. However, they are slow to train (due to backprop-through-time) and, to the best of our knowledge, have not been systematically evaluated on a large variety of datasets. In this paper, we perform the first large-scale comparison of recent state-of-the-art hybrid neural compression algorithms, while exploring the effects of alternative training strategies (when applicable). The hybrid recurrent neural decoder is a former state-of-the-art model (recently overtaken by a Google model) that can be trained using backprop-through-time (BPTT) or with alternative algorithms like sparse attentive backtracking (SAB), unbiased online recurrent optimization (UORO), and real-time recurrent learning (RTRL). We compare these training alternatives along with the Google models (GOOG and E2E) on 6 benchmark datasets. Surprisingly, we found that the model trained with SAB performs better (outperforming even BPTT), resulting in faster convergence and a better peak signal-to-noise ratio."}}
{"id": "puiezpiacH", "cdate": 1640995200000, "mdate": 1668657716502, "content": {"title": "Reconstruction Attacks on Aggressive Relaxations of Differential Privacy", "abstract": "Differential privacy is a widely accepted formal privacy definition that allows aggregate information about a dataset to be released while controlling privacy leakage for individuals whose records appear in the data. Due to the unavoidable tension between privacy and utility, there have been many works trying to relax the requirements of differential privacy to achieve greater utility. One class of relaxation, which is starting to gain support outside the privacy community is embodied by the definitions of individual differential privacy (IDP) and bootstrap differential privacy (BDP). The original version of differential privacy defines a set of neighboring database pairs and achieves its privacy guarantees by requiring that each pair of neighbors should be nearly indistinguishable to an attacker. The privacy definitions we study, however, aggressively reduce the set of neighboring pairs that are protected. Both IDP and BDP define a measure of \"privacy loss\" that satisfies formal privacy properties such as postprocessing invariance and composition, and achieve dramatically better utility than the traditional variants of differential privacy. However, there is a significant downside - we show that they allow a significant portion of the dataset to be reconstructed using algorithms that have arbitrarily low privacy loss under their privacy accounting rules. We demonstrate these attacks using the preferred mechanisms of these privacy definitions. In particular, we design a set of queries that, when protected by these mechanisms with high noise settings (i.e., with claims of very low privacy loss), yield more precise information about the dataset than if they were not protected at all."}}
{"id": "l3rQ3YBbB1", "cdate": 1640995200000, "mdate": 1668657716293, "content": {"title": "Exact Privacy Analysis of the Gaussian Sparse Histogram Mechanism", "abstract": "Sparse histogram methods can be useful for returning differentially private counts of items in large or infinite histograms, large group-by queries, and more generally, releasing a set of statistics with sufficient item counts. We consider the Gaussian version of the sparse histogram mechanism and study the exact $\\epsilon,\\delta$ differential privacy guarantees satisfied by this mechanism. We compare these exact $\\epsilon,\\delta$ parameters to the simpler overestimates used in prior work to quantify the impact of their looser privacy bounds."}}
{"id": "Ps_lCrh-es", "cdate": 1640995200000, "mdate": 1668657716358, "content": {"title": "Bayesian and Frequentist Semantics for Common Variations of Differential Privacy: Applications to the 2020 Census", "abstract": "The purpose of this paper is to guide interpretation of the semantic privacy guarantees for some of the major variations of differential privacy, which include pure, approximate, R\\'enyi, zero-concentrated, and $f$ differential privacy. We interpret privacy-loss accounting parameters, frequentist semantics, and Bayesian semantics (including new results). The driving application is the interpretation of the confidentiality protections for the 2020 Census Public Law 94-171 Redistricting Data Summary File released August 12, 2021, which, for the first time, were produced with formal privacy guarantees."}}
{"id": "Keut8EwG3o", "cdate": 1640995200000, "mdate": 1668657716501, "content": {"title": "Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder", "abstract": "Recent advances in deep learning have led to superhuman performance across a variety of applications. Recently, these methods have been successfully employed to improve the rate-distortion performance in the task of image compression. However, current methods either use additional post-processing blocks on the decoder end to improve compression or propose an end-to-end compression scheme based on heuristics. For the majority of these, the trained deep neural networks (DNNs) are not compatible with standard encoders and would be difficult to deply on personal computers and cellphones. In light of this, we propose a system that learns to improve the encoding performance by enhancing its internal neural representations on both the encoder and decoder ends, an approach we call Neural JPEG. We propose frequency domain pre-editing and post-editing methods to optimize the distribution of the DCT coefficients at both encoder and decoder ends in order to improve the standard compression (JPEG) method. Moreover, we design and integrate a scheme for jointly learning quantization tables within this hybrid neural compression framework.Experiments demonstrate that our approach successfully improves the rate-distortion performance over JPEG across various quality metrics, such as PSNR and MS-SSIM, and generates visually appealing images with better color retention quality."}}
{"id": "BfHBMX-_ES", "cdate": 1640995200000, "mdate": 1668657716487, "content": {"title": "Constructing a Large-Scale Landslide Database Across Heterogeneous Environments Using Task-Specific Model Updates", "abstract": "Recent small-scale studies for pixel-wise labeling of potential landslide areas in remotely-sensed images using deep learning (DL) showed potential but were based on data from very small, homogeneous regions with unproven model transferability. In this paper we consider a more realistic and practical setting for large-scale heterogeneous landslide data collection and DL-based labeling. In this setting, remotely sensed images are collected sequentially in temporal batches, where each batch focuses on images from a particular ecoregion, but different batches can focus on different ecoregions with distinct landscape characteristics. For such a scenario, we study the following questions: (1) How well do DL models trained in homogeneous regions perform when they are transferred to different ecoregions? (2) Does increasing the spatial coverage in the data improve model performance in a given ecoregion? and (3) Can a landslide pixel labeling model be incrementally updated with new data, but without access to the old data and without losing performance on the old data? We address these questions by developing a mechanism for incremental training of semantic segmentation models. We call the resulting extension task-specific model updates (TSMU). A national compilation of landslide inventories by the U.S. Geological Survey (USGS) was used to develop a global database for this study. Our results indicate that the TSMU framework can be used to aid in the creation of new landslide inventories or expanding existing datasets, and also to rapidly develop hazard maps for situational awareness following a widespread landslide event."}}
{"id": "9CgBGje9uO", "cdate": 1640995200000, "mdate": 1668657716292, "content": {"title": "Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder", "abstract": "Recent advances in deep learning have led to superhuman performance across a variety of applications. Recently, these methods have been successfully employed to improve the rate-distortion performance in the task of image compression. However, current methods either use additional post-processing blocks on the decoder end to improve compression or propose an end-to-end compression scheme based on heuris-tics. For the majority of these, the trained deep neural networks (DNNs) are not compatible with standard encoders and would be difficult to deploy on personal com-puters and cellphones. In light of this, we propose a system that learns to improve the encoding performance by enhancing its internal neural representations on both the encoder and decoder ends, an approach we call Neural JPEG. We propose frequency domain pre-editing and post-editing methods to optimize the distribution of the DCT coefficients at both encoder and decoder ends in order to improve the stan-dard compression (JPEG) method. Moreover, we design and integrate a scheme for jointly learning quantization tables within this hybrid neural compression framework. In summary, our contributions are as follows:"}}
{"id": "6F1FjvslAb", "cdate": 1640995200000, "mdate": 1668657716486, "content": {"title": "Geographic Spines in the 2020 Census Disclosure Avoidance System TopDown Algorithm", "abstract": "The 2020 Census Disclosure Avoidance System (DAS) is a formally private mechanism that first adds independent noise to cross tabulations for a set of pre-specified hierarchical geographic units, which is known as the geographic spine. After post-processing these noisy measurements, DAS outputs a formally private database with fields indicating location in the standard census geographic spine, which is defined by the United States as a whole, states, counties, census tracts, block groups, and census blocks. This paper describes how the geographic spine used internally within DAS to define the initial noisy measurements impacts accuracy of the output database. Specifically, tabulations for geographic areas tend to be most accurate for geographic areas that both 1) can be derived by aggregating together geographic units above the block geographic level of the internal spine, and 2) are closer to the geographic units of the internal spine. After describing the accuracy tradeoffs relevant to the choice of internal DAS geographic spine, we provide the settings used to define the 2020 Census production DAS runs."}}
