{"id": "U086TJFWy4p", "cdate": 1663850428663, "mdate": null, "content": {"title": "Causally-guided Regularization of Graph Attention improves Generalizability", "abstract": "Graph attention networks estimate the relational importance of node neighbors to aggregate relevant information over local neighborhoods for a prediction task. However, the inferred attentions are vulnerable to spurious correlations and connectivity in the training data, hampering the generalizability of the model. We introduce CAR, a general-purpose regularization framework for graph attention networks. Embodying a causal inference approach, CAR aligns the attention mechanism with the causal effects of active interventions on graph connectivity in a scalable manner. CAR is compatible with a variety of graph attention architectures, and we show that it systematically improves generalizability on various node classification tasks. Our ablation studies indicate that CAR hones in on the aspects of graph structure most pertinent to the prediction (e.g., homophily), and does so more effectively than alternative approaches. Finally, we also show that CAR enhances interpretability of attention weights by accentuating node-neighbor relations that point to causal hypotheses. For social media network-sized graphs, a CAR-guided graph rewiring approach could allow us to combine the scalability of graph convolutional methods with the higher performance of graph attention."}}
{"id": "m1oqEOAozQU", "cdate": 1663850079527, "mdate": null, "content": {"title": "Graph Neural Networks for Link Prediction with Subgraph Sketching", "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs).  It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH."}}
{"id": "1ScXxcHU_WT", "cdate": 1652283602010, "mdate": 1652283602010, "content": {"title": "Towards more patient friendly clinical notes through language models and ontologies", "abstract": "Clinical notes are an efficient way to record patient information but are notoriously hard to decipher for non-experts. Automatically simplifying medical text can empower patients with valuable information about their health, while saving clinicians time. We present a novel approach to automated simplification of medical text based on word frequencies and language modelling, grounded on medical ontologies enriched with layman terms. We release a new dataset of pairs of publicly available medical sentences and a version of them simplified by clinicians. Also, we define a novel text simplification metric and evaluation framework, which we use to conduct a large-scale human evaluation of our method against the state of the art. Our method based on a language model trained on medical forum data generates simpler sentences while preserving both grammar and the original meaning, surpassing the current state of the art."}}
{"id": "y-k-Tyy-a4-", "cdate": 1640995200000, "mdate": 1681395862966, "content": {"title": "Causally-guided Regularization of Graph Attention Improves Generalizability", "abstract": ""}}
{"id": "dhM8aJ6hfho", "cdate": 1640995200000, "mdate": 1681395862958, "content": {"title": "Graph Neural Networks for Link Prediction with Subgraph Sketching", "abstract": ""}}
{"id": "890b-cw81ad", "cdate": 1609459200000, "mdate": 1681395863054, "content": {"title": "Towards more patient friendly clinical notes through language models and ontologies", "abstract": ""}}
{"id": "4e--iM5oV9", "cdate": 1609459200000, "mdate": 1681395862965, "content": {"title": "Towards more patient friendly clinical notes through language models and ontologies", "abstract": ""}}
{"id": "mZySukl5OVP", "cdate": 1577836800000, "mdate": 1681395862966, "content": {"title": "Neural Temporal Point Processes For Modelling Electronic Health Records", "abstract": ""}}
{"id": "iMRAh0c8GR6", "cdate": 1577836800000, "mdate": 1663777583036, "content": {"title": "Neural Temporal Point Processes For Modelling Electronic Health Records", "abstract": "The modelling of Electronic Health Records (EHRs) has the potential to drive more efficient allocation of healthcare resources, enabling early intervention strategies and advancing personalised hea..."}}
{"id": "Z_gQPmdwRDd", "cdate": 1577836800000, "mdate": null, "content": {"title": "Estimating Mutual Information Between Dense Word Embeddings", "abstract": "Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks. Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity). Another excellent candidate for a similarity measure is mutual information (MI), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression. Unfortunately, its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating MI for continuous data. In this work we go through a vast literature on estimating MI in such cases and single out the most promising methods, yielding a simple and elegant similarity measure for word embeddings. We show that mutual information is a viable alternative to correlations, gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods."}}
