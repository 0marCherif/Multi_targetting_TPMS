{"id": "fExcSKdDo_", "cdate": 1632875553175, "mdate": null, "content": {"title": "Learning to Dequantise with Truncated Flows", "abstract": "Dequantisation is a general technique used for transforming data described by a discrete random variable $x$ into a continuous (latent) random variable $z$, for the purpose of it being modeled by likelihood-based density models. Dequantisation was first introduced in the context of ordinal data, such as image pixel values.  However, when the data is categorical, the dequantisation scheme is not obvious.\nWe learn such a dequantisation scheme $q(z | x)$, using variational inference with TRUncated FLows (TRUFL) --- a novel flow-based model that allows the dequantiser to have a learnable truncated support. Unlike previous work, the TRUFL dequantiser is (i) capable of embedding the data losslessly in certain cases, since the truncation allows the conditional distributions $q(z | x)$ to have non-overlapping bounded supports, while being (ii) trainable with back-propagation. Addtionally, since the support of the marginal $q(z)$ is bounded and the support of prior $p(z)$ is not, we propose renormalising the prior distribution over the support of $q(z)$. We derive a lower bound for training, and propose a rejection sampling scheme to account for the invalid samples during generation.\nExperimentally, we benchmark TRUFL on constrained generation tasks, and find that it outperforms prior approaches. In addition, we find that rejection sampling results in higher validity for the constrained problems."}}
{"id": "BkgqL0EtPH", "cdate": 1569439313746, "mdate": null, "content": {"title": "{COMPANYNAME}11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery", "abstract": "We release the largest public ECG dataset of continuous raw signals for representation learning containing over 11k patients and 2 billion labelled beats. Our goal is to enable semi-supervised ECG models to be made as well as to discover unknown subtypes of arrhythmia and anomalous ECG signal events. To this end, we propose an unsupervised representation learning task, evaluated in a semi-supervised fashion.  We provide a set of baselines for different feature extractors that can be built upon.  Additionally, we perform qualitative evaluations on results from PCA embeddings, where we identify some clustering of known subtypes indicating the potential for representation learning in arrhythmia sub-type discovery."}}
{"id": "B1l6qiR5F7", "cdate": 1538087828909, "mdate": null, "content": {"title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks", "abstract": "Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference."}}
{"id": "SyZwQvkDz", "cdate": 1518461784733, "mdate": null, "content": {"title": "Inferring Identity Factors for Grouped Examples", "abstract": "We propose a method for modelling groups of face images from the same identity.\nThe model is trained to infer a distribution over the latent space for identity given\na small set of \u201ctraining data\u201d. One can then sample images using that latent representation\nto produce images of the same identity. We demonstrate that the model\nextracts disentangled factors for identity factors and image-specific vectors. We\nalso perform generative classification over identities to assess its feasibility for\nfew-shot face recognition."}}
{"id": "r1Zb6U-_WS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Improving Explorability in Variational Inference with Annealed Variational Objectives", "abstract": "Despite the advances in the representational capacity of approximate distributions for variational inference, the optimization process can still limit the density that is ultimately learned. We demonstrate the drawbacks of biasing the true posterior to be unimodal, and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods. Inspired by Annealed Importance Sampling, the proposed method facilitates learning by incorporating energy tempering into the optimization objective. In our experiments, we demonstrate our method's robustness to deterministic warm up, and the benefits of encouraging exploration in the latent space."}}
