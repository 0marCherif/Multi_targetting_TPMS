{"id": "RTemQ-x0SI", "cdate": 1672531200000, "mdate": 1679982060686, "content": {"title": "Implicit Regularization for Group Sparsity", "abstract": ""}}
{"id": "d7Q0vVfJ0wO", "cdate": 1663850472952, "mdate": null, "content": {"title": "Implicit Regularization for Group Sparsity", "abstract": "We study the implicit regularization of gradient descent towards structured sparsity via a novel neural reparameterization, which we call a diagonally grouped linear neural network. We show the following intriguing property of our reparameterization: gradient descent over the squared regression loss, without any explicit regularization, biases towards solutions with a group sparsity structure. In contrast to many existing works in understanding implicit regularization, we prove that our training trajectory cannot be simulated by mirror descent. We analyze the gradient dynamics of the corresponding regression problem in the general noise setting and obtain minimax-optimal error rates. Compared to existing bounds for implicit sparse regularization using diagonal linear networks, our analysis with the new reparameterization shows improved sample complexity. In the degenerate case of size-one groups, our approach gives rise to a new algorithm for sparse linear regression. Finally, we demonstrate the efficacy of our approach with several numerical experiments."}}
{"id": "MVMWHOGBnIj", "cdate": 1640995200000, "mdate": 1681184987360, "content": {"title": "Deep Spatio-Temporal Wind Power Forecasting", "abstract": ""}}
{"id": "QM8oG0bz1o", "cdate": 1621629885696, "mdate": null, "content": {"title": "Implicit Sparse Regularization: The Impact of Depth and Early Stopping", "abstract": "In this paper, we study the implicit bias of gradient descent for sparse regression. We extend results on regression with quadratic parametrization, which amounts to depth-2 diagonal linear networks, to more general depth-$N$ networks, under more realistic settings of noise and correlated designs. We show that early stopping is crucial for gradient descent to converge to a sparse model, a phenomenon that we call \\emph{implicit sparse regularization}. This result is in sharp contrast to known results for noiseless and uncorrelated-design cases. \n  \nWe characterize the impact of depth and early stopping and show that for a general depth parameter $N$, gradient descent with early stopping achieves minimax optimal sparse recovery with sufficiently small initialization $w_0$ and step size $\\eta$. In particular, we show that increasing depth enlarges the scale of working initialization and the early-stopping window so that this implicit sparse regularization effect is more likely to take place."}}
{"id": "tJErbwx6YL", "cdate": 1609459200000, "mdate": 1681184987344, "content": {"title": "Deep Spatio-Temporal Wind Power Forecasting", "abstract": ""}}
{"id": "2t1whMb1Ghj", "cdate": 1609459200000, "mdate": 1681184987377, "content": {"title": "Implicit Sparse Regularization: The Impact of Depth and Early Stopping", "abstract": ""}}
{"id": "0X2OjqgNHtq", "cdate": 1609459200000, "mdate": 1679982152445, "content": {"title": "Implicit Sparse Regularization: The Impact of Depth and Early Stopping", "abstract": ""}}
