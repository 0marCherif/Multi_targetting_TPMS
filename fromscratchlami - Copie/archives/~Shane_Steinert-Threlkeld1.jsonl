{"id": "kRPgb-mC6T", "cdate": 1672531200000, "mdate": 1710795452098, "content": {"title": "Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages", "abstract": "Pre-trained multilingual language models underpin a large portion of modern NLP tools outside of English. A strong baseline for specializing these models for specific languages is Language-Adaptive Pre-Training (LAPT). However, retaining a large cross-lingual vocabulary and embedding matrix comes at considerable excess computational cost during adaptation. In this study, we propose several simple techniques to replace a cross-lingual vocabulary with a compact, language-specific one. Namely, we address strategies for re-initializing the token embedding matrix after vocabulary specialization. We then provide a systematic experimental comparison of our techniques, in addition to the recently-proposed Focus method. We demonstrate that: 1) Embedding-replacement techniques in the monolingual transfer literature are inadequate for adapting multilingual models. 2) Replacing cross-lingual vocabularies with smaller specialized ones provides an efficient method to improve performance in low-resource languages. 3) Simple embedding re-initialization techniques based on script-wise sub-distributions rival techniques such as Focus, which rely on similarity scores obtained from an auxiliary model."}}
{"id": "kDIT__xtRb", "cdate": 1672531200000, "mdate": 1710795452153, "content": {"title": "Uncovering the Structure of Semantic Representations Using a Computational Model of Decision-Making", "abstract": "According to logical theories of meaning, a meaning of an expression can be formalized and encoded in truth conditions. Vagueness of the language and individual differences between people are a chall..."}}
{"id": "OAbiuPrVhPn", "cdate": 1672531200000, "mdate": 1710795452094, "content": {"title": "The Weighted M\u00f6bius Score: A Unified Framework for Feature Attribution", "abstract": "Feature attribution aims to explain the reasoning behind a black-box model's prediction by identifying the impact of each feature on the prediction. Recent work has extended feature attribution to interactions between multiple features. However, the lack of a unified framework has led to a proliferation of methods that are often not directly comparable. This paper introduces a parameterized attribution framework -- the Weighted M\\\"obius Score -- and (i) shows that many different attribution methods for both individual features and feature interactions are special cases and (ii) identifies some new methods. By studying the vector space of attribution methods, our framework utilizes standard linear algebra tools and provides interpretations in various fields, including cooperative game theory and causal mediation analysis. We empirically demonstrate the framework's versatility and effectiveness by applying these attribution methods to feature interactions in sentiment analysis and chain-of-thought prompting."}}
{"id": "AaVmiz7-aNt", "cdate": 1672531200000, "mdate": 1710795452100, "content": {"title": "Evaluating Transformer's Ability to Learn Mildly Context-Sensitive Languages", "abstract": "Despite the fact that Transformers perform well in NLP tasks, recent studies suggest that self-attention is theoretically limited in learning even some regular and context-free languages. These findings motivated us to think about their implications in modeling natural language, which is hypothesized to be mildly context-sensitive. We test the Transformer's ability to learn mildly context-sensitive languages of varying complexities, and find that they generalize well to unseen in-distribution data, but their ability to extrapolate to longer strings is worse than that of LSTMs. Our analyses show that the learned self-attention patterns and representations modeled dependency relations and demonstrated counting behavior, which may have helped the models solve the languages."}}
{"id": "JZv9mZRUGe", "cdate": 1669852800000, "mdate": 1688732665710, "content": {"title": "Beyond Anthropocentrism in Comparative Cognition: Recentering Animal Linguistics", "abstract": ""}}
{"id": "SUqrM7WR7W5", "cdate": 1646678298083, "mdate": null, "content": {"title": "Emergent Communication Fine-tuning (EC-FT) for Pretrained Language Models", "abstract": "It has recently been argued that the currently dominant paradigm in NLP of pretraining on text-only corpora will not yield robust natural language understanding systems.  One strain of this argumentation highlights the need for grounded, goal-oriented, and interactive language learning.  In this position paper, we articulate how Emergent Communication (EC) can be used in conjunction with large pretrained language models as a `Fine-Tuning' (FT) step (hence, EC-FT) in order to provide them with supervision from such learning scenarios.  We discuss methodological issues and difficulties with making this work, and then illustrate the overall idea with a case study in unsupervised machine translation, before concluding with a discussion on the relation to multimodal pretraining."}}
{"id": "gsm3vw2TiI", "cdate": 1640995200000, "mdate": 1710795452247, "content": {"title": "Testing Pre-trained Language Models' Understanding of Distributivity via Causal Mediation Analysis", "abstract": "To what extent do pre-trained language models grasp semantic knowledge regarding the phenomenon of distributivity? In this paper, we introduce DistNLI, a new diagnostic dataset for natural language inference that targets the semantic difference arising from distributivity, and employ the causal mediation analysis framework to quantify the model behavior and explore the underlying mechanism in this semantically-related task. We find that the extent of models' understanding is associated with model size and vocabulary size. We also provide insights into how models encode such high-level semantic knowledge."}}
{"id": "W2_C6DC83k2", "cdate": 1640995200000, "mdate": 1710795452210, "content": {"title": "Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining", "abstract": ""}}
{"id": "DyCqKVSyhmb", "cdate": 1640995200000, "mdate": 1682279433241, "content": {"title": "Learning to translate by learning to communicate", "abstract": "We formulate and test a technique to use Emergent Communication (EC) with a pretrained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the currently dominant paradigm in NLP of pretraining on text-only corpora will not yield robust natural language understanding systems, and the need for grounded, goal-oriented, and interactive language learning has been highlighted. In our approach, we embed a modern multilingual model (mBART, Liu et. al. 2020) into an EC image-reference game, in which the model is incentivized to use multilingual generations to accomplish a vision-grounded task, with the hypothesis that this will align multiple languages to a shared task space. We present two variants of EC Fine-Tuning (Steinert-Threlkeld et. al. 2022), one of which outperforms a backtranslation-based baseline in 6/8 translation settings, and proves especially beneficial for the very low-resource languages of Nepali and Sinhala."}}
{"id": "AezltXFVf2", "cdate": 1640995200000, "mdate": 1710795452152, "content": {"title": "Probing for Understanding of English Verb Classes and Alternations in Large Pre-trained Language Models", "abstract": "We investigate the extent to which verb alternation classes, as described by Levin (1993), are encoded in the embeddings of Large Pre-trained Language Models (PLMs) such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively constructed diagnostic classifiers for word and sentence-level prediction tasks. We follow and expand upon the experiments of Kann et al. (2019), which aim to probe whether static embeddings encode frame-selectional properties of verbs. At both the word and sentence level, we find that contextual embeddings from PLMs not only outperform non-contextual embeddings, but achieve astonishingly high accuracies on tasks across most alternation classes. Additionally, we find evidence that the middle-to-upper layers of PLMs achieve better performance on average than the lower layers across all probing tasks."}}
