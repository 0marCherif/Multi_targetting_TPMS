{"id": "yMWT-D-eLo", "cdate": 1672531200000, "mdate": 1680112441562, "content": {"title": "Group Fairness with Uncertainty in Sensitive Attributes", "abstract": ""}}
{"id": "e9NQvYNUOAl", "cdate": 1664815576778, "mdate": null, "content": {"title": "On counterfactual inference with unobserved confounding", "abstract": "Given an observational study with $n$ independent but heterogeneous units and one $p$-dimensional sample per unit containing covariates, interventions, and outcomes, our goal is to learn counterfactual distribution for each unit. We consider studies with unobserved confounding which introduces statistical biases between interventions and outcomes as well as exacerbates the heterogeneity across units. Modeling the underlying joint distribution as an exponential family and under suitable conditions, we reduce learning the $n$ unit-level counterfactual distributions to learning $n$ exponential family distributions with heterogeneous parameters and only one sample per distribution. We introduce a convex objective that pools all $n$ samples to jointly learn all $n$ parameters and provide a unit-wise mean squared error bound that scales linearly with the metric entropy of the parameter space. For example, when the parameters are $s$-sparse linear combination of $k$ known vectors, the error is $O(s\\log k/p)$. En route, we derive sufficient conditions for compactly supported distributions to satisfy the logarithmic Sobolev inequality."}}
{"id": "XCbg7_o1nk", "cdate": 1640995200000, "mdate": 1681488142105, "content": {"title": "Optimal Compression of Locally Differentially Private Mechanisms", "abstract": ""}}
{"id": "RXFjiaRJU3b", "cdate": 1640995200000, "mdate": 1659279548625, "content": {"title": "Selective Regression under Fairness Criteria", "abstract": "Selective regression allows abstention from prediction if the confidence to make an accurate prediction is not sufficient. In general, by allowing a reject option, one expects the performance of a ..."}}
{"id": "9p7vEPmWiFa", "cdate": 1640995200000, "mdate": 1675422011674, "content": {"title": "Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge", "abstract": "Treatment effect estimation from observational data is a fundamental problem in causal inference. There are two very different schools of thought that have tackled this problem. On the one hand, the Pearlian framework commonly assumes structural knowledge (provided by an expert) in the form of directed acyclic graphs and provides graphical criteria such as the back-door criterion to identify the valid adjustment sets. On the other hand, the potential outcomes (PO) framework commonly assumes that all the observed features satisfy ignorability (i.e., no hidden confounding), which in general is untestable. In prior works that attempted to bridge these frameworks, there is an observational criteria to identify an anchor variable and if a subset of covariates (not involving the anchor variable) passes a suitable conditional independence criteria, then that subset is a valid back-door. Our main result strengthens these prior results by showing that under a different expert-driven structural knowledge \u2014 that one variable is a direct causal parent of the treatment variable \u2014 remarkably, testing for subsets (not involving the known parent variable) that are valid back-doors is equivalent to an invariance test. Importantly, we also cover the non-trivial case where the entire set of observed features is not ignorable (generalizing the PO framework) without requiring the knowledge of all the parents of the treatment variable. Our key technical idea involves generation of a synthetic sub-sampling (or environment) variable that is a function of the known parent variable. In addition to designing an invariance test, this sub-sampling variable allows us to leverage Invariant Risk Minimization, and thus, connects finding valid adjustments (in non-ignorable observational settings) to representation learning. We demonstrate the effectiveness and tradeoffs of these approaches on a variety of synthetic datasets as well as real causal effect estimation benchmarks."}}
{"id": "fxGT4XaLkpX", "cdate": 1621630047976, "mdate": null, "content": {"title": "A Computationally Efficient Method for Learning Exponential Family Distributions", "abstract": "We consider the question of learning the natural parameters of a $k$ parameter \\textit{minimal} exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We provide finite sample guarantees to achieve an ($\\ell_2$) error of $\\alpha$ in the parameter estimation  with sample complexity $O(\\mathrm{poly}(k/\\alpha))$ and computational complexity ${O}(\\mathrm{poly}(k/\\alpha))$. To establish these results, we show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. "}}
{"id": "B9WXduMZBEM", "cdate": 1621630047976, "mdate": null, "content": {"title": "A Computationally Efficient Method for Learning Exponential Family Distributions", "abstract": "We consider the question of learning the natural parameters of a $k$ parameter \\textit{minimal} exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We provide finite sample guarantees to achieve an ($\\ell_2$) error of $\\alpha$ in the parameter estimation  with sample complexity $O(\\mathrm{poly}(k/\\alpha))$ and computational complexity ${O}(\\mathrm{poly}(k/\\alpha))$. To establish these results, we show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. "}}
{"id": "O67qv2bbsUe", "cdate": 1609459200000, "mdate": 1681488142055, "content": {"title": "A Computationally Efficient Method for Learning Exponential Family Distributions", "abstract": ""}}
{"id": "BWWCUNBSec", "cdate": 1609459200000, "mdate": 1645724757550, "content": {"title": "Treatment Effect Estimation Using Invariant Risk Minimization", "abstract": "Inferring causal individual treatment effect (ITE) from observational data is a challenging problem whose difficulty is exacerbated by the presence of treatment assignment bias. In this work, we propose a new way to estimate the ITE using the domain generalization framework of invariant risk minimization (IRM). IRM uses data from multiple domains, learns predictors that do not exploit spurious domain-dependent factors, and generalizes better to unseen domains. We propose an IRM-based ITE estimator aimed at tackling treatment assignment bias when there is little support overlap between the control group and the treatment group. We accomplish this by creating diversity: given a single dataset, we split the data into multiple domains artificially. These diverse domains are then exploited by IRM to more effectively generalize regression-based models to data regions that lack support overlap. We show gains over classical regression approaches to ITE estimation in settings when support mismatch is more pronounced."}}
{"id": "2YXjL81HhFO", "cdate": 1609459200000, "mdate": 1681488141959, "content": {"title": "On Learning Continuous Pairwise Markov Random Fields", "abstract": ""}}
