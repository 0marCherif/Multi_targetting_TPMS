{"id": "uECAYvIQOh", "cdate": 1672531200000, "mdate": 1688570398008, "content": {"title": "Unicom: Universal and Compact Representation Learning for Image Retrieval", "abstract": ""}}
{"id": "fyZKd1rw54v", "cdate": 1672531200000, "mdate": 1698064093116, "content": {"title": "ALIP: Adaptive Language-Image Pre-training with Synthetic Caption", "abstract": "Contrastive Language-Image Pre-training (CLIP) has significantly boosted the performance of various vision-language tasks by scaling up the dataset with image-text pairs collected from the web. However, the presence of intrinsic noise and unmatched image-text pairs in web data can potentially affect the performance of representation learning. To address this issue, we first utilize the OFA model to generate synthetic captions that focus on the image content. The generated captions contain complementary information that is beneficial for pre-training. Then, we propose an Adaptive Language-Image Pre-training (ALIP), a bi-path model that integrates supervision from both raw text and synthetic caption. As the core components of ALIP, the Language Consistency Gate (LCG) and Description Consistency Gate (DCG) dynamically adjust the weights of samples and image-text/caption pairs during the training process. Meanwhile, the adaptive contrastive loss can effectively reduce the impact of noise data and enhances the efficiency of pre-training data. We validate ALIP with experiments on different scales of models and pre-training datasets. Experiments results show that ALIP achieves state-of-the-art performance on multiple downstream tasks including zero-shot image-text retrieval and linear probe. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/ALIP."}}
{"id": "3YFDsSRSxB-", "cdate": 1663850021356, "mdate": null, "content": {"title": "Unicom: Universal and Compact Representation Learning for Image Retrieval", "abstract": "Modern image retrieval methods typically rely on fine-tuning pre-trained encoders to extract image-level descriptors.\nHowever, the most widely used models are pre-trained on ImageNet-1K with limited classes. The pre-trained feature representation is therefore not universal enough to generalize well to the diverse open-world classes. \nIn this paper, we first cluster the large-scale \\laion{} into one million pseudo classes based on the joint textual and visual features extracted by the CLIP model. Due to the confusion of label granularity, the automatically clustered dataset inevitably contains heavy inter-class conflict. To alleviate such conflict, we randomly select partial inter-class prototypes to construct the margin-based softmax loss. To further enhance the low-dimensional feature representation, we randomly select partial feature dimensions when calculating the similarities between embeddings and class-wise prototypes. The dual random partial selections are with respect to the class dimension and the feature dimension of the prototype matrix, making the classification conflict-robust and the feature embedding compact. Our method significantly outperforms state-of-the-art unsupervised and supervised image retrieval approaches on multiple benchmarks. The code and pre-trained models are released to facilitate future research \\url{https://github.com/deepglint/unicom}. "}}
{"id": "7kZsuSfw4O", "cdate": 1640995200000, "mdate": 1667524834409, "content": {"title": "Killing Two Birds with One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC", "abstract": "Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the largescale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at https://github.com/deepinsight/insightface/tree/master/recognition."}}
{"id": "F-o9YGfiUld", "cdate": 1609459200000, "mdate": 1663651901489, "content": {"title": "Partial FC: Training 10 Million Identities on a Single Machine", "abstract": "Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memory is gradually becoming irreconcilable. In this work, we theoretically analyze the upper limit of model parallelism in face recognition in the first place. Then we propose a load-balanced sparse distributed classification training method, Partial FC, which is capable of using a machine with only 8 Nvidia Tesla V100 GPUs to implement training on a face recognition data set with up to 29 million IDs. Furthermore, we are able to train on data set with 100 million IDs in 64 RTX2080Ti GPUs. We have verified the effectiveness of Partial FC in 8 mainstream face recognition trainsets, and find that Partial FC is effective in all face recognition training sets. The code of this paper has been made available at https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc."}}
{"id": "5ftoMDsv91c", "cdate": 1609459200000, "mdate": 1663651901489, "content": {"title": "Masked Face Recognition Challenge: The InsightFace Track Report", "abstract": "During the COVID-19 coronavirus epidemic, almost everyone wears a facial mask, which poses a huge challenge to deep face recognition. In this workshop, we organize Masked Face Recognition (MFR) challenge <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> and focus on bench-marking deep face recognition methods under the existence of facial masks. In the MFR challenge, there are two main tracks: the InsightFace track and the WebFace260M track [38]. For the InsightFace track, we manually collect a large-scale masked face test set with 7K identities. In addition, we also collect a children test set including 14K identities and a multi-racial test set containing 242K identities. By using these three test sets, we build up an online model testing system, which can give a comprehensive evaluation of face recognition models. To avoid data privacy problems, no test image is released to the public. As the challenge is still under-going, we will keep on updating the top-ranked solutions as well as this report on the arxiv."}}
