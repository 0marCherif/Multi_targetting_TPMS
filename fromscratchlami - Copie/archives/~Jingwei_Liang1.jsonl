{"id": "HJgBhESeLB", "cdate": 1567802540970, "mdate": null, "content": {"title": "Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration", "abstract": "The alternating direction method of multipliers (ADMM) is one of the most widely used first-order optimisation methods in the literature owing to its simplicity and efficiency. Over the years, different efforts are made to improve the method, such as the inertial technique. By studying the geometric properties of ADMM, we discuss the limitations of current inertial accelerated ADMM and then present and analyse an adaptive acceleration scheme for ADMM. Numerical experiments on problems arising from image processing, statistics and machine learning demonstrate the advantages of the proposed algorithm.  "}}
{"id": "HyW6Msb_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Local Convergence Properties of SAGA/Prox-SVRG and Acceleration", "abstract": "In this paper, we present a local convergence anal- ysis for a class of stochastic optimisation meth- ods: the proximal variance reduced stochastic gradient methods, and mainly focus on SAGA (Defaz..."}}
{"id": "HkWb4DW_bS", "cdate": 1451606400000, "mdate": null, "content": {"title": "A Multi-step Inertial Forward-Backward Splitting Method for Non-convex Optimization", "abstract": "In this paper, we propose a multi-step inertial Forward--Backward splitting algorithm for minimizing the sum of two non-necessarily convex functions, one of which is proper lower semi-continuous while the other is differentiable with a Lipschitz continuous gradient. We first prove global convergence of the scheme with the help of the Kurdyka\u2013\u0141ojasiewicz property. Then, when the non-smooth part is also partly smooth relative to a smooth submanifold, we establish finite identification of the latter and provide sharp local linear convergence analysis. The proposed method is illustrated on a few problems arising from statistics and machine learning."}}
{"id": "rJWoruWdWS", "cdate": 1388534400000, "mdate": null, "content": {"title": "Local Linear Convergence of Forward-Backward under Partial Smoothness", "abstract": "In this paper, we consider the Forward--Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $\\mathcal{M}$. We propose a generic framework in which we show that the Forward--Backward (i) correctly identifies the active manifold $\\mathcal{M}$ in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning."}}
