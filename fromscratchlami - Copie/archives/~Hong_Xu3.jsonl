{"id": "GsKz-tmH2Py", "cdate": 1609459200000, "mdate": null, "content": {"title": "Joint Switch-Controller Association and Control Devolution for SDN Systems: An Integrated Online Perspective of Control and Learning", "abstract": "In software-defined networking (SDN) systems, it is a common practice to adopt a multi-controller design and control devolution techniques to improve the performance of the control plane. However, in such systems the decision-making for joint switch-controller association and control devolution often involves various uncertainties, e.g., the temporal variations of controller accessibility, and computation and communication costs of switches. In practice, statistics of such uncertainties are unattainable and need to be learned in an online fashion, calling for an integrated design of learning and control. In this article, we formulate a stochastic network optimization problem that aims to minimize time-average system costs and ensure queue stability. By transforming the problem into a combinatorial multi-armed bandit problem with long-term stability constraints, we adopt bandit learning methods and optimal control techniques to handle the exploration-exploitation tradeoff and long-term stability constraints, respectively. Through an integrated design of online learning and online control, we propose an effective Learning-Aided Switch-Controller Association and Control Devolution (LASAC) scheme. Our theoretical analysis and simulation results show that LASAC achieves a tunable tradeoff between queue stability and system cost reduction with a sublinear time-averaged regret bound over a finite time horizon."}}
{"id": "UbzmBMEnxpg", "cdate": 1577836800000, "mdate": null, "content": {"title": "Simulating Performance of ML Systems with Offline Profiling", "abstract": "We advocate that simulation based on offline profiling is a promising approach to better understand and improve the complex ML systems. Our approach uses operation-level profiling and dataflow based simulation to ensure it offers a unified and automated solution for all frameworks and ML models, and is also accurate by considering the various parallelization strategies in a real system."}}
{"id": "OkQ0gO6t5U-", "cdate": 1577836800000, "mdate": null, "content": {"title": "Irina: Accelerating DNN Inference with Efficient Online Scheduling", "abstract": "DNN inference is becoming prevalent for many real-world applications. Current machine learning frameworks usually schedule inference tasks with the goal of optimizing throughput under predictable workloads and task arrival patterns. Yet, inference workloads are becoming more dynamic with bursty queries generated by various video analytics pipelines which run expensive inference only on a fraction of video frames. Thus it is imperative to optimize the completion time of these unpredictable queries and improve customer experience. We propose the preliminary design of the first online inference task scheduling system, called Irina, that takes completion time under unpredictable workload as its primary objective. Irina augments the design space of inference task scheduling with three new strategies, namely batching, stacking, and preemption, in order to more flexibly schedule the tasks and reduce overall latency. Simulation results with empirical inference execution data shows that Irina can improve average task completion time by 1.3x\u20132.5x over TensorFlow Serving scheduling."}}
{"id": "HyTY1Xf5_Wc", "cdate": 1577836800000, "mdate": null, "content": {"title": "OPS: Optimized Shuffle Management System for Apache Spark", "abstract": "In recent years, distributed computing frameworks, such as Hadoop MapReduce and Spark, are widely used for big data processing. With the explosive growth of the amount of data, companies tend to store intermediate data of the shuffle phase on disk instead of memory. Therefore, intensive network and disk I/O are both involved in the shuffle phase. To optimize the overhead of the shuffle phase, we propose OPS, an open-source distributed computing shuffle management system based on Spark, which provides an independent shuffle service for Spark. By using early-merge and early-shuffle strategy, OPS alleviates the I/O overhead in the shuffle phase and efficiently schedules the I/O and computing resources. OPS also proposes a slot-based scheduling algorithm to predict and calculate the optimal scheduling result of the reduce task. Besides, OPS provides a taint-redo strategy to ensure the fault tolerance of computing jobs. We evaluated the performance of OPS on a 100-node Amazon AWS EC2 cluster. Overall, OPS optimizes the overhead of shuffle by nearly 50%. In the test cases of HiBench, OPS improves end-to-end completion time by nearly 30% on average."}}
{"id": "C4jHSj083mO", "cdate": 1577836800000, "mdate": null, "content": {"title": "Automated Traffic Engineering in SDWAN: Beyond Reinforcement Learning", "abstract": "Traffic engineering (TE) is a critical and difficult problem that involves assigning traffic with various requirements to paths with different constraints. Recently, machine learning algorithms, especially deep neural networks (DNN), are applied to TE, yet they all assume that the network is a black box, limiting them to only model-free reinforcement learning (RL) algorithms. In this paper, we introduce differentiable programming to TE, and show that the network environment can be sufficiently modeled for TE optimization. Specifically, we design a fully-differentiable network environment, \u2202NE, that can be directly integrated into any DNN models. With \u2202NE, we can differentiate with respect to control parameters, and directly evaluate gradients between actions and states to facilitate gradient descent based training of DNN models. We show with a proof-of-concept prototype that \u2202NE accelerates DNN training for TE by 228\u00d7 and achieves higher scalability compared to existing network simulators. Most importantly, dNE opens up the possibility to apply arbitrary deep learning models to TE beyond RL."}}
{"id": "2T-gt4boK1N", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bottleneck-Aware Coflow Scheduling Without Prior Knowledge", "abstract": "Coflow scheduling is critical to the communication efficiency of data-parallel applications in data centers. While schemes like Varys can achieve optimal performance, they require a priori information about coflows which is hard to obtain in practice. Existing non-clairvoyant solutions like Aalo generalize least attained service (LAS) scheduling discipline to address this issue. However they fail to identify the bottleneck flows in a coflow and tend to allocate excessive bandwidth to the non-bottleneck flows within the coflow, leading to bandwidth wastage and inferior overall performance. To this end, we present Fai that strives to improve the overall coflow performance by accelerating the bottleneck flow without prior knowledge. Fai employs bottleneck-aware scheduling for coflows. Fai adopts loose coordination to update coflow priority and flow rates based on total bytes sent. In addition, Fai detects bottleneck flows based on a flow's rate and bytes sent, and de-allocates bandwidth for other flows to match the bottleneck rate without affecting the coflow completion time (CCT). The saved bandwidth is then distributed among coflows according to their priority to improve overall performance. Both testbed deployments and trace-driven simulations show that Fai outperforms Aalo substantially."}}
{"id": "zA5KmnVdPLG", "cdate": 1546300800000, "mdate": null, "content": {"title": "Sentinel: Failure Recovery in Centralized Traffic Engineering", "abstract": "Network failures are common in wide area networks (WANs). Failure recovery in a software-defined WAN takes minutes or longer, as the controller needs to calculate a new traffic engineering solution and update the forwarding rules across all switches. This severely degrades application performance. Existing reactive and proactive approaches inevitably lead to transient congestion or bandwidth underutilization and impair the efficiency of running the expensive WANs. We present Sentinel, a novel failure recovery system for traffic engineering in software-defined WANs. Sentinel pre-computes and installs backup tunnels to accelerate failure recovery. When a link fails, switches locally redirect traffic to backup tunnels and recover immediately in the data plane, thus substantially reducing the transient congestion compared to reactive rescaling. On the other hand, Sentinel completely avoids the bandwidth headroom required by existing proactive approaches. Extensive experiments on Mininet and numerical simulations show that similar to state-of-the-art FFC, Sentinel reduces congestion by 45% compared with rescaling, and its algorithm runs much faster than FFC. Sentinel only introduces a small number of additional forwarding rules and can be readily implemented on today's Openflow switches."}}
{"id": "qr8QrYE-1h", "cdate": 1546300800000, "mdate": null, "content": {"title": "Tyrus: PHY-Assisted Neural Adaptive Congestion Control for Cellular Networks", "abstract": ""}}
{"id": "pH6fzO-JVNz", "cdate": 1546300800000, "mdate": null, "content": {"title": "C2Net: A Network-Efficient Approach to Collision Counting LSH Similarity Join", "abstract": "Similarity join of two datasets <inline-formula><tex-math notation=\"LaTeX\">$P$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math></inline-formula> is a primitive operation that is useful in many application domains. The operation involves identifying pairs <inline-formula><tex-math notation=\"LaTeX\">$(p,q)$</tex-math></inline-formula> , in the Cartesian product of <inline-formula><tex-math notation=\"LaTeX\">$P$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math></inline-formula> such that <inline-formula><tex-math notation=\"LaTeX\">$(p,q)$</tex-math></inline-formula> satisfies a stipulated similarity condition. In a high-dimensional space, an approximate similarity join based on locality-sensitive hashing (LSH) provides a good solution while reducing the processing cost with a predictable loss of accuracy. A distributed processing framework such as MapReduce allows the handling of large and high-dimensional datasets. However, network cost estimation frequently turns into a bottleneck in a distributed processing environment, thus resulting in a challenge of achieving faster and more efficient similarity join. This paper focuses on collision counting LSH-based similarity join in MapReduce and proposes a network-efficient solution called C2Net to improve the utilization of MapReduce combiners. The solution uses two graph partitioning schemes: (i) <i>minimum spanning tree</i> for organizing LSH buckets replication; and (ii) <i>spectral clustering</i> for runtime collision counting task scheduling. Experiments have shown that, in comparison to the state of the art, the proposed solution is able to achieve 20 percent data reduction and 50 percent reduction in shuffle time."}}
{"id": "oeRUpZv3_pP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Luopan: Sampling-Based Load Balancing in Data Center Networks", "abstract": "Data center networks demand high-performance, robust, and practical data plane load balancing protocols. Despite progress, existing work falls short of meeting these requirements. We design, analyze, and evaluate Luopan, a novel sampling based load balancing protocol that overcomes these challenges. Luopan operates at flowcell granularity similar to Presto. It periodically samples a few paths for each destination switch and directs flowcells to the least congested one. By being congestion-aware, Luopan improves flow completion time (FCT), and is more robust to topological asymmetries compared to Presto. The sampling approach simplifies the protocol and makes it much more scalable for implementation in large-scale networks compared to existing congestion-aware schemes. We provide analysis to show that Luopan's periodic sampling has the same asymptotic behavior as instantaneous sampling: taking 2 random samples provides exponential improvements over 1 sample. We conduct comprehensive packet-level simulations with production workloads. The results show that Luopan consistently outperforms state-of-the-art schemes in large-scale topologies. Compared to Presto, Luopan with 2 samples improves the 99.9%ile FCT of mice flows by up to 35 percent, and average FCT of medium and elephant flows by up to 30 percent. Luopan also performs significantly better than Local Sampling with large asymmetry."}}
