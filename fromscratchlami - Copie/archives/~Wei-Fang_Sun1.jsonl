{"id": "Vm3wd9XOgZ", "cdate": 1672531200000, "mdate": 1681886059272, "content": {"title": "COLAB: Collaborative and Efficient Processing of Replicated Cache Requests in GPU", "abstract": "In this work, we aim to capture replicated cache requests between Stream Multiprocessors (SMs) within an SM cluster to alleviate the Network-on-Chip (NoC) congestion problem of modern GPUs. To achieve this objective, we incorporate a per-cluster Cache line Ownership Lookup tABle (COLAB) that keeps track of which SM within a cluster holds a copy of a specific cache line. With the assistance of COLAB, SMs can collaboratively and efficiently process replicated cache requests within SM clusters by redirecting them according to the ownership information stored in COLAB. By servicing replicated cache requests within SM clusters that would otherwise consume precious NoC bandwidth, the heavy pressure on the NoC interconnection can be eased. Our experimental results demonstrate that the adoption of COLAB can indeed alleviate the excessive NoC pressure caused by replicated cache requests, and improve the overall system throughput of the baseline GPU while incurring minimal overhead. On average, COLAB can reduce 38% of the NoC traffic and improve instructions per cycle (IPC) by 43%."}}
{"id": "Rxz3KShR8FB", "cdate": 1672531200000, "mdate": 1695954866753, "content": {"title": "A Unified Framework for Factorizing Distributional Value Functions for Multi-Agent Reinforcement Learning", "abstract": "In fully cooperative multi-agent reinforcement learning (MARL) settings, environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of other agents. To address the above issues, we proposed a unified framework, called DFAC, for integrating distributional RL with value function factorization methods. This framework generalizes expected value function factorization methods to enable the factorization of return distributions. To validate DFAC, we first demonstrate its ability to factorize the value functions of a simple matrix game with stochastic rewards. Then, we perform experiments on all Super Hard maps of the StarCraft Multi-Agent Challenge and six self-designed Ultra Hard maps, showing that DFAC is able to outperform a number of baselines."}}
{"id": "ALuRpkAeQP", "cdate": 1663849810487, "mdate": null, "content": {"title": "Quasi-Conservative Score-based Generative Models", "abstract": "Existing Score-based Generative Models (SGMs) can be categorized into constrained SGMs (CSGMs) or unconstrained SGMs (USGMs) according to their parameterization approaches. CSGMs model the probability density functions as Boltzmann distributions, and assign their predictions as the negative gradients of some scalar-valued energy functions. On the other hand, USGMs employ flexible architectures capable of directly estimating scores without the need to explicitly model energy functions. In this paper, we demonstrate that the architectural constraints of CSGMs may limit their score-matching ability. In addition, we show that USGMs' inability to preserve the property of conservativeness may lead to serious sampling inefficiency and degraded sampling performance in practice. To address the above issues, we propose Quasi-Conservative Score-based Generative Models (QCSGMs) for keeping the advantages of both CSGMs and USGMs. Our theoretical derivations demonstrate that the training objective of QCSGMs can be efficiently integrated into the training processes by leveraging the Hutchinson trace estimator. In addition, our experimental results on the Cifar-10, Cifar-100, ImageNet, and SVHN datasets validate the effectiveness of QCSGMs. Finally, we justify the advantage of QCSGMs using an example of a one-layered autoencoder."}}
{"id": "ynm4g25hlp", "cdate": 1640995200000, "mdate": 1681707895471, "content": {"title": "Denoising Likelihood Score Matching for Conditional Score-based Data Generation", "abstract": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we theoretically formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidences show that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated."}}
{"id": "LcF-EEt8cCC", "cdate": 1632875427773, "mdate": null, "content": {"title": "Denoising Likelihood Score Matching for Conditional Score-based Data Generation", "abstract": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we theoretically formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidences show that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated."}}
{"id": "tNuXw62ajIS", "cdate": 1609459200000, "mdate": 1683880031008, "content": {"title": "Critique of \"Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility\" by SCC Team From National Tsing Hua University", "abstract": "As a special activity of the Student Cluster Competition at SC19 conference, we made an attempt to reproduce the scalability evaluations of a highly paralleled polynomial filtering eigensolver for computing planetary interior normal modes. Our experiments were conducted on a Mars dataset using a small scale 4-node cluster with Intel Skylake CPU architecture, while the original article\u2019s were conducted on a Moon dataset using a large scale 256-node supercomputer with Intel CPU Skylake and KNL architectures. This article shares our experiences and observations from our reproducibility activity and discusses our findings on three main sections: the weak scalability, the strong scalability, and the relationships between variables. The results of weak scalability and strong scalability were successfully reproduced. But due to the differences on the problem scale, input dataset, and system architecture, different behaviors regarding the polynomial degree were observed."}}
{"id": "eIdd2kCiynD", "cdate": 1609459200000, "mdate": 1683880030961, "content": {"title": "A Distributional Perspective on Value Function Factorization Methods for Multi-Agent Reinforcement Learning", "abstract": ""}}
{"id": "4PEYCjQ8A0", "cdate": 1609459200000, "mdate": 1683880030923, "content": {"title": "DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning", "abstract": "In fully cooperative multi-agent reinforcement learning (MARL) settings, the environments are highly stochastic due to the partial observability of each agent and the continuously changing policies..."}}
{"id": "JXMJWWoUtx", "cdate": 1577836800000, "mdate": 1683880031043, "content": {"title": "Quatros: a preemptive multithreaded embedded OS for education", "abstract": "Hands-on experience is crucial to truly understanding the principles of operating systems (OS). This paper describes a preemptive, multithreaded embedded OS assigned as a project in a junior-level OS course. It targets the popular 8051 instruction set architecture (ISA) and can run with as little as 128 bytes of RAM and 1 KB of program memory with preemption and synchronization primitives such as semaphores. It is written in C and assembly and compiled using the open-source Small Device C Compiler (SDCC). It runs on the free EdSim51 simulator, which simulates common peripherals such as LCD, keypads, 7-segment LEDs, ADC, and UART all in high fidelity. This course project has received positive feedback from students who took the course."}}
