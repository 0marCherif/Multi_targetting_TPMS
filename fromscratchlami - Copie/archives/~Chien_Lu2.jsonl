{"id": "Fh9l_pVsBfv", "cdate": 1652737652348, "mdate": null, "content": {"title": "Gaussian Copula Embeddings", "abstract": "Learning latent vector representations via embedding models has been shown promising in machine learning. However, most of the embedding models are still limited to a single type of observation data. We propose a Gaussian copula embedding model to learn latent vector representations of items in a heterogeneous data setting. The proposed model can effectively incorporate different types of observed data and, at the same time, yield robust embeddings. We demonstrate the proposed model can effectively learn in many different scenarios, outperforming competing models in modeling quality and task performance."}}
{"id": "rGffMdIi9g9", "cdate": 1646077545898, "mdate": null, "content": {"title": "Nonparametric Exponential Family Graph Embeddings for Multiple Representation Learning", "abstract": "In graph data, each node often serves multiple functionalities. However, most graph embedding models assume that each node can only possess one representation. We address this issue by proposing a nonparametric graph embedding model. The model allows each node to learn multiple representations where they are needed to represent the complexity of random walks in the graph. It extends the Exponential family graph embedding model with two nonparametric prior settings, the Dirichlet process, and the uniform process. The model combines the ability of Exponential family graph embedding to take the number of occurrences of context nodes into account with nonparametric priors giving it the flexibility to learn more than one latent representation for each node. The learned embedding outperforms other state-of-the-art approaches in node classification and link prediction tasks."}}
