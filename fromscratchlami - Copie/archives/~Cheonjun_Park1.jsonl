{"id": "E9_04otJ62", "cdate": 1663849802746, "mdate": null, "content": {"title": "Winograd Structured Pruning for Fast Winograd Convolution ", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits deployment into mobile devices.  \nTo minimize operation counts in CNNs, pruning optimization techniques and Winograd\u2019s minimal filtering algorithm are widely used; however, the benefit of pruning disappears when both optimizations are simply applied together in CNN. \nTo take full advantage of both approaches, two previous pruning methods were proposed: one is to apply pruning after kernel transformation, and the other is applying filter pruning on Winograd convolution. \nUnfortunately, the first method is hardware-unfriendly and the second approach suffers from a significant loss of accuracy.\nThus, we propose structured pruning method specialized for Winograd convolution, that maximizes the hardware utilization by considering the conversion algorithm of parallel processors. \nWe analyze the conversion algorithm of Winograd convolution on parallel processing units; then, we prune the weights in the Winograd-domain in a structured form with optimized pruning unit size, which maximizes the parallelism of the hardware while minimizing the loss of accuracy. \nFor VGG-16 on the ImageNet dataset, the inference time of our method is $1.84$ and $2.89$ times better than previous two pruning methods with less than $1\\%$ accuracy loss."}}
