{"id": "Zexi4-zg7H_", "cdate": 1648692100750, "mdate": 1648692100750, "content": {"title": "Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation", "abstract": "While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers \"red\" to \"What color is the balloon?\", it might answer \"no\" if asked, \"Is the balloon red?\". These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon's color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA's answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the ConVQA datasets and is a strong baseline for further research."}}
{"id": "nmt6LZqSqX", "cdate": 1623615590652, "mdate": 1623615590652, "content": {"title": "Knowing What VQA Does Not: Pointing to Error-Inducing Regions to Improve Explanation Helpfulness", "abstract": "Attention maps, a popular heatmap-based explanation method for Visual Question Answering (VQA), are supposed to help users understand the model by highlighting portions of the image/question used by the model to infer answers. However, we see that users are often misled by current attention map visualizations that point to relevant regions despite the model producing an incorrect answer. Hence, we propose Error Maps that clarify the error by highlighting image regions where the model is prone to err. Error maps can indicate when a correctly attended region may be processed incorrectly leading to an incorrect answer, and hence, improve users' understanding of those cases. To evaluate our new explanations, we further introduce a metric that simulates users' interpretation of explanations to evaluate their potential helpfulness to understand model correctness. We finally conduct user studies to see that our new explanations help users understand model correctness better than baselines by an expected 30% and that our proxy helpfulness metrics correlate strongly (\u03c1>0.97) with how well users can predict model correctness."}}
{"id": "2Qhu1cVSxCt", "cdate": 1623615308279, "mdate": 1623615308279, "content": {"title": "Hybrid Consistency Training with Prototype Adaptation for Few-Shot Learning", "abstract": "Few-Shot Learning (FSL) aims to improve a model's generalization capability in low data regimes. Recent FSL works have made steady progress via metric learning, meta learning, representation learning, etc. However, FSL remains challenging due to the following longstanding difficulties. 1) The seen and unseen classes are disjoint, resulting in a distribution shift between training and testing. 2) During testing, labeled data of previously unseen classes is sparse, making it difficult to reliably extrapolate from labeled support examples to unlabeled query examples. To tackle the first challenge, we introduce Hybrid Consistency Training to jointly leverage interpolation consistency, including interpolating hidden features, that imposes linear behavior locally and data augmentation consistency that learns robust embeddings against sample variations. As for the second challenge, we use unlabeled examples to iteratively normalize features and adapt prototypes, as opposed to commonly used one-time update, for more reliable prototype-based transductive inference. We show that our method generates a 2% to 5% improvement over the state-of-the-art methods with similar backbones on five FSL datasets and, more notably, a 7% to 8% improvement for more challenging cross-domain FSL.\n"}}
{"id": "kB5uBWeg0UL", "cdate": 1623615126426, "mdate": 1623615126426, "content": {"title": "Training Deep Spiking Neural Networks", "abstract": "Computation using brain-inspired spiking neural networks (SNNs) with neuromorphic hardware may offer orders of magnitude higher energy efficiency compared to the current analog neural networks (ANNs). Unfortunately, training SNNs with the same number of layers as state of the art ANNs remains a challenge. To our knowledge the only method which is successful in this regard is supervised training of ANN and then converting it to SNN. In this work we directly train deep SNNs using backpropagation with surrogate gradient and find that due to implicitly recurrent nature of feed forward SNN's the exploding or vanishing gradient problem severely hinders their training. We show that this problem can be solved by tuning the surrogate gradient function. We also propose using batch normalization from ANN literature on input currents of SNN neurons. Using these improvements we show that is is possible to train SNN with ResNet50 architecture on CIFAR100 and Imagenette object recognition datasets. The trained SNN falls behind in accuracy compared to analogous ANN but requires several orders of magnitude less inference time steps (as low as 10) to reach good accuracy compared to SNNs obtained by conversion from ANN which require on the order of 1000 time steps.\n"}}
{"id": "ll-B7tFZa9", "cdate": 1582302989999, "mdate": null, "content": {"title": "Contribution of area MT to perception of three-dimensional shape: a computational study", "abstract": "uccessful recognition and manipulation of objects in one's visual environment is critically dependent upon the ability to recover three-dimensional (3D) surface geometry from two-dimensional (2D) retinal images. The relative motion of image features, caused by relative displacement of object and observer, has characteristic properties that betray components of the 3D source geometry (distance, tilt, slant and curvature) and is among the most valuable sources of information used for 3D surface recovery by the primate visual system. We have considered the behavior of motion-sensitive neurons in primate visual cortex and found that their properties closely resemble those of differential motion operators that can be used to formally characterize the 3D shape of a smooth moving surface. Our analysis has led us to identify a set of three orders of filters for differential motion detection. "}}
{"id": "U6TxzD2v0", "cdate": 1582302837492, "mdate": null, "content": {"title": "Efficient discrimination of temporal patterns by motion-sensitive neurons in primate visual cortex", "abstract": "Although motion-sensitive neurons in macaque middle temporal (MT) area are conventionally characterized using stimuli whose velocity remains constant for 1\u20133 s, many ecologically relevant stimuli change on a shorter time scale (30\u2013300 ms). We compared neuronal responses to conventional (constant-velocity) and time-varying stimuli in alert primates. The responses to both stimulus ensembles were well described as rate-modulated Poisson processes but with very high precision (\u223c3 ms) modulation functions underlying the time-varying responses. Information-theoretic analysis revealed that the responses encoded only \u223c1 bit/s about constant-velocity stimuli but up to 29 bits/s about the time-varying stimuli. Analysis of local field potentials revealed that part of the residual response variability arose from \u201cnoise\u201d sources extrinsic to the neuron. "}}
{"id": "PBXiqgfBy3", "cdate": 1582302744654, "mdate": null, "content": {"title": "Global effects of feature-based attention in human visual cortex", "abstract": "The content of visual experience depends on how selective attention is distributed in the visual field. We used functional magnetic resonance imaging (fMRI) in humans to test whether feature-based attention can globally influence visual cortical responses to stimuli outside the attended location. Attention to a stimulus feature (color or direction of motion) increased the response of cortical visual areas to a spatially distant, ignored stimulus that shared the same feature."}}
{"id": "sfuuktKa2T", "cdate": 1582302478287, "mdate": null, "content": {"title": "Generating Natural Language Explanations for Visual Question Answering using Scene Graphs and Visual Attention", "abstract": "In this paper, we present a novel approach for the task of eXplainable Question Answering (XQA), i.e., generating natural language (NL) explanations for the Visual Question Answering (VQA) problem. We generate NL explanations comprising of the evidence to support the answer to a question asked to an image using two sources of information: (a) annotations of entities in an image (e.g., object labels, region descriptions, relation phrases) generated from the scene graph of the image, and (b) the attention map generated by a VQA model when answering the question. We show how combining the visual attention map with the NL representation of relevant scene graph entities, carefully selected using a language model, can give reasonable textual explanations without the need of any additional collected data (explanation captions, etc). We run our algorithms on the Visual Genome (VG) dataset and conduct internal user-studies to demonstrate the efficacy of our approach over a strong baseline. We have also released a live web demo showcasing our VQA and textual explanation generation using scene graphs and visual attention."}}
{"id": "nU3EqYEVQP", "cdate": 1582302347100, "mdate": null, "content": {"title": "Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval", "abstract": "While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as \"helpful\" significantly improve human performance, \"incorrect\" and \"unhelpful\" explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanations on a human-AI collaborative task."}}
{"id": "clmHIhLjQ9", "cdate": 1582302201885, "mdate": null, "content": {"title": "Sunny and Dark Outside?! Improving Consistency of VQA Models through Entailed Question Generation", "abstract": "While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers \"red\" to \"What color is the balloon?\", it might answer \"no\" if asked, \"Is the balloon red?\". These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon's color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA's answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the ConVQA datasets and is a strong baseline for further research."}}
