{"id": "FILleBqk31S", "cdate": 1663850470285, "mdate": null, "content": {"title": "Do Not Blindly Imitate the Teacher: Loss Perturbation for Knowledge Distillation", "abstract": "Knowledge distillation (KD) is a popular model compression technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. We argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution, and forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss improves the student generalizability by effectively distilling knowledge from a shifted distribution closer to the ground truth data. We also propose a method to compute this shifted teacher distribution, named Proxy Teacher, which enables us to select the perturbation coefficients in PTLoss. We theoretically show the perturbed loss reduces the deviation from the true population risk compared to the vanilla KL-based distillation loss functions. Experiments on three tasks with teachers of different scales show that our method significantly outperforms vanilla distillation loss functions and other perturbation methods."}}
{"id": "zKLz7NWX4a", "cdate": 1640995200000, "mdate": 1668466346146, "content": {"title": "AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models", "abstract": "Yue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang, Chao Zhang. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "uasJXE3_uQ", "cdate": 1640995200000, "mdate": 1674022267636, "content": {"title": "PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning", "abstract": ""}}
{"id": "bMk0HALIhBN", "cdate": 1640995200000, "mdate": 1662015668029, "content": {"title": "Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning", "abstract": ""}}
{"id": "_XUinC65ax", "cdate": 1640995200000, "mdate": 1674022267608, "content": {"title": "Adaptive Multi-view Rule Discovery for Weakly-Supervised Compatible Products Prediction", "abstract": ""}}
{"id": "NVRUW3_wOk", "cdate": 1640995200000, "mdate": 1674022323258, "content": {"title": "Adaptive Multi-view Rule Discovery for Weakly-Supervised Compatible Products Prediction", "abstract": ""}}
{"id": "ERy3DNXGdp", "cdate": 1640995200000, "mdate": 1667631906150, "content": {"title": "Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach", "abstract": "We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \\url{https://github.com/yueyu1030/Patron}."}}
{"id": "9zIwypro75r", "cdate": 1609459200000, "mdate": 1668466346159, "content": {"title": "ATM: An Uncertainty-aware Active Self-training Framework for Label-efficient Text Classification", "abstract": "While pre-trained language model (PLM) fine-tuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose {\\ours}, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. AcTune switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and low-uncertainty ones for model self-training. Under this framework, we design (1) a region-aware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2\\% on average. Our implementation will be available at \\url{https://github.com/yueyu1030/actune}."}}
{"id": "duzqQSBo8-l", "cdate": 1577836800000, "mdate": 1632410001067, "content": {"title": "SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup", "abstract": "Rongzhi Zhang, Yue Yu, Chao Zhang. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "GSrukhPDHbg", "cdate": 1577836800000, "mdate": 1674022267635, "content": {"title": "SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup", "abstract": ""}}
