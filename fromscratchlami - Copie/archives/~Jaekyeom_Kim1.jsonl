{"id": "sWNT5lT7l9G", "cdate": 1652737761990, "mdate": null, "content": {"title": "Constrained GPI for Zero-Shot Transfer in Reinforcement Learning", "abstract": "For zero-shot transfer in reinforcement learning where the reward function varies between different tasks, the successor features framework has been one of the popular approaches. However, in this framework, the transfer to new target tasks with generalized policy improvement (GPI) relies on only the source successor features [5] or additional successor features obtained from the function approximators\u2019 generalization to novel inputs [11]. The goal of this work is to improve the transfer by more tightly bounding the value approximation errors of successor features on the new target tasks. Given a set of source tasks with their successor features, we present lower and upper bounds on the optimal values for novel task vectors that are expressible as linear combinations of source task vectors. Based on the bounds, we propose constrained GPI as a simple test-time approach that can improve transfer by constraining action-value approximation errors on new target tasks. Through experiments in the Scavenger and Reacher environment with state observations as well as the DeepMind Lab environment with visual observations, we show that the proposed constrained GPI significantly outperforms the prior GPI\u2019s transfer performance. Our code and additional information are available at https://jaekyeom.github.io/projects/cgpi/."}}
{"id": "hzP-7_X-GM", "cdate": 1640995200000, "mdate": 1681652243141, "content": {"title": "Lipschitz-constrained Unsupervised Skill Discovery", "abstract": ""}}
{"id": "FSN6ITjvAF", "cdate": 1640995200000, "mdate": 1681652243157, "content": {"title": "Lipschitz-constrained Unsupervised Skill Discovery", "abstract": ""}}
{"id": "BGvt0ghNgA", "cdate": 1632875526414, "mdate": null, "content": {"title": "Lipschitz-constrained Unsupervised Skill Discovery", "abstract": "We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner \u2014 i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/."}}
{"id": "xNmhYNQruJX", "cdate": 1621629999389, "mdate": null, "content": {"title": "Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods", "abstract": "In reinforcement learning, continuous time is often discretized by a time scale $\\delta$, to which the resulting performance is known to be highly sensitive. In this work, we seek to find a $\\delta$-invariant algorithm for policy gradient (PG) methods, which performs well regardless of the value of $\\delta$. We first identify the underlying reasons that cause PG methods to fail as $\\delta \\to 0$, proving that the variance of the PG estimator can diverge to infinity in stochastic environments under a certain assumption of stochasticity. While durative actions or action repetition can be employed to have $\\delta$-invariance, previous action repetition methods cannot immediately react to unexpected situations in stochastic environments. We thus propose a novel $\\delta$-invariant method named Safe Action Repetition (SAR) applicable to any existing PG algorithm. SAR can handle the stochasticity of environments by adaptively reacting to changes in states during action repetition. We empirically show that our method is not only $\\delta$-invariant but also robust to stochasticity, outperforming previous $\\delta$-invariant approaches on eight MuJoCo environments with both deterministic and stochastic settings. Our code is available at https://vision.snu.ac.kr/projects/sar."}}
{"id": "Js3Fj4AuxQ", "cdate": 1609459200000, "mdate": 1681652243138, "content": {"title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration", "abstract": ""}}
{"id": "J4JDaltdCx", "cdate": 1609459200000, "mdate": 1681652243314, "content": {"title": "Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods", "abstract": ""}}
{"id": "Dxzaa2caNyG", "cdate": 1609459200000, "mdate": 1681652243239, "content": {"title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration", "abstract": ""}}
{"id": "AKaQJPwBmk_", "cdate": 1609459200000, "mdate": 1629182074565, "content": {"title": "Unsupervised Skill Discovery with Bottleneck Option Learning", "abstract": "Having the ability to acquire inherent skills from environments without any external rewards or supervision like humans is an important problem. We propose a novel unsupervised skill discovery method named Information Bottleneck Option Learning (IBOL). On top of the linearization of environments that promotes more various and distant state transitions, IBOL enables the discovery of diverse skills. It provides the abstraction of the skills learned with the information bottleneck framework for the options with improved stability and encouraged disentanglement. We empirically demonstrate that IBOL outperforms multiple state-of-the-art unsupervised skill discovery methods on the information-theoretic evaluations and downstream tasks in MuJoCo environments, including Ant, HalfCheetah, Hopper and D'Kitty."}}
{"id": "30bKEjzNrA1", "cdate": 1609459200000, "mdate": 1681652243231, "content": {"title": "Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods", "abstract": ""}}
