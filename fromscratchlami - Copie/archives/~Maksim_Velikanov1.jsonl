{"id": "bzaPGEllsjE", "cdate": 1663849872928, "mdate": null, "content": {"title": "A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions,  benefit from negative momenta.", "abstract": "Mini-batch SGD with momentum is a fundamental algorithm for learning large predictive models. In this paper we develop a new analytic framework to analyze noise-averaged properties of mini-batch SGD for linear models at constant learning rates, momenta and sizes of batches. Our key idea is to consider the dynamics of the second moments of model parameters for a special family of \"Spectrally Expressible\" approximations. This allows to obtain an explicit expression for the generating function of the sequence of loss values. By analyzing this generating function, we find, in particular, that 1) the SGD dynamics exhibits several convergent and divergent regimes depending on the spectral distributions of the problem; 2) the convergent regimes admit explicit stability conditions, and explicit loss asymptotics in the case of power-law spectral distributions; 3) the optimal convergence rate can be achieved at negative momenta. We verify our theoretical predictions by extensive experiments with MNIST and synthetic problems, and find a good quantitative agreement."}}
{"id": "_qhSc7CTjN", "cdate": 1640995200000, "mdate": 1664551095591, "content": {"title": "Embedded Ensembles: infinite width limit and operating regimes", "abstract": "A memory efficient approach to ensembling neural networks is to share most weights among the ensembled models by means of a single reference network. We refer to this strategy as Embedded Ensembling (EE); its particular examples are BatchEnsembles and Monte-Carlo dropout ensembles. In this paper we perform a systematic theoretical and empirical analysis of embedded ensembles with different number of models. Theoretically, we use a Neural-Tangent-Kernel-based approach to derive the wide network limit of the gradient descent dynamics. In this limit, we identify two ensemble regimes - independent and collective - depending on the architecture and initialization strategy of ensemble models. We prove that in the independent regime the embedded ensemble behaves as an ensemble of independent models. We confirm our theoretical prediction with a wide range of experiments with finite networks, and further study empirically various effects such as transition between the two regimes, scaling of ensemble performance with the network width and number of models, and dependence of performance on a number of architecture and hyperparameter choices."}}
{"id": "Q5cKOSRuC7", "cdate": 1640995200000, "mdate": 1680426950082, "content": {"title": "A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta", "abstract": ""}}
{"id": "75SZJ-lGTl", "cdate": 1640995200000, "mdate": 1681650783381, "content": {"title": "Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions", "abstract": ""}}
{"id": "EHUsTBGIP17", "cdate": 1621630321066, "mdate": null, "content": {"title": "Explicit loss asymptotics in the gradient descent training of neural networks", "abstract": "Current theoretical results on optimization trajectories of neural networks trained by gradient descent typically have the form of rigorous but potentially loose bounds on the loss values. In the present work we take a different approach and show that the learning trajectory of a wide network in a lazy training regime can be characterized by an explicit asymptotic at large training times. Specifically, the leading term in the asymptotic expansion of the loss behaves as a power law $L(t) \\sim C t^{-\\xi}$ with exponent $\\xi$ expressed only through the data dimension, the smoothness of the activation function, and the class of function being approximated. Our results are based on spectral analysis of the integral operator representing the linearized evolution of a large network trained on the expected loss. Importantly, the techniques we employ do not require a specific form of the data distribution, for example Gaussian, thus making our findings sufficiently universal.   "}}
{"id": "yMeC4HBF65", "cdate": 1609459200000, "mdate": 1681650796264, "content": {"title": "Universal scaling laws in the gradient descent training of neural networks", "abstract": ""}}
{"id": "36I_y0qWY1", "cdate": 1609459200000, "mdate": 1681650783346, "content": {"title": "Explicit loss asymptotics in the gradient descent training of neural networks", "abstract": ""}}
