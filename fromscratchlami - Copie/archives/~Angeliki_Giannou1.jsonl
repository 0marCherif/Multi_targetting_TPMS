{"id": "VTmRbdNDnV", "cdate": 1695597901549, "mdate": 1695597901549, "content": {"title": "Dissecting Chain-of-Thought: A Study on Compositional In-Context Learning of MLPs", "abstract": "Chain-of-thought (CoT) is a method that enables language models to handle complex reasoning tasks by decomposing them into simpler steps. Despite its success, the underlying mechanics of CoT are not yet fully understood. In an attempt to shed light on this, our study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs). In this setting, we reveal that the success of CoT can be attributed to breaking down in-context learning of a compositional function into two distinct phases: focusing on data related to each step of the composition and in-context learning the single-step composition function. Through both experimental and theoretical evidence, we demonstrate how CoT significantly reduces the sample complexity of in-context learning (ICL) and facilitates the learning of complex functions that non-CoT methods struggle with. Furthermore, we illustrate how transformers can transition from vanilla in-context learning to mastering a compositional function with CoT by simply incorporating an additional layer that performs the necessary filtering for CoT via the attention mechanism. In addition to these test-time benefits, we highlight how CoT accelerates pretraining by learning shortcuts to represent complex functions and how filtering plays an important role in pretraining. These findings collectively provide insights into the mechanics of CoT, inviting further investigation of its role in complex reasoning tasks."}}
{"id": "oJb6kPIjOu", "cdate": 1695597703720, "mdate": 1695597703720, "content": {"title": "The expressive power of tuning only the normalization layers", "abstract": "Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\\sqrt{width})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work."}}
{"id": "f2lX4uFQCLl", "cdate": 1675827743531, "mdate": null, "content": {"title": "LOOPED TRANSFORMERS AS PROGRAMMABLE COMPUTERS", "abstract": "We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches.\nUsing this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms  to programs that   can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can  emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention."}}
{"id": "3V9tRSJLw8", "cdate": 1675191751925, "mdate": 1675191751925, "content": {"title": "Looped Transformers as Programmable Computers", "abstract": "We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches.\nUsing these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms  to programs that   can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can  emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs."}}
{"id": "QedyATtQ1H", "cdate": 1652737467829, "mdate": null, "content": {"title": "On the convergence of policy gradient methods to Nash equilibria in general stochastic games", "abstract": "Learning in stochastic games is a notoriously difficult problem because, in addition to each other's strategic decisions, the players must also contend with the fact that the game itself evolves over time, possibly in a very complicated manner. Because of this, the convergence properties of popular learning algorithms \u2014 like policy gradient and its variants \u2014 are poorly understood, except in specific classes of games (such as potential or two-player, zero-sum games). In view of this, we examine the long-run behavior of policy gradient methods with respect to Nash equilibrium policies that are second-order stationary (SOS) in a sense similar to the type of sufficiency conditions used in optimization. Our first result is that SOS policies are locally attracting with high probability, and we show that policy gradient trajectories with gradient estimates provided by the REINFORCE algorithm achieve an $\\mathcal{O}(1/\\sqrt{n})$ distance-squared convergence rate if the method's step-size is chosen appropriately. Subsequently, specializing to the class of deterministic Nash policies, we show that this rate can be improved dramatically and, in fact, policy gradient methods converge within a finite number of iterations in that case."}}
{"id": "VMUdhfyw91", "cdate": 1627483373131, "mdate": 1627483373131, "content": {"title": "Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information", "abstract": "In this paper, we examine the Nash equilibrium convergence properties of no-regret learning in general N-player games. For concreteness, we focus on the archetypal follow the regularized leader (FTRL) family of algorithms, and we consider the full spectrum of uncertainty that the players may encounter - from noisy, oracle-based feedback, to bandit, payoff-based information. In this general context, we establish a comprehensive equivalence between the stability of a Nash equilibrium and its support: a Nash equilibrium is stable and attracting with arbitrarily high probability if and only if it is strict (i.e., each equilibrium strategy has a unique best response). This equivalence extends existing continuous-time versions of the folk theorem of evolutionary game theory to a bona fide algorithmic learning setting, and it provides a clear refinement criterion for the prediction of the day-to-day behavior of no-regret learning in games"}}
{"id": "8IiakjcUHFH", "cdate": 1621629951885, "mdate": null, "content": {"title": "On the Rate of Convergence of Regularized Learning in Games: From Bandits and Uncertainty to Optimism and Beyond", "abstract": "In this paper, we examine the convergence rate of a wide range of regularized methods for learning in games. To that end, we propose a unified algorithmic template that we call \u201cfollow the generalized leader\u201d (FTGL), and which includes as\nspecial cases the canonical \u201cfollow the regularized leader\u201d algorithm, its optimistic variants, extra-gradient schemes, and many others. The proposed framework is also sufficiently flexible to account for several different feedback models \u2013 from\nfull information to bandit feedback. In this general setting, we show that FTGL algorithms converge locally to strict Nash equilibria at a rate which does not depend on the level of uncertainty faced by the players, but only on the geometry of the regularizer near the equilibrium. In particular, we show that algorithms based on entropic regularization \u2013 like the exponential weights algorithm \u2013 enjoy a linear convergence rate, while Euclidean projection methods converge to equilibrium in a finite number of iterations, even with bandit feedback."}}
