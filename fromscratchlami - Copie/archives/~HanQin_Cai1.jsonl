{"id": "surCGuyFA47", "cdate": 1709251200000, "mdate": 1709275435675, "content": {"title": "Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruptions", "abstract": ""}}
{"id": "xnI-tbRNLPx", "cdate": 1704067200000, "mdate": 1707867446298, "content": {"title": "On the Robustness of Cross-Concentrated Sampling for Matrix Completion", "abstract": "Matrix completion is one of the crucial tools in modern data science research. Recently, a novel sampling model for matrix completion coined cross-concentrated sampling (CCS) has caught much attention. However, the robustness of the CCS model against sparse outliers remains unclear in the existing studies. In this paper, we aim to answer this question by exploring a novel Robust CCS Completion problem. A highly efficient non-convex iterative algorithm, dubbed Robust CUR Completion (RCURC), is proposed. The empirical performance of the proposed algorithm, in terms of both efficiency and robustness, is verified in synthetic and real datasets."}}
{"id": "GeC6HDKLfUS", "cdate": 1690848000000, "mdate": 1696180540182, "content": {"title": "Matrix Completion With Cross-Concentrated Sampling: Bridging Uniform Sampling and CUR Sampling", "abstract": "While uniform sampling has been widely studied in the matrix completion literature, CUR sampling approximates a low-rank matrix via row and column samples. Unfortunately, both sampling models lack flexibility for various circumstances in real-world applications. In this work, we propose a novel and easy-to-implement sampling strategy, coined Cross-Concentrated Sampling (CCS). By bridging uniform sampling and CUR sampling, CCS provides extra flexibility that can potentially save sampling costs in applications. In addition, we also provide a sufficient condition for CCS-based matrix completion. Moreover, we propose a highly efficient non-convex algorithm, termed Iterative CUR Completion (ICURC), for the proposed CCS model. Numerical experiments verify the empirical advantages of CCS and ICURC against uniform sampling and its baseline algorithms, on both synthetic and real-world datasets."}}
{"id": "1oy9Pp3B_Ev", "cdate": 1685577600000, "mdate": 1707867446305, "content": {"title": "Structured Gradient Descent for Fast Robust Low-Rank Hankel Matrix Completion", "abstract": "We study the robust matrix completion problem for the low-rank Hankel matrix, which detects the sparse corruptions caused by extreme outliers while we try to recover the original Hankel matrix from partial observation. In this paper, we explore the convenient Hankel structure and propose a novel nonconvex algorithm, coined Hankel structured gradient descent (HSGD), for large-scale robust Hankel matrix completion problems. HSGD is highly computing- and sample-efficient compared to the state of the art. The recovery guarantee with a linear convergence rate has been established for HSGD under some mild assumptions. The empirical advantages of HSGD are verified on both synthetic datasets and real-world nuclear magnetic resonance signals."}}
{"id": "tp4BY0LW1z", "cdate": 1684287846057, "mdate": 1684287846057, "content": {"title": "Accelerated Alternating Projections for Robust Principal Component Analysis", "abstract": "We study robust PCA for the fully observed setting, which is about separating a low rank matrix $L$ and a sparse matrix $S$ from their sum $D=L+S$. In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which significantly improves the computational efficiency of the existing alternating projections proposed in [Netrapalli, Praneeth, et al., 2014] when updating the low rank factor. The acceleration is achieved by first projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-the-art algorithms for robust PCA."}}
{"id": "yEA5ZCHkyU", "cdate": 1672531200000, "mdate": 1681695110614, "content": {"title": "Non-convex approaches for low-rank tensor completion under tubal sampling", "abstract": "Tensor completion is an important problem in modern data analysis. In this work, we investigate a specific sampling strategy, referred to as tubal sampling. We propose two novel non-convex tensor completion frameworks that are easy to implement, named tensor $L_1$-$L_2$ (TL12) and tensor completion via CUR (TCCUR). We test the efficiency of both methods on synthetic data and a color image inpainting problem. Empirical results reveal a trade-off between the accuracy and time efficiency of these two methods in a low sampling ratio. Each of them outperforms some classical completion methods in at least one aspect."}}
{"id": "I9I9nb4DQ9g", "cdate": 1672531200000, "mdate": 1707867446336, "content": {"title": "Non-Convex Approaches for Low-Rank Tensor Completion under Tubal Sampling", "abstract": "Tensor completion is an important problem in modern data analysis. In this work, we investigate a specific sampling strategy, referred to as tubal sampling. We propose two novel non-convex tensor completion frameworks that are easy to implement, named tensor L <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</inf> -L <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</inf> (TL12) and tensor completion via CUR (TCCUR). We test the efficiency of both methods on synthetic data and a color image inpainting problem. Empirical results reveal a trade-off between the accuracy and time efficiency of these two methods in a low sampling ratio. Each of them outperforms some classical completion methods in at least one aspect."}}
{"id": "HsATGdPYXx_", "cdate": 1672531200000, "mdate": 1696180540189, "content": {"title": "Towards Constituting Mathematical Structures for Learning to Optimize", "abstract": "Learning to Optimize (L2O), a technique that utilizes machine learning to learn an optimization algorithm automatically from data, has gained arising attention in recent years. A generic L2O approach parameterizes the iterative update rule and learns the update direction as a black-box network. While the generic approach is widely applicable, the learned model can overfit and may not generalize well to out-of-distribution test sets. In this paper, we derive the basic mathematical conditions that successful update rules commonly satisfy. Consequently, we propose a novel L2O model with a mathematics-inspired structure that is broadly applicable and generalized well to out-of-distribution problems. Numerical simulations validate our theoretical findings and demonstrate the superior empirical performance of the proposed L2O model."}}
{"id": "G1LfFGvwru1", "cdate": 1672531200000, "mdate": 1696180540127, "content": {"title": "Towards Constituting Mathematical Structures for Learning to Optimize", "abstract": "Learning to Optimize (L2O), a technique that utilizes machine learning to learn an optimization algorithm automatically from data, has gained arising attention in recent years. A generic L2O approa..."}}
{"id": "4Hq0GnJyayj", "cdate": 1672531200000, "mdate": 1684114394154, "content": {"title": "Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruption", "abstract": "We study the tensor robust principal component analysis (TRPCA) problem, a tensorial extension of matrix robust principal component analysis (RPCA), that aims to split the given tensor into an underlying low-rank component and a sparse outlier component. This work proposes a fast algorithm, called Robust Tensor CUR Decompositions (RTCUR), for large-scale non-convex TRPCA problems under the Tucker rank setting. RTCUR is developed within a framework of alternating projections that projects between the set of low-rank tensors and the set of sparse tensors. We utilize the recently developed tensor CUR decomposition to substantially reduce the computational complexity in each projection. In addition, we develop four variants of RTCUR for different application settings. We demonstrate the effectiveness and computational advantages of RTCUR against state-of-the-art methods on both synthetic and real-world datasets."}}
