{"id": "gie9uvNX9J", "cdate": 1688575933589, "mdate": 1688575933589, "content": {"title": "Medical Dialogue Response Generation with Pivotal Information Recalling", "abstract": "Medical dialogue generation is an important yet challenging task. Most previous works rely on the attention mechanism and large-scale pretrained language models. However, these methods often fail to acquire pivotal information from the long dialogue history to yield an accurate and informative response, due to the fact that the medical entities usually scatters throughout multiple utterances along with the complex relationships between them. To mitigate this problem, we propose a medical response generation model with Pivotal Information Recalling (MedPIR), which is built on two components, i.e., knowledge-aware dialogue graph encoder and recall-enhanced generator. The knowledge-aware dialogue graph encoder constructs a dialogue graph by exploiting the knowledge relationships between entities in the utterances, and encodes it with a graph attention network. Then, the recall-enhanced generator strengthens the usage of these pivotal information by generating a summary of the dialogue before producing the actual response. Experimental results on two large-scale medical dialogue datasets show that MedPIR outperforms the strong baselines in BLEU scores and medical entities F1 measure."}}
{"id": "9odhuWulun", "cdate": 1688575766947, "mdate": 1688575766947, "content": {"title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks", "abstract": "Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) -- it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 -> 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5. Our code and datasets are available at https://github. com/uclnlp/EMAT."}}
{"id": "xz5R2jeZOq", "cdate": 1640995200000, "mdate": 1682348116033, "content": {"title": "Medical Dialogue Response Generation with Pivotal Information Recalling", "abstract": "Medical dialogue generation is an important yet challenging task. Most previous works rely on the attention mechanism and large-scale pretrained language models. However, these methods often fail to acquire pivotal information from the long dialogue history to yield an accurate and informative response, due to the fact that the medical entities usually scatters throughout multiple utterances along with the complex relationships between them. To mitigate this problem, we propose a medical response generation model with Pivotal Information Recalling (MedPIR), which is built on two components, i.e., knowledge-aware dialogue graph encoder and recall-enhanced generator. The knowledge-aware dialogue graph encoder constructs a dialogue graph by exploiting the knowledge relationships between entities in the utterances, and encodes it with a graph attention network. Then, the recall-enhanced generator strengthens the usage of these pivotal information by generating a summary of the dialogue before producing the actual response. Experimental results on two large-scale medical dialogue datasets show that MedPIR outperforms the strong baselines in BLEU scores and medical entities F1 measure."}}
{"id": "xirtqwNRzZI", "cdate": 1640995200000, "mdate": 1682348116164, "content": {"title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets", "abstract": ""}}
{"id": "rrQYoqJNytW", "cdate": 1640995200000, "mdate": 1682348116165, "content": {"title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks", "abstract": ""}}
{"id": "qYfiBdJvNV", "cdate": 1640995200000, "mdate": 1682348116238, "content": {"title": "DHA: Product Title Generation with Discriminative Hierarchical Attention for E-commerce", "abstract": "Product titles play an important role in E-Commerce sites. However, manually crafting product titles needs tremendous time and human effort. It is expected that product titles can be automatically generated, but existing generation methods usually require densely labeled data that are unavailable in the real world. We formulate a novel product title generation task that generates the title from the image and auxiliary information (e.g., category) to address the gap. To generate titles that are consistent with search queries, we construct the first large-scale dataset (AEPro) and propose a Discriminative Hierarchical Attention (DHA) model. The DHA model first identifies the image regions related to the product of interest (POI) with a POI attention module. Then, based on the title context, the identified image regions are further revised by a generation attention module. Finally, the titles are generated by dynamically attending to these image regions. Experiments on the AEPro dataset demonstrate the effectiveness of the DHA model. Besides, online A/B testing results show that $$61.8\\%$$ of the titles generated by the DHA model are accepted directly or with minor modifications. The exposure rate of the products with machine-generated titles is improved by $$40\\%$$ ."}}
{"id": "oKkf-ZZrW5", "cdate": 1640995200000, "mdate": 1665054270500, "content": {"title": "Towards Fine-grained Causal Reasoning and QA", "abstract": "Understanding causality is key to the success of NLP applications, especially in high-stakes domains. Causality comes in various perspectives such as enable and prevent that, despite their importance, have been largely ignored in the literature. This paper introduces a novel fine-grained causal reasoning dataset and presents a series of novel predictive tasks in NLP, such as causality detection, event causality extraction, and Causal QA. Our dataset contains human annotations of 25K cause-effect event pairs and 24K question-answering pairs within multi-sentence samples, where each can have multiple causal relationships. Through extensive experiments and analysis, we show that the complex relations in our dataset bring unique challenges to state-of-the-art methods across all three tasks and highlight potential research opportunities, especially in developing \"causal-thinking\" methods."}}
{"id": "lkglp_qaMc", "cdate": 1640995200000, "mdate": 1682348116170, "content": {"title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets", "abstract": "Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard."}}
{"id": "UmhRmWdanoI", "cdate": 1640995200000, "mdate": 1682348116244, "content": {"title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks", "abstract": "Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) -- it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 -> 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5. Our code and datasets are available at https://github. com/uclnlp/EMAT."}}
{"id": "AJnFvMOmGc", "cdate": 1640995200000, "mdate": 1682348116167, "content": {"title": "Medical Dialogue Response Generation with Pivotal Information Recalling", "abstract": "Medical dialogue generation is an important yet challenging task. Most previous works rely on the attention mechanism and large-scale pretrained language models. However, these methods often fail to acquire pivotal information from the long dialogue history to yield an accurate and informative response, due to the fact that the medical entities usually scatters throughout multiple utterances along with the complex relationships between them. To mitigate this problem, we propose a medical response generation model with Pivotal Information Recalling (MedPIR), which is built on two components, i.e., knowledge-aware dialogue graph encoder and recall-enhanced generator. The knowledge-aware dialogue graph encoder constructs a dialogue graph by exploiting the knowledge relationships between entities in the utterances, and encodes it with a graph attention network. Then, the recall-enhanced generator strengthens the usage of these pivotal information by generating a summary of the dialogue before producing the actual response. Experimental results on two large-scale medical dialogue datasets show that MedPIR outperforms the strong baselines in BLEU scores and medical entities F1 measure."}}
