{"id": "w21o7AXrwgq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Safe Policy Improvement for Non-Stationary MDPs", "abstract": "Many real-world sequential decision-making problems involve critical systems with financial risks and human-life risks. While several works in the past have proposed methods that are safe for deployment, they assume that the underlying problem is stationary. However, many real-world problems of interest exhibit non-stationarity, and when stakes are high, the cost associated with a false stationarity assumption may be unacceptable. We take the first steps towards ensuring safety, with high confidence, for smoothly-varying non-stationary decision problems. Our proposed method extends a type of safe algorithm, called a Seldonian algorithm, through a synthesis of model-free reinforcement learning with time-series analysis. Safety is ensured using sequential hypothesis testing of a policy\u2019s forecasted performance, and confidence intervals are obtained using wild bootstrap."}}
{"id": "iLQCdJQrhPt", "cdate": 1577836800000, "mdate": null, "content": {"title": "Evaluating the Performance of Reinforcement Learning Algorithms", "abstract": "Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks."}}
{"id": "FDuS6puwuH", "cdate": 1577836800000, "mdate": null, "content": {"title": "Evaluating the Performance of Reinforcement Learning Algorithms", "abstract": "Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsist..."}}
{"id": "3cIWu-ZPBA", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Safe Policy Improvement for Non-Stationary MDPs", "abstract": "Many real-world sequential decision-making problems involve critical systems with financial risks and human-life risks. While several works in the past have proposed methods that are safe for deployment, they assume that the underlying problem is stationary. However, many real-world problems of interest exhibit non-stationarity, and when stakes are high, the cost associated with a false stationarity assumption may be unacceptable. We take the first steps towards ensuring safety, with high confidence, for smoothly-varying non-stationary decision problems. Our proposed method extends a type of safe algorithm, called a Seldonian algorithm, through a synthesis of model-free reinforcement learning with time-series analysis. Safety is ensured using sequential hypothesis testing of a policy's forecasted performance, and confidence intervals are obtained using wild bootstrap."}}
{"id": "rkWCLs-_Wr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning Action Representations for Reinforcement Learning", "abstract": "Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provide..."}}
{"id": "PASTZGgoZYe", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning a Better Negative Sampling Policy with Deep Neural Networks for Search", "abstract": "In information retrieval, sampling methods used to select documents for neural models must often deal with large class imbalances during training. This issue necessitates careful selection of negative instances when training neural models to avoid the risk of overfitting. For most work, heuristic sampling approaches, or policies, are created based off of domain experts, such as choosing samples with high BM25 scores or a random process over candidate documents. However, these sampling approaches are done with the test distribution in mind. In this paper, we demonstrate that the method chosen to sample negative documents during training plays a critical role in both the stability of training, as well as overall performance. Furthermore, we establish that using reinforcement learning to optimize a policy over a set of sampling functions can significantly improve performance over standard training practices with respect to IR metrics and is robust to hyperparameters and random seeds."}}
{"id": "OmUMWaqeRe6", "cdate": 1546300800000, "mdate": null, "content": {"title": "Classical Policy Gradient: Preserving Bellman's Principle of Optimality", "abstract": "We propose a new objective function for finite-horizon episodic Markov decision processes that better captures Bellman's principle of optimality, and provide an expression for the gradient of the objective."}}
{"id": "OpKXSwPP4fr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Distributed Evaluations: Ending Neural Point Metrics", "abstract": "With the rise of neural models across the field of information retrieval, numerous publications have incrementally pushed the envelope of performance for a multitude of IR tasks. However, these networks often sample data in random order, are initialized randomly, and their success is determined by a single evaluation score. These issues are aggravated by neural models achieving incremental improvements from previous neural baselines, leading to multiple near state of the art models that are difficult to reproduce and quickly become deprecated. As neural methods are starting to be incorporated into low resource and noisy collections that further exacerbate this issue, we propose evaluating neural models both over multiple random seeds and a set of hyperparameters within $\\epsilon$ distance of the chosen configuration for a given metric."}}
