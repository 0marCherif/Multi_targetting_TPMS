{"id": "qwkCMA5xHB", "cdate": 1672531200000, "mdate": 1693587398565, "content": {"title": "Federated Learning under Distributed Concept Drift", "abstract": "Federated Learning (FL) under distributed concept drift is a largely unexplored area. Although concept drift is itself a well-studied phenomenon, it poses particular challenges for FL, because drif..."}}
{"id": "obWeb64UmI", "cdate": 1672531200000, "mdate": 1693587398594, "content": {"title": "Enhancing Network Management Using Code Generated by Large Language Models", "abstract": "Analyzing network topologies and communication graphs plays a crucial role in contemporary network management. However, the absence of a cohesive approach leads to a challenging learning curve, heightened errors, and inefficiencies. In this paper, we introduce a novel approach to facilitate a natural-language-based network management experience, utilizing large language models (LLMs) to generate task-specific code from natural language queries. This method tackles the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, eliminating the need to share network data with LLMs, and concentrating on application-specific requests combined with general program synthesis techniques. We design and evaluate a prototype system using benchmark applications, showcasing high accuracy, cost-effectiveness, and the potential for further enhancements using complementary program synthesis techniques."}}
{"id": "XE3BTiKYCO", "cdate": 1672531200000, "mdate": 1693587398532, "content": {"title": "RECL: Responsive Resource-Efficient Continuous Learning for Video Analytics", "abstract": ""}}
{"id": "O6D-ZrwXT1m", "cdate": 1672531200000, "mdate": 1693587398537, "content": {"title": "Mitigating the Performance Impact of Network Failures in Public Clouds", "abstract": "Some faults in data center networks require hours to days to repair because they may need reboots, re-imaging, or manual work by technicians. To reduce traffic impact, cloud providers \\textit{mitigate} the effect of faults, for example, by steering traffic to alternate paths. The state-of-art in automatic network mitigations uses simple safety checks and proxy metrics to determine mitigations. SWARM, the approach described in this paper, can pick orders of magnitude better mitigations by estimating end-to-end connection-level performance (CLP) metrics. At its core is a scalable CLP estimator that quickly ranks mitigations with high fidelity and, on failures observed at a large cloud provider, outperforms the state-of-the-art by over 700$\\times$ in some cases."}}
{"id": "dOvcWRIcLA", "cdate": 1664928780992, "mdate": null, "content": {"title": "Federated Learning under Distributed Concept Drift", "abstract": "Federated Learning (FL) under distributed concept drift is a largely unexplored area. Although concept drift is itself a well-studied phenomenon, it poses particular challenges for FL, because drifts arise staggered in time and space (across clients). Our work is the first to explicitly study data heterogeneity in both dimensions. We first demonstrate that prior solutions to drift adaptation, with their single global model, are ill-suited to staggered drifts, necessitating multiple-model solutions. We identify the problem of drift adaptation as a time-varying clustering problem, and we propose two new clustering algorithms for reacting to drifts based on local drift detection and hierarchical clustering. Empirical evaluation shows that our solutions achieve significantly higher accuracy than existing baselines, and are comparable to an idealized algorithm with oracle knowledge of the ground-truth clustering of clients to concepts at each time step."}}
{"id": "nKLgsONGLO", "cdate": 1640995200000, "mdate": 1682320894698, "content": {"title": "Matchmaker: Data Drift Mitigation in Machine Learning for Large-Scale Systems", "abstract": ""}}
{"id": "dIOU52c9Ve", "cdate": 1640995200000, "mdate": 1682320894716, "content": {"title": "Federated Learning under Distributed Concept Drift", "abstract": "Federated Learning (FL) under distributed concept drift is a largely unexplored area. Although concept drift is itself a well-studied phenomenon, it poses particular challenges for FL, because drifts arise staggered in time and space (across clients). To the best of our knowledge, this work is the first to explicitly study data heterogeneity in both dimensions. We first demonstrate that prior solutions to drift adaptation that use a single global model are ill-suited to staggered drifts, necessitating multiple-model solutions. We identify the problem of drift adaptation as a time-varying clustering problem, and we propose two new clustering algorithms for reacting to drifts based on local drift detection and hierarchical clustering. Empirical evaluation shows that our solutions achieve significantly higher accuracy than existing baselines, and are comparable to an idealized algorithm with oracle knowledge of the ground-truth clustering of clients to concepts at each time step."}}
{"id": "H5WAgpmfXq", "cdate": 1640995200000, "mdate": 1648667894197, "content": {"title": "FedSpace: An Efficient Federated Learning Framework at Satellites and Ground Stations", "abstract": "Large-scale deployments of low Earth orbit (LEO) satellites collect massive amount of Earth imageries and sensor data, which can empower machine learning (ML) to address global challenges such as real-time disaster navigation and mitigation. However, it is often infeasible to download all the high-resolution images and train these ML models on the ground because of limited downlink bandwidth, sparse connectivity, and regularization constraints on the imagery resolution. To address these challenges, we leverage Federated Learning (FL), where ground stations and satellites collaboratively train a global ML model without sharing the captured images on the satellites. We show fundamental challenges in applying existing FL algorithms among satellites and ground stations, and we formulate an optimization problem which captures a unique trade-off between staleness and idleness. We propose a novel FL framework, named FedSpace, which dynamically schedules model aggregation based on the deterministic and time-varying connectivity according to satellite orbits. Extensive numerical evaluations based on real-world satellite images and satellite networks show that FedSpace reduces the training time by 1.7 days (38.6%) over the state-of-the-art FL algorithms."}}
{"id": "-VjuXu9gBp", "cdate": 1640995200000, "mdate": 1682320894735, "content": {"title": "Ekya: Continuous Learning of Video Analytics Models on Edge Compute Servers", "abstract": ""}}
{"id": "reZ7Rla7zmq", "cdate": 1609459200000, "mdate": 1648667894221, "content": {"title": "Interpret-able feedback for AutoML systems", "abstract": "Automated machine learning (AutoML) systems aim to enable training machine learning (ML) models for non-ML experts. A shortcoming of these systems is that when they fail to produce a model with high accuracy, the user has no path to improve the model other than hiring a data scientist or learning ML -- this defeats the purpose of AutoML and limits its adoption. We introduce an interpretable data feedback solution for AutoML. Our solution suggests new data points for the user to label (without requiring a pool of unlabeled data) to improve the model's accuracy. Our solution analyzes how features influence the prediction among all ML models in an AutoML ensemble, and we suggest more data samples from feature ranges that have high variance in such analysis. Our evaluation shows that our solution can improve the accuracy of AutoML by 7-8% and significantly outperforms popular active learning solutions in data efficiency, all the while providing the added benefit of being interpretable."}}
