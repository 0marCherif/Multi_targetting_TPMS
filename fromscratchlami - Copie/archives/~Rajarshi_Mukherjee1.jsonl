{"id": "DvFCKBqj7y", "cdate": 1695257000923, "mdate": 1695257000923, "content": {"title": "Assumption-lean falsification tests of rate double-robustness of double-machine-learning estimators", "abstract": "The class of doubly-robust (DR) functionals studied by Rotnitzky et al. (2021) is of central importance in economics and biostatistics. It strictly includes both (i) the class of mean-square continuous functionals that can be written as an expectation of an affine functional of a conditional expectation studied by Chernozhukov et al. (2022b) and (ii) the class of functionals studied by Robins et al. (2008). The present state-of-the-art estimators for DR functionals \u03c8 are double-machine-learning (DML) estimators (Chernozhukov et al., 2018). A DML estimator \u03c8\u02c61 of \u03c8 depends on estimates p\u02c6(x) and b\u02c6(x) of a pair of nuisance functions p(x) and b(x), and is said to satisfy \"rate double-robustness\" if the Cauchy--Schwarz upper bound of its bias is o(n\u22121/2). Were it achievable, our scientific goal would have been to construct valid, assumption-lean (i.e. no complexity-reducing assumptions on b or p) tests of the validity of a nominal (1\u2212\u03b1) Wald confidence interval (CI) centered at \u03c8\u02c61. But this would require a test of the bias to be o(n\u22121/2), which can be shown not to exist. We therefore adopt the less ambitious goal of falsifying, when possible, an analyst's justification for her claim that the reported (1\u2212\u03b1) Wald CI is valid. In many instances, an analyst justifies her claim by imposing complexity-reducing assumptions on b and p to ensure \"rate double-robustness\". Here we exhibit valid, assumption-lean tests of H0: \"rate double-robustness holds\", with non-trivial power against certain alternatives. If H0 is rejected, we will have falsified her justification. However, no assumption-lean test of H0, including ours, can be a consistent test. Thus, the failure of our test to reject is not meaningful evidence in favor of H0."}}
{"id": "crRhj1Y2wv", "cdate": 1652737776338, "mdate": null, "content": {"title": "Towards a Unified Framework for Uncertainty-aware Nonlinear Variable Selection with Theoretical Guarantees", "abstract": "We develop a simple and unified framework for nonlinear variable importance estimation that incorporates uncertainty in the prediction function and is compatible with a wide range of machine learning models (e.g., tree ensembles, kernel methods, neural networks, etc). In particular, for a learned nonlinear model $f(\\mathbf{x})$, we consider quantifying the importance of an input variable $\\mathbf{x}^j$ using the integrated partial derivative $\\Psi_j = \\Vert \\frac{\\partial}{\\partial \\mathbf{x}^j} f(\\mathbf{x})\\Vert^2_{P_\\mathcal{X}}$. We then (1) provide a principled approach for quantifying uncertainty in variable importance by deriving its posterior distribution, and (2) show that the approach is generalizable even to non-differentiable models such as tree ensembles. Rigorous Bayesian nonparametric theorems are derived to guarantee the posterior consistency and asymptotic uncertainty of the proposed approach. Extensive  simulations and experiments on healthcare benchmark datasets confirm that the proposed algorithm outperforms existing classical and recent variable selection methods. "}}
{"id": "SYB4WrJql1n", "cdate": 1632875540577, "mdate": null, "content": {"title": "On the Existence of Universal Lottery Tickets", "abstract": "The lottery ticket hypothesis conjectures the existence of sparse subnetworks of large randomly initialized deep neural networks that can be successfully trained in isolation. Recent work has experimentally observed that some of these tickets can be practically reused across a variety of tasks, hinting at some form of universality. We formalize this concept and theoretically prove that not only do such universal tickets exist but they also do not require further training. Our proofs introduce a couple of technical innovations related to pruning for strong lottery tickets, including extensions of subset sum results and a strategy to leverage higher amounts of depth. Our explicit sparse constructions of universal function families might be of independent interest, as they highlight representational benefits induced by univariate convolutional architectures. "}}
{"id": "zD7jOts3zDP", "cdate": 1622297224111, "mdate": null, "content": {"title": "On nearly assumption-free tests of nominal confidence interval coverage for causal parameters estimated by machine learning", "abstract": "For many causal effect parameters of interest, doubly robust machine learning (DRML) estimators  are the state-of-the-art, incorporating the good prediction performance of machine learning; the decreased bias of doubly robust estimators; and the analytic tractability and bias reduction of sample splitting with cross-fitting. Nonetheless, even in the absence of confounding by unmeasured factors, the nominal  Wald confidence interval  may still undercover even in large samples, because the bias of  may be of the same or even larger order than its standard error of order ."}}
{"id": "TtzDTnAlEiF", "cdate": 1622296900484, "mdate": null, "content": {"title": "Adaptive estimation of nonparametric functionals", "abstract": "We provide general adaptive upper bounds for estimating nonparametric functionals based on second-order U-statistics arising from finite-dimensional approximation of the infinite-dimensional models. We then provide examples of functionals for which the theory produces rate optimally matching adaptive upper and lower bounds. Our results are automatically adaptive in both para- metric and nonparametric regimes of estimation and are automatically adaptive and semiparametric efficient in the regime of parametric convergence rate."}}
