{"id": "x0_iotVq9ly", "cdate": 1640995200000, "mdate": 1681711843804, "content": {"title": "Online gradient descent algorithms for functional data learning", "abstract": ""}}
{"id": "vZVH6V2nTDv", "cdate": 1640995200000, "mdate": 1681711843838, "content": {"title": "Capacity dependent analysis for functional online learning algorithms", "abstract": "This article provides convergence analysis of online stochastic gradient descent algorithms for functional linear models. Adopting the characterizations of the slope function regularity, the kernel space capacity, and the capacity of the sampling process covariance operator, significant improvement on the convergence rates is achieved. Both prediction problems and estimation problems are studied, where we show that capacity assumption can alleviate the saturation of the convergence rate as the regularity of the target function increases. We show that with properly selected kernel, capacity assumptions can fully compensate for the regularity assumptions for prediction problems (but not for estimation problems). This demonstrates the significant difference between the prediction problems and the estimation problems in functional data analysis."}}
{"id": "PZOJIzK0XY", "cdate": 1609899662263, "mdate": null, "content": {"title": "Sparsity and Error Analysis of Empirical Feature-Based Regularization Schemes", "abstract": "We consider a learning algorithm generated by a regularization scheme with a concave regularizer for the purpose of achieving sparsity and good learning rates in a least squares regression setting. The regularization is induced for linear combinations of empirical features, constructed in the literatures of kernel principal component analysis and kernel projection machines, based on kernels and samples. In addition to the separability of the involved optimization problem caused by the empirical features, we carry out sparsity and error analysis, giving bounds in the norm of the reproducing kernel Hilbert space, based on a priori conditions which do not require assumptions on sparsity in terms of any basis or system. In particular, we show that as the concave exponent q of the concave regularizer increases to 1, the learning ability of the algorithm improves. Some numerical simulations for both artificial and real MHC-peptide binding data involving the \u2113q regularizer and the SCAD penalty are presented to demonstrate the sparsity and error analysis."}}
{"id": "FQ11NQeoYuD", "cdate": 1609899588224, "mdate": null, "content": {"title": "Thresholded spectral algorithms for sparse approximations", "abstract": "Spectral algorithms form a general framework that unifies many regularization schemes in learning theory. In this paper, we propose and analyze a class of thresholded spectral algorithms that are designed based on empirical features. Soft thresholding is adopted to achieve sparse approximations. Our analysis shows that without sparsity assumption of the regression function, the output functions of thresholded spectral algorithms are represented by empirical features with satisfactory sparsity, and the convergence rates are comparable to those of the classical spectral algorithms in the literature."}}
{"id": "hDiPM60uD52", "cdate": 1609899520349, "mdate": null, "content": {"title": "Semi-supervised learning with summary statistics", "abstract": "Nowadays, the extensive collection and analyzing of data is stimulating widespread privacy concerns, and therefore is increasing tensions between the potential sources of data and researchers. A privacy-friendly learning framework can help to ease the tensions, and to free up more data for research. We propose a new algorithm, LESS (Learning with Empirical feature-based Summary statistics from Semi-supervised data), which uses only summary statistics instead of raw data for regression learning. The selection of empirical features serves as a trade-off between prediction precision and the protection of privacy. We show that LESS achieves the minimax optimal rate of convergence in terms of the size of the labeled sample. LESS extends naturally to the applications where data are separately held by different sources. Compared with the existing literature on distributed learning, LESS removes the restriction of minimum sample size on single data sources."}}
{"id": "ZkWLNqv7jHq", "cdate": 1609899398589, "mdate": null, "content": {"title": "Distributed Learning with Regularized Least Squares", "abstract": "We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the L2-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature."}}
{"id": "bgYIn97N9Zd", "cdate": 1609898907896, "mdate": null, "content": {"title": "Distributed Minimum Error Entropy Algorithms", "abstract": "Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-and-conquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with sufficient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an inflation of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators."}}
{"id": "SQuuFTJHN", "cdate": 1609459200000, "mdate": 1681711844020, "content": {"title": "Agreeing to Disagree: Choosing Among Eight Topic-Modeling Methods", "abstract": ""}}
{"id": "LE8-2YZ9N5", "cdate": 1609459200000, "mdate": 1681711843872, "content": {"title": "The uses and abuses of an age-period-cohort method: On the linear algebra and statistical properties of intrinsic and related estimators", "abstract": "As a sophisticated and popular age-period-cohort method, the Intrinsic Estimator (IE) and related estimators have evoked intense debate in demography, sociology, epidemiology and statistics. This study aims to provide a more holistic review and critical assessment of the overall methodological significance of the IE and related estimators in age-period-cohort analysis. We derive the statistical properties of the IE from a linear algebraic perspective, provide more precise mathematical proofs relevant to the current debate, and demonstrate the essential, yet overlooked, link between the IE and classical statistical tools that have been employed by scholars for decades. This study offers guidelines for the future use of the IE and related estimators in demographic research. The exposition of the IE and related estimators may help redirect, if not settle, the logic of the debate."}}
{"id": "yJRK2Ax3QcZ", "cdate": 1577836800000, "mdate": 1681711844049, "content": {"title": "Distributed Minimum Error Entropy Algorithms", "abstract": "Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-and-conquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with sufficient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an inflation of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators."}}
