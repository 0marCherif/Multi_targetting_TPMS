{"id": "EnrY5TOrbQ", "cdate": 1663850173818, "mdate": null, "content": {"title": "Agnostic Learning of General ReLU Activation Using Gradient Descent", "abstract": "We provide a convergence analysis of gradient descent for the problem of agnostically learning a single ReLU function under Gaussian distributions. Unlike prior work that studies the setting of zero bias, we consider the more challenging scenario when the bias of the ReLU function is non-zero. Our main result establishes that starting from random initialization, in a polynomial number of iterations gradient descent outputs, with high probability, a ReLU function that achieves an error that is within a constant factor of the optimal i.e., it is guaranteed to achieve an error of $O(OPT)$, where $OPT$ is the error of the best ReLU function. This is a significant improvement over existing guarantees for gradient descent, which only guarantee  error of $O(\\sqrt{d \\cdot OPT})$ even in the zero-bias case  (Frei et al., 2020). We also provide finite sample guarantees, and obtain similar guarantees for a broader class of marginal distributions beyond Gaussians. "}}
{"id": "g7TXnKjn3Y", "cdate": 1663850173331, "mdate": null, "content": {"title": "Tabular Data to Image Generation: Benchmark Data, Approaches, and Evaluation", "abstract": "In this work, we study the problem of generating a set of images from an arbitrary tabular dataset. The set of generated images provides an intuitive visual summary of the tabular data that can be quickly and easily communicated and understood by the user.\nMore specifically, we formally introduce this new dataset to image generation task and discuss a few motivating applications including exploratory data analysis and understanding customer segments for creating better marketing campaigns. \nWe then curate a benchmark dataset for training such models, which we release publicly for others to use and develop new models for other important applications of interest.\nFurther, we describe a general and flexible framework that serves as a fundamental basis for studying and developing models for this new task of generating images from tabular data.\nFrom the framework, we propose a few different approaches with varying levels of complexity and tradeoffs.\nOne such approach leverages both numerical and textual data as the input to our image generation pipeline. \nThe pipeline consists of an image decoder and a conditional auto-regressive sequence generation model which also includes a pre-trained tabular representation in the input layer.\nWe evaluate the performance of these approaches through several quantitative metrics (FID for image quality and LPIPS scores for image diversity)."}}
{"id": "6Ddt0bvKoeh", "cdate": 1621629934054, "mdate": null, "content": {"title": "Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations", "abstract": "We present polynomial time and sample efficient algorithms for learning an unknown depth-2 feedforward neural network with general ReLU activations, under mild non-degeneracy assumptions. In particular, we consider learning an unknown network of the form $f(x) = {a}^{\\mathsf{T}}\\sigma({W}^\\mathsf{T}x+b)$, where $x$ is drawn from the Gaussian distribution, and $\\sigma(t) = \\max(t,0)$ is the ReLU activation. Prior works for learning networks with ReLU activations assume that the bias ($b$) is zero. \n\nIn order to deal with the presence of the bias terms, our proposed algorithm consists of robustly decomposing multiple higher order tensors arising from the Hermite expansion of the function $f(x)$. Using these ideas we also establish identifiability of the network parameters under very mild assumptions."}}
