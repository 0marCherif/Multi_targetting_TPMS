{"id": "cj4vuS6Sgk", "cdate": 1640995200000, "mdate": 1682760672460, "content": {"title": "DARCAS: Dynamic Association Regulator Considering Airtime Over SDN-Enabled Framework", "abstract": "The massive influx of mobile devices and their increasing use in recent years have resulted in the overprovision of access points (APs) in networks. Unlike in residential environments, network administrators in enterprises and universities make every endeavor to enhance the user experience (UX) of WiFi networks where the network dynamics (e.g., traffic load and user mobility) are usually unexpected. To this end, an existing mechanism for WiFi association is client driven, i.e., users associate themselves to the AP with higher signal strength. However, they still incur dissatisfaction due to the insufficient available bandwidth. To cope with this in a centralized manner, we propose DARCAS, a software-defined network (SDN)-enabled WiFi framework for association regulation. DARCAS adopts a notion of bandwidth satisfaction ratio (BSR), which is closely related to UX. It maximizes the aggregated network throughput while satisfying the BSR of each user with sufficient airtime (i.e., channel occupancy time) provision. We use this idea in a metaheuristic genetic algorithm called DARCAS-GA, which effectively finds the suboptimal association distribution of the maximum BSR in polynomial time. We implement the DARCAS system on off-the-shelf wireless routers and an SDN controller. We report real-life experimental results in the considered scenarios and conduct extensive simulations on the NS-3 simulator to examine its performance with scalability. With fine-tuned settings, DARCAS exhibits up to 80% of the BSR gain compared to existing solutions."}}
{"id": "b8kbrvvqoY", "cdate": 1640995200000, "mdate": 1682760672375, "content": {"title": "Stable Federated Learning with Dataset Condensation", "abstract": "Federated learning (FL) is a new machine learning paradigm, where multiple clients learn their local models to collaboratively integrate into a single global model. Unlike centralized learning, the global model being integrated cannot be tested in FL as the server does not collect any data samples; further, the global model is often sent back and immediately applied to clients even in the middle of training such as Gboard. Therefore, if the performance of the global model is not stable, which is, unfortunately, the case in many FL scenarios with non-IID data, clients can be provided with an inaccurate model. This paper explores the main reason for this training instability of FL, that is, what we call temporary imbalance that happens across rounds, leading to loss of knowledge from previous rounds. To solve this problem, we propose a dataset condensation method to summarize the local data for each client without compromising on privacy. The condensed data are transmitted to the server with the local model and utilized by the server to ensure stable and consistent performance of the global model. Experimental results show that the global model not only achieves training stability but also exhibits a fast convergence speed."}}
{"id": "aVRS7a9Knd", "cdate": 1640995200000, "mdate": 1682760672334, "content": {"title": "Attractive and repulsive training to address inter-task forgetting issues in continual learning", "abstract": ""}}
{"id": "aNPRd8mMQx", "cdate": 1640995200000, "mdate": 1682760672303, "content": {"title": "QueryNet: Querying neural networks for lightweight specialized models", "abstract": ""}}
{"id": "HUuZ4hHj6zE", "cdate": 1640995200000, "mdate": 1682760672418, "content": {"title": "Better Generalized Few-Shot Learning Even Without Base Data", "abstract": "This paper introduces and studies zero-base generalized few-shot learning (zero-base GFSL), which is an extreme yet practical version of few-shot learning problem. Motivated by the cases where base data is not available due to privacy or ethical issues, the goal of zero-base GFSL is to newly incorporate the knowledge of few samples of novel classes into a pretrained model without any samples of base classes. According to our analysis, we discover the fact that both mean and variance of the weight distribution of novel classes are not properly established, compared to those of base classes. The existing GFSL methods attempt to make the weight norms balanced, which we find helps only the variance part, but discard the importance of mean of weights particularly for novel classes, leading to the limited performance in the GFSL problem even with base data. In this paper, we overcome this limitation by proposing a simple yet effective normalization method that can effectively control both mean and variance of the weight distribution of novel classes without using any base samples and thereby achieve a satisfactory performance on both novel and base classes. Our experimental results somewhat surprisingly show that the proposed zero-base GFSL method that does not utilize any base samples even outperforms the existing GFSL methods that make the best use of base data. Our implementation is available at: https://github.com/bigdata-inha/Zero-Base-GFSL."}}
{"id": "t33xm8dpwL", "cdate": 1609459200000, "mdate": 1682760672361, "content": {"title": "Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks", "abstract": "In spite of the great success of deep learning technologies, training and delivery of a practically serviceable model is still a highly time-consuming process. Furthermore, a resulting model is usually too generic and heavyweight, and hence essentially goes through another expensive model compression phase to fit in a resource-limited device like embedded systems. Inspired by the fact that a machine learning task specifically requested by mobile users is often much simpler than it is supported by a massive generic model, this paper proposes a framework, called Pool of Experts (PoE), that instantly builds a lightweight and task-specific model without any training process. For a realtime model querying service, PoE first extracts a pool of primitive components, called experts, from a well-trained and sufficiently generic network by exploiting a novel conditional knowledge distillation method, and then performs our train-free knowledge consolidation to quickly combine necessary experts into a lightweight network for a target task. Thanks to this train-free property, in our thorough empirical study, PoE can build a fairly accurate yet compact model in a realtime manner, whereas it takes a few minutes per query for the other training methods to achieve a similar level of the accuracy."}}
{"id": "jmFv7O5_6ct", "cdate": 1609459200000, "mdate": 1682760672325, "content": {"title": "Similar Patch Selection in Embedding Space for Multi-View Image Denoising", "abstract": "This paper proposes an image patch selection that finds similar patches in multiple images so that image denoising can suppress noise more effectively by exploiting the identified similar patches from the multi-view images. We encode all image patches in multi-view images into a low-dimensional space, and it allows for a denoiser to find similar patches effectively from the space. Our approach enables existing patch-based denoisers, which often find similar patches within an image window, to identify more similar patches by extending the limited search space into the entire space (i.e., all input images). We integrate our technique into state-of-the-art single-view denoising (block-matching and 3D filtering (BM3D)), and demonstrate that the BM3D combined with our approach is able to conduct multi-view image denoising effectively, without a major alteration to the existing algorithm."}}
{"id": "W6aCXJ7Rqr4", "cdate": 1609459200000, "mdate": 1682760672437, "content": {"title": "Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks", "abstract": "In spite of the great success of deep learning technologies, training and delivery of a practically serviceable model is still a highly time-consuming process. Furthermore, a resulting model is usually too generic and heavyweight, and hence essentially goes through another expensive model compression phase to fit in a resource-limited device like embedded systems. Inspired by the fact that a machine learning task specifically requested by mobile users is often much simpler than it is supported by a massive generic model, this paper proposes a framework, called Pool of Experts (PoE), that instantly builds a lightweight and task-specific model without any training process. For a realtime model querying service, PoE first extracts a pool of primitive components, called experts, from a well-trained and sufficiently generic network by exploiting a novel conditional knowledge distillation method, and then performs our train-free knowledge consolidation to quickly combine necessary experts into a lightweight network for a target task. Thanks to this train-free property, in our thorough empirical study, PoE can build a fairly accurate yet compact model in a realtime manner, whereas it takes a few minutes per query for the other training methods to achieve a similar level of the accuracy."}}
{"id": "6nOBJ8opK3", "cdate": 1609459200000, "mdate": 1682760672310, "content": {"title": "Recency-based sequential pattern mining in multiple event sequences", "abstract": "The standard sequential pattern mining scheme hardly considers the positions of events in a sequence, and therefore it is difficult to focus on more interesting patterns that represent better the causal relationships between events. Without quantifying how close two events are in a sequence, we may fail to evaluate how likely an event is caused by the others from the pattern, which is a severe drawback for some applications like prediction. Motivated by this, we propose the recency-based sequential pattern mining scheme together with a novel measure of pattern interestingness to effectively capture recency as well as frequency. To efficiently extract all the recency-based sequential patterns, we devise a mining algorithm, called Recency-based Frequent pattern Miner (RF-Miner), together with an effective prediction method to evaluate the quality of recency-based patterns in terms of their prediction power. The experimental results show that our RF-Miner algorithm can extract more diverse and important patterns that can be used to make prediction of the next event, and can be more efficiently performed by using the upper bounds of our measure than baseline algorithms."}}
{"id": "1s1EFLgO4mn", "cdate": 1609459200000, "mdate": 1682760672454, "content": {"title": "Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network", "abstract": "Continual learning has been a major problem in the deep learning community, where the main challenge is how to effectively learn a series of newly arriving tasks without forgetting the knowledge of previous tasks. Initiated by Learning without Forgetting (LwF), many of the existing works report that knowledge distillation is effective to preserve the previous knowledge, and hence they commonly use a soft label for the old task, namely a knowledge distillation (KD) loss, together with a class label for the new task, namely a cross entropy (CE) loss, to form a composite loss for a single neural network. However, this approach suffers from learning the knowledge by a CE loss as a KD loss often more strongly influences the objective function when they are in a competitive situation within a single network. This could be a critical problem particularly in a class incremental scenario, where the knowledge across tasks as well as within the new task, both of which can only be acquired by a CE loss, is essentially learned due to the existence of a unified classifier. In this paper, we propose a novel continual learning method, called Split-and-Bridge, which can successfully address the above problem by partially splitting a neural network into two partitions for training the new task separated from the old task and re-connecting them for learning the knowledge across tasks. In our thorough experimental analysis, our Split-and-Bridge method outperforms the state-of-the-art competitors in KD-based continual learning."}}
