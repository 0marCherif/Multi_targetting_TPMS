{"id": "3yTCGW3Fup", "cdate": 1640995200000, "mdate": 1668102896876, "content": {"title": "Comparing Correspondences: Video Prediction with Correspondence-wise Losses", "abstract": "Image prediction methods often struggle on tasks that require changing the positions of objects, such as video prediction, producing blurry images that average over the many positions that objects might occupy. In this paper, we propose a simple change to existing image similarity metrics that makes them more robust to positional errors: we match the images using optical flow, then measure the visual similarity of corresponding pixels. This change leads to crisper and more perceptually accurate predictions, and does not require modifications to the image prediction network. We apply our method to a variety of video prediction tasks, where it obtains strong performance with simple network architectures, and to the closely related task of video interpolation. Code and results are available at our webpage: https://dangeng.github.io/CorrWiseLosses"}}
{"id": "g6iPdEfz86", "cdate": 1609459200000, "mdate": 1668102896879, "content": {"title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments", "abstract": "Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning."}}
{"id": "0En2FEZGWxf", "cdate": 1609459200000, "mdate": null, "content": {"title": "Comparing Correspondences: Video Prediction with Correspondence-wise Losses", "abstract": "Image prediction methods often struggle on tasks that require changing the positions of objects, such as video prediction, producing blurry images that average over the many positions that objects might occupy. In this paper, we propose a simple change to existing image similarity metrics that makes them more robust to positional errors: we match the images using optical flow, then measure the visual similarity of corresponding pixels. This change leads to crisper and more perceptually accurate predictions, and does not require modifications to the image prediction network. We apply our method to a variety of video prediction tasks, where it obtains strong performance with simple network architectures, and to the closely related task of video interpolation. Code and results are available at our webpage: https://dangeng.github.io/CorrWiseLosses"}}
{"id": "cPZOyoDloxl", "cdate": 1601308154579, "mdate": null, "content": {"title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments", "abstract": "Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning."}}
{"id": "H1lDbaVYvH", "cdate": 1569438975324, "mdate": null, "content": {"title": "SMiRL: Surprise Minimizing RL in Entropic Environments", "abstract": "All living organisms struggle against the forces of nature to carve out niches where\nthey can maintain relative stasis. We propose that such a search for order amidst\nchaos might offer a unifying principle for the emergence of useful behaviors in\nartificial agents. We formalize this idea into an unsupervised reinforcement learning\nmethod called surprise minimizing RL (SMiRL). SMiRL trains an agent with the\nobjective of maximizing the probability of observed states under a model trained on\nall previously seen states. The resulting agents acquire several proactive behaviors\nto seek and maintain stable states such as balancing and damage avoidance, that\nare closely tied to the affordances of the environment and its prevailing sources\nof entropy, such as winds, earthquakes, and other agents.  We demonstrate that\nour surprise minimizing agents can successfully play Tetris, Doom, and control\na  humanoid  to  avoid  falls,  without  any  task-specific  reward  supervision.   We\nfurther show that SMiRL can be used as an unsupervised pre-training objective\nthat substantially accelerates subsequent reward-driven learning"}}
{"id": "CXJ_-oYNt_V", "cdate": 1546300800000, "mdate": null, "content": {"title": "SMiRL: Surprise Minimizing RL in Dynamic Environments", "abstract": "Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning."}}
{"id": "2inVIpzYd5v", "cdate": 1546300800000, "mdate": null, "content": {"title": "Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control", "abstract": "Autonomous agents situated in real-world environments must be able to master large repertoires of skills. While a single short skill can be learned quickly, it would be impractical to learn every task independently. Instead, the agent should share knowledge across behaviors such that each task can be learned efficiently, and such that the resulting model can generalize to new tasks, especially ones that are compositions or subsets of tasks seen previously. A policy conditioned on a goal or demonstration has the potential to share knowledge between tasks if it sees enough diversity of inputs. However, these methods may not generalize to a more complex task at test time. We introduce compositional plan vectors (CPVs) to enable a policy to perform compositions of tasks without additional supervision. CPVs represent trajectories as the sum of the subtasks within them. We show that CPVs can be learned within a one-shot imitation learning framework without any additional supervision or information about task hierarchy, and enable a demonstration-conditioned policy to generalize to tasks that sequence twice as many skills as the tasks seen during training. Analogously to embeddings such as word2vec in NLP, CPVs can also support simple arithmetic operations -- for example, we can add the CPVs for two different tasks to command an agent to compose both tasks, without any additional training."}}
{"id": "0CeGGW97Pfr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Compositional Plan Vectors", "abstract": "Autonomous agents situated in real-world environments must be able to master large repertoires of skills. While a single short skill can be learned quickly, it would be impractical to learn every task independently. Instead, the agent should share knowledge across behaviors such that each task can be learned efficiently, and such that the resulting model can generalize to new tasks, especially ones that are compositions or subsets of tasks seen previously. A policy conditioned on a goal or demonstration has the potential to share knowledge between tasks if it sees enough diversity of inputs. However, these methods may not generalize to a more complex task at test time. We introduce compositional plan vectors (CPVs) to enable a policy to perform compositions of tasks without additional supervision. CPVs represent trajectories as the sum of the subtasks within them. We show that CPVs can be learned within a one-shot imitation learning framework without any additional supervision or information about task hierarchy, and enable a demonstration-conditioned policy to generalize to tasks that sequence twice as many skills as the tasks seen during training. Analogously to embeddings such as word2vec in NLP, CPVs can also support simple arithmetic operations -- for example, we can add the CPVs for two different tasks to command an agent to compose both tasks, without any additional training."}}
