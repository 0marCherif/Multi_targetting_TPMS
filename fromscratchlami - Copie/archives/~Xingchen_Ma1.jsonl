{"id": "RBPvr4Ehojh", "cdate": 1663939398872, "mdate": null, "content": {"title": "Tackling Personalized Federated Learning with Label Concept Drift via Hierarchical Bayesian Modeling", "abstract": "Federated Learning (FL) is a distributed learning scheme to train a shared model across clients. One fundamental challenge in FL is that the sets of data across clients could be non-identically distributed. Personalized Federated Learning (PFL) attempts to solve this challenge. Most methods in the literature of PFL focus on the data heterogeneity that clients differ in their label distributions. In this work, we focus on label concept drift which is a broad but relatively unexplored area. We present a general framework for PFL based on hierarchical Bayesian inference and propose a variational inference algorithm based on this framework. We demonstrate our methods through empirical studies on CIFAR100 and SUN397. Experimental results show our approach significantly outperforms the baselines when tackling the label concept drift across clients."}}
{"id": "wDGMgubQaMg", "cdate": 1609459200000, "mdate": 1632933060567, "content": {"title": "Additive Tree-Structured Conditional Parameter Spaces in Bayesian Optimization: A Novel Covariance Function and a Fast Implementation", "abstract": "Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem and on pruning pre-trained VGG16 and ResNet50 models. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017)."}}
{"id": "a8jj9jugYbJ", "cdate": 1609459200000, "mdate": 1623586383686, "content": {"title": "Meta-Cal: Well-controlled Post-hoc Calibration by Ranking", "abstract": "In many applications, it is desirable that a classifier not only makes accurate predictions, but also outputs calibrated posterior probabilities. However, many existing classifiers, especially deep neural network classifiers, tend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a model by learning a calibration map. Existing approaches mostly focus on constructing calibration maps with low calibration errors, however, this quality is inadequate for a calibrator being useful. In this paper, we introduce two constraints that are worth consideration in designing a calibration map for post-hoc calibration. Then we present Meta-Cal, which is built from a base calibrator and a ranking model. Under some mild assumptions, two high-probability bounds are given with respect to these constraints. Empirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular network architectures show our proposed method significantly outperforms the current state of the art for post-hoc multi-class classification calibration."}}
{"id": "ZYqc6nRGPmi", "cdate": 1609459200000, "mdate": 1632933060494, "content": {"title": "Meta-Cal: Well-controlled Post-hoc Calibration by Ranking", "abstract": "In many applications, it is desirable that a classifier not only makes accurate predictions, but also outputs calibrated posterior probabilities. However, many existing classifiers, especially deep..."}}
{"id": "qwcdwZeD1M", "cdate": 1577836800000, "mdate": null, "content": {"title": "Additive Tree-Structured Covariance Function for Conditional Parameter Spaces in Bayesian Optimization", "abstract": "Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, as well as on a neural network model compression problem, and experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017)."}}
{"id": "geB_XtU--c", "cdate": 1577836800000, "mdate": 1623586383752, "content": {"title": "Additive Tree-Structured Conditional Parameter Spaces in Bayesian Optimization: A Novel Covariance Function and a Fast Implementation", "abstract": "Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem, on pruning pre-trained VGG16 and ResNet50 models as well as on searching activation functions of ResNet20. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017)."}}
{"id": "CPjnWyvDUik8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Additive Tree-Structured Covariance Function for Conditional Parameter Spaces in Bayesian Optimization", "abstract": "Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditiona..."}}
{"id": "EMq2LI9hsbI", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Bayesian Optimization Framework for Neural Network Compression", "abstract": "Neural network compression is an important step for deploying neural networks where speed is of high importance, or on devices with limited memory. It is necessary to tune compression parameters in order to achieve the desired trade-off between size and performance. This is often done by optimizing the loss on a validation set of data, which should be large enough to approximate the true risk and therefore yield sufficient generalization ability. However, using a full validation set can be computationally expensive. In this work, we develop a general Bayesian optimization framework for optimizing functions that are computed based on U-statistics. We propagate Gaussian uncertainties from the statistics through the Bayesian optimization framework yielding a method that gives a probabilistic approximation certificate of the result. We then apply this to parameter selection in neural network compression. Compression objectives that can be written as U-statistics are typically based on empirical risk and knowledge distillation for deep discriminative models. We demonstrate our method on VGG and ResNet models, and the resulting system can find optimal compression parameters for relatively high-dimensional parametrizations in a matter of minutes on a standard desktop machine, orders of magnitude faster than competing methods."}}
{"id": "R_oIhb73JB1", "cdate": 1451606400000, "mdate": null, "content": {"title": "DepAudioNet: An Efficient Deep Model for Audio based Depression Classification", "abstract": "This paper presents a novel and effective audio based method on depression classification. It focuses on two important issues, \\emph{i.e.} data representation and sample imbalance, which are not well addressed in literature. For the former one, in contrast to traditional shallow hand-crafted features, we propose a deep model, namely DepAudioNet, to encode the depression related characteristics in the vocal channel, combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) to deliver a more comprehensive audio representation. For the latter one, we introduce a random sampling strategy in the model training phase to balance the positive and negative samples, which largely alleviates the bias caused by uneven sample distribution. Evaluations are carried out on the DAIC-WOZ dataset for the Depression Classification Sub-challenge (DCC) at the 2016 Audio-Visual Emotion Challenge (AVEC), and the experimental results achieved clearly demonstrate the effectiveness of the proposed approach."}}
{"id": "G0TvYHCu9sZ", "cdate": 1451606400000, "mdate": null, "content": {"title": "Cost-Sensitive Two-Stage Depression Prediction Using Dynamic Visual Clues", "abstract": "This paper presents a novel and effective approach to depression recognition in the visual modality of videos, which automatically predicts the depression level through two cost-sensitive stages. It delivers an improved solution in two ways compared with other vision based methods. On the one hand, current techniques regard depression recognition as either a classification or a regression problem, which tends to incur overfitting due to the high complexity of the model and the limited number of training samples. To handle such an issue, we propose a two-stage framework consisting of a coarse classifier and a fine regressor. The former makes use of a set of linear functions, corresponding to different depression intensities, to approximate the complex non-linear model, where a coarse range of the test sample is preliminarily located. The latter then predicts its precise depression level within the given range. On the other hand, depression recognition is different from the general classification and regression tasks, since its analysis is cost-sensitive as the diagnosis of heart diseases and cancers. However, this critical cue is not taken into account in the previous investigations, thus making their results problematic. To address this drawback, we embed the indicator of medical risk assessment into both the two stages by constraining the classifier using a weight matrix and loosening the regressor to an expanded range of depression level. The proposed method is evaluated on the Audio and Video Emotion Challenge (AVEC) 2013, and the performance is superior to the best one so far reported using the visual modality. Furthermore, it proves complementary to the audio based methods, and their joint use further ameliorates the accuracy. These facts clearly highlight the effectiveness of the proposed method on depression recognition."}}
