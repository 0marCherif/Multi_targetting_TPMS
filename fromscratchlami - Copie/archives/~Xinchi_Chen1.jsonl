{"id": "BXC4PlGgdpr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Switch-LSTMs for Multi-Criteria Chinese Word Segmentation.", "abstract": "Multi-criteria Chinese word segmentation is a promising but challenging task, which exploits several different segmentation criteria and mines their common underlying knowledge. In this paper, we propose a flexible multi-criteria learning for Chinese word segmentation. Usually, a segmentation criterion could be decomposed into multiple sub-criteria, which are shareable with other segmentation criteria. The process of word segmentation is a routing among these sub-criteria. From this perspective, we present Switch-LSTMs to segment words, which consist of several long short-term memory neural networks (LSTM), and a switcher to automatically switch the routing among these LSTMs. With these auto-switched LSTMs, our model provides a more flexible solution for multi-criteria CWS, which is also easy to transfer the learned knowledge to new criteria. Experiments show that our model obtains significant improvements on eight corpora with heterogeneous segmentation criteria, compared to the previous method and single-criterion learning."}}
{"id": "rJ-LyHMuZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Attention-Fused Deep Matching Network for Natural Language Inference", "abstract": "Natural language inference aims to predict whether a premise sentence can infer another hypothesis sentence. Recent progress on this task only relies on a shallow interaction between sentence pairs, which is insufficient for modeling complex relations. In this paper, we present an attention-fused deep matching network (AF-DMN) for natural language inference. Unlike existing models, AF-DMN takes two sentences as input and iteratively learns the attention-aware representations for each side by multi-level interactions. Moreover, we add a self-attention mechanism to fully exploit local context information within each sentence. Experiment results show that AF-DMN achieves state-of-the-art performance and outperforms strong baselines on Stanford natural language inference (SNLI), multi-genre natural language inference (MultiNLI), and Quora duplicate questions datasets."}}
{"id": "SyNptQMOWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Toward Diverse Text Generation with Inverse Reinforcement Learning", "abstract": "Text generation is a crucial task in NLP. Recently, several adversarial generative models have been proposed to improve the exposure bias problem in text generation. Though these models gain great success, they still suffer from the problems of reward sparsity and mode collapse. In order to address these two problems, in this paper, we employ inverse reinforcement learning (IRL) for text generation. Specifically, the IRL framework learns a reward function on training data, and then an optimal policy to maximum the expected total reward. Similar to the adversarial models, the reward and policy function in IRL are optimized alternately. Our method has two advantages: (1) the reward function can produce more dense reward signals. (2) the generation policy, trained by ``entropy regularized'' policy gradient, encourages to generate more diversified texts. Experiment results demonstrate that our proposed method can generate higher quality texts than the previous methods."}}
{"id": "Sk4TTZGOZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Convolutional Interaction Network for Natural Language Inference", "abstract": "Author Summary The ever growing size of neural populations simultaneously recorded in electrophysiological experiments calls for urgent analytical progress in understanding how to compactly describe all sensory information present both in the spatial and temporal structure of single-trial neural population activity. Here we show the power of analytical methods, termed space-by-time tensor factorizations, which detect groups of simultaneously coactive neurons and the temporal profiles of their coactivation. By validation on simulated data and on retinal recordings, we show that the tensor decomposition performs competitively compared to other techniques both in terms of data robustness and ability to find informative patterns across diverse stimuli. We show that this method can determine the spatial and temporal resolution of neural population codes, and find which spatial or temporal components of neural responses carry information not available in other aspects of the population code. When applied to experimental data, the method demonstrates the importance of first-spike latencies in retinal population coding of visual images, particularly for decoding fine spatial details of natural images from population activity. This work shows that this methodology can improve our knowledge of population coding by allowing the discovery of informative spatial and temporal firing patterns in populations of simultaneously recorded neurons."}}
{"id": "ryViJ2l_-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation", "abstract": "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github."}}
{"id": "BJ-GrVMdWS", "cdate": 1483228800000, "mdate": null, "content": {"title": "A Feature-Enriched Neural Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging", "abstract": "Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this work, we propose a feature-enriched neural model for joint Chinese word segmentation and part-of-speech tagging task. Specifically, to simulate the feature templates of traditional discrete feature based models, we use different filters to model the complex compositional features with convolutional and pooling layer, and then utilize long distance dependency information with recurrent layer. Experimental results on five different datasets show the effectiveness of our proposed model."}}
{"id": "HyVKyfGObS", "cdate": 1420070400000, "mdate": null, "content": {"title": "Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks", "abstract": "Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model."}}
{"id": "ByV8NnxuZB", "cdate": 1420070400000, "mdate": null, "content": {"title": "Gated Recursive Neural Network for Chinese Word Segmentation", "abstract": "Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this paper, we propose a gated recursive neural network (GRNN) for Chinese word segmentation, which contains reset and update gates to incorporate the complicated combinations of the context characters. Since GRNN is relative deep, we also use a supervised layer-wise training method to avoid the problem of gradient diffusion. Experiments on the benchmark datasets show that our model outperforms the previous neural network models as well as the state-of-the-art methods."}}
{"id": "Bk4IhWf_-r", "cdate": 1420070400000, "mdate": null, "content": {"title": "Sentence Modeling with Gated Recursive Neural Network", "abstract": "Recently, neural network based sentence modeling methods have achieved great progress. Among these methods, the recursive neural networks (RecNNs) can effectively model the combination of the words in sentence. However, RecNNs need a given external topological structure, like syntactic tree. In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model."}}
{"id": "BJWN1fzObS", "cdate": 1420070400000, "mdate": null, "content": {"title": "Long Short-Term Memory Neural Networks for Chinese Word Segmentation", "abstract": "Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features aremostly extracted from a local context. Thesemethods cannot utilize the long distance information which is also crucial for word segmentation. In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information inmemory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods."}}
