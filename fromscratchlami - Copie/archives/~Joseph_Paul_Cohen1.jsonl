{"id": "76fkZiP_J4U", "cdate": 1685198070352, "mdate": 1685198070352, "content": {"title": "Problems in the deployment of machine-learned models in health care", "abstract": "KEY POINTS\n+ Decision-support systems or clinical prediction tools based on machine learning (including the special case of deep learning) are similar to clinical support tools developed using classical statistical models and, as such, have similar limitations.\n+ If a machine-learned model is trained using data that do not match the data it will encounter when deployed, its performance may be lower than expected.\n+ When training, machine learning algorithms take the \u201cpath of least resistance,\u201d leading them to learn features from the data that are spuriously correlated with target outputs instead of the correct features; this can impair the effective generalization of the resulting learned model.\n+ Avoiding errors related to these problems involves careful evaluation of machine-learned models using new data from the performance distribution, including data samples that are expected to \u201ctrick\u201d the model, such as those with different population demographics, difficult conditions or bad-quality inputs.\n\nIn a companion article, Verma and colleagues discuss how machine-learned solutions can be developed and implemented to support medical decision-making.1 Both decision-support systems and clinical prediction tools developed using machine learning (including the special case of deep learning) are similar to clinical support tools developed using classical statistical models and, as such, have similar limitations.2,3 A model that makes incorrect predictions can lead its users to make errors they otherwise would not have made when caring for patients, and therefore it is important to understand how these models can fail.4 We discuss these limitations \u2014 focusing on 2 issues in particular: out-of-distribution (or out-of-sample) generalization and incorrect feature attribution \u2014 to underscore the need to consider potential caveats when using machine-learned solutions."}}
{"id": "eq1NHYRbRV", "cdate": 1672531200000, "mdate": 1695966107560, "content": {"title": "The Effect of Counterfactuals on Reading Chest X-rays", "abstract": "This study evaluates the effect of counterfactual explanations on the interpretation of chest X-rays. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to rate their confidence that the model's prediction is correct using a 5 point scale. Half of the predictions are false positives. Each prediction is explained twice, once using traditional attribution methods and once with a counterfactual explanation. The overall results indicate that counterfactual explanations allow a radiologist to have more confidence in true positive predictions compared to traditional approaches (0.15$\\pm$0.95 with p=0.01) with only a small increase in false positive predictions (0.04$\\pm$1.06 with p=0.57). We observe the specific prediction tasks of Mass and Atelectasis appear to benefit the most compared to other tasks."}}
{"id": "uHwZEmdaHJZ", "cdate": 1653431729949, "mdate": 1653431729949, "content": {"title": "The TCGA Meta-Dataset Clinical Benchmark", "abstract": "Machine learning is bringing a paradigm shift to healthcare by changing the process of disease diagnosis and prognosis in clinics and hospitals. This development equips doctors and medical staff with tools to evaluate their hypotheses and hence make more precise decisions. Although most current research in the literature seeks to develop techniques and methods for predicting one particular clinical outcome, this approach is far from the reality of clinical decision making in which you have to consider several factors simultaneously. In addition, it is difficult to follow the recent progress concretely as there is a lack of consistency in benchmark datasets and task definitions in the field of Genomics. To address the aforementioned issues, we provide a clinical Meta-Dataset derived from the publicly available data hub called The Cancer Genome Atlas Program (TCGA) that contains 174 tasks. We believe those tasks could be good proxy tasks to develop methods which can work on a few samples of gene expression data. Also, learning to predict multiple clinical variables using gene-expression data is an important task due to the variety of phenotypes in clinical problems and lack of samples for some of the rare variables. The defined tasks cover a wide range of clinical problems including predicting tumor tissue site, white cell count, histological type, family history of cancer, gender, and many others which we explain later in the paper. Each task represents an independent dataset. We use regression and neural network baselines for all the tasks using only 150 samples and compare their performance."}}
{"id": "DIlxc2cGMn", "cdate": 1653431597009, "mdate": 1653431597009, "content": {"title": "Torchmeta: A Meta-Learning library for PyTorch", "abstract": "The constant introduction of standardized benchmarks in the literature has helped\naccelerating the recent advances in meta-learning research. They offer a way\nto get a fair comparison between different algorithms, and the wide range of\ndatasets available allows full control over the complexity of this evaluation.\nHowever, for a large majority of code available online, the data pipeline is often specific to one dataset, and testing on another dataset requires significant\nrework. We introduce Torchmeta, a library built on top of PyTorch that enables seamless and consistent evaluation of meta-learning algorithms on multiple datasets, by providing data-loaders for most of the standard benchmarks\nin few-shot classification and regression, with a new meta-dataset abstraction.\nIt also features some extensions for PyTorch to simplify the development of\nmodels compatible with meta-learning algorithms. The code is available here:\nhttps://github.com/tristandeleu/pytorch-meta."}}
{"id": "TUsk8TScdwz", "cdate": 1640995200000, "mdate": 1695966107559, "content": {"title": "The promise of machine learning applications in solid organ transplantation", "abstract": "Solid-organ transplantation is a life-saving treatment for end-stage organ disease in highly selected patients. Alongside the tremendous progress in the last several decades, new challenges have emerged. The growing disparity between organ demand and supply requires optimal patient/donor selection and matching. Improvements in long-term graft and patient survival require data-driven diagnosis and management of post-transplant complications. The growing abundance of clinical, genetic, radiologic, and metabolic data in transplantation has led to increasing interest in applying machine-learning (ML) tools that can uncover hidden patterns in large datasets. ML algorithms have been applied in predictive modeling of waitlist mortality, donor\u2013recipient matching, survival prediction, post-transplant complications diagnosis, and prediction, aiming to optimize immunosuppression and management. In this review, we provide insight into the various applications of ML in transplant medicine, why these were used to evaluate a specific clinical question, and the potential of ML to transform the care of transplant recipients. 36 articles were selected after a comprehensive search of the following databases: Ovid MEDLINE; Ovid MEDLINE Epub Ahead of Print and In-Process &amp; Other Non-Indexed Citations; Ovid Embase; Cochrane Database of Systematic Reviews (Ovid); and Cochrane Central Register of Controlled Trials (Ovid). In summary, these studies showed that ML techniques hold great potential to improve the outcome of transplant recipients. Future work is required to improve the interpretability of these algorithms, ensure generalizability through larger-scale external validation, and establishment of infrastructure to permit clinical integration."}}
{"id": "HHxbKUPmBx9", "cdate": 1640995200000, "mdate": 1645717329366, "content": {"title": "CheXstray: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging AI", "abstract": "Clinical Artificial lntelligence (AI) applications are rapidly expanding worldwide, and have the potential to impact to all areas of medical practice. Medical imaging applications constitute a vast majority of approved clinical AI applications. Though healthcare systems are eager to adopt AI solutions a fundamental question remains: \\textit{what happens after the AI model goes into production?} We use the CheXpert and PadChest public datasets to build and test a medical imaging AI drift monitoring workflow to track data and model drift without contemporaneous ground truth. We simulate drift in multiple experiments to compare model performance with our novel multi-modal drift metric, which uses DICOM metadata, image appearance representation from a variational autoencoder (VAE), and model output probabilities as input. Through experimentation, we demonstrate a strong proxy for ground truth performance using unsupervised distributional shifts in relevant metadata, predicted probabilities, and VAE latent representation. Our key contributions include (1) proof-of-concept for medical imaging drift detection that includes the use of VAE and domain specific statistical methods, (2) a multi-modal methodology to measure and unify drift metrics, (3) new insights into the challenges and solutions to observe deployed medical imaging AI, and (4) creation of open-source tools that enable others to easily run their own workflows and scenarios. This work has important implications. It addresses the concerning translation gap found in continuous medical imaging AI model monitoring common in dynamic healthcare environments."}}
{"id": "97PC7Vjsb4", "cdate": 1640995200000, "mdate": 1671903977565, "content": {"title": "Medical Image Segmentation Review: The success of U-Net", "abstract": "Automatic medical image segmentation is a crucial topic in the medical domain and successively a critical counterpart in the computer-aided diagnosis paradigm. U-Net is the most widespread image segmentation architecture due to its flexibility, optimized modular design, and success in all medical image modalities. Over the years, the U-Net model achieved tremendous attention from academic and industrial researchers. Several extensions of this network have been proposed to address the scale and complexity created by medical tasks. Addressing the deficiency of the naive U-Net model is the foremost step for vendors to utilize the proper U-Net variant model for their business. Having a compendium of different variants in one place makes it easier for builders to identify the relevant research. Also, for ML researchers it will help them understand the challenges of the biological tasks that challenge the model. To address this, we discuss the practical aspects of the U-Net model and suggest a taxonomy to categorize each network variant. Moreover, to measure the performance of these strategies in a clinical application, we propose fair evaluations of some unique and famous designs on well-known datasets. We provide a comprehensive implementation library with trained models for future research. In addition, for ease of future studies, we created an online list of U-Net papers with their possible official implementation. All information is gathered in https://github.com/NITR098/Awesome-U-Net repository."}}
{"id": "_5iri84DJmE", "cdate": 1639080123171, "mdate": null, "content": {"title": "TorchXRayVision: A library of chest X-ray datasets and models", "abstract": "TorchXRayVision is an open source software library for working with chest X-ray datasets and deep learning models. It provides a common interface and common pre-processing chain for a wide set of publicly available chest X-ray datasets. In addition, a number of classification and representation learning models with different architectures, trained on different data combinations, are available through the library to serve as baselines or feature extractors."}}
{"id": "oUg5rC_95OM", "cdate": 1623124977255, "mdate": null, "content": {"title": "A Benchmark of Medical Out of Distribution Detection", "abstract": "Motivation: Deep learning models deployed on medical tasks can be equipped with Out-of-Distribution Detection (OoDD) methods in order to avoid erroneous predictions. However it is unclear which OoDD methods are effective in practice. \n\nSpecific Problem: Systems trained for one particular domain of images cannot be expected to perform accurately on images of a different domain.  These images should be flagged by an OoDD method prior to prediction. \n\nOur approach: This paper defines 3 categories of OoD examples and benchmarks popular OoDD methods in three domains of medical imaging: chest X-ray, fundus imaging, and histology slides. \n\nResults: Our experiments show that despite methods yielding good results on some categories of out-of-distribution samples, they fail to recognize images close to the training distribution.\n\nConclusion: We find a simple binary classifier on the feature representation has the best accuracy and AUPRC on average. Users of diagnostic tools which employ these OoDD methods should still remain vigilant that images very close to the training distribution yet not in it could yield unexpected results."}}
{"id": "rnunjvgxAMt", "cdate": 1612917596099, "mdate": null, "content": {"title": "Gifsplanation via Latent Shift: A Simple Autoencoder Approach to Counterfactual Generation for Chest X-rays", "abstract": "Motivation: Traditional image attribution methods struggle to satisfactorily explain predictions of neural networks. Prediction explanation is important, especially in medical imaging, for avoiding the unintended consequences of deploying AI systems when false positive predictions can impact patient care. Thus, there is a pressing need to develop improved models for model explainability and introspection. \nSpecific problem: A new approach is to transform input images to increase or decrease features which cause the prediction. However, current approaches are difficult to implement as they are monolithic or rely on GANs. These hurdles prevent wide adoption.\nOur approach: Given an arbitrary classifier, we propose a simple autoencoder and gradient update (Latent Shift) that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction. We use this method to study chest X-ray classifiers and evaluate their performance. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to identify which ones are false positives (half are) using traditional attribution maps or our proposed method.\nResults: We found low overlap with ground truth pathology masks for models with reasonably high accuracy. However, the results from our reader study indicate that these models are generally looking at the correct features.\nWe also found that the Latent Shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches (0.15\u00b10.95 in a 5 point scale with p=0.01) with only a small increase in false positive predictions (0.04\u00b11.06 with p=0.57).\n\nAccompanying webpage: https://mlmed.org/gifsplanation/\nSource code: https://github.com/mlmed/gifsplanation"}}
