{"id": "O5oghizPQb7", "cdate": 1698772176150, "mdate": 1698772176150, "content": {"title": "Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning", "abstract": "We introduce Patch Aligned Contrastive Learning\n(PACL), a modified compatibility function for CLIP\u2019s contrastive loss, intending to train an alignment between the\npatch tokens of the vision encoder and the CLS token of the\ntext encoder. With such an alignment, a model can identify\nregions of an image corresponding to a given text input, and\ntherefore transfer seamlessly to the task of open vocabulary\nsemantic segmentation without requiring any segmentation\nannotations during training. Using pre-trained CLIP encoders with PACL, we are able to set the state-of-the-art\non the task of open vocabulary zero-shot segmentation on\n4 different segmentation benchmarks: Pascal VOC, Pascal\nContext, COCO Stuff and ADE20K. Furthermore, we show\nthat PACL is also applicable to image-level predictions and\nwhen used with a CLIP backbone, provides a general improvement in zero-shot classification accuracy compared to\nCLIP, across a suite of 12 image classification datasets."}}
{"id": "bAE1y8wG-ng", "cdate": 1652737812177, "mdate": null, "content": {"title": "Few-Shot Fast-Adaptive Anomaly Detection", "abstract": "The ability to detect anomaly has long been recognized as an inherent human ability, yet to date, practical AI solutions to mimic such capability have been lacking. This lack of progress can be attributed to several factors. To begin with, the distribution of ``abnormalities'' is intractable. Anything outside of a given normal population is by definition an anomaly. This explains why a large volume of work in this area has been dedicated to modeling the normal distribution of a given task followed by detecting deviations from it. This direction is however unsatisfying as it would require modeling the normal distribution of every task that comes along, which includes tedious data collection. In this paper, we report our work aiming to handle these issues. To deal with the intractability of abnormal distribution, we leverage Energy Based Model (EBM). EBMs learn to associates low energies to correct values and higher energies to incorrect values. At its core, the EBM employs Langevin Dynamics (LD) in generating these incorrect samples based on an iterative optimization procedure, alleviating the intractable problem of modeling the world of anomalies. Then, in order to avoid training an anomaly detector for every task, we utilize an adaptive sparse coding layer. Our intention is to design a plug and play feature that can be used to quickly update what is normal during inference time. Lastly, to avoid tedious data collection, this mentioned update of the sparse coding layer needs to be achievable with just a few shots. Here, we employ a meta learning scheme that simulates such a few shot setting during training. We support our findings with strong empirical evidence."}}
{"id": "sS0dHmaH1I", "cdate": 1632875435531, "mdate": null, "content": {"title": "Fast Adaptive Anomaly Detection", "abstract": "The ability to detect anomaly has long been recognized as an inherent human ability, yet to date, practical AI solutions to mimic such capability have been lacking.This lack of progress can be attributed to several factors.  To begin with, the distribution of \u201cabnormalities\u201d is intractable.   Anything outside of a given normal population is by definition an anomaly. This explains why a large volume of workin this area has been dedicated to modeling the normal distribution of a given task followed by detecting deviations from it. This direction is however unsatisfying as it would require modeling the normal distribution of every task that comes along, which includes tedious data collection.  In this paper, we report our work aiming to handle these issues. To deal with the intractability of abnormal distribution, we leverage Energy Based Model (EBM). EBMs learn to associates low energies to correct values and higher energies to incorrect values.  As its core, the EBM em-ploys Langevin Dynamics (LD) in generating these incorrect samples based on an iterative optimization procedure, alleviating the intractable problem of modeling the world of anomalies.  Then, in order to avoid training an anomaly detector for every task, we utilize an adaptive sparse coding layer. Our intention is to design a plug and play feature that can be used to quickly update what is normal during inference time. Lastly, to avoid tedious data collection, this mentioned update of the sparse coding layer needs to be achievable with just a few shots. Here, we employ a meta learning scheme that simulates such a few shot setting during training. We support our findings with strong empirical evidence."}}
{"id": "sqQvgqytFP", "cdate": 1580443206409, "mdate": null, "content": {"title": "One-to-many face recognition with bilinear CNNs", "abstract": "The recent explosive growth in convolutional neural net-work (CNN) research has produced a variety of new architectures  for  deep  learning.   One  intriguing  new  architecture is the bilinear CNN (B-CNN),  which has shown dramatic performance gains on certain fine-grained recognition problems [15].   We apply this new CNN to the challenging new face recognition benchmark, the IARPA Janus Benchmark A (IJB-A) [12].  It features faces from a large number of identities in challenging real-world conditions. Because the face images were not identified automatically using  a  computerized  face  detection  system,  it  does  not have the bias inherent in such a database.  We demonstrate the  performance  of  the  B-CNN  model  beginning  from  an AlexNet-style  network  pre-trained  on  ImageNet.   We  then show  results  for  fine-tuning  using  a  moderate-sized  and public external database, FaceScrub [17].  We also present results  with  additional  fine-tuning  on  the  limited  training data provided by the protocol.  In each case, the fine-tuned bilinear  model  shows  substantial  improvements  over  thestandard  CNN.  Finally,  we  demonstrate  how  a  standard CNN  pre-trained  on  a  large  face  database,  the  recentlyreleased  VGG-Face  model  [20],  can  be  converted  into  a B-CNN without any additional feature training. This B-CNN improves upon the CNN performance on the IJB-A bench-mark, achieving 89.5% rank-1 recall."}}
{"id": "SjxlWGjVgOaB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Bilinear Convolutional Neural Networks for Fine-Grained Visual Recognition.", "abstract": "We present a simple and effective architecture for fine-grained recognition called Bilinear Convolutional Neural Networks (B-CNNs). These networks represent an image as a pooled outer product of features derived from two CNNs and capture localized feature interactions in a translationally invariant manner. B-CNNs are related to orderless texture representations built on deep features but can be trained in an end-to-end manner. Our most accurate model obtains 84.1, 79.4, 84.5 and 91.3 percent per-image accuracy on the Caltech-UCSD birds [1], NABirds [2], FGVC aircraft [3], and Stanford cars [4] dataset respectively and runs at 30 frames-persecond on a NVIDIA Titan X GPU. We then present a systematic analysis of these networks and show that (1) the bilinear features are highly redundant and can be reduced by an order of magnitude in size without significant loss in accuracy, (2) are also effective for other image classification tasks such as texture and scene recognition, and (3) can be trained from scratch on the ImageNet dataset offering consistent improvements over the baseline architecture. Finally, we present visualizations of these models on various datasets using top activations of neural units and gradient-based inversion techniques. The source code for the complete system is available at http://vis-www.cs.umass.edu/bcnn."}}
{"id": "HkNc9cbOWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Second-Order Democratic Aggregation", "abstract": "Aggregated second-order features extracted from deep convolutional networks have been shown to be effective for texture generation, fine-grained recognition, material classification, and scene understanding. In this paper, we study a class of orderless aggregation functions designed to minimize interference or equalize contributions in the context of second-order features and we show that they can be computed just as efficiently as their first-order counterparts and they have favorable properties over aggregation by summation. Another line of work has shown that matrix power normalization after aggregation can significantly improve the generalization of second-order representations. We show that matrix power normalization implicitly equalizes contributions during aggregation thus establishing a connection between matrix normalization techniques and prior work on minimizing interference. Based on the analysis we present $$\\gamma $$ -democratic aggregators that interpolate between sum ( $$\\gamma $$ \u00a0=\u00a01) and democratic pooling ( $$\\gamma $$ \u00a0=\u00a00) outperforming both on several classification tasks. Moreover, unlike power normalization, the $$\\gamma $$ -democratic aggregations can be computed in a low dimensional space by sketching that allows the use of very high-dimensional second-order features. This results in a state-of-the-art performance on several datasets."}}
{"id": "BJ-L3h-dbr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Visualizing and Understanding Deep Texture Representations", "abstract": "A number of recent approaches have used deep convolutional neural networks (CNNs) to build texture representations. Nevertheless, it is still unclear how these models represent texture and invariances to categorical variations. This work conducts a systematic evaluation of recent CNN-based texture descriptors for recognition and attempts to understand the nature of invariances captured by these representations. First we show that the recently proposed bilinear CNN model [25] is an excellent generalpurpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks. The model is translationally invariant and obtains better accuracy on the ImageNet dataset without requiring spatial jittering of data compared to corresponding models trained with spatial jittering. Based on recent work [13, 28] we propose a technique to visualize pre-images, providing a means for understanding categorical properties that are captured by these representations. Finally, we show preliminary results on how a unified parametric model of texture analysis and synthesis can be used for attribute-based image manipulation, e.g. to make an image more swirly, honeycombed, or knitted. The source code and additional visualizations are available at http://vis-www.cs.umass.edu/texture."}}
{"id": "BJZepxzdWS", "cdate": 1420070400000, "mdate": null, "content": {"title": "Bilinear CNN Models for Fine-Grained Visual Recognition", "abstract": "We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization. It also generalizes various orderless texture descriptors such as the Fisher vector, VLAD and O2P. We present experiments with bilinear models where the feature extractors are based on convolutional neural networks. The bilinear form simplifies gradient computation and allows end-to-end training of both networks using image labels only. Using networks initialized from the ImageNet dataset followed by domain specific fine-tuning we obtain 84.1% accuracy of the CUB-200-2011 dataset requiring only category labels at training time. We present experiments and visualizations that analyze the effects of fine-tuning and the choice two networks on the speed and accuracy of the models. Results show that the architecture compares favorably to the existing state of the art on a number of fine-grained datasets while being substantially simpler and easier to train. Moreover, our most accurate model is fairly efficient running at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete system will be made available at http://vis-www.cs.umass.edu/bcnn."}}
