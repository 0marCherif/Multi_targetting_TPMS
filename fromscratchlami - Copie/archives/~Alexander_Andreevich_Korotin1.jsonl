{"id": "Va_afHKs_l9", "cdate": 1672531200000, "mdate": 1684316735599, "content": {"title": "Extremal Domain Translation with Neural Optimal Transport", "abstract": "We propose the extremal transport (ET) which is a mathematical formalization of the theoretically best possible unpaired translation between a pair of domains w.r.t. the given similarity function. Inspired by the recent advances in neural optimal transport (OT), we propose a scalable algorithm to approximate ET maps as a limit of partial OT maps. We test our algorithm on toy examples and on the unpaired image-to-image translation task."}}
{"id": "Js1z30VHzbG", "cdate": 1672531200000, "mdate": 1684316735651, "content": {"title": "Energy-guided Entropic Neural Optimal Transport", "abstract": "Energy-Based Models (EBMs) are known in the Machine Learning community for decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. From the theoretical perspectives, we prove generalization bounds for our approach. In practice, we validate its applicability on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For simplicity, we choose simple long-run EBMs as a backbone of our Energy-guided Entropic OT method, leaving the application of more sophisticated EBMs for future research."}}
{"id": "9f2xFQ9HHbk", "cdate": 1672531200000, "mdate": 1684316735632, "content": {"title": "Partial Neural Optimal Transport", "abstract": "We propose a novel neural method to compute partial optimal transport (OT) maps, i.e., OT maps between parts of measures of the specified masses. We test our partial neural optimal transport algorithm on synthetic examples."}}
{"id": "5eKuOdYy-O2", "cdate": 1672531200000, "mdate": 1684316735621, "content": {"title": "Neural Gromov-Wasserstein Optimal Transport", "abstract": "We present a scalable neural method to solve the Gromov-Wasserstein (GW) Optimal Transport (OT) problem with the inner product cost. In this problem, given two distributions supported on (possibly different) spaces, one has to find the most isometric map between them. Our proposed approach uses neural networks and stochastic mini-batch optimization which allows to overcome the limitations of existing GW methods such as their poor scalability with the number of samples and the lack of out-of-sample estimation. To demonstrate the effectiveness of our proposed method, we conduct experiments on the synthetic data and explore the practical applicability of our method to the popular task of the unsupervised alignment of word embeddings."}}
{"id": "0g1JdUJF7Fr", "cdate": 1663849992944, "mdate": null, "content": {"title": "An Optimal Transport Perspective on Unpaired Image Super-Resolution", "abstract": "Real-world image super-resolution (SR) tasks often do not have paired\ndatasets, which limits the application of supervised techniques. As a result, the tasks are usually approached by unpaired techniques based on Generative Adversarial Networks (GANs), which yield complex training losses with several regularization terms, e.g., content or identity losses. We theoretically investigate optimization problems which arise in such models and find two surprizing observations. First, the learned SR map is always an optimal transport (OT) map. Second, we theoretically prove and empirically show that the learned map is biased, i.e., it does not actually transform the distribution of low-resolution images to high-resolution ones. Inspired by these findings, we propose an algorithm for unpaired SR which learns an unbiased OT map for the perceptual transport cost. Unlike the existing GAN-based alternatives, our algorithm has a simple optimization objective reducing the need for complex  hyperparameter selection and an application of additional regularizations. At the same time, it provides a nearly state-of-the-art performance on the large-scale unpaired AIM19 dataset."}}
{"id": "Zuc_MHtUma4", "cdate": 1663849923580, "mdate": null, "content": {"title": "Kernel Neural Optimal Transport", "abstract": "We study the Neural Optimal Transport (NOT) algorithm which uses the general optimal transport formulation and learns stochastic transport plans. We show that NOT with the weak quadratic cost may learn fake plans which are not optimal. To resolve this issue, we introduce kernel weak quadratic costs. We show that they provide improved theoretical guarantees and practical performance. We test NOT with kernel costs on the unpaired image-to-image translation task."}}
{"id": "d8CBRlWNkqH", "cdate": 1663849923456, "mdate": null, "content": {"title": "Neural Optimal Transport", "abstract": "We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs. To justify the usage of neural networks, we prove that they are universal approximators of transport plans between probability distributions. We evaluate the performance of our optimal transport algorithm on toy examples and on the unpaired image-to-image translation."}}
{"id": "EwoBTLCY-Y", "cdate": 1663849922147, "mdate": null, "content": {"title": "Neural Optimal Transport with General Cost Functionals", "abstract": "We present a novel neural-networks-based algorithm to compute optimal transport (OT) plans and maps for general cost functionals. The algorithm is based on a saddle point reformulation of the OT problem and generalizes prior OT methods for weak and strong cost functionals. As an application, we construct a functional to map data distributions with preserving the class-wise structure of data."}}
{"id": "VtEEpi-dGlt", "cdate": 1654378604043, "mdate": null, "content": {"title": "Kantorovich Strikes Back! Wasserstein GANs are not Optimal Transport?", "abstract": "Wasserstein Generative Adversarial Networks (WGANs) are the popular generative models built on the theory of Optimal Transport (OT) and the Kantorovich duality. Despite the success of WGANs, it is still unclear how well the underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance, W1) and the OT gradient needed to update the generator. In this paper, we address these questions. We construct 1-Lipschitz functions and use them to build ray monotone transport plans. This strategy yields pairs of continuous benchmark distributions with the analytically known OT plan, OT cost and OT gradient in high-dimensional spaces such as spaces of images. We thoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral normalization, entropic regularization, etc.) using these benchmark pairs. Even though these solvers perform well in WGANs, none of them faithfully compute W1 in high dimensions. Nevertheless, many provide a meaningful approximation of the OT gradient. These observations suggest that these solvers should not be treated as good estimators of W1 but to some extent they indeed can be used in variational problems requiring the minimization of W1."}}
{"id": "GiEnzxTnaMN", "cdate": 1652737510101, "mdate": null, "content": {"title": "Wasserstein Iterative Networks for Barycenter Estimation", "abstract": "Wasserstein barycenters have become popular due to their ability to represent the average of probability measures in a geometrically meaningful way. In this paper, we present an algorithm to approximate the Wasserstein-2 barycenters of continuous measures via a generative model. Previous approaches rely on regularization (entropic/quadratic) which introduces bias or on input convex neural networks which are not expressive enough for large-scale tasks. In contrast, our algorithm does not introduce bias and allows using arbitrary neural networks. In addition, based on the celebrity faces dataset, we construct Ave, celeba! dataset which can be used for quantitative evaluation of barycenter algorithms by using standard metrics of generative models such as FID. "}}
