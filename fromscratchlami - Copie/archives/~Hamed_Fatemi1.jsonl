{"id": "RAjc2DxwHeP", "cdate": 1640995200000, "mdate": 1682647057405, "content": {"title": "ACTION: Automated Hardware-Software Codesign Framework for Low-precision Numerical Format SelecTION in TinyML", "abstract": "In this paper, a new low-precision hardware-software codesign framework is presented, to optimally select the numerical formats and bit-precision for TinyML models and benchmarks. The selection is performed by integer linear programming using constraints mandated by tiny edge devices. Practitioners can use the proposed framework to reduce design costs in the early stages of designing accelerators for TinyML models. The efficacy of various numerical formats is studied within a new low-precision framework, ACTION. Results assert that generalized posit and tapered fixed are suitable numerical formats for TinyML when the trade-off between accuracy and hardware complexity is desired."}}
{"id": "hqKQMNJoqu", "cdate": 1609459200000, "mdate": 1682647057405, "content": {"title": "ALPS: Adaptive Quantization of Deep Neural Networks With GeneraLized PositS", "abstract": "In this paper, a new adaptive quantization algorithm for generalized posit format is presented, to optimally represent the dynamic range and distribution of deep neural network parameters. Adaptation is achieved by minimizing the intra-layer posit quantization error with a compander. The efficacy of the proposed quantization algorithm is studied within a new low-precision framework, ALPS, on ResNet-50 and EfficientNet models for classification tasks. Results assert that the accuracy and energy dissipation of low-precision DNNs using generalized posits outperform other well-known numerical formats, including standard posits."}}
{"id": "FPrN6sojmdl", "cdate": 1609459200000, "mdate": 1682647057407, "content": {"title": "TENT: Efficient Quantization of Neural Networks on the tiny Edge with Tapered FixEd PoiNT", "abstract": "In this research, we propose a new low-precision framework, TENT, to leverage the benefits of a tapered fixed-point numerical format in TinyML models. We introduce a tapered fixed-point quantization algorithm that matches the numerical format's dynamic range and distribution to that of the deep neural network model's parameter distribution at each layer. An accelerator architecture for the tapered fixed-point with TENT framework is proposed. Results show that the accuracy on classification tasks improves up to ~31 % with an energy overhead of ~17-30 % as compared to fixed-point, for ConvNet and ResNet-18 models."}}
{"id": "xCLZfHvH3iU", "cdate": 1608571691908, "mdate": null, "content": {"title": "TENT: Efficient Quantization of Neural Networks on the tiny Edge with Tapered FixEd PoiNT", "abstract": "In this research, we propose a new low-precision framework, TENT, to leverage the benefits of a tapered fixed-point numerical format in TinyML models. We introduce a tapered fixed-point quantization algorithm that matches the numerical format's dynamic range and distribution to that of the deep neural network model's parameter distribution at each layer.  An accelerator architecture for the tapered fixed-point with TENT framework is proposed. Results show that the accuracy on classification tasks improves up to $\\approx$ 31 %  with an energy overhead of $\\approx$17-30 % as compared to fixed-point, for ConvNet and ResNet-18 models. "}}
{"id": "ETtE2aNWtzF", "cdate": 1577836800000, "mdate": 1623594458906, "content": {"title": "Adaptive Posit: Parameter aware numerical format for deep learning inference on the edge", "abstract": "Ultra low-precision (<; 8-bit width) arithmetic is a discernible approach to deploy deep learning networks on to edge devices. Recent findings show that posit with linear quantization has similar dynamic range as the weight and activation values across the deep neural network layers. This characteristic can benefit the data representation of deep neural networks without impacting the overall accuracy. When capturing the full dynamic range of weights and activations, posit with mixed precision or linear quantization leads to a surge in hardware resource requirements. We propose adaptive posit, which has the ability to capture the non-homogeneous dynamic range of weights and activation's across the deep neural network layers. A fine granular control is achieved by embedding the hyper-parameters in the numerical format. To evaluate the overall system efficiency, we design a parameterized ASIC soft core for the adaptive posit encoder and decoder. Benchmarking and evaluation of the adaptive posit is performed on three datasets: Fashion-MNIST, CIFAR-10, and ImageNet. Results assert that on average the performance on inference with <; 8-bit adaptive posits surpasses (2% to 10%) that of posit."}}
{"id": "zJAEL-lYh9f", "cdate": 1546300800000, "mdate": 1623594458827, "content": {"title": "Deep Learning Training on the Edge with Low-Precision Posits", "abstract": "Recently, the posit numerical format has shown promise for DNN data representation and compute with ultra-low precision ([5..8]-bit). However, majority of studies focus only on DNN inference. In this work, we propose DNN training using posits and compare with the floating point training. We evaluate on both MNIST and Fashion MNIST corpuses, where 16-bit posits outperform 16-bit floating point for end-to-end DNN training."}}
{"id": "ejhwxuIwmfG", "cdate": 1546300800000, "mdate": 1623594458847, "content": {"title": "Exploiting Randomness in Deep Learning Algorithms", "abstract": "The recent surge of interest in using deep neural networks for real-world tasks has led to training complex networks with billions of parameters that use enormous amounts of training data. Performing backpropagation in these deep networks is time consuming and requires large amount of resources that is usually limited by the underlying hardware. In order to move towards agile deep learning, we are motivated to exploit randomness in the networks. In this work, we explore the effects of utilizing random weights in convolutional neural networks. This is achieved through random initialization of weights and by freezing them. The training occurs only in the output layer. We also propose a novel weight distribution method based on the sum of sinusoids for random convolutional neural networks. Our experiments show that by leaving the weights random in convolutional neural networks relatively high performance can be achieved for MSTAR and CIFAR-10 datasets."}}
{"id": "Cm0x5-e0-yh", "cdate": 1546300800000, "mdate": 1623594458844, "content": {"title": "Cheetah: Mixed Low-Precision Hardware & Software Co-Design Framework for DNNs on the Edge", "abstract": "Low-precision DNNs have been extensively explored in order to reduce the size of DNN models for edge devices. Recently, the posit numerical format has shown promise for DNN data representation and compute with ultra-low precision in [5..8]-bits. However, previous studies were limited to studying posit for DNN inference only. In this paper, we propose the Cheetah framework, which supports both DNN training and inference using posits, as well as other commonly used formats. Additionally, the framework is amenable for different quantization approaches and supports mixed-precision floating point and fixed-point numerical formats. Cheetah is evaluated on three datasets: MNIST, Fashion MNIST, and CIFAR-10. Results indicate that 16-bit posits outperform 16-bit floating point in DNN training. Furthermore, performing inference with [5..8]-bit posits improves the trade-off between performance and energy-delay-product over both [5..8]-bit float and fixed-point."}}
{"id": "Al8-0iwTXBJ", "cdate": 1546300800000, "mdate": 1623594458894, "content": {"title": "Performance-Efficiency Trade-off of Low-Precision Numerical Formats in Deep Neural Networks", "abstract": "Deep neural networks (DNNs) have been demonstrated as effective prognostic models across various domains, e.g. natural language processing, computer vision, and genomics. However, modern-day DNNs demand high compute and memory storage for executing any reasonably complex task. To optimize the inference time and alleviate the power consumption of these networks, DNN accelerators with low-precision representations of data and DNN parameters are being actively studied. An interesting research question is in how low-precision networks can be ported to edge-devices with similar performance as high-precision networks. In this work, we employ the fixed-point, floating point, and posit numerical formats at $\\leq$8-bit precision within a DNN accelerator, Deep Positron, with exact multiply-and-accumulate (EMAC) units for inference. A unified analysis quantifies the trade-offs between overall network efficiency and performance across five classification tasks. Our results indicate that posits are a natural fit for DNN inference, outperforming at $\\leq$8-bit precision, and can be realized with competitive resource requirements relative to those of floating point."}}
{"id": "5lqJi5i54NU", "cdate": 1546300800000, "mdate": 1623594458885, "content": {"title": "Deep Positron: A Deep Neural Network Using the Posit Number System", "abstract": "The recent surge of interest in Deep Neural Networks (DNNs) has led to increasingly complex networks that tax computational and memory resources. Many DNNs presently use 16-bit or 32-bit floating point operations. Significant performance and power gains can be obtained when DNN accelerators support low-precision numerical formats. Despite considerable research, there is still a knowledge gap on how low-precision operations can be realized for both DNN training and inference. In this work, we propose a DNN architecture, Deep Positron, with posit numerical format operating successfully at \u22648 bits for inference. We propose a precision-adaptable FPGA soft core for exact multiply-and-accumulate for uniform comparison across three numerical formats, fixed, floating-point and posit. Preliminary results demonstrate that 8-bit posit has better accuracy than 8-bit fixed or floating-point for three different low-dimensional datasets. Moreover, the accuracy is comparable to 32-bit floating-point on a Xilinx Virtex-7 FPGA device. The trade-offs between DNN performance and hardware resources, i.e. latency, power, and resource utilization, show that posit outperforms in accuracy and latency at 8-bit and below."}}
