{"id": "SL4SwMNnwIe", "cdate": 1652737587321, "mdate": null, "content": {"title": "Acceleration in Distributed Sparse Regression", "abstract": "We study acceleration for distributed sparse regression in   {\\it  high-dimensions},  which allows the parameter size  to exceed and grow faster than the sample size. When applicable, existing  distributed algorithms employing acceleration perform poorly  in this setting, theoretically and numerically.  We  propose a new accelerated distributed algorithm suitable for high-dimensions. The method couples  a suitable instance of accelerated Nesterov's proximal gradient  with consensus and gradient-tracking mechanisms, aiming at estimating locally the gradient of the empirical loss while enforcing agreement on the local estimates.  Under standard assumptions on the statistical model and tuning parameters, the proposed method is proved to  globally converge   at {\\it linear} rate  to an estimate that is within the {\\it statistical precision} of the model. The iteration  complexity scales as $\\mathcal{O}(\\sqrt{\\kappa})$, while the communications per iteration are at most $\\widetilde{\\mathcal{O}}(\\log m/(1-\\rho))$, \n where $\\kappa$ is the restricted condition number of the empirical loss, $m$ is the number of agents, and $\\rho\\in (0,1)$ measures the network connectivity. As by-product of our design, we also report    an accelerated method for high-dimensional estimations over  master-worker architectures, which is of independent interest and  compares favorably with existing works."}}
{"id": "QEODRZ7j3L_", "cdate": 1652737586634, "mdate": null, "content": {"title": "DGD^2: A Linearly Convergent Distributed Algorithm For High-dimensional Statistical Recovery", "abstract": "We study linear regression from data distributed over a network of agents (with no master node) under high-dimensional scaling, which allows the ambient dimension to grow faster than the sample size. We propose a novel decentralization of the projected gradient algorithm whereby agents iteratively update their local estimates by a \u201cdouble-mixing\u201d mechanism, which suitably combines averages of iterates and gradients of neighbouring nodes. Under standard assumptions on the statistical model and network connectivity, the proposed method enjoys global linear convergence up to the statistical precision of the model. This improves on guarantees of (plain) DGD algorithms, whose iteration complexity grows undesirably with the ambient dimension. Our technical contribution is a novel convergence analysis that resembles (albeit different) algorithmic stability arguments extended to high-dimensions and distributed setting, which is of independent interest."}}
