{"id": "L1cYd4IJH_m", "cdate": 1686250303854, "mdate": null, "content": {"title": "Diverse Offline Imitation via Fenchel Duality", "abstract": "There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an offline skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel-Rockafellar duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert."}}
{"id": "il-rK4V1ds", "cdate": 1686250303740, "mdate": null, "content": {"title": "Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning", "abstract": "We introduce a simple but effective method for managing risk in model-based reinforcement learning with trajectory sampling that involves probabilistic safety constraints and balancing of optimism in the face of epistemic uncertainty and pessimism in the face of aleatoric uncertainty of an ensemble of stochastic neural networks. Various experiments indicate that the separation of uncertainties is essential  to performing well with data-driven MPC approaches in uncertain and safety-critical control environments."}}
{"id": "ZfC_e3HUUy", "cdate": 1686250301226, "mdate": null, "content": {"title": "Colored Noise in PPO: Improved Exploration and Performance Through Correlated Action Sampling", "abstract": "Proximal Policy Optimization (PPO), a popular on-policy deep\n reinforcement learning method, employs a stochastic policy for\n exploration. In this paper, we propose a colored noise-based\n stochastic policy variant of PPO. Previous research highlighted the\n importance of temporal correlation in action noise for effective\n exploration in off-policy reinforcement learning. Building on\n this, we investigate whether correlated noise can also enhance\n exploration in on-policy methods like PPO. We discovered that\n correlated noise for action selection improves learning performance\n and outperforms the currently popular uncorrelated white noise\n approach in on-policy methods. Unlike off-policy learning, where pink\n noise was found to be highly effective, we found that a colored\n noise, intermediate between white and pink, performed best for\n on-policy learning in PPO. We examined the impact of varying the amount \nof data collected for each update by modifying  the number of parallel \nsimulation environments for data collection and observed that\n a larger number of parallel environments benefits from more\n correlated noise. However, overall, we found four parallel\n environments to work best. Due to the significant impact and ease of\n implementation, we recommend switching to correlated noise as the\n default noise source in PPO."}}
{"id": "zvIEMSVBbT", "cdate": 1686250300222, "mdate": null, "content": {"title": "Online Learning under Adversarial Nonlinear Constraints", "abstract": "In many applications, learning systems are required to process continuous non-stationary data streams.\nWe study this problem in an online learning framework and propose an algorithm that can deal with adversarial time-varying and nonlinear constraints.\nAs we show in our work, the algorithm called Constraint Violation Velocity Projection (CVV-Pro) achieves $\\sqrt{T}$ regret and converges to the feasible set at a rate of $1/\\sqrt{T}$, despite the fact that the feasible set is slowly time-varying and a priori unknown to the learner.\nCVV-Pro only relies on local sparse linear approximations of the feasible set and therefore avoids optimizing over the entire set at each iteration, which is in sharp contrast to projected gradients or Frank-Wolfe methods.\nWe also empirically evaluate our algorithm on two-player games, where the players are subjected to a shared constraint."}}
{"id": "5qappsbO73r", "cdate": 1685982299752, "mdate": null, "content": {"title": "Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics", "abstract": "Hierarchical world models have the potential to significantly improve model-based reinforcement learning (MBRL) and planning by enabling reasoning across multiple time scales. Nonetheless, the majority of state-of-the-art MBRL methods still employ flat, non-hierarchical models. The challenge lies in learning suitable hierarchical abstractions. We propose Temporal Hierarchies from Invariant Context Kernels (THICK), an algorithm that learns a world model hierarchy based on discrete latent dynamics. The lower level of the THICK world model selectively updates parts of its latent state sparsely in time, forming invariant contexts. The higher level is trained exclusively to predict situations involving these sparse context state changes. Our experiments demonstrate that THICK learns categorical, interpretable, temporal abstractions on the high level while maintaining precise low-level predictions. Furthermore, we show that the developing hierarchical predictive model can seamlessly enhance the abilities of MBRL or planning methods. We believe that THICK-like, hierarchical world models will be key for developing more sophisticated agents capable of exploring, planning, and reasoning about the future across multiple time scales."}}
{"id": "kWvzjOjFF0", "cdate": 1685532024554, "mdate": null, "content": {"title": "Regularity as Intrinsic Reward for Free Play", "abstract": "We propose regularity as a novel reward signal for intrinsically-motivated reinforcement learning. Taking inspiration from child development, we postulate that striving for structure and order helps guide exploration towards a subspace of tasks that are not favored by naive uncertainty-based intrinsic rewards. Our generalized formulation of Regularity as Intrinsic Reward (RaIR) allows us to operationalize it within model-based reinforcement learning. In a synthetic environment, we showcase the plethora of structured patterns that can emerge from pursuing this regularity objective. We also demonstrate the strength of our method in a multi-object robotic manipulation environment. We incorporate RaIR into free play and use it to complement the model\u2019s epistemic uncertainty as an intrinsic reward. Doing so, we witness the autonomous construction of towers and other regular structures during free play, which leads to a substantial improvement in zero-shot downstream task performance on assembly tasks."}}
{"id": "F9YMQInsDcf", "cdate": 1685532019517, "mdate": null, "content": {"title": "Goal-conditioned Offline Planning from Curious Exploration", "abstract": "Curiosity has established itself as a powerful exploration strategy in deep reinforcement learning. Notably, leveraging expected future novelty as intrinsic motivation has been shown to efficiently generate exploratory trajectories, as well as a robust dynamics model. We consider the challenge of extracting goal-conditioned behavior from the products of such unsupervised exploration techniques, without any additional environment interaction. We find that conventional goal-conditioned reinforcement learning approaches for extracting a value function and policy fall short in this difficult offline setting. By analyzing the geometry of optimal goal-conditioned value functions, we relate this issue to a specific class of estimation artifacts in learned values. In order to mitigate their occurrence, we propose to combine model-based planning over learned value landscapes with a graph-based value aggregation scheme. We show how this combination can correct both local and global artifacts, obtaining significant improvements in zero-shot goal-reaching performance across diverse simulated environments."}}
{"id": "YmIJ7xR1FdP", "cdate": 1681800667720, "mdate": 1681800667720, "content": {"title": "Inductive biases in deep learning models for weather prediction", "abstract": "Deep learning has recently gained immense popularity in the Earth sciences as it enables us to formulate purely data-driven models of complex Earth system processes. Deep learning-based weather prediction (DLWP) models have made significant progress in the last few years, achieving forecast skills comparable to established numerical weather prediction (NWP) models with comparatively lesser computational costs. In order to train accurate, reliable, and tractable DLWP models with several millions of parameters, the model design needs to incorporate suitable inductive biases that encode structural assumptions about the data and modelled processes. When chosen appropriately, these biases enable faster learning and better generalisation to unseen data. Although inductive biases play a crucial role in successful DLWP models, they are often not stated explicitly and how they contribute to model performance remains unclear. Here, we review and analyse the inductive biases of six state-of-the-art DLWP models, involving a deeper look at five key design elements: input data, forecasting objective, loss components, layered design of the deep learning architectures, and optimisation methods. We show how the design choices made in each of the five design elements relate to structural assumptions. Given recent developments in the broader DL community, we anticipate that the future of DLWP will likely see a wider use of foundation models -- large models pre-trained on big databases with self-supervised learning -- combined with explicit physics-informed inductive biases that allow the models to provide competitive forecasts even at the more challenging subseasonal-to-seasonal scales."}}
{"id": "dyRVJkqdyKv", "cdate": 1672531200000, "mdate": 1684171136621, "content": {"title": "Minsight: A Fingertip-Sized Vision-Based Tactile Sensor for Robotic Manipulation", "abstract": "Intelligent interaction with the physical world requires perceptual abilities beyond vision and hearing; vibrant tactile sensing is essential for autonomous robots to dexterously manipulate unfamiliar objects or safely contact humans. Therefore, robotic manipulators need high-resolution touch sensors that are compact, robust, inexpensive, and efficient. The soft vision-based haptic sensor presented herein is a miniaturized and optimized version of the previously published sensor Insight. Minsight has the size and shape of a human fingertip and uses machine learning methods to output high-resolution maps of 3D contact force vectors at 60 Hz. Experiments confirm its excellent sensing performance, with a mean absolute force error of 0.07 N and contact location error of 0.6 mm across its surface area. Minsight's utility is shown in two robotic tasks on a 3-DoF manipulator. First, closed-loop force control enables the robot to track the movements of a human finger based only on tactile data. Second, the informative value of the sensor output is shown by detecting whether a hard lump is embedded within a soft elastomer with an accuracy of 98%. These findings indicate that Minsight can give robots the detailed fingertip touch sensing needed for dexterous manipulation and physical human-robot interaction."}}
{"id": "WEqy_IlnA-", "cdate": 1672531200000, "mdate": 1684171135854, "content": {"title": "Predicting the Force Map of an ERT-Based Tactile Sensor Using Simulation and Deep Networks", "abstract": "Electrical resistance tomography (ERT) can be used to create large-scale soft tactile sensors that are flexible and robust. Good performance requires a fast and accurate mapping from the sensor\u2019s sequential voltage measurements to the distribution of force across its surface. However, particularly with multiple contacts, this task is challenging for both previously developed approaches: physics-based modeling and end-to-end data-driven learning. Some promising results were recently achieved using sim-to-real transfer learning, but estimating multiple contact locations and accurate contact forces remains difficult because simulations tend to be less accurate with a high number of contact locations and/or high force. This paper introduces a modular hybrid method that combines simulation data synthesized from an electromechanical finite element model with real measurements collected from a new ERT-based tactile sensor. We use about 290 000 simulated and 90000 real measurements to train two deep neural networks: the first (Transfer-Net) captures the inevitable gap between simulation and reality, and the second (Recon-Net) reconstructs contact forces from voltage measurements. The number of contacts, contact locations, force magnitudes, and contact diameters are evaluated for a manually collected multi-contact dataset of 150 measurements. Our modular pipeline\u2019s results outperform predictions by both a physics-based model and end-to-end learning. Note to Practitioners\u2013ERT-based tactile sensors use high-speed voltage measurements from electrodes distributed over a piezoresistive area to output a force map that shows where contact is occurring, and how strong each contact is. Such sensors hold promise for giving robots and other devices a sense of touch over large surfaces with low hardware complexity. However, the software problem of converting voltages to an accurate force map has not previously been solved well, requiring either extensive model calibration or extensive contact data collection. This paper suggests a hybrid approach where a straightforward physics model simulates multi-contact experiments that are too costly to acquire in reality and a practical automatic indentation setup acquires real but geometrically limited multi-contact data. Although the number of real measurements required to learn the discrepancy between the sensor and the model is still large due to the inherent inverse nature of ERT-based tactile sensors, our combination of simulation and deep networks achieves better performance than either physical modeling or learning alone. This approach can advance practical large-area tactile sensing for industrial automation systems where multiple contacts occur, such as in manufacturing and assistive robotics. It could also likely be adapted to other nonlinear inverse problems."}}
