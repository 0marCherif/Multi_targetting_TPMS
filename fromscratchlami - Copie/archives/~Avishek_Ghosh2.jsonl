{"id": "Dsg-ojMws-U", "cdate": 1609459200000, "mdate": null, "content": {"title": "Escaping Saddle Points in Distributed Newton's Method with Communication efficiency and Byzantine Resilience", "abstract": "The problem of saddle-point avoidance for non-convex optimization is quite challenging in large scale distributed learning frameworks, such as Federated Learning, especially in the presence of Byzantine workers. The celebrated cubic-regularized Newton method of \\cite{nest} is one of the most elegant ways to avoid saddle-points in the standard centralized (non-distributed) setup. In this paper, we extend the cubic-regularized Newton method to a distributed framework and simultaneously address several practical challenges like communication bottleneck and Byzantine attacks. Note that the issue of saddle-point avoidance becomes more crucial in the presence of Byzantine machines since rogue machines may create \\emph{fake local minima} near the saddle-points of the loss function, also known as the saddle-point attack. Being a second order algorithm, our iteration complexity is much lower than the first order counterparts. Furthermore we use compression (or sparsification) techniques like $\\delta$-approximate compression for communication efficiency. We obtain theoretical guarantees for our proposed scheme under several settings including approximate (sub-sampled) gradients and Hessians. Moreover, we validate our theoretical findings with experiments using standard datasets and several types of Byzantine attacks, and obtain an improvement of $25\\%$ with respect to first order methods in iteration complexity."}}
{"id": "xDccQeerMsQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Max-affine regression with universal parameter estimation for small-ball designs", "abstract": "We study the max-affine regression model, where the unknown regression function is modeled as a maximum of a fixed number of affine functions. In recent work [1], we showed that end-to-end parameter estimates were obtainable using this model with an alternating minimization (AM) algorithm provided the covariates (or designs) were normally distributed, and chosen independently of the underlying parameters. In this paper, we show that AM is significantly more robust than the setting of [1]: It converges locally under small-ball design assumptions (which is a much broader class, including bounded log-concave distributions), and even when the underlying parameters are chosen with knowledge of the realized covariates. Once again, the final rate obtained by the procedure is near-parametric and minimax optimal (up to a polylogarithmic factor) as a function of the dimension, sample size, and noise variance. As a by-product of our analysis, we obtain convergence guarantees on a classical algorithm for the (real) phase retrieval problem in the presence of noise under considerably weaker assumptions on the design distribution than was previously known."}}
{"id": "w_bTgJAnNyr", "cdate": 1577836800000, "mdate": null, "content": {"title": "Communication Efficient and Byzantine Tolerant Distributed Learning", "abstract": "We develop a communication-efficient distributed learning algorithm that is robust against Byzantine worker machines. We propose and analyze a distributed gradient-descent algorithm that performs a simple thresholding based on gradient norms to mitigate Byzantine failures. We show the (statistical) error-rate of our algorithm matches that of Yin et al., 2018, which uses more complicated schemes (like coordinate-wise median or trimmed mean). Furthermore, for communication efficiency, we consider a generic class of \u03b4-approximate compressors from Karimireddy et al., 2019, that encompasses sign-based compressors and top-k sparsification. Our algorithm uses compressed gradients and gradient norms for aggregation and Byzantine removal respectively. We establish the statistical error rate of the algorithm for arbitrary (convex or non-convex) smooth loss function. We show that, in certain regime of \u03b4, the rate of convergence is not affected by the compression operation. We have experimentally validated our results and shown good performance in convergence for convex (least-square regression) and non-convex (neural network training) problems."}}
{"id": "vwNzosBtR_v", "cdate": 1577836800000, "mdate": null, "content": {"title": "An Efficient Framework for Clustered Federated Learning", "abstract": "We address the problem of Federated Learning (FL) where users are distributed and partitioned into clusters. This setup captures settings where different groups of users have their own objectives (learning tasks) but by aggregating their data with others in the same cluster (same learning task), they can leverage the strength in numbers in order to perform more efficient Federated Learning. We propose a new framework dubbed the Iterative Federated Clustering Algorithm (IFCA), which alternately estimates the cluster identities of the users and optimizes model parameters for the user clusters via gradient descent. We analyze the convergence rate of this algorithm first in a linear model with squared loss and then for generic strongly convex and smooth loss functions. We show that in both settings, with good initialization, IFCA converges at an exponential rate, and discuss the optimality of the statistical error rate. When the clustering structure is ambiguous, we propose to train the models by combining IFCA with the weight sharing technique in multi-task learning. In the experiments, we show that our algorithm can succeed even if we relax the requirements on initialization with random initialization and multiple restarts. We also present experimental results showing that our algorithm is efficient in non-convex problems such as neural networks. We demonstrate the benefits of IFCA over the baselines on several clustered FL benchmarks."}}
{"id": "cMr8AWeHmxs", "cdate": 1577836800000, "mdate": null, "content": {"title": "Alternating Minimization Converges Super-Linearly for Mixed Linear Regression", "abstract": "We address the problem of solving mixed random linear equations. In this problem, we have unlabeled observations coming from multiple linear regressions, and each observation corresponds to exactly..."}}
{"id": "_mLyE8NH_f", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distributed Newton Can Communicate Less and Resist Byzantine Workers", "abstract": "We develop a distributed second order optimization algorithm that is communication-efficient as well as robust against Byzantine failures of the worker machines. We propose an iterative approximate Newton-type algorithm, where the worker machines communicate \\emph{only once} per iteration with the central machine. This is in sharp contrast with the state-of-the-art distributed second order algorithms like GIANT \\cite{giant}, DINGO\\cite{dingo}, where the worker machines send (functions of) local gradient and Hessian sequentially; thus ending up communicating twice with the central machine per iteration. Furthermore, we employ a simple norm based thresholding rule to filter-out the Byzantine worker machines. We establish the linear-quadratic rate of convergence of our proposed algorithm and establish that the communication savings and Byzantine resilience attributes only correspond to a small statistical error rate for arbitrary convex loss functions. To the best of our knowledge, this is the first work that addresses the issue of Byzantine resilience in second order distributed optimization. Furthermore, we validate our theoretical results with extensive experiments on synthetically generated and benchmark LIBSVM \\cite{libsvm} data-set and demonstrate convergence guarantees."}}
{"id": "DiUlM4DY0fp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Problem-Complexity Adaptive Model Selection for Stochastic Linear Bandits", "abstract": "We consider the problem of model selection for two popular stochastic linear bandit settings, and propose algorithms that adapts to the unknown problem complexity. In the first setting, we consider the $K$ armed mixture bandits, where the mean reward of arm $i \\in [K]$, is $\\mu_i+ \\langle \\alpha_{i,t},\\theta^* \\rangle $, with $\\alpha_{i,t} \\in \\mathbb{R}^d$ being the known context vector and $\\mu_i \\in [-1,1]$ and $\\theta^*$ are unknown parameters. We define $\\|\\theta^*\\|$ as the problem complexity and consider a sequence of nested hypothesis classes, each positing a different upper bound on $\\|\\theta^*\\|$. Exploiting this, we propose Adaptive Linear Bandit (ALB), a novel phase based algorithm that adapts to the true problem complexity, $\\|\\theta^*\\|$. We show that ALB achieves regret scaling of $O(\\|\\theta^*\\|\\sqrt{T})$, where $\\|\\theta^*\\|$ is apriori unknown. As a corollary, when $\\theta^*=0$, ALB recovers the minimax regret for the simple bandit algorithm without such knowledge of $\\theta^*$. ALB is the first algorithm that uses parameter norm as model section criteria for linear bandits. Prior state of art algorithms \\cite{osom} achieve a regret of $O(L\\sqrt{T})$, where $L$ is the upper bound on $\\|\\theta^*\\|$, fed as an input to the problem. In the second setting, we consider the standard linear bandit problem (with possibly an infinite number of arms) where the sparsity of $\\theta^*$, denoted by $d^* \\leq d$, is unknown to the algorithm. Defining $d^*$ as the problem complexity, we show that ALB achieves $O(d^*\\sqrt{T})$ regret, matching that of an oracle who knew the true sparsity level. This methodology is then extended to the case of finitely many arms and similar results are proven. This is the first algorithm that achieves such model selection guarantees. We further verify our results via synthetic and real-data experiments."}}
{"id": "ANH3E9cEhM3", "cdate": 1577836800000, "mdate": null, "content": {"title": "Communication Efficient Distributed Approximate Newton Method", "abstract": "In this paper, we develop a communication efficient second order distributed Newton-type algorithm. For communication efficiency, we consider a generic class of \u03b4-approximate compressors (Karimireddy et al., 2019), which includes sign-based compression and top-k sparsification. We provide three potential settings where compression can be employed; and provide rate of convergence for smooth objectives. We show that, in the regime where \u03b4 is constant, our theoretical convergence rate matches that of a state-of-the-art distributed second order algorithm called DINGO (Crane and Roosta, 2019). This implies that we get the compression for free in this regime. The full paper can be found at https://tinyurl.com/ujnpt4c."}}
{"id": "VEazuhM4x-", "cdate": 1546300800000, "mdate": null, "content": {"title": "Communication-Efficient and Byzantine-Robust Distributed Learning", "abstract": "We develop a communication-efficient distributed learning algorithm that is robust against Byzantine worker machines. We propose and analyze a distributed gradient-descent algorithm that performs a simple thresholding based on gradient norms to mitigate Byzantine failures. We show the (statistical) error-rate of our algorithm matches that of Yin et al.~\\cite{dong}, which uses more complicated schemes (coordinate-wise median, trimmed mean). Furthermore, for communication efficiency, we consider a generic class of $\\delta$-approximate compressors from Karimireddi et al.~\\cite{errorfeed} that encompasses sign-based compressors and top-$k$ sparsification. Our algorithm uses compressed gradients and gradient norms for aggregation and Byzantine removal respectively. We establish the statistical error rate for non-convex smooth loss functions. We show that, in certain range of the compression factor $\\delta$, the (order-wise) rate of convergence is not affected by the compression operation. Moreover, we analyze the compressed gradient descent algorithm with error feedback (proposed in \\cite{errorfeed}) in a distributed setting and in the presence of Byzantine worker machines. We show that exploiting error feedback improves the statistical error rate. Finally, we experimentally validate our results and show good performance in convergence for convex (least-square regression) and non-convex (neural network training) problems."}}
{"id": "OK5N_GH7ys", "cdate": 1546300800000, "mdate": null, "content": {"title": "Robust Federated Learning in a Heterogeneous Environment", "abstract": "We study a recently proposed large-scale distributed learning paradigm, namely Federated Learning, where the worker machines are end users' own devices. Statistical and computational challenges arise in Federated Learning particularly in the presence of heterogeneous data distribution (i.e., data points on different devices belong to different distributions signifying different clusters) and Byzantine machines (i.e., machines that may behave abnormally, or even exhibit arbitrary and potentially adversarial behavior). To address the aforementioned challenges, first we propose a general statistical model for this problem which takes both the cluster structure of the users and the Byzantine machines into account. Then, leveraging the statistical model, we solve the robust heterogeneous Federated Learning problem \\emph{optimally}; in particular our algorithm matches the lower bound on the estimation error in dimension and the number of data points. Furthermore, as a by-product, we prove statistical guarantees for an outlier-robust clustering algorithm, which can be considered as the Lloyd algorithm with robust estimation. Finally, we show via synthetic as well as real data experiments that the estimation error obtained by our proposed algorithm is significantly better than the non-Byzantine-robust algorithms; in particular, we gain at least by 53\\% and 33\\% for synthetic and real data experiments, respectively, in typical settings."}}
