{"id": "4JoV9g5R1M", "cdate": 1663850084051, "mdate": null, "content": {"title": "Emergent Communication with Attention", "abstract": "To develop computational agents that can better communicate with others using their own emergent language, we endow the agents with an ability to focus their attention on particular concepts in the environment. Humans often understand a thing or scene as a composite of concepts and those concepts are further mapped onto words. We implement this intuition as attention mechanisms in Speaker and Listener agents in a referential game and show attention leads to more compositional and interpretable emergent language. We also demonstrate how attention helps us understand the learned communication protocol by investigating the attention weights associated with each message symbol and the alignment of attention weights between Speaker and Listener agents. Overall, our results suggest that attention is a promising mechanism for developing more human-like emergent language."}}
{"id": "IjAiHBsG3Wi", "cdate": 1591975449944, "mdate": null, "content": {"title": "Pow-Wow: A Dataset and Study on Collaborative Communication in Pommerman", "abstract": "In multi-agent learning, agents must coordinate with each other in order to succeed.  For humans, this coordination is typically accomplished through the use of language.  In this work we perform a controlled study of human language use in a competitive team-based game, and search for useful lessons for structuring communication protocol between autonomous agents.\n\nWe construct Pow-Wow, a new dataset for studying situated goal-directed human communication.  Using the Pommerman game environment, we enlisted teams of humans to play against teams of AI agents, recording their observations, actions, and communications.\n\nWe analyze the types of communications which result in effective game strategies, annotate them accordingly, and present corpus-level statistical analysis of how trends in communications affect game outcomes.  Based on this analysis, we design a communication policy for learning agents, and show that agents which utilize communication achieve higher win-rates against baseline systems than those which do not."}}
{"id": "rk-sjz-_WS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Gender Bias in Coreference Resolution", "abstract": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \"Winogender schemas,\" we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics."}}
{"id": "H1EAr2euZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Structured Variational Autoencoder for Contextual Morphological Inflection", "abstract": "Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases."}}
{"id": "HybBsfz_ZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Break it Down for Me: A Study in Automated Lyric Annotation", "abstract": "Research Foundation - Flanders (FWO) and U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1)"}}
{"id": "Hkbomsb_WB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Programming with a Differentiable Forth Interpreter", "abstract": "Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior ..."}}
{"id": "HybwVng_WB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing", "abstract": ""}}
{"id": "H1Z-MQbuZS", "cdate": 1420070400000, "mdate": null, "content": {"title": "WOLFE: An NLP-friendly Declarative Machine Learning Stack", "abstract": "Developing machine learning algorithms for natural language processing (NLP) applications is inherently an iterative process, involving a continuous refinement of the choice of model, engineering of features, selection of inference algorithms, search for the right hyperparameters, and error analysis. Existing probabilistic program languages (PPLs) only provide partial solutions; most of them do not support commonly used models such as matrix factorization or neural networks, and do not facilitate interactive and iterative programming that is crucial for rapid development of these models. In this demo we introduce WOLFE, a stack designed to facilitate the development of NLP applications: (1) the WOLFE language allows the user to concisely define complex models, enabling easy modification and extension, (2) the WOLFE interpreter transforms declarative machine learning code into automatically differentiable terms or, where applicable, into factor graphs that allow for complex models to be applied to real-world applications, and (3) the WOLFE IDE provides a number of different visual and interactive elements, allowing intuitive exploration and editing of the data representations, the underlying graphical models, and the execution of the inference algorithms."}}
{"id": "B1NLh2luWH", "cdate": 1420070400000, "mdate": null, "content": {"title": "Matrix and Tensor Factorization Methods for Natural Language Processing", "abstract": "Tensor and matrix factorization methods have attracted a lot of attention recently thanks to their successful applications to information extraction, knowledge base population, lexical semantics and dependency parsing. In the first part, we will first cover the basics of matrix and tensor factorization theory and optimization, and then proceed to more advanced topics involving convex surrogates and alternative losses. In the second part we will discuss recent NLP applications of these methods and show the connections with other popular methods such as transductive learning, topic models and neural networks. The aim of this tutorial is to present in detail applied factorization methods, as well as to introduce more recently proposed methods that are likely to be useful to NLP applications."}}
{"id": "S1VotzMuWB", "cdate": 1325376000000, "mdate": null, "content": {"title": "Improving NLP through Marginalization of Hidden Syntactic Structure", "abstract": "Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable. For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling. \u00a9 2012 Association for Computational Linguistics."}}
