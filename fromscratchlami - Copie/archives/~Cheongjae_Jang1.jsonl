{"id": "wrti5p4BrW", "cdate": 1683700437431, "mdate": 1683700437431, "content": {"title": "Unscented Kalman Filtering for Simultaneous Estimation of Attitude and Gyroscope Bias", "abstract": "We present an unscented Kalman filtering (UKF)\nalgorithm for simultaneously estimating attitude and gyroscope\nbias from an inertial measurement unit (IMU). The algorithm is\nformulated as a discrete-time stochastic nonlinear filter, with state\nspace given by the direct product matrix Lie group SO(3) \u00d7 R3, \nand observations in SO(3) reconstructed from IMU measurements of gravity and the earth\u2019s magnetic field. Computationally\nefficient implementations of our filter are made possible by\nformulating the state space dynamics and measurement equations\nin a way that leads to closed-form equations for covariance propagation and update. The resulting attitude estimates are invariant\nwith respect to choice of fixed and moving reference frames.\nThe performance advantages of our filter vis-a-vis existing state- `\nof-the-art IMU attitude estimation algorithms are validated via\nnumerical and hardware experiments involving both synthetic\nand real data."}}
{"id": "GYwOtBGHcRV", "cdate": 1683700231012, "mdate": 1683700231012, "content": {"title": "A Riemannian geometric framework for manifold learning of non-Euclidean data", "abstract": "A growing number of problems in data analysis and classification involve data that\nare non-Euclidean. For such problems, a naive application of vector space analysis\nalgorithms will produce results that depend on the choice of local coordinates used to\nparametrize the data. At the same time, many data analysis and classification problems\neventually reduce to an optimization, in which the criteria being minimized can be\ninterpreted as the distortion associated with a mapping between two curved spaces.\nExploiting this distortion minimizing perspective, we first show that manifold learning\nproblems involving non-Euclidean data can be naturally framed as seeking a mapping\nbetween two Riemannian manifolds that is closest to being an isometry. A family of\ncoordinate-invariant first-order distortion measures is then proposed that measure the\nproximity of the mapping to an isometry, and applied to manifold learning for non\u0002Euclidean data sets. Case studies ranging from synthetic data to human mass-shape\ndata demonstrate the many performance advantages of our Riemannian distortion\nminimization framework."}}
{"id": "szpdL1613YU", "cdate": 1683700117766, "mdate": 1683700117766, "content": {"title": "Learning to increase matching efficiency in identifying additional b-jets in the ttbb process", "abstract": "The t\u00aftH(bb) process is an essential channel in revealing the Higgs boson properties; however, its final state has an irreducible \u00af\nbackground from the t\u00aftbb process, which produces a top quark pair in association with a b quark pair. Therefore, understanding the \u00af\nt\u00aftbb process is crucial for improving the sensitivity of a search for the t \u00af \u00aftH(bb) process. To this end, when measuring the differential \u00af\ncross section of the t\u00aftbb process, we need to distinguish the b-jets originating from top quark decays and additional b-jets originating \u00af\nfrom gluon splitting. In this paper, we train deep neural networks that identify the additional b-jets in the t\u00aftbb events under the \u00af\nsupervision of a simulated t\u00aftbb event data set in which true additional b-jets are indicated. By exploiting the special structure of \u00af\nthe t\u00aftbb event data, several loss functions are proposed and minimized to directly increase matching efficiency, i.e., the accuracy \u00af\nof identifying additional b-jets. We show that, via a proof-of-concept experiment using synthetic data, our method can be more\nadvantageous for improving matching efficiency than the deep learning-based binary classification approach presented in [1]. Based\non simulated t\u00aftbb event data in the lepton+jets channel from pp collision at \u00af \u221as \u0003 13 TeV, we then verify that our method can identify\nadditional b-jets more accurately: compared with the approach in [1], the matching efficiency improves from 62.1% to 64.5% and\nfrom 59.9% to 61.7% for the leading order and the next-to-leading order simulations, respectively."}}
{"id": "_q7A0m3vXH0", "cdate": 1663850551553, "mdate": null, "content": {"title": "Geometrically regularized autoencoders for non-Euclidean data", "abstract": "Regularization is almost {\\it de rigueur} when designing autoencoders that are sparse and robust to noise. Given the recent surge of interest in machine learning problems involving non-Euclidean data, in this paper we address the regularization of autoencoders on curved spaces. We show that by ignoring the underlying geometry of the data and applying standard vector space regularization techniques, autoencoder performance can be severely degraded, or worse, training can fail to converge. Assuming that both the data space and latent space can be modeled as Riemannian manifolds, we show how to construct regularization terms in a coordinate-invariant way, and develop geometric generalizations of the denoising autoencoder and reconstruction contractive autoencoder such that the essential properties that enable the estimation of the derivative of the log-probability density are preserved. Drawing upon various non-Euclidean data sets, we show that our geometric autoencoder regularization techniques can have important performance advantages over vector-spaced methods while avoiding other breakdowns that can result from failing to account for the underlying geometry."}}
{"id": "bH-kCY6LdKg", "cdate": 1663850464672, "mdate": null, "content": {"title": "A new characterization of the edge of stability based on a sharpness measure aware of batch gradient distribution", "abstract": "For full-batch gradient descent (GD), it has been empirically shown that the sharpness, the top eigenvalue of the Hessian, increases and then hovers above $2/\\text{(learning rate)}$, and this is called ``the edge of stability'' phenomenon. However, it is unclear why the sharpness is somewhat larger than $2/\\text{(learning rate)}$ and how this can be extended to general mini-batch stochastic gradient descent (SGD). We propose a new sharpness measure (interaction-aware-sharpness) aware of the \\emph{interaction} between the batch gradient distribution and the loss landscape geometry. This leads to a more refined and general characterization of the edge of stability for SGD. Moreover, based on the analysis of a concentration measure of the batch gradient, we propose a more accurate scaling rule, Linear and Saturation Scaling Rule (LSSR), between batch size and learning rate."}}
{"id": "KHoV9zn1jLE", "cdate": 1652737726295, "mdate": null, "content": {"title": "Implicitly regularized interaction between SGD and the loss landscape geometry", "abstract": "We study unstable dynamics of stochastic gradient descent (SGD) and its impact on generalization in neural networks. We find that SGD induces an implicit regularization on the interaction between the gradient distribution and the loss landscape geometry. Moreover, based on the analysis of a concentration measure of the batch gradient, we propose a more accurate scaling rule, Linear and Saturation Scaling Rule (LSSR), between batch size and learning rate."}}
{"id": "AVh_HTC76u", "cdate": 1652737717766, "mdate": null, "content": {"title": "A Reparametrization-Invariant Sharpness Measure Based on Information Geometry", "abstract": "It has been observed that the generalization performance of neural networks correlates with the sharpness of their loss landscape. Dinh et al. (2017) have observed that existing formulations of sharpness measures fail to be invariant with respect to scaling and reparametrization. While some scale-invariant measures have recently been proposed, reparametrization-invariant measures are still lacking. Moreover, they often do not provide any theoretical insights into generalization performance nor lead to practical use to improve the performance. Based on an information geometric analysis of the neural network parameter space, in this paper we propose a reparametrization-invariant sharpness measure that captures the change in loss with respect to changes in the probability distribution modeled by neural networks, rather than with respect to changes in the parameter values. We reveal some theoretical connections of our measure to generalization performance. In particular, experiments confirm that using our measure as a regularizer in neural network training significantly improves performance."}}
