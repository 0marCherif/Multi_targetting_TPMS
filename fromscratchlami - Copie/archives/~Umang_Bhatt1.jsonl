{"id": "VBAEc1scnz7", "cdate": 1684671230210, "mdate": 1684671230210, "content": {"title": "Human-in-the-Loop Mixup", "abstract": "Aligning model representations to humans has been found to improve robustness and\ngeneralization. However, such methods often focus on standard observational data. Synthetic\ndata is proliferating and powering many advances in machine learning; yet, it is not always\nclear whether synthetic labels are perceptually aligned to humans \u2013 rendering it likely model\nrepresentations are not human aligned. We focus on the synthetic data used in mixup: a\npowerful regularizer shown to improve model robustness, generalization, and calibration. We\ndesign a comprehensive series of elicitation interfaces, which we release as HILL MixE Suite,\nand recruit 159 participants to provide perceptual judgments along with their uncertainties,\nover mixup examples. We find that human perceptions do not consistently align with the\nlabels traditionally used for synthetic points, and begin to demonstrate the applicability of\nthese findings to potentially increase the reliability of downstream models, particularly when\nincorporating human uncertainty. We release all elicited judgments in a new data hub we\ncall H-Mix."}}
{"id": "wNCOHjUaIVJ", "cdate": 1684671117356, "mdate": 1684671117356, "content": {"title": "Human Uncertainty in Concept-Based AI Systems", "abstract": "Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks."}}
{"id": "j9yLaMK4BME", "cdate": 1684670824147, "mdate": 1684670824147, "content": {"title": "Eliciting and learning with soft labels from every annotator", "abstract": "The labels used to train machine learning (ML) models are of paramount importance. Typically for ML classification tasks, datasets contain hard labels, yet learning using soft labels has been shown to yield benefits for model generalization, robustness, and calibration. Earlier work found success in forming soft labels from multiple annotators' hard labels; however, this approach may not converge to the best labels and necessitates many annotators, which can be expensive and inefficient. We focus on efficiently eliciting soft labels from individual annotators. We collect and release a dataset of soft labels (which we call CIFAR-10S) over the CIFAR-10 test set via a crowdsourcing study (N= 248). We demonstrate that learning with our labels achieves comparable model performance to prior approaches while requiring far fewer annotators--albeit with significant temporal costs per elicitation. Our elicitation methodology therefore shows nuanced promise in enabling practitioners to enjoy the benefits of improved model performance and reliability with fewer annotators, and serves as a guide for future dataset curators on the benefits of leveraging richer information, such as categorical uncertainty, from individual annotators."}}
{"id": "zwywBS3GyFs", "cdate": 1677713826810, "mdate": null, "content": {"title": "GeValDi: Generative Validation of Discriminative Models", "abstract": "Evaluation of machine learning (ML) models is critically important for reliable use. Though typically done via unseen data, such validation datasets often need to be large and hard to procure; additionally, mutliple models may perform equally well on such datasets. To address these challenges, we offer GeValdi: a data-efficient method to validate discriminative classifiers by creating samples where such classifiers maximally differ. We demonstrate how such ``maximally different samples'' can be constructed and leveraged to probe the failure modes of classifiers and offer a hierarchically-aware metric to further support fine-grained, comparative model evaluation. "}}
{"id": "JHcj9gcks3", "cdate": 1676827102536, "mdate": null, "content": {"title": "On the Informativeness of Supervision Signals", "abstract": "Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. We use information theory to compare how a number of commonly used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalization. We validate these results empirically in a series of experiments with over 1 million crowdsourced image annotations and conduct a cost-benefit analysis to establish a tradeoff curve that enables users to optimize the cost of supervising representation learning on their own datasets.\n\n\n"}}
{"id": "BW6oQ0qZl0El", "cdate": 1676827080010, "mdate": null, "content": {"title": "Human-in-the-Loop Mixup", "abstract": "Aligning model representations to humans has been found to improve robustness and generalization. However, such methods often focus on standard observational data. Synthetic data is proliferating and powering many advances in machine learning; yet, it is not always clear whether synthetic labels are perceptually aligned to humans -- rendering it likely model representations are not human aligned. We focus on the synthetic data used in mixup: a powerful regularizer shown to improve model robustness, generalization, and calibration. We design a comprehensive series of elicitation interfaces, which we release as HILL MixE Suite, and recruit 159 participants to provide perceptual judgments along with their uncertainties, over mixup examples. We find that human perceptions do not consistently align with the labels traditionally used for synthetic points, and begin to demonstrate the applicability of these findings to potentially increase the reliability of downstream models, particularly when incorporating human uncertainty. We release all elicited judgments in a new data hub we call H-Mix."}}
{"id": "2BZDR5JMMS_", "cdate": 1676472365305, "mdate": null, "content": {"title": "GeValDi: Generative Validation of Discriminative Models", "abstract": "The evaluation of machine learning (ML) models is a core tenet of trustworthy use. Evaluation is typically done via a held-out dataset. However, such validation datasets often need to be large and are hard to procure; further, multiple models may perform equally well on such sets. To address these challenges, we offer GeValdi: an efficient method to validate discriminative classifiers by creating samples where such classifiers maximally differ. We demonstrate how such ``maximally different samples'' can be constructed via and leveraged to probe the failure mode of classifiers and offer a hierarchically-aware metric to further support fine-grained, comparative model evaluation. "}}
{"id": "y74x53ooSig", "cdate": 1640995200000, "mdate": 1682349143430, "content": {"title": "How transparency modulates trust in artificial intelligence", "abstract": ""}}
{"id": "jvN1kc_5mv", "cdate": 1640995200000, "mdate": 1682349143256, "content": {"title": "On the Fairness of Causal Algorithmic Recourse", "abstract": "Algorithmic fairness is typically studied from the perspective of predictions. Instead, here we investigate fairness from the perspective of recourse actions suggested to individuals to remedy an unfavourable classification. We propose two new fair-ness criteria at the group and individual level, which\u2014unlike prior work on equalising the average group-wise distance from the decision boundary\u2014explicitly account for causal relationships between features, thereby capturing downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We study theoretically and empirically how to enforce fair causal recourse by altering the classifier and perform a case study on the Adult dataset. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier."}}
{"id": "gYerMgCpk5", "cdate": 1640995200000, "mdate": 1682349143281, "content": {"title": "Eliciting and Learning with Soft Labels from Every Annotator", "abstract": "The labels used to train machine learning (ML) models are of paramount importance. Typically for ML classification tasks, datasets contain hard labels, yet learning using soft labels has been shown to yield benefits for model generalization, robustness, and calibration. Earlier work found success in forming soft labels from multiple annotators' hard labels; however, this approach may not converge to the best labels and necessitates many annotators, which can be expensive and inefficient. We focus on efficiently eliciting soft labels from individual annotators. We collect and release a dataset of soft labels (which we call CIFAR-10S) over the CIFAR-10 test set via a crowdsourcing study (N=248). We demonstrate that learning with our labels achieves comparable model performance to prior approaches while requiring far fewer annotators -- albeit with significant temporal costs per elicitation. Our elicitation methodology therefore shows nuanced promise in enabling practitioners to enjoy the benefits of improved model performance and reliability with fewer annotators, and serves as a guide for future dataset curators on the benefits of leveraging richer information, such as categorical uncertainty, from individual annotators."}}
