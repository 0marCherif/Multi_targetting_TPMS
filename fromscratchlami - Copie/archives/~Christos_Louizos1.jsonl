{"id": "QwL8ZGl_QGG", "cdate": 1663939404682, "mdate": null, "content": {"title": "Decentralized Learning with Random Walks and Communication-Efficient Adaptive Optimization", "abstract": "We tackle the problem of federated learning (FL) in a peer-to-peer fashion without a central server.  While prior work mainly considered gossip-style protocols for learning, our solution is based on random walks. This allows to communicate only to a single peer at a time, thereby reducing the total communication and enabling asynchronous execution. To improve convergence and reduce the need for extensive tuning, we consider an adaptive optimization method -- Adam. Two extensions reduce its communication costs: state compression and multiple local updates on each client. We theoretically analyse the convergence behaviour of the proposed algorithm and its modifications in the non-convex setting. We show that our method can achieve performance comparable to centralized FL without communication overhead. Empirical results are reported on a variety of tasks (vision, text), neural network architectures and large-scale federations (up to $\\sim342$k clients)."}}
{"id": "47DzlkyH3dM", "cdate": 1663850561088, "mdate": null, "content": {"title": "Variational Learning ISTA", "abstract": "Compressed sensing combines the power of convex optimization techniques with a sparsity inducing prior on the signal space to solve an underdetermined system of equations. For many problems, the sparsifying dictionary is not directly given, nor its existence can be assumed. Besides, the sensing matrix can change across different scenarios. Addressing these issues requires solving a sparse representation learning problem, namely dictionary learning, taking into account the epistemic uncertainty on the learned dictionaries and, finally, jointly learning sparse representations and reconstructions under varying sensing matrix conditions.\nWe propose a variant of the LISTA architecture that incorporates the sensing matrix into the architecture. In particular, we propose to learn a distribution over dictionaries via a variational approach, dubbed \\ac{VLISTA}, which approximates a posterior distribution over the dictionaries as part of an unfolded LISTA-based recovery network. Such a variational posterior distribution is updated after each iteration, and thereby adapts the dictionary according to the optimization dynamics. As a result, \\ac{VLISTA} provides a probabilistic way to jointly learn the dictionary distribution and the reconstruction algorithm with varying sensing matrices. We provide theoretical and experimental support for our architecture and show that it learns calibrated uncertainties."}}
{"id": "nAgdXgfmqj", "cdate": 1663850367966, "mdate": null, "content": {"title": "Hyperparameter Optimization through Neural Network Partitioning", "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance --- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the \"out-of-training-sample\" loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging."}}
{"id": "b-ZaBVGx8Q", "cdate": 1632875727701, "mdate": null, "content": {"title": "DP-REC: Private & Communication-Efficient Federated Learning", "abstract": "Privacy and communication efficiency are important challenges in federated training of neural networks, and combining them is still an open problem. In this work, we develop a method that unifies highly compressed communication and differential privacy (DP). We introduce a compression technique based on Relative Entropy Coding (REC) to the federated setting. With a minor modification to REC, we obtain a provably differentially private learning algorithm, DP-REC, and show how to compute its privacy guarantees. Our experiments demonstrate that DP-REC drastically reduces communication costs while providing privacy guarantees comparable to the state-of-the-art."}}
{"id": "InGI-IMDL18", "cdate": 1601308368883, "mdate": null, "content": {"title": "Secure Federated Learning of User Verification Models", "abstract": "We consider the problem of training User Verification (UV) models in federated setup, where the conventional loss functions are not applicable due to the constraints that each user has access to the data of only one class and user embeddings cannot be shared with the server or other users.  To address this problem, we propose Federated User Verification (FedUV), a framework for private and secure training of UV models. In FedUV, users jointly learn a set of vectors and maximize the  correlation  of  their  instance  embeddings  with  a  secret  user-defined  linear combination of those vectors. We show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vectors.  We present the experimental results for user verification with voice, face, and handwriting data and show that FedUV is on par with existing approaches, while not sharing the embeddings with other users or the server."}}
{"id": "eoQBpdMy81m", "cdate": 1601308352080, "mdate": null, "content": {"title": "Federated Averaging as Expectation Maximization", "abstract": "Federated averaging (FedAvg), despite its simplicity, has been the main approach in training neural networks in the federated learning setting. In this work, we show that the algorithmic choices of the FedAvg algorithm correspond to optimizing a single objective function that involves the global and all of the shard specific models using a hard version of the well known Expectation-Maximization (EM) algorithm. As a result, we gain a better understanding of the behavior and design choices of federated averaging while being able to provide interesting connections to recent literature. Based on this view, we further propose FedSparse, a version of federated averaging that employs prior distributions to promote model sparsity. In this way, we obtain a procedure that leads to reductions in both server-client and client-server communication costs as well as more efficient models. "}}
{"id": "YgrdmztE4OY", "cdate": 1601308122828, "mdate": null, "content": {"title": "Federated Mixture of Experts", "abstract": "Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity; different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user\u2019s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-specific selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model while requiring similar or less communication costs."}}
{"id": "_Y2R3L1Iye-", "cdate": 1596131154177, "mdate": null, "content": {"title": "Up or Down? Adaptive Rounding for Post-Training Quantization", "abstract": "When quantizing neural networks, assigning each\nfloating-point weight to its nearest fixed-point\nvalue is the predominant approach. We find that,\nperhaps surprisingly, this is not the best we can\ndo. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training\nquantization that adapts to the data and the task\nloss. AdaRound is fast, does not require finetuning of the network, and only uses a small\namount of unlabelled data. We start by theoretically analyzing the rounding problem for a\npre-trained neural network. By approximating the\ntask loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a\nlayer-wise local loss and propose to optimize this\nloss with a soft relaxation. AdaRound not only\noutperforms rounding-to-nearest by a significant\nmargin but also establishes a new state-of-the-art\nfor post-training quantization on several networks\nand tasks. Without fine-tuning, we can quantize\nthe weights of Resnet18 and Resnet50 to 4 bits\nwhile staying within an accuracy loss of 1%."}}
{"id": "eRYfdb9Kxl", "cdate": 1596131071764, "mdate": null, "content": {"title": "Bayesian Bits: Unifying Quantization and Pruning", "abstract": "We introduce Bayesian Bits, a practical method\nfor joint mixed precision quantization and pruning\nthrough gradient based optimization. Bayesian\nBits employs a novel decomposition of the quantization operation, which sequentially considers\ndoubling the bit width. At each new bit width,\nthe residual error between the full precision value\nand the previously rounded value is quantized.\nWe then decide whether or not to add this quantized residual error for a higher effective bit width\nand lower quantization noise. By starting with a\npower-of-two bit width, this decomposition will\nalways produce hardware-friendly configurations,\nand through an additional 0-bit option, serves\nas a unified view of pruning and quantization.\nBayesian Bits then introduces learnable stochastic gates, which collectively control the bit width\nof the given tensor. As a result, we can obtain low\nbit solutions by performing approximate inference\nover the gates, with prior distributions that encourage most of them to be switched off. We further\nshow that, under some assumptions, L0 regularization of the network parameters corresponds to\na specific instance of the aforementioned framework. We experimentally validate our proposed\nmethod on several benchmark datasets and show\nthat we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width\nequivalents"}}
{"id": "RmNckVums7", "cdate": 1579955641375, "mdate": null, "content": {"title": "DIVA: Domain Invariant Variational Autoencoder", "abstract": "We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further."}}
