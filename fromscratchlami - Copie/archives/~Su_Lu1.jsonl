{"id": "uaaZt57Ys_r", "cdate": 1640995200000, "mdate": 1651741611706, "content": {"title": "Faculty Distillation with Optimal Transport", "abstract": "The outpouring of various pre-trained models empowers knowledge distillation~(KD) by providing abundant teacher resources. Meanwhile, exploring the massive model repository to select a suitable teacher and further extracting its knowledge become daunting challenges. Standard KD fails to surmount two obstacles when training a student with the availability of plentiful pre-trained teachers, i.e., the \"faculty\". First, we need to seek out the most contributive teacher in the faculty efficiently rather than enumerating all of them for a student. Second, since the teacher may be pre-trained on different tasks w.r.t. the student, we must distill the knowledge from a more general label space. This paper studies this ``faculty distillation'' where a student performs teacher assessment and generalized knowledge reuse. We take advantage of optimal transport to construct a unifying objective for both problems, which bridges the semantic gap and measures the relatedness between a pair of models. This objective can select the most relevant teacher, and we minimize the same objective over student parameters to transfer the knowledge from the selected teacher subsequently. Experiments in various settings demonstrate the succinctness and versatility of our proposed method."}}
{"id": "Htnjc4kHsNF", "cdate": 1621629724729, "mdate": null, "content": {"title": "Towards Enabling Meta-Learning from Target Models", "abstract": "Meta-learning can extract an inductive bias from previous learning experience and assist the training of new tasks. It is often realized through optimizing a meta-model with the evaluation loss of task-specific solvers. Most existing algorithms sample non-overlapping $\\mathit{support}$ sets and $\\mathit{query}$ sets to train and evaluate the solvers respectively due to simplicity ($\\mathcal{S}$/$\\mathcal{Q}$ protocol). Different from $\\mathcal{S}$/$\\mathcal{Q}$ protocol, we can also evaluate a task-specific solver by comparing it to a target model $\\mathcal{T}$, which is the optimal model for this task or a model that behaves well enough on this task ($\\mathcal{S}$/$\\mathcal{T}$ protocol). Although being short of research, $\\mathcal{S}$/$\\mathcal{T}$ protocol has unique advantages such as offering more informative supervision, but it is computationally expensive. This paper looks into this special evaluation method and takes a step towards putting it into practice. We find that with a small ratio of tasks armed with target models, classic meta-learning algorithms can be improved a lot without consuming many resources. We empirically verify the effectiveness of $\\mathcal{S}$/$\\mathcal{T}$ protocol in a typical application of meta-learning, $\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models by fine-tuning the pre-trained network on those hard tasks, we match the task-specific solvers and target models via knowledge distillation."}}
{"id": "wskZmTN3Fla", "cdate": 1609459200000, "mdate": 1651741611734, "content": {"title": "Support-Target Protocol for Meta-Learning", "abstract": "Meta-learning can extract an inductive bias from previous learning experience and assist the training of new tasks. It is often realized through optimizing a meta-model with the evaluation loss of task-specific solvers. Most existing algorithms sample non-overlapping $\\mathit{support}$ sets and $\\mathit{query}$ sets to train and evaluate the solvers respectively due to simplicity ($\\mathcal{S}$/$\\mathcal{Q}$ protocol). Different from $\\mathcal{S}$/$\\mathcal{Q}$ protocol, we can also evaluate a task-specific solver by comparing it to a target model $\\mathcal{T}$, which is the optimal model for this task or a model that behaves well enough on this task ($\\mathcal{S}$/$\\mathcal{T}$ protocol). Although being short of research, $\\mathcal{S}$/$\\mathcal{T}$ protocol has unique advantages such as offering more informative supervision, but it is computationally expensive. This paper looks into this special evaluation method and takes a step towards putting it into practice. We find that with a small ratio of tasks armed with target models, classic meta-learning algorithms can be improved a lot without consuming many resources. We empirically verify the effectiveness of $\\mathcal{S}$/$\\mathcal{T}$ protocol in a typical application of meta-learning, $\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models by fine-tuning the pre-trained network on those hard tasks, we match the task-specific solvers and target models via knowledge distillation."}}
{"id": "mnU9-TwOya", "cdate": 1609459200000, "mdate": 1651741611736, "content": {"title": "Few-Shot Action Recognition with Compromised Metric via Optimal Transport", "abstract": "Although vital to computer vision systems, few-shot action recognition is still not mature despite the wide research of few-shot image classification. Popular few-shot learning algorithms extract a transferable embedding from seen classes and reuse it on unseen classes by constructing a metric-based classifier. One main obstacle to applying these algorithms in action recognition is the complex structure of videos. Some existing solutions sample frames from a video and aggregate their embeddings to form a video-level representation, neglecting important temporal relations. Others perform an explicit sequence matching between two videos and define their distance as matching cost, imposing too strong restrictions on sequence ordering. In this paper, we propose Compromised Metric via Optimal Transport (CMOT) to combine the advantages of these two solutions. CMOT simultaneously considers semantic and temporal information in videos under Optimal Transport framework, and is discriminative for both content-sensitive and ordering-sensitive tasks. In detail, given two videos, we sample segments from them and cast the calculation of their distance as an optimal transport problem between two segment sequences. To preserve the inherent temporal ordering information, we additionally amend the ground cost matrix by penalizing it with the positional distance between a pair of segments. Empirical results on benchmark datasets demonstrate the superiority of CMOT."}}
{"id": "DYYJ0eyA1rT", "cdate": 1609459200000, "mdate": 1651741611736, "content": {"title": "Tailoring Embedding Function to Heterogeneous Few-Shot Tasks by Global and Local Feature Adaptors", "abstract": "Few-Shot Learning (FSL) is essential for visual recognition. Many methods tackle this challenging problem via learning an embedding function from seen classes and transfer it to unseen classes with a few labeled instances. Researchers recently found it beneficial to incorporate task-specific feature adaptation into FSL models, which produces the most representative features for each task. However, these methods ignore the diversity of classes and apply a global transformation to the task. In this paper, we propose Global and Local Feature Adaptor (GLoFA), a unifying framework that tailors the instance representation to specific tasks by global and local feature adaptors. We claim that class-specific local transformation helps to improve the representation ability of feature adaptor. Global masks tend to capture sketchy patterns, while local masks focus on detailed characteristics. A strategy to measure the relationship between instances adaptively based on the characteristics of both tasks and classes endow GLoFA with the ability to handle mix-grained tasks. GLoFA outperforms other methods on a heterogeneous task distribution and achieves competitive results on benchmark datasets."}}
{"id": "27WDGDwvMND", "cdate": 1609459200000, "mdate": 1651741611713, "content": {"title": "Towards Enabling Meta-Learning from Target Models", "abstract": "Meta-learning can extract an inductive bias from previous learning experience and assist the training of new tasks. It is often realized through optimizing a meta-model with the evaluation loss of task-specific solvers. Most existing algorithms sample non-overlapping $\\mathit{support}$ sets and $\\mathit{query}$ sets to train and evaluate the solvers respectively due to simplicity ($\\mathcal{S}$/$\\mathcal{Q}$ protocol). Different from $\\mathcal{S}$/$\\mathcal{Q}$ protocol, we can also evaluate a task-specific solver by comparing it to a target model $\\mathcal{T}$, which is the optimal model for this task or a model that behaves well enough on this task ($\\mathcal{S}$/$\\mathcal{T}$ protocol). Although being short of research, $\\mathcal{S}$/$\\mathcal{T}$ protocol has unique advantages such as offering more informative supervision, but it is computationally expensive. This paper looks into this special evaluation method and takes a step towards putting it into practice. We find that with a small ratio of tasks armed with target models, classic meta-learning algorithms can be improved a lot without consuming many resources. We empirically verify the effectiveness of $\\mathcal{S}$/$\\mathcal{T}$ protocol in a typical application of meta-learning, $\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models by fine-tuning the pre-trained network on those hard tasks, we match the task-specific solvers and target models via knowledge distillation."}}
{"id": "p_TVA1B17sl", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distilling Cross-Task Knowledge via Relationship Matching", "abstract": "The discriminative knowledge from a high-capacity deep neural network (a.k.a. the \"teacher\") could be distilled to facilitate the learning efficacy of a shallow counterpart (a.k.a. the \"student\"). This paper deals with a general scenario reusing the knowledge from a cross-task teacher --- two models are targeting non-overlapping label spaces. We emphasize that the comparison ability between instances acts as an essential factor threading knowledge across domains, and propose the RElationship FacIlitated Local cLassifiEr Distillation (ReFilled) approach, which decomposes the knowledge distillation flow into branches for embedding and the top-layer classifier. In particular, different from reconciling the instance-label confidence between models, ReFilled requires the teacher to reweight the hard triplets push forwarded by the student so that the similarity comparison levels between instances are matched. A local embedding-induced classifier from the teacher further supervises the student's classification confidence. ReFilled demonstrates its effectiveness when reusing cross-task models, and also achieves state-of-the-art performance on the standard knowledge distillation benchmarks. The code of the paper can be accessed at https://github.com/njulus/ReFilled."}}
{"id": "yQYcuVxdXVt", "cdate": 1546300800000, "mdate": 1651741611716, "content": {"title": "Multi-View Anomaly Detection: Neighborhood in Locality Matters", "abstract": "Identifying anomalies in multi-view data is a difficult task due to the complicated data characteristics of anomalies. Specifically, there are two types of anomalies in multi-view data\u2013anomalies that have inconsistent features across multiple views and anomalies that are consistently anomalous in each view. Existing multi-view anomaly detection approaches have some issues, e.g., they assume multiple views of a normal instance share consistent and normal clustering structures while anomaly exhibits anomalous clustering characteristics across multiple views. When there are no clusters in data, it is difficult for existing approaches to detect anomalies. Besides, existing approaches construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. The objective is formulated to profile normal instances, but not to estimate the set of normal instances, which results in sub-optimal detectors. In addition, the model trained to profile normal instances uses the entire dataset including anomalies. However, anomalies could undermine the model, i.e., the model is not robust to anomalies. To address these issues, we propose the nearest neighborbased MUlti-View Anomaly Detection (MUVAD) approach. Specifically, we first propose an anomaly measurement criterion and utilize this criterion to formulate the objective of MUVAD to estimate the set of normal instances explicitly. We further develop two concrete relaxations for implementing the MUVAD as MUVAD-QPR and MUVAD-FSR. Experimental results validate the superiority of the proposed MUVAD approaches."}}
{"id": "BmEvIT-g_aH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Multi-View Anomaly Detection: Neighborhood in Locality Matters.", "abstract": "Identifying anomalies in multi-view data is a difficult task due to the complicated data characteristics of anomalies. Specifically, there are two types of anomalies in multi-view data\u2013anomalies that have inconsistent features across multiple views and anomalies that are consistently anomalous in each view. Existing multi-view anomaly detection approaches have some issues, e.g., they assume multiple views of a normal instance share consistent and normal clustering structures while anomaly exhibits anomalous clustering characteristics across multiple views. When there are no clusters in data, it is difficult for existing approaches to detect anomalies. Besides, existing approaches construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. The objective is formulated to profile normal instances, but not to estimate the set of normal instances, which results in sub-optimal detectors. In addition, the model trained to profile normal instances uses the entire dataset including anomalies. However, anomalies could undermine the model, i.e., the model is not robust to anomalies. To address these issues, we propose the nearest neighborbased MUlti-View Anomaly Detection (MUVAD) approach. Specifically, we first propose an anomaly measurement criterion and utilize this criterion to formulate the objective of MUVAD to estimate the set of normal instances explicitly. We further develop two concrete relaxations for implementing the MUVAD as MUVAD-QPR and MUVAD-FSR. Experimental results validate the superiority of the proposed MUVAD approaches."}}
