{"id": "ndud6Uw512", "cdate": 1684335978748, "mdate": 1684335978748, "content": {"title": "A support vector machine approach to identification of proteins relevant to learning in a mouse model of Down Syndrome", "abstract": "Down Syndrome is a common disorder which\ncauses intellectual disability among other symptoms. To date, no\ntreatment exists for the learning difficulties associated with Down\nSyndrome. However, the pharmaceutical drug memantine has\nbeen shown to improve learning ability in a Down Syndrome\nmodel of mice (Ts65Dn) exposed to Context Fear Conditioning\n(CFC), an existing technique used in determining the extent of\nlearning capability of mice. While the effect of memantine on\nlearning capability in Ts65Dn mice is significant, the biological\nmechanism responsible for restoration of learning capability by\nmemantine is poorly understood. One possible way to\ncharacterize this mechanism is by analyzing the neural protein\nprofile data of normal and Down Syndrome mice with and\nwithout memantine treatment. In this work, we use a series of\nlinear support vector machines to model the differential\nexpression of 77 proteins obtained from the nuclear cortex of\nnormal and Ts65Dn mice, with and without memantine\ntreatment and with and without CFC stimulation. We use feature\nselection by weight threshold to select those proteins which play a\nsignificant role in characterizing each model. Per our findings,\nthese subsets of proteins can be used to build more accurate\nclassification models of the data than those subsets chosen using\nunsupervised learning or statistical analyses in previous studies.\nWe recommend that the subsets of proteins selected using our\nproposed method be utilized in further biological study aiming to\nunderstand the effects of memantine on learning restoration. "}}
{"id": "SkWCS2-dbB", "cdate": 1514764800000, "mdate": null, "content": {"title": "K-means clustering using random matrix sparsification", "abstract": "K-means clustering algorithm using Lloyd\u2019s heuristic is one of the most commonly used tools in data mining and machine learning that shows promising performance. However, it suffers from a high com..."}}
{"id": "H1ZjasZuWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Improved nearest neighbor search using auxiliary information and priority functions", "abstract": "Nearest neighbor search using random projection trees has recently been shown to achieve superior performance, in terms of better accuracy while retrieving less number of data points, compared to l..."}}
{"id": "r1ZIa8WuZS", "cdate": 1325376000000, "mdate": null, "content": {"title": "Near-optimal Differentially Private Principal Components", "abstract": "Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension. Many current data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We demonstrate that on real data, there this a large performance gap between the existing methods and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling."}}
{"id": "HJEn-wbubr", "cdate": 1230768000000, "mdate": null, "content": {"title": "Semi-supervised Learning using Sparse Eigenfunction Bases", "abstract": "We present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices. It turns out that when the \\emph{cluster assumption} holds, that is, when the high density regions are sufficiently separated by low density valleys, each high density area corresponds to a unique representative eigenvector. Linear combination of such eigenvectors (or, more precisely, of their Nystrom extensions) provide good candidates for good classification functions. By first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data with Lasso to select a classifier in the span of these eigenvectors, we obtain a classifier, which has a very sparse representation in this basis. Importantly, the sparsity appears naturally from the cluster assumption. Experimental results on a number of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis)."}}
{"id": "ry-O1ubd-S", "cdate": 1167609600000, "mdate": null, "content": {"title": "The Value of Labeled and Unlabeled Examples when the Model is Imperfect", "abstract": "Semi-supervised learning, i.e. learning from both labeled and unlabeled data has received signi(cid:2)cant attention in the machine learning literature in recent years. Still our understanding of the theoretical foundations of the usefulness of unla- beled data remains somewhat limited. The simplest and the best understood sit- uation is when the data is described by an identi(cid:2)able mixture model, and where each class comes from a pure component. This natural setup and its implications ware analyzed in [11, 5]. One important result was that in certain regimes, labeled data becomes exponentially more valuable than unlabeled data. However, in most realistic situations, one would not expect that the data comes from a parametric mixture distribution with identi(cid:2)able components. There have been recent efforts to analyze the non-parametric situation, for example, (cid:147)cluster(cid:148) and (cid:147)manifold(cid:148) assumptions have been suggested as a basis for analysis. Still, a satisfactory and fairly complete theoretical understanding of the nonparametric problem, similar to that in [11, 5] has not yet been developed. In this paper we investigate an intermediate situation, when the data comes from a probability distribution, which can be modeled, but not perfectly, by an identi(cid:2)able mixture distribution. This seems applicable to many situation, when, for example, a mixture of Gaussians is used to model the data. the contribution of this paper is an analysis of the role of labeled and unlabeled data depending on the amount of imperfection in the model."}}
{"id": "rJbFR4W_WH", "cdate": 1104537600000, "mdate": null, "content": {"title": "A framework to support multiple query optimization for complex mining tasks", "abstract": "With an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targetting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms for this purpose. We show the design of a knowledgeable cache which can store the past query results from queries on multiple datasets. We present algorithms which enable the use of the results stored in such a cache to further optimize multiple queries.We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries."}}
{"id": "Bk-bQEb_Wr", "cdate": 1104537600000, "mdate": null, "content": {"title": "Simultaneous optimization of complex mining tasks with a knowledgeable cache", "abstract": "With an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targeting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms to simultaneously optimize multiple such queries and use a knowledgeable cache to store and utilize the past query results. We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries."}}
