{"id": "FD0mho7ZCX", "cdate": 1577836800000, "mdate": null, "content": {"title": "Automatic Setting of DNN Hyper-Parameters by Mixing Bayesian Optimization and Tuning Rules", "abstract": "Deep learning techniques play an increasingly important role in industrial and research environments due to their outstanding results. However, the large number of hyper-parameters to be set may lead to errors if they are set manually. The state-of-the-art hyper-parameters tuning methods are grid search, random search, and Bayesian Optimization. The first two methods are expensive because they try, respectively, all possible combinations and random combinations of hyper-parameters. Bayesian Optimization, instead, builds a surrogate model of the objective function, quantifies the uncertainty in the surrogate using Gaussian Process Regression and uses an acquisition function to decide where to sample the new set of hyper-parameters. This work faces the field of Hyper-Parameters Optimization (HPO). The aim is to improve Bayesian Optimization applied to Deep Neural Networks. For this goal, we build a new algorithm for evaluating and analyzing the results of the network on the training and validation sets and use a set of tuning rules to add new hyper-parameters and/or to reduce the hyper-parameter search space to select a better combination."}}
{"id": "DvKPQYl3aUF", "cdate": 1577836800000, "mdate": null, "content": {"title": "MAP Inference for Probabilistic Logic Programming", "abstract": "In Probabilistic Logic Programming (PLP) the most commonly studied inference task is to compute the marginal probability of a query given a program. In this paper, we consider two other important tasks in the PLP setting: the Maximum-A-Posteriori (MAP) inference task, which determines the most likely values for a subset of the random variables given evidence on other variables, and the Most Probable Explanation (MPE) task, the instance of MAP where the query variables are the complement of the evidence variables. We present a novel algorithm, included in the PITA reasoner, which tackles these tasks by representing each problem as a Binary Decision Diagram and applying a dynamic programming procedure on it. We compare our algorithm with the version of ProbLog that admits annotated disjunctions and can perform MAP and MPE inference. Experiments on several synthetic datasets show that PITA outperforms ProbLog in many cases."}}
{"id": "B1g910HvwS", "cdate": 1569312225862, "mdate": null, "content": {"title": "KRaider: a Crawler for Linked Data", "abstract": "The aim of the Semantic Web and Linked Data principles is to create a web of data that can be processed by machines. The web of data is seen as a single globally distributed dataset. During the years, an increasing amount of data was published on the Web. In particular, large knowledge bases such as Wikidata, DBPedia, LinkedGeoData, and others are freely available as Linked Data and SPARQL endpoints. Exploring and performing reasoning tasks on such huge knowledge graphs is practically impossible. Moreover, triples involving an entity can be distributed among different datasets hosted by different SPARQL end-points. Given an entity of interest and a task, we are interested into extracting a fragment of knowledge relevant to that entity, such that the results of the given task performed on the fragment are the same as if the task was performed on the whole web of data.\nHere we propose a system, called KRaider (\u201cKnowledge Raider\u201d), for extracting the relevant fragment from different SPARQL endpoints, without the user knowing their location. The extracted triples are then converted into an OWL ontology, in order to allow inference tasks. The system is part of a - still under development - framework called SRL-Frame (\u201cStatistical Relational Learning Framework\u201d)."}}
{"id": "yl0fwdVdch9Y", "cdate": 1546300800000, "mdate": null, "content": {"title": "Using SWISH to Realize Interactive Web-based Tutorials for Logic-based Languages", "abstract": "Programming environments have evolved from purely text based to using graphical user interfaces, and now we see a move toward web-based interfaces, such as Jupyter. Web-based interfaces allow for the creation of interactive documents that consist of text and programs, as well as their output. The output can be rendered using web technology as, for example, text, tables, charts, or graphs. This approach is particularly suitable for capturing data analysis workflows and creating interactive educational material. This article describes SWISH, a web front-end for Prolog that consists of a web server implemented in SWI-Prolog and a client web application written in JavaScript. SWISH provides a web server where multiple users can manipulate and run the same material, and it can be adapted to support Prolog extensions. In this article we describe the architecture of SWISH, and describe two case studies of extensions of Prolog, namely Probabilistic Logic Programming and Logic Production System, which have used SWISH to provide tutorial sites."}}
{"id": "xcGVlAko0Gr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Preface to special issue on Inductive Logic Programming, ILP 2017 and 2018", "abstract": ""}}
{"id": "sEnFCHoy_N5", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Comparison of MCMC Sampling for Probabilistic Logic Programming", "abstract": "Markov Chain Monte Carlo (MCMC) methods are a class of algorithms used to perform approximate inference in probabilistic models. When direct sampling from a probability distribution is difficult, MCMC algorithms provide accurate results by constructing a Markov chain that gradually approximates the desired distribution. In this paper we describe and compare the performances of two MCMC sampling algorithms, Gibbs sampling and Metropolis Hastings sampling, with rejection sampling for probabilistic logic programs. In particular, we analyse the relation between execution time and number of samples and how fast each algorithm converges."}}
{"id": "lLrGAOMdK7", "cdate": 1546300800000, "mdate": null, "content": {"title": "Quantum Weighted Model Counting", "abstract": "In Weighted Model Counting (WMC) we assign weights to Boolean literals and we want to compute the sum of the weights of the models of a Boolean function where the weight of a model is the product of the weights of its literals. WMC was shown to be particularly effective for performing inference in graphical models, with a complexity of $O(n2^w)$ where $n$ is the number of variables and $w$ is the treewidth. In this paper, we propose a quantum algorithm for performing WMC, Quantum WMC (QWMC), that modifies the quantum model counting algorithm to take into account the weights. In turn, the model counting algorithm uses the algorithms of quantum search, phase estimation and Fourier transform. In the black box model of computation, where we can only query an oracle for evaluating the Boolean function given an assignment, QWMC solves the problem approximately with a complexity of $\\Theta(2^{\\frac{n}{2}})$ oracle calls while classically the best complexity is $\\Theta(2^n)$, thus achieving a quadratic speedup."}}
{"id": "MgbPifP-dIr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Editorial: Statistical Relational Artificial Intelligence", "abstract": "section{Introduction}Statistical Relational Artificial Intelligence (StarAI) aims at integrating logical (or relational) AI with probabilistic (or statistical) AI~\\citep{deraedt2016,riguzzi18}. Relational AI achieved impressive results in structured machine learning and data mining, especially in bio- and chemo-informatics. Statistical AI is based on probabilistic (graphical) models that enable efficient reasoning and learning, and that have been applied to a wide variety of fields such as diagnosis, network communication, computational biology, computer vision, and robotics. Ultimately, StarAI may provide good starting points for developing \\textit{Systems AI} ----the computational and mathematical modeling of complex AI systems--- and in turn an engineering discipline for Artificial Intelligence and Machine Learning.This Research Topic `Statistical Relational Artificial Intelligence' aims at presenting an overview of the latest approaches in StarAI. This topic was followed by a summer school\\footnote{\\url{http://acai2018.unife.it/}} held in 2018 in Ferrara, Italy, as part of the series of Advanced Courses on AI (ACAI) promoted by the European Association for Artificial Intelligence.\\section*{Papers Included in this Research Topic}Previous issues on similar topics in other journals and books mainly focused on modeling, learning purely from data and/or lifted inference. This issue went further and addressed more novel and pertinent topics such as grammatical inference, hu..."}}
{"id": "LXuHp3Us_B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Probabilistic DL Reasoning with Pinpointing Formulas: A Prolog-based Approach", "abstract": "When modeling real-world domains, we have to deal with information that is incomplete or that comes from sources with different trust levels. This motivates the need for managing uncertainty in the Semantic Web. To this purpose, we introduced a probabilistic semantics, named DISPONTE, in order to combine description logics (DLs) with probability theory. The probability of a query can be then computed from the set of its explanations by building a Binary Decision Diagram (BDD). The set of explanations can be found using the tableau algorithm, which has to handle non-determinism. Prolog, with its efficient handling of non-determinism, is suitable for implementing the tableau algorithm. TRILL and TRILLP are systems offering a Prolog implementation of the tableau algorithm. TRILLP builds a pinpointing formula that compactly represents the set of explanations and can be directly translated into a BDD. Both reasoners were shown to outperform state-of-the-art DL reasoners. In this paper, we present an improvement of TRILLP, named TORNADO, in which the BDD is directly built during the construction of the tableau, further speeding up the overall inference process. An experimental comparison shows the effectiveness of TORNADO. All systems can be tried online in the TRILL on SWISH web application at http://trill.ml.unife.it/."}}
{"id": "HdQE_Mq5w8-", "cdate": 1546300800000, "mdate": null, "content": {"title": "Analyzing Transaction Fees with Probabilistic Logic Programming", "abstract": "Fees are used in Bitcoin to prioritize transactions. Transactions with high associated fee are usually included in a block faster than those with lower fees. Users would like to pay just the minimum amount to make the transaction confirmed in the desired time. Fees are collected as a reward when transactions are included in a block so, on the other perspective, miners usually process first the most profitable transactions, i.e. the one with higher fee rate. Bitcoin is a dynamic system influenced by several variables, such as transaction arrival time and block discovery time making the prediction of the confirmation time a hard task. In this paper we use probabilistic logic programming to model how fees influence the confirmation time and how much fees affect miner\u2019s revenue."}}
