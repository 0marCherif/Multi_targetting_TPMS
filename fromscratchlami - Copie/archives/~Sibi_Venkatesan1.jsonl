{"id": "ERAQ5ZCP9t", "cdate": 1601308267242, "mdate": null, "content": {"title": "Robust Multi-view Representation Learning", "abstract": "Multi-view data has become ubiquitous, especially with multi-sensor systems like self-driving cars or medical patient-side monitors.\n\nWe look at modeling multi-view data through robust representation learning, with the goal of leveraging relationships between views and building resilience to missing information.\nWe propose a new flavor of multi-view AutoEncoders, the Robust Multi-view AutoEncoder, which explicitly encourages robustness to missing views.\nThe principle we use is straightforward: we apply the idea of drop-out to the level of views.\nDuring training, we leave out views as input to our model while forcing it to reconstruct all of them.\nWe also consider a flow-based generative modeling extension of our approach in the case where all the views are available.\n\nWe conduct experiments for different scenarios: directly using the learned representations for reconstruction, as well as a two-step process where the learned representation is subsequently used as features for the data for a down-stream application.\nOur synthetic and real-world experiments show promising results for the application of these models to robust representation learning."}}
{"id": "rJEkA7zuZS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Scaling Active Search using Linear Similarity Functions", "abstract": "Active Search has become an increasingly useful tool in information retrieval problems where the goal is to discover as many target elements as possible using only limited label queries. With the advent of big data, there is a growing emphasis on the scalability of such techniques to handle very large and very complex datasets. In this paper, we consider the problem of Active Search where we are given a similarity function between data points. We look at an algorithm introduced by Wang et al. [Wang et al., 2013] known as Active Search on Graphs and propose crucial modifications which allow it to scale significantly. Their approach selects points by minimizing an energy function over the graph induced by the similarity function on the data. Our modifications require the similarity function to be a dot-product between feature vectors of data points, equivalent to having a linear kernel for the adjacency matrix. With this, we are able to scale tremendously: for n data points, the original algorithm runs in O(n^2) time per iteration while ours runs in only O(nr + r^2) given r-dimensional features. We also describe a simple alternate approach using a weighted-neighbor predictor which also scales well. In our experiments, we show that our method is competitive with existing semi-supervised approaches. We also briefly discuss conditions under which our algorithm performs well."}}
