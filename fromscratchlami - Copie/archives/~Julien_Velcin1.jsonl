{"id": "zu0i3pWA8Q", "cdate": 1681722443299, "mdate": 1681722443299, "content": {"title": "Multivariate Powered Dirichlet-Hawkes Process", "abstract": "The publication time of a document carries a relevant in- formation about its semantic content. The Dirichlet-Hawkes process has been proposed to jointly model textual information and publication dy- namics. This approach has been used with success in several recent works, and extended to tackle specific challenging problems \u2013typically for short texts or entangled publication dynamics. However, the prior in its cur- rent form does not allow for complex publication dynamics. In particular, inferred topics are independent from each other \u2013a publication about fi- nance is assumed to have no influence on publications about politics, for instance.\nIn this work, we develop the Multivariate Powered Dirichlet-Hawkes Pro- cess (MPDHP), that alleviates this assumption. Publications about var- ious topics can now influence each other. We detail and overcome the technical challenges that arise from considering interacting topics. We conduct a systematic evaluation of MPDHP on a range of synthetic datasets to define its application domain and limitations. Finally, we develop a use case of the MPDHP on Reddit data. At the end of this article, the interested reader will know how and when to use MPDHP, and when not to."}}
{"id": "A_bRaHYi1P", "cdate": 1675318109339, "mdate": null, "content": {"title": "Behavioral differences: insights, explanations and comparisons of French and US Twitter usage during elections", "abstract": "Social networks and social media have played a key role for observing and influencing how the political landscape takes shape and dynamically shifts. It is especially true in events such as national elections as indicated by earlier studies with Facebook (Williams and Gulati, in: Proceedings of the annual meeting of the American Political Science Association, 2009) and Twitter (Larsson and Moe in New Med Soc 14(5):729\u2013747, 2012). Not surprisingly in an attempt to better understand and simplify these networks, community discovery methods have been used, such as the Louvain method (Blondel et al. in J Stat Mechanics Theory Exp 2008(10):P10008, 2008) to understand elections (Gaumont et al. in PLoS ONE 13(9):e0201879, 2018). However, most community-based studies first simplify the complex Twitter data into a single network based on (for example) follower, retweet or friendship properties. This requires ignoring some information or combining many types of information into a graph, which can mask many insights. In this paper, we explore Twitter data as a time-stamped vertex-labeled graph. The graph structure can be given by a structural relation between the users such as retweet, friendship or follower relation, whilst the behavior of the individual is given by their posting behavior which is modeled as a time-evolving vertex labels. We explore leveraging existing community discovery methods to find communities using just the structural data and then describe these communities using behavioral data. We explore two complimentary directions: (1)  creating a taxonomy of hashtags based on their community usage and (2) efficiently describing the communities expanding our recently published work. We have created two datasets, one each for the French and US elections from which we compare and contrast insights on the usage of hashtags."}}
{"id": "2anIiLnwSIp", "cdate": 1664952474760, "mdate": 1664952474760, "content": {"title": "Writing Style Author Embedding Evaluation", "abstract": "Learning authors representations from their textual productions is now widely used to solve multiple downstream tasks, such as classification, link prediction or user recommendation. Author embedding methods are often built on top of either Doc2Vec (Le and Mikolov, 2014) or the Transformer architecture (Devlin et al., 2019). Evaluating the quality of these embeddings and what they capture is a difficult task. Most articles use either classification accuracy or authorship attribution, which does not clearly measure the quality of the representation space, if it really captures what it has been built for. In this paper, we propose a novel evaluation framework of author embedding methods based on the writing style. It allows to quantify if the embedding space effectively captures a set of stylistic features, chosen to be the best proxy of an author writing style. This approach gives less importance to the topics conveyed by the documents. It turns out that recent models are mostly driven by the inner semantic of authors\u2019 production. They are outperformed by simple baselines, based on state-of-the-art pretrained sentence embedding models, on several linguistic axes. These baselines can grasp complex linguistic phenomena and writing style more efficiently, paving the way for designing new style-driven author embedding models."}}
{"id": "ovha5L1PIbv", "cdate": 1664952385906, "mdate": 1664952385906, "content": {"title": "Gaussian Embedding of Linked Documents from a Pretrained Semantic Space", "abstract": "Gaussian Embedding of Linked Documents (GELD) is a new method that embeds linked documents (e.g., citation networks) onto a pretrained semantic space (e.g., a set of word embeddings). We formulate the problem in such a way that we model each document as a Gaussian distribution in the word vector space. We design a generative model that combines both words and links in a consistent way. Leveraging the variance of a document allows us to model the uncertainty related to word and link generation. In most cases, our method outperforms state-of-the-art methods when using our document vectors as features for usual downstream tasks. In particular, GELD achieves better accuracy in classification and link prediction on Cora and Dblp. In addition, we demonstrate qualitatively the convenience of several properties of our method. We provide the implementation of GELD and the evaluation datasets to the community (https://github.com/AntoineGourru/DNEmbedding)."}}
{"id": "UEbXZRy-BDW", "cdate": 1664952326150, "mdate": 1664952326150, "content": {"title": "Document network projection in pretrained word embedding space", "abstract": "We present Regularized Linear Embedding (RLE), a novel method that projects a collection of linked documents (e.g., citation network) into a pretrained word embedding space. In addition to the textual content, we leverage a matrix of pairwise similarities providing complementary information (e.g., the network proximity of two documents in a citation graph). We first build a simple word vector average for each document, and we use the similarities to alter this average representation. The document representations can help to solve many information retrieval tasks, such as recommendation, classification and clustering. We demonstrate that our approach outperforms or matches existing document network embedding methods on node classification and link prediction tasks. Furthermore, we show that it helps identifying relevant keywords to describe document classes."}}
{"id": "TncPCYbesS", "cdate": 1648728023017, "mdate": 1648728023017, "content": {"title": "Powered Hawkes-Dirichlet Process: Challenging Textual Clustering using a Flexible Temporal Prior", "abstract": "The textual content of a document and its publica- tion date are intertwined. For example, the publication of a news article on a topic is influenced by previous publications on similar issues, according to underlying temporal dynamics. However, it can be challenging to retrieve meaningful information when textual information conveys little information or when temporal dynamics are hard to unveil. Furthermore, the textual content of a document is not always linked to its temporal dynamics. We develop a flexible method to create clusters of textual documents according to both their content and publication time, the Powered Dirichlet-Hawkes process (PDHP). We show PDHP yields significantly better results than state-of-the-art models when temporal information or textual content is weakly informative. The PDHP also alleviates the hypothesis that textual content and temporal dynamics are always perfectly correlated. PDHP allows retrieving textual clusters, temporal clusters, or a mixture of both with high accuracy when they are not. We demonstrate that PDHP generalizes previous work \u2013such as the Dirichlet-Hawkes process (DHP) and Uniform process (UP). Finally, we illustrate the changes induced by PDHP over DHP and UP in a real-world application using Reddit data."}}
{"id": "ecsoxXEbxL", "cdate": 1648727877908, "mdate": 1648727877908, "content": {"title": "Dynamic Gaussian Embedding of Authors", "abstract": "Authors publish documents in a dynamic manner. Their topic of interest and writing style might shift over time. Tasks such as author classification, author identification or link prediction are difficult to solve in such complex data settings. We propose a new represen- tation learning model, DGEA (for Dynamic Gaussian Embedding of Authors), that is more suited to solve these tasks by capturing this temporal evolution. We formulate a general embedding frame- work: author representation at time \ud835\udc61 is a Gaussian distribution that leverages pre-trained document vectors, and that depends on the publications observed until \ud835\udc61 . The representations should retain some form of multi-topic information and temporal smoothness. We propose two models that fit into this framework. The first one, K-DGEA, uses a first order Markov model optimized with an Expectation Maximization Algorithm with Kalman Equations. The second, R-DGEA, makes use of a Recurrent Neural Network to model the time dependence. We evaluate our method on several quantitative tasks: author identification, classification, and co-authorship prediction, on two datasets written in English. In addition, our model is language agnostic since it only requires pre-trained document embeddings. It outperforms existing baselines by up to 18% on an author classification task on a news articles dataset."}}
{"id": "DaFryuOJ5Iu", "cdate": 1635524038948, "mdate": 1635524038948, "content": {"title": "Monitoring geometrical properties of word embeddings for detecting the emergence of new topics", "abstract": "Slow emerging topic detection is a task be- tween event detection, where we aggregate be- haviors of different words on short period of time, and language evolution, where we moni- tor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of weak signals at the word level. We propose to monitor the behavior of words representation in an embedding space and use one of its geometrical properties to character- ize the emergence of topics. As evaluation is typically hard for this kind of task, we present a framework for quantitative evaluation. We show positive results that outperform state-of- the-art methods on two public datasets of press and scientific articles."}}
{"id": "y3eou5N-ZH", "cdate": 1634308573036, "mdate": 1634308573036, "content": {"title": "Gaussian Embedding of Linked Documents from a Pretrained Semantic Space", "abstract": "Gaussian Embedding of Linked Documents (GELD) is a new method that embeds linked doc- uments (e.g., citation networks) onto a pretrained semantic space (e.g., a set of word embeddings). We formulate the problem in such a way that we model each document as a Gaussian distribution in the word vector space. We design a generative model that combines both words and links in a con- sistent way. Leveraging the variance of a document allows us to model the uncertainty related to word and link generation. In most cases, our method out- performs state-of-the-art methods when using our document vectors as features for usual downstream tasks. In particular, GELD achieves better accuracy in classification and link prediction on Cora and Dblp. In addition, we demonstrate qualitatively the convenience of several properties of our method. We provide the implementation of GELD and the evaluation datasets to the community (https://github.com/AntoineGourru/DNEmbedding)."}}
{"id": "Hkb05ZWuZH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Global Vectors for Node Representations", "abstract": "Most network embedding algorithms consist in measuring co-occur-rences of nodes via random walks then learning the embeddings using Skip-Gram with Negative Sampling. While it has proven to be a relevant choice, there are alternatives, such as GloVe, which has not been investigated yet for network embedding. Even though SGNS better handles non co-occurrence than GloVe, it has a worse time-complexity. In this paper, we propose a matrix factorization approach for network embedding, inspired by GloVe, that better handles non co-occurrence with a competitive time-complexity. We also show how to extend this model to deal with networks where nodes are documents, by simultaneously learning word, node and document representations. Quantitative evaluations show that our model achieves state-of-the-art performance, while not being so sensitive to the choice of hyper-parameters. Qualitatively speaking, we show how our model helps exploring a network of documents by generating complementary network-oriented and content-oriented keywords."}}
