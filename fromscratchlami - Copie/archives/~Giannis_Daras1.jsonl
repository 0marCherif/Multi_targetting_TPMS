{"id": "zRRavpHyuOJ", "cdate": 1672531200000, "mdate": 1683779454420, "content": {"title": "DataComp: In search of the next generation of multimodal datasets", "abstract": "Multimodal datasets are a critical component in recent breakthroughs such as Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. In particular, our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DataComp and all accompanying code at www.datacomp.ai."}}
{"id": "ZboSXN44ACn", "cdate": 1672531200000, "mdate": 1683913463608, "content": {"title": "Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers", "abstract": "We develop a framework for non-asymptotic analysis of deterministic samplers used for diffusion generative modeling. Several recent works have analyzed stochastic samplers using tools like Girsanov's theorem and a chain rule variant of the interpolation argument. Unfortunately, these techniques give vacuous bounds when applied to deterministic samplers. We give a new operational interpretation for deterministic sampling by showing that one step along the probability flow ODE can be expressed as two steps: 1) a restoration step that runs gradient ascent on the conditional log-likelihood at some infinitesimally previous time, and 2) a degradation step that runs the forward process using noise pointing back towards the current iterate. This perspective allows us to extend denoising diffusion implicit models to general, non-linear forward processes. We then develop the first polynomial convergence bounds for these samplers under mild conditions on the data distribution."}}
{"id": "-ZVFt42-2n", "cdate": 1672531200000, "mdate": 1683913463566, "content": {"title": "Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent", "abstract": "Imperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a \\emph{consistency} property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm"}}
{"id": "jxeSZaVzpmg", "cdate": 1664310938820, "mdate": null, "content": {"title": "Discovering the Hidden Vocabulary of DALLE-2", "abstract": "We discover that DALLE-2 seems to have a hidden vocabulary that can be used to generate images with absurd prompts. For example, it seems that ``Apoploe vesrreaitais'' means birds and ``Contarra ccetnxniams luryca tanniounons'' (sometimes) means bugs or pests. We find that these prompts are often consistent in isolation but also sometimes in combinations. We present our black-box method to discover words that seem random but have some correspondence to visual concepts. This creates important security and interpretability challenges. "}}
{"id": "3JCa_cqKaLy", "cdate": 1664310938693, "mdate": null, "content": {"title": "Multiresolution Textual Inversion", "abstract": "We extend Textual Inversion to learn pseudo-words that represent a concept at different resolutions. This allows us to generate images that use the concept at different resolutions and also to manipulate different resolutions using language.\nOnce learned, the user can generate images that agree with the original concept at different levels of detail; ``A photo of $S^*(0)$'' produces the exact object while the prompt ``A photo of $S^*(0.8)$'' only matches the rough outlines and colors. \nOur framework allows us to generate images that use different resolutions of an image (e.g. details, textures, styles)  as separate pseudo-words that can be composed in various ways. "}}
{"id": "-hWhz9xfrB9", "cdate": 1663850454933, "mdate": null, "content": {"title": "Lovasz Theta Contrastive Learning", "abstract": "We establish a connection between the Lovasz theta function of a graph and the widely used InfoNCE loss. We show that under certain conditions, the minima of the InfoNCE loss are related to minimizing the Lovasz theta function on the empty similarity graph between the samples. Building on this connection, we generalize contrastive learning on weighted similarity graphs between samples. Our Lovasz theta contrastive loss uses a weighted graph that can be learned to take into account similarities between our data. We evaluate our method on image classification tasks, demonstrating an improvement of $1 \\%$ in the supervised case and up to $4 \\%$ in the unsupervised case."}}
{"id": "QsVditUhXR", "cdate": 1663850412378, "mdate": null, "content": {"title": "Soft Diffusion: Score Matching For General Corruptions", "abstract": "We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. \nOur new loss trains the model to predict a clean image, that after corruption, matches the diffused observation.\nWe show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. \nWe further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. \nWe show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking.\nWe achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. \nWe also show significant computational benefits compared to vanilla denoising diffusion."}}
{"id": "SyD-b2m2meG", "cdate": 1652737835237, "mdate": null, "content": {"title": "Multitasking Models are Robust to Structural Failure: A Neural Model for Bilingual Cognitive Reserve", "abstract": "We find a surprising connection between multitask learning and robustness to neuron failures. Our experiments show that bilingual language models retain higher performance under various neuron perturbations, such as random deletions, magnitude pruning and weight noise. Our study is motivated by research in cognitive science showing that symptoms of dementia and cognitive decline appear later in bilingual speakers compared to monolingual patients with similar brain damage, a phenomenon called bilingual cognitive reserve. Our language model experiments replicate this phenomenon on bilingual GPT-2 and other models.\nWe provide a theoretical justification of this robustness by mathematically analyzing linear representation learning and showing that multitasking creates more robust representations. We open-source our code and models in the following URL: https://github.com/giannisdaras/multilingual\\_robustness."}}
{"id": "wlEiilPOqwv", "cdate": 1640995200000, "mdate": 1683913463949, "content": {"title": "Multiresolution Textual Inversion", "abstract": "We extend Textual Inversion to learn pseudo-words that represent a concept at different resolutions. This allows us to generate images that use the concept with different levels of detail and also to manipulate different resolutions using language. Once learned, the user can generate images at different levels of agreement to the original concept; \"A photo of $S^*(0)$\" produces the exact object while the prompt \"A photo of $S^*(0.8)$\" only matches the rough outlines and colors. Our framework allows us to generate images that use different resolutions of an image (e.g. details, textures, styles) as separate pseudo-words that can be composed in various ways. We open-soure our code in the following URL: https://github.com/giannisdaras/multires_textual_inversion"}}
{"id": "jN7iKx1y8Aj", "cdate": 1640995200000, "mdate": 1668706873994, "content": {"title": "Soft Diffusion: Score Matching for General Corruptions", "abstract": "We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, \\textit{that after corruption}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion."}}
