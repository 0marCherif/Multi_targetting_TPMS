{"id": "wHnD2NSMEkf", "cdate": 1640995200000, "mdate": 1681767651895, "content": {"title": "Director: Generator-Classifiers For Supervised Language Modeling", "abstract": "Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, Jason Weston. Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2022."}}
{"id": "psdI7XpahID", "cdate": 1640995200000, "mdate": 1670981931232, "content": {"title": "OPT: Open Pre-trained Transformer Language Models", "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models."}}
{"id": "kiYAL4LRuui", "cdate": 1640995200000, "mdate": 1681767651799, "content": {"title": "The CRINGE Loss: Learning what language not to model", "abstract": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data -- examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the CRINGE loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement."}}
{"id": "j2GkSk-q_A", "cdate": 1640995200000, "mdate": 1678822293906, "content": {"title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization", "abstract": ""}}
{"id": "j-4tasC9S-", "cdate": 1640995200000, "mdate": 1681767651881, "content": {"title": "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion", "abstract": "Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available."}}
{"id": "iw9sYqSv9nL", "cdate": 1640995200000, "mdate": 1681767651793, "content": {"title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage", "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction."}}
{"id": "ZXLgQAUUxv", "cdate": 1640995200000, "mdate": 1681767651879, "content": {"title": "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion", "abstract": ""}}
{"id": "UDzERmwrTa", "cdate": 1640995200000, "mdate": 1681767651868, "content": {"title": "When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels", "abstract": "Deployed dialogue agents have the potential to integrate human feedback to continuously improve themselves. However, humans may not always provide explicit signals when the chatbot makes mistakes during interactions. In this work, we propose Juicer, a framework to make use of both binary and free-form textual human feedback. It works by: (i) extending sparse binary feedback by training a satisfaction classifier to label the unlabeled data; and (ii) training a reply corrector to map the bad replies to good ones. We find that augmenting training with model-corrected replies improves the final dialogue model, and we can further improve performance by using both positive and negative replies through the recently proposed Director model."}}
{"id": "KZpR2Z-bkgy", "cdate": 1640995200000, "mdate": 1681767652071, "content": {"title": "Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an Identity", "abstract": ""}}
{"id": "ILvR5a1XWH", "cdate": 1640995200000, "mdate": 1681767651629, "content": {"title": "Internet-Augmented Dialogue Generation", "abstract": ""}}
