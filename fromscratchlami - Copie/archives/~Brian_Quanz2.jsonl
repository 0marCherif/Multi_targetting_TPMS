{"id": "ezO_eJnPFO", "cdate": 1640995200000, "mdate": 1673031549799, "content": {"title": "Towards Creativity Characterization of Generative Models via Group-Based Subset Scanning", "abstract": ""}}
{"id": "Zqo5GB-fKo", "cdate": 1640995200000, "mdate": 1682339192247, "content": {"title": "Distributed Incremental Machine Learning for Big Time Series Data", "abstract": ""}}
{"id": "VvDkn4Qgek", "cdate": 1640995200000, "mdate": 1673031549744, "content": {"title": "Towards Creativity Characterization of Generative Models via Group-based Subset Scanning", "abstract": ""}}
{"id": "LILheKNffMT", "cdate": 1640995200000, "mdate": 1673031549555, "content": {"title": "Hierarchy-guided Model Selection for Time Series Forecasting", "abstract": ""}}
{"id": "vKbY_WHriDA", "cdate": 1634067439912, "mdate": null, "content": {"title": "Math Programming based Reinforcement Learning for Multi-Echelon Inventory Management", "abstract": "Reinforcement Learning has lead to considerable break-throughs in diverse areas\nsuch as robotics, games and many others. But the application of RL to complex real world decision making problems remains limited. Many problems in Operations Management (inventory and revenue management, for example) are characterized\nby large action spaces and stochastic system dynamics. These characteristics\nmake the problem considerably harder to solve for existing RL methods that\nrely on enumeration techniques to solve per step action problems. To resolve\nthese issues, we develop Programmable Actor Reinforcement Learning (PARL), a\npolicy iteration method that uses techniques from integer programming and sample\naverage approximation. Analytically, we show that the for a given critic, the learned\npolicy in each iteration converges to the optimal policy as the underlying samples\nof the uncertainty go to infinity. Practically, we show that a properly selected\ndiscretization of the underlying uncertain distribution can yield near optimal actor\npolicy even with very few samples from the underlying uncertainty. We then apply\nour algorithm to real-world inventory management problems with complex supply\nchain structures and show that PARL outperforms state-of-the-art RL and inventory\noptimization methods in these settings. We find that PARL outperforms commonly\nused base stock heuristic by 51.3% and RL based methods by up to 9.58% on\naverage across different supply chain environments."}}
{"id": "lRYfPNKCRu", "cdate": 1621630012155, "mdate": null, "content": {"title": "Predicting Deep Neural Network Generalization with Perturbation Response Curves", "abstract": "The field of Deep Learning is rich with empirical evidence of human-like performance on a variety of prediction tasks. However, despite these successes, the recent Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020 competition suggests that there is a need for more robust and efficient measures of network generalization. In this work, we propose a new framework for evaluating the generalization capabilities of trained networks. We use perturbation response (PR) curves that capture the accuracy change of a given network as a function of varying levels of training sample perturbation. From these PR curves, we derive novel statistics that capture generalization capability. Specifically, we introduce two new measures for accurately predicting generalization gaps: the Gi-score and Pal-score, which are inspired by the Gini coefficient and Palma ratio (measures of income inequality), that accurately predict generalization gaps. Using our framework applied to intra and inter-class sample mixup, we attain better predictive scores than the current state-of-the-art measures on a majority of tasks in the PGDL competition. In addition, we show that our framework and the proposed statistics can be used to capture to what extent a trained network is invariant to a given parametric input transformation, such as rotation or translation. Therefore, these generalization gap prediction statistics also provide a useful means for selecting optimal network architectures and hyperparameters that are invariant to a certain perturbation."}}
{"id": "iiuZkJaerD", "cdate": 1609459200000, "mdate": 1673031549339, "content": {"title": "Learning to shortcut and shortlist order fulfillment deciding", "abstract": ""}}
{"id": "dAnlJqsaAF", "cdate": 1609459200000, "mdate": 1673031549885, "content": {"title": "Math Programming based Reinforcement Learning for Multi-Echelon Inventory Management", "abstract": ""}}
{"id": "YS1dXQwlt0xp", "cdate": 1609459200000, "mdate": null, "content": {"title": "Gi and Pal Scores: Deep Neural Network Generalization Statistics", "abstract": "The field of Deep Learning is rich with empirical evidence of human-like performance on a variety of regression, classification, and control tasks. However, despite these successes, the field lacks strong theoretical error bounds and consistent measures of network generalization and learned invariances. In this work, we introduce two new measures, the Gi-score and Pal-score, that capture a deep neural network's generalization capabilities. Inspired by the Gini coefficient and Palma ratio, measures of income inequality, our statistics are robust measures of a network's invariance to perturbations that accurately predict generalization gaps, i.e., the difference between accuracy on training and test sets."}}
{"id": "Tij5tapdFB", "cdate": 1609459200000, "mdate": 1673031549884, "content": {"title": "Predicting Deep Neural Network Generalization with Perturbation Response Curves", "abstract": ""}}
