{"id": "vozU5JHMiob", "cdate": 1640995200000, "mdate": 1682429233489, "content": {"title": "Formalizing Preferences Over Runtime Distributions", "abstract": ""}}
{"id": "rr9HQbWdEEt", "cdate": 1577836800000, "mdate": null, "content": {"title": "Smarter Parking: Using AI to Identify Parking Inefficiencies in Vancouver", "abstract": "On-street parking is convenient, but has many disadvantages: on-street spots come at the expense of other road uses such as traffic lanes, transit lanes, bike lanes, or parklets; drivers looking for parking contribute substantially to traffic congestion and hence to greenhouse gas emissions; safety is reduced both due to the fact that drivers looking for spots are more distracted than other road users and that people exiting parked cars pose a risk to cyclists. These social costs may not be worth paying when off-street parking lots are nearby and have surplus capacity. To see where this might be true in downtown Vancouver, we used artificial intelligence techniques to estimate the amount of time it would take drivers to both park on and off street for destinations throughout the city. For on-street parking, we developed (1) a deep-learning model of block-by-block parking availability based on data from parking meters and audits and (2) a computational simulation of drivers searching for an on-street spot. For off-street parking, we developed a computational simulation of the time it would take drivers drive from their original destination to the nearest city-owned off-street lot and then to queue for a spot based on traffic and lot occupancy data. Finally, in both cases we also computed the time it would take the driver to walk from their parking spot to their original destination. We compared these time estimates for destinations in each block of Vancouver's downtown core and each hour of the day. We found many areas where off street would actually save drivers time over searching the streets for a spot, and many more where the time cost for parking off street was small. The identification of such areas provides an opportunity for the city to repurpose valuable curbside space for community-friendly uses more in line with its transportation goals."}}
{"id": "JuS7JV2ec6T", "cdate": 1577836800000, "mdate": 1682429233487, "content": {"title": "ImpatientCapsAndRuns: Approximately Optimal Algorithm Configuration from an Infinite Pool", "abstract": "Algorithm configuration procedures optimize parameters of a given algorithm to perform well over a distribution of inputs. Recent theoretical work focused on the case of selecting between a small number of alternatives. In practice, parameter spaces are often very large or infinite, and so successful heuristic procedures discard parameters ``impatiently'', based on very few observations. Inspired by this idea, we introduce ImpatientCapsAndRuns, which quickly discards less promising configurations, significantly speeding up the search procedure compared to previous algorithms with theoretical guarantees, while still achieving optimal runtime up to logarithmic factors under mild assumptions. Experimental results demonstrate a practical improvement."}}
{"id": "Hkx6p6EFDr", "cdate": 1569439172758, "mdate": null, "content": {"title": "Equivariant Entity-Relationship Networks", "abstract": "Due to its extensive use in databases, the relational model is ubiquitous in representing big-data. However, recent progress in deep learning with relational data has been focused on (knowledge) graphs. In this paper we propose Equivariant Entity-Relationship Networks, the class of parameter-sharing neural networks derived from the entity-relationship model. We prove that our proposed feed-forward layer is the most expressive linear layer under the given equivariance constraints, and subsumes recently introduced equivariant models for sets, exchangeable tensors, and graphs. The proposed feed-forward layer has linear complexity in the the data and can be used for both inductive and transductive reasoning about relational databases, including database embedding, and the prediction of missing records. This, provides a principled theoretical foundation for the application of deep learning to one of the most abundant forms of data."}}
{"id": "SJlzR4rg8H", "cdate": 1567802569804, "mdate": null, "content": {"title": "Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration", "abstract": "Algorithm configuration methods optimize the performance of a parameterized heuristic algorithm on a given distribution of problem instances. Recent work introduced an algorithm configuration procedure (``Structured Procrastination'') that provably achieves near optimal performance with high probability and with nearly minimal runtime in the worst case. It also offers an anytime property: it keeps tightening its optimality guarantees the longer it is run. Unfortunately, Structured Procrastination is not adaptive to characteristics of the parameterized algorithm: it treats every input like the worst case. Follow-up work (``Leaps and Bounds'') achieves adaptivity but trades away the anytime property. This paper introduces a new algorithm configuration method, ``Structured Procrastination with Confidence'', that preserves the near-optimality and anytime properties of Structured Procrastination while adding adaptivity. In particular, the new algorithm will perform dramatically faster in settings where many algorithm configurations perform poorly. We show empirically that such settings arise frequently in practice, and that the anytime property can help find good configurations in a short period of time."}}
{"id": "B61ZsfqSlB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Deep Models for Relational Databases", "abstract": "The relational model is a ubiquitous representation of big-data, in part due to its extensive use in databases. In this paper, we propose the Equivariant Entity-Relationship Network (EERN), which is a Multilayer Perceptron equivariant to the symmetry transformations of the Entity-Relationship model. To this end, we identify the most expressive family of linear maps that are exactly equivariant to entity relationship symmetries, and further show that they subsume recently introduced equivariant maps for sets, exchangeable tensors, and graphs. The proposed feed-forward layer has linear complexity in the data and can be used for both inductive and transductive reasoning about relational databases, including database embedding, and the prediction of missing records. This provides a principled theoretical foundation for the application of deep learning to one of the most abundant forms of data. Empirically, EERN outperforms different variants of coupled matrix tensor factorization in both synthetic and real-data experiments."}}
{"id": "0bzRlJwU6lO", "cdate": 1546300800000, "mdate": 1682429233490, "content": {"title": "Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration", "abstract": ""}}
{"id": "Sk-y4jWObS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep Models of Interactions Across Sets", "abstract": "We use deep learning to model interactions across two or more sets of objects, such as user{\u2013}movie ratings or protein{\u2013}drug bindings. The canonical representation of such interactions is a matrix..."}}
{"id": "PufYrtG4IEb", "cdate": 1514764800000, "mdate": 1682429233489, "content": {"title": "Deep Models of Interactions Across Sets", "abstract": ""}}
