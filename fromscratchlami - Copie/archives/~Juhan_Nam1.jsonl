{"id": "4Z-7itppp0", "cdate": 1702039255386, "mdate": 1702039255386, "content": {"title": "Teaching Chorale Generation Model to Avoid Parallel Motions", "abstract": "This paper presents a music generation model trained with Bach's chorales and classical music theory rules. Although previous work has shown promising results in generating the four-part harmony, one of the limitations is the frequent appearance of parallel 5th or 8th, which are prohibited in music theory and rarely used in Bach's chorale. To address this issue, we propose an additional loss that minimizes the probability of prohibited patterns, comparing the results with those from inference using a post-hoc probability manipulation to prevent parallel 5th and 8th. The experimental result shows that applying the proposed loss term can help to reduce parallel motion without losing other quality."}}
{"id": "nmtmjfJQLS", "cdate": 1669057832710, "mdate": null, "content": {"title": "Music Playlist Title Generation Using Artist Information", "abstract": "Automatically generating or captioning music playlist titles given a set of tracks is of significant interest in music streaming services as customized playlists are widely used in personalized music recommendation, and well-composed text titles attract users and help their music discovery. We present an encoder-decoder model that generates a playlist title from a sequence of music tracks. While previous work takes track IDs as tokenized input for playlist title generation, we use artist IDs corresponding to the tracks to mitigate the issue from the long-tail distribution of tracks included in the playlist dataset. Also, we introduce a chronological data split method to deal with newly-released tracks in real-world scenarios. Comparing the track IDs and artist IDs as input sequences, we show that the artist-based approach significantly enhances the performance in terms of word overlap, semantic relevance, and diversity. "}}
{"id": "xLLAtHpt2_z", "cdate": 1640995200000, "mdate": 1648709471939, "content": {"title": "Pseudo-Label Transfer From Frame-Level To Note-Level In A Teacher-Student Framework for Singing Transcription From Polyphonic Music", "abstract": "Lack of large-scale note-level labeled data is the major obstacle to singing transcription from polyphonic music. We address the issue by using pseudo labels from vocal pitch estimation models given unlabeled data. The proposed method first converts the frame-level pseudo labels to note-level through pitch and rhythm quantization steps. Then, it further improves the label quality through self-training in a teacher-student framework. To validate the method, we conduct various experiment settings by investigating two vocal pitch estimation models as pseudo-label generators, two setups of teacher-student frameworks, and the number of iterations in self-training. The results show that the proposed method can effectively leverage large-scale unlabeled audio data and self-training with the noisy student model helps to improve performance. Finally, we show that the model trained with only unlabeled data has comparable performance to previous works and the model trained with additional labeled data achieves higher accuracy than the model trained with only labeled data."}}
{"id": "y_rXroL8ES6", "cdate": 1609459200000, "mdate": 1648709471918, "content": {"title": "A Melody-Unsupervision Model for Singing Voice Synthesis", "abstract": "Recent studies in singing voice synthesis have achieved high-quality results leveraging advances in text-to-speech models based on deep neural networks. One of the main issues in training singing voice synthesis models is that they require melody and lyric labels to be temporally aligned with audio data. The temporal alignment is a time-exhausting manual work in preparing for the training data. To address the issue, we propose a melody-unsupervision model that requires only audio-and-lyrics pairs without temporal alignment in training time but generates singing voice audio given a melody and lyrics input in inference time. The proposed model is composed of a phoneme classifier and a singing voice generator jointly trained in an end-to-end manner. The model can be fine-tuned by adjusting the amount of supervision with temporally aligned melody labels. Through experiments in melody-unsupervision and semi-supervision settings, we compare the audio quality of synthesized singing voice. We also show that the proposed model is capable of being trained with speech audio and text labels but can generate singing voice in inference time."}}
{"id": "lQGhISNOBKw", "cdate": 1609459200000, "mdate": 1648709471921, "content": {"title": "Music Playlist Title Generation: A Machine-Translation Approach", "abstract": "We propose a machine-translation approach to automatically generate a playlist title from a set of music tracks. We take a sequence of track IDs as input and a sequence of words in a playlist title as output, adapting the sequence-to-sequence framework based on Recurrent Neural Network (RNN) and Transformer to the music data. Considering the orderless nature of music tracks in a playlist, we propose two techniques that remove the order of the input sequence. One is data augmentation by shuffling and the other is deleting the positional encoding. We also reorganize the existing music playlist datasets to generate phrase-level playlist titles. The result shows that the Transformer models generally outperform the RNN model. Also, removing the order of input sequence improves the performance further."}}
{"id": "iEZS4QKq15g", "cdate": 1609459200000, "mdate": 1648709471940, "content": {"title": "Learning a cross-domain embedding space of vocal and mixed audio with a structure-preserving triplet loss", "abstract": ""}}
{"id": "eAtfeHcMmvH", "cdate": 1609459200000, "mdate": 1648709471932, "content": {"title": "EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation", "abstract": "While there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced `yee-m\\`{o}-pi-uh') dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the methodology for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks."}}
{"id": "OlOI4GdZP6X", "cdate": 1609459200000, "mdate": 1648709471933, "content": {"title": "EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation", "abstract": ""}}
{"id": "449stiOjmju", "cdate": 1609459200000, "mdate": 1648709471925, "content": {"title": "PocketVAE: A Two-step Model for Groove Generation and Control", "abstract": "Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model."}}
{"id": "3g5VC1Su2hz", "cdate": 1609459200000, "mdate": 1648709471896, "content": {"title": "Investigating Time-Frequency Representations for Audio Feature Extraction in Singing Technique Classification", "abstract": "Singing techniques are used for expressive vocal performances by employing temporal fluctuations of timbre, pitch, and other components of the voice. In this study, we compare the performances of hand-crafted features and automat-ically extracted features using deep learning methods to identify different singing techniques. Hand-crafted acoustic features are based on expert knowledge of singing voice whereas the deep learning methods take low-level feature representations, such as spectrograms and raw waveforms, as inputs and learn features automatically using convolutional neural networks (CNNs). These extracted features are used as an input to the random forest classifier for comparison with the hand-crafted features for 10-class singing technique classification. We show that the CNN-based features outperform the hand-crafted features in terms of classification accuracy. Furthermore, we explore various time-frequency representations as an input to the CNNs. We show that the best performing input is multi-resolution short-time Fourier Transform (STFTs), when the CNN kernels are oblong and they slide on the frequency- and time-axis directions separately."}}
