{"id": "K9kmhW27Fr", "cdate": 1619638361272, "mdate": null, "content": {"title": "Training Binarized Neural Networks using Ternary Multipliers", "abstract": "Deep learning offers the promise of intelligent devices that are able to perceive, reason and take intuitive actions. The rising adoption of deep learning techniques has motivated researchers and developers to seek low-cost and high-speed software/hardware solutions for deployment on smart devices. In recent years work has focused on reducing the complexity of inference in deep neural networks, however, a method enabling low-complexity on-chip learning is still missing. In this work, we introduce a gradient estimation method that performs back-propagation with ternary (2-bit) quantized gradients. Our method replaces all full-precision multipliers (i.e., 16-bit fixed-point multipliers) in neural network training with ternary operators while maintaining a comparable accuracy. Furthermore, we propose a new stochastic computing-based neural network (SC-based NN) which employs a new stochastic representation (i.e., dynamic sign-magnitude stochastic sequence). Our proposed SC-based NN produces state-of-the-art results while using shorter sequence lengths (i.e., sequence length of 16) compared to its SC-based counterparts.\n "}}
{"id": "8RxSO-uZG82", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fast and Efficient Convolutional Accelerator for Edge Computing.", "abstract": "Convolutional neural networks (CNNs) are a vital approach in machine learning. However, their high complexity and energy consumption make them challenging to embed in mobile applications at the edge requiring real-time processes such as smart phones. In order to meet the real-time constraint of edge devices, recently proposed custom hardware CNN accelerators have exploited parallel processing elements (PEs) to increase throughput. However, this straightforward parallelization of PEs and high memory bandwidth require high data movement, leading to large energy consumption. As a result, only a certain number of PEs can be instantiated when designing bandwidth-limited custom accelerators targeting edge devices. While most bandwidth-limited designs claim a peak performance of a few hundred giga operations per second, their average runtime performance is substantially lower than their roofline when applied to state-of-the-art CNNs such as AlexNet, VGGNet and ResNet, as a result of low resource utilization and arithmetic intensity. In this work, we propose a zero-activation-skipping convolutional accelerator (ZASCA) that avoids noncontributory multiplications with zero-valued activations. ZASCA employs a dataflow that minimizes the gap between its average and peak performances while maximizing its arithmetic intensity for both sparse and dense representations of activations, targeting the bandwidth-limited edge computing scenario. More precisely, ZASCA achieves a performance efficiency of up to 94 percent over a set of state-of-the-art CNNs for image classification with dense representation where the performance efficiency is the ratio between the average runtime performance and the peak performance. Using its zero-skipping feature, ZASCA can further improve the performance efficiency of the state-of-the-art CNNs by up to 1.9\u00d7 depending on the sparsity degree of activations. The implementation results in 65-nm TSMC CMOS technology show that, compared to the most energy-efficient accelerator, ZASCA can process convolutions from 5.5\u00d7 to 17.5\u00d7 faster, and is between 2.1\u00d7 and 4.5\u00d7 more energy efficient while occupying 2.1\u00d7 less silicon area."}}
{"id": "SygqpVSxLB", "cdate": 1567802561591, "mdate": null, "content": {"title": "The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic", "abstract": "The emergence of XNOR networks seek to reduce the model size and computational cost of neural networks for their deployment on specialized hardware requiring real-time processes with limited hardware resources. In XNOR networks, both weights and activations are binary, bringing great benefits to specialized hardware by replacing expensive multiplications with simple XNOR operations. Although XNOR convolutional and fully-connected neural networks have been successfully developed during the past few years, there is no XNOR network implementing commonly-used variants of recurrent neural networks such as long short-term memories (LSTMs). The main computational core of LSTMs involves vector-matrix multiplications followed by a set of non-linear functions and element-wise multiplications to obtain the gate activations and state vectors, respectively. Several previous attempts on quantization of LSTMs only focused on quantization of the vector-matrix multiplications in LSTMs while retaining the element-wise multiplications in full precision. In this paper, we propose a method that converts all the multiplications in LSTMs to XNOR operations using stochastic computing. To this end, we introduce a weighted finite-state machine and its synthesis method to approximate the non-linear functions used in LSTMs on stochastic bit streams. Experimental results show that the proposed XNOR LSTMs reduce the computational complexity of their quantized counterparts by a factor of 86x without any sacrifice on latency while achieving a better accuracy across various temporal tasks."}}
{"id": "vVG9YX6oauR", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning to Skip Ineffectual Recurrent Computations in LSTMs.", "abstract": "Long Short-Term Memory (LSTM) is a special class of recurrent neural network, which has shown remarkable successes in processing sequential data. The typical architecture of an LSTM involves a set of states and gates: the states retain information over arbitrary time intervals and the gates regulate the flow of information. Due to the recursive nature of LSTMs, they are computationally intensive to deploy on edge devices with limited hardware resources. To reduce the computational complexity of LSTMs, we first introduce a method that learns to retain only the important information in the states by pruning redundant information. We then show that our method can prune over 90% of information in the states without incurring any accuracy degradation over a set of temporal tasks. This observation suggests that a large fraction of the recurrent computations are ineffectual and can be avoided to speed up the process during the inference as they involve noncontributory multiplications/accumulations with zero-valued states. Finally, we introduce a custom hardware accelerator that can perform the recurrent computations using both sparse and dense states. Experimental measurements show that performing the computations using the sparse states speeds up the process and improves energy efficiency by up to 5.2\u00d7 when compared to implementation results of the accelerator performing the computations using dense states."}}
{"id": "TR1cORs_Lxf", "cdate": 1546300800000, "mdate": null, "content": {"title": "Design and Implementation of a Polar Codes Blind Detection Scheme.", "abstract": "In blind detection, a set of candidates has to be decoded within a strict time constraint, to identify which transmissions are directed at the user equipment. Blind detection is required by the 3GPP LTE/LTE-A and fifth generation (5G) standards. With the selection of polar codes in 5G, the issue of blind detection of polar codes needs to be addressed. A polar code blind detection scheme has been recently proposed where the user ID is transmitted instead of some of the frozen bits. We propose an architecture to implement an improved version of such scheme. A first, coarse decoding phase helps selecting a subset of candidates that is decoded by a more powerful algorithm: an early stopping criterion is also introduced for the second decoding phase. The architecture relies on a tunable decoder that can be used for both phases. The architecture is synthesized and implementation results are reported for various system parameters. The reported area occupation and latency, obtained in 65-nm CMOS technology, are able to meet 5G requirements."}}
{"id": "HkNGYjR9FX", "cdate": 1538087802358, "mdate": null, "content": {"title": "Learning Recurrent Binary/Ternary Weights", "abstract": "Recurrent neural networks (RNNs) have shown excellent performance in processing sequence data. However, they are both complex and memory intensive due to their recursive nature. These limitations make RNNs difficult to embed on mobile devices requiring real-time processes with limited hardware resources. To address the above issues, we introduce a method that can learn binary and ternary weights during the training phase to facilitate hardware implementations of RNNs. As a result, using this approach replaces all multiply-accumulate operations by simple accumulations, bringing significant benefits to custom hardware in terms of silicon area and power consumption. On the software side, we evaluate the performance (in terms of accuracy) of our method using long short-term memories (LSTMs) and gated recurrent units (GRUs) on various sequential models including sequence classification and language modeling. We demonstrate that our method achieves competitive results on the aforementioned tasks while using binary/ternary weights during the runtime. On the hardware side, we present custom hardware for accelerating the recurrent computations of LSTMs with binary/ternary weights. Ultimately, we show that LSTMs with binary/ternary weights can achieve up to 12x memory saving and 10x inference speedup compared to the full-precision hardware implementation design."}}
{"id": "z9c1WTKDAb", "cdate": 1514764800000, "mdate": null, "content": {"title": "An Architecture to Accelerate Convolution in Deep Neural Networks.", "abstract": "In the past few years, the demand for real-time hardware implementations of deep neural networks (DNNs), especially convolutional neural networks (CNNs), has dramatically increased, thanks to their excellent performance on a wide range of recognition and classification tasks. When considering real-time action recognition and video/image classification systems, latency is of paramount importance. Therefore, applications strive to maximize the accuracy while keeping the latency under a given application-specific maximum: in most cases, this threshold cannot exceed a few hundred milliseconds. Until now, the research on DNNs has mainly focused on achieving a better classification or recognition accuracy, whereas very few works in literature take in account the computational complexity of the model. In this paper, we propose an efficient computational method, which is inspired by a computational core of fully connected neural networks, to process convolutional layers of state-of-the-art deep CNNs within strict latency requirements. To this end, we implemented our method customized for VGG and VGG-based networks which have shown state-of-the-art performance on different classification/recognition data sets. The implementation results in 65-nm CMOS technology show that the proposed accelerator can process convolutional layers of VGGNet up to 9.5 times faster than state-of-the-art accelerators reported to-date while occupying 3.5 mm <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> ."}}
{"id": "nc6_dSRkhN-", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Multi-Mode Accelerator for Pruned Deep Neural Networks.", "abstract": "Convolutional Neural Networks (CNNs) are constituted of complex, slow convolutional layers and memory-demanding fully-connected layers. Current pruning techniques can reduce memory accesses and power consumption, but cannot speed up the convolutional layers. In this paper, we introduce a pruning technique able to reduce the number of kernels in convolutional layers of up to 90% with negligible accuracy degradation. We propose an architecture to accelerate fully-connected and convolutional computations within a single computational core, with power/energy consumption below mobile devices budget. The proposed pruning technique speeds up convolutional computations by up to 6.9\u00d7, reducing memory accesses by the same factor."}}
{"id": "aMOVwrqAmL_", "cdate": 1514764800000, "mdate": null, "content": {"title": "Design and Implementation of a Polar Codes Blind Detection Scheme.", "abstract": "In blind detection, a set of candidates has to be decoded within a strict time constraint, to identify which transmissions are directed at the user equipment. Blind detection is required by the 3GPP LTE/LTE-Advanced standard, and it will be required in the 5th generation wireless communication standard (5G) as well. Polar codes have been selected for use in 5G: thus, the issue of blind detection of polar codes must be addressed. We propose a polar code blind detection scheme where the user ID is transmitted instead of some of the frozen bits. A first, coarse decoding phase helps selecting a subset of candidates that is decoded by a more powerful algorithm: an early stopping criterion is also introduced for the second decoding phase. Simulations results show good missed detection and false alarm rates, along with substantial latency gains thanks to early stopping. We then propose an architecture to implement the devised blind detection scheme, based on a tunable decoder that can be used for both phases. The architecture is synthesized and implementation results are reported for various system parameters. The reported area occupation and latency, obtained in 65 nm CMOS technology, are able to meet 5G requirements, and are guaranteed to meet them with even less resource usage in the latest technology nodes."}}
{"id": "aFMQybyJOVF", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Convolutional Accelerator for Neural Networks With Binary Weights.", "abstract": "Parallel processors and GP-GPUs have been routinely used in the past to perform the computations of convolutional neural networks (CNNs). However, their large power consumption has pushed researchers towards application-specific integrated circuits and on-chip accelerators implement neural networks. Nevertheless, within the Internet of Things (IoT) scenario, even these accelerators fail to meet the power and latency constraints. To address this issue, binary-weight networks were introduced, where weights are constrained to -1 and 1. Therefore, these networks facilitate hardware implementation of neural networks by replacing multiply-and-accumulate units with simple accumulators, as well as reducing the weight storage. In this paper, we introduce a convolutional accelerator for binary-weight neural networks. The proposed architecture only consumes 128 mW at a frequency of 200 MHz and occupies 1.2 mm <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> when synthesized in TSMC 65 nm CMOS technology. Moreover, it achieves a high area-efficiency of 176 Gops/MGC and performance efficiency of 89%, outperforming the state-of-the-art architecture for binary-weight networks by 1.8\u00d7 and 3.2\u00d7, respectively."}}
