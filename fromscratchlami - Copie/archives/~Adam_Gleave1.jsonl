{"id": "2qDp-zV14i", "cdate": 1668734799058, "mdate": null, "content": {"title": "On The Fragility of Learned Reward Functions", "abstract": "Reward functions are notoriously difficult to specify, especially for tasks with complex goals. Reward learning approaches attempt to infer reward functions from human feedback and preferences. Prior works on reward learning have mainly focused on the performance of policies trained alongside the reward function. This practice, however, may fail to detect learned rewards that are not capable of training new policies from scratch and thus do not capture the intended behavior. Our work focuses on demonstrating and studying the causes of these relearning failures in the domain of preference-based reward learning. We demonstrate with experiments in tabular and continuous control environments that the severity of relearning failures can be sensitive to changes in reward model design and the trajectory dataset composition. Based on our findings, we emphasize the need for more retraining-based evaluations in the literature."}}
{"id": "Hn21kZHiCK", "cdate": 1668734795781, "mdate": null, "content": {"title": "A general framework for reward function distances", "abstract": "In reward learning, it is helpful to be able to measure distances between reward functions, for example to evaluate learned reward models. Using simple metrics such as L^2 distances is not ideal because reward functions that are equivalent in terms of their optimal policies can nevertheless have high L^2 distance. EPIC and DARD are distances specifically designed for reward functions that address this by being invariant under certain transformations that leave optimal policies unchanged. However, EPIC and DARD are designed in an ad-hoc manner, only consider a subset of relevant reward transformations, and suffer from serious pathologies in some settings. In this paper, we define a general class of reward function distance metrics, of which EPIC is a special case. This framework lets as address all these issues with EPIC and DARD, and allows for the development of reward function distance metrics in a more principled manner."}}
{"id": "o2NozkVfkIU", "cdate": 1668734788269, "mdate": null, "content": {"title": "Adversarial Policies Beat Professional-Level Go AIs", "abstract": "We attack the state-of-the-art Go-playing AI system, KataGo, by training an adversarial policy that plays against a frozen KataGo victim. Our attack achieves a >99\\% win-rate against KataGo without search, and a >50% win-rate when KataGo uses enough search to be near-superhuman. To the best of our knowledge, this is the first successful end-to-end attack against a Go AI playing at the level of a top human professional. Notably, the adversary does not win by learning to play Go better than KataGo---in fact, the adversary is easily beaten by human amateurs. Instead, the adversary wins by tricking KataGo into ending the game prematurely at a point that is favorable to the adversary. Our results demonstrate that even professional-level AI systems may harbor surprising failure modes. Our results demonstrate that AI systems which are normally superhuman may still be less robust than humans. Example games are available at https://goattack.alignmentfund.org/"}}
{"id": "9gj9vXfeS-y", "cdate": 1665251236862, "mdate": null, "content": {"title": "On The Fragility of Learned Reward Functions", "abstract": "Reward functions are notoriously difficult to specify, especially for tasks with\ncomplex goals. Reward learning approaches attempt to infer reward functions from\nhuman feedback and preferences. Prior works on reward learning have mainly\nfocused on the performance of policies trained alongside the reward function.\nThis practice, however, may fail to detect learned rewards that are not capable of\ntraining new policies from scratch and thus do not capture the intended behavior.\nOur work focuses on demonstrating and studying the causes of these relearning\nfailures in the domain of preference-based reward learning. We demonstrate with\nexperiments in tabular and continuous control environments that the severity of\nrelearning failures can be sensitive to changes in reward model design and the\ntrajectory dataset composition. Based on our findings, we emphasize the need for\nmore retraining-based evaluations in the literature."}}
{"id": "-eid7uct1My", "cdate": 1665251235493, "mdate": null, "content": {"title": "Training Equilibria in Reinforcement Learning", "abstract": "In partially observable environments, reinforcement learning algorithms such as policy gradient and Q-learning may have multiple equilibria---policies that are stable under further training---and can converge to equilibria that are strictly suboptimal. \nPrior work blames insufficient exploration, but suboptimal equilibria can arise despite full exploration and other favorable circumstances like a flexible policy parametrization.\nWe show theoretically that the core problem is that in partially observed environments, an agent's past actions induce a distribution on hidden states.\nEquipping the policy with memory helps it model the hidden state and leads to convergence to a higher reward equilibrium, \\emph{even when there exists a memoryless optimal policy}.\nExperiments show that \npolicies with insufficient memory tend to learn to use the environment as auxiliary memory, and parameter noise helps policies escape suboptimal equilibria. \n"}}
{"id": "ZIWHEw9yU-", "cdate": 1665251229856, "mdate": null, "content": {"title": "Adversarial Policies Beat Professional-Level Go AIs", "abstract": "We attack the state-of-the-art Go-playing AI system, KataGo, by training an adversarial policy that plays against a frozen KataGo victim. Our attack achieves a >99% win-rate against KataGo without search, and a >80% win-rate when KataGo uses enough search to be near-superhuman. To the best of our knowledge, this is the first successful end-to-end attack against a Go AI playing at the level of a top human professional. Notably, the adversary does not win by learning to play Go better than KataGo---in fact, the adversary is easily beaten by human amateurs. Instead, the adversary wins by tricking KataGo into ending the game prematurely at a point that is favorable to the adversary. Our results demonstrate that even professional-level AI systems may harbor surprising failure modes."}}
{"id": "Kyz1SaAcnd", "cdate": 1663850374632, "mdate": null, "content": {"title": "Adversarial Policies Beat Professional-Level Go AIs", "abstract": "We attack the state-of-the-art Go-playing AI system, KataGo, by training an adversarial policy that plays against a frozen KataGo victim. Our attack achieves a >99% win-rate against KataGo without search, and a >80% win-rate when KataGo uses enough search to be near-superhuman. To the best of our knowledge, this is the first successful end-to-end attack against a Go AI playing at the level of a top human professional. Notably, the adversary does not win by learning to play Go better than KataGo---in fact, the adversary is easily beaten by human amateurs. Instead, the adversary wins by tricking KataGo into ending the game prematurely at a point that is favorable to the adversary. Our results demonstrate that even professional-level AI systems may harbor surprising failure modes."}}
{"id": "lpxeg8dhJ-", "cdate": 1663850364802, "mdate": null, "content": {"title": "Training Equilibria in Reinforcement Learning", "abstract": "In partially observable environments, reinforcement learning algorithms such as policy gradient and Q-learning may have multiple equilibria---policies that are stable under further training---and can converge to policies that are strictly suboptimal. \nPrior work blames insufficient exploration, but suboptimal equilibria can arise despite full exploration and other favorable circumstances like a flexible policy parametrization.\nWe show theoretically that the core problem is that in partially observed environments, an agent's past actions induce a distribution on hidden states.\nEquipping the policy with memory helps it model the hidden state and leads to convergence to a higher reward equilibrium, \\emph{even when there exists a memoryless optimal policy}.\nExperiments show that \npolicies with insufficient memory tend to learn to use the environment as auxiliary memory,and parameter noise helps policies escape suboptimal equilibria. "}}
{"id": "CCgGnWUBTmX", "cdate": 1650422874335, "mdate": 1650422874335, "content": {"title": "Adversarial Policies: Attacking Deep Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at this https URL."}}
{"id": "RDu3zez5mNG", "cdate": 1648675560090, "mdate": 1648675560090, "content": {"title": "A Primer on Maximum Causal Entropy Inverse Reinforcement Learning", "abstract": "Inverse Reinforcement Learning (IRL) algorithms infer a reward function that explains demonstrations provided by an expert acting in the environment. Maximum Causal Entropy (MCE) IRL is currently the most popular formulation of IRL, with numerous extensions. In this tutorial, we present a compressed derivation of MCE IRL and the key results from contemporary implementations of MCE IRL algorithms. We hope this will serve both as an introductory resource for those new to the field, and as a concise reference for those already familiar with these topics."}}
