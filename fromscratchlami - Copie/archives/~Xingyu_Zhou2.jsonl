{"id": "PNHGvlDEDHF", "cdate": 1677628800000, "mdate": 1682382338368, "content": {"title": "(Private) Kernelized Bandits with Distributed Biased Feedback", "abstract": "In this paper, we study kernelized bandits with distributed biased feedback. This problem is motivated by several real-world applications (such as dynamic pricing, cellular network configuration, and policy making), where users from a large population contribute to the reward of the action chosen by a central entity, but it is difficult to collect feedback from all users. Instead, only biased feedback (due to user heterogeneity) from a subset of users may be available. In addition to such partial biased feedback, we are also faced with two practical challenges due to communication cost and computation complexity. To tackle these challenges, we carefully design a new distributed phase-then-batch-based elimination (DPBE) algorithm, which samples users in phases for collecting feedback to reduce the bias and employs maximum variance reduction to select actions in batches within each phase. By properly choosing the phase length, the batch size, and the confidence width used for eliminating suboptimal actions, we show that DPBE achieves a sublinear regret of ~O(T1-&#945;/2 +&#8730;&#947;T T), where &#945; &#8712; (0,1) is the user-sampling parameter one can tune. Moreover, DPBE can significantly reduce both communication cost and computation complexity in distributed kernelized bandits, compared to some variants of the state-of-the-art algorithms (originally developed for standard kernelized bandits). Furthermore, by incorporating various differential privacy models (including the central, local, and shuffle models), we generalize DPBE to provide privacy guarantees for users participating in the distributed learning process. Finally, we conduct extensive simulations to validate our theoretical results and evaluate the empirical performance."}}
{"id": "zATbhf0_KG_", "cdate": 1672531200000, "mdate": 1682382338209, "content": {"title": "Provably Efficient Model-Free Algorithms for Non-stationary CMDPs", "abstract": "We study model-free reinforcement learning (RL) algorithms in episodic non-stationary constrained Markov Decision Processes (CMDPs), in which an agent aims to maximize the expected cumulative reward subject to a cumulative constraint on the expected utility (cost). In the non-stationary environment, reward, utility functions, and transition kernels can vary arbitrarily over time as long as the cumulative variations do not exceed certain variation budgets. We propose the first model-free, simulator-free RL algorithms with sublinear regret and zero constraint violation for non-stationary CMDPs in both tabular and linear function approximation settings with provable performance guarantees. Our results on regret bound and constraint violation for the tabular case match the corresponding best results for stationary CMDPs when the total budget is known. Additionally, we present a general framework for addressing the well-known challenges associated with analyzing non-stationary CMDPs, without requiring prior knowledge of the variation budget. We apply the approach for both tabular and linear approximation settings."}}
{"id": "FYvX2WzkhCi", "cdate": 1672531200000, "mdate": 1682382338329, "content": {"title": "On Differentially Private Federated Linear Contextual Bandits", "abstract": "We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos (agents) interact with the local users and communicate via a central server to realize collaboration while without sacrificing each user's privacy. We identify three issues in the state-of-the-art: (i) failure of claimed privacy protection and (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost. To resolve these issues, we take a two-step principled approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm. To further improve the regret performance, we next consider shuffle model of differential privacy, under which we show that our algorithm can achieve nearly ``optimal'' regret without a trusted server. We accomplish this via two different schemes -- one relies on a new result on privacy amplification via shuffling for DP mechanisms and another one leverages the integration of a shuffle protocol for vector sum into the tree-based mechanism, both of which might be of independent interest. Finally, we support our theoretical results with numerical evaluations over contextual bandit instances generated from both synthetic and real-life data."}}
{"id": "7dCaEVg34wj", "cdate": 1672531200000, "mdate": 1682382338367, "content": {"title": "(Private) Kernelized Bandits with Distributed Biased Feedback", "abstract": "In this paper, we study kernelized bandits with distributed biased feedback. This problem is motivated by several real-world applications (such as dynamic pricing, cellular network configuration, and policy making), where users from a large population contribute to the reward of the action chosen by a central entity, but it is difficult to collect feedback from all users. Instead, only biased feedback (due to user heterogeneity) from a subset of users may be available. In addition to such partial biased feedback, we are also faced with two practical challenges due to communication cost and computation complexity. To tackle these challenges, we carefully design a new \\emph{distributed phase-then-batch-based elimination (\\texttt{DPBE})} algorithm, which samples users in phases for collecting feedback to reduce the bias and employs \\emph{maximum variance reduction} to select actions in batches within each phase. By properly choosing the phase length, the batch size, and the confidence width used for eliminating suboptimal actions, we show that \\texttt{DPBE} achieves a sublinear regret of $\\tilde{O}(T^{1-\\alpha/2}+\\sqrt{\\gamma_T T})$, where $\\alpha\\in (0,1)$ is the user-sampling parameter one can tune. Moreover, \\texttt{DPBE} can significantly reduce both communication cost and computation complexity in distributed kernelized bandits, compared to some variants of the state-of-the-art algorithms (originally developed for standard kernelized bandits). Furthermore, by incorporating various \\emph{differential privacy} models (including the central, local, and shuffle models), we generalize \\texttt{DPBE} to provide privacy guarantees for users participating in the distributed learning process. Finally, we conduct extensive simulations to validate our theoretical results and evaluate the empirical performance."}}
{"id": "aEOd6wh5GA", "cdate": 1665069637541, "mdate": null, "content": {"title": "Distributed Differential Privacy in Multi-Armed Bandits", "abstract": "We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server. Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\\epsilon,\\delta$) or approximate-DP guarantee by sacrificing an additive $O\\!\\left(\\!\\frac{K\\log T\\sqrt{\\log(1/\\delta)}}{\\epsilon}\\!\\right)\\!$ factor in $T$-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger ($\\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\\Theta\\!\\left(\\!\\frac{K\\log T}{\\epsilon}\\!\\right)\\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We numerically simulate regret performance of our algorithm, which corroborate our theoretical findings."}}
{"id": "cw8FeirkIfU", "cdate": 1663850196278, "mdate": null, "content": {"title": "Distributed Differential Privacy in Multi-Armed Bandits", "abstract": "We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server.  Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\\epsilon,\\delta$) or approximate-DP guarantee by sacrificing an additive $O\\!\\left(\\!\\frac{K\\log T\\sqrt{\\log(1/\\delta)}}{\\epsilon}\\!\\right)\\!$ factor in $T$-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger ($\\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\\Theta\\!\\left(\\!\\frac{K\\log T}{\\epsilon}\\!\\right)\\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures \\emph{R\\'{e}nyi differential privacy} -- a stronger notion than approximate DP -- under distributed trust model with a privacy cost of $O\\!\\left(\\!\\frac{K\\sqrt{\\log T}}{\\epsilon}\\!\\right)\\!$. Finally, as a by-product of our techniques, we also recover the best-known regret bounds for bandits under central and local models while using only \\emph{discrete privacy noise}, which can avoid the privacy leakage due to floating point arithmetic of continuous noise on finite computers."}}
{"id": "zZhX4eYNeeh", "cdate": 1663850174536, "mdate": null, "content": {"title": "Achieving Sub-linear Regret in Infinite Horizon Average Reward Constrained MDP with Linear Function Approximation", "abstract": "We study the infinite horizon average reward constrained Markov Decision Process (CMDP). In contrast to existing works on model-based, finite state space, we consider the model-free linear CMDP setup.  We first propose a computationally inefficient algorithm and show that $\\tilde{\\mathcal{O}}(\\sqrt{d^3T})$ regret and constraint violation can be achieved, in which $T$ is the number of interactions, and $d$ is the dimension of the feature mapping. We also propose an efficient variant based on the primal-dual adaptation of the LSVI-UCB algorithm and show that $\\tilde{\\mathcal{O}}((dT)^{3/4})$ regret and constraint violation can be achieved. \nThis improves the known regret bound of $\\tilde{\\mathcal{O}}(T^{5/6})$ for the finite state-space model-free constrained RL which was obtained under a stronger assumption compared to ours.  We also develop an efficient policy-based algorithm via novel adaptation of the MDP-EXP2 algorithm to our primal-dual set up with $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret and even zero constraint violation bound under a stronger set of assumptions."}}
{"id": "wgRQ1IM4g_w", "cdate": 1652737750447, "mdate": null, "content": {"title": "On Kernelized Multi-Armed Bandits with Constraints", "abstract": "We study a stochastic bandit problem with a general unknown reward function and a general unknown constraint function. Both functions can be non-linear (even non-convex) and are assumed to lie in a reproducing kernel Hilbert space (RKHS) with a bounded norm. This kernelized bandit setup strictly generalizes standard multi-armed bandits and linear bandits. In contrast to safety-type hard constraints studied in prior works, we consider soft constraints that may be violated in any round as long as the cumulative violations are small, which is motivated by various practical applications. Our ultimate goal is to study how to utilize the nature of soft constraints to attain a finer complexity-regret-constraint trade-off in the kernelized bandit setting. To this end, leveraging primal-dual optimization, we propose a general framework for both algorithm design and performance analysis. This framework builds upon a novel sufficient condition, which not only is satisfied under general exploration strategies, including \\emph{upper confidence bound} (UCB), \\emph{Thompson sampling} (TS), and new ones based on \\emph{random exploration}, but also enables a unified analysis for showing both sublinear regret and sublinear or even zero constraint violation. We demonstrate the superior performance of our proposed algorithms via numerical experiments based on both synthetic and real-world datasets. Along the way, we also make the first detailed comparison between two popular methods for analyzing constrained bandits and Markov decision processes (MDPs) by discussing the key difference and some subtleties in the analysis, which could be of independent interest to the communities."}}
{"id": "Gf5DxrgD2cT", "cdate": 1652737644825, "mdate": null, "content": {"title": "Provably Efficient Model-Free Constrained RL with Linear Function Approximation", "abstract": "We study the constrained reinforcement learning problem, in which an agent aims to maximize the expected cumulative reward subject to a constraint on the expected total value of a utility function.  In contrast to existing model-based approaches or model-free methods accompanied with a `simulator\u2019, we aim to develop the first \\emph{model-free}, \\emph{simulator-free} algorithm that achieves a sublinear regret and a sublinear constraint violation even in \\emph{large-scale} systems. To this end, we consider the episodic constrained Markov decision processes with linear function approximation, where the transition dynamics and the reward function can be represented as a linear function of some known feature mapping. We show that $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ regret and  $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ constraint violation bounds can be achieved, where $d$ is the dimension of the feature mapping, $H$ is the length of the episode, and $T$ is the total number of steps. Our bounds are attained without explicitly estimating the unknown transition model or requiring a simulator, and they depend on the state space only through the dimension of the feature mapping. Hence our bounds hold even when the number of states goes to infinity. Our main results are achieved via novel adaptations of the standard LSVI-UCB algorithms. In particular, we first introduce primal-dual optimization into the LSVI-UCB algorithm to balance between regret and constraint violation. More importantly, we replace the standard greedy selection with respect to the state-action function with a soft-max policy. \nThis turns out to be key in establishing uniform concentration (a critical step for provably efficient model-free exploration) for the constrained case via its approximation-smoothness trade-off. Finally, we also show that one can achieve an even zero constraint violation for large enough $T$ by trading the regret a little bit but still maintaining the same order with respect to $T$."}}
{"id": "xMhOT-TGdw", "cdate": 1640995200000, "mdate": 1682382338388, "content": {"title": "Distributed Differential Privacy in Multi-Armed Bandits", "abstract": "We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server. Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\\epsilon,\\delta$) or approximate-DP guarantee by sacrificing an additional additive $O\\!\\left(\\!\\frac{K\\log T\\sqrt{\\log(1/\\delta)}}{\\epsilon}\\!\\right)\\!$ cost in $T$-step cumulative regret. In contrast, the optimal privacy cost for achieving a stronger ($\\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\\Theta\\!\\left(\\!\\frac{K\\log T}{\\epsilon}\\!\\right)\\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures \\emph{R\\'{e}nyi differential privacy} -- a stronger notion than approximate DP -- under distributed trust model with a privacy cost of $O\\!\\left(\\!\\frac{K\\sqrt{\\log T}}{\\epsilon}\\!\\right)\\!$."}}
