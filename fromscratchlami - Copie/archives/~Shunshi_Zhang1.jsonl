{"id": "SnEez8-mrl5", "cdate": 1645715785876, "mdate": 1645715785876, "content": {"title": "Analysis of Langevin Monte Carlo from Poincar\u00e9 to Log-Sobolev", "abstract": "Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution \u03c0 under the sole assumption that \u03c0 satisfies a Poincar\u00e9 inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or R\u00e9nyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that \u03c0 satisfies either a Lata\u0142a--Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincar\u00e9 and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions."}}
{"id": "l_uEPrXqdxL", "cdate": 1640995200000, "mdate": 1681768142279, "content": {"title": "Towards a Theory of Non-Log-Concave Sampling: First-Order Stationarity Guarantees for Langevin Monte Carlo", "abstract": "For the task of sampling from a density $\\pi \\propto \\exp(-V)$ on $\\R^d$, where $V$ is possibly non-convex but $L$-gradient Lipschitz, we prove that averaged Langevin Monte Carlo outputs a sample w..."}}
{"id": "kh27wRIlvy", "cdate": 1640995200000, "mdate": 1681768142096, "content": {"title": "Analysis of Langevin Monte Carlo from Poincare to Log-Sobolev", "abstract": "Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution $\\pi$ under the sole assumption that $\\pi$ satisfies a Poincar\u00e9 inequality. Using thi..."}}
{"id": "XpXvQy2ob2b", "cdate": 1640995200000, "mdate": 1682317798732, "content": {"title": "Convergence of Langevin Monte Carlo in Chi-Squared and R\u00e9nyi Divergence", "abstract": "We study sampling from a target distribution $\\nu_* = e^{-f}$ using the unadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$ satisfies a strong dissipativity condition and it is first-order smooth with a Lipschitz gradient. We prove that, initialized with a Gaussian random vector that has sufficiently small variance, iterating the LMC algorithm for $\\widetilde{\\mathcal{O}}(\\lambda^2 d\\epsilon^{-1})$ steps is sufficient to reach $\\epsilon$-neighborhood of the target in both Chi-squared and R\u00e9nyi divergence, where $\\lambda$ is the logarithmic Sobolev constant of $\\nu_*$. Our results do not require warm-start to deal with the exponential dimension dependency in Chi-squared divergence at initialization. In particular, for strongly convex and first-order smooth potentials, we show that the LMC algorithm achieves the rate estimate $\\widetilde{\\mathcal{O}}(d\\epsilon^{-1})$ which improves the previously known rates in both of these metrics, under the same assumptions. Translating this rate to other metrics, our results also recover the state-of-the-art rate estimates in KL divergence, total variation and $2$-Wasserstein distance in the same setup. Finally, as we rely on the logarithmic Sobolev inequality, our framework covers a range of non-convex potentials that are first-order smooth and exhibit strong convexity outside of a compact region."}}
{"id": "OpMgwwEwSe", "cdate": 1640995200000, "mdate": 1666775788696, "content": {"title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings", "abstract": "Policy gradient methods have been frequently applied to problems in control and reinforcement learning with great success, yet existing convergence analysis still relies on non-intuitive, impractical and often opaque conditions. In particular, existing rates are achieved in limited settings, under strict regularity conditions. In this work, we establish explicit convergence rates of policy gradient methods, extending the convergence regime to weakly smooth policy classes with L2 integrable gradient. We provide intuitive examples to illustrate the insight behind these new conditions. Notably, our analysis also shows that convergence rates are achievable for both the standard policy gradient and the natural policy gradient algorithms under these assumptions. Lastly we provide performance guarantees for the converged policies."}}
{"id": "1kZw8lSIIXj", "cdate": 1609459200000, "mdate": 1666775788940, "content": {"title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings", "abstract": "Policy gradient methods have been frequently applied to problems in control and reinforcement learning with great success, yet existing convergence analysis still relies on non-intuitive, impractical and often opaque conditions. In particular, existing rates are achieved in limited settings, under strict regularity conditions. In this work, we establish explicit convergence rates of policy gradient methods, extending the convergence regime to weakly smooth policy classes with $L_2$ integrable gradient. We provide intuitive examples to illustrate the insight behind these new conditions. Notably, our analysis also shows that convergence rates are achievable for both the standard policy gradient and the natural policy gradient algorithms under these assumptions. Lastly we provide performance guarantees for the converged policies."}}
{"id": "s-TuDxPQUu", "cdate": 1577836800000, "mdate": 1682317798717, "content": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "abstract": "Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext."}}
{"id": "r1e9GCNKvH", "cdate": 1569439250185, "mdate": null, "content": {"title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation", "abstract": "  Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext. "}}
{"id": "H1lefTEKDS", "cdate": 1569438983892, "mdate": null, "content": {"title": "Benchmarking Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) is widely seen as having the potential\nto be significantly more sample efficient than model-free RL. However, research in\nmodel-based RL has not been very standardized. It is fairly common for authors to\nexperiment with self-designed environments, and there are several separate lines of\nresearch, which are sometimes closed-sourced or not reproducible. Accordingly, it\nis an open question how these various existing algorithms perform relative to each\nother. To facilitate research in MBRL, in this paper we gather a wide collection\nof MBRL algorithms and propose over 18 benchmarking environments specially\ndesigned for MBRL. We benchmark these algorithms with unified problem settings,\nincluding noisy environments. Beyond cataloguing performance, we explore\nand unify the underlying algorithmic differences across MBRL algorithms. We\ncharacterize three key research challenges for future MBRL research: the dynamics\nbottleneck, the planning horizon dilemma, and the early-termination dilemma.\nFinally, to facilitate future research on MBRL, we open-source our benchmark."}}
{"id": "eQnYjm87BU", "cdate": 1546300800000, "mdate": 1682317798759, "content": {"title": "Benchmarking Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/~tingwuwang/mbrl.html."}}
