{"id": "XHIY3cQ8Tew", "cdate": 1681425420920, "mdate": null, "content": {"title": "AutoGluon\u2013TimeSeries: AutoML for Probabilistic Time Series Forecasting", "abstract": "We introduce AutoGluon\u2013TimeSeries\u2014an open-source AutoML library for probabilistic time series forecasting. Focused on ease of use and robustness, AutoGluon\u2013TimeSeries enables users to generate accurate point and quantile forecasts with just 3 lines of Python code. Built on the design philosophy of AutoGluon, AutoGluon\u2013TimeSeries leverages ensembles of diverse forecasting models to deliver high accuracy within a short training time. AutoGluon\u2013TimeSeries combines both conventional statistical models, machine-learning based forecasting approaches, and ensembling techniques. In our evaluation on 29 benchmark datasets, AutoGluon\u2013TimeSeries demonstrates strong empirical performance, outperforming a range of forecasting methods in terms of both point and quantile forecast accuracy, and often even improving upon the best-in-hindsight combination of prior methods."}}
{"id": "zaBJZmaSOf7", "cdate": 1679417874566, "mdate": null, "content": {"title": "Obeying the Order: Introducing Ordered Transfer Hyperparameter Optimisation", "abstract": "We introduce ordered transfer hyperparameter optimisation (OTHPO), a version of transfer\nlearning for hyperparameter optimisation (HPO) where the tasks follow a sequential\norder. Unlike for state-of-the-art transfer HPO, the assumption is that each task is most\ncorrelated to those immediately before it. This matches many deployed settings, where\nhyperparameters are retuned as more data is collected; for instance tuning a sequence of\nmovie recommendation systems as more movies and ratings are added. We propose a formal\ndefinition, outline the differences to related problems and propose a basic OTHPO method\nthat outperforms state-of-the-art transfer HPO. We empirically show the importance of taking order into\naccount using ten benchmarks. The benchmarks are in the setting of gradually accumulating\ndata, and span XGBoost, random forest, approximate k-nearest neighbor, elastic net,\nsupport vector machines and a separate real-world motivated optimisation problem. We\nopen source the benchmarks to foster future research on ordered transfer HPO."}}
{"id": "BNeNQWaBIgq", "cdate": 1645792506921, "mdate": null, "content": {"title": "Automatic Termination for Hyperparameter Optimization", "abstract": "Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) \nin machine learning. \nAt its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops \nthe procedure if it is sufficiently close to the global optimum. Our key insight is that the discrepancy between the true objective (predictive performance on test data) and the computable target (validation performance) suggests stopping once the suboptimality in optimizing the target is dominated by the statistical estimation error.\nAcross an extensive range of real-world HPO problems and baselines, we show that our termination criterion achieves a better trade-off between the test performance and optimization time.\nAdditionally, we find that overfitting may occur in the context of HPO, which is arguably an overlooked problem in the literature, and show how our termination criterion helps to mitigate this phenomenon on both small and large datasets."}}
{"id": "2NqIV8dzR7N", "cdate": 1632875617467, "mdate": null, "content": {"title": "Automatic Termination for Hyperparameter Optimization", "abstract": "Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) of machine learning algorithms. At its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops the procedure if it is sufficiently close to the global optima. Across an extensive range of real-world HPO problems, we show that our termination criterion achieves better test performance compared to existing baselines from the literature, such as stopping when the probability of improvement drops below a fixed threshold. We also provide evidence that these baselines are, compared to our method, highly sensitive to the choices of their own hyperparameters. Additionally, we find that overfitting might occur in the context of HPO, which is arguably an overlooked problem in the literature, and show that our termination criterion mitigates this phenomenon on both small and large datasets."}}
{"id": "SKUHN5b3PmG", "cdate": 1609459200000, "mdate": null, "content": {"title": "Overfitting in Bayesian Optimization: an empirical study and early-stopping solution", "abstract": "Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) in machine learning. At its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops the procedure if it is sufficiently close to the global optimum. Our key insight is that the discrepancy between the true objective (predictive performance on test data) and the computable target (validation performance) suggests stopping once the suboptimality in optimizing the target is dominated by the statistical estimation error. Across an extensive range of real-world HPO problems and baselines, we show that our termination criterion achieves a better trade-off between the test performance and optimization time. Additionally, we find that overfitting may occur in the context of HPO, which is arguably an overlooked problem in the literature, and show how our termination criterion helps to mitigate this phenomenon on both small and large datasets."}}
{"id": "RBKqMb3di-D", "cdate": 1609459200000, "mdate": 1631523466520, "content": {"title": "Amazon SageMaker Automatic Model Tuning: Scalable Gradient-Free Optimization", "abstract": "Tuning complex machine learning systems is challenging. Machine learning typically requires to set hyperparameters, be it regularization, architecture, or optimization parameters, whose tuning is critical to achieve good predictive performance. To democratize access to machine learning systems, it is essential to automate the tuning. This paper presents Amazon SageMaker Automatic Model Tuning (AMT), a fully managed system for gradient-free optimization at scale. AMT finds the best version of a trained machine learning model by repeatedly evaluating it with different hyperparameter configurations. It leverages either random search or Bayesian optimization to choose the hyperparameter values resulting in the best model, as measured by the metric chosen by the user. AMT can be used with built-in algorithms, custom algorithms, and Amazon SageMaker pre-built containers for machine learning frameworks. We discuss the core functionality, system architecture, our design principles, and lessons learned. We also describe more advanced features of AMT, such as automated early stopping and warm-starting, showing in experiments their benefits to users."}}
{"id": "Zj80wPudiRq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Amazon SageMaker Automatic Model Tuning: Scalable Black-box Optimization", "abstract": "Tuning complex machine learning systems is challenging. Machine learning typically requires to set hyperparameters, be it regularization, architecture, or optimization parameters, whose tuning is critical to achieve good predictive performance. To democratize access to machine learning systems, it is essential to automate the tuning. This paper presents Amazon SageMaker Automatic Model Tuning (AMT), a fully managed system for gradient-free optimization at scale. AMT finds the best version of a trained machine learning model by repeatedly evaluating it with different hyperparameter configurations. It leverages either random search or Bayesian optimization to choose the hyperparameter values resulting in the best model, as measured by the metric chosen by the user. AMT can be used with built-in algorithms, custom algorithms, and Amazon SageMaker pre-built containers for machine learning frameworks. We discuss the core functionality, system architecture, our design principles, and lessons learned. We also describe more advanced features of AMT, such as automated early stopping and warm-starting, showing in experiments their benefits to users."}}
{"id": "UsKM4v9lwJx", "cdate": 1577836800000, "mdate": 1631523466174, "content": {"title": "Amazon SageMaker Autopilot: a white box AutoML solution at scale", "abstract": "AutoML systems provide a black-box solution to machine learning problems by selecting the right way of processing features, choosing an algorithm and tuning the hyperparameters of the entire pipeline. Although these systems perform well on many datasets, there is still a non-negligible number of datasets for which the one-shot solution produced by each particular system would provide sub-par performance. In this paper, we present Amazon SageMaker Autopilot: a fully managed system providing an automated ML solution that can be modified when needed. Given a tabular dataset and the target column name, Autopilot identifies the problem type, analyzes the data and produces a diverse set of complete ML pipelines including feature preprocessing and ML algorithms, which are tuned to generate a leaderboard of candidate models. In the scenario where the performance is not satisfactory, a data scientist is able to view and edit the proposed ML pipelines in order to infuse their expertise and business knowledge without having to revert to a fully manual solution. This paper describes the different components of Autopilot, emphasizing the infrastructure choices that allow scalability, high quality models, editable ML pipelines, consumption of artifacts of offline meta-learning, and a convenient integration with the entire SageMaker suite allowing these trained models to be used in a production setting."}}
{"id": "RTvkOXWLyT6", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Quantile-based Approach for Hyperparameter Transfer Learning", "abstract": "Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Traditionally, BO focuses on a single task at a time and is not designed to leverag..."}}
{"id": "6dJ5u3bxeVC", "cdate": 1577836800000, "mdate": 1631523466521, "content": {"title": "Amazon SageMaker Autopilot: a white box AutoML solution at scale", "abstract": "We present Amazon SageMaker Autopilot: a fully managed system that provides an automatic machine learning solution. Given a tabular dataset and the target column name, Autopilot identifies the problem type, analyzes the data and produces a diverse set of complete ML pipelines, which are tuned to generate a leaderboard of candidate models that the customer can choose from. The diversity allows users to balance between different needs such as model accuracy vs. latency. By exposing not only the final models but the way they are trained, meaning the pipelines, we allow to customize the generated training pipeline, thus catering the need of users with different levels of expertise. This trait is crucial for users and is the main novelty of Autopilot; it provides a solution that on one hand is not fully black-box and can be further worked on, while on the other hand is not a do it yourself solution, requiring expertise in all aspects of machine learning. This paper describes the different components in the eco-system of Autopilot, emphasizing the infrastructure choices that allow scalability, high quality models, editable ML pipelines, consumption of artifacts of offline meta-learning, and a convenient integration with the entire SageMaker system allowing these trained models to be used in a production setting."}}
