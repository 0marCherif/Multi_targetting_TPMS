{"id": "dkTmdhgapdn", "cdate": 1667358203826, "mdate": 1667358203826, "content": {"title": "Direct Association of Object Queries for Video Instance Segmentation", "abstract": "Recent Transformer-based offline Video Instance Segmentation (VIS) studies have shown that localizing the information in Transformer layers is more effective than attending to the entire spatio-temporal feature volume. From this observation, we hypothesize that explicit use of object-oriented information on spatial scenes can be a strong solution for understanding the context of the entire sequence. Thus, we introduce a new paradigm for offline VIS that learns to integrate decoded object queries from independent frames. Specifically, we propose a simple module that can be easily built on top of an off-the-shelf Transformer-based image instance segmentation model. Leaving the frame-level model to distill the rich knowledge of the spatial scene into its object queries, the proposed module directly associates and identifies the given potential objects by building temporal interactions in between. With a Swin-L backbone, our proposed method sets a record of 50.7 AP which ranks the 3rd place in Track 2-Video Instance Segmentation of the 4th Large-scale Video Object Segmentation Challenge."}}
{"id": "xnuN2vGmZA0", "cdate": 1652737316727, "mdate": null, "content": {"title": "VITA: Video Instance Segmentation via Object Token Association", "abstract": "We introduce a novel paradigm for offline Video Instance Segmentation (VIS), based on the hypothesis that explicit object-oriented information can be a strong clue for understanding the context of the entire sequence. To this end, we propose VITA, a simple structure built on top of an off-the-shelf Transformer-based image instance segmentation model. Specifically, we use an image object detector as a means of distilling object-specific contexts into object tokens. VITA accomplishes video-level understanding by associating frame-level object tokens without using spatio-temporal backbone features. By effectively building relationships between objects using the condensed information, VITA achieves the state-of-the-art on VIS benchmarks with a ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 & 2021, and 19.6 AP on OVIS. Moreover, thanks to its object token-based structure that is disjoint from the backbone features, VITA shows several practical advantages that previous offline VIS methods have not explored - handling long and high-resolution videos with a common GPU, and freezing a frame-level detector trained on image domain. Code is available at the link."}}
{"id": "uUUyTNKKuj", "cdate": 1640995200000, "mdate": 1667358010320, "content": {"title": "VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation", "abstract": "For online video instance segmentation (VIS), fully utilizing the information from previous frames in an efficient manner is essential for real-time applications. Most previous methods follow a two-stage approach requiring additional computations such as RPN and RoIAlign, and do not fully exploit the available information in the video for all subtasks in VIS. In this paper, we propose a novel single-stage framework for online VIS built based on the grid structured feature representation. The grid-based features allow us to employ fully convolutional networks for real-time processing, and also to easily reuse and share features within different components. We also introduce cooperatively operating modules that aggregate information from available frames, in order to enrich the features for all subtasks in VIS. Our design fully takes advantage of previous information in a grid form for all tasks in VIS in an efficient way, and we achieved the new state-of-the-art accuracy (38.6 AP and 36.9 AP) and speed (40.0 FPS) on YouTube-VIS 2019 and 2021 datasets among online VIS methods. The code is available at https://github.com/SuHoHan95/VISOLQ"}}
{"id": "e3TiC2KJRd", "cdate": 1640995200000, "mdate": 1693576895467, "content": {"title": "VITA: Video Instance Segmentation via Object Token Association", "abstract": "We introduce a novel paradigm for offline Video Instance Segmentation (VIS), based on the hypothesis that explicit object-oriented information can be a strong clue for understanding the context of the entire sequence. To this end, we propose VITA, a simple structure built on top of an off-the-shelf Transformer-based image instance segmentation model. Specifically, we use an image object detector as a means of distilling object-specific contexts into object tokens. VITA accomplishes video-level understanding by associating frame-level object tokens without using spatio-temporal backbone features. By effectively building relationships between objects using the condensed information, VITA achieves the state-of-the-art on VIS benchmarks with a ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 &amp; 2021, and 19.6 AP on OVIS. Moreover, thanks to its object token-based structure that is disjoint from the backbone features, VITA shows several practical advantages that previous offline VIS methods have not explored - handling long and high-resolution videos with a common GPU, and freezing a frame-level detector trained on image domain. Code is available at the link."}}
{"id": "L1NMS6tMP6Y", "cdate": 1640995200000, "mdate": 1667358010368, "content": {"title": "Integrating Pose and Mask Predictions for Multi-person in Videos", "abstract": "In real-world applications for video editing, humans are arguably the most important objects. When editing videos of humans, the efficient tracking of fine-grained masks and body joints is the fundamental requirement. In this paper, we propose a simple and efficient system for jointly tracking pose and segmenting high-quality masks for all humans in the video. We design a pipeline that globally tracks pose and locally segments fine-grained masks. Specifically, CenterTrack is first employed to track human poses by viewing the whole scene, and then the proposed local segmentation network leverages the pose information as a powerful query to carry out high-quality segmentation. Furthermore, we adopt a highly light-weight MLP-Mixer layer within the segmentation network that can efficiently propagate the query pose throughout the region of interest with minimal overhead. For the evaluation, we collect a new benchmark called KineMask which includes various appearances and actions. The experimental results demonstrate that our method has superior fine-grained segmentation performance. Moreover, it runs at 33 fps, achieving a great balance of speed and accuracy compared to the prevailing online Video Instance Segmentation methods."}}
{"id": "2IXgvhBhYq", "cdate": 1640995200000, "mdate": 1667358010358, "content": {"title": "Cannot See the Forest for the Trees: Aggregating Multiple Viewpoints to Better Classify Objects in Videos", "abstract": "Recently, both long-tailed recognition and object tracking have made great advances individually. TAO benchmark presented a mixture of the two, long-tailed object tracking, in order to further reflect the aspect of the real-world. To date, existing solutions have adopted detectors showing robustness in long-tailed distributions, which derive per-frame results. Then, they used tracking algorithms that combine the temporally independent detections to finalize tracklets. However, as the approaches did not take temporal changes in scenes into account, inconsistent classification results in videos led to low overall performance. In this paper, we present a set classifier that improves accuracy of classifying tracklets by aggregating information from multiple viewpoints contained in a tracklet. To cope with sparse annotations in videos, we further propose augmentation of tracklets that can maximize data efficiency. The set classifier is plug-and-playable to existing object trackers, and highly improves the performance of long-tailed object tracking. By simply attaching our method to QDTrack on top of ResNet-101, we achieve the new state-of-the-art, 19.9% and 15.7% <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$TrackAP_{50}$</tex> on TAO validation and test sets, respectively. Our code is available at this link <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> https://github.com/sukjunhwang/setclassifier."}}
{"id": "0bwOHVQOlml", "cdate": 1640995200000, "mdate": 1667358010382, "content": {"title": "Single-shot Path Integrated Panoptic Segmentation", "abstract": "Panoptic segmentation, which is a novel task of unifying instance segmentation and semantic segmentation, has attracted a lot of attention lately. However, most of the previous methods are composed of multiple pathways with each pathway specialized to a designated segmentation task. In this paper, we propose to resolve panoptic segmentation in single-shot by integrating the execution flows. With the integrated pathway, a unified feature map called Panoptic-Feature is generated, which includes the information of both things and stuffs. Panoptic-Feature becomes more sophisticated by auxiliary problems that guide to cluster pixels that belong to the same instance and differentiate between objects of different classes. A collection of convolutional filters, where each filter represents either a thing or stuff, is applied to Panoptic-Feature at once, materializing the single-shot panoptic segmentation. Taking the advantages of both top-down and bottom-up approaches, our method, named SPINet, enjoys high efficiency and accuracy on major panoptic segmentation benchmarks: COCO and Cityscapes."}}
{"id": "s95BePNvykX", "cdate": 1621629772292, "mdate": null, "content": {"title": "Video Instance Segmentation using Inter-Frame Communication Transformers", "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. \nRecently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. \nHowever, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality.\nIn this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip.\nSpecifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene.\nThe features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.\nWe validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). \nOur method can also be applied to near-online inference for processing a video in real-time with only a small delay.\nThe code is available at https://github.com/sukjunhwang/IFC"}}
{"id": "pvjfA4wogD6", "cdate": 1621629772292, "mdate": null, "content": {"title": "Video Instance Segmentation using Inter-Frame Communication Transformers", "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. \nRecently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. \nHowever, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality.\nIn this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip.\nSpecifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene.\nThe features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.\nWe validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). \nOur method can also be applied to near-online inference for processing a video in real-time with only a small delay.\nThe code is available at https://github.com/sukjunhwang/IFC"}}
{"id": "NHtV22nknWm", "cdate": 1609459200000, "mdate": 1667358010367, "content": {"title": "Video Instance Segmentation using Inter-Frame Communication Transformers", "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality.In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip.Specifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene.The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.We validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay.The code is available at https://github.com/sukjunhwang/IFC"}}
