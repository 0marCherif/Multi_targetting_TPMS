{"id": "wQVjGP5NbP9", "cdate": 1652737767985, "mdate": null, "content": {"title": "Near-Optimal Correlation Clustering with Privacy", "abstract": "Correlation clustering is a central problem in unsupervised learning, with applications spanning community detection, duplicate detection, automated labeling and many more. In the correlation clustering problem one receives as input a set of nodes and for each node a list of co-clustering preferences, and the goal is to output a clustering that minimizes the disagreement with the specified nodes' preferences. In this paper, we introduce a simple and computationally efficient algorithm for the correlation clustering problem with provable privacy guarantees. Our additive error is stronger than those obtained in prior work and is optimal up to polylogarithmic factors for fixed privacy parameters."}}
{"id": "sc15XrTSgf", "cdate": 1640995200000, "mdate": 1681668973329, "content": {"title": "Deletion Robust Non-Monotone Submodular Maximization over Matroids", "abstract": "Maximizing a submodular function is a fundamental task in machine learning and in this paper we study the deletion robust version of the problem under the classic matroids constraint. Here the goal is to extract a small size summary of the dataset that contains a high value independent set even after an adversary deleted some elements. We present constant-factor approximation algorithms, whose space complexity depends on the rank $k$ of the matroid and the number $d$ of deleted elements. In the centralized setting we present a $(4.597+O(\\varepsilon))$-approximation algorithm with summary size $O( \\frac{k+d}{\\varepsilon^2}\\log \\frac{k}{\\varepsilon})$ that is improved to a $(3.582+O(\\varepsilon))$-approximation with $O(k + \\frac{d}{\\varepsilon^2}\\log \\frac{k}{\\varepsilon})$ summary size when the objective is monotone. In the streaming setting we provide a $(9.435 + O(\\varepsilon))$-approximation algorithm with summary size and memory $O(k + \\frac{d}{\\varepsilon^2}\\log \\frac{k}{\\varepsilon})$; the approximation factor is then improved to $(5.582+O(\\varepsilon))$ in the monotone case."}}
{"id": "iEkF7LXAs7b", "cdate": 1640995200000, "mdate": 1681721850725, "content": {"title": "Submodular Maximization Subject to Matroid Intersection on the Fly", "abstract": "Despite a surge of interest in submodular maximization in the data stream model, there remain significant gaps in our knowledge about what can be achieved in this setting, especially when dealing with multiple constraints. In this work, we nearly close several basic gaps in submodular maximization subject to $k$ matroid constraints in the data stream model. We present a new hardness result showing that super polynomial memory in $k$ is needed to obtain an $o(k / \\log k)$-approximation. This implies near optimality of prior algorithms. For the same setting, we show that one can nevertheless obtain a constant-factor approximation by maintaining a set of elements whose size is independent of the stream size. Finally, for bipartite matching constraints, a well-known special case of matroid intersection, we present a new technique to obtain hardness bounds that are significantly stronger than those obtained with prior approaches. Prior results left it open whether a $2$-approximation may exist in this setting, and only a complexity-theoretic hardness of $1.91$ was known. We prove an unconditional hardness of $2.69$."}}
{"id": "_RznQD4Qs2H", "cdate": 1640995200000, "mdate": 1649106181883, "content": {"title": "Deletion Robust Submodular Maximization over Matroids", "abstract": "Maximizing a monotone submodular function is a fundamental task in machine learning. In this paper, we study the deletion robust version of the problem under the classic matroids constraint. Here the goal is to extract a small size summary of the dataset that contains a high value independent set even after an adversary deleted some elements. We present constant-factor approximation algorithms, whose space complexity depends on the rank $k$ of the matroid and the number $d$ of deleted elements. In the centralized setting we present a $(3.582+O(\\varepsilon))$-approximation algorithm with summary size $O(k + \\frac{d \\log k}{\\varepsilon^2})$. In the streaming setting we provide a $(5.582+O(\\varepsilon))$-approximation algorithm with summary size and memory $O(k + \\frac{d \\log k}{\\varepsilon^2})$. We complement our theoretical results with an in-depth experimental analysis showing the effectiveness of our algorithms on real-world datasets."}}
{"id": "XPlzJwHhFhJ", "cdate": 1640995200000, "mdate": 1681721850721, "content": {"title": "Submodular Maximization Subject to Matroid Intersection on the Fly", "abstract": "Despite a surge of interest in submodular maximization in the data stream model, there remain significant gaps in our knowledge about what can be achieved in this setting, especially when dealing with multiple constraints. In this work, we nearly close several basic gaps in submodular maximization subject to k matroid constraints in the data stream model. We present a new hardness result showing that super polynomial memory in k is needed to obtain an o(k/(log k))-approximation. This implies near optimality of prior algorithms. For the same setting, we show that one can nevertheless obtain a constant-factor approximation by maintaining a set of elements whose size is independent of the stream size. Finally, for bipartite matching constraints, a well-known special case of matroid intersection, we present a new technique to obtain hardness bounds that are significantly stronger than those obtained with prior approaches. Prior results left it open whether a 2-approximation may exist in this setting, and only a complexity-theoretic hardness of 1.91 was known. We prove an unconditional hardness of 2.69."}}
{"id": "JfeIWtJ7w0_", "cdate": 1640995200000, "mdate": 1681721850723, "content": {"title": "Near-Optimal Correlation Clustering with Privacy", "abstract": "Correlation clustering is a central problem in unsupervised learning, with applications spanning community detection, duplicate detection, automated labelling and many more. In the correlation clustering problem one receives as input a set of nodes and for each node a list of co-clustering preferences, and the goal is to output a clustering that minimizes the disagreement with the specified nodes' preferences. In this paper, we introduce a simple and computationally efficient algorithm for the correlation clustering problem with provable privacy guarantees. Our approximation guarantees are stronger than those shown in prior work and are optimal up to logarithmic factors."}}
{"id": "HuCsi2v9nKl", "cdate": 1640995200000, "mdate": 1681668973187, "content": {"title": "Deletion Robust Submodular Maximization over Matroids", "abstract": "Maximizing a monotone submodular function is a fundamental task in machine learning. In this paper we study the deletion robust version of the problem under the classic matroids constraint. Here th..."}}
{"id": "9ZyeXdmafh", "cdate": 1640995200000, "mdate": 1681721850725, "content": {"title": "Approximate Cluster Recovery from Noisy Labels", "abstract": "Designing algorithms for machine learning problems targeting beyond worst-case analysis and, in particular, analyzing the effect of side-information on the complexity of such problems is a very imp..."}}
{"id": "5K_d9_IbHc", "cdate": 1640995200000, "mdate": 1681721850725, "content": {"title": "Streaming Submodular Maximization Under Matroid Constraints", "abstract": "Recent progress in (semi-)streaming algorithms for monotone submodular function maximization has led to tight results for a simple cardinality constraint. However, current techniques fail to give a similar understanding for natural generalizations, including matroid constraints. This paper aims at closing this gap. For a single matroid of rank k (i.e., any solution has cardinality at most k), our main results are: - A single-pass streaming algorithm that uses O\u0303(k) memory and achieves an approximation guarantee of 0.3178. - A multi-pass streaming algorithm that uses O\u0303(k) memory and achieves an approximation guarantee of (1-1/e - \u03b5) by taking a constant (depending on \u03b5) number of passes over the stream. This improves on the previously best approximation guarantees of 1/4 and 1/2 for single-pass and multi-pass streaming algorithms, respectively. In fact, our multi-pass streaming algorithm is tight in that any algorithm with a better guarantee than 1/2 must make several passes through the stream and any algorithm that beats our guarantee of 1-1/e must make linearly many passes (as well as an exponential number of value oracle queries). Moreover, we show how the approach we use for multi-pass streaming can be further strengthened if the elements of the stream arrive in uniformly random order, implying an improved result for p-matchoid constraints."}}
{"id": "bhdntUKwA1", "cdate": 1621630173458, "mdate": null, "content": {"title": "Parallel and Efficient Hierarchical k-Median Clustering", "abstract": "As a fundamental unsupervised learning task, hierarchical clustering has been extensively studied in the past decade. In particular, standard metric formulations as hierarchical $k$-center, $k$-means,  and $k$-median received a lot of attention and the problems have been studied extensively in different models of computation. Despite all this interest, not many efficient parallel algorithms are known for these problems. In this paper we introduce a new parallel algorithm for the Euclidean hierarchical $k$-median problem that, when using machines with memory $s$ (for $s\\in \\Omega(\\log^2 (n+\\Delta+d))$), outputs a hierarchical clustering such that for every fixed value of $k$ the cost of the solution is at most an $O(\\min\\{d, \\log n\\} \\log \\Delta)$ factor larger in expectation than that of an optimal solution. Furthermore, we also get that for all $k$ simultanuously the cost of the solution is at most an $O(\\min\\{d, \\log n\\} \\log \\Delta \\log (\\Delta d n))$ factor bigger that the corresponding optimal solution.  The algorithm requires in $O\\left(\\log_{s} (nd\\log(n+\\Delta))\\right)$ rounds. Here $d$ is the dimension of the data set and $\\Delta$ is  the ratio between the maximum and minimum distance of two points in the input dataset. To the best of our knowledge, this is the first \\emph{parallel} algorithm  for the hierarchical $k$-median problem with theoretical guarantees. We further complement our theoretical results with an empirical study of our algorithm that shows its effectiveness in practice."}}
