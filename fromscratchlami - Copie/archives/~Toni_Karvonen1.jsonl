{"id": "cpq2S-HiFE", "cdate": 1640995200000, "mdate": 1681710340163, "content": {"title": "Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Regression", "abstract": "It is common to model a deterministic response function, such as the output of a computer experiment, as a Gaussian process with a Mat\\'ern covariance kernel. The smoothness parameter of a Mat\\'ern kernel determines many important properties of the model in the large data limit, including the rate of convergence of the conditional mean to the response function. We prove that the maximum likelihood estimate of the smoothness parameter cannot asymptotically undersmooth the truth when the data are obtained on a fixed bounded subset of $\\mathbb{R}^d$. That is, if the data-generating response function has Sobolev smoothness $\\nu_0 > d/2$, then the smoothness parameter estimate cannot be asymptotically less than $\\nu_0$. The lower bound is sharp. Additionally, we show that maximum likelihood estimation recovers the true smoothness for a class of compactly supported self-similar functions. For cross-validation we prove an asymptotic lower bound $\\nu_0 - d/2$, which however is unlikely to be sharp. The results are based on approximation theory in Sobolev spaces and some general theorems that restrict the set of values that the parameter estimators can take."}}
{"id": "XER-f0OJjK", "cdate": 1640995200000, "mdate": 1681710340194, "content": {"title": "Approximation in Hilbert spaces of the Gaussian and other weighted power series kernels", "abstract": "This article considers linear approximation based on function evaluations in reproducing kernel Hilbert spaces of the Gaussian kernel and a more general class of weighted power series kernels on the interval $[-1, 1]$. We derive almost matching upper and lower bounds on the worst-case error, measured both in the uniform and $L^2([-1,1])$-norm, in these spaces. The results show that if the power series kernel expansion coefficients $\\alpha_n^{-1}$ decay at least factorially, their rate of decay controls that of the worst-case error. Specifically, (i) the $n$th minimal error decays as $\\alpha_n^{{ -1/2}}$ up to a sub-exponential factor and (ii) for any $n$ sampling points in $[-1, 1]$ there exists a linear algorithm whose error is $\\alpha_n^{{ -1/2}}$ up to an exponential factor. For the Gaussian kernel the dominating factor in the bounds is $(n!)^{-1/2}$."}}
{"id": "WpPY-rcYCI", "cdate": 1640995200000, "mdate": 1681710340299, "content": {"title": "Orthonormal Expansions for Translation-Invariant Kernels", "abstract": "We present a general Fourier analytic technique for constructing orthonormal basis expansions of translation-invariant kernels from orthonormal bases of $\\mathscr{L}_2(\\mathbb{R})$. This allows us to derive explicit expansions on the real line for (i) Mat\\'ern kernels of all half-integer orders in terms of associated Laguerre functions, (ii) the Cauchy kernel in terms of rational functions, and (iii) the Gaussian kernel in terms of Hermite functions."}}
{"id": "WGuU-9UsvdL", "cdate": 1640995200000, "mdate": 1681673666285, "content": {"title": "Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed", "abstract": "Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed, that is, when the predictions of the regression model are insensitive to small perturbations of the data. This article identifies scenarios where the maximum likelihood estimator fails to be well-posed, in that the predictive distributions are not Lipschitz in the data with respect to the Hellinger distance. These failure cases occur in the noiseless data setting, for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is part of Gaussian process folklore, these rigorous theoretical results appear to be the first of their kind. The implication of these negative results is that well-posedness may need to be assessed post-hoc, on a case-by-case basis, when maximum likelihood estimation is used to train a Gaussian process model."}}
{"id": "H1c6bKxWSp0", "cdate": 1640995200000, "mdate": 1681710340178, "content": {"title": "Error analysis for a statistical finite element method", "abstract": "The recently proposed statistical finite element (statFEM) approach synthesises measurement data with finite element models and allows for making predictions about the true system response. We provide a probabilistic error analysis for a prototypical statFEM setup based on a Gaussian process prior under the assumption that the noisy measurement data are generated by a deterministic true system response function that satisfies a second-order elliptic partial differential equation for an unknown true source term. In certain cases, properties such as the smoothness of the source term may be misspecified by the Gaussian process model. The error estimates we derive are for the expectation with respect to the measurement noise of the $L^2$-norm of the difference between the true system response and the mean of the statFEM posterior. The estimates imply polynomial rates of convergence in the numbers of measurement points and finite element basis functions and depend on the Sobolev smoothness of the true source term and the Gaussian process model. A numerical example for Poisson's equation is used to illustrate these theoretical results."}}
{"id": "FackmHUDcXX", "cdate": 1621629773823, "mdate": null, "content": {"title": "Black Box Probabilistic Numerics", "abstract": "Probabilistic numerics casts numerical tasks, such the numerical solution of differential equations, as inference problems to be solved. One approach is to model the unknown quantity of interest as a random variable, and to constrain this variable using data generated during the course of a traditional numerical method. However, data may be nonlinearly related to the quantity of interest, rendering the proper conditioning of random variables difficult and limiting the range of numerical tasks that can be addressed. Instead, this paper proposes to construct probabilistic numerical methods based only on the final output from a traditional method. A convergent sequence of approximations to the quantity of interest constitute a dataset, from which the limiting quantity of interest can be extrapolated, in a probabilistic analogue of Richardson\u2019s deferred approach to the limit. This black box approach (1) massively expands the range of tasks to which probabilistic numerics can be applied, (2) inherits the features and performance of state-of-the-art numerical methods, and (3) enables provably higher orders of convergence to be achieved. Applications are presented for nonlinear ordinary and partial differential equations, as well as for eigenvalue problems\u2014a setting for which no probabilistic numerical methods have yet been developed."}}
{"id": "owjJWfci-z", "cdate": 1609459200000, "mdate": 1681710340201, "content": {"title": "Kernel-based interpolation at approximate Fekete points", "abstract": "We construct approximate Fekete point sets for kernel-based interpolation by maximising the determinant of a kernel Gram matrix obtained via truncation of an orthonormal expansion of the kernel. Uniform error estimates are proved for kernel interpolants at the resulting points. If the kernel is Gaussian, we show that the approximate Fekete points in one dimension are the solution to a convex optimisation problem and that the interpolants converge with a super-exponential rate. Numerical examples are provided for the Gaussian kernel."}}
{"id": "ncX-xE3asP8", "cdate": 1609459200000, "mdate": 1681673666228, "content": {"title": "Improved Calibration of Numerical Integration Error in Sigma-Point Filters", "abstract": "The sigma-point filters, such as the unscented Kalman filter, are popular alternatives to the ubiquitous extended Kalman filter. The classical quadrature rules used in the sigma-point filters are motivated via polynomial approximation of the integrand; however, in the applied context, these assumptions cannot always be justified. As a result, a quadrature error can introduce bias into estimated moments, for which there is no compensatory mechanism in the classical sigma-point filters. This can lead in turn to estimates and predictions that are poorly calibrated. In this article, we investigate the Bayes-Sard quadrature method in the context of sigma-point filters, which enables uncertainty due to quadrature error to be formalized within a probabilistic model. Our first contribution is to derive the well-known classical quadratures as special cases of the Bayes-Sard quadrature method. Based on this, a general-purpose moment transform is developed and utilized in the design of a novel sigma-point filter, which explicitly accounts for the additional uncertainty due to quadrature error."}}
{"id": "b_5V4rfYOXa", "cdate": 1609459200000, "mdate": 1681710340337, "content": {"title": "Gaussian Approximations of SDES in Metropolis-Adjusted Langevin Algorithms", "abstract": "Markov chain Monte Carlo (MCMC) methods are a cornerstone of Bayesian inference and stochastic simulation. The Metropolis-adjusted Langevin algorithm (MALA) is an MCMC method that relies on the simulation of a stochastic differential equation (SDE) whose stationary distribution is the desired target density using the Euler-Maruyama algorithm and accounts for simulation errors using a Metropolis step. In this paper we propose a modification of the MALA which uses Gaussian assumed density approximations for the integration of the SDE. The effectiveness of the algorithm is illustrated on simulated and real data sets."}}
{"id": "Xptr_5TPIbQ", "cdate": 1609459200000, "mdate": 1681710340193, "content": {"title": "Taylor Moment Expansion for Continuous-Discrete Gaussian Filtering", "abstract": "This article is concerned with Gaussian filtering in nonlinear continuous-discrete state-space models. We propose a novel Taylor moment expansion (TME) Gaussian filter, which approximates the moments of the stochastic differential equation with a temporal Taylor expansion. Differently from classical linearization or It\u00f4-Taylor approaches, the Taylor expansion is formed for the moment functions directly and in time variable, not by using a Taylor expansion on the nonlinear functions in the model. We analyze the theoretical properties, including the positive definiteness of the covariance estimate and stability of the TME filter. By numerical experiments, we demonstrate that the proposed TME Gaussian filter significantly outperforms the state-of-the-art methods in terms of estimation accuracy and numerical stability."}}
