{"id": "Gt_GiNkBhu", "cdate": 1676827084547, "mdate": null, "content": {"title": "Learning To Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning", "abstract": "Gradient inversion attack enables recovery of training samples from model gradients in federated learning (FL), and constitutes a serious threat to data privacy. To mitigate this vulnerability, prior work proposed both principled defenses based on differential privacy, as well as heuristic defenses based on gradient compression as countermeasures. These defenses have so far been very effective, in particular those based on gradient compression that allow the model to maintain high accuracy while greatly reducing the effectiveness of attacks. In this work, we argue that such findings underestimate the privacy risk in FL. As a counterexample, we show that existing defenses can be broken by a simple adaptive attack, where a model trained on auxiliary data is able to invert gradients on both vision and language tasks."}}
{"id": "lKjWqBWZfM_", "cdate": 1675347811959, "mdate": 1675347811959, "content": {"title": "Product Kernel Interpolation for Scalable Gaussian Processes", "abstract": "Recent work shows that inference for Gaussian processes can be performed efficiently using iterative methods that rely only on matrix-vector multiplications (MVMs). Structured Kernel Interpolation (SKI) exploits these techniques by deriving approximate kernels with very fast MVMs. Unfortunately, such strategies suffer badly from the curse of dimensionality. We develop a new technique for MVM based learning that exploits product kernel structure. We demonstrate that this technique is broadly applicable, resulting in linear rather than exponential runtime with dimension for SKI, as well as state-of-the-art asymptotic complexity for multi-task GPs."}}
{"id": "deit1AdsFU", "cdate": 1663850266962, "mdate": null, "content": {"title": "Learning To Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning", "abstract": "Gradient inversion attack enables recovery of training samples from model updates in federated learning (FL) and constitutes a serious threat to data privacy. To mitigate this vulnerability, prior work proposed both principled defenses based on differential privacy, as well as heuristic defenses based on gradient compression as countermeasures. These defenses have so far been very effective, in particular those based on gradient compression that allow the model to maintain high accuracy while greatly reducing the attack's effectiveness. In this work, we argue that such findings do not accurately reflect the privacy risk in FL, and show that existing defenses can be broken by a simple adaptive attack that trains a model using auxiliary data to learn how to invert gradients on both vision and language tasks."}}
{"id": "SAlemvIoql9", "cdate": 1646077530528, "mdate": null, "content": {"title": "Differentially Private Multi-Party Data Release for Linear Regression", "abstract": "Differentially Private (DP) data release is a promising technique to disseminate data without compromising the privacy of data subjects. However the majority of prior work has focused on scenarios where a single party owns all the data. In this paper we focus on the multi-party setting, where different stakeholders own disjoint sets of attributes belonging to the same group of data subjects. Within the context of linear regression that allow all parties to train models on the complete data without the ability to infer private attributes or identities of individuals, we start with directly applying Gaussian mechanism and show it has the small eigenvalue problem. We further propose our novel method and prove it asymptotically converges to the optimal (non-private) solutions with increasing dataset size. We substantiate the theoretical results through experiments on both artificial and real-world datasets.  "}}
{"id": "HlfST67Mmc", "cdate": 1640995200000, "mdate": 1648668093443, "content": {"title": "Does Label Differential Privacy Prevent Label Inference Attacks?", "abstract": "Label differential privacy (LDP) is a popular framework for training private ML models on datasets with public features and sensitive private labels. Despite its rigorous privacy guarantee, it has been observed that in practice LDP does not preclude label inference attacks (LIAs): Models trained with LDP can be evaluated on the public training features to recover, with high accuracy, the very private labels that it was designed to protect. In this work, we argue that this phenomenon is not paradoxical and that LDP merely limits the advantage of an LIA adversary compared to predicting training labels using the Bayes classifier. At LDP $\\epsilon=0$ this advantage is zero, hence the optimal attack is to predict according to the Bayes classifier and is independent of the training labels. Finally, we empirically demonstrate that our result closely captures the behavior of simulated attacks on both synthetic and real world datasets."}}
{"id": "xZvuqfT6Otj", "cdate": 1621630028588, "mdate": null, "content": {"title": "Fixes That Fail: Self-Defeating Improvements in Machine-Learning Systems", "abstract": "Machine-learning systems such as self-driving cars or virtual assistants are composed of a large number of machine-learning models that recognize image content, transcribe speech, analyze natural language, infer preferences, rank options, etc. Models in these systems are often developed and trained independently, which raises an obvious concern: Can improving a machine-learning model make the overall system worse? We answer this question affirmatively by showing that improving a model can deteriorate the performance of downstream models, even after those downstream models are retrained. Such self-defeating improvements are the result of entanglement between the models in the system. We perform an error decomposition of systems with multiple machine-learning models, which sheds light on the types of errors that can lead to self-defeating improvements. We also present the results of experiments which show that self-defeating improvements emerge in a realistic stereo-based detection system for cars and pedestrians."}}
{"id": "j7YA-y0P3-", "cdate": 1621629865872, "mdate": null, "content": {"title": "Online Adaptation to Label Distribution Shift", "abstract": "Machine learning models often encounter distribution shifts when deployed in the real world.  In this paper, we focus on adaptation to label distribution shift in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. This setting is common in many real world scenarios such as medical diagnosis, where disease prevalences can vary substantially at different times of the year. Leveraging a novel analysis, we show that the lack of true label does not hinder estimation of the expected test loss, which enables the reduction of online label shift adaptation to conventional online learning. Informed by this observation, we propose adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. We empirically verify our findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of challenging label shift scenarios."}}
{"id": "6h14cMLgb5q", "cdate": 1621629865872, "mdate": null, "content": {"title": "Online Adaptation to Label Distribution Shift", "abstract": "Machine learning models often encounter distribution shifts when deployed in the real world.  In this paper, we focus on adaptation to label distribution shift in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. This setting is common in many real world scenarios such as medical diagnosis, where disease prevalences can vary substantially at different times of the year. Leveraging a novel analysis, we show that the lack of true label does not hinder estimation of the expected test loss, which enables the reduction of online label shift adaptation to conventional online learning. Informed by this observation, we propose adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. We empirically verify our findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of challenging label shift scenarios."}}
{"id": "S_bWHp6XfQ5", "cdate": 1609459200000, "mdate": 1648668093443, "content": {"title": "Fixes That Fail: Self-Defeating Improvements in Machine-Learning Systems", "abstract": "Machine-learning systems such as self-driving cars or virtual assistants are composed of a large number of machine-learning models that recognize image content, transcribe speech, analyze natural language, infer preferences, rank options, etc. Models in these systems are often developed and trained independently, which raises an obvious concern: Can improving a machine-learning model make the overall system worse? We answer this question affirmatively by showing that improving a model can deteriorate the performance of downstream models, even after those downstream models are retrained. Such self-defeating improvements are the result of entanglement between the models in the system. We perform an error decomposition of systems with multiple machine-learning models, which sheds light on the types of errors that can lead to self-defeating improvements. We also present the results of experiments which show that self-defeating improvements emerge in a realistic stereo-based detection system for cars and pedestrians."}}
{"id": "SMGSpaQMQc", "cdate": 1609459200000, "mdate": 1648668093442, "content": {"title": "Spatial-Temporal Graph Neural Network For Interaction-Aware Vehicle Trajectory Prediction", "abstract": "In this paper, a Spatial Temporal Graph Neural Network (STGNN) model is developed, including a temporal block and Graph Neural Network (GNN) block, to solve the problem of vehicle trajectory prediction in unstructured scenes. Specifically, a temporal block combines a recurrent neural network and non-local operation to extract the features from past trajectories, and a GNN block models the subtle interactions between vehicles. The proposed model is evaluated on two datasets: Unstructured Scene Dataset and Argoverse Dataset. Experiment results show that the STGNN model achieves a better performance in the unstructured scenes and can be applied to common scenes where rules of the road dominate."}}
