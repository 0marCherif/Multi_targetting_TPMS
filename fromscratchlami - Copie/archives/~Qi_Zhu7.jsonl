{"id": "zsmvAxscZm", "cdate": 1696511596465, "mdate": 1696511596465, "content": {"title": "Patton: Language Model Pretraining on Text-rich Networks", "abstract": "A real-world text corpus sometimes comprises not only text documents but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships). Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton. Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently."}}
{"id": "meLzymorEz", "cdate": 1674892659688, "mdate": 1674892659688, "content": {"title": "Collective Multi-type Entity Alignment Between Knowledge Graphs", "abstract": "Knowledge graph (e.g. Freebase, YAGO) is a multi-relational graph\nrepresenting rich factual information among entities of various\ntypes. Entity alignment is the key step towards knowledge graph\nintegration from multiple sources. It aims to identify entities across\ndifferent knowledge graphs that refer to the same real world entity.\nHowever, current entity alignment systems overlook the sparsity\nof different knowledge graphs and can not align multi-type entities by one single model. In this paper, we present a Collective\nGraph neural network for Multi-type entity Alignment, called CGMuAlign. Different from previous work, CG-MuAlign jointly aligns\nmultiple types of entities, collectively leverages the neighborhood\ninformation and generalizes to unlabeled entity types. Specifically,\nwe propose novel collective aggregation function tailored for this\ntask, that (1) relieves the incompleteness of knowledge graphs via\nboth cross-graph and self attentions, (2) scales up efficiently with\nmini-batch training paradigm and effective neighborhood sampling\nstrategy. We conduct experiments on real world knowledge graphs\nwith millions of entities and observe the superior performance\nbeyond existing methods. In addition, the running time of our approach is much less than the current state-of-the-art deep learning\nmethods."}}
{"id": "CXm7uzRlvxf", "cdate": 1664046168265, "mdate": null, "content": {"title": "Shift-Robust Node Classification via Graph Clustering Co-training", "abstract": "It is widely known that machine learning models only achieve sub-optimal performance when testing data exhibit distribution shift against training \\ie, $\\Pr_\\text{train}(X,Y) \\neq \\Pr_\\text{test}(X,Y)$. Although Graph Neural Networks (GNNs) have become de facto models for semi-supervised learning tasks, they suffer even more from distribution shift because multiple types of shifts origin from not only node features but graph structures. Existing domain adaptation methods only work for specific type of shifts. In response, we propose Shift-Robust Node Classification (SRNC) - a unified domain adaptation framework for different kinds of distribution shifts on graph. Specifically, we co-train an unsupervised cluster GNN, which captures the data distribution by graph homophily on target graph. Then a shift-robust classifier is optimized on training graph and pseudo samples from target graph, which are provided by cluster GNN. Compared to the existing domain adaptation algorithms on graph, our approach works for both open-set and close-set shifts with convergence guarantees.\nIn our experiments, the classification accuracy is improved at least $3\\%$ against the second-best baseline under open-set shifts. On time-evolving graph with close-set shift, existing domain adaption algorithms can barely improve the generalization if not worse. SRNC is still able to mitigate the negative effect ($>2\\%$  absolute improvements) of the shift across different testing-times."}}
{"id": "nGyWzq-703u", "cdate": 1663850505390, "mdate": null, "content": {"title": "GNN Domain Adaptation using Optimal Transport", "abstract": "While Graph Convolutional Networks (GCNs) have recently grown in popularity due to their excellent performance on graph data, their performance under domain shift has not been studied extensively. In this work, we first explore the ability of GCNs to generalize to out-of-distribution data using contextual stochastic block models (CSBMs) on the node classification task.  Our results in this area provide the first generalization criteria for GCNs on feature distribution and structure changes. Next we examine a popular Unsupervised Domain Adaptation (UDA) covariate shift assumption and demonstrate that it rarely holds for graph data. Motivated by these results, we propose addressing bias in graph models using domain adaptation with optimal transport - GDOT which features a transportation plan that minimizes the cost of the joint feature and estimated label distribution $P(X,\\hat{Y})$ between source and target domains. Additionally, we demonstrate that such transportation cost metric serves as a good proxy for estimating transferability between source and target graphs, and is better as a transferability metric than other common metrics like maximum mean discrepancy (MMD). In our controlled CSBM experiments, GDOT demonstrates robustness towards distributional shift, resulting in 90\\% ROC AUC (vs.\\ the second-best algorithm achieving $<80$\\% on feature shift). Comprehensive experiments on both semi-supervised and supervised real-world node classification problems show that our method is the only one that performs consistently better than baseline GNNs in the cross-domain adaptation setting."}}
{"id": "XE0cIoi-sZ1", "cdate": 1663850104726, "mdate": null, "content": {"title": "Can Single-Pass Contrastive Learning Work for Both Homophilic and Heterophilic Graph?", "abstract": "Existing graph contrastive learning (GCL) typically requires two forward pass for a single instance to construct the contrastive loss. Despite its remarkable success, it is unclear whether such a dual-pass design is (theoretically) necessary. Besides, the empirical results are hitherto limited to the homophilic graph benchmarks. Then a natural question arises: Can we design a method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this, we theoretically analyze the concentration property of features obtained by neighborhood aggregation on both homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss based on the property, and provide performance guarantees of the minimizer of the loss on downstream tasks. As a direct consequence of our theory, we introduce the Single-Pass Graph Contrastive Learning method (SP-GCL). Empirically, on 14 benchmark datasets with varying\ndegrees of heterophily, the features learned by the SP-GCL can match or outperform existing strong baselines with significantly less computational overhead, and empirical results show the feasibility of conclusions derived by our analysis in real-world cases."}}
{"id": "uY-XMIbyXec", "cdate": 1621630333995, "mdate": null, "content": {"title": "Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training data", "abstract": "There has been a recent surge of interest in designing Graph Neural Networks (GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed that the nodes labeled for use in training were selected uniformly at random (i.e. are an IID sample). However in many real world scenarios gathering labels for graph nodes is both expensive and inherently biased -- so this assumption can not be met. GNNs can suffer poor generalization when this occurs, by overfitting to superfluous regularities present in the training data. In this work we present a method, Shift-Robust GNN (SR-GNN), designed to account for distributional differences between biased training data and the graph's true inference distribution. SR-GNN adapts GNN models for the presence of distributional shifts between the nodes which have had labels provided for training and the rest of the dataset. We illustrate the effectiveness of SR-GNN in a variety of experiments with biased training datasets on common GNN benchmark datasets for semi-supervised learning, where we see that SR-GNN outperforms other GNN baselines by accuracy, eliminating at least (~40%) of the negative effects introduced by biased training data. On the largest dataset we consider, ogb-arxiv, we observe an 2% absolute improvement over the baseline and reduce 30% of the negative effects."}}
{"id": "CzVPfeqPOBu", "cdate": 1621630114502, "mdate": null, "content": {"title": "Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization", "abstract": "Graph neural networks (GNNs) have achieved superior performance in various applications, but training dedicated GNNs can be costly for large-scale graphs. Some recent work started to study the pre-training of GNNs. However, none of them provide theoretical insights into the design of their frameworks, or clear requirements and guarantees towards their transferability. In this work, we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs. Firstly, we propose a novel view towards the essential graph information and advocate the capturing of it as the goal of transferable GNN training, which motivates the design of EGI (Ego-Graph Information maximization) to analytically achieve this goal. Secondly,\nwhen node features are structure-relevant, we conduct an analysis of EGI transferability regarding the difference between the local graph Laplacians of the source and target graphs. We conduct controlled synthetic experiments to directly justify our theoretical conclusions. Comprehensive experiments on two real-world network datasets show consistent results in the analyzed setting of direct-transfering, while those on large-scale knowledge graphs show promising results in the more practical setting of transfering with fine-tuning."}}
{"id": "J_pvI6ap5Mn", "cdate": 1601308099164, "mdate": null, "content": {"title": "Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization", "abstract": "Graph neural networks (GNNs) have been shown with superior performance in various applications, but training dedicated GNNs can be costly for large-scale graphs. Some recent work started to study the pre-training of GNNs. However, none of them provide theoretical insights into the design of their frameworks, or clear requirements and guarantees towards the transferability of GNNs. In this work, we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs. Firstly, we propose a novel view towards the essential graph information and advocate the capturing of it as the goal of transferable GNN training, which motivates the design of EGI (ego-graph information maximization) to analytically achieve this goal. Secondly, we specify the requirement of structure-respecting node features as the GNN input, and conduct a rigorous analysis of GNN transferability based on the difference between the local graph Laplacians of the source and target graphs. Finally, we conduct controlled synthetic experiments to directly justify our theoretical conclusions. Extensive experiments on real-world networks towards role identification show consistent results in the rigorously analyzed setting of direct-transfering (freezing parameters), while those towards large-scale relation prediction show promising results in the more generalized and practical setting of transfering with fine-tuning."}}
