{"id": "eEhlmy-6FD", "cdate": 1698796800000, "mdate": 1699803910336, "content": {"title": "TCSD: Triple Complementary Streams Detector for Comprehensive Deepfake Detection", "abstract": "Advancements in computer vision and deep learning have made it difficult to distinguish deepfake visual media. While existing detection frameworks have achieved significant performance on challenging deepfake datasets, these approaches consider only a single perspective. More importantly, in urban scenes, neither complex scenarios can be covered by a single view nor can the correlation between multiple datasets of information be well utilized. In this article, to mine the new view for deepfake detection and utilize the correlation of multi-view information contained in images, we propose a novel triple complementary streams detector (TCSD). First, a novel depth estimator is designed to extract depth information (DI), which has not been used in previous methods. Then, to supplement depth information for obtaining comprehensive forgery clues, we consider the incoherence between image foreground and background information (FBI) and the inconsistency between local and global information (LGI). In addition, we designed an attention-based multi-scale feature extraction (MsFE) module to extract more complementary features from DI, FBI, and LGI. Finally, two attention-based feature fusion modules are proposed to adaptively fuse information. Extensive experiment results show that the proposed approach achieves state-of-the-art performance on detecting deepfakes."}}
{"id": "H7ZDNQeJerW", "cdate": 1693526400000, "mdate": 1699803910149, "content": {"title": "LQGDNet: A Local Quaternion and Global Deep Network for Facial Depression Recognition", "abstract": "Recent visual-based depression recognition methods mostly use hand-crafted features with information lost in color channels, or deep network features with a limited performance from the finite data. In this paper, we propose a method called Local Quaternion and Global Deep Network (LQGDNet) which can combine advantages from hand-crafted and deep features. Specifically, the Quaternion XOR Asymmetrical Regional Local Gradient Coding (XOR-AR-LGC) is first designed, which encodes the facial images with local textures in the quaternion domain to keep the dependence of color channels, and integrated into the Quaternion Feature Extractor (QFE). To the best of our knowledge, it is the first attempt to use a quaternion-based method for facial depression recognition. Second, we design the Local Quaternion Representation Module (LQRM) composed of Local Deep Feature Extractor (LDFE) and QFE to output local quaternion facial features. Third, global deep facial features are encoded from the Global Deep Representation Module (GDRM) with the deep convolutional neural network. Finally, the LQGDNet integrates LQRM and GDRM with the local quaternion and global deep features and predicts the depression score. The experimental results on AVEC 2013 and AVEC 2014 show the superiority of our method compared to the state-of-the-art approaches."}}
{"id": "HAh-YuQ0qJm", "cdate": 1685326330224, "mdate": 1685326330224, "content": {"title": "Cogradient descent for bilinear optimization", "abstract": "Conventional learning methods simplify the bilinear model by regarding two intrinsically coupled factors independently, which degrades the optimization procedure. One reason lies in the insufficient training due to the asynchronous gradient descent, which results in vanishing gradients for the coupled variables. In this paper, we introduce a Cogradient Descent algorithm (CoGD) to address the bilinear problem, based on a theoretical framework to coordinate the gradient of hidden variables via a projection function. We solve one variable by considering its coupling relationship with the other, leading to a synchronous gradient descent to facilitate the optimization procedure. Our algorithm is applied to solve problems with one variable under the sparsity constraint, which is widely used in the learning paradigm. We validate our CoGD considering an extensive set of applications including image reconstruction, inpainting, and network pruning. Experiments show that it improves the state-of-the-art by a significant margin."}}
{"id": "pniNWTfD-Kt", "cdate": 1680307200000, "mdate": 1699803910154, "content": {"title": "Exploring attribute localization and correlation for pedestrian attribute recognition", "abstract": ""}}
{"id": "U27_2Q14EC", "cdate": 1680307200000, "mdate": 1699803910150, "content": {"title": "Scene Text Detection Based on Multi-Dimensional Feature Fusion with Instance-Wise Loss", "abstract": "Over the past few years, scene text detection has witnessed rapid progress due to the development of deep neural networks. However, segmentation-based methods may fail to detect pixels near boundar..."}}
{"id": "jLTz99_7eZU", "cdate": 1675209600000, "mdate": 1699803910336, "content": {"title": "A Novel Reversible Data Hiding Scheme Based on Pixel-Residual Histogram", "abstract": "Prediction-error expansion (PEE) is the most popular reversible data hiding (RDH) technique due to its efficient capacity-distortion tradeoff. With the generated prediction-error histogram (PEH) and adaptively selected expansion bins, the image redundancy is well exploited by PEE. However, for the most widely used rhombus predictor, the rounding operation which groups different prediction-errors into one value is completely unnecessary. The embedding can be extended to a general case by removing the rounding operation, and more histogram bins can be derived for expansion with a new mapping mechanism. Therefore, in this article, instead of pixel prediction-error, we propose to compute the pixel residuals without the rounding operation, and a new embedding mechanism based on pixel-residual histogram (PRH) modification is devised. In PRH, four bins correspond to one bin in PEH. Then, different from the one-to-one mapping between the prediction-error and pixel modification, a four-to-one mapping between the pixel-residual and pixel modification is established, and the performance is optimized by adaptively selecting four expansion bin pairs for embedding. Since more modification selections are considered, better performance can be obtained. Moreover, the proposed scheme is extended to the two-dimensional (2D) histogram and multiple histograms based embedding, and the performance is further enhanced. The superiority of the proposed method is experimentally verified by comparing it with some state-of-the-art works."}}
{"id": "vjF1veO-Gy4", "cdate": 1672531200000, "mdate": 1699803910463, "content": {"title": "Defending Black-Box Skeleton-Based Human Activity Classifiers", "abstract": "Skeletal motions have been heavily relied upon for human activity recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR has been identified across a variety of classifiers and data, calling for mitigation. To this end, we propose the first black-box defense method for skeleton-based HAR to our best knowledge. Our method is featured by full Bayesian treatments of the clean data, the adversaries and the classifier, leading to (1) a new Bayesian Energy-based formulation of robust discriminative classifiers, (2) a new adversary sampling scheme based on natural motion manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is straightforward but elegant, which turns vulnerable black-box classifiers into robust ones without sacrificing accuracy. It demonstrates surprising and universal effectiveness across a wide range of skeletal HAR classifiers and datasets, under various attacks. Appendix and code are available."}}
{"id": "vbxOOjiZxr4", "cdate": 1672531200000, "mdate": 1699803910335, "content": {"title": "FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing", "abstract": "The availability of handy multi-modal (i.e., RGB-D) sensors has brought about a surge of face anti-spoofing research. However, the current multi-modal face presentation attack detection (PAD) has two defects: (1) The framework based on multi-modal fusion requires providing modalities consistent with the training input, which seriously limits the deployment scenario. (2) The performance of ConvNet-based model on high fidelity datasets is increasingly limited. In this work, we present a pure transformer-based framework, dubbed the Flexible Modal Vision Transformer (FM-ViT), for face anti-spoofing to flexibly target any single-modal (i.e., RGB) attack scenarios with the help of available multi-modal data. Specifically, FM-ViT retains a specific branch for each modality to capture different modal information and introduces the Cross-Modal Transformer Block (CMTB), which consists of two cascaded attentions named Multi-headed Mutual-Attention (MMA) and Fusion-Attention (MFA) to guide each modal branch to mine potential features from informative patch tokens, and to learn modality-agnostic liveness features by enriching the modal information of own <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CLS</monospace> token, respectively. Experiments demonstrate that the single model trained based on FM-ViT can not only flexibly evaluate different modal samples, but also outperforms existing single-modal frameworks by a large margin, and approaches the multi-modal frameworks introduced with smaller FLOPs and model parameters."}}
{"id": "vMwwiJadMhV", "cdate": 1672531200000, "mdate": 1699803910901, "content": {"title": "DCP-NAS: Discrepant Child-Parent Neural Architecture Search for 1-bit CNNs", "abstract": "Neural architecture search (NAS) proves to be among the effective approaches for many tasks by generating an application-adaptive neural architecture, which is still challenged by high computational cost and memory consumption. At the same time, 1-bit convolutional neural networks (CNNs) with binary weights and activations show their potential for resource-limited embedded devices. One natural approach is to use 1-bit CNNs to reduce the computation and memory cost of NAS by taking advantage of the strengths of each in a unified framework, while searching the 1-bit CNNs is more challenging due to the more complicated processes involved. In this paper, we introduce Discrepant Child-Parent Neural Architecture Search (DCP-NAS) to efficiently search 1-bit CNNs, based on a new framework of searching the 1-bit model (Child) under the supervision of a real-valued model (Parent). Particularly, we first utilize a Parent model to calculate a tangent direction, based on which the tangent propagation method is introduced to search the optimized 1-bit Child. We further observe a coupling relationship between the weights and architecture parameters existing in such differentiable frameworks. To address the issue, we propose a decoupled optimization method to search an optimized architecture. Extensive experiments demonstrate that our DCP-NAS achieves much better results than prior arts on both CIFAR-10 and ImageNet datasets. In particular, the backbones achieved by our DCP-NAS achieve strong generalization performance on person re-identification and object detection."}}
{"id": "lCYLVra5gIW", "cdate": 1672531200000, "mdate": 1699803910273, "content": {"title": "Divergence-Driven Consistency Training for Semi-Supervised Facial Age Estimation", "abstract": "Facial age estimation has attracted considerable attention owing to its great potential in applications. However, it still falls short of reliable age estimation due to the lack of sufficient training data with accurate age labels. Using conventional semi-supervised methods to exploit unlabeled data appears to be a good solution, but it does not yield sufficient performance gains while significantly increasing training time. Therefore, to tackle these problems, we present a Divergence-driven Consistency Training (DCT) method for enhancing both efficiency and performance in this paper. Following the idea of pseudo-labeling and consistency regularization, we assign pseudo labels predicted by the teacher model to unlabeled samples and then train the student model on labeled and unlabeled samples based on consistency regularization. Based on this, we propose two main promotions. The first is the Efficient Sample Selection (ESS) strategy, which is based on the Divergence Score to select effective samples from massive unlabeled images to reduce the training time and improve efficiency. The second is Identity Consistency (IC) regularization as the additional loss function, which introduces a high dependency of aging traits on a person. Moreover, we propose Local Prediction (LP), which is a plug-and-play component, to capture local semantics. Extensive experiments on multiple age benchmark datasets, including CACD, Morph II, MIVIA, and Chalearn LAP 2015, indicate DCT outperforms the state-of-the-art approaches significantly."}}
