{"id": "SL4SwMNnwIe", "cdate": 1652737587321, "mdate": null, "content": {"title": "Acceleration in Distributed Sparse Regression", "abstract": "We study acceleration for distributed sparse regression in   {\\it  high-dimensions},  which allows the parameter size  to exceed and grow faster than the sample size. When applicable, existing  distributed algorithms employing acceleration perform poorly  in this setting, theoretically and numerically.  We  propose a new accelerated distributed algorithm suitable for high-dimensions. The method couples  a suitable instance of accelerated Nesterov's proximal gradient  with consensus and gradient-tracking mechanisms, aiming at estimating locally the gradient of the empirical loss while enforcing agreement on the local estimates.  Under standard assumptions on the statistical model and tuning parameters, the proposed method is proved to  globally converge   at {\\it linear} rate  to an estimate that is within the {\\it statistical precision} of the model. The iteration  complexity scales as $\\mathcal{O}(\\sqrt{\\kappa})$, while the communications per iteration are at most $\\widetilde{\\mathcal{O}}(\\log m/(1-\\rho))$, \n where $\\kappa$ is the restricted condition number of the empirical loss, $m$ is the number of agents, and $\\rho\\in (0,1)$ measures the network connectivity. As by-product of our design, we also report    an accelerated method for high-dimensional estimations over  master-worker architectures, which is of independent interest and  compares favorably with existing works."}}
{"id": "QEODRZ7j3L_", "cdate": 1652737586634, "mdate": null, "content": {"title": "DGD^2: A Linearly Convergent Distributed Algorithm For High-dimensional Statistical Recovery", "abstract": "We study linear regression from data distributed over a network of agents (with no master node) under high-dimensional scaling, which allows the ambient dimension to grow faster than the sample size. We propose a novel decentralization of the projected gradient algorithm whereby agents iteratively update their local estimates by a \u201cdouble-mixing\u201d mechanism, which suitably combines averages of iterates and gradients of neighbouring nodes. Under standard assumptions on the statistical model and network connectivity, the proposed method enjoys global linear convergence up to the statistical precision of the model. This improves on guarantees of (plain) DGD algorithms, whose iteration complexity grows undesirably with the ambient dimension. Our technical contribution is a novel convergence analysis that resembles (albeit different) algorithmic stability arguments extended to high-dimensions and distributed setting, which is of independent interest."}}
{"id": "QrK0WDLVHZt", "cdate": 1652737338075, "mdate": null, "content": {"title": "Optimal Gradient Sliding and its Application to Optimal Distributed Optimization Under Similarity", "abstract": "We study structured convex  optimization problems, with additive objective   $r:=p + q$, where $r$ is ($\\mu$-strongly) convex, $q$ is $L_q$-smooth and convex, and $p$ is $L_p$-smooth, possibly nonconvex. For such a class of problems, we proposed an inexact accelerated gradient sliding method that can skip the gradient computation for one of these   components while still achieving optimal   complexity of gradient calls of $p$ and $q$, that is, $\\mathcal{O}(\\sqrt{L_p/\\mu})$ and $\\mathcal{O}(\\sqrt{L_q/\\mu})$, respectively. This result is much sharper than the classic black-box  complexity $\\mathcal{O}(\\sqrt{(L_p+L_q)/\\mu})$,   especially when  the difference between $L_p$ and $L_q$ is large. We then apply the proposed method to solve distributed optimization problems over master-worker architectures, under agents' function similarity, due to statistical data similarity or otherwise. The distributed algorithm achieves for the first time lower complexity bounds on both communication and local  gradient calls, with the former having being a long-standing open problem. Finally the method is extended to distributed saddle-problems (under function similarity) by means of solving a class of variational inequalities, achieving lower communication and computation complexity bounds."}}
{"id": "pj2x2ysaapJ", "cdate": 1640995200000, "mdate": 1652649077515, "content": {"title": "Kernel Regression Imputation in Manifolds Via Bi-Linear Modeling: The Dynamic-MRI Case", "abstract": "This paper introduces a non-parametric approximation framework for imputation-by-regression on data with missing entries. The framework, coined kernel regression imputation in manifolds (KRIM), is built on the hypothesis that features, generated by the measured data, lie close to an unknown-to-the-user smooth manifold. A reproducing kernel Hilbert space (RKHS) forms the feature space where the smooth manifold is embedded in. Aiming at concise representations, KRIM identifies a small number of \u201clandmark points\u201d to define approximating \u201clinear patches\u201d that mimic tangent spaces to smooth manifolds. This geometric information is infused into the design through a novel bi-linear model which can be easily extended to accommodate multi-kernel contributions in the non-parametric approximations. To effect imputation-by-regression, a bi-linear inverse problem is solved by an iterative algorithm with guaranteed convergence to a stationary point of a non-convex loss function. To showcase KRIM\u2019s modularity, the application of KRIM to dynamic magnetic resonance imaging (dMRI) is detailed, where reconstruction of images from severely under-sampled dMRI data is desired. Extensive numerical tests on synthetic and real dMRI data demonstrate the superior performance of KRIM over state-of-the-art approaches under several metrics and with a small computational footprint."}}
{"id": "JrWhGYwUJMm", "cdate": 1640995200000, "mdate": 1652649077514, "content": {"title": "High-Dimensional Inference over Networks: Linear Convergence and Statistical Guarantees", "abstract": "We study sparse linear regression over a network of agents, modeled as an undirected graph and no server node. The estimation of the $s$-sparse parameter is formulated as a constrained LASSO problem wherein each agent owns a subset of the $N$ total observations. We analyze the convergence rate and statistical guarantees of a distributed projected gradient tracking-based algorithm under high-dimensional scaling, allowing the ambient dimension $d$ to grow with (and possibly exceed) the sample size $N$. Our theory shows that, under standard notions of restricted strong convexity and smoothness of the loss functions, suitable conditions on the network connectivity and algorithm tuning, the distributed algorithm converges globally at a {\\it linear} rate to an estimate that is within the centralized {\\it statistical precision} of the model, $O(s\\log d/N)$. When $s\\log d/N=o(1)$, a condition necessary for statistical consistency, an $\\varepsilon$-optimal solution is attained after $\\mathcal{O}(\\kappa \\log (1/\\varepsilon))$ gradient computations and $O (\\kappa/(1-\\rho) \\log (1/\\varepsilon))$ communication rounds, where $\\kappa$ is the restricted condition number of the loss function and $\\rho$ measures the network connectivity. The computation cost matches that of the centralized projected gradient algorithm despite having data distributed; whereas the communication rounds reduce as the network connectivity improves. Overall, our study reveals interesting connections between statistical efficiency, network connectivity \\& topology, and convergence rate in high dimensions."}}
{"id": "dJcUhDVu1G", "cdate": 1621630053635, "mdate": null, "content": {"title": "Distributed Saddle-Point Problems Under Data Similarity", "abstract": "We study solution methods for (strongly-)convex-(strongly)-concave Saddle-Point Problems (SPPs) over networks of two type--master/workers (thus centralized) architectures  and  mesh (thus decentralized) networks. The local functions at each node are assumed to be \\textit{similar}, due to statistical data similarity or otherwise. We establish lower complexity bounds for a fairly general  class of algorithms solving the SPP. We show that   a given suboptimality $\\epsilon>0$ is achieved over master/workers networks in $\\Omega\\big(\\Delta\\cdot  \\delta/\\mu\\cdot \\log (1/\\varepsilon)\\big)$ rounds of communications, where $\\delta>0$ measures the degree of similarity of the local functions, $\\mu$ is their strong convexity constant, and $\\Delta$ is the diameter of the network. The lower communication complexity bound over mesh networks reads     $\\Omega\\big(1/{\\sqrt{\\rho}} \\cdot  {\\delta}/{\\mu}\\cdot\\log (1/\\varepsilon)\\big)$, where $\\rho$ is the (normalized) eigengap of the gossip matrix used for the communication between neighbouring nodes.  We then propose algorithms matching the lower bounds over either  types of networks (up to log-factors). We assess the effectiveness of the proposed algorithms on a robust regression  problem."}}
{"id": "S4lRXFd8Jsy", "cdate": 1621630053635, "mdate": null, "content": {"title": "Distributed Saddle-Point Problems Under Data Similarity", "abstract": "We study solution methods for (strongly-)convex-(strongly)-concave Saddle-Point Problems (SPPs) over networks of two type--master/workers (thus centralized) architectures  and  mesh (thus decentralized) networks. The local functions at each node are assumed to be \\textit{similar}, due to statistical data similarity or otherwise. We establish lower complexity bounds for a fairly general  class of algorithms solving the SPP. We show that   a given suboptimality $\\epsilon>0$ is achieved over master/workers networks in $\\Omega\\big(\\Delta\\cdot  \\delta/\\mu\\cdot \\log (1/\\varepsilon)\\big)$ rounds of communications, where $\\delta>0$ measures the degree of similarity of the local functions, $\\mu$ is their strong convexity constant, and $\\Delta$ is the diameter of the network. The lower communication complexity bound over mesh networks reads     $\\Omega\\big(1/{\\sqrt{\\rho}} \\cdot  {\\delta}/{\\mu}\\cdot\\log (1/\\varepsilon)\\big)$, where $\\rho$ is the (normalized) eigengap of the gossip matrix used for the communication between neighbouring nodes.  We then propose algorithms matching the lower bounds over either  types of networks (up to log-factors). We assess the effectiveness of the proposed algorithms on a robust regression  problem."}}
{"id": "rNyflMJl_OH", "cdate": 1609459200000, "mdate": 1652649078305, "content": {"title": "Newton Method over Networks is Fast up to the Statistical Precision", "abstract": "We propose a distributed cubic regularization of the Newton method for solving (constrained) empirical risk minimization problems over a network of agents, modeled as undirected graph. The algorithm employs an inexact, preconditioned Newton step at each agent's side: the gradient of the centralized loss is iteratively estimated via a gradient-tracking consensus mechanism and the Hessian is subsampled over the local data sets. No Hessian matrices are thus exchanged over the network. We derive global complexity bounds for convex and strongly convex losses. Our analysis reveals an interesting interplay between sample and iteration/communication complexity: statistically accurate solutions are achievable in roughly the same number of iterations of the centralized cubic Newton method, with a communication cost per iteration of the order of $\\widetilde{\\mathcal{O}}\\big(1/\\sqrt{1-\\rho}\\big)$, where $\\rho$ characterizes the connectivity of the network. This demonstrates a significant communication saving with respect to that of existing, statistically oblivious, distributed Newton-based methods over networks."}}
{"id": "kd44_1-30I", "cdate": 1609459200000, "mdate": 1652649078423, "content": {"title": "Distributed Algorithms for Composite Optimization: Unified Framework and Convergence Analysis", "abstract": "We study distributed composite optimization over networks: agents minimize a sum of smooth (strongly) convex functions-the agents' sum-utility-plus a nonsmooth (extended-valued) convex one. We propose a general unified algorithmic framework for such a class of problems and provide a convergence analysis leveraging the theory of operator splitting. Distinguishing features of our scheme are: (i) When each of the agent's functions is strongly convex, the algorithm converges at a linear rate, whose dependence on the agents' functions and network topology is decoupled; (ii) When the objective function is convex (but not strongly convex), similar decoupling as in (i) is established for the coefficient of the proved sublinear rate. This also reveals the role of function heterogeneity on the convergence rate. (iii) The algorithm can adjust the ratio between the number of communications and computations to achieve a rate (in terms of computations) independent on the network connectivity; and (iv) A by-product of our analysis is a tuning recommendation for several existing (non-accelerated) distributed algorithms yielding provably faster (worst-case) convergence rate for the class of problems under consideration."}}
{"id": "buD1O03hVZ_", "cdate": 1609459200000, "mdate": 1652649078354, "content": {"title": "Finite Rate Distributed Weight-Balancing and Average Consensus Over Digraphs", "abstract": "This article proposes the first distributed algorithm that solves the weight-balancing problem using only finite rate and simplex communications among nodes, compliant with the directed nature of the graph edges. It is proved that the algorithm converges to a weight-balanced solution at sublinear rate. The analysis builds upon a new metric inspired by positional system representations, which characterizes the dynamics of information exchange over the network, and on a novel step-size rule. Building on this result, a novel distributed algorithm is proposed that solves the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">average</i> consensus problem over digraphs, using, at each timeslot, finite rate simplex communications between adjacent nodes\u2014some bits for the weight-balancing problem and others for the average consensus. Convergence of the proposed quantized consensus algorithm to the average of the node's <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">unquantized</i> initial values is established, both almost surely and in the moment generating function of the error; and a sublinear convergence rate is proved for sufficiently large step-sizes. Numerical results validate our theoretical findings."}}
