{"id": "s_2mY4TV-H1", "cdate": 1672531200000, "mdate": 1695982658921, "content": {"title": "Bounding Evidence and Estimating Log-Likelihood in VAE", "abstract": "Many crucial problems in deep learning and statistical inference are caused by a variational gap, i.e., a difference between model evidence (log-likelihood) and evidence lower bound (ELBO). In part..."}}
{"id": "xcfkXba5Zt", "cdate": 1640995200000, "mdate": 1681658137400, "content": {"title": "Target layer regularization for continual learning using Cramer-Wold distance", "abstract": ""}}
{"id": "hxA53wtPmo", "cdate": 1640995200000, "mdate": 1681658137342, "content": {"title": "Generative models with kernel distance in data space", "abstract": ""}}
{"id": "gx6qkpKP7sF", "cdate": 1640995200000, "mdate": 1681658137557, "content": {"title": "Batch Size Reconstruction-Distribution Trade-Off In Kernel Based Generative Autoencoders", "abstract": "Most autoencoder-based generative machine learning models use a two-factor cost function composed of reconstruction error and prior distribution distance. The latter is often evaluated with kernel\u2013based methods. We notice that the impact of the batch size is different on each of the factors: kernel distribution profits from larger batches, while the reconstruction term achieves peak performance on smaller ones. Thus, we define a batch size reconstruction-distribution trade-off. Instead of searching for an optimum global size of the batch, we propose to use small batches for the sake of reconstruction together with a vector enhanced with previously computed latent data for the sake of prior distribution optimization. We evaluate our method on standard benchmarks and illustrate that it can improve the model\u2019s generative scores."}}
{"id": "OYCDhhPzA-", "cdate": 1640995200000, "mdate": 1681658137412, "content": {"title": "Bounding Evidence and Estimating Log-Likelihood in VAE", "abstract": "Many crucial problems in deep learning and statistics are caused by a variational gap, i.e., a difference between evidence and evidence lower bound (ELBO). As a consequence, in the classical VAE model, we obtain only the lower bound on the log-likelihood since ELBO is used as a cost function, and therefore we cannot compare log-likelihood between models. In this paper, we present a general and effective upper bound of the variational gap, which allows us to efficiently estimate the true evidence. We provide an extensive theoretical study of the proposed approach. Moreover, we show that by applying our estimation, we can easily obtain lower and upper bounds for the log-likelihood of VAE models."}}
{"id": "0gxVmYyIQc", "cdate": 1640995200000, "mdate": 1681658137430, "content": {"title": "HyperPocket: Generative Point Cloud Completion", "abstract": "Scanning real-life scenes with modern registration devices typically give incomplete point cloud representations, mostly due to the limitations of the scanning process and 3D occlusions. Therefore, completing such partial representations remains a fundamental challenge of many computer vision applications. Most of the existing approaches aim to solve this problem by learning to reconstruct individual 3D objects in a synthetic setup of an uncluttered environment, which is far from a real-life scenario. In this work, we reformulate the problem of point cloud completion into an objects hallucination task. Thus, we introduce a novel autoencoder-based architecture called HyperPocket that disentangles latent representations and, as a result, enables the generation of multiple variants of the completed 3D point clouds. Furthermore, we split point cloud processing into two disjoint data streams and leverage a hypernetwork paradigm to fill the spaces, dubbed pockets, that are left by the missing object parts. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the scene. Moreover, our method offers competitive performances to the other state-of-the-art models, enabling a plethora of novel applications."}}
{"id": "Ly6_LGwoi_V", "cdate": 1632875628749, "mdate": null, "content": {"title": "Target Layer Regularization for Continual Learning Using Cramer-Wold Generator", "abstract": "We propose an effective regularization strategy (CW-TaLaR) for solving continual learning problems. It uses a penalizing term expressed by the Cramer-Wold distance between two probability distributions defined on a target layer of an underlying neural network that is shared by all tasks, and the simple architecture of the Cramer-Wold generator for modeling output data representation. Our strategy preserves target layer distribution while learning a new task but does not require remembering previous tasks\u2019 datasets. We perform experiments involving several common supervised frameworks, which prove the competitiveness of the CWTaLaR method in comparison to a few existing state-of-the-art continual learning models."}}
{"id": "Gw9vA80c8_n", "cdate": 1632875618025, "mdate": null, "content": {"title": "HyperCube: Implicit Field Representations of Voxelized 3D Models", "abstract": "Recently introduced implicit field representations offer an effective way of generating 3D object shapes. They leverage implicit decoder trained to take a 3D point coordinate concatenated with a shape encoding and to output a value which indicates whether the point is outside the shape or not. Although this approach enables efficient rendering of visually plausible objects, it has two significant limitations. First, it is based on a single neural network dedicated for all objects from a training set which results in a cumbersome training procedure and its application in real life. More importantly, the implicit decoder takes only points sampled within voxels (and not the entire voxels) which yields problems at the classification boundaries and results in empty spaces within the rendered mesh.\n\nTo solve the above limitations, we introduce a new HyperCube architecture based on interval arithmetic network, that enables direct processing of 3D voxels, trained using a hypernetwork paradigm to enforce model convergence. \nInstead of processing individual 3D samples from within a voxel, our approach allows to input the entire voxel (3D cube) represented with its convex hull coordinates, while the target network constructed by a hypernet assigns it to an inside or outside category. \nAs a result our HyperCube model outperforms the competing approaches both in terms of training and inference efficiency, as well as the final mesh quality. "}}
{"id": "yLc7DVNNnO", "cdate": 1609459200000, "mdate": 1681658137424, "content": {"title": "Target Layer Regularization for Continual Learning Using Cramer-Wold Generator", "abstract": "We propose an effective regularization strategy (CW-TaLaR) for solving continual learning problems. It uses a penalizing term expressed by the Cramer-Wold distance between two probability distributions defined on a target layer of an underlying neural network that is shared by all tasks, and the simple architecture of the Cramer-Wold generator for modeling output data representation. Our strategy preserves target layer distribution while learning a new task but does not require remembering previous tasks' datasets. We perform experiments involving several common supervised frameworks, which prove the competitiveness of the CW-TaLaR method in comparison to a few existing state-of-the-art continual learning models."}}
{"id": "b3LeDH0FtRH", "cdate": 1609459200000, "mdate": null, "content": {"title": "HyperPocket: Generative Point Cloud Completion", "abstract": "Scanning real-life scenes with modern registration devices typically give incomplete point cloud representations, mostly due to the limitations of the scanning process and 3D occlusions. Therefore, completing such partial representations remains a fundamental challenge of many computer vision applications. Most of the existing approaches aim to solve this problem by learning to reconstruct individual 3D objects in a synthetic setup of an uncluttered environment, which is far from a real-life scenario. In this work, we reformulate the problem of point cloud completion into an object hallucination task. Thus, we introduce a novel autoencoder-based architecture called HyperPocket that disentangles latent representations and, as a result, enables the generation of multiple variants of the completed 3D point clouds. We split point cloud processing into two disjoint data streams and leverage a hypernetwork paradigm to fill the spaces, dubbed pockets, that are left by the missing object parts. As a result, the generated point clouds are not only smooth but also plausible and geometrically consistent with the scene. Our method offers competitive performances to the other state-of-the-art models, and it enables a~plethora of novel applications."}}
