{"id": "aVE3nsl88O", "cdate": 1680307200000, "mdate": 1687320512648, "content": {"title": "EVA2.0: Investigating Open-domain Chinese Dialogue Systems with Large-scale Pre-training", "abstract": "Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and will make our models and codes publicly available. Automatic and human evaluations show that EVA2.0 significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future research directions on large-scale Chinese open-domain dialogue systems."}}
{"id": "rjK2p2DAMy", "cdate": 1672531200000, "mdate": 1690950444714, "content": {"title": "Tailoring Language Generation Models under Total Variation Distance", "abstract": ""}}
{"id": "fM4rLn3Jt0", "cdate": 1672531200000, "mdate": 1690950444716, "content": {"title": "DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering", "abstract": ""}}
{"id": "_-oG9Gr4A5D", "cdate": 1672531200000, "mdate": 1690950444716, "content": {"title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning", "abstract": ""}}
{"id": "2n3PZn5Eqs", "cdate": 1672531200000, "mdate": 1702661861452, "content": {"title": "Unveiling the Implicit Toxicity in Large Language Models", "abstract": ""}}
{"id": "VELL0PlWfc", "cdate": 1663850002458, "mdate": null, "content": {"title": "Tailoring Language Generation Models under Total Variation Distance", "abstract": "The standard paradigm of neural language generation adopts maximum likelihood estimation (MLE) as the optimizing method. From a distributional view, MLE in fact minimizes the Kullback-Leibler divergence (KLD) between the distribution of the real data and that of the model. However, this approach forces the model to distribute non-zero (sometimes large) probability mass to all training samples regardless of their quality. Moreover, in the attempt to cover the low-probability regions in the data distribution, the model systematically overestimates the probability of corrupted text sequences, which we conjecture is one of the main reasons for text degeneration during autoregressive decoding. To remedy this problem, we leverage the total variation distance (TVD) with its robustness to outliers, and develop practical bounds to apply it to language generation. Then, we introduce the TaiLr objective that balances the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data samples that have low model probabilities with tunable penalization intensity. Experimental results show that our method alleviates the overestimation of degenerated sequences without sacrificing diversity and improves generation quality on a wide range of text generation tasks."}}
{"id": "vvtOQ1UJkQ8", "cdate": 1640995200000, "mdate": 1659940802267, "content": {"title": "Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation", "abstract": "Despite the success of text-to-text pre-trained models in various natural language generation (NLG) tasks, the generation performance is largely restricted by the number of labeled data in downstream tasks, particularly in data-to-text generation tasks. Existing works mostly utilize abundant unlabeled structured data to conduct unsupervised pre-training for task adaption, which fail to model the complex relationship between source structured data and target texts. Thus, we introduce self-training as a better few-shot learner than task-adaptive pre-training, which explicitly captures this relationship via pseudo-labeled data generated by the pre-trained model. To alleviate the side-effect of low-quality pseudo-labeled data during self-training, we propose a novel method called Curriculum-Based Self-Training (CBST) to effectively leverage unlabeled data in a rearranged order determined by the difficulty of text generation. Experimental results show that our method can outperform fine-tuning and task-adaptive pre-training methods, and achieve state-of-the-art performance in the few-shot setting of data-to-text generation."}}
{"id": "kDl4eU8lik6", "cdate": 1640995200000, "mdate": 1653279378762, "content": {"title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation", "abstract": "Pei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Xiaoyan Zhu, Minlie Huang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "M8XHGUOt1lZ", "cdate": 1640995200000, "mdate": 1653279378763, "content": {"title": "Rethinking and Refining the Distinct Metric", "abstract": "Siyang Liu, Sahand Sabour, Yinhe Zheng, Pei Ke, Xiaoyan Zhu, Minlie Huang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2022."}}
{"id": "08CAaZr-rSl", "cdate": 1640995200000, "mdate": 1680235070182, "content": {"title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization", "abstract": ""}}
