{"id": "dp8rVCao2El", "cdate": 1702367429316, "mdate": 1702367429316, "content": {"title": "PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition", "abstract": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels."}}
{"id": "6ruVLB727MC", "cdate": 1663850533029, "mdate": null, "content": {"title": "UL2: Unifying Language Learning Paradigms", "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We release Flax-based T5X model checkpoints for the 20B model publicly.\n"}}
{"id": "wXdEKf5mV6N", "cdate": 1663850453493, "mdate": null, "content": {"title": "Is margin all you need? An extensive empirical study of active learning on tabular data", "abstract": "Given a labeled training set and a collection of unlabeled data, the goal of active learning (AL) is to identify the best unlabeled points to label. In this comprehensive study, we analyze the performance of a variety of AL algorithms on deep neural networks trained on 69 real-world tabular classification datasets from the OpenML-CC18 benchmark. We consider different data regimes and the effect of self-supervised model pre-training. Surprisingly, we find that the classical margin sampling technique matches or outperforms all others, including current state-of-art, in a wide range of experimental settings. To researchers, we hope to encourage rigorous benchmarking against margin, and to practitioners facing tabular data labeling constraints that hyper-parameter-free margin may often be all they need."}}
{"id": "Vu-B0clPfq", "cdate": 1652737811988, "mdate": null, "content": {"title": "Transformer Memory as a Differentiable Search Index", "abstract": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup."}}
{"id": "uLYc4L3C81A", "cdate": 1652737747608, "mdate": null, "content": {"title": "Confident Adaptive Language Modeling", "abstract": "Recent advances in Transformer-based large language models (LLMs) have led to significant performance improvements across many tasks. These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time. In practice, however, the series of generations made by LLMs is composed of varying levels of difficulty. While certain predictions truly benefit from the models' full capacity, other continuations are more trivial and can be solved with reduced compute. In this work, we introduce Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep. Early exit decoding involves several challenges that we address here, such as: (1) what confidence measure to use; (2) connecting sequence-level constraints to local per-token exit decisions; and (3) attending back to missing hidden representations due to early exits in previous tokens. Through theoretical analysis and empirical experiments on three diverse text generation tasks, we demonstrate the efficacy of our framework in reducing compute---potential speedup of up to $\\times 3$---while provably maintaining high performance. "}}
{"id": "rbajSHy1Ny", "cdate": 1640995200000, "mdate": 1681592590621, "content": {"title": "Conformal Prediction Sets with Limited False Positives", "abstract": ""}}
{"id": "hqD84OffeQv", "cdate": 1640995200000, "mdate": 1681592590624, "content": {"title": "Confident Adaptive Language Modeling", "abstract": ""}}
{"id": "bdiMyqWj2g", "cdate": 1640995200000, "mdate": 1681592590584, "content": {"title": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation", "abstract": ""}}
{"id": "ZHzU8fXi7J", "cdate": 1640995200000, "mdate": 1681592590628, "content": {"title": "PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition", "abstract": ""}}
{"id": "UoX5oLFnWdu", "cdate": 1640995200000, "mdate": 1681592590608, "content": {"title": "Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters", "abstract": ""}}
