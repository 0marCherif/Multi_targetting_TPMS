{"id": "vINj_Hv9szL", "cdate": 1663850112813, "mdate": null, "content": {"title": "Benchmarking Constraint Inference in Inverse Reinforcement Learning", "abstract": "When deploying Reinforcement Learning (RL) agents into a physical system, we must ensure that these agents are well aware of the underlying constraints. In many real-world problems, however, the constraints are often hard to specify mathematically and unknown to the RL agents. To tackle these issues, Inverse Constrained Reinforcement Learning (ICRL) empirically estimates constraints from expert demonstrations. As an emerging research topic, ICRL does not have common benchmarks, and previous works tested algorithms under hand-crafted environments with manually-generated expert demonstrations. In this paper, we construct an ICRL benchmark in the context of RL application domains, including robot control, and autonomous driving. For each environment, we design relevant constraints and train expert agents to generate demonstration data. Besides, unlike existing baselines that learn a deterministic constraint, we propose a variational ICRL method to model a posterior distribution of candidate constraints. We conduct extensive experiments on these algorithms under our benchmark and show how they can facilitate studying important research challenges for ICRL. The benchmark, including the instructions for reproducing ICRL algorithms, is available at https://github.com/Guiliang/ICRL-benchmarks-public."}}
{"id": "QoHSzxp7tSN", "cdate": 1652737461086, "mdate": null, "content": {"title": "Uncertainty-Aware Reinforcement Learning for Risk-Sensitive Player Evaluation in Sports Game", "abstract": "A major task of sports analytics is player evaluation. Previous methods commonly measured the impact of players' actions on desirable outcomes (e.g., goals or winning) without considering the risk induced by stochastic game dynamics.  In this paper, we design an uncertainty-aware Reinforcement Learning (RL) framework to learn a risk-sensitive player evaluation metric from stochastic game dynamics. To embed the risk of a player\u2019s movements into the distribution of action-values, we model their 1) aleatoric uncertainty, which represents the intrinsic stochasticity in a sports game, and 2) epistemic uncertainty, which is due to a model's insufficient knowledge regarding Out-of-Distribution (OoD) samples. We demonstrate how a distributional Bellman operator and a feature-space density model can capture these uncertainties. Based on such uncertainty estimation, we propose a Risk-sensitive Game Impact Metric (RiGIM) that measures players' performance over a season by conditioning on a specific confidence level. Empirical evaluation, based on over 9M play-by-play ice hockey and soccer events, shows that RiGIM correlates highly with standard success measures and has a consistent risk sensitivity."}}
{"id": "sosTsRNLmQR", "cdate": 1640995200000, "mdate": 1665585731738, "content": {"title": "Distributional Reinforcement Learning with Monotonic Splines", "abstract": "Distributional Reinforcement Learning (RL) differs from traditional RL by estimating the distribution over returns to capture the intrinsic uncertainty of MDPs. One key challenge in distributional RL lies in how to parameterize the quantile function when minimizing the Wasserstein metric of temporal differences. Existing algorithms use step functions or piecewise linear functions. In this paper, we propose to learn smooth continuous quantile functions represented by monotonic rational-quadratic splines, which also naturally solve the quantile crossing problem. Experiments in stochastic environments show that a dense estimation for quantile functions enhances distributional RL in terms of faster empirical convergence and higher rewards in most cases."}}
{"id": "mo8vimeyQ8c", "cdate": 1640995200000, "mdate": 1681651612872, "content": {"title": "Learning Selective Communication for Multi-Agent Path Finding", "abstract": ""}}
{"id": "KfYp2h5Y5UZ", "cdate": 1640995200000, "mdate": 1665585731738, "content": {"title": "Benchmarking Constraint Inference in Inverse Reinforcement Learning", "abstract": "When deploying Reinforcement Learning (RL) agents into a physical system, we must ensure that these agents are well aware of the underlying constraints. In many real-world problems, however, the constraints followed by expert agents (e.g., humans) are often hard to specify mathematically and unknown to the RL agents. To tackle these issues, Constraint Inverse Reinforcement Learning (CIRL) considers the formalism of Constrained Markov Decision Processes (CMDPs) and estimates constraints from expert demonstrations by learning a constraint function. As an emerging research topic, CIRL does not have common benchmarks, and previous works tested their algorithms with hand-crafted environments (e.g., grid worlds). In this paper, we construct a CIRL benchmark in the context of two major application domains: robot control and autonomous driving. We design relevant constraints for each environment and empirically study the ability of different algorithms to recover those constraints based on expert trajectories that respect those constraints. To handle stochastic dynamics, we propose a variational approach that infers constraint distributions, and we demonstrate its performance by comparing it with other CIRL baselines on our benchmark. The benchmark, including the information for reproducing the performance of CIRL algorithms, is publicly available at https://github.com/Guiliang/CIRL-benchmarks-public"}}
{"id": "C8Ltz08PtBp", "cdate": 1632875717339, "mdate": null, "content": {"title": "Distributional Reinforcement Learning with Monotonic Splines", "abstract": "Distributional Reinforcement Learning (RL) differs from traditional RL by estimating the distribution over returns to capture the intrinsic uncertainty of MDPs. One key challenge in distributional RL lies in how to parameterize the quantile function when minimizing the Wasserstein metric of temporal differences. Existing algorithms use step functions or piecewise linear functions. In this paper, we propose to learn smooth continuous quantile functions represented by monotonic rational-quadratic splines, which also naturally solve the quantile crossing problem. Experiments in stochastic environments show that a dense estimation for quantile functions enhances distributional RL in terms of faster empirical convergence and higher rewards in most cases."}}
{"id": "coYBB4HqfU", "cdate": 1609459200000, "mdate": 1681651612875, "content": {"title": "Distributed Heuristic Multi-Agent Path Finding with Communication", "abstract": ""}}
{"id": "bBbaTPXpKV6", "cdate": 1609459200000, "mdate": 1681651613060, "content": {"title": "Leveraging Approximate Constraints for Localized Data Error Detection", "abstract": ""}}
{"id": "FR1edhtN36", "cdate": 1609459200000, "mdate": 1681651612942, "content": {"title": "Distributed Heuristic Multi-Agent Path Finding with Communication", "abstract": ""}}
{"id": "9iVjaQ7NHid", "cdate": 1609459200000, "mdate": 1681651613059, "content": {"title": "Learning Selective Communication for Multi-Agent Path Finding", "abstract": ""}}
