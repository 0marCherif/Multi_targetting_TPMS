{"id": "O1OZELYSMR", "cdate": 1681651393304, "mdate": 1681651393304, "content": {"title": "Two Polyak-Type Step Sizes for Mirror Descent", "abstract": "We propose two Polyak-type step sizes for mirror descent and prove their convergences for minimizing convex locally Lipschitz functions. Both step sizes, unlike the original Polyak step size, do not need the optimal value of the objective function."}}
{"id": "_jEy7aDYM", "cdate": 1672531200000, "mdate": 1707098113045, "content": {"title": "Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging", "abstract": "Consider the problem of minimizing an expected logarithmic loss over either the probability simplex or the set of quantum density matrices. This problem encompasses tasks such as solving the Poisson inverse problem, computing the maximum-likelihood estimate for quantum state tomography, and approximating positive semi-definite matrix permanents with the currently tightest approximation ratio. Although the optimization problem is convex, standard iteration complexity guarantees for first-order methods do not directly apply due to the absence of Lipschitz continuity and smoothness in the loss function. In this work, we propose a stochastic first-order algorithm named $B$-sample stochastic dual averaging with the logarithmic barrier. For the Poisson inverse problem, our algorithm attains an $\\varepsilon$-optimal solution in $\\tilde{O} (d^2/\\varepsilon^2)$ time, matching the state of the art. When computing the maximum-likelihood estimate for quantum state tomography, our algorithm yields an $\\varepsilon$-optimal solution in $\\tilde{O} (d^3/\\varepsilon^2)$ time, where $d$ denotes the dimension. This improves on the time complexities of existing stochastic first-order methods by a factor of $d^{\\omega-2}$ and those of batch methods by a factor of $d^2$, where $\\omega$ denotes the matrix multiplication exponent. Numerical experiments demonstrate that empirically, our algorithm outperforms existing methods with explicit complexity guarantees."}}
{"id": "ZPHc0-UP1b", "cdate": 1672531200000, "mdate": 1681650507465, "content": {"title": "Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States", "abstract": ""}}
{"id": "XF2Eq70Fe1u", "cdate": 1672531200000, "mdate": 1707098113036, "content": {"title": "Online Learning Quantum States with the Logarithmic Loss via VB-FTRL", "abstract": "Online learning quantum states with the logarithmic loss (LL-OLQS) is a quantum generalization of online portfolio selection, a classic open problem in the field of online learning for over three decades. The problem also emerges in designing randomized optimization algorithms for maximum-likelihood quantum state tomography. Recently, Jezequel et al. (arXiv:2209.13932) proposed the VB-FTRL algorithm, the first nearly regret-optimal algorithm for OPS with moderate computational complexity. In this note, we generalize VB-FTRL for LL-OLQS. Let $d$ denote the dimension and $T$ the number of rounds. The generalized algorithm achieves a regret rate of $O ( d^2 \\log ( d + T ) )$ for LL-OLQS. Each iteration of the algorithm consists of solving a semidefinite program that can be implemented in polynomial time by, e.g., cutting-plane methods. For comparison, the best-known regret rate for LL-OLQS is currently $O ( d^2 \\log T )$, achieved by the exponential weight method. However, there is no explicit implementation available for the exponential weight method for LL-OLQS. To facilitate the generalization, we introduce the notion of VB-convexity. VB-convexity is a sufficient condition for the logarithmic barrier associated with any function to be convex and is of independent interest."}}
{"id": "jtW73TIGnd", "cdate": 1652737643681, "mdate": null, "content": {"title": "Maximum-Likelihood Quantum State Tomography by Soft-Bayes", "abstract": "Quantum state tomography (QST), the task of estimating an unknown quantum state given measurement outcomes, is essential to building reliable quantum computing devices. Whereas computing the maximum-likelihood (ML) estimate corresponds to solving a finite-sum convex optimization problem, the objective function is not smooth nor Lipschitz, so most existing convex optimization methods lack sample complexity guarantees; moreover, both the sample size and dimension grow exponentially with the number of qubits in a QST experiment, so a desired algorithm should be highly scalable with respect to the dimension and sample size, just like stochastic gradient descent. In this paper, we propose a stochastic first-order algorithm that computes an $\\varepsilon$-approximate ML estimate in $O( ( D \\log D ) / \\varepsilon ^ 2 )$ iterations with $O( D^3 )$ per-iteration time complexity, where $D$ denotes the dimension of the unknown quantum state and $\\varepsilon$ denotes the optimization error. Our algorithm is an extension of Soft-Bayes to the quantum setup."}}
{"id": "fYBO--HhzVu", "cdate": 1640995200000, "mdate": 1681650507296, "content": {"title": "Faster Stochastic First-Order Method for Maximum-Likelihood Quantum State Tomography", "abstract": ""}}
{"id": "dLxAvBqtNX", "cdate": 1640995200000, "mdate": 1681650507675, "content": {"title": "Minimizing Quantum R\u00e9nyi Divergences via Mirror Descent with Polyak Step Size", "abstract": ""}}
{"id": "OCj4z9mfP-m", "cdate": 1609459200000, "mdate": 1652759705746, "content": {"title": "Minimizing Quantum Renyi Divergences via Mirror Descent with Polyak Step Size", "abstract": "Quantum information quantities play a substantial role in characterizing operational quantities in various quantum information-theoretic problems. We consider numerical computation of four quantum information quantities: Petz-Augustin information, sandwiched Augustin information, conditional sandwiched Renyi entropy and sandwiched Renyi information. To compute these quantities requires minimizing some order-$\\alpha$ quantum Renyi divergences over the set of quantum states. Whereas the optimization problems are obviously convex, they violate standard bounded gradient/Hessian conditions in literature, so existing convex optimization methods and their convergence guarantees do not directly apply. In this paper, we propose a new class of convex optimization methods called mirror descent with the Polyak step size. We prove their convergence under a weak condition, showing that they provably converge for minimizing quantum Renyi divergences. Numerical experiment results show that entropic mirror descent with the Polyak step size converges fast in minimizing quantum Renyi divergences."}}
{"id": "M58JK7pS10-", "cdate": 1609459200000, "mdate": 1652759705746, "content": {"title": "Maximum-Likelihood Quantum State Tomography by Cover's Method with Non-Asymptotic Analysis", "abstract": "We propose an iterative algorithm that computes the maximum-likelihood estimate in quantum state tomography. The optimization error of the algorithm converges to zero at an $O ( ( 1 / k ) \\log D )$ rate, where $k$ denotes the number of iterations and $D$ denotes the dimension of the quantum state. The per-iteration computational complexity of the algorithm is $O ( D ^ 3 + N D ^2 )$, where $N$ denotes the number of measurement outcomes. The algorithm can be considered as a parameter-free correction of the $R \\rho R$ method [A. I. Lvovsky. Iterative maximum-likelihood reconstruction in quantum homodyne tomography. \\textit{J. Opt. B: Quantum Semiclass. Opt.} 2004] [G. Molina-Terriza et al. Triggered qutrits for quantum communication protocols. \\textit{Phys. Rev. Lett.} 2004.]."}}
{"id": "p1AYGf04Vz", "cdate": 1577836800000, "mdate": 1681650674478, "content": {"title": "An Online Algorithm for Maximum-Likelihood Quantum State Tomography", "abstract": ""}}
