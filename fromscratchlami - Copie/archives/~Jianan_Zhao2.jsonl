{"id": "9KxmgfE7Cx", "cdate": 1686188698463, "mdate": 1686188698463, "content": {"title": "To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion", "abstract": "Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate this problem, we present Vertical Learning Paradigm (VLP), which extends embedding models by allowing to explicitly copy target information from related factual triples for more accurate prediction. Rather than solely relying on the implicit memory, VLP directly provides additional cues to improve the generalization ability of embedding models, especially making the distant link prediction significantly easier. Moreover, we also propose a novel relative distance based negative sampling technique (ReD) for more effective optimization. Experiments demonstrate the validity and generality of our proposals on two standard benchmarks. Our code is available at https://github.com/rui9812/VLP."}}
{"id": "vvEm1Mr-uh", "cdate": 1672531200000, "mdate": 1681650559375, "content": {"title": "Self-Supervised Graph Structure Refinement for Graph Neural Networks", "abstract": ""}}
{"id": "q0nmYciuuZN", "cdate": 1663850206166, "mdate": null, "content": {"title": "Learning on Large-scale Text-attributed Graphs via Variational Inference", "abstract": "This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and  GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach."}}
{"id": "tufBoHTd33", "cdate": 1640995200000, "mdate": 1667982774986, "content": {"title": "Improving Relevance Modeling via Heterogeneous Behavior Graph Learning in Bing Ads", "abstract": "As the fundamental basis of sponsored search, relevance modeling measures the closeness between the input queries and the candidate ads. Conventional relevance models solely rely on the textual data, which suffer from the scarce semantic signals within the short queries. Recently, user historical click behaviors are incorporated in the format of click graphs to provide additional correlations beyond pure textual semantics, which contributes to advancing the relevance modeling performance. However, user behaviors are usually arbitrary and unpredictable, leading to the noisy and sparse graph topology. In addition, there exist other types of user behaviors besides clicks, which may also provide complementary information. In this paper, we study the novel problem of heterogeneous behavior graph learning to facilitate relevance modeling task. Our motivation lies in learning an optimal and task-relevant heterogeneous behavior graph consisting of multiple types of user behaviors. We further propose a novel HBGLR model to learn the behavior graph structure by mining the sophisticated correlations between node semantics and graph topology, and encode the textual semantics and structural heterogeneity into the learned representations. Our proposal is evaluated over real-world industry datasets, and has been mainstreamed in the Bing ads. Both offline and online experimental results demonstrate its superiority."}}
{"id": "bVCgJfn19_T", "cdate": 1640995200000, "mdate": 1681650559298, "content": {"title": "Prohibited Item Detection via Risk Graph Structure Learning", "abstract": ""}}
{"id": "Yq_rChjnn_e", "cdate": 1640995200000, "mdate": 1675194269283, "content": {"title": "Rx-refill Graph Neural Network to Reduce Drug Overprescribing Risks (Extended Abstract)", "abstract": "Prescription (aka Rx) drugs can be easily overprescribed and lead to drug abuse or opioid overdose. Accordingly, a state-run prescription drug monitoring program (PDMP) in the United States has been developed to reduce overprescribing. However, PDMP has limited capability in detecting patients' potential overprescribing behaviors, impairing its effectiveness in preventing drug abuse and overdose in patients. In this paper, we propose a novel model RxNet, which builds 1) a dynamic heterogeneous graph to model Rx refills that are essentially prescribing and dispensing (P&D) relationships among various patients, 2) an RxLSTM network to explore the dynamic Rx-refill behavior and medical condition variation of patients, and 3) a dosing-adaptive network to extract and recalibrate dosing patterns and obtain the refined patient representations which are finally utilized for overprescribing detection. The extensive experimental results on a one-year state-wide PDMP data demonstrate that RxNet consistently outperforms state-of-the-art methods in predicting patients at high risk of opioid overdose and drug abuse."}}
{"id": "WtOMJ4_2ij", "cdate": 1640995200000, "mdate": 1681553193553, "content": {"title": "Test-Time Training for Graph Neural Networks", "abstract": ""}}
{"id": "T8zOAIzuGb", "cdate": 1640995200000, "mdate": 1667982774937, "content": {"title": "HousE: Knowledge Graph Embedding with Householder Parameterization", "abstract": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity. In this work, we propose a more powerful KGE framework named HousE, which involves a novel parameterization based on two kinds of Householder transformations: (1) Householder rotations to achieve superior capacity of modeling relation patterns; (2) Householder projections to handle sophisticated relation mapping properties. Theoretically, HousE is capable of modeling crucial relation patterns and mapping properties simultaneously. Besides, HousE is a generalization of existing rotation-based models while extending the rotations to high-dimensional spaces. Empirically, HousE achieves new state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/anrep/HousE."}}
{"id": "K9CBFmXfbxV", "cdate": 1640995200000, "mdate": 1674575566851, "content": {"title": "Adaptive Kernel Graph Neural Network", "abstract": "Graph neural networks (GNNs) have demonstrated great success in representation learning for graph-structured data. The layer-wise graph convolution in GNNs is shown to be powerful at capturing graph topology. During this process, GNNs are usually guided by pre-defined kernels such as Laplacian matrix, adjacency matrix, or their variants. However, the adoptions of pre-defined kernels may restrain the generalities to different graphs: mismatch between graph and kernel would entail sub-optimal performance. For example, GNNs that focus on low-frequency information may not achieve satisfactory performance when high-frequency information is significant for the graphs, and vice versa. To solve this problem, in this paper, we propose a novel framework - i.e., namely Adaptive Kernel Graph Neural Network (AKGNN) - which learns to adapt to the optimal graph kernel in a unified manner at the first attempt. In the proposed AKGNN, we first design a data-driven graph kernel learning mechanism, which adaptively modulates the balance between all-pass and low-pass filters by modifying the maximal eigenvalue of the graph Laplacian. Through this process, AKGNN learns the optimal threshold between high and low frequency signals to relieve the generality problem. Later, we further reduce the number of parameters by a parameterization trick and enhance the expressive power by a global readout function. Extensive experiments are conducted on acknowledged benchmark datasets and promising results demonstrate the outstanding performance of our proposed AKGNN by comparison with state-of-the-art GNNs. The source code is publicly available at: https://github.com/jumxglhf/AKGNN."}}
{"id": "4OFORWdQbNp", "cdate": 1640995200000, "mdate": 1681553193741, "content": {"title": "Learning on Large-scale Text-attributed Graphs via Variational Inference", "abstract": ""}}
