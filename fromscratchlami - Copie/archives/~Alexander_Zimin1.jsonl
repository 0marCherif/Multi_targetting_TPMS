{"id": "T-dqkZhF7E", "cdate": 1609459200000, "mdate": 1695971297670, "content": {"title": "Event stream classification with limited labeled data for e-commerce monitoring", "abstract": "Monitoring and diagnostics of large software systems is crucial to ensure uninterrupted functioning of modern businesses. Reliability engineers have to rely on automatic event processing to identify and mitigate any potential disruptions of the system health from underlying computer networks. As obtaining impact labels for individual events is expensive, systems operators usually maintain only a small rep-resentative dataset, making it hard for machine learning practitioners to train models on large-scale data streams. By formulating the problem within the multiple instance learning framework, we propose an approach to event classification that can be effectively trained using this limited information. Our evaluation results show potential 65% reduction in minutes spent by the network reliability engineers on disruption investigations when the proposed model is used. By automatically quantifying the network impact, the proposed approach streamlines the investigation process and reduces the risk of unnecessary wake-up calls among on-call reliability engineers and resolver personnel."}}
{"id": "SkEMMD-uZB", "cdate": 1356998400000, "mdate": null, "content": {"title": "Online learning in episodic Markovian decision processes by relative entropy policy search", "abstract": "We study the problem of online learning in finite episodic Markov decision processes where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a finite action space $\\A$ and the state space $\\X$ has a layered structure with $L$ layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after $T$ episodes is $2\\sqrt{L\\nX\\nA T\\log(\\nX\\nA/L)}$ in the bandit setting and $2L\\sqrt{T\\log(\\nX\\nA/L)}$ in the full information setting. These guarantees largely improve previously known results under much milder assumptions and cannot be significantly improved under general assumptions."}}
