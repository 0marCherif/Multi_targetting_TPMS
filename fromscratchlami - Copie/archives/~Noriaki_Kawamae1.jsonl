{"id": "I7aQAOSqpi9", "cdate": 1672531200000, "mdate": 1677762483855, "content": {"title": "Friendly Conditional Text Generator", "abstract": ""}}
{"id": "96kgRrpnkgS", "cdate": 1663850103146, "mdate": null, "content": {"title": "Topic and Hyperbolic Transformer to Handle Multi-modal Dependencies", "abstract": "As multi-modal search relies on jointly learning image-text representations and has been investigated in the literature,\nour innovation is to develop Chimera, a framework in which to learn their representations and similarities.\nBecause the core of multi-modal search is learning the modalities in a shared semantic space and measuring their similarities,\nsearch quality depends on which expressive space is utilized in learning.\nThis motivates us to identify the space that can elucidate their semantic and complex relationships with small information loss.\nNovelty is assured by introducing the topic and hyperbolic as spaces,\nand performing contrastive/metric learning tasks to ensure the cooperation of these spaces with Transformer.\nExperiments show that Chimera empowers pre-trained models for multi-modal search tasks and demonstrate the ability of the layers it introduces."}}
{"id": "WOfOf53mVyo", "cdate": 1663849965873, "mdate": null, "content": {"title": "Topic Aware Transformer: Domain Shift for Unconditional Text Generation Model", "abstract": "Our goal is to adapt pre-trained language models (PLMs) to support unconditional text generation tasks.\nBecause Transformer-based models are pre-trained on more massive and heterogeneous corpora than specific target corpus,\nthe gap between these corpora and the target corpus raises the question of whether these PLMs will actually benefit this task even after fine-tuning.\nAs the domain adaptation of PLMs needs to bridge this gap,\nwe propose a framework, Topic Aware Transformer (TAT), that adapts PLMs for target-aware text generation while alleviating catastrophic forgetting.\nThe motivation of TAT to distill the target-specific knowledge as topics,\nand steer PLMs toward these topics.\nThis requirement and motivation lead us to introduce a topic steering layer (TSL) as an additional layer,\nand Topic Distribution Modeling (TDM) as a training task.\nExperiments show that these components resolve the gap as the domain shift,\nand can tailor PLMs to generate text to better reflect a given small fine-tuning corpus."}}
{"id": "Cy0n0WCvLPU", "cdate": 1632875656213, "mdate": null, "content": {"title": "Topic Aware Neural Language Model: Domain Adaptation of Unconditional Text Generation Models", "abstract": "Our goal is to adapt pre-trained neural language models (NLMs) to the unconditional text generation task within the target domain.\nBecause many Transformer based NLMs are trained on more massive and heterogeneous corpora than this target domain,\nthe difference between these corpora and the target domain raises the question of whether these NLMs can provide their benefits to this task even after the fine-tuning.\nTo tackle these problems, our approach focuses on topics to bridge the semantic gap between these corpora and the target domain corpus,\nand relates them at a topic level.\nThat is, this approach injects topics into these NLMs and trains them via topics behind these dependencies over segments,\nintroducing both topic alignment (TA) and training tasks (TDM and TEM),\nwhile previous Transformer based NLMs are better at learning from the predefined segment length such as the context.\nExperiments show that this approach contributes to resolve the imbalance between these corpora,\nand can tailor previous pre-trained NLMs to generate coherent and semantically valid text reflecting a given small fine-tuning corpus."}}
{"id": "8Ub2tTBqF9Q", "cdate": 1609459200000, "mdate": 1653199234464, "content": {"title": "A Text Generation Model that Maintains the Order of Words, Topics, and Parts of Speech via Their Embedding Representations and Neural Language Models", "abstract": "Our goal is to generate coherent text accurately in terms of their semantic information and syntactic structure. Embedding methods and neural language models are indispensable in generating coherent text as they learn semantic information, and syntactic structure, respectively, and they are indispensable methods for generating coherent text. We focus here on parts of speech (POS) (e.g. noun, verb, preposition, etc.) so as to enhance these models, and allow us to generate truly coherent text more efficiently than is possible by using any of them in isolation. This leads us to derive Words and Topics and POS 2 Vec (WTP2Vec) as an embedding method, and Structure Aware Unified Language Model (SAUL) as a neural language model. Experiments show that our approach enhances previous models and generates coherent and semantically valid text with natural syntactic structure."}}
{"id": "qPYBs53borV", "cdate": 1546300800000, "mdate": null, "content": {"title": "Topic Structure-Aware Neural Language Model: Unified language model that maintains word and topic ordering by their embedded representations", "abstract": "Our goal is to exploit a unified language model so as to explain the generative process of documents precisely in view of their semantic and topic structures. Because various methods model documents in disparate ways, we are motivated by the expectation that coordinating these methods will allow us to achieve this goal more efficiently than using them in isolation; we combine topic models, embedding models, and neural language models. As we focus on the fact that topic models can be shared among, and indeed complement embedding models and neural language models, we propose Word and topic 2 vec (Wat2vec), and Topic Structure-Aware Neural Language Model (TSANL). Wat2vec uses topics as global semantic information and local semantic information as embedding representations of topics and words, and embeds both words and topics in the same space. TSANL uses recurrent neural networks to capture long-range dependencies over topics and words. Since existing topic models demand time consuming learning and have poor scalability, both due to breaking the document?s structure such as order of words and topics, TSANL maintains the orders of words and topics as phrases and segments, respectively. TSANL reduces the calculation cost and required memory by feeding topic recurrent neural networks, and topic specific word networks with these embedding representations. Experiments show that TSANL maintains both segments and topical phrases, and so enhances previous models."}}
{"id": "DF89XP9i1bZ", "cdate": 1514764800000, "mdate": null, "content": {"title": "Topic Chronicle Forest for Topic Discovery and Tracking", "abstract": "To ease comprehension of given time-stamped corpora, we extend topic models to handle both the specificity and temporality of topics; this is a significant advance over previous models which fail to provide both views simultaneously. Our proposed model consists of the Topic Chronicle Forest(TCF) and Thematic Dirichlet Processes(TDP). TCF is a set of Topic Chronicle Trees, where each tree is a hierarchy of topics that becomes more specialized toward the leaves. Only one tree is defined in each time interval, a region, and is used for TDP to generate a document. The advantage of our approach lies in providing more compact topic organization, while preserving both the semantic of a given corpus and the thematic of each document. Experiments show that TCF is a useful extension for longitudinal topic discovery and tracking, and helps us to organize and digest data sets."}}
{"id": "Sk-_mZWObS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Time Series Analysis Using NOC", "abstract": "We present a time series analysis employing natural language processing (NLP) techniques, and show the effect of N-gram over Context (NOC), that is a one of topic models that enjoy success in NLP, in this analysis."}}
{"id": "S1Vet-ZubB", "cdate": 1451606400000, "mdate": null, "content": {"title": "N-gram over Context", "abstract": "Our proposal, $N$-gram over Context (NOC), is a nonparametric topic model that aims to help our understanding of a given corpus, and be applied to many text mining applications. Like other topic models, NOC represents each document as a mixture of topics and generates each word from one topic. Unlike these models, NOC focuses on both a topic structure as an internal linguistic structure, and N-gram as an external linguistic structure. To improve the quality of topic specific N-grams, NOC reveals a tree of topics that captures the semantic relationship between topics from a given corpus as context, and forms $N$-gram by offering power-law distributions for word frequencies on this topic tree. To gain both these linguistic structures efficiently, NOC learns them from a given corpus in a unified manner. By accessing this entire tree at the word level in the generative process of each document, NOC enables each document to maintain a thematic coherence and form $N$-grams over context. We develop a parallelizable inference algorithm, D-NOC, to support large data sets. Experiments on review articles/papers/tweet show that NOC is useful as a generative model to discover both the topic structure and the corresponding N-grams, and well complements human experts and domain specific knowledge. D-NOC can process large data sets while preserving full generative model performance, by the help of an open-source distributed machine learning framework."}}
{"id": "rJEPVNWd-B", "cdate": 1420070400000, "mdate": null, "content": {"title": "Real Time Recommendations from Connoisseurs", "abstract": "The information overload problem remains serious for both consumers and service/content providers, leading to heightened demands for personalized recommendations. For recommender systems, updating user models is one of the most important tasks to keep up with their changing preferences and trends. Especially since new consumers and items emerge every day, which are promptly rated or reviewed, updating lists of items and rankings is crucial. In this paper, we set the goal of real time recommendation, to present these items instantly. Unlike standard collaborative filtering algorithms, our offline approach focuses only innovative consumers for these predictions, and then uses as few consumers as possible while keeping the same precision. Since innovators exist in many communities, and their opinions will spread and then stimulate their followers to adopt the same behavior, our approach is based on the hypothesis that a set of innova- tive consumers is sufficient to represent the most representative opinions in each community. Following this hypothesis, we derive a scalable method to detect both communities and innovative consumers in each community from a web- scale data from a behavior log. Our evaluation shows that our proposed weighting method can accurately sample given logs, and be compatible only with previous algorithms for real time recommendations."}}
