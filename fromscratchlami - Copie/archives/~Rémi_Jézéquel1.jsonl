{"id": "ejmqyWW0MK6", "cdate": 1621630170120, "mdate": null, "content": {"title": "Mixability made efficient: Fast online multiclass logistic regression", "abstract": "Mixability has been shown to be a powerful tool to obtain algorithms with optimal regret. However, the resulting methods often suffer from high computational complexity which has reduced their practical applicability. For example, in the case of multiclass logistic regression, the aggregating forecaster (see Foster et al. 2018) achieves a regret of $O(\\log(Bn))$ whereas Online Newton Step achieves $O(e^B\\log(n))$ obtaining a double exponential gain in $B$ (a bound on the norm of comparative functions). However, this high statistical performance is at the price of a prohibitive computational complexity $O(n^{37})$.\nIn this paper, we use quadratic surrogates to make aggregating forecasters more efficient. We show that the resulting algorithm has still high statistical performance for a large class of losses. In particular, we derive an algorithm for multiclass regression with a regret bounded by $O(B\\log(n))$ and computational complexity of only $O(n^4)$."}}
{"id": "CA3Iddi586o", "cdate": 1577836800000, "mdate": null, "content": {"title": "Efficient improper learning for online logistic regression", "abstract": "We consider the setting of online logistic regression and consider the regret with respect to the $\\ell_2$-ball of radius $B$. It is known (see Hazan et al. (2014)) that any proper algorithm which has logarithmic regret in the number of samples (denoted $n$) necessarily suffers an exponential multiplicative constant in $B$. In this work, we design an efficient improper algorithm that avoids this exponential constant while preserving a logarithmic regret. Indeed, Foster et al. (2018) showed that the lower bound does not apply to improper algorithms and proposed a strategy based on exponential weights with prohibitive computational complexity. Our new algorithm based on regularized empirical risk minimization with surrogate losses satisfies a regret scaling as $O(B\\log(Bn))$ with a per-round time-complexity of order $O(d^2 + \\log(n))$."}}
{"id": "x_DurUzS_u", "cdate": 1546300800000, "mdate": null, "content": {"title": "Efficient online learning with kernels for adversarial large scale problems", "abstract": "We are interested in a framework of online learning with kernels for low-dimensional, but large-scale and potentially adversarial datasets. We study the computational and theoretical performance of online variations of kernel Ridge regression. Despite its simplicity, the algorithm we study is the first to achieve the optimal regret for a wide range of kernels with a per-round complexity of order $n^\\alpha$ with $\\alpha &lt; 2$. The algorithm we consider is based on approximating the kernel with the linear span of basis functions. Our contributions are twofold: 1) For the Gaussian kernel, we propose to build the basis beforehand (independently of the data) through Taylor expansion. For $d$-dimensional inputs, we provide a (close to) optimal regret of order $O((\\log n)^{d+1})$ with per-round time complexity and space complexity $O((\\log n)^{2d})$. This makes the algorithm a suitable choice as soon as $n \\gg e^d$ which is likely to happen in a scenario with small dimensional and large-scale dataset; 2) For general kernels with low effective dimension, the basis functions are updated sequentially, adapting to the data, by sampling Nystr\u00f6m points. In this case, our algorithm improves the computational trade-off known for online kernel regression."}}
