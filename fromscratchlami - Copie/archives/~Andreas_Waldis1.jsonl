{"id": "FYibbhZp4g", "cdate": 1686042453036, "mdate": null, "content": {"title": "PrivacyGLUE: A Benchmark Dataset for General Language Understanding in Privacy Policies", "abstract": "Benchmarks for general language understanding have been rapidly developing in recent years of NLP research, particularly because of their utility in choosing strong-performing models for practical downstream applications. While benchmarks have been proposed in the legal language domain, virtually no such benchmarks exist for privacy policies despite their increasing importance in modern digital life. This could be explained by privacy policies falling under the legal language domain, but we find evidence to the contrary that motivates a separate benchmark for privacy policies. Consequently, we propose PrivacyGLUE as the first comprehensive benchmark of relevant and high-quality privacy tasks for measuring general language understanding in the privacy language domain. Furthermore, we release performances from multiple transformer language models and perform model\u2013pair agreement analysis to detect tasks where models benefited from domain specialization. Our findings show the importance of in-domain pretraining for privacy policies. We believe PrivacyGLUE can accelerate NLP research and improve general language understanding for humans and AI algorithms in the privacy language domain, thus supporting the adoption and acceptance rates of solutions based on it."}}
{"id": "R-C71wge494E", "cdate": 1640995200000, "mdate": 1671179921681, "content": {"title": "Privacy and Customer's Education: NLP for Information Resources Suggestions and Expert Finder Systems", "abstract": "Privacy is one of the key issues for citizen\u2019s everyday online activities, with the United Nations defining it as \u201ca human right in the digital age\u201d. Despite the introduction of data privacy regulations almost everywhere around the globe, the biggest barrier to effectiveness is the customer\u2019s capacity to map the privacy statement received with the regulation in force and understand their terms. This study advocates the creation of a convenient and cost-efficient question-answering service for answering customers\u2019 queries on data privacy. It proposes a dual step approach, allowing consumers to ask support to a conversational agent boosted by a smart knowledge base, attempting to answer the question using the most appropriate legal document. Being the self-help approach insufficient, our system enacts a second step suggesting a ranked list of legal experts for focused advice. To achieve our objective, we need large enough and specialised dataset and we plan to apply state-of-the-art Natural Language Processing (NLP) techniques in the field of open domain question answering. This paper describes the initial steps and some early results we achieved in this direction and the next steps we propose to develop a one-stop solution for consumers privacy needs."}}
{"id": "5whxQxvmxQ", "cdate": 1640995200000, "mdate": 1672053479288, "content": {"title": "Composing Structure-Aware Batches for Pairwise Sentence Classification", "abstract": ""}}
{"id": "1L0Yja5KIj", "cdate": 1640995200000, "mdate": 1672053479293, "content": {"title": "Contextual information integration for stance detection via cross-attention", "abstract": "Stance detection deals with identifying an author's stance towards a target. Most existing stance detection models are limited because they do not consider relevant contextual information which allows for inferring the stance correctly. Complementary context can be found in knowledge bases but integrating the context into pretrained language models is non-trivial due to the graph structure of standard knowledge bases. To overcome this, we explore an approach to integrate contextual information as text which allows for integrating contextual information from heterogeneous sources, such as structured knowledge sources and by prompting large language models. Our approach can outperform competitive baselines on a large and diverse stance detection benchmark in a cross-target setup, i.e. for targets unseen during training. We demonstrate that it is more robust to noisy context and can regularize for unwanted correlations between labels and target-specific vocabulary. Finally, it is independent of the pretrained language model in use."}}
{"id": "bVmv55AUU7r", "cdate": 1609459200000, "mdate": 1631644658793, "content": {"title": "Security Rules Identification and Validation: The Role of Explainable Clustering and Information Visualisation", "abstract": "In the context of data access and export control from enterprise information systems, one of the issue is the generation of the rules. Currently, this time consuming and difficult task is highly based on experience. Expert security analysts merge their experience of Enterprise Resource Planning (ERP) systems with the random exploration of the logs generated by the system to try to envision the most relevant attack paths. This project allowed to explore different approaches for creating support for human experts in security rule identification and validation, while preserving interpretability of the results and inspectability of the approach used. This resulted in a tool that complements the security engine by supporting experts in defining uncommon patterns as security-related events to be monitored and vetted by the event classification engine. The result is a promising instrument allowing the human inspection of candidate security-related relevant events/patterns. Main focus being the definition of security rules to be enforced by the specific security engine at run-time. An initial evaluation round shows a positive trend into the users\u2019 perception, even tough a miss of contextual information still hinders its usage by more business-oriented profiles."}}
{"id": "EEhH6JII6N", "cdate": 1609459200000, "mdate": 1631644658788, "content": {"title": "Nested and Balanced Entity Recognition using Multi-Task Learning", "abstract": "Entity Recognition (ER) within a text is a fundamental exercise in Natural Language Processing, enabling further depending tasks such as Knowledge Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of single words or of a consecutive sequence of terms, constituting the basic building blocks for communication. Mainstream ER approaches are mainly limited to flat structures, concentrating on the outermost entities while ignoring the inner ones. This paper introduces a partly-layered network architecture that deals with the complexity of overlapping and nested cases. The proposed architecture consists of two parts: (1) a shared Sequence Layer and (2) a stacked component with multiple Tagging Layers. The adoption of such an architecture has the advantage of preventing overfit to a specific word-length, thus maintaining performance for longer entities despite their lower frequency. To verify the proposed architecture's effectiveness, we train and evaluate this architecture to recognise two kinds of entities - Concepts (CR) and Named Entities (NER). Our approach achieves state-of-the-art NER performances, while it outperforms previous CR approaches. Considering these promising results, we see the possibility to evolve the architecture for other cases such as the extraction of events or the detection of argumentative components."}}
{"id": "fZdnyALPyI", "cdate": 1577836800000, "mdate": 1631644658792, "content": {"title": "Towards eXplainable AI in Text Features Engineering for Concept Recognition", "abstract": "The rapid and pervasive development of methods from Artificial Intelligence (AI) affects our everyday life. Its application improves the users\u2019 experience of many daily tasks. Despite the enhancements provided, such approaches have a substantial limitation in the shortfall of people\u2019s trust connected with their lack of explainability. In natural language understanding (NLU) and processing (NLP), a fundamental objective is to support human interactions using sense-making of the language for communication. Such methods try to comprehend and reproduce the self-evident processes of human communication. This applies either in receiving speech signals or in extracting relevant information from a text. Furthermore, the pervasiveness of AI methods in the workplace and on the free time demands a sustainable and verified support of users\u2019 trust, as a natural condition for their acceptance. The objective of this work is to introduce a framework for the calculation and selection of understandable text features. Such features can increase the confidence placed into adopted NLP solutions. The following work outlines the Text Feature Framework and its text features, based on statistical information coming from a general text corpus. The showcase experiment uses those features to verify them on the concept recognition task. The results shows their capability to explain a model and its predictions. The resulting concept recognition models are competitive with other methods existing in the literature. It has the definitive advantage of being able to externalize the supporting evidence for a choice of concept identification."}}
{"id": "yhiAzBCA7QM", "cdate": 1514764800000, "mdate": 1631644658895, "content": {"title": "Concept Recognition with Convolutional Neural Networks to Optimize Keyphrase Extraction", "abstract": "For knowledge management purposes, it would be useful to automatically classify and tag documents based on their content. Keyphrase extraction is one way of achieving this automatically by using statistical or semantic methods. Whereas corpus-index-based keyphrase extraction can extract relevant concepts for documents, the inverse document index grows exponentially with the number of words that candidate concepts can have. Document-based heuristics can solve this issue, but often result in keyphrases that are not concepts. To increase concept precision, or the percentage of extracted keyphrases that represent actual concepts, we contribute a method to filter keyphrases based on a pre\u2013trained convolutional neural network (CNN). We tested CNNs containing vertical and horizontal filters to decide whether an n-gram (i.e, a consecutive sequence of N words) is a concept or not, from a training set with labeled examples. The classification training signal is derived from the Wikipedia corpus, assuming that an n-gram certainly represents a concept if a corresponding Wikipedia page title exists. The CNN input feature is the vector representation of each word, derived from a word embedding model; the output is the probability of an n-gram to represent a concept. Multiple configurations for vertical and horizontal filters are analyzed and optimised through a hyper-parameterization process. The results demonstrated concept precision for extracted keywords of between 60 and 80% on average. Consequently, by applying a CNN-based concept recognition filter, the concept precision of keyphrase extraction was significantly improved. For an optimal parameter configuration with an average of five extracted keyphrases per document, the concept precision could be increased from 0.65 to 0.8, meaning that on average, at least four out of five keyphrases extracted by our algorithm were actual concepts verified by Wikipedia titles."}}
{"id": "VMpGfX1V7iG", "cdate": 1514764800000, "mdate": 1631644658892, "content": {"title": "A Domain Specific ESA Inspired Approach for Document Semantic Description", "abstract": "Document semantic similarity is a current research field, in particular when the concept-based characterization (or signature) of the entities should be automatically extracted from their content. This becomes critical whenever someone would like to build an effective recommender system on top of this similarity measure and its usage for document retrieval and ranking. In this work, our research goal is an expert system for job placement, based on skills, capabilities, areas of expertise present into someone\u2019s curriculum vitae and personal preferences. The challenge is to take into account all the personal educational experiences (formal, informal, and on-the-job), but also work-related know-how, to create a concept based profile of the person. This will allow a reasoned matching process with existing job positions, but also towards additional educational experience for profile improvement. Taking inspiration from the explicit semantic analysis (ESA), we developed a domain-specific approach to semantically characterize documents and to compare them for similarity. Thanks to an enriching and a filtering process, we transform the general purpose German Wikipedia dump into a domain specific model for our task. The domain is defined also through a German knowledge base of description for educational experiences and for job offers. Initial testing with a small set of documents demonstrated that our approach covers the main requirements. There are still open issues that we would like to tackle in the next project steps. Alongside, we have other research directions we plan to take into account, ranging from the consideration of information granulation theories to the best parameters set for algorithm tuning, till the extensibility of our solution to a multi-lingual context."}}
{"id": "QFYxD1mB_sy", "cdate": 1514764800000, "mdate": 1631644658791, "content": {"title": "Concept Extraction with Convolutional Neural Networks", "abstract": ""}}
