{"id": "VE8QRTrWAMb", "cdate": 1652737556919, "mdate": null, "content": {"title": "Near-Optimal Regret for Adversarial MDP with Delayed Bandit Feedback", "abstract": "The standard assumption in reinforcement learning (RL) is that agents observe feedback for their actions immediately. However, in practice feedback is often observed in delay. This paper studies online learning in episodic Markov decision process (MDP) with unknown transitions, adversarially changing costs, and unrestricted delayed bandit feedback. More precisely, the feedback for the agent in episode $k$ is revealed only in the end of episode $k + d^k$, where the delay $d^k$ can be changing over episodes and chosen by an oblivious adversary. We present the first algorithms that achieve near-optimal $\\sqrt{K + D}$ regret, where $K$ is the number of episodes and $D = \\sum_{k=1}^K d^k$ is the total delay, significantly improving upon the best known regret bound of $(K + D)^{2/3}$."}}
{"id": "Rk7B9kmp7R8", "cdate": 1621629718503, "mdate": null, "content": {"title": "Minimax Regret for Stochastic Shortest Path", "abstract": "We study the Stochastic Shortest Path (SSP) problem in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the problem, the agent has no prior knowledge about the costs and dynamics of the model. She repeatedly interacts with the model for $K$ episodes, and has to minimize her regret. In this work we show that the minimax regret for this setting is $\\widetilde O(\\sqrt{ (B_\\star^2 + B_\\star) |S| |A| K})$ where $B_\\star$ is a bound on the expected cost of the optimal policy from any state, $S$ is the state space, and $A$ is the action space. This matches the $\\Omega (\\sqrt{ B_\\star^2 |S| |A| K})$ lower bound of Rosenberg et al. [2020] for $B_\\star \\ge 1$, and improves their regret bound by a factor of $\\sqrt{|S|}$. For $B_\\star < 1$ we prove a matching lower bound of $\\Omega (\\sqrt{ B_\\star |S| |A| K})$. Our algorithm is based on a novel reduction from SSP to finite-horizon MDPs.  To that end, we provide an algorithm for the finite-horizon setting whose leading term in the regret depends polynomially on the expected cost of the optimal policy and only logarithmically on the horizon."}}
{"id": "BS4SiQ3U9t6", "cdate": 1621629718313, "mdate": null, "content": {"title": "Oracle-Efficient Regret Minimization in Factored MDPs with Unknown Structure", "abstract": "We study regret minimization in non-episodic factored Markov decision processes (FMDPs), where all existing algorithms make the strong assumption that the factored structure of the FMDP is known to the learner in advance. In this paper, we provide the first algorithm that learns the structure of the FMDP while minimizing the regret. Our algorithm is based on the optimism in face of uncertainty principle, combined with a simple statistical method for structure learning, and can be implemented efficiently given oracle-access to an FMDP planner. Moreover, we give a variant of our algorithm that remains efficient even when the oracle is limited to non-factored actions, which is the case with almost all existing approximate planners. Finally, we leverage our techniques to prove a novel lower bound for the known structure case, closing the gap to the regret bound of Chen et al. [2021]."}}
{"id": "1bYD7uBYKSE", "cdate": 1609459200000, "mdate": null, "content": {"title": "Minimax Regret for Stochastic Shortest Path", "abstract": "We study the Stochastic Shortest Path (SSP) problem in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the problem, the agent has no prior knowledge about the costs and dynamics of the model. She repeatedly interacts with the model for $K$ episodes, and has to minimize her regret. In this work we show that the minimax regret for this setting is $\\widetilde O(\\sqrt{ (B_\\star^2 + B_\\star) |S| |A| K})$ where $B_\\star$ is a bound on the expected cost of the optimal policy from any state, $S$ is the state space, and $A$ is the action space. This matches the $\\Omega (\\sqrt{ B_\\star^2 |S| |A| K})$ lower bound of Rosenberg et al. [2020] for $B_\\star \\ge 1$, and improves their regret bound by a factor of $\\sqrt{|S|}$. For $B_\\star < 1$ we prove a matching lower bound of $\\Omega (\\sqrt{ B_\\star |S| |A| K})$. Our algorithm is based on a novel reduction from SSP to finite-horizon MDPs. To that end, we provide an algorithm for the finite-horizon setting whose leading term in the regret depends polynomially on the expected cost of the optimal policy and only logarithmically on the horizon."}}
{"id": "yzfzwyHSIR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Oracle-Efficient Reinforcement Learning in Factored MDPs with Unknown Structure", "abstract": "We study regret minimization in non-episodic factored Markov decision processes (FMDPs), where all existing algorithms make the strong assumption that the factored structure of the FMDP is known to the learner in advance. In this paper, we provide the first algorithm that learns the structure of the FMDP while minimizing the regret. Our algorithm is based on the optimism in face of uncertainty principle, combined with a simple statistical method for structure learning, and can be implemented efficiently given oracle-access to an FMDP planner. Moreover, we give a variant of our algorithm that remains efficient even when the oracle is limited to non-factored actions, which is the case with almost all existing approximate planners. Finally, we leverage our techniques to prove a novel lower bound for the known structure case, closing the gap to the regret bound of Chen et al. [2021]."}}
{"id": "rWrIFXTecas", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimistic Policy Optimization with Bandit Feedback", "abstract": "Policy optimization methods are one of the most widely used classes of Reinforcement Learning (RL) algorithms. Yet, so far, such methods have been mostly analyzed from an optimization perspective, ..."}}
{"id": "XKQhsZloZ5I", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adversarial Stochastic Shortest Path", "abstract": "Stochastic shortest path (SSP) is a well-known problem in planning and control, in which an agent has to reach a goal state in minimum total expected cost. In this paper we present the adversarial SSP model that also accounts for adversarial changes in the costs over time, while the underlying transition function remains unchanged. Formally, an agent interacts with an SSP environment for $K$ episodes, the cost function changes arbitrarily between episodes, and the transitions are unknown to the agent. We develop the first algorithms for adversarial SSPs and prove high probability regret bounds of $\\widetilde O (\\sqrt{K})$ assuming all costs are strictly positive, and $\\widetilde O (K^{3/4})$ in the general case. We are the first to consider this natural setting of adversarial SSP and obtain sub-linear regret for it."}}
{"id": "SU7ASf9uPGO", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Adversarial Markov Decision Processes with Delayed Feedback", "abstract": "Reinforcement learning typically assumes that agents observe feedback for their actions immediately, but in many real-world applications (like recommendation systems) feedback is observed in delay. This paper studies online learning in episodic Markov decision processes (MDPs) with unknown transitions, adversarially changing costs and unrestricted delayed feedback. That is, the costs and trajectory of episode $k$ are revealed to the learner only in the end of episode $k + d^k$, where the delays $d^k$ are neither identical nor bounded, and are chosen by an oblivious adversary. We present novel algorithms based on policy optimization that achieve near-optimal high-probability regret of $\\sqrt{K + D}$ under full-information feedback, where $K$ is the number of episodes and $D = \\sum_{k} d^k$ is the total delay. Under bandit feedback, we prove similar $\\sqrt{K + D}$ regret assuming the costs are stochastic, and $(K + D)^{2/3}$ regret in the general case. We are the first to consider regret minimization in the important setting of MDPs with delayed feedback."}}
{"id": "A1XjCPCeELe", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimistic Policy Optimization with Bandit Feedback", "abstract": "Policy optimization methods are one of the most widely used classes of Reinforcement Learning (RL) algorithms. Yet, so far, such methods have been mostly analyzed from an optimization perspective, without addressing the problem of exploration, or by making strong assumptions on the interaction with the environment. In this paper we consider model-based RL in the tabular finite-horizon MDP setting with unknown transitions and bandit feedback. For this setting, we propose an optimistic trust region policy optimization (TRPO) algorithm for which we establish $\\tilde O(\\sqrt{S^2 A H^4 K})$ regret for stochastic rewards. Furthermore, we prove $\\tilde O( \\sqrt{ S^2 A H^4 } K^{2/3} ) $ regret for adversarial rewards. Interestingly, this result matches previous bounds derived for the bandit feedback case, yet with known transitions. To the best of our knowledge, the two results are the first sub-linear regret bounds obtained for policy optimization algorithms with unknown transitions and bandit feedback."}}
{"id": "-2_D_0x_-k", "cdate": 1577836800000, "mdate": null, "content": {"title": "Near-optimal Regret Bounds for Stochastic Shortest Path", "abstract": "Stochastic shortest path (SSP) is a well-known problem in planning and control, in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the proble..."}}
