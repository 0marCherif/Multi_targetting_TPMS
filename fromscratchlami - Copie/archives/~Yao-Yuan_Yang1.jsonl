{"id": "T7F9PXUxXTc", "cdate": 1668734791614, "mdate": null, "content": {"title": "What You See is What You Get: Principled Deep Learning via Distributional Generalization", "abstract": "Having similar behavior at train-time and test-time---what we call a ``What You See Is What You Get (WYSIWYG)'' property---is desirable in machine learning. However, models trained with standard stochastic gradient descent (SGD) are known to not capture it. Their behaviors such as subgroup performance, or adversarial robustness can be very different during training and testing. We show that Differentially-Private (DP) training provably ensures the high-level WYSIWYG property, which we quantify using a notion of Distributional Generalization (DG). Applying this connection, we introduce new conceptual tools for designing deep-learning methods by reducing generalization concerns to optimization ones: to mitigate unwanted behavior at test time, it is provably sufficient to mitigate this behavior on the train datasets. By applying this novel design principle, which bypasses ``pathologies'' of SGD, we construct simple algorithms that are competitive with SOTA in several distributional robustness applications, significantly improve the privacy vs. disparate impact tradeoff of DP-SGD, and mitigate robust overfitting in adversarial training. Finally, we also improve on known theoretical bounds relating DP, stability, and distributional generalization."}}
{"id": "lrzX-rNuRvw", "cdate": 1663849939719, "mdate": null, "content": {"title": "Understanding Rare Spurious Correlations in Neural Networks", "abstract": "Neural networks are known to use spurious correlations such as background information for classification. While prior work has looked at spurious correlations that are widespread in the training data, in this work, we investigate how sensitive neural networks are to $rare$ spurious correlations, which may be harder to detect and correct, and may lead to privacy leaks. We introduce spurious patterns correlated with a fixed class to a few training examples and find that it takes only a handful of such examples for the network to learn the correlation. Furthermore, these rare spurious correlations also impact accuracy and privacy. We empirically and theoretically analyze different factors involved in rare spurious correlations and propose mitigation methods accordingly. Specifically, we observe that $\\ell_2$ regularization and adding Gaussian noise to inputs can reduce the undesirable effects."}}
{"id": "iHU9Ze_5X7n", "cdate": 1653750179081, "mdate": null, "content": {"title": "Understanding Rare Spurious Correlations in Neural Networks", "abstract": "Neural networks are known to use spurious correlations such as background information for classification. While prior work has looked at spurious correlations that are widespread in the training data, in this work, we investigate how sensitive neural networks are to rare spurious correlations, which may be harder to detect and correct, and may lead to privacy leaks.  We introduce spurious patterns correlated with a fixed class to a few training examples and find that it takes only a handful of such examples for the network to learn the correlation. Furthermore, these rare spurious correlations also impact accuracy and privacy. We empirically and theoretically analyze different factors involved in rare spurious correlations and propose mitigation methods accordingly. Specifically, we observe that $\\ell_2$ regularization and adding Gaussian noise to inputs can reduce the undesirable effects."}}
{"id": "g05fHAvNeXx", "cdate": 1652737580204, "mdate": null, "content": {"title": "What You See is What You Get: Principled Deep Learning via Distributional Generalization", "abstract": "Having similar behavior at training time and test time\u2014what we call a \u201cWhat You See Is What You Get\u201d (WYSIWYG) property\u2014is desirable in machine learning. Models trained with standard stochastic gradient descent (SGD), however, do not necessarily have this property, as their complex behaviors such as robustness or subgroup performance can differ drastically between training and test time. In contrast, we show that Differentially-Private (DP) training provably ensures the high-level WYSIWYG property, which we quantify using a notion of distributional generalization. Applying this connection, we introduce new conceptual tools for designing deep-learning methods by reducing generalization concerns to optimization ones: to mitigate unwanted behavior at test time, it is provably sufficient to mitigate this behavior on the training data. By applying this novel design principle, which bypasses \u201cpathologies\u201d of SGD, we construct simple algorithms that are competitive with SOTA in several distributional-robustness applications, significantly improve the privacy vs. disparate impact trade-off of DP-SGD, and mitigate robust overfitting in adversarial training. Finally, we also improve on theoretical bounds relating DP, stability, and distributional generalization."}}
{"id": "kdHQ0Q3mngn", "cdate": 1640995200000, "mdate": 1681707324336, "content": {"title": "A Principled Approach to Trustworthy Machine Learning", "abstract": "Author(s): Yang, Yao-Yuan | Advisor(s): Chaudhuri, Kamalika | Abstract: Traditional machine learning operates under the assumption that training and testing data are drawn independently from the same distribution. However, this assumption does not always hold. In this thesis, we take a principled approach toward three major challenges in settings where this assumption fails to hold -- i) robustness to adversarial inputs, ii) handling unseen examples during test time, and iii) avoiding learning spurious correlations.We study what happens when small adversarial perturbations are made to the inputs. We investigate neural networks, which frequently operate on natural datasets such as images. We find that in these datasets, differently labeled examples are often far away from each other. Under this condition, we prove that a perfectly robust and accurate classifier exists, suggesting that there is no intrinsic tradeoff between adversarial robustness and accuracy on these datasets. Next, we look into non-parametric classifiers, which operate on datasets without such a separation condition. We design a defense algorithm -- adversarial pruning -- that successfully improves the robustness of many non-parametric classifiers, including k-nearest neighbor, decision tree, and random forest. Adversarial pruning can also be seen as a finite sample approximation to the classifier with the highest accuracy under robustness constraints. Finally, we connect robustness and interpretability on decision trees by designing an algorithm that is guaranteed to achieve good accuracy, robustness, and interpretability when the data is linearly separated.Next, we explore what would happen if examples that do not belong in the training set, i.e., out-of-distribution (OOD) examples, are given to a model as input during testing time. We identify that neural networks tend to predict OOD inputs as the label of the closest training example, and adversarially robust networks amplify this behavior. These findings can shed light on many long-standing questions surrounding generalization, including how adversarial robust training methods change the decision boundary, why an adversarially robust network performs better on corrupted data, and when OOD examples can be hard to detect.Finally, we investigate the case where a few examples of a certain class with spurious features are presented during training. We find that merely three of these spurious examples can cause the network to learn a spurious correlation. Our result suggests that neural networks are highly sensitive to small amounts of training data. Although this feature enables efficient learning, it also results in rapid learning of spurious correlation."}}
{"id": "JcnBf6MFh4", "cdate": 1640995200000, "mdate": 1681707324184, "content": {"title": "Understanding Rare Spurious Correlations in Neural Networks", "abstract": "Neural networks are known to use spurious correlations such as background information for classification. While prior work has looked at spurious correlations that are widespread in the training data, in this work, we investigate how sensitive neural networks are to rare spurious correlations, which may be harder to detect and correct, and may lead to privacy leaks. We introduce spurious patterns correlated with a fixed class to a few training examples and find that it takes only a handful of such examples for the network to learn the correlation. Furthermore, these rare spurious correlations also impact accuracy and privacy. We empirically and theoretically analyze different factors involved in rare spurious correlations and propose mitigation methods accordingly. Specifically, we observe that $\\ell_2$ regularization and adding Gaussian noise to inputs can reduce the undesirable effects. Code available at https://github.com/yangarbiter/rare-spurious-correlation."}}
{"id": "JcbGjofsuc6", "cdate": 1640995200000, "mdate": 1681484262120, "content": {"title": "Torchaudio: Building Blocks for Audio and Speech Processing", "abstract": ""}}
{"id": "IKCvvIoKcu", "cdate": 1640995200000, "mdate": 1681492861016, "content": {"title": "What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning", "abstract": ""}}
{"id": "mlMpWEImriq", "cdate": 1609459200000, "mdate": 1681707324349, "content": {"title": "Hide and Seek: Choices of Virtual Backgrounds in Video Chats and Their Effects on Perception", "abstract": "In two studies, we investigate how users choose virtual backgrounds and how these backgrounds influence viewers' impressions. In Study 1, we created a web prototype allowing users to apply different virtual backgrounds to their camera views and asked users to select backgrounds that they believed would change viewers' perceptions of their personality traits. In Study 2, we then applied virtual backgrounds picked by participants in Study 1 to a subset of videos drawn from the First Impression Dataset. We then ran a series of three online experiments on Amazon Mechanical Turk (MTurk) to compare participants' personality trait ratings for subjects (1) with the selected virtual backgrounds, (2) with the original video backgrounds, and (3) with a gray screen as a background. The selected virtual backgrounds did not change the personality trait ratings in the intended direction. Instead, virtual background use of any kind results in a consistent \"muting effect\" that mitigates very high or low ratings (i.e., compressing ratings to the mean level) compared to the ratings of the video with the original background."}}
{"id": "YSCe9eltvf", "cdate": 1609459200000, "mdate": 1681707324217, "content": {"title": "Connecting Interpretability and Robustness in Decision Trees through Separation", "abstract": "Recent research has recognized interpretability and robustness as essential properties of trustworthy classification. Curiously, a connection between robustness and interpretability was empirically observed, but the theoretical reasoning behind it remained elusive. In this paper, we rigorously investigate this connection. Specifically, we focus on interpretation using decision trees and robustness to $l_{\\infty}$-perturbation. Previous works defined the notion of $r$-separation as a sufficient condition for robustness. We prove upper and lower bounds on the tree size in case the data is $r$-separated. We then show that a tighter bound on the size is possible when the data is linearly separated. We provide the first algorithm with provable guarantees both on robustness, interpretability, and accuracy in the context of decision trees. Experiments confirm that our algorithm yields classifiers that are both interpretable and robust and have high accuracy. The code for the experiments is available at https://github.com/yangarbiter/interpretable-robust-trees ."}}
