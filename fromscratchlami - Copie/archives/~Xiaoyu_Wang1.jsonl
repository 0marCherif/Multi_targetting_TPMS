{"id": "S5ikb8puqfl", "cdate": 1672531200000, "mdate": 1683938596760, "content": {"title": "BiTGAN: bilateral generative adversarial networks for Chinese ink wash painting style transfer", "abstract": ""}}
{"id": "KDE_RKXXrPC", "cdate": 1672531200000, "mdate": 1683938597040, "content": {"title": "Few-shot Face Image Translation via GAN Prior Distillation", "abstract": "Face image translation has made notable progress in recent years. However, when training on limited data, the performance of existing approaches significantly declines. Although some studies have attempted to tackle this problem, they either failed to achieve the few-shot setting (less than 10) or can only get suboptimal results. In this paper, we propose GAN Prior Distillation (GPD) to enable effective few-shot face image translation. GPD contains two models: a teacher network with GAN Prior and a student network that fulfills end-to-end translation. Specifically, we adapt the teacher network trained on large-scale data in the source domain to the target domain with only a few samples, where it can learn the target domain's knowledge. Then, we can achieve few-shot augmentation by generating source domain and target domain images simultaneously with the same latent codes. We propose an anchor-based knowledge distillation module that can fully use the difference between the training and the augmented data to distill the knowledge of the teacher network into the student network. The trained student network achieves excellent generalization performance with the absorption of additional knowledge. Qualitative and quantitative experiments demonstrate that our method achieves superior results than state-of-the-art approaches in a few-shot setting."}}
{"id": "DkEZbApJjT", "cdate": 1672531200000, "mdate": 1683938596760, "content": {"title": "An Efficient Transformer Based on Global and Local Self-Attention for Face Photo-Sketch Synthesis", "abstract": "Face photo-sketch synthesis tasks have been dominated by convolutional neural networks (CNNs), especially CNN-based generative adversarial networks (GANs), because of their strong texture modeling capabilities and thus their ability to generate more realistic face photos/sketches beyond traditional methods. However, due to CNNs\u2019 locality and spatial invariance properties, there have weaknesses in capturing the global and structural information which are extremely important for face images. Inspired by the recent phenomenal success of the Transformer in vision tasks, we propose replacing CNNs with Transformers that are able to model long-range dependencies to synthesize more structured and realistic face images. However, the existing vision Transformers are mainly designed for high-level vision tasks and lack the dense prediction ability to generate high resolution images due to the quadratic computational complexity of their self-attention mechanism. In addition, the original Transformer is not capable of modeling local correlations which is an important skill for image generation. To address these challenges, we propose two types of memory-friendly Transformer encoders, one for processing local correlations via local self-attention and another for modeling global information via global self-attention. By integrating the two proposed Transformer encoders, we present an efficient GL-Transformer for face photo-sketch synthesis, which can synthesize realistic face photo/sketch images from coarse to fine. Extensive experiments demonstrate that our model achieves a comparable or better performance beyond the state-of-the-art CNN-based methods both qualitatively and quantitatively."}}
{"id": "AD_NnvViZiY", "cdate": 1649643749817, "mdate": 1649643749817, "content": {"title": "Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin", "abstract": "Only parts of unlabeled data are selected to train mod\u0002els for most semi-supervised learning methods, whose confi\u0002dence scores are usually higher than the pre-defined thresh\u0002old (i.e., the confidence margin). We argue that the recog\u0002nition performance should be further improved by making\nfull use of all unlabeled data. In this paper, we learn an\nAdaptive Confidence Margin (Ada-CM) to fully leverage all\nunlabeled data for semi-supervised deep facial expression\nrecognition. All unlabeled samples are partitioned into two\nsubsets by comparing their confidence scores with the adap\u0002tively learned confidence margin at each training epoch:\n(1) subset I including samples whose confidence scores are\nno lower than the margin; (2) subset II including samples\nwhose confidence scores are lower than the margin. For\nsamples in subset I, we constrain their predictions to match\npseudo labels. Meanwhile, samples in subset II participate\nin the feature-level contrastive objective to learn effective\nfacial expression features. We extensively evaluate Ada\u0002CM on four challenging datasets, showing that our method\nachieves state-of-the-art performance, especially surpass\u0002ing fully-supervised baselines in a semi-supervised man\u0002ner. Ablation study further proves the effectiveness of our\nmethod. The code will be publicly available."}}
{"id": "rrSlRgTr8xc", "cdate": 1645792501736, "mdate": null, "content": {"title": "ALBench: A Framework for Evaluating Active Learning in Object Detection", "abstract": "Active learning is an important technology for automated machine learning systems. In contrast to Neural Architecture Search (NAS) which aims at automating neural network architecture design, active learning aims at automating training data selection process. It is especially critical for training a long-tailed task, in which positive samples are sparsely distributed. Active learning alleviates the expensive data annotation issue through incrementally training models powered with efficient data selection. Instead of annotating all unlabeled samples, it iteratively selects and annotates the most valuable samples. Active learning has been popular in image classification, but has not been fully explored in object detection. Most of current works on object detection are evaluated with different settings, making it difficult to fairly compare their performance. To facilitate the research in this field, this paper contributes an active learning benchmark framework named as ALBench for evaluating active learning in object detection. Developed on an automatic deep model training system, this ALBench framework is easy-to-use, compatible with different active learning algorithms, and ensures the same training and testing protocols. We hope this automated benchmark system helps researchers to easily reproduce literature's performance and have fair comparisons with prior arts."}}
{"id": "y-PrxLZWksS", "cdate": 1640995200000, "mdate": 1666604352344, "content": {"title": "Semi-parametric Makeup Transfer via Semantic-aware Correspondence", "abstract": "The large discrepancy between the source non-makeup image and the reference makeup image is one of the key challenges in makeup transfer. Conventional approaches for makeup transfer either learn disentangled representation or perform pixel-wise correspondence in a parametric way between two images. We argue that non-parametric techniques have a high potential for addressing the pose, expression, and occlusion discrepancies. To this end, this paper proposes a \\textbf{S}emi-\\textbf{p}arametric \\textbf{M}akeup \\textbf{T}ransfer (SpMT) method, which combines the reciprocal strengths of non-parametric and parametric mechanisms. The non-parametric component is a novel \\textbf{S}emantic-\\textbf{a}ware \\textbf{C}orrespondence (SaC) module that explicitly reconstructs content representation with makeup representation under the strong constraint of component semantics. The reconstructed representation is desired to preserve the spatial and identity information of the source image while \"wearing\" the makeup of the reference image. The output image is synthesized via a parametric decoder that draws on the reconstructed representation. Extensive experiments demonstrate the superiority of our method in terms of visual quality, robustness, and flexibility. Code and pre-trained model are available at \\url{https://github.com/AnonymScholar/SpMT."}}
{"id": "xDlTQaMW-Hs", "cdate": 1640995200000, "mdate": 1668607583796, "content": {"title": "Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin", "abstract": "Only parts of unlabeled data are selected to train models for most semi-supervised learning methods, whose confidence scores are usually higher than the pre-defined threshold (i.e., the confidence margin). We argue that the recognition performance should be further improved by making full use of all unlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM) to fully leverage all unlabeled data for semi-supervised deep facial expression recognition. All unlabeled samples are partitioned into two subsets by comparing their confidence scores with the adaptively learned confidence margin at each training epoch: (1) subset I including samples whose confidence scores are no lower than the margin; (2) subset II including samples whose confidence scores are lower than the margin. For samples in subset I, we constrain their predictions to match pseudo labels. Meanwhile, samples in subset II participate in the feature-level contrastive objective to learn effective facial expression features. We extensively evaluate Ada-CM on four challenging datasets, showing that our method achieves state-of-the-art performance, especially surpassing fully-supervised baselines in a semi-supervised manner. Ablation study further proves the effectiveness of our method. The source code is available at https://github.com/hangyu94/Ada-CM."}}
{"id": "safNxypi_Sp", "cdate": 1640995200000, "mdate": 1668763618492, "content": {"title": "Improving Adversarial Robustness via Mutual Information Estimation", "abstract": "Deep neural networks (DNNs) are found to be vulnerable to adversarial noise. They are typically misled by adversarial samples to make wrong predictions. To alleviate this negative effect, in this p..."}}
{"id": "iKpHmTF7e4", "cdate": 1640995200000, "mdate": 1668607583783, "content": {"title": "Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin", "abstract": "Only parts of unlabeled data are selected to train models for most semi-supervised learning methods, whose confidence scores are usually higher than the pre-defined threshold (i.e., the confidence margin). We argue that the recognition performance should be further improved by making full use of all unlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM) to fully leverage all unlabeled data for semi-supervised deep facial expression recognition. All unlabeled samples are partitioned into two subsets by comparing their confidence scores with the adaptively learned confidence margin at each training epoch: (1) subset I including samples whose confidence scores are no lower than the margin; (2) subset II including samples whose confidence scores are lower than the margin. For samples in subset I, we constrain their predictions to match pseudo labels. Meanwhile, samples in subset II participate in the feature-level contrastive objective to learn effective facial expression features. We extensively evaluate Ada-CM on four challenging datasets, showing that our method achieves state-of-the-art performance, especially surpassing fully-supervised baselines in a semi-supervised manner. Ablation study further proves the effectiveness of our method. The source code is available at https://github.com/hangyu94/Ada-CM."}}
{"id": "gzVH888foi", "cdate": 1640995200000, "mdate": 1663108834072, "content": {"title": "ALBench: A Framework for Evaluating Active Learning in Object Detection", "abstract": "Active learning is an important technology for automated machine learning systems. In contrast to Neural Architecture Search (NAS) which aims at automating neural network architecture design, active learning aims at automating training data selection. It is especially critical for training a long-tailed task, in which positive samples are sparsely distributed. Active learning alleviates the expensive data annotation issue through incrementally training models powered with efficient data selection. Instead of annotating all unlabeled samples, it iteratively selects and annotates the most valuable samples. Active learning has been popular in image classification, but has not been fully explored in object detection. Most of current approaches on object detection are evaluated with different settings, making it difficult to fairly compare their performance. To facilitate the research in this field, this paper contributes an active learning benchmark framework named as ALBench for evaluating active learning in object detection. Developed on an automatic deep model training system, this ALBench framework is easy-to-use, compatible with different active learning algorithms, and ensures the same training and testing protocols. We hope this automated benchmark system help researchers to easily reproduce literature's performance and have objective comparisons with prior arts. The code will be release through Github."}}
