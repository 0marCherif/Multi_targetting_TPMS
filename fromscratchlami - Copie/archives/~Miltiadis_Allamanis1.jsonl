{"id": "ri9z4oHkl4", "cdate": 1683619767016, "mdate": 1683619767016, "content": {"title": "CoRGi: Content-Rich Graph Neural Networks with Attention", "abstract": "Graph representations of a target domain often project it to a set of entities (nodes) and their relations (edges). However, such projections often miss important and rich information. For example, in graph representations used in missing value imputation, items --- represented as nodes --- may contain rich textual information. However, when processing graphs with graph neural networks (GNN), such information is either ignored or summarized into a single vector representation used to initialize the GNN. Towards addressing this, we present CoRGi, a GNN that considers the rich data within nodes in the context of their neighbors. This is achieved by endowing CoRGi's message passing with a personalized attention mechanism over the content of each node. This way, CoRGi assigns user-item-specific attention scores with respect to the words that appear in an item's content. We evaluate CoRGi on two edge-value prediction tasks and show that CoRGi is better at making edge-value predictions over existing methods, especially on sparse regions of the graph."}}
{"id": "rS3UdBTsZx", "cdate": 1680425457842, "mdate": 1680425457842, "content": {"title": "Flexeme: Untangling Commits Using Lexical Flows", "abstract": "Today, most developers bundle changes into commits that they sub-\nmit to a shared code repository. Tangled commits intermix distinct\nconcerns, such as a bug fix and a new feature. They cause issues\nfor developers, reviewers, and researchers alike: they restrict the\nusability of tools such as git bisect, make patch comprehension\nmore difficult, and force researchers who mine software reposi-\ntories to contend with noise. We present a novel data structure,\nthe \ud835\udeff-NFG, a multiversion Program Dependency Graph augmented\nwith name flows. A \ud835\udeff-NFG directly and simultaneously encodes dif-\nferent program versions, thereby capturing commits, and annotates\ndata flow edges with the names/lexemes that flow across them. Our\ntechnique, Flexeme, builds a \ud835\udeff-NFG from commits, then applies\nAgglomerative Clustering using Graph Similarity to that \ud835\udeff-NFG to\nuntangle its commits. At the untangling task on a C# corpus, our\nimplementation, Heddle, improves the state-of-the-art on accuracy\nby 0.14, achieving 0.81, in a fraction of the time: Heddle is 32 times\nfaster than the previous state-of-the-art."}}
{"id": "6DPVXzjnbDK", "cdate": 1664815579658, "mdate": null, "content": {"title": "Deep End-to-end Causal Inference", "abstract": "Causal inference is essential for data-driven decision making across domains such as business engagement, medical treatment and policy making.  However, research on causal discovery has evolved separately from causal inference, preventing straightforward combination of methods from both fields. In this work, we develop Deep End-to-end Causal Inference (DECI), a non-linear additive noise model with neural network functional relationships that takes in observational data and can perform both causal discovery and inference, including conditional average treatment effect (CATE) estimation. We provide a theoretical guarantee that DECI can asymptotically recover the ground truth causal graph and treatment effects when correctly specified. Our results show the competitive performance of DECI when compared to relevant baselines for both causal discovery and (C)ATE estimation in over a thousand experiments on both synthetic datasets and causal machine learning benchmarks."}}
{"id": "4rm6tzBjChe", "cdate": 1652737535614, "mdate": null, "content": {"title": "Simultaneous Missing Value Imputation and Structure Learning with Groups", "abstract": "Learning structures between groups of variables from data with missing values is an important task in the real world, yet difficult to solve. One typical scenario is discovering the structure among topics in the education domain to identify learning pathways. Here, the observations are student performances for questions under each topic which contain missing values. However, most existing methods focus on learning structures between a few individual variables from the complete data. In this work, we propose VISL, a novel scalable structure learning approach that can simultaneously infer structures between groups of variables under missing data and perform missing value imputations with deep learning. Particularly, we propose a generative model with a structured latent space and a graph neural network-based architecture, scaling to a large number of variables. Empirically, we conduct extensive experiments on synthetic, semi-synthetic, and real-world education data sets. We show improved performances on both imputation and structure learning accuracy compared to popular and recent approaches. "}}
{"id": "kHeotl7q9dU", "cdate": 1652737394493, "mdate": null, "content": {"title": "NS3: Neuro-symbolic Semantic Code Search", "abstract": "Semantic code search is the task of retrieving a code snippet given a textual description of its functionality. Recent work has been focused on using similarity metrics between neural embeddings of text and code. However, current language models are known to struggle with longer, compositional sentences, and multi-step reasoning. To overcome this limitation, we propose supplementing the query sentence with a layout of its semantic structure. The semantic layout is used to break down the final reasoning decision into a series of lower-level decisions. We use a Neural Module Network architecture to implement this idea. We compare our model - $NS^3$  (Neuro-Symbolic Semantic Search) - to a number of baselines, including state-of-the-art semantic code retrieval methods, such as CodeBERT, CuBERT and GraphCodeBERT, and evaluate on two datasets - Code Search Net (CSN) and Code Search and Question Answering (CoSQA). On these datasets, we demonstrate that our approach results in higher performance. We also perform additional studies to show the effectiveness of our modular design when handling compositional queries.  "}}
{"id": "P-6_A5xhX_Z", "cdate": 1652217086746, "mdate": 1652217086746, "content": {"title": "Learning to Represent Programs with Graphs", "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.\nIn this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects. "}}
{"id": "NZGXKoq2LZ3", "cdate": 1652217031519, "mdate": 1652217031519, "content": {"title": "Graph Neural Networks on Program Analysis", "abstract": "Program analysis aims to determine if a program\u2019s behavior complies with some specification. Commonly, program analyses need to be defined and tuned by humans. This is a costly process. Recently, machine learning methods have shown promise for probabilistically realizing a wide range of program analyses. Given the structured nature of programs, and the commonality of graph representations in program analysis, graph neural networks (GNN) offer an elegant way to represent, learn, and reason about programs and are commonly used in machine learning-based program analyses. This chapter discusses the use of GNNs for program analysis, highlighting two practical use cases: variable misuse detection and type inference."}}
{"id": "b76APFXz5yU", "cdate": 1652216972864, "mdate": 1652216972864, "content": {"title": "Self-Supervised Bug Detection and Repair", "abstract": "Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software. "}}
{"id": "kZW1s1JizFE", "cdate": 1652216871206, "mdate": 1652216871206, "content": {"title": "HEAT: Hyperedge Attention Networks", "abstract": "Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code.\n\nIn this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality. "}}
{"id": "rubeJ2ObyWc", "cdate": 1646364838754, "mdate": null, "content": {"title": "NS3: Neuro-Symbolic Semantic Code Search", "abstract": "Semantic code search is the task of retrieving a code snippet given a textual description of its functionality. Recent work has been focused on using similarity metrics between neural embeddings of text and code. However, current language models are known to struggle with longer, compositional sentences, and multi-step reasoning.  To overcome this limitation, we propose supplementing the query sentence with a layout of its semantic structure. The semantic layout is used to break down the final reasoning decision into a series of lower-level decisions. We use a Neural Module Network architecture to implement this idea. We compare our model - $NS^3$ (Neuro-Symbolic Semantic Search) - to a number of baselines, including state-of-the-art semantic code retrieval methods, such as CodeBERT, CuBERT and GraphCodeBERT, and evaluate on two datasets - Code Search Net (CSN) and Code Search and Question Answering (CoSQA). On these datasets, we demonstrate that our approach results in higher performance. We also perform additional studies to show the effectiveness of our modular design when handling compositional queries.  "}}
