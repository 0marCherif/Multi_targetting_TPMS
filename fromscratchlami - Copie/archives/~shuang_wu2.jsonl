{"id": "E2vL12SwO1", "cdate": 1686324882907, "mdate": null, "content": {"title": "PreCo: Enhancing Generalization in Co-Design of Modular Soft Robots via Brain-Body Pre-Training", "abstract": "Brain-body co-design, which involves the collaborative design of control strategies and morphologies, has emerged as a promising approach to enhance a robot's adaptability to its environment. However, the conventional co-design process often starts from scratch, lacking the utilization of prior knowledge. This can result in time-consuming and costly endeavors. In this paper, we present PreCo, a novel methodology that efficiently integrates brain-body pre-training into the co-design process of modular soft robots. PreCo is based on the insight of embedding co-design principles into models, achieved by pre-training a universal co-design policy on a diverse set of tasks. This pre-trained co-designer is utilized to generate initial designs and control policies, which are then fine-tuned for specific co-design tasks. Through experiments on a modular soft robot system, our method demonstrates zero-shot generalization to unseen co-design tasks, facilitating few-shot adaptation while significantly reducing the number of policy iterations required."}}
{"id": "r9fX833CsuN", "cdate": 1663850574491, "mdate": null, "content": {"title": "Curriculum-based Co-design of Morphology and Control of Voxel-based Soft Robots", "abstract": "Co-design of morphology and control of a Voxel-based Soft Robot (VSR) is challenging due to the notorious bi-level optimization. In this paper, we present a Curriculum-based Co-design (CuCo) method for learning to design and control VSRs through an easy-to-difficult process. Specifically, we expand the design space from a small size to the target size gradually through a predefined curriculum. At each learning stage of the curriculum, we use reinforcement learning to simultaneously train the design policy and the control policy, which is enabled by incorporating the design process into the environment and using differentiable policy representations. The converged morphology and the learned policies from last stage are inherited and then serve as the starting point for the next stage. In empirical studies, we show that CuCo is more efficient in creating larger robots with better performance by reusing the practical design and control patterns learned within each stage, in comparison to prior approaches that learn from scratch in the space of target size."}}
{"id": "bLmSMXbqXr", "cdate": 1663850262171, "mdate": null, "content": {"title": "Quality-Similar Diversity via Population Based Reinforcement Learning", "abstract": "Diversity is a growing research topic in Reinforcement Learning (RL). Previous research on diversity has mainly focused on promoting diversity to encourage exploration and thereby improve quality (the cumulative reward), maximizing diversity subject to quality constraints, or jointly maximizing quality and diversity, known as the quality-diversity problem. In this work, we present the quality-similar diversity problem that features diversity among policies of similar qualities. In contrast to task-agnostic diversity, we focus on task-specific diversity defined by a set of user-specified Behavior Descriptors (BDs). A BD is a scalar function of a trajectory (e.g., the fire action rate for an Atari game), which delivers the type of diversity the user prefers. To derive the gradient of the user-specified diversity with respect to a policy, which is not trivially available, we introduce a set of BD estimators and connect it with the classical policy gradient theorem. Based on the diversity gradient, we develop a population-based RL algorithm to adaptively and efficiently optimize the population diversity at multiple quality levels throughout training. Extensive results on MuJoCo and Atari demonstrate that our algorithm significantly outperforms previous methods in terms of generating user-specified diverse policies across different quality levels."}}
{"id": "zycJDAgQDb", "cdate": 1640995200000, "mdate": 1682325599941, "content": {"title": "Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game", "abstract": "The deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-specific abstractions to deal with large-scale games. Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-information game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modifies the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modification is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justified as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong."}}
{"id": "1Pk5CQlA-QZ", "cdate": 1640995200000, "mdate": 1682325599937, "content": {"title": "Greedy when Sure and Conservative when Uncertain about the Opponents", "abstract": "We develop a new approach, named Greedy when Sure and Conservative when Uncertain (GSCU), to competing online against unknown and nonstationary opponents. GSCU improves in four aspects: 1) introduc..."}}
{"id": "14v4fp9U1P", "cdate": 1640995200000, "mdate": 1682325599766, "content": {"title": "Speedup Training Artificial Intelligence for Mahjong via Reward Variance Reduction", "abstract": "Despite significant breakthroughs in developing gaming artificial intelligence (AI), Mahjong remains quite challenging as a popular multi-player imperfect information game. Compared with games such as Go and Texas Hold\u2019em, Mahjong has much more invisible information, unfixed game order, and a complicated scoring system, resulting in high randomness and variance of the rewarding signals during the reinforcement learning process. This paper presents a Mahjong AI by introducing Reward Variance Reduction (RVR) into a new self-play deep reinforcement learning algorithm. RVR handles the invisibility via a relative value network which leverages the global information to guide the model to converge to the optimal strategy under an oracle with perfect information. Moreover, RVR improves the training stability using an expected reward network to adapt to the complex, dynamic, and highly stochastic reward environment. Extensive experimental results show that RVR significantly reduces the variance in Mahjong AI training and improves the model performance. After only three days of self-play training on a single server with 8 GPUs, RVR defeats 62.5% opponents on the Botzone platform."}}
{"id": "DTXZqTNV5nW", "cdate": 1632875520000, "mdate": null, "content": {"title": "Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game", "abstract": "The deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-specific abstractions to deal with large-scale games.  Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-information game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modifies the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modification is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justified as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong."}}
{"id": "8Jiy_glofqI", "cdate": 1609459200000, "mdate": 1683615622410, "content": {"title": "Hybrid neural state machine for neural network", "abstract": "The integration of computer-science-oriented and neuroscience-oriented approaches is believed to be a promising way for the development of artificial general intelligence (AGI). Recently, a hybrid Tianjic chip that integrates both approaches has been reported, providing a general platform to facilitate the research of AGI. The control algorithm for handling various neural networks is the key to this platform; however, it is still primitive. In this work, we propose a hybrid neural state machine (H-NSM) framework that can efficiently cooperate with artificial neural networks and spiking neural networks and control the workflows to accomplish complex tasks. The H-NSM receives input from different types of networks, makes decisions according to the fusing of various information, and sends control signals to the sub-network or actuator. The H-NSM can be trained to adapt to context-aware tasks or sequential tasks, thereby improving system robustness. The training algorithm works correctly even if only 50% of the forced state information is provided. It achieved performance comparable to the optimum algorithm on the Tower of Hanoi task and achieved multiple tasks control on a self-driving bicycle. After only 50 training epochs, the transfer accuracy reaches 100% in the test case. It proves that H-NSM has the potential to advance control logic for hybrid systems, paving the way for designing complex intelligent systems and facilitating the research towards AGI."}}
{"id": "wEjXkZyRZsT", "cdate": 1577836800000, "mdate": 1683615622373, "content": {"title": "SSM: a high-performance scheme for in situ training of imprecise memristor neural networks", "abstract": ""}}
{"id": "vSvhGQCVcX", "cdate": 1577836800000, "mdate": 1682346440001, "content": {"title": "Training high-performance and large-scale deep neural networks with full 8-bit integers", "abstract": ""}}
