{"id": "whVfxP0Z5-", "cdate": 1672531200000, "mdate": 1682348007200, "content": {"title": "High-Performance Row Pattern Recognition Using Joins", "abstract": ""}}
{"id": "UWt1pnEFedh", "cdate": 1640995200000, "mdate": 1682348007168, "content": {"title": "TSEXPLAIN: Explaining Aggregated Time Series by Surfacing Evolving Contributors", "abstract": "Aggregated time series are generated effortlessly everywhere, e.g., \"total confirmed covid-19 cases since 2019\" and \"total liquor sales over time.\" Understanding \"how\" and \"why\" these key performance indicators (KPI) evolve over time is critical to making data-informed decisions. Existing explanation engines focus on explaining one aggregated value or the difference between two relations. However, this falls short of explaining KPIs' continuous changes over time. Motivated by this, we propose TSEXPLAIN, a system that explains aggregated time series by surfacing the underlying evolving top contributors. Under the hood, we leverage prior works on two-relations diff as a building block and formulate a K-Segmentation problem to segment the time series such that each segment after segmentation shares consistent explanations, i.e., contributors. To quantify consistency in each segment, we propose a novel within-segment variance design that is explanation-aware; to derive the optimal K-Segmentation scheme, we develop an efficient dynamic programming algorithm. Experiments on synthetic and real-world datasets show that our explanation-aware segmentation can effectively identify evolving explanations for aggregated time series and outperform explanation-agnostic segmentation. Further, we proposed an optimal selection strategy of K and several optimizations to speed up TSEXPLAIN for interactive user experience, achieving up to 13X efficiency improvement."}}
{"id": "ZOrnPO1pkts", "cdate": 1609459200000, "mdate": 1682348007244, "content": {"title": "Frugal Optimization for Cost-related Hyperparameters", "abstract": "The increasing demand for democratizing machine learning algorithms calls for hyperparameter optimization (HPO) solutions at low cost. Many machine learning algorithms have hyperparameters which can cause a large variation in the training cost. But this effect is largely ignored in existing HPO methods, which are incapable to properly control cost during the optimization process. To address this problem, we develop a new cost-frugal HPO solution. The core of our solution is a simple but new randomized direct-search method, for which we provide theoretical guarantees on the convergence rate and the total cost incurred to achieve convergence. We provide strong empirical results in comparison with state-of-the-art HPO methods on large AutoML benchmarks."}}
{"id": "LMnjp5iWZzU", "cdate": 1609459200000, "mdate": 1682348007337, "content": {"title": "A Demonstration of Relic: A System for REtrospective Lineage InferenCe of Data Workflows", "abstract": ""}}
{"id": "-gnTYXO0Q_", "cdate": 1609459200000, "mdate": 1682348007376, "content": {"title": "Economic Hyperparameter Optimization with Blended Search Strategy", "abstract": "We study the problem of using low cost to search for hyperparameter configurations in a large search space with heterogeneous evaluation cost and model quality. We propose a blended search strategy..."}}
{"id": "-ZLgVTVVpFp", "cdate": 1609459200000, "mdate": 1682348007166, "content": {"title": "TSExplain: Surfacing Evolving Explanations for Time Series", "abstract": "Understanding the underlying explanations for what has happened is more and more crucial in today's business decision-making processes. Existing explanation engines focus on explaining the difference between two given sets. However, for time-series, the explanations usually evolve as time advances. Thus, only considering two end timestamps would miss all explanations in between. To mitigate this, we demonstrate TSExplain, a system to help users understand the underlying evolving explanations for any aggregated time-series. Internally, TSExplain models the explanation problem as a segmentation problem over the time dimension and uses existing works on two-sets diff as building blocks. In our demonstration, conference attendees will be able to easily and interactively explore the evolving explanations and visualize how these explanations contribute to the overall changes in various datasets: COVID-19, S&P500, Iowa Liquor Sales. Questions-like \"which states make COVID-19 total confirmed case number go up dramatically during the past year?\", \"which stocks drive the dramatic crashes of S&P500 in Mar and the quick rebound later?\", and \"how does Liquor sales trend look like from January 2020 till now and why\"-can all get well-answered by TSExplain."}}
{"id": "VbLH04pRA3", "cdate": 1601308293693, "mdate": null, "content": {"title": "ECONOMIC HYPERPARAMETER OPTIMIZATION WITH BLENDED SEARCH STRATEGY", "abstract": "We study the problem of using low cost to search for hyperparameter configurations in a large search space with heterogeneous evaluation cost and model quality.\nWe propose a blended search strategy to combine the strengths of global and local search, and prioritize them on the fly with the goal of minimizing the total cost spent in finding good configurations. Our approach demonstrates robust performance for tuning both tree-based models and deep neural networks on a large AutoML benchmark, as well as superior performance in model quality, time, and resource consumption for a production transformer-based NLP model fine-tuning task."}}
{"id": "l32_s-oOeG8", "cdate": 1577836800000, "mdate": 1682348007247, "content": {"title": "Effective Data Versioning for Collaborative Data Analytics", "abstract": "With the massive proliferation of datasets in a variety of sec-tors, data science teams in these sectors spend vast amounts of time collaboratively constructing, curating, and analyzing these datasets. Versions of datasets are routinely generated during this data science process, via various data processing operations like data transformation and cleaning, feature engineering and normalization, among others. However, no existing systems enable us to effectively store, track, and query these versioned datasets, leading to massive redundancy in versioned data storage and making true collaboration and sharing impossible. In my PhD thesis, we develop solutions for versioned data management for collaborative data analytics. In the first part of my dissertation, we extend a relational database to support versioning of structured data. Specifically, we build a system, OrpheusDB, on top of a relational database with a carefully designed data representation and an intelligent partitioning algorithm for fast version control operations. OrpheusDB inherits much of the same benefits of relational databases, while also compactly storing, keeping track of, and recreating versions on demand. However, OrpheusDB implicitly makes a few assumptions, namely that:(a) the SQL assumption: a SQL-like language is the best fit for querying data and versioning information;(b) the structural assumption: the data is in a relational for-mat with a regular structure;(c) the from-scratch assumption: users adopt OrpheusDB from the very beginning of their project and register each data version along with full meta-data in the system. In the second part of my dissertation, we remove each of these assumptions, one at a time. First, we remove the SQL assumption and propose a generalized query language for querying data along with versioning and provenance information. Second, we remove the structural assumption and develop solutions for compact storage and fast retrieval of arbitrary data representations [4]. Finally, we remove the \"from-scratch\" assumption, by developing techniques to infer lineage relationships among versions residing in an existing data repository."}}
{"id": "g_X3kbgmJf", "cdate": 1577836800000, "mdate": 1682348007403, "content": {"title": "Cost Effective Optimization for Cost-related Hyperparameters", "abstract": "The increasing demand for democratizing machine learning algorithms calls for hyperparameter optimization (HPO) solutions at low cost. Many machine learning algorithms have hyperparameters which can cause a large variation in the training cost. But this effect is largely ignored in existing HPO methods, which are incapable to properly control cost during the optimization process. To address this problem, we develop a new cost-frugal HPO solution. The core of our solution is a simple but new randomized direct-search method, for which we prove a convergence rate of $O(\\frac{\\sqrt{d}}{\\sqrt{K}})$ and an $O(d\\epsilon^{-2})$-approximation guarantee on the total cost. We provide strong empirical results in comparison with state-of-the-art HPO methods on large AutoML benchmarks."}}
{"id": "RbuoHAiiLg", "cdate": 1577836800000, "mdate": 1682348007240, "content": {"title": "Uncovering Effective Explanations for Interactive Genomic Data Analysis", "abstract": ""}}
