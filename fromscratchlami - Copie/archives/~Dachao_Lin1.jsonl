{"id": "Q9hZdUBTC9S", "cdate": 1621629844009, "mdate": null, "content": {"title": "Faster Directional Convergence of Linear Neural Networks under Spherically Symmetric Data", "abstract": "In this paper, we study gradient methods for training deep linear neural networks with binary cross-entropy loss. In particular, we show global directional convergence guarantees from a polynomial rate to a linear rate for (deep) linear networks with spherically symmetric data distribution, which can be viewed as a specific zero-margin dataset. Our results do not require the assumptions in other works such as small initial loss, presumed convergence of weight direction, or overparameterization. We also characterize our findings in experiments."}}
{"id": "cI4c6OpwIKq", "cdate": 1621629803202, "mdate": null, "content": {"title": "Greedy and Random Quasi-Newton Methods with Faster Explicit Superlinear Convergence", "abstract": "In this paper, we follow Rodomanov and Nesterov\u2019s work to study quasi-Newton methods. We focus on the common SR1 and BFGS quasi-Newton methods to establish better explicit (local) superlinear convergence rates. First, based on the greedy quasi-Newton update which greedily selects the direction to maximize a certain measure of progress, we improve the convergence rate to a condition-number-free superlinear convergence rate. Second, based on the random quasi-Newton update that selects the direction randomly from a spherically symmetric distribution, we show the same superlinear convergence rate established as above. Our analysis is closely related to the approximation of a given Hessian matrix, unconstrained quadratic objective, as well as the general strongly convex, smooth, and strongly self-concordant functions."}}
{"id": "NPab8GcO5Pw", "cdate": 1601308069089, "mdate": null, "content": {"title": "On the Landscape of Sparse Linear Networks", "abstract": "Network pruning, or sparse network has a long history and practical significance in modern applications. Although the loss functions of neural networks may yield bad landscape due to non-convexity, we focus on linear activation which already owes benign landscape. With no unrealistic assumption, we conclude the following statements for the squared loss objective of general sparse linear neural networks: 1) every local minimum is a global minimum for scalar output with any sparse structure, or non-intersected sparse first layer and dense other layers with orthogonal training data; 2) sparse linear networks have sub-optimal local-min for only sparse first layer due to low rank constraint, or output larger than three dimensions due to the global minimum of a sub-network. Overall, sparsity breaks the normal structure, cutting out the decreasing path in original fully-connected networks."}}
{"id": "BJ4Dyib_ZB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Toward Understanding the Importance of Noise in Training Neural Networks", "abstract": "Numerous empirical evidence has corroborated that the noise plays a crucial rule in effective and efficient training of deep neural networks. The theory behind, however, is still largely unknown. T..."}}
{"id": "S1e_ssC5F7", "cdate": 1538087839738, "mdate": null, "content": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate."}}
