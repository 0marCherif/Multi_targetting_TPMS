{"id": "n8oWh81sdd", "cdate": 1683699523159, "mdate": 1683699523159, "content": {"title": "Towards Fair Representation Learning in Knowledge Graph with Stable Adversarial Debiasing", "abstract": "With graph-structured tremendous information, Knowledge Graphs (KG) aroused increasing interest in academic research and industrial applications. Recent studies have shown demographic bias, in terms of sensitive attributes (e.g., gender and race), exist in the learned representations of KG entities. Such bias negatively affects specific populations, especially minorities and underrepresented groups, and exacerbates machine learning-based human inequality. Adversarial learning is regarded as an effective way to alleviate bias in the representation learning model by simultaneously training a task-specific predictor and a sensitive attribute-specific discriminator. However, due to the unique challenge caused by topological structure and the comprehensive relationship between knowledge entities, adversarial learning-based debiasing is rarely studied in representation learning in knowledge graphs. In this paper, we propose a framework to learn unbiased representations for nodes and edges in knowledge graph mining. Specifically, we integrate a simple-but-effective normalization technique with Graph Neural Networks (GNNs) to constrain the weights updating process. Moreover, as a work-in-progress paper, we also find that the introduced weights normalization technique can mitigate the pitfalls of instability in adversarial debasing towards fair-and-stable machine learning. We evaluate the proposed framework on a benchmarking graph with multiple edge types and node types. The experimental results show that our model achieves comparable or better gender fairness over three competitive baselines on Equality of Odds. Importantly, our superiority in the fair model does not scarify the performance in the knowledge graph task (i.e., multi-class edge classification)."}}
{"id": "0trwt7-sYU", "cdate": 1674878729501, "mdate": 1674878729501, "content": {"title": "Fairness Interventions as (Dis)Incentives for Strategic Manipulation", "abstract": "Although machine learning (ML) algorithms are widely used to make decisions about individuals in various domains, concerns have arisen that (1) these algorithms are vulnerable to strategic manipulation and \"gaming the algorithm\"; and (2) ML decisions may exhibit bias against certain social groups. Existing works have largely examined these as two separate issues, e.g., by focusing on building ML algorithms robust to strategic manipulation, or on training a fair ML algorithm. In this study, we set out to understand the impact they each have on the other, and examine how to characterize fair policies in the presence of strategic behavior. The strategic interaction between a decision maker and individuals (as decision takers) is modeled as a two-stage (Stackelberg) game; when designing an algorithm, the former anticipates the latter may manipulate their features in order to receive more favorable decisions. We analytically characterize the equilibrium strategies of both, and examine how the algorithms and their resulting fairness properties are affected when the decision maker is strategic (anticipates manipulation), as well as the impact of fairness interventions on equilibrium strategies. In particular, we identify conditions under which anticipation of strategic behavior may mitigate/exacerbate unfairness, and conditions under which fairness interventions can serve as (dis)incentives for strategic manipulation."}}
{"id": "1kq98Mkc1Tm", "cdate": 1674878646017, "mdate": 1674878646017, "content": {"title": "Incentive Mechanisms for Strategic Classification and Regression Problems", "abstract": "We study the design of a class of incentive mechanisms that can effectively prevent cheating in a strategic classification and regression problem. A conventional strategic classification or regression problem is modeled as a Stackelberg game, or a principal-agent problem between the designer of a classifier (the principal) and individuals subject to the classifier's decisions (the agents), potentially from different demographic groups. The former benefits from the accuracy of its decisions, whereas the latter may have an incentive to game the algorithm into making favorable but erroneous decisions. While prior works tend to focus on how to design an algorithm to be more robust to such strategic maneuvering, this study focuses on an alternative, which is to design incentive mechanisms to shape the utilities of the agents and induce effort that genuinely improves their skills, which in turn benefits both parties in the Stackelberg game. Specifically, the principal and the mechanism provider (which could also be the principal itself) move together in the first stage, publishing and committing to a classifier and an incentive mechanism. The agents are (simultaneous) second movers and best respond to the published classifier and incentive mechanism. When an agent's strategic action merely changes its observable features, it hurts the performance of the algorithm. However, if the action leads to improvement in the agent's true label, it not only helps the agent achieve better decision outcomes, but also preserves the performance of the algorithm. We study how a subsidy mechanism can induce improvement actions, positively impact a number of social well-being metrics, such as the overall skill levels of the agents (efficiency) and positive or true positive rate differences between different demographic groups (fairness)."}}
{"id": "tge5NiX4CZo", "cdate": 1664816295137, "mdate": null, "content": {"title": "Counterfactual Fairness in Synthetic Data Generation", "abstract": "Synthetic data generation (SDG) is proposed as a promising solution for data sharing as in many high-stake applications due to privacy concerns, releasing the real dataset is not an option. While the main goal of private SDG is to create a dataset that preserves the privacy of individuals contributing to the dataset, the use of synthetic data also creates an opportunity to improve the fairness issue at the source. Since there exist historical biases in the datasets, using the biased data to train an ML model can lead to an unfair model which may exacerbate the discrimination. Using synthetic data, we can attempt to remove the bias from the dataset before releasing the data. In this work, we formalize the definition of fairness in synthetic data generation and propose a method to achieve counterfactual fairness."}}
{"id": "10tgIzcC2vY", "cdate": 1663850205442, "mdate": null, "content": {"title": "Upcycled-FL: Improving Accuracy and Privacy with Less Computation in Federated Learning", "abstract": "Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized edge devices to collaboratively learn toward a common objective without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as sensitive information can be inferred from intermediate computations. As the same data is repeatedly used over an iterative process, information leakage accumulates substantially over time, making it difficult to balance the trade-off between privacy and accuracy. In this paper we introduce Upcycled-FL, a novel federated learning framework, where first-order approximation is  applied at every even iteration. Under such a scheme, half of the steps incur no privacy loss and require much less computation. Theoretically, we establish the convergence rate performance of Upcycled-FL and provide privacy analysis based on objective and output perturbations. Experiments on  real-world data show that Upcycled-FL consistently outperforms existing methods over heterogeneous data, and significantly improves privacy-accuracy trade-off, while reducing 48% of the training time on average."}}
{"id": "vtDzHJOsmfJ", "cdate": 1632875602341, "mdate": null, "content": {"title": "Non-convex Optimization for Learning a Fair Predictor under Equalized Loss Fairness Constraint", "abstract": "Supervised learning models have been increasingly used in various domains such as lending, college admission, natural language processing, face recognition, etc.  These models may inherit pre-existing biases from training datasets and exhibit discrimination against protected social groups. Various fairness notions have been introduced to address fairness issues. In general, finding a fair predictor leads to a constrained optimization problem, and depending on the fairness notion, it may be non-convex. In this work, we focus on Equalized Loss ($\\textsf{EL}$), a fairness notion that requires the prediction error/loss to be equalized across different demographic groups. Imposing this constraint to the learning process leads to a non-convex optimization problem even if the loss function is convex. We introduce algorithms that can leverage off-the-shelf convex programming tools and efficiently find the $\\textit{global}$ optimum of this non-convex problem. In particular, we first propose the $\\mathtt{ELminimizer}$ algorithm, which finds the optimal $\\textsf{EL}$ fair predictor by reducing the non-convex optimization problem to a sequence of convex constrained optimizations. We then propose a simple algorithm that is computationally more efficient compared to $\\mathtt{ELminimizer}$ and finds a sub-optimal $\\textsf{EL}$ fair predictor using $\\textit{unconstrained}$ convex programming tools. Experiments on real-world data show the effectiveness of our algorithms.  "}}
{"id": "k4jzOHrZ7F5", "cdate": 1632875511674, "mdate": null, "content": {"title": "Interpreting Black-boxes Using Primitive Parameterized Functions", "abstract": "One approach for interpreting black-box machine learning models is to find a global approximation of the model using simple interpretable functions, which is called a metamodel (a model of the model). Approximating the black-box with a metamodel can be used to 1) estimate instance-wise feature importance; 2) understand the functional form of the model; 3) analyze feature interactions. In this work, we propose a new method for finding interpretable metamodels. Our approach utilizes Kolmogorov superposition theorem, which expresses multivariate functions as a composition of univariate functions (our primitive parameterized functions). This composition can be represented in the form of a tree. Inspired by symbolic regression, we use a modified form of genetic programming to search over different tree configurations. Gradient descent is used to optimize the parameters of a given configuration. Using several experiments, we show that our method outperforms recent metamodeling approaches suggested for interpreting black-boxes.\n"}}
{"id": "w-EabDtADg", "cdate": 1621629811582, "mdate": null, "content": {"title": "Fair Sequential Selection Using Supervised Learning Models", "abstract": "We consider a selection problem where sequentially arrived applicants apply for a limited number of positions/jobs. At each time step, a decision maker accepts or rejects the given applicant using a pre-trained supervised learning model until all the vacant positions are filled. In this paper, we discuss whether the fairness notions (e.g., equal opportunity, statistical parity, etc.) that are commonly used in classification problems are suitable for the sequential selection problems. In particular, we show that even with a pre-trained model that satisfies the common fairness notions, the selection outcomes may still be biased against certain demographic groups. This observation implies that the fairness notions used in classification problems are not suitable for a selection problem where the applicants compete for a limited number of positions.  We introduce a new fairness notion, ``Equal Selection (ES),'' suitable for sequential selection problems and propose a post-processing approach to satisfy the ES fairness notion. We also consider a setting where the applicants have privacy concerns, and the decision maker only has access to the noisy version of sensitive attributes. In this setting, we can show that the \\textit{perfect} ES fairness can still be attained under certain conditions."}}
{"id": "vLGPgt7Vvn7", "cdate": 1598887126371, "mdate": null, "content": {"title": "Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms", "abstract": "Alternating direction method of multiplier (ADMM) is a popular method used to design distributed versions of a machine learning algorithm, whereby local computations are performed on local data with the output exchanged among neighbors in an iterative fashion. During this iterative process the leakage of data privacy arises. A differentially private ADMM was proposed in prior work (Zhang & Zhu, 2017) where only the privacy loss of a single node during one iteration was bounded, a method that makes it difficult to balance the tradeoff between the utility attained through distributed computation and privacy guarantees when considering the total privacy loss of all nodes over the entire iterative process. We propose a perturbation method for ADMM where the perturbed term is correlated with the penalty parameters; this is shown to improve the utility and privacy simultaneously. The method is based on a modified ADMM where each node independently determines its own penalty parameter in every iteration and decouples it from the dual updating step size. The condition for convergence of the modified ADMM and the lower bound on the convergence rate are also derived."}}
{"id": "gU6GGnHuZwe", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Continuously Updated, Computationally Efficient Stress Recognition Framework Using Electroencephalogram (EEG) by Applying Online Multitask Learning Algorithms (OMTL).", "abstract": "Recognizing the factors that cause stress is a crucial step toward early detection of stressors. In this regard, several studies make an effort to recognize individuals' stress using an Electroencephalogram (EEC). However, current EEC-based stress recognition frameworks have several drawbacks. First, they are mostly designed to recognize individuals' stress only in a controlled laboratory environment. Second, they do not take into account the changes in the EEC signals of different subjects under the same stressors. Third, most of the current stress recognition algorithms occur in an offline setting. To address these issues, this study proposes an EEC-based stress recognition framework that takes into account each subject's brainwave patterns to train the stress recognition classifier and continuously update its classifier based on new input signals in near real-time. The proposed framework first removes EEC signal artifacts, then extracts a broad range of EEC signal features, and finally applies different online multitask learning (OMTL) algorithms to recognize individuals' stress in near real time. The proposed framework was applied on the EEC collected in two environments-first on the EEC collected in a controlled lab environment using a wired-EEC and second on the EEC collected at in the field using a wearable EEC device. The OMTL-VonNeuman method resulted in the best prediction accuracy on both datasets (71.14% on the first dataset and 77.61% on second) among all tested algorithms. The proposed stress recognition framework continuously updates its classifier and therefore contributes to stress recognition for new stressful situations that are beyond the range of predefined stressful conditions in near real time both in a controlled lab environment and at real job sites."}}
