{"id": "TScq50yPzJ", "cdate": 1680530915639, "mdate": 1680530915639, "content": {"title": "Strong Memory Lower Bounds for Learning Natural Models", "abstract": "We give lower bounds on the amount of memory required by one-pass streaming algorithms for solving several natural learning problems. In a setting where examples lie in $\\{0,1\\}^d$ and the optimal classifier can be encoded using $\\kappa$ bits, we show that algorithms which learn using a near-minimal number of examples, $\\tilde{O}(\\kappa)$, must use $\\tilde{\\Omega}(d\\kappa)$ bits of space. Our space bounds match the dimension of the ambient space of the problem's natural parametrization, even when it is quadratic in the size of examples and the final classifier. For instance, in the setting of $d$-sparse linear classifiers over degree-2 polynomial features, for which $\\kappa=\\Theta(d \\log d)$, our space lower bound is $\\tilde{\\Omega}(d^2)$. Our bounds degrade gracefully with the stream length $N$, generally having the form $\\tilde{\\Omega}(dk \\cdot \\frac{\\kappa}{N})$.\nBounds of the form $\\Omega(d\\kappa)$ were known for learning parity and other problems defined over finite fields. Bounds that apply in a narrow range of sample sizes are also known for linear regression. Ours are the first such bounds for problems of the type commonly seen in recent learning applications that apply for a large range of input sizes. "}}
{"id": "vHiCXW2HKwS", "cdate": 1680530728213, "mdate": 1680530728213, "content": {"title": "When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?", "abstract": "Modern machine learning models are complex and frequently encode surprising amounts of information about individual inputs. In extreme cases, complex models appear to memorize entire input examples, including seemingly irrelevant information (social security numbers from text, for example). In this paper, we aim to understand whether this sort of memorization is necessary for accurate learning. We describe natural prediction problems in which every sufficiently accurate training algorithm must encode, in the prediction model, essentially all the information about a large subset of its training examples. This remains true even when the examples are high-dimensional and have entropy much higher than the sample size, and even when most of that information is ultimately irrelevant to the task at hand. Further, our results do not depend on the training algorithm or the class of models used for learning.\nOur problems are simple and fairly natural variants of the next-symbol prediction and the cluster labeling tasks. These tasks can be seen as abstractions of text- and image-related prediction problems. To establish our results, we reduce from a family of one-way communication problems for which we prove new information complexity lower bounds. Additionally, we present synthetic-data experiments demonstrating successful attacks on logistic regression and neural network classifiers. "}}
{"id": "rIcqohnMtw", "cdate": 1672531200000, "mdate": 1682355808098, "content": {"title": "Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization", "abstract": "The notion of replicable algorithms was introduced in Impagliazzo et al. [STOC '22] to describe randomized algorithms that are stable under the resampling of their inputs. More precisely, a replicable algorithm gives the same output with high probability when its randomness is fixed and it is run on a new i.i.d. sample drawn from the same distribution. Using replicable algorithms for data analysis can facilitate the verification of published results by ensuring that the results of an analysis will be the same with high probability, even when that analysis is performed on a new data set. In this work, we establish new connections and separations between replicability and standard notions of algorithmic stability. In particular, we give sample-efficient algorithmic reductions between perfect generalization, approximate differential privacy, and replicability for a broad class of statistical problems. Conversely, we show any such equivalence must break down computationally: there exist statistical problems that are easy under differential privacy, but that cannot be solved replicably without breaking public-key cryptography. Furthermore, these results are tight: our reductions are statistically optimal, and we show that any computational separation between DP and replicability must imply the existence of one-way functions. Our statistical reductions give a new algorithmic framework for translating between notions of stability, which we instantiate to answer several open questions in replicability and privacy. This includes giving sample-efficient replicable algorithms for various PAC learning, distribution estimation, and distribution testing problems, algorithmic amplification of $\\delta$ in approximate DP, conversions from item-level to user-level privacy, and the existence of private agnostic-to-realizable learning reductions under structured distributions."}}
{"id": "rHeGMFekSP1", "cdate": 1672531200000, "mdate": 1682355808060, "content": {"title": "Approximate degree lower bounds for oracle identification problems", "abstract": "The approximate degree of a Boolean function is the minimum degree of real polynomial that approximates it pointwise. For any Boolean function, its approximate degree serves as a lower bound on its quantum query complexity, and generically lifts to a quantum communication lower bound for a related function. We introduce a framework for proving approximate degree lower bounds for certain oracle identification problems, where the goal is to recover a hidden binary string $x \\in \\{0, 1\\}^n$ given possibly non-standard oracle access to it. Our lower bounds apply to decision versions of these problems, where the goal is to compute the parity of $x$. We apply our framework to the ordered search and hidden string problems, proving nearly tight approximate degree lower bounds of $\\Omega(n/\\log^2 n)$ for each. These lower bounds generalize to the weakly unbounded error setting, giving a new quantum query lower bound for the hidden string problem in this regime. Our lower bounds are driven by randomized communication upper bounds for the greater-than and equality functions."}}
{"id": "yIqeiaSju1", "cdate": 1640995200000, "mdate": 1682355808467, "content": {"title": "Approximate Degree in Classical and Quantum Computing", "abstract": ""}}
{"id": "v__IeHpP9j", "cdate": 1640995200000, "mdate": 1682355808492, "content": {"title": "Strong Memory Lower Bounds for Learning Natural Models", "abstract": "We give lower bounds on the amount of memory required by a one-pass streaming algorithms for solving several natural learning problems. In a setting where examples lie in $\\{0,1\\}^d$ and the optima..."}}
{"id": "kgao-KzD3cl", "cdate": 1640995200000, "mdate": 1682355808034, "content": {"title": "Controlling Privacy Loss in Sampling Schemes: An Analysis of Stratified and Cluster Sampling", "abstract": "Sampling schemes are fundamental tools in statistics, survey design, and algorithm design. A fundamental result in differential privacy is that a differentially private mechanism run on a simple random sample of a population provides stronger privacy guarantees than the same algorithm run on the entire population. However, in practice, sampling designs are often more complex than the simple, data-independent sampling schemes that are addressed in prior work. In this work, we extend the study of privacy amplification results to more complex, data-dependent sampling schemes. We find that not only do these sampling schemes often fail to amplify privacy, they can actually result in privacy degradation. We analyze the privacy implications of the pervasive cluster sampling and stratified sampling paradigms, as well as provide some insight into the study of more general sampling designs."}}
{"id": "S5vEoH0n5ZD", "cdate": 1640995200000, "mdate": 1681593528881, "content": {"title": "Private and Online Learnability Are Equivalent", "abstract": ""}}
{"id": "OnDTMnFoDL", "cdate": 1640995200000, "mdate": 1682355808099, "content": {"title": "Strong Memory Lower Bounds for Learning Natural Models", "abstract": "We give lower bounds on the amount of memory required by one-pass streaming algorithms for solving several natural learning problems. In a setting where examples lie in $\\{0,1\\}^d$ and the optimal classifier can be encoded using $\\kappa$ bits, we show that algorithms which learn using a near-minimal number of examples, $\\tilde O(\\kappa)$, must use $\\tilde \\Omega( d\\kappa)$ bits of space. Our space bounds match the dimension of the ambient space of the problem's natural parametrization, even when it is quadratic in the size of examples and the final classifier. For instance, in the setting of $d$-sparse linear classifiers over degree-2 polynomial features, for which $\\kappa=\\Theta(d\\log d)$, our space lower bound is $\\tilde\\Omega(d^2)$. Our bounds degrade gracefully with the stream length $N$, generally having the form $\\tilde\\Omega\\left(d\\kappa \\cdot \\frac{\\kappa}{N}\\right)$. Bounds of the form $\\Omega(d\\kappa)$ were known for learning parity and other problems defined over finite fields. Bounds that apply in a narrow range of sample sizes are also known for linear regression. Ours are the first such bounds for problems of the type commonly seen in recent learning applications that apply for a large range of input sizes."}}
{"id": "MedjBiWm4Np", "cdate": 1640995200000, "mdate": 1682355808054, "content": {"title": "The Complexity of Verifying Boolean Programs as Differentially Private", "abstract": "We study the complexity of the problem of verifying differential privacy for while-like programs working over boolean values and making probabilistic choices. Programs in this class can be interpreted into finite-state discrete-time Markov Chains (DTMC). We show that the problem of deciding whether a program is differentially private for specific values of the privacy parameters is PSPACE-complete. To show that this problem is in PSPACE, we adapt classical results about computing hitting probabilities for DTMC. To show PSPACE-hardness we use a reduction from the problem of checking whether a program almost surely terminates or not. We also show that the problem of approximating the privacy parameters that a program provides is PSPACE-hard. Moreover, we investigate the complexity of similar problems also for several relaxations of differential privacy: Renyi differential privacy, concentrated differential privacy, and truncated concentrated differential privacy. For these notions, we consider gap-versions of the problem of deciding whether a program is private or not and we show that all of them are PSPACE-complete."}}
