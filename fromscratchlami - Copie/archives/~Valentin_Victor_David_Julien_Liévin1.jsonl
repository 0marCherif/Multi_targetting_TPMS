{"id": "ocfxd-7M-j6", "cdate": 1682575563979, "mdate": 1682575563979, "content": {"title": "Variational Open-domain Question Answering", "abstract": "Retrieval-augmented models have proven to be effective in natural language processing tasks, yet there remains a lack of research on their opti- mization using variational inference. We intro- duce the Variational Open-Domain (VOD) frame- work for end-to-end training and evaluation of retrieval-augmented models, focusing on open- domain question answering and language mod- elling. The VOD objective, a self-normalized es- timate of the R\u00e9nyi variational bound, is a lower bound to the task marginal likelihood and evalu- ated under samples drawn from an auxiliary sam- pling distribution (cached retriever and/or approx- imate posterior). It remains tractable, even for re- triever distributions defined on large corpora. We demonstrate VOD\u2019s versatility by training reader- retriever BERT-sized models on multiple-choice medical exam questions. On the MedMCQA dataset, we outperform the domain-tuned Med- PaLM by +5.3% despite using 2.500\u00d7 fewer pa- rameters. Our retrieval-augmented BioLinkBERT model scored 62.9% on the MedMCQA and 55.0% on the MedQA-USMLE. Last, we show the effectiveness of our learned retriever compo- nent in the context of medical semantic search."}}
{"id": "HPAdr9XxMy", "cdate": 1682575459862, "mdate": 1682575459862, "content": {"title": "Can large language models reason about medical questions?", "abstract": "Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\\ too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%."}}
{"id": "bdyTNdZvZV9", "cdate": 1599638376813, "mdate": null, "content": {"title": "BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling", "abstract": "With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks."}}
{"id": "berXNBWYzDL", "cdate": 1599638272661, "mdate": null, "content": {"title": "Optimal Variance Control of the Score Function Gradient Estimator for Importance Weighted Bounds", "abstract": "This paper introduces novel results for the score function gradient estimator of the\nimportance weighted variational bound (IWAE). We prove that in the limit of large\nK (number of importance samples) one can choose the control variate such that the\nSignal-to-Noise ratio (SNR) of the estimator grows as \u221aK. This is in contrast to the\nstandard pathwise gradient estimator where the SNR decreases as 1/\u221aK. Based on\nour theoretical findings we develop a novel control variate that extends on VIMCO.\nEmpirically, for the training of both continuous and discrete generative models,\nthe proposed method yields superior variance reduction, resulting in an SNR\nfor IWAE that increases with K without relying on the reparameterization trick.\nThe novel estimator is competitive with state-of-the-art reparameterization-free\ngradient estimators such as Reweighted Wake-Sleep (RWS) and the thermodynamic\nvariational objective (TVO) when training generative models"}}
{"id": "HkxXcy2EYB", "cdate": 1571237771217, "mdate": null, "content": {"title": "Towards Hierarchical Discrete Variational Autoencoders", "abstract": "Variational Autoencoders (VAEs) have proven to be powerful latent variable models. How- ever, the form of the approximate posterior can limit the expressiveness of the model. Categorical distributions are flexible and useful building blocks for example in neural memory layers. We introduce the Hierarchical Discrete Variational Autoencoder (HD-VAE): a hi- erarchy of variational memory layers. The Concrete/Gumbel-Softmax relaxation allows maximizing a surrogate of the Evidence Lower Bound by stochastic gradient ascent. We show that, when using a limited number of latent variables, HD-VAE outperforms the Gaussian baseline on modelling multiple binary image datasets. Training very deep HD-VAE remains a challenge due to the relaxation bias that is induced by the use of a surrogate objective. We introduce a formal definition and conduct a preliminary theoretical and empirical study of the bias."}}
