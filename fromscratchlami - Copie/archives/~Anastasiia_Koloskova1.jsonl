{"id": "zJzaoGTd3O", "cdate": 1672531200000, "mdate": 1681670560943, "content": {"title": "Convergence of Gradient Descent with Linearly Correlated Noise and Applications to Differentially Private Learning", "abstract": "We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise linearly correlated over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anticorrelated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretically and empirically."}}
{"id": "HbXIYRpRW6", "cdate": 1672531200000, "mdate": 1681916191190, "content": {"title": "Decentralized Gradient Tracking with Local Steps", "abstract": "Gradient tracking (GT) is an algorithm designed for solving decentralized optimization problems over a network (such as training a machine learning model). A key feature of GT is a tracking mechanism that allows to overcome data heterogeneity between nodes. We develop a novel decentralized tracking mechanism, $K$-GT, that enables communication-efficient local updates in GT while inheriting the data-independence property of GT. We prove a convergence rate for $K$-GT on smooth non-convex functions and prove that it reduces the communication overhead asymptotically by a linear factor $K$, where $K$ denotes the number of local steps. We illustrate the robustness and effectiveness of this heterogeneity correction on convex and non-convex benchmark problems and on a non-convex neural network training task with the MNIST dataset."}}
{"id": "Quz3n455QZt", "cdate": 1664731454554, "mdate": null, "content": {"title": "Data-heterogeneity-aware Mixing for Decentralized Learning", "abstract": "Decentralized learning provides an effective framework to train machine learning models with data distributed over arbitrary communication graphs. However, most existing approaches towards decentralized learning disregard the interaction between data heterogeneity and graph topology. In this paper, we characterize the dependence of convergence on the relationship between the mixing weights of the graph and the data heterogeneity across nodes. We propose a metric that quantifies the ability of a graph to mix the current gradients. We further prove that the metric controls the convergence rate, particularly in settings where the heterogeneity across nodes dominates the stochasticity between updates for a given node. Motivated by our analysis, we propose an approach that periodically and efficiently optimizes the metric using standard convex constrained optimization and sketching techniques. "}}
{"id": "PxO6WDSnQv", "cdate": 1664731450520, "mdate": null, "content": {"title": "Decentralized Stochastic Optimization with Client Sampling", "abstract": "\tDecentralized optimization is a key setting toward enabling data privacy and on-device learning over networks.\n\tExisting research primarily focuses on distributing the objective function across $n$ nodes/clients, lagging behind the real-world challenges such as i) node availability---not all $n$ nodes are always available during the optimization---and ii) slow information propagation (caused by a large number of nodes $n$). \n\tIn this work, we study Decentralized Stochastic Gradient Descent (D-SGD) with node subsampling, i.e. when only $s~(s \\leq n)$ nodes are randomly sampled out of $n$ nodes per iteration.\n\tWe provide the theoretical convergence rates in smooth (convex and non-convex) problems with heterogeneous (non-identically distributed data) functions.\n\tOur theoretical results capture the effect of node subsampling and choice of the topology on the sampled nodes, through a metric termed \\emph{the expected consensus rate}.\n\tOn a number of common topologies, including ring and torus, we theoretically and empirically demonstrate the effectiveness of such a metric."}}
{"id": "4_oCZgBIVI", "cdate": 1652737721975, "mdate": null, "content": {"title": "Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning", "abstract": "We study the asynchronous stochastic gradient descent algorithm, for distributed training over $n$ workers that might be heterogeneous. In this algorithm, workers compute stochastic gradients in parallel at their own pace and return them to the server without any synchronization.\n\nExisting convergence rates of this algorithm for non-convex smooth objectives depend on the maximum delay $\\tau_{\\max}$ and reach an $\\epsilon$-stationary point after $O\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{\\max}\\epsilon^{-1}\\right)$ iterations,  where $\\sigma$ is the variance of stochastic gradients. In this work (i) we obtain a tighter convergence rate of $O\\!\\left(\\sigma^2\\epsilon^{-2}+ \\sqrt{\\tau_{\\max}\\tau_{avg}}\\epsilon^{-1}\\right)$ *without any change in the algorithm* where $\\tau_{avg}$ is the average delay, which can be significantly smaller than $\\tau_{\\max}$. We also provide (ii) a simple delay-adaptive learning rate scheme, under which asynchronous SGD achieves a convergence rate of $O\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{avg}\\epsilon^{-1}\\right)$, and does not require any extra hyperparameter tuning nor extra communications. Our result allows to show *for the first time* that asynchronous SGD is *always faster* than mini-batch SGD. In addition, (iii) we consider the case of heterogeneous functions motivated by federated learning applications and improve the convergence rate by proving a weaker dependence on the maximum delay compared to prior works."}}
{"id": "Y4vT7m4e3d", "cdate": 1652737337903, "mdate": null, "content": {"title": "Decentralized Local Stochastic Extra-Gradient for Variational Inequalities", "abstract": "We consider distributed stochastic variational inequalities (VIs) on unbounded domains with the problem data that is heterogeneous (non-IID) and distributed across many devices. We make a very general assumption on the computational network that, in particular, covers the settings of fully decentralized calculations with time-varying networks and centralized topologies commonly used in Federated Learning. Moreover, multiple local updates on the workers can be made for reducing the communication frequency between the workers.\nWe extend the stochastic extragradient method to this very general setting and theoretically analyze its convergence rate in the strongly-monotone, monotone, and non-monotone (when a Minty solution exists) settings. The provided rates explicitly exhibit the dependence on network characteristics (e.g., mixing time), iteration counter, data heterogeneity, variance, number of devices, and other standard parameters. As a special case, our method and analysis apply to distributed stochastic saddle-point problems (SPP), e.g., to the training of Deep Generative Adversarial Networks (GANs) for which decentralized training has been reported to be extremely challenging. In experiments for the decentralized training of GANs we demonstrate the effectiveness of our proposed approach."}}
{"id": "xH6YZCvEcol", "cdate": 1640995200000, "mdate": 1681916191624, "content": {"title": "Data-heterogeneity-aware Mixing for Decentralized Learning", "abstract": "Decentralized learning provides an effective framework to train machine learning models with data distributed over arbitrary communication graphs. However, most existing approaches toward decentralized learning disregard the interaction between data heterogeneity and graph topology. In this paper, we characterize the dependence of convergence on the relationship between the mixing weights of the graph and the data heterogeneity across nodes. We propose a metric that quantifies the ability of a graph to mix the current gradients. We further prove that the metric controls the convergence rate, particularly in settings where the heterogeneity across nodes dominates the stochasticity between updates for a given node. Motivated by our analysis, we propose an approach that periodically and efficiently optimizes the metric using standard convex constrained optimization and sketching techniques. Through comprehensive experiments on standard computer vision and NLP benchmarks, we show that our approach leads to improvement in test performance for a wide range of tasks."}}
{"id": "K8lGdfgln8", "cdate": 1640995200000, "mdate": 1681916191814, "content": {"title": "An Improved Analysis of Gradient Tracking for Decentralized Machine Learning", "abstract": "We consider decentralized machine learning over a network where the training data is distributed across $n$ agents, each of which can compute stochastic model updates on their local data. The agent's common goal is to find a model that minimizes the average of all local loss functions. While gradient tracking (GT) algorithms can overcome a key challenge, namely accounting for differences between workers' local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter $p$ (related to the spectral gap of the connectivity matrix). We provide a tighter analysis of the GT method in the stochastic strongly convex, convex and non-convex settings. We improve the dependency on $p$ from $\\mathcal{O}(p^{-2})$ to $\\mathcal{O}(p^{-1}c^{-1})$ in the noiseless case and from $\\mathcal{O}(p^{-3/2})$ to $\\mathcal{O}(p^{-1/2}c^{-1})$ in the general stochastic case, where $c \\geq p$ is related to the negative eigenvalues of the connectivity matrix (and is a constant in most practical applications). This improvement was possible due to a new proof technique which could be of independent interest."}}
{"id": "F3y9jimpv0", "cdate": 1640995200000, "mdate": 1681916191226, "content": {"title": "Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning", "abstract": "We study the asynchronous stochastic gradient descent algorithm for distributed training over $n$ workers which have varying computation and communication frequency over time. In this algorithm, workers compute stochastic gradients in parallel at their own pace and return those to the server without any synchronization. Existing convergence rates of this algorithm for non-convex smooth objectives depend on the maximum gradient delay $\\tau_{\\max}$ and show that an $\\epsilon$-stationary point is reached after $\\mathcal{O}\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{\\max}\\epsilon^{-1}\\right)$ iterations, where $\\sigma$ denotes the variance of stochastic gradients. In this work (i) we obtain a tighter convergence rate of $\\mathcal{O}\\!\\left(\\sigma^2\\epsilon^{-2}+ \\sqrt{\\tau_{\\max}\\tau_{avg}}\\epsilon^{-1}\\right)$ without any change in the algorithm where $\\tau_{avg}$ is the average delay, which can be significantly smaller than $\\tau_{\\max}$. We also provide (ii) a simple delay-adaptive learning rate scheme, under which asynchronous SGD achieves a convergence rate of $\\mathcal{O}\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{avg}\\epsilon^{-1}\\right)$, and does not require any extra hyperparameter tuning nor extra communications. Our result allows to show for the first time that asynchronous SGD is always faster than mini-batch SGD. In addition, (iii) we consider the case of heterogeneous functions motivated by federated learning applications and improve the convergence rate by proving a weaker dependence on the maximum delay compared to prior works. In particular, we show that the heterogeneity term in convergence rate is only affected by the average delay within each worker."}}
{"id": "CmI7NqBR4Ua", "cdate": 1621630226836, "mdate": null, "content": {"title": "An Improved Analysis of Gradient Tracking for Decentralized Machine Learning", "abstract": "We consider decentralized machine learning over a network where the training data is distributed across $n$ agents, each of which can compute stochastic model updates on their local data. The agent's common goal is to find a model that minimizes the average of all local loss functions. While gradient tracking (GT) algorithms can overcome a key challenge, namely accounting for differences between workers' local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter $p$ (related to the spectral gap of the connectivity matrix).\nWe provide a tighter analysis of the GT method in the stochastic strongly convex, convex and non-convex settings. We improve the dependency on $p$ from $\\mathcal{O}(p^{-2})$ to $\\mathcal{O}(p^{-1}c^{-1})$ in the noiseless case and from $\\mathcal{O}(p^{-3/2})$ to $\\mathcal{O}(p^{-1/2}c^{-1})$ in the general stochastic case, where $c \\geq p$ is related to the negative eigenvalues of the connectivity matrix (and is a constant in most practical applications). This improvement was possible due to a new proof technique which could be of independent interest."}}
