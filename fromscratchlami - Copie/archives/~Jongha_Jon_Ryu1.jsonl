{"id": "QW81mmGrWz", "cdate": 1683124659920, "mdate": 1683124659920, "content": {"title": "On Universal Portfolios with Continuous Side Information", "abstract": "A new portfolio selection strategy that adapts to a continuous side-information sequence is presented, with a universal wealth guarantee against a class of state-constant rebalanced portfolios with respect to a state function that maps each side-information symbol to a finite set of states. In particular, given that a state function belongs to a collection of functions of finite Natarajan dimension, the proposed strategy is shown to achieve, asymptotically to first order in the exponent, the same wealth as the best state-constant rebalanced portfolio with respect to the best state function, chosen in hindsight from observed market. This result can be viewed as an extension of the seminal work of Cover and Ordentlich (1996) that assumes a single-state function.\n"}}
{"id": "yMWT-D-eLo", "cdate": 1672531200000, "mdate": 1680112441562, "content": {"title": "Group Fairness with Uncertainty in Sensitive Attributes", "abstract": ""}}
{"id": "1dmz-T1Ha7k", "cdate": 1640995200000, "mdate": 1680112635235, "content": {"title": "Parameter-Free Online Linear Optimization with Side Information via Universal Coin Betting", "abstract": ""}}
{"id": "r1g1CAEKDH", "cdate": 1569439431449, "mdate": null, "content": {"title": "Wyner VAE: A Variational Autoencoder with Succinct Common Representation Learning", "abstract": "A new variational autoencoder (VAE) model is proposed that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is based on two information theoretic problems---distributed simulation and channel synthesis---in which Wyner's common information arises as the fundamental limit of the succinctness of the common representation. The Wyner VAE decomposes a pair of correlated data variables into their common representation (e.g., a shared concept) and  local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information between the data variables and the common representation as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images. Experimental results show that learning a succinct common representation achieves better generative performance and that the proposed model outperforms existing VAE variants and the variational information bottleneck method."}}
{"id": "kjMjum4-PrP", "cdate": 1514764800000, "mdate": 1680112561957, "content": {"title": "Conditional Distribution Learning with Neural Networks and its Application to Universal Image Denoising", "abstract": ""}}
{"id": "4eTb2-MetjB", "cdate": 1514764800000, "mdate": 1680112561958, "content": {"title": "Nearest neighbor density functional estimation based on inverse Laplace transform", "abstract": ""}}
{"id": "krPhUrUyfE", "cdate": 1483228800000, "mdate": 1680112561956, "content": {"title": "Energy-Based Sequence GANs for Recommendation and Their Connection to Imitation Learning", "abstract": ""}}
