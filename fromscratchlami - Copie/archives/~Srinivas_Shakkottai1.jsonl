{"id": "4lCNGAtH26W", "cdate": 1677628800000, "mdate": 1682318042187, "content": {"title": "OpenGridGym: An Open-Source AI-Friendly Toolkit for Distribution Market Simulation", "abstract": "This paper presents OpenGridGym, an open-source Python-based package that allows for seamless integration of distribution market simulation with state-of-the-art artificial intelligence (AI) decision-making algorithms. We present the architecture and design choice for the proposed framework, elaborate on how users interact with OpenGridGym, and highlight its value by providing multiple cases to demonstrate its use. Four modules are used in any simulation: (1) the physical grid, (2) market mechanisms, (3) a set of trainable agents which interact with the former two modules, and (4) environment module that connects and coordinates the above three. We provide templates for each of those four, but they are easily interchangeable with custom alternatives. Several case studies are presented to illustrate the capability and potential of this toolkit in helping researchers address key design and operational questions in distribution electricity markets."}}
{"id": "kCtnkLv-_W0", "cdate": 1652737799021, "mdate": null, "content": {"title": "Enhanced Meta Reinforcement Learning via Demonstrations in Sparse Reward Environments", "abstract": "Meta reinforcement learning (Meta-RL) is an approach wherein the experience gained from solving a variety of tasks is distilled into a meta-policy. The meta-policy, when adapted over only a small (or just a single) number of steps, is able to perform near-optimally on a new, related task.  However, a major challenge to adopting this approach to solve real-world problems is that they are often associated with sparse reward functions that only indicate whether a task is completed partially or fully. We consider the situation where some data, possibly generated by a sub-optimal agent, is available for each task. We then develop a class of algorithms entitled Enhanced Meta-RL via Demonstrations (EMRLD) that exploit this information---even if sub-optimal---to obtain guidance during training. We show how EMRLD jointly utilizes RL and supervised learning over the offline data to generate a meta-policy that demonstrates monotone performance improvements. We also develop a warm started variant called EMRLD-WS that is particularly efficient for sub-optimal demonstration data. Finally, we show that our EMRLD algorithms significantly outperform existing approaches in a variety of sparse reward environments, including that of a mobile robot."}}
{"id": "U4BUMoVTrB2", "cdate": 1652737798970, "mdate": null, "content": {"title": "DOPE: Doubly Optimistic and Pessimistic Exploration for Safe Reinforcement Learning", "abstract": "Safe reinforcement learning is extremely challenging--not only must the agent explore an unknown environment, it must do so while ensuring no safety constraint violations. We formulate this safe  reinforcement learning (RL) problem using the framework of a finite-horizon Constrained Markov Decision Process (CMDP) with an unknown transition probability function, where we model the safety requirements as constraints on the expected cumulative costs that must be satisfied during all episodes of learning.  We propose a model-based safe RL algorithm that we call Doubly Optimistic and Pessimistic Exploration (DOPE), and show that it achieves an objective regret $\\tilde{O}(|\\mathcal{S}|\\sqrt{|\\mathcal{A}| K})$ without violating the safety constraints during learning, where  $|\\mathcal{S}|$ is the number of states, $|\\mathcal{A}|$ is the number of actions, and $K$ is the number of learning episodes.  Our key idea is to combine a reward bonus for exploration (optimism) with a conservative constraint (pessimism), in addition to the standard optimistic model-based exploration.  DOPE is not only able to improve the objective regret bound, but also shows a significant empirical performance improvement as compared to earlier optimism-pessimism approaches. "}}
{"id": "sjL7ktVvoA3", "cdate": 1640995200000, "mdate": 1682318042872, "content": {"title": "Energy System Digitization in the Era of AI: A Three-Layered Approach towards Carbon Neutrality", "abstract": "The transition towards carbon-neutral electricity is one of the biggest game changers in addressing climate change since it addresses the dual challenges of removing carbon emissions from the two largest sectors of emitters: electricity and transportation. The transition to a carbon-neutral electric grid poses significant challenges to conventional paradigms of modern grid planning and operation. Much of the challenge arises from the scale of the decision making and the uncertainty associated with the energy supply and demand. Artificial Intelligence (AI) could potentially have a transformative impact on accelerating the speed and scale of carbon-neutral transition, as many decision making processes in the power grid can be cast as classic, though challenging, machine learning tasks. We point out that to amplify AI's impact on carbon-neutral transition of the electric energy systems, the AI algorithms originally developed for other applications should be tailored in three layers of technology, markets, and policy."}}
{"id": "qjPy0etchoI", "cdate": 1640995200000, "mdate": 1682318041899, "content": {"title": "Energy system digitization in the era of AI: A three-layered approach toward carbon neutrality", "abstract": ""}}
{"id": "pEfpvOWlT1e", "cdate": 1640995200000, "mdate": 1659140070950, "content": {"title": "QFlow: A Learning Approach to High QoE Video Streaming at the Wireless Edge", "abstract": "The predominant use of wireless access networks is for media streaming applications. However, current access networks treat all packets identically, and lack the agility to determine which clients are most in need of service at a given time. Software reconfigurability of networking devices has seen wide adoption, and this in turn implies that agile control policies can be now instantiated on access networks. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to develop QFlow, a platform that instantiates this feedback loop, and instantiate a variety of control policies over it. We use the popular application of video streaming over YouTube as our use case. Our context is priority queueing, with the action space being that of determining which clients should be assigned to each queue at each decision period. We first develop policies based on model-based and model-free reinforcement learning. We then design an auction-based system under which clients place bids for priority service, as well as a more structured index-based policy. Through experiments, we show how these learning-based policies on QFlow are able to select the right clients for prioritization in a high-load scenario to outperform the best known solutions with over 25&#x0025; improvement in QoE, and a perfect QoE score of 5 over 85&#x0025; of the time."}}
{"id": "nYPQ-Uci9u", "cdate": 1640995200000, "mdate": 1671929032914, "content": {"title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration", "abstract": "A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback. Often, what is available is an intuitive but sparse reward function that only indicates whether the task is completed partially or fully. However, the lack of carefully designed, fine grain feedback implies that most existing RL algorithms fail to learn an acceptable policy in a reasonable time frame. This is because of the large number of exploration actions that the policy has to perform before it gets any useful feedback that it can learn from. In this work, we address this challenging problem by developing an algorithm that exploits the offline demonstration data generated by {a sub-optimal behavior policy} for faster and efficient online RL in such sparse reward settings. The proposed algorithm, which we call the Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demonstration data. The key idea is that by obtaining guidance from - not imitating - the offline {data}, LOGO orients its policy in the manner of the sub-optimal {policy}, while yet being able to learn beyond and approach optimality. We provide a theoretical analysis of our algorithm, and provide a lower bound on the performance improvement in each learning episode. We also extend our algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation. We demonstrate the superior performance of our algorithm over state-of-the-art approaches on a number of benchmark environments with sparse rewards {and censored state}. Further, we demonstrate the value of our approach via implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance, where it shows excellent performance."}}
{"id": "UtA_L4Ooaug", "cdate": 1640995200000, "mdate": 1659140070949, "content": {"title": "Learning to Cache and Caching to Learn: Regret Analysis of Caching Algorithms", "abstract": "Crucial performance metrics of a caching algorithm include its ability to quickly and accurately learn a popularity distribution of requests. However, a majority of work on analytical performance analysis focuses on hit probability after an asymptotically large time has elapsed. We consider an online learning viewpoint, and characterize the &#x201C;regret&#x201D; in terms of the finite time difference between the hits achieved by a candidate caching algorithm with respect to a genie-aided scheme that places the most popular items in the cache. We first consider the Full Observation regime wherein all requests are seen by the cache. We show that the Least Frequently Used (LFU) algorithm is able to achieve order optimal regret, which is matched by an efficient counting algorithm design that we call LFU-Lite. We then consider the Partial Observation regime wherein only requests for items currently cached are seen by the cache, making it similar to an online learning problem related to the multi-armed bandit problem. We show how approaching this &#x201C;caching bandit&#x201D; using traditional approaches yields either high complexity or regret, but a simple algorithm design that exploits the structure of the distribution can ensure order optimal regret. We conclude by illustrating our insights using numerical simulations."}}
{"id": "OEGLOuzGvC", "cdate": 1640995200000, "mdate": 1682318041662, "content": {"title": "Realtime intelligent control for NextG cellular radio access networks", "abstract": "RAN Intelligent Control (RIC) has developed in parallel with Open Radio Access Networks (O-RAN) as a means of utilizing newly available interfaces. Focus has been largely on non-realtime (non-RT: > 1 sec) dealing with RAN management and offline training, and near-realtime (near-RT: 10 ms to 1 sec) dealing with UE load balancing and RAN configuration. We contend that the true power of RIC can be unleashed only with realtime (RT: < 100 \u03bcs) measurement, optimization, and control of RAN resources, corresponding to the cellular transmission time interval (TTI: 125 \u03bcs to 1 ms)."}}
{"id": "MPd6DgINRK", "cdate": 1640995200000, "mdate": 1682318043267, "content": {"title": "Multi-Agent Learning via Markov Potential Games in Marketplaces for Distributed Energy Resources", "abstract": "Much change is happening in electricity markets due to the entrance of small-scale prosumers that both generate and consume electricity. Both large and small consumers can also be incentivized to reduce their demand during peak load periods, referred to as demand-response. The net effect of such distributed energy resources (DERs) on the grid can be quite substantial, and designing secondary markets wherein such DERs can participate repeatedly over time has become important. Many such marketplaces have a so-called potential game structure, in that a unilateral change in the strategy of an agent causes equivalent changes in both its own reward and a global potential function. We consider a dynamic setting in which each stage is a potential game, but is accompanied by Markovian state transitions, which we call Markov Potential Games (MPG). It is well known that it is formidably challenging to compute or learn Nash Equilibria (NE) in Markov Games. We develop a key concept that we term as the potential value function that ties together the potential function in the stage game with the value function in a Markov Decision Process. We first show that an NE can be computed in a centralized manner by maximizing the potential value function. We also show NE can also be obtained in a multi-agent manner via asynchronous better (not necessarily best) response updates that are consistent with a simple multi-agent reinforcement learning algorithm. Finally, we show several examples wherein the MPG framework applies to DER dynamics in an electricity marketplace, and numerically study the efficiency of the equilibria attained."}}
