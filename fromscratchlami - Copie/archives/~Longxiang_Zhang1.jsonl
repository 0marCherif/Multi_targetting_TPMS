{"id": "5ftvY-QYDoN", "cdate": 1682692240883, "mdate": 1682692240883, "content": {"title": "Are You Dictating to Me? Detecting Embedded Dictations in Doctor-Patient Conversations", "abstract": "Medical scribes chart doctor-patient conversations in real time or by listening to an audio recording afterwards. Doctors sometimes dictate during a patient encounter, a highly informative part for a scribe. We introduced a light-weight annotation schema and ana-lyzed recordings of 105 randomly selected doctor-patient encounters from 21 physicians to quantify the frequency and automatically de-tect dictated regions. Dictation behavior of individual doctors was consistent but varied among them. A linguistic analysis is provided to describe differences of doctors speech when talking to a patient or dictating. A description of the data is given, highlighting challenges of segmenting audio into conversation and dictation regions. We in-vestigate different features and methods to segment conversations including keyword spotting, acoustic features and class-conditioned language models. Results are anchored to a majority class base-line. Using only acoustic features allows to predict dictated speech without the need of a speech recognition system performing com-parable to a rule-based approach using lexical features derived from a speech recognition system. Performance is assessed using leave-one-physician-out cross validation and an analysis using a random forest classifier indicates that language model derived features are most useful, and that a combination of acoustic and lexical features performed best."}}
{"id": "IsXHPx_MEK", "cdate": 1682691988275, "mdate": 1682691988275, "content": {"title": "Extract and Abstract with BART for Clinical Notes from Doctor-Patient Conversations", "abstract": "Reducing the burden of documentation physicians are required to do with speech understanding is a challenging and worthwhile goal with the potential to improve care. When transcripts of doctor-patient conversations are available, automatic summarization with deep neural networks is one promising solution to reducing documentation workload. We develop an ``extract-and-abstract'' approach to automatic generation of the History of Present Illness (HPI) section in clinical notes with BART: we train a classifier on annotated data to predict a clinical section each utterance is most relevant to; we then utilize the trained classifier to select only utterances from conversations relevant to HPI to be considered as input to BART for summarization; we experiment with additional filtering methods on selected utterances to further reduce input truncation due to the token limit of BART model. Results show that the generated summaries from our approach improve in both ROUGE scores and extracted medical concepts over previously published results. Considering the improvement is achieved with a relatively small set of doctor-patient conversations, we expect further improvement with more labeled data in the future."}}
{"id": "7qIhsY-WDuh", "cdate": 1682691822796, "mdate": 1682691822796, "content": {"title": "In-Domain Pre-Training Improves Clinical Note Generation from Doctor-Patient Conversations", "abstract": "Summarization of doctor-patient conversations into clinical notes by medical scribes is an essential process for effective clinical care. Pre-trained transformer models have shown a great amount of success in this area, but the domain shift from standard NLP tasks to the medical domain continues to present challenges. We build upon several recent works to show that additional pre-training with in-domain medical conversations leads to performance gains for clinical summarization. In addition to conventional evaluation metrics, we also explore a clinical named entity recognition model for concept-based evaluation. Finally, we contrast long-sequence transformers with a common transformer model, BART. Overall, our findings corroborate research in non-medical domains and suggest that in-domain pre-training combined with transformers for long sequences are effective strategies for summarizing clinical encounters."}}
{"id": "4E6Tfv2s0ai", "cdate": 1682691751615, "mdate": 1682691751615, "content": {"title": "Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations", "abstract": "Fine-tuning pretrained models for automatically summarizing doctor-patient conversation\ntranscripts presents many challenges: limited\ntraining data, significant domain shift, long\nand noisy transcripts, and high target summary variability. In this paper, we explore\nthe feasibility of using pretrained transformer\nmodels for automatically summarizing doctorpatient conversations directly from transcripts.\nWe show that fluent and adequate summaries\ncan be generated with limited training data by\nfine-tuning BART on a specially constructed\ndataset. The resulting models greatly surpass\nthe performance of an average human annotator and the quality of previous published work\nfor the task. We evaluate multiple methods for\nhandling long conversations, comparing them\nto the obvious baseline of truncating the conversation to fit the pretrained model length\nlimit. We introduce a multistage approach\nthat tackles the task by learning two finetuned models: one for summarizing conversation chunks into partial summaries, followed\nby one for rewriting the collection of partial\nsummaries into a complete summary1\n. Using a carefully chosen fine-tuning dataset, this\nmethod is shown to be effective at handling\nlonger conversations, improving the quality\nof generated summaries. We conduct both\nan automatic evaluation (through ROUGE and\ntwo concept-based metrics focusing on medical findings) and a human evaluation (through\nqualitative examples from literature, assessing\nhallucination, generalization, fluency, and general quality of the generated summaries)."}}
