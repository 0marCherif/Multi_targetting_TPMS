{"id": "-uECc7yLFQ", "cdate": 1672531200000, "mdate": 1681525944530, "content": {"title": "A Bias-Variance-Privacy Trilemma for Statistical Estimation", "abstract": ""}}
{"id": "c63eTNYh9Y", "cdate": 1652737344050, "mdate": null, "content": {"title": "New Lower Bounds for Private Estimation and a Generalized Fingerprinting Lemma", "abstract": "We prove new lower bounds for statistical estimation tasks under the constraint of $(\\varepsilon,\\delta)$-differential privacy. First, we provide tight lower bounds for private covariance estimation of Gaussian distributions. We show that estimating the covariance matrix in Frobenius norm requires $\\Omega(d^2)$ samples, and in spectral norm requires $\\Omega(d^{3/2})$ samples, both matching upper bounds up to logarithmic factors. We prove these bounds via our main technical contribution, a broad generalization of the fingerprinting method to exponential families. Additionally, using the private Assouad method of Acharya, Sun, and Zhang, we show a tight $\\Omega(d/(\\alpha^2 \\varepsilon))$ lower bound for estimating the mean of a distribution with bounded covariance to $\\alpha$-error in $\\ell_2$-distance. Prior known lower bounds for all these problems were either polynomially weaker or held under the stricter condition of $(\\varepsilon,0)$-differential privacy."}}
{"id": "pyvPo-lmvhq", "cdate": 1640995200000, "mdate": 1681525944517, "content": {"title": "A Private and Computationally-Efficient Estimator for Unbounded Gaussians", "abstract": ""}}
{"id": "XuQB5pRM74T", "cdate": 1640995200000, "mdate": 1681525944569, "content": {"title": "New Lower Bounds for Private Estimation and a Generalized Fingerprinting Lemma", "abstract": ""}}
{"id": "TjI_v6NfAIV", "cdate": 1609459200000, "mdate": 1652906857416, "content": {"title": "A Private and Computationally-Efficient Estimator for Unbounded Gaussians", "abstract": "We give the first polynomial-time, polynomial-sample, differentially private estimator for the mean and covariance of an arbitrary Gaussian distribution $\\mathcal{N}(\\mu,\\Sigma)$ in $\\mathbb{R}^d$. All previous estimators are either nonconstructive, with unbounded running time, or require the user to specify a priori bounds on the parameters $\\mu$ and $\\Sigma$. The primary new technical tool in our algorithm is a new differentially private preconditioner that takes samples from an arbitrary Gaussian $\\mathcal{N}(0,\\Sigma)$ and returns a matrix $A$ such that $A \\Sigma A^T$ has constant condition number."}}
