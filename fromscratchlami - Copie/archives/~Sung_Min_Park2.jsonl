{"id": "Din42zykEHM", "cdate": 1672531200000, "mdate": 1681491670115, "content": {"title": "TRAK: Attributing Model Behavior at Scale", "abstract": ""}}
{"id": "74UfM1WeBx", "cdate": 1664928787637, "mdate": null, "content": {"title": "A Unified Framework for Comparing Learning Algorithms", "abstract": "Understanding model biases is crucial to understanding how models will perform out-of-distribution (OOD). These biases often stem from particular design choices (e.g., architecture or data augmentation). We propose a framework for (learning) algorithm comparisons, wherein the goal is to find similarities and differences between models trained with two different learning algorithms. We begin by formalizing the goal of algorithm comparison as finding distinguishing feature transformations, input transformations that change the predictions of models trained with one learning algorithm but not the other. We then present a two-stage method for algorithm comparisons based on comparing how models use the training data, leveraging the recently proposed datamodel representations [IPE+22]. We demonstrate our framework through a case study comparing classifiers trained on the Waterbirds [SKH+20] dataset with/without ImageNet pre-training.\n"}}
{"id": "Ew9gIwAQ7wr", "cdate": 1663850120326, "mdate": null, "content": {"title": "FFCV: Accelerating Training by Removing Data Bottlenecks", "abstract": "We present FFCV, a library for easy, fast, resource-efficient training of machine learning models. FFCV speeds up model training by eliminating (often subtle) data bottlenecks from the training process. In particular, we combine techniques such as an efficient file storage format, caching, data pre-loading, asynchronous data transfer, and just-in-time compilation to (a) make data loading and transfer significantly more efficient, ensuring that GPUs can reach full utilization; and (b) offload as much data processing as possible to the CPU asynchronously, freeing GPU up capacity for training. Using FFCV, we train ResNet-18 and ResNet-50 on the ImageNet dataset with a state-of-the-art tradeoff between accuracy and training time. For example, across the range of ResNet-50 models we test, we obtain the same accuracy as the best baselines in half the time. We demonstrate FFCV's performance, ease-of-use, extensibility, and ability to adapt to resource constraints through several case studies."}}
{"id": "IrUFsuTxVfY", "cdate": 1663850045187, "mdate": null, "content": {"title": "A Data-Based Perspective on Transfer Learning", "abstract": "It is commonly believed that more pre-training data leads to better transfer learning performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we present a framework for probing the impact of the source dataset's composition on transfer learning performance. Our framework facilitates new capabilities such as identifying transfer learning brittleness and detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer performance from ImageNet on a variety of transfer tasks."}}
{"id": "dYQnWPqCCAs", "cdate": 1663850028258, "mdate": null, "content": {"title": "A Unified Framework for Comparing Learning Algorithms", "abstract": "We propose a framework for {\\em (learning) algorithm comparisons}, wherein the goal is to find similarities and differences between models trained with two different learning algorithms. We begin by formalizing the goal of algorithm comparison as finding {\\em distinguishing feature transformations}, input transformations that change the predictions of models trained with one learning algorithm but not the other. We then present a two-stage method for algorithm comparisons based on comparing how models use the training data, leveraging the recently proposed datamodel representations [Ilyas et al., 2022]. We demonstrate our framework through three case studies that compare models trained with/without standard data augmentation, with/without pre-training, and with different optimizer hyperparameters. "}}
{"id": "z-OG4cwKknV", "cdate": 1652636120846, "mdate": 1652636120846, "content": {"title": "Sparse PCA from Sparse Linear Regression", "abstract": "Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but an explicit connection between the two had not been made. We show how to efficiently transform a black-box solver for SLR into an algorithm for SPCA: assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the SPCA algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomialtime algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. We provide experimental results on simulated data comparing our proposed framework to other algorithms for SPCA."}}
{"id": "EPDLRUMI_Uf", "cdate": 1652636073977, "mdate": 1652636073977, "content": {"title": "Datamodels: Predicting Predictions from Training Data", "abstract": "We present a conceptual framework, datamodeling, for analyzing the behavior of a model class in terms of the training data. For any fixed \"target\" example x, training set S, and learning algorithm, a datamodel is a parameterized function 2S\u2192\u211d that for any subset of S\u2032\u2282S -- using only information about which examples of S are contained in S\u2032 -- predicts the outcome of training a model on S\u2032 and evaluating on x. Despite the potential complexity of the underlying process being approximated (e.g., end-to-end training and evaluation of deep neural networks), we show that even simple linear datamodels can successfully predict model outputs. We then demonstrate that datamodels give rise to a variety of applications, such as: accurately predicting the effect of dataset counterfactuals; identifying brittle predictions; finding semantically similar examples; quantifying train-test leakage; and embedding data into a well-behaved and feature-rich representation space. Data for this paper (including pre-computed datamodels as well as raw predictions from four million trained deep neural networks) is available at this https URL ."}}
{"id": "a669VvcdE1", "cdate": 1640995200000, "mdate": 1668797182347, "content": {"title": "A Data-Based Perspective on Transfer Learning", "abstract": "It is commonly believed that in transfer learning including more pre-training data translates into better performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we take a closer look at the role of the source dataset's composition in transfer learning and present a framework for probing its impact on downstream performance. Our framework gives rise to new capabilities such as pinpointing transfer learning brittleness as well as detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer learning performance from ImageNet on a variety of target tasks. Code is available at https://github.com/MadryLab/data-transfer"}}
{"id": "KWn1LIw1TAk", "cdate": 1640995200000, "mdate": 1681491670041, "content": {"title": "Datamodels: Predicting Predictions from Training Data", "abstract": ""}}
{"id": "4ytwTlORZ_d", "cdate": 1640995200000, "mdate": 1681491669925, "content": {"title": "Datamodels: Understanding Predictions with Data and Data with Predictions", "abstract": ""}}
