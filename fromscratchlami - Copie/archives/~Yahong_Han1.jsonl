{"id": "yseZHK8LjB", "cdate": 1698796800000, "mdate": 1696076122431, "content": {"title": "Source-free and black-box domain adaptation via distributionally adversarial training", "abstract": ""}}
{"id": "0jzr0Px47Tk", "cdate": 1698569040790, "mdate": 1698569040790, "content": {"title": "Camouflaged Object Segmentation Based on Matching\u2013Recognition\u2013Refinement Network", "abstract": "In the biosphere, camouflaged objects take the advantage of visional wholeness by keeping the color and texture of the objects highly consistent with the background, thereby confusing the visual mechanism of other creatures and achieving a concealed effect. This is also the main reason why the task of camouflaged object detection is challenging. In this paper, we break the visual wholeness and see through the camouflage from the perspective of matching the appropriate field of view. We propose a matching-recognition-refinement network (MRRNet), which consists of two key modules, i.e., the visual field matching and recognition module (VFMRM) and the step-wise refinement module (SWRM). In the VFMRM, various feature receptive fields are used to match candidate areas of camouflaged objects of different sizes and shapes, and adaptively activate and recognize the approximate area of the real camouflaged object. The SWRM then uses the features extracted by the backbone to gradually refine the camouflaged region obtained by VFMRM, and thus yielding the complete camouflaged object. In addition, a more efficient deep supervision method is exploited, making the features from the backbone input into the SWRM more critical and not redundant. Extensive experimental results demonstrate that our MRR-Net runs in real-time (82.6 FPS) and significantly outperforms 30 state-of-the-art models on three challenging datasets under three standard metrics. Furthermore, MRR-Net is applied to 4 downstream tasks of camouflaged object segmentation, and the results validate its practical application value. Our code is publicly available at: https://github.com/XinyuYanTJU/MRR-Net."}}
{"id": "hk-y9mMD_T", "cdate": 1690848000000, "mdate": 1696076122417, "content": {"title": "Weakly supervised anomaly detection with multi-level contextual modeling", "abstract": "Due to the low frequency of abnormal behaviors in the real world and the complexity of the definition of abnormal behaviors, anomaly detection in the surveillance video is very challenging. Weakly supervised video anomaly detection has recently been formulated as a multiple instance learning task. Although current methods show effective detection performance and alleviate the imbalanced data problem caused by the scarcity of abnormal behaviors, it still faces great challenges in distinguishing abnormal behaviors similar to normal behaviors. Also, the frequently used methods for weakly supervised video anomaly detection sometimes overlook the impact of the temporal factor. For example, the surrounding of the most anomalous video segment is more likely to be abnormal also. To alleviate the issue mentioned above, we propose a cascaded multi-level contextual content analysis module (CMC), which adapts temporal-aware graph convolutional network and non-local neural network to aggregate the contextual features of local and non-local video clips. CMC enlarges the distance between hard positive abnormal instances and normal instances and further strengthens the expression of multiple instance features. We evaluate our method on two benchmark datasets and conduct extensive ablation studies. The performance improvement demonstrates the effectiveness of our method."}}
{"id": "QLQ_wAsZj6O", "cdate": 1690848000000, "mdate": 1696076122410, "content": {"title": "Domain-specific feature elimination: multi-source domain adaptation for image classification", "abstract": "Multi-source domain adaptation utilizes multiple source domains to learn the knowledge and transfers it to an unlabeled target domain. To address the problem, most of the existing methods aim to minimize the domain shift by auxiliary distribution alignment objectives, which reduces the effect of domain-specific features. However, without explicitly modeling the domain-specific features, it is not easy to guarantee that the domain-invariant representation extracted from input domains contains domain-specific information as few as possible. In this work, we present a different perspective on MSDA, which employs the idea of feature elimination to reduce the influence of domain-specific features. We design two different ways to extract domain-specific features and total features and construct the domain-invariant representations by eliminating the domain-specific features from total features. The experimental results on different domain adaptation datasets demonstrate the effectiveness of our method and the generalization ability of our model."}}
{"id": "FWzaKSHNOm", "cdate": 1682899200000, "mdate": 1696076122434, "content": {"title": "Multi-Source Collaborative Contrastive Learning for Decentralized Domain Adaptation", "abstract": "Unsupervised multi-source domain adaptation aims to obtain a model working well on the unlabeled target domain by reducing the domain gap between the labeled source domains and the unlabeled target domain. Considering the data privacy and storage cost, data from multiple source domains and target domain are isolated and decentralized. This data decentralization scenario brings the difficulty of domain alignment for reducing the domain gap between the decentralized source domains and target domain, respectively. For conducting domain alignment under the data decentralization scenario, we propose Multi-source Collaborative Contrastive learning for decentralized Domain Adaptation (MCC-DA). The models from other domains are used as the bridge to reduce the domain gap. On the source domains and target domain, we penalize the inconsistency of data features extracted from the source domain models and target domain model by contrastive alignment. With the collaboration of source domain models and target domain model, the domain gap between decentralized source domains and target domain is reduced without accessing the data from other domains. The experiment results on multiple benchmarks indicate that our method can reduce the domain gap effectively and outperform the state-of-the-art methods significantly."}}
{"id": "svtNLGw1VY", "cdate": 1672531200000, "mdate": 1696076122405, "content": {"title": "Discriminative and Contrastive Consistency for Semi-supervised Domain Adaptive Image Classification", "abstract": "With sufficient source and limited target supervised information, semi-supervised domain adaptation (SSDA) aims to perform well on unlabeled target domain. Although various strategies have been proposed in SSDA field, they fail to fully exploit limited target labels and adequately explore domain-invariant knowledge. In this study, we propose a framework that first introduces consistent processing of augmented training data based on contrastive learning. Specifically, supervised contrastive learning is introduced to assist the classical cross-entropy iteration to make full use of the limited target labels. Additionally, traditional unsupervised contrastive learning and pseudo-labeling are utilized to further minimize the intra-domain discrepancy. Besides, an adversarial loss is then combined with a sharpening function to acquire a more certain category center that is domain-invariant. Experimental results on DomainNet, Office-Home, and Office show the effectiveness of our method. Particularly, for 1-shot case of Office-Home with AlexNet as backbone, our method outperforms the previous state-of-the-art by 5.6% in terms of mean accuracy."}}
{"id": "a8-6ufhyr2", "cdate": 1672531200000, "mdate": 1684308551581, "content": {"title": "Weighted progressive alignment for multi-source domain adaptation", "abstract": "Multi-source domain adaptation (MSDA) dedicates to establishing knowledge transfer from multiple labeled source domains to an unlabeled target domain. Although data from multiple source domains can provide rich information, it also brings two problems. First, it is not easy to directly align multiple source domains and target domain, because there are complex interactions among multiple source domains with different distributions. Second, some of the source samples may contribute negatively to domain adaptation. Thus, how to select the appropriate source domain samples is worth exploring. To solve these problems, we propose a novel framework of weighted progressive alignment (WPA), in which we develop a two-stage alignment with two distinct domain classifiers, as well as a dedicated classifier to judge the importance of source domain samples. Our proposed method progressively achieves multi-source domain adaptation through domain-adversarial training and coarse-to-fine alignment. We evaluate our framework on four public benchmark datasets. The extensive experimental results demonstrate that the proposed method achieves great performance."}}
{"id": "Zr1mciR9tL", "cdate": 1672531200000, "mdate": 1683983908914, "content": {"title": "Query-Efficient Black-Box Adversarial Attack With Customized Iteration and Sampling", "abstract": "It is a challenging task to fool an image classifier based on deep neural networks under the black-box setting where the target model can only be queried. Among existing black-box attacks, transfer-based methods tend to overfit the substitute model on parameter settings. Decision-based methods have low query efficiency due to fixed sampling and greedy search strategy. To alleviate the above problems, we present a new framework for query-efficient black-box adversarial attack by bridging transfer-based and decision-based attacks. We reveal the relationship between current noise and variance of sampling, the monotonicity of noise compression, and the influence of transition function on the decision-based attack. Guided by the new framework, we propose a black-box adversarial attack named Customized Iteration and Sampling Attack (CISA). CISA estimates the distance from nearby decision boundary to set the stepsize, and uses a dual-direction iterative trajectory to find the intermediate adversarial example. Based on the intermediate adversarial example, CISA conducts customized sampling according to the noise sensitivity of each pixel to further compress noise, and relaxes the state transition function to achieve higher query efficiency. Extensive experiments demonstrate CISA\u2019s advantage in query efficiency of black-box adversarial attacks."}}
{"id": "GcBggZUyd4c", "cdate": 1672531200000, "mdate": 1683983912414, "content": {"title": "Active and Compact Entropy Search for High-Dimensional Bayesian Optimization", "abstract": "Entropy search and its derivative methods are one class of Bayesian Optimization methods that achieve active exploration of black-box functions. They maximize the information gain about the position in the input space where the black-box function gets the global optimum. However, existing entropy search methods suffer from harassment caused by high dimensional optimization problems. On the one hand, the computation for estimating entropies increases exponentially as dimensions increase, which limits the applicability of entropy search to high dimensional problems. On the other hand, many high-dimensional problems have the property that a large number of dimensions have little influence on the objective function, but currently there is no compress mechanism to exclude these redundant dimensions. In this work, we propose Active Compact Entropy Search (AcCES) to fix the above two defects. Under the guidance of historical evaluation, we bring forward a novel acquisition function that considers the correlation between dimensions in entropy search, which is ignored by existing Bayesian Optimization methods. The correlation term added in the acquisition function will help discover the potential correlation between dimensions. In order to build a more compact input space, redundant dimensions are compressed by exploiting inter-dimensional correlations. We use Pearson Correlation Coefficient and curve fitting to represent the inter-dimensional correlations. Extensive experiments on several benchmarks demonstrate that AcCES achieves higher query efficiency as well as optimal results after convergence than existing entropy search methods."}}
{"id": "CwQCeJnteii", "cdate": 1652737614496, "mdate": null, "content": {"title": "Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal", "abstract": "Vision transformers (ViTs) have demonstrated impressive performance and stronger adversarial robustness compared to Convolutional Neural Networks (CNNs). On the one hand, ViTs' focus on global interaction between individual patches reduces the local noise sensitivity of images. On the other hand, the neglect of noise sensitivity differences between image regions by existing decision-based attacks further compromises the efficiency of noise compression, especially for ViTs. Therefore, validating the black-box adversarial robustness of ViTs when the target model can only be queried still remains a challenging problem. In this paper, we theoretically analyze the limitations of existing decision-based attacks from the perspective of noise sensitivity difference between regions of the image, and propose a new decision-based black-box attack against ViTs, termed Patch-wise Adversarial Removal (PAR). PAR divides images into patches through a coarse-to-fine search process and compresses the noise on each patch separately. PAR records the noise magnitude and noise sensitivity of each patch and selects the patch with the highest query value for noise compression. In addition, PAR can be used as a noise initialization method for other decision-based attacks to improve the noise compression efficiency on both ViTs and CNNs without introducing additional calculations. Extensive experiments on three datasets demonstrate that PAR achieves a much lower noise magnitude with the same number of queries."}}
