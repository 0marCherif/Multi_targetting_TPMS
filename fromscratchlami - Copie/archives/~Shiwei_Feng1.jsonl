{"id": "N8yEnKn6gy", "cdate": 1672531200000, "mdate": 1699146461035, "content": {"title": "Fusion is Not Enough: Single-Modal Attacks to Compromise Fusion Models in Autonomous Driving", "abstract": "Multi-sensor fusion (MSF) is widely adopted for perception in autonomous vehicles (AVs), particularly for the task of 3D object detection with camera and LiDAR sensors. The rationale behind fusion is to capitalize on the strengths of each modality while mitigating their limitations. The exceptional and leading performance of fusion models has been demonstrated by advanced deep neural network (DNN)-based fusion techniques. Fusion models are also perceived as more robust to attacks compared to single-modal ones due to the redundant information in multiple modalities. In this work, we challenge this perspective with single-modal attacks that targets the camera modality, which is considered less significant in fusion but more affordable for attackers. We argue that the weakest link of fusion models depends on their most vulnerable modality, and propose an attack framework that targets advanced camera-LiDAR fusion models with adversarial patches. Our approach employs a two-stage optimization-based strategy that first comprehensively assesses vulnerable image areas under adversarial attacks, and then applies customized attack strategies to different fusion models, generating deployable patches. Evaluations with five state-of-the-art camera-LiDAR fusion models on a real-world dataset show that our attacks successfully compromise all models. Our approach can either reduce the mean average precision (mAP) of detection performance from 0.824 to 0.353 or degrade the detection score of the target object from 0.727 to 0.151 on average, demonstrating the effectiveness and practicality of our proposed attack framework."}}
{"id": "E39mJ-6cwm", "cdate": 1672531200000, "mdate": 1681586542382, "content": {"title": "Detecting Backdoors in Pre-trained Encoders", "abstract": ""}}
{"id": "1L2LPjz2yqH", "cdate": 1672531200000, "mdate": 1680903477154, "content": {"title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense", "abstract": ""}}
{"id": "Xo2E217_M4n", "cdate": 1663850525684, "mdate": null, "content": {"title": "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning", "abstract": "Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP."}}
{"id": "O_N-L67HNb", "cdate": 1577836800000, "mdate": 1667309730983, "content": {"title": "Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem", "abstract": ""}}
