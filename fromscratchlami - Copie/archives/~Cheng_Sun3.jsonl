{"id": "ix-zOkWK69i1", "cdate": 1640995200000, "mdate": 1665999649944, "content": {"title": "Self-supervised 360\u00b0 Room Layout Estimation", "abstract": "We present the first self-supervised method to train panoramic room layout estimation models without any labeled data. Unlike per-pixel dense depth that provides abundant correspondence constraints, layout representation is sparse and topological, hindering the use of self-supervised reprojection consistency on images. To address this issue, we propose Differentiable Layout View Rendering, which can warp a source image to the target camera pose given the estimated layout from the target image. As each rendered pixel is differentiable with respect to the estimated layout, we can now train the layout estimation model by minimizing reprojection loss. Besides, we introduce regularization losses to encourage Manhattan alignment, ceiling-floor alignment, cycle consistency, and layout stretch consistency, which further improve our predictions. Finally, we present the first self-supervised results on ZilloIndoor and MatterportLayout datasets. Our approach also shows promising solutions in data-scarce scenarios and active learning, which would have an immediate value in the real estate virtual tour software. Code is available at https://github.com/joshua049/Stereo-360-Layout."}}
{"id": "cZlE8VeVzBi", "cdate": 1640995200000, "mdate": 1665999650170, "content": {"title": "Improved Direct Voxel Grid Optimization for Radiance Fields Reconstruction", "abstract": "In this technical report, we improve the DVGO framework (called DVGOv2), which is based on Pytorch and uses the simplest dense grid representation. First, we re-implement part of the Pytorch operations with cuda, achieving 2-3x speedup. The cuda extension is automatically compiled just in time. Second, we extend DVGO to support Forward-facing and Unbounded Inward-facing capturing. Third, we improve the space time complexity of the distortion loss proposed by mip-NeRF 360 from O(N^2) to O(N). The distortion loss improves our quality and training speed. Our efficient implementation could allow more future works to benefit from the loss."}}
{"id": "azgdzf_-Eh_", "cdate": 1640995200000, "mdate": 1680746409546, "content": {"title": "Data Efficient 3D Learner via Knowledge Transferred from 2D Model", "abstract": ""}}
{"id": "U1OlK221eeE", "cdate": 1640995200000, "mdate": 1680746409543, "content": {"title": "Multiview Regenerative Morphing with Dual Flows", "abstract": ""}}
{"id": "HW9tp6twV_5", "cdate": 1640995200000, "mdate": 1665999650143, "content": {"title": "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction", "abstract": "We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO."}}
{"id": "9WbFMayYeJF", "cdate": 1640995200000, "mdate": 1680746409538, "content": {"title": "Data Efficient 3D Learner via Knowledge Transferred from 2D Model", "abstract": ""}}
{"id": "jsYOzVMPgu9", "cdate": 1609459200000, "mdate": 1665999650180, "content": {"title": "HoHoNet: 360 Indoor Holistic Understanding With Latent Horizontal Features", "abstract": "We present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution 512x1024 panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin."}}
{"id": "QjKeZRfViZof", "cdate": 1609459200000, "mdate": 1665999649927, "content": {"title": "Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation", "abstract": "We present a novel pyramidal output representation to ensure parsimony with our \"specialize and fuse\" process for semantic segmentation. A pyramidal \"output\" representation consists of coarse-to-fine levels, where each level is \"specialize\" in a different class distribution (e.g., more stuff than things classes at coarser levels). Two types of pyramidal outputs (i.e., unity and semantic pyramid) are \"fused\" into the final semantic output, where the unity pyramid indicates unity-cells (i.e., all pixels in such cell share the same semantic label). The process ensures parsimony by predicting a relatively small number of labels for unity-cells (e.g., a large cell of grass) to build the final semantic output. In addition to the \"output\" representation, we design a coarse-to-fine contextual module to aggregate the \"features\" representation from different levels. We validate the effectiveness of each key module in our method through comprehensive ablation studies. Finally, our approach achieves state-of-the-art performance on three widely-used semantic segmentation datasets\u2014ADE20K, COCO-Stuff, and Pascal-Context."}}
{"id": "FLJtwo0C7zbr", "cdate": 1609459200000, "mdate": 1665999649945, "content": {"title": "Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama", "abstract": "We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets."}}
{"id": "-SxxzadIPz", "cdate": 1609459200000, "mdate": 1667877829381, "content": {"title": "Indoor Panorama Planar 3D Reconstruction via Divide and Conquer", "abstract": "Indoor panorama typically consists of human-made structures parallel or perpendicular to gravity. We leverage this phenomenon to approximate the scene in a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this end, we propose an effective divide-and-conquer strategy that divides pixels based on their plane orientation estimation; then, the succeeding instance segmentation module conquers the task of planes clustering more easily in each plane orientation group. Besides, parameters of V-planes depend on camera yaw rotation, but translation-invariant CNNs are less aware of the yaw change. We thus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We create a benchmark for indoor panorama planar reconstruction by extending existing 360 depth datasets with ground truth H&V-planes (referred to as \"PanoH&V\" dataset) and adopt state-of-the-art planar reconstruction methods to predict H&V-planes as our baselines. Our method outperforms the baselines by a large margin on the proposed dataset."}}
