{"id": "q72C7Yr3eho", "cdate": 1672531200000, "mdate": 1681504037032, "content": {"title": "Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series", "abstract": ""}}
{"id": "1X03klAkwJ-", "cdate": 1672531200000, "mdate": 1681504036495, "content": {"title": "Generative Modeling with Flow-Guided Density Ratio Learning", "abstract": ""}}
{"id": "zjSeBTEdXp1", "cdate": 1663850096360, "mdate": null, "content": {"title": "Deep Generative Wasserstein Gradient Flows", "abstract": "Deep generative modeling is a rapidly-advancing field with a wealth of modeling choices developed in the past decades. Amongst them, Wasserstein gradient flows (WGF) are a powerful and theoretically rich class of methods.\nHowever, their applications to high-dimensional distributions remain relatively underexplored. In this paper, we present Deep Generative Wasserstein Gradient Flows (DGGF), which constructs a WGF between two distributions by minimizing the entropy-regularized $f$-divergence. We demonstrate how to train a deep density ratio estimator that is required for the WGF and apply it to the task of generative modeling. Experiments demonstrate that DGGF is able to synthesize high-fidelity images of resolutions up to $128\\times128$, directly in data space. We demonstrate that DGGF has an interpretable diagnostic of sample quality by naturally estimating the KL divergence throughout the gradient flow. Finally, we show DGGF's modularity by composition with external density ratio estimators for conditional generation, as well as for unpaired image-to-image translation with no modifications to the framework."}}
{"id": "LaM6G4yrMy0", "cdate": 1621629979511, "mdate": null, "content": {"title": "Deep Explicit Duration Switching Models for Time Series", "abstract": "Many complex time series can be effectively subdivided into distinct regimes that exhibit persistent dynamics. Discovering the switching behavior and the statistical patterns in these regimes is important for understanding the underlying dynamical system. We propose the Recurrent Explicit Duration Switching Dynamical System (RED-SDS), a flexible model that is capable of identifying both state- and time-dependent switching dynamics. State-dependent switching is enabled by a recurrent state-to-switch connection and an explicit duration count variable is used to improve the time-dependent switching behavior. We demonstrate how to perform efficient inference using a hybrid algorithm that approximates the posterior of the continuous states via an inference network and performs exact inference for the discrete switches and counts. The model is trained by maximizing a Monte Carlo lower bound of the marginal log-likelihood that can be computed efficiently as a byproduct of the inference routine. Empirical results on multiple datasets demonstrate that RED-SDS achieves  considerable improvement in time series segmentation and competitive forecasting performance against the state of the art. "}}
{"id": "WzyoslQAQS", "cdate": 1609459200000, "mdate": 1681504036655, "content": {"title": "Deep Explicit Duration Switching Models for Time Series", "abstract": ""}}
{"id": "Vshd-njgPfi", "cdate": 1609459200000, "mdate": 1681504036419, "content": {"title": "Refining Deep Generative Models via Discriminator Gradient Flow", "abstract": ""}}
{"id": "RvlGEErg7v", "cdate": 1609459200000, "mdate": 1681504036837, "content": {"title": "Deep Explicit Duration Switching Models for Time Series", "abstract": ""}}
{"id": "Zbc-ue9p_rE", "cdate": 1601308093354, "mdate": null, "content": {"title": "Refining Deep Generative Models via Discriminator Gradient Flow", "abstract": "Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient $f$low (DG$f$low), a new technique that improves generated samples via the gradient flow of entropy-regularized $f$-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DG$f$low leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods."}}
{"id": "ZLyx6uXC3Y", "cdate": 1577836800000, "mdate": 1681504036575, "content": {"title": "Event-Driven Visual-Tactile Sensing and Learning for Robots", "abstract": ""}}
{"id": "MXqFm0RRn0k", "cdate": 1577836800000, "mdate": 1681504037039, "content": {"title": "Refining Deep Generative Models via Wasserstein Gradient Flows", "abstract": ""}}
