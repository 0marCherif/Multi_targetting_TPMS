{"id": "CPg5IRu9PL", "cdate": 1663850228677, "mdate": null, "content": {"title": "Efficient Large-scale Transformer Training via Random and Layerwise Token Dropping", "abstract": "Large-scale transformer models have become the de-facto architectures for various machine learning applications, e.g., CV and NLP. \nHowever, those large models also introduce prohibitive training costs. \nTo mitigate this issue, we propose a novel random and layerwise token dropping method (\\OURS), which skips the computation of a subset of the input tokens at all middle layers.\nParticularly, \\OURS achieves considerable speedups and comparable accuracy as the standard training baseline. \nCompared to other token dropping methods, \\OURS does not require (1) any importance score-based metrics, (2) any   special token treatment (e.g., \\texttt{[CLS]}), and (3) many layers in full sequence length training except the first and the last layers. \nBesides, a new \\layertoken learning rate schedule is proposed for pretraining problems that resolve the heavy tuning requirement for our proposed training mechanism. \nFinally, we demonstrate that \\OURS can be applied to broader applications, including \\gpt and \\bert pretraining as well as ViT and \\gpt finetuning tasks. \nOur results show that \\OURS can save about 33.3\\% theoretical compute cost and 25.6\\% wall-clock training time while achieving similar zero-shot evaluations on \\gptb as compared to baseline."}}
{"id": "HO2q49XYRC", "cdate": 1663850161930, "mdate": null, "content": {"title": "SaMoE: Parameter Efficient MoE Language Models via Self-Adaptive Expert Combination", "abstract": "Recently, Mixture-of-Experts (MoE) has demonstrated success in scaling models to have large amounts of parameters without significant increases in computational cost. However, MoEs have been also reported to be parameter inefficient such that larger models do not always lead to better performance.  \nIn this work, we study how to build parameter-efficient MoE models. Our analysis identifies that MoE layers exhibit poor gradient flow as the number of experts increases, leading to insufficient training of experts. To overcome this issue, we propose a new MoE architecture design (SaMoE), which improves the parameter efficiency of MoE models by learning a soft combination of a global set of expert layers for each MoE layer. Such a scheme enables substantial parameter savings on MoE while achieving comparable or better accuracy than the standard MoE training baseline. Extensive experiments on billion-scale GPT-3 style autoregressive MoE language models demonstrate that SaMoE significantly improves the parameter efficiency of MoE models by reducing up to 5.2X total parameters while obtaining superior pre-training and zero-shot generalization results as compared to baseline. "}}
{"id": "-CefY2EOupj", "cdate": 1663849920638, "mdate": null, "content": {"title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam", "abstract": "1-bit gradient compression and local steps are two representative techniques that enable drastic communication reduction in distributed SGD. Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose \\textbf{0/1 Adam} that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation. \\textbf{0/1 Adam} performs an Adam-like step to preserve the adaptivity, while its linearity allows utilizing 1-bit compression and local steps simultaneously for wall-clock time speed up. We provide convergence guarantee for \\textbf{0/1 Adam} on smooth non-convex objectives. On various large-scale benchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we demonstrate on up to 128 GPUs that \\textbf{0/1 Adam} is able to reduce up to 87\\% of data volume, 54\\% of communication rounds, and achieve up to 2$\\times$ higher training throughput and end-to-end training time reduction compared to the state-of-the-art baseline 1-bit Adam; while enjoying the same statistical convergence speed and end task model accuracy on GLUE dataset and ImageNet validation set. "}}
{"id": "_5jWpg7TK9b", "cdate": 1663849878775, "mdate": null, "content": {"title": "On the Effectiveness of Adapting Pre-trained Transformer Models via Adversarial Noise", "abstract": "Pretraining Transformer-based language models followed by adapting the pre-trained models to a downstream task is an effective transfer mechanism in NLP. While it is well-known that the pretraining stage is computationally expensive, the downstream adaptation also becomes costly as Transformers grow in size rapidly and the wide usage scenarios of fine-tuning pre-trained Transformers. In this work, we find that techniques that have demonstrated success in accelerating the pre-training tasks, such as large-batch optimizations, lead to severe accuracy degradation. We find strong regularization techniques such as adversarial training help to close the accuracy gap. However, the computational complexity associated with this approach, due to the high cost of generating adversaries, prevents it from reducing adaptation costs even with a large number of GPUs. As such, we systematically study both the computation efficiency and generalization of adversarial training for adapting pre-trained transformers, under a large-batch optimization regime. Our investigation yields simple yet effective algorithms for adapting transformer models. We show in experiments that our proposed method attains up to 9.8$\\times$ adaptation speedups over the baseline on BERT$_{base}$ and RoBERTa$_{large}$, while achieving comparable and sometimes higher accuracy than fine-tuning using existing baselines."}}
{"id": "xNeAhc2CNAl", "cdate": 1652737656006, "mdate": null, "content": {"title": "XTC: Extreme Compression for Pre-trained Transformers Made Simple and Efficient", "abstract": "Extreme compression, particularly ultra-low bit precision (binary/ternary) quantization, has been proposed to fit large NLP models on resource-constraint devices. \nHowever, to preserve the accuracy for such aggressive compression schemes, cutting-edge methods usually introduce complicated compression pipelines, e.g., multi-stage expensive knowledge distillation with extensive hyperparameter tuning. \nAlso, they oftentimes focus less on smaller transformer models that have already been heavily compressed via knowledge distillation and lack a systematic study to show the effectiveness of their methods.\nIn this paper, we perform a very comprehensive systematic study to measure the impact of many key hyperparameters and training strategies from previous. \nAs a result, we find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. \nBased on our study, we propose a simple yet effective compression pipeline for extreme compression. \nOur simplified pipeline demonstrates that\n(1) we can skip the pre-training knowledge distillation to obtain a 5-layer \\bert while achieving better performance than previous state-of-the-art methods, like TinyBERT; \n(2) extreme quantization plus layer reduction is able to reduce the model size by 50x, resulting in new state-of-the-art results on GLUE tasks."}}
{"id": "JpZ5du_Kdh", "cdate": 1652737640848, "mdate": null, "content": {"title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models", "abstract": "Recent works have demonstrated great success in pre-training large-scale autoregressive language models (e.g., GPT-3) on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efficiency dilemma: increasing the batch sizes and learning rates leads to better training efficiency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model with public dataset. We find that there is a strong correlation between training instability and extreme values of gradient variance. We further identify that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability.\n\nBased on the analysis, we present a simple yet effective Sequence Length Warmup method that aims to solve the training stability-efficiency dilemma by avoiding extreme gradient variance values. Moreover, we present a lightweight tuning strategy that allows us to tune our method with just a small portion of the expensive full training. Experiments replicating GPT-2 models (117M and 1.5B) show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99\\% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95\\% of accuracy under lower learning rate."}}
{"id": "iivHwZoWzR4", "cdate": 1652737556474, "mdate": null, "content": {"title": "On the Computational Efficiency of Adapting Transformer Models via Adversarial Noise", "abstract": "Pretraining Transformer-based language models followed by adapting the pre-trained models to a downstream task is an effective transfer mechanism in NLP. While it is well-known that the pretraining stage is computationally expensive, even the adaptation starts to become time-consuming for many downstream tasks as Transformers grow in size rapidly. \nPrior work focuses on reducing the pretraining wall-clock time via increasing the batch size to obtain higher training throughput on multiple processors. However, few studies have explored how such a scheme may benefit the adaptation phase. On the other hand, adversarial training has shown improved generalization for adapting Transformer models on many NLP tasks, but it is often treated as a separate line of research, where its effectiveness under the large-batch regime is not well understood. \nIn this paper, we show that adversarial training obtains promising model accuracy even with a considerably larger batch size. However, the computational complexity associated with this approach, due to the high cost of generating adversaries, prevents it from reducing adaptation costs with an increasing number of processors. As such, we systematically study adversarial large-batch optimization for adapting transformers from a computational complexity perspective. Our investigation yields efficient and practical algorithms for adapting transformer models. We show in experiments that our proposed method attains up to 9.8$\\times$ adaptation speedups over the baseline on BERT$_{base}$ and RoBERTa$_{large}$, while achieving comparable and sometimes higher accuracy than the state-of-the-art large-batch optimization methods."}}
{"id": "f-fVCElZ-G1", "cdate": 1652737484803, "mdate": null, "content": {"title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers", "abstract": "How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements.\nIn this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as \\OURS. \n\\OURS is an end-to-end quantization and inference pipeline with three main components: \n(1) a fine-grained hardware-friendly quantization scheme for both weight and activations; \n(2) a novel affordable layer-by-layer knowledge distillation algorithm (\\lwd) even without the original training data access;\n(3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead.\nAs such, we are able to show that:\n(1) \\OURS can reduce the precision for weight and activations to INT8 in a cost-free way for both \\bert and \\gpt-style \nmodels with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on \\bert/\\gpt-style models compared to FP16 inference, separately;\n(2) \\OURS plus \\lwd can affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model;\n(3) \\OURS can be directly applied to two of the largest open-sourced language models, including \\gptneox, for which our INT8 model achieves similar accuracy as the FP16 model but achieves 5.2x better efficiency.\nOur code is open-sourced at~\\cite{code_compression}."}}
{"id": "eAEcdRkcMHh", "cdate": 1632875764280, "mdate": null, "content": {"title": "HoloFormer: Deep Compression of Pre-Trained Transforms via Unified Optimization of N:M Sparsity and Integer Quantization", "abstract": "In recent years, large pre-trained Transformer networks have demonstrated dramatic improvements in many Natural Language Processing (NLP) tasks. However, the huge size of these models brings significant challenges to fine-tuning and online deployment due to latency and cost constraints. Recently, hardware manufacturers have released new architectures that support efficient N:M sparsity and low-precision integer computation for fast inferencing. In contrast to unstructured sparsity, N:M sparsity specifies that out of each chunk of N contiguous weight parameters, exactly M parameters are non-zero. Moreover, these architectures also support processing data with reduced precision, such as INT8. Prior work often considers inducing N:M sparsity and integer quantization in isolation or as independent pieces of a compression pipeline. However, there lacks a systematic investigation towards how N:M sparsity and integer quantization can be effectively combined to exploit the maximum degree of redundancy and enable even faster acceleration for pre-trained Transformer networks.\n\nIn this work, we propose a unified, systematic approach to learning N:M sparsity and integer quantization for pre-trained Transformers using the Alternating Directions Method of Multipliers (ADMM). We show that both N:M sparsity and integer quantization and their combinations can be framed as non-convex constrained optimization problems and\nsolved in a unified manner. When evaluated across the GLUE suite of NLP benchmarks, our approach outperforms baselines that consider each of these problems independently, retaining 99.4\\% accuracy of the dense baseline while being able to execute on newly released hardware effectively. "}}
{"id": "m7S4NvprHVl", "cdate": 1632875724863, "mdate": null, "content": {"title": "Demystifying Hyperparameter Optimization in Federated Learning", "abstract": "Federated Learning (FL) is a new machine learning paradigm that enables training models collaboratively across clients without sharing private data. In FL, data is non-uniformly distributed among clients (i.e.,  data heterogeneity) and cannot be balanced nor monitored like in conventional ML. Such data heterogeneity and privacy requirements bring unique challenges for learning hyperparameter optimization as the training dynamics change across clients even within the same training round and they are difficult to measure due to privacy constraints. State-of-the-art frameworks in FL focus on developing better aggregation algorithms and policies with the aim of mitigating these challenges. However, almost all existing FL systems adopt a ``global'' tuning method that uses a single set of learning hyperparameters across all the clients, regardless of their underlying data distributions. Our study shows that such a widely adopted global tuning method is not suitable for FL due to its data heterogeneity-oblivious nature. We demonstrate that the data quantity and distribution of the clients have a significant impact on the choice of hyperparameters, making it necessary to have customized tuning for each client. Based on these observations, we propose a first of its kind heterogeneity-aware hyperparameter optimization methodology, FedTune, that adopts a proxy data based hyperparameter customization approach to address the privacy and tuning cost challenges. Together with a Bayesian strengthened tuner, the proposed customized tuning approach is effective, lightweight, and privacy preserving. Extensive evaluation demonstrates that FedTune can achieve up to 7/4/4/6% better accuracy than the widely adopted globally tuned method for popular FL benchmarks FEMNIST, Cifar100, Cifar10, and Fashion-MNIST respectively."}}
