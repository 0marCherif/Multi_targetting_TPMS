{"id": "U9HW6vyNClg", "cdate": 1663850143994, "mdate": null, "content": {"title": "Towards Minimax Optimal Reward-free Reinforcement Learning in Linear MDPs", "abstract": "We study reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). In this setting, an agent first interacts with the environment without accessing the reward function in the exploration phase. In the subsequent planning phase, it is given a reward function and asked to output an $\\epsilon$-optimal policy. We propose a novel algorithm LSVI-RFE under the linear MDP setting, where the transition probability and reward functions are linear in a feature mapping. We prove an $\\widetilde{O}(H^{4} d^{2}/\\epsilon^2)$ sample complexity upper bound for LSVI-RFE, where $H$ is the episode length and $d$ is the feature dimension. We also establish a sample complexity lower bound of $\\Omega(H^{3} d^{2}/\\epsilon^2)$. To the best of our knowledge, LSVI-RFE is the first computationally efficient algorithm that achieves the minimax optimal sample complexity in linear MDP settings up to an $H$ and logarithmic factors. Our LSVI-RFE algorithm is based on a novel variance-aware exploration mechanism to avoid overly-conservative exploration in prior works. Our sharp bound relies on the decoupling of UCB bonuses during two phases, and a Bernstein-type self-normalized bound, which remove the extra dependency of sample complexity on $H$ and $d$, respectively."}}
{"id": "DJEEqoAq7to", "cdate": 1663850140451, "mdate": null, "content": {"title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch", "abstract": "Training deep reinforcement learning (DRL) models usually requires high computation costs. Therefore, compressing DRL models possesses immense potential for training acceleration and model deployment. However, existing methods that generate small models mainly adopt the knowledge distillation-based approach by iteratively training a dense network. As a result, the training process still demands massive computing resources. Indeed, sparse training from scratch in DRL has not been well explored and is particularly challenging due to non-stationarity in bootstrap training. In this work, we propose a novel sparse DRL training framework, \u201cthe Rigged Reinforcement Learning Lottery\u201d (RLx2), which builds upon gradient-based topology evolution and is capable of training a sparse DRL model based entirely on a sparse network. Specifically, RLx2 introduces a novel multi-step TD target mechanism with a dynamic-capacity replay buffer to achieve robust value learning and efficient topology exploration in sparse models. It also reaches state-of-the-art sparse training performance in several tasks, showing $7.5\\times$-$20\\times$ model compression with less than $3\\%$ performance degradation and up to $20\\times$ and $50\\times$ FLOPs reduction for training and inference, respectively."}}
{"id": "x7af9Kvzkz", "cdate": 1640995200000, "mdate": 1681130581944, "content": {"title": "Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation", "abstract": ""}}
{"id": "OvrUdURWSqb", "cdate": 1640995200000, "mdate": 1681130581844, "content": {"title": "Effective multi-user delay-constrained scheduling with deep recurrent reinforcement learning", "abstract": ""}}
{"id": "UG6pmVb0DY6", "cdate": 1546300800000, "mdate": 1681130581925, "content": {"title": "Optimal Multi-User Scheduling for the Unbalanced Full-Duplex Buffer-Aided Relay Systems", "abstract": ""}}
{"id": "yM9FLRnAJe", "cdate": 1514764800000, "mdate": 1681130581845, "content": {"title": "Optimal Multi-User Scheduling of Buffer-Aided Relay Systems", "abstract": ""}}
{"id": "9zqQ-axEcRE", "cdate": 1514764800000, "mdate": 1681130581926, "content": {"title": "Optimal Hybrid Full-Duplex/Half-Duplex Scheme of the Buffer Aided Relay System", "abstract": ""}}
