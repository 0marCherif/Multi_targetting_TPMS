{"id": "hCMNwVrW8BD", "cdate": 1680849928234, "mdate": 1680849928234, "content": {"title": "Ablating Concepts in Text-to-Image Diffusion Models", "abstract": "Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model."}}
{"id": "LfxdeviYus", "cdate": 1672531200000, "mdate": 1709048744290, "content": {"title": "Evaluating Data Attribution for Text-to-Image Models", "abstract": "While large text-to-image models are able to synthesize \"novel\" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models \u2013 which of the images in the training set are most responsible for the appearance of a given generated image \u2013 is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through \"customization\" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allow us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images."}}
{"id": "KAR7QnINAQK", "cdate": 1672531200000, "mdate": 1705523554012, "content": {"title": "Content-based Search for Deep Generative Models", "abstract": "The growing proliferation of customized and pretrained generative models has made it infeasible for a user to be fully cognizant of every model in existence. To address this need, we introduce the task of content-based model search: given a query and a large set of generative models, finding the models that best match the query. As each generative model produces a distribution of images, we formulate the search task as an optimization problem to select the model with the highest probability of generating similar content as the query. We introduce a formulation to approximate this probability given the query from different modalities, e.g., image, sketch, and text. Furthermore, we propose a contrastive learning framework for model retrieval, which learns to adapt features for various query modalities. We demonstrate that our method outperforms several baselines on Generative Model Zoo, a new benchmark we create for the model retrieval task."}}
{"id": "eRO9Vi6Wp65", "cdate": 1640995200000, "mdate": 1667335898267, "content": {"title": "Rewriting geometric rules of a GAN", "abstract": ""}}
{"id": "MAMMRwNxNQw", "cdate": 1609459200000, "mdate": 1668541159728, "content": {"title": "Sketch Your Own GAN", "abstract": "Can a user create a deep generative model by sketching a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of exemplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessible way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an original GAN model according to user sketches. We encourage the model\u2019s output to match the user sketches through a crossdomain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model\u2019s diversity and image quality. Experiments have shown that our method can mold GANs to match shapes and poses specified by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing."}}
{"id": "F3LRQk57Yds", "cdate": 1577836800000, "mdate": 1624081360441, "content": {"title": "CNN-Generated Images Are Surprisingly Easy to Spot... for Now", "abstract": "In this work we ask whether it is possible to create a \"universal\" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used. To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2). Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis."}}
{"id": "qI1QmPnQAX2", "cdate": 1546300800000, "mdate": 1624081360388, "content": {"title": "Detecting Photoshopped Faces by Scripting Photoshop", "abstract": "Most malicious photo manipulations are created using standard image editing tools, such as Adobe Photoshop. We present a method for detecting one very popular Photoshop manipulation -- image warping applied to human faces -- using a model trained entirely using fake images that were automatically generated by scripting Photoshop itself. We show that our model outperforms humans at the task of recognizing manipulated images, can predict the specific location of edits, and in some cases can be used to \"undo\" a manipulation to reconstruct the original, unedited image. We demonstrate that the system can be successfully applied to artist-created image manipulations."}}
