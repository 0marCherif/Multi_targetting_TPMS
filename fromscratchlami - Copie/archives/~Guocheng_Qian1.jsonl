{"id": "FN8NcpN1em", "cdate": 1668603377026, "mdate": 1668603377026, "content": {"title": "When NAS Meets Trees: An Efficient Algorithm for Neural Architecture Search", "abstract": "The key challenge in neural architecture search (NAS) is designing how to explore wisely in the huge search space. We propose a new NAS method called TNAS (NAS with trees), which improves search efficiency by exploring only a small number of architectures while also achieving a higher search accuracy. TNAS introduces an architecture tree and a binary operation tree, to factorize the search space and substantially reduce the exploration size. TNAS performs a modified bi-level Breadth-First Search in the proposed trees to discover a high-performance architecture. Impressively, TNAS finds the global optimal architecture on CIFAR-10 with test accuracy of 94.37% in four GPU hours in NAS-Bench-201. The average test accuracy is 94.35%, which outperforms the state-of-the-art. Code is available at: https://github. com/guochengqian/TNAS."}}
{"id": "EAcWgk7JM58", "cdate": 1652737290728, "mdate": null, "content": {"title": "PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies", "abstract": "PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9% to 86.1%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7 on ScanObjectNN, surpassing PointMLP by 2.3%, while being 10x faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. The code and models are available at https://github.com/guochengqian/pointnext."}}
{"id": "dDqML9fpcHs", "cdate": 1640995200000, "mdate": 1667341365916, "content": {"title": "Pix4Point: Image Pretrained Transformers for 3D Point Cloud Understanding", "abstract": "Pure Transformer models have achieved impressive success in natural language processing and computer vision. However, one limitation with Transformers is their need for large training data. In the realm of 3D point clouds, the availability of large datasets is a challenge, which exacerbates the issue of training Transformers for 3D tasks. In this work, we empirically study and investigate the effect of utilizing knowledge from a large number of images for point cloud understanding. We formulate a pipeline dubbed \\textit{Pix4Point} that allows harnessing pretrained Transformers in the image domain to improve downstream point cloud tasks. This is achieved by a modality-agnostic pure Transformer backbone with the help of tokenizer and decoder layers specialized in the 3D domain. Using image-pretrained Transformers, we observe significant performance gains of Pix4Point on the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS benchmarks, respectively. Our code and models are available at: \\url{https://github.com/guochengqian/Pix4Point}."}}
{"id": "cd5ujle1wj", "cdate": 1640995200000, "mdate": 1667341365912, "content": {"title": "PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies", "abstract": "PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9% to 86.1%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7 on ScanObjectNN, surpassing PointMLP by 2.3%, while being 10x faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. The code and models are available at https://github.com/guochengqian/pointnext."}}
{"id": "QirKpGzPjl", "cdate": 1640995200000, "mdate": 1667341365890, "content": {"title": "When NAS Meets Trees: An Efficient Algorithm for Neural Architecture Search", "abstract": "The key challenge in neural architecture search (NAS) is designing how to explore wisely in the huge search space. We propose a new NAS method called TNAS (NAS with trees), which improves search efficiency by exploring only a small number of architectures while also achieving a higher search accuracy. TNAS introduces an architecture tree and a binary operation tree, to factorize the search space and substantially reduce the exploration size. TNAS performs a modified bi-level Breadth-First Search in the proposed trees to discover a high-performance architecture. Impressively, TNAS finds the global optimal architecture on CIFAR-10 with test accuracy of 94.37% in four GPU hours in NAS-Bench-201. The average test accuracy is 94.35%, which outperforms the state-of-the-art. Code is available at: https://github.com/guochengqian/TNAS."}}
{"id": "IKHvU4-WDO", "cdate": 1640995200000, "mdate": 1667341365913, "content": {"title": "Rethinking Learning-based Demosaicing, Denoising, and Super-Resolution Pipeline", "abstract": "Imaging is usually a mixture problem of incomplete color sampling, noise degradation, and limited resolution. This mixture problem is typically solved by a sequential solution that applies demosaicing (DM), denoising (DN), and super-resolution (SR) sequentially in a fixed and predefined pipeline (execution order of tasks), DM\u2192DN\u2192SR. The most recent work on image processing focuses on developing more sophisticated architectures to achieve higher image quality. Little attention has been paid to the design of the pipeline, and it is still not clear how significant the pipeline is to image quality. In this work, we comprehensively study the effects of pipelines on the mixture problem of learning-based DN, DM, and SR, in both sequential and joint solutions. On the one hand, in sequential solutions, we find that the pipeline has a non-trivial effect on the resulted image quality. Our suggested pipeline DN\u2192SR\u2192DM yields consistently better performance than other sequential pipelines in various experimental settings and benchmarks. On the other hand, in joint solutions, we propose an end-to-end Trinity Pixel Enhancement NETwork (TENet) that achieves the state-of-the-art performance for the mixture problem. We further present a novel and simple method that can integrate a certain pipeline into a given end-to-end network by providing intermediate supervision using a detachable head. Extensive experiments show that an end-to-end network with the proposed pipeline can attain only a consistent but insignificant improvement. Our work indicates that the investigation of pipelines is applicable in sequential solutions, but is not very necessary in end-to-end networks."}}
{"id": "qOcf6HgSmRH", "cdate": 1632875717535, "mdate": null, "content": {"title": "DeeperGCN: All You Need to Train Deeper GCNs", "abstract": "Graph Neural Networks (GNNs) have been drawing significant attention to the power of representation learning on graphs. Recent works developed frameworks to train very deep GNNs. Such works show impressive results in tasks like point cloud learning and protein interaction prediction. In this work, we study the performance of such deep models in large-scale graph datasets from the Open Graph Benchmark (OGB). In particular, we look at the effect of adequately choosing an aggregation function and its effect on final performance. Common choices of aggregation are mean, max, and sum. It has been shown that GNNs are sensitive to such aggregations when applied to different datasets. We systematically study this point on large-scale graphs and propose to alleviate it by introducing a novel Generalized Aggregation Function. Proposed aggregation functions extend beyond the commonly used ones. The generalized aggregation functions are fully differentiable, and thus their parameters can be learned in an end-to-end fashion. We show that deep residual GNNs equipped with generalized aggregation functions achieve state-of-the-art results in several benchmarks from OGB across tasks and domains."}}
{"id": "X0ein5pH4YJ", "cdate": 1621629901246, "mdate": null, "content": {"title": "ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning", "abstract": "Access to 3D point cloud representations has been widely facilitated by LiDAR sensors embedded in various mobile devices. This has led to an emerging need for fast and accurate point cloud processing techniques. In this paper, we revisit and dive deeper into PointNet++, one of the most influential yet under-explored networks, and develop faster and more accurate variants of the model. We first present a novel Separable Set Abstraction (SA) module that disentangles the vanilla SA module used in PointNet++ into two separate learning stages: (1) learning channel correlation and (2) learning spatial correlation. The Separable SA module is significantly faster than the vanilla version, yet it achieves comparable performance.  We then introduce a new Anisotropic Reduction function into our Separable SA module and propose an Anisotropic Separable SA (ASSA) module that substantially increases the network's accuracy. We later replace the vanilla SA modules in PointNet++ with the proposed ASSA modules, and denote the modified network as ASSANet. Extensive experiments on point cloud classification, semantic segmentation, and part segmentation show that ASSANet outperforms PointNet++ and other methods, achieving much higher accuracy and faster speeds. In particular, ASSANet outperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6 \\times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled ASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more than $54 \\times$ faster."}}
{"id": "VC6OMYLFwC", "cdate": 1609459200000, "mdate": 1667341365919, "content": {"title": "ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning", "abstract": "title> <meta name=\"citation_title\" content=\"ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning\" /> <meta name=\"citation_author\" content=\"Qian, Guocheng\" /> <meta name=\"citation_author\" content=\"Hammoud, Hasan\" /> <meta name=\"citation_author\" content=\"Li, Guohao\" /> <meta name=\"citation_author\" content=\"Thabet, Ali\" /> <meta name=\"citation_author\" content=\"Ghanem, Bernard\" /> <meta name=\"citation_journal_title\" content=\"Advances in Neural Information Processing Systems\" /> <meta name=\"citation_volume\" content=\"34\" /> <meta name=\"citation_firstpage\" content=\"28119\" /> <meta name=\"citation_lastpage\" content=\"28130\" /> <meta name=\"citation_pdf_url\" content=\"https://proceedings.neurips.cc/paper/2021/file/ecf5631507a8aedcae34cef231aa7348-Paper.pdf\" /> <meta name=\"citation_publication_date\" content=\"2021-12-06\" /><!-- Bootstrap CSS --> <!-- https://codepen.io/surjithctly/pen/PJqKzQ --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\" /> <link href=\"/static/menus/css/menus.css\" rel=\"stylesheet\" id=\"bootstrap-css\" /> <link rel=\"stylesheet\" href=\"https://use.fontawesome.com/releases/v5.8.1/css/all.css\" integrity=\"sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf\" crossorigin=\"anonymous\" /> <script type=\"text/javascript\" async=\"async\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML\"></script> <script type=\"text/x-mathjax-config\"> <![CDATA[ MathJax.Hub.Config({ \"tex2jax\": { \"inlineMath\": [[\"$\",\"$\"], [\"\\\\(\",\"\\\\)\"]], \"displayMath\": [[\"\\\\[\",\"\\\\]\"]], \"processEscapes\": true } } ); ]]> </script> <style> <![CDATA[ @media (prefers-color-scheme: dark) { body { background-color: #333; color: #eee; } } .btn-spacer { margin: 2px; } .footer { position: fixed; left: 0; bottom: 0; width: 100%; background-color: #eee; color: black; } ]]> </style> <nav class=\"navbar navbar-expand-md navbar-light bg-light\"> <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarToggler6\" aria-controls=\"navbarToggler6\" aria-expanded=\"false\" aria-label=\"Toggle navigation\"><span class=\"navbar-toggler-icon\"></span></button> <div class=\"collapse navbar-collapse\" id=\"navbarToggler6\"> <a class=\"navbar-brand\" href=\"/\">NeurIPS Proceedings</a> <ul class=\"navbar-nav mr-auto mt-2 mt-md-0\"> <li class=\"nav-item\"> <a class=\"nav-link\" href=\"/admin/login/?next=/admin/\"><i class=\"fas fa-sign-in-alt\" title=\"Login\"></i></a> <li class=\"nav-item\"> <a class=\"nav-link\" href=\"/admin/logout/?nextp=/admin\"><i class=\"fas fa-sign-out-alt\" title=\"Logout\"></i></a> <form class=\"form-inline my-2 my-lg-0\" method=\"get\" role=\"search\" action=\"/papers/search\"> <input class=\"form-control mr-sm-2\" type=\"text\" name=\"q\" placeholder=\"Search\" aria-label=\"Search\" id=\"navsearch\" /> <button class=\"btn btn-outline-success my-2 my-sm-0\" type=\"submit\">Search</button>"}}
{"id": "KQikKp02ob", "cdate": 1609459200000, "mdate": 1667341365903, "content": {"title": "PU-GCN: Point Cloud Upsampling Using Graph Convolutional Networks", "abstract": "The effectiveness of learning-based point cloud upsampling pipelines heavily relies on the upsampling modules and feature extractors used therein. For the point upsampling module, we propose a novel model called NodeShuffle, which uses a Graph Convolutional Network (GCN) to better encode local point information from point neighborhoods. NodeShuffle is versatile and can be incorporated into any point cloud upsampling pipeline. Extensive experiments show how NodeShuffle consistently improves state-of-the-art upsampling methods. For feature extraction, we also propose a new multi-scale point feature extractor, called Inception DenseGCN. By aggregating features at multiple scales, this feature extractor enables further performance gain in the final upsampled point clouds. We combine Inception DenseGCN with NodeShuffle into a new point upsampling pipeline called PU-GCN. PU-GCN sets new state-of-art performance with much fewer parameters and more efficient inference."}}
