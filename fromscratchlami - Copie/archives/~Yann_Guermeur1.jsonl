{"id": "l9Lmvts0b2", "cdate": 1640995200000, "mdate": 1675771841577, "content": {"title": "Generalization Bounds for Inductive Matrix Completion in Low-noise Settings", "abstract": "We study inductive matrix completion (matrix completion with side information) under an i.i.d. subgaussian noise assumption at a low noise regime, with uniform sampling of the entries. We obtain for the first time generalization bounds with the following three properties: (1) they scale like the standard deviation of the noise and in particular approach zero in the exact recovery case; (2) even in the presence of noise, they converge to zero when the sample size approaches infinity; and (3) for a fixed dimension of the side information, they only have a logarithmic dependence on the size of the matrix. Differently from many works in approximate recovery, we present results both for bounded Lipschitz losses and for the absolute loss, with the latter relying on Talagrand-type inequalities. The proofs create a bridge between two approaches to the theoretical analysis of matrix completion, since they consist in a combination of techniques from both the exact recovery literature and the approximate recovery literature."}}
{"id": "75cbTNhVSQD", "cdate": 1640995200000, "mdate": 1682613586781, "content": {"title": "ProtNAff: protein-bound Nucleic Acid filters and fragment libraries", "abstract": "Atomistic models of nucleic acids (NA) fragments can be used to model the 3D structures of specific protein\u2013NA interactions and address the problem of great NA flexibility, especially in their single-stranded regions. One way to obtain relevant NA fragments is to extract them from existing 3D structures corresponding to the targeted context (e.g. specific 2D structures, protein families, sequences) and to learn from them. Several databases exist for specific NA 3D motifs, especially in RNA, but none can handle the variety of possible contexts."}}
{"id": "MySSUrw_d8y", "cdate": 1577836800000, "mdate": 1682613586791, "content": {"title": "Rademacher complexity of margin multi-category classifiers", "abstract": "One of the main open problems of the theory of margin multi-category pattern classification is the characterization of the optimal dependence of the confidence interval of a guaranteed risk on the three basic parameters which are the sample size m, the number C of categories and the scale parameter $$\\gamma$$ \u03b3 . This is especially the case when working under minimal learnability hypotheses. The starting point is a basic supremum inequality whose capacity measure depends on the choice of the margin loss function. Then, transitions are made, from capacity measure to capacity measure. At some level, a structural result performs the transition from the multi-class case to the bi-class one. In this article, we highlight the advantages and drawbacks inherent to the three major options for this decomposition: using Rademacher complexities, covering numbers or scale-sensitive combinatorial dimensions."}}
{"id": "SVgvCdlcV93", "cdate": 1546300800000, "mdate": 1682613586793, "content": {"title": "Rademacher complexity and generalization performance of multi-category margin classifiers", "abstract": ""}}
{"id": "uRQZX9UVtPm", "cdate": 1514764800000, "mdate": 1682613586957, "content": {"title": "Combinatorial and Structural Results for gamma-Psi-dimensions", "abstract": "This article deals with the generalization performance of margin multi-category classifiers, when minimal learnability hypotheses are made. In that context, the derivation of a guaranteed risk is based on the handling of capacity measures belonging to three main families: Rademacher/Gaussian complexities, metric entropies and scale-sensitive combinatorial dimensions. The scale-sensitive combinatorial dimensions dedicated to the classifiers of this kind are the gamma-Psi-dimensions. We introduce the combinatorial and structural results needed to involve them in the derivation of guaranteed risks and establish the corresponding upper bounds on the metric entropies and the Rademacher complexity. Two major conclusions can be drawn: 1. the gamma-Psi-dimensions always bring an improvement compared to the use of the fat-shattering dimension of the class of margin functions; 2. thanks to their capacity to take into account basic features of the classifier, they represent a promising alternative to performing the transition from the multi-class case to the binary one with covering numbers."}}
{"id": "uQncZoxaKEd", "cdate": 1514764800000, "mdate": 1682613586792, "content": {"title": "A sharper bound on the Rademacher complexity of margin multi-category classifiers", "abstract": ""}}
{"id": "tYdUM-p3ZWs", "cdate": 1483228800000, "mdate": 1682613586818, "content": {"title": "Rademacher complexity of margin multi-category classifiers", "abstract": "In the framework of agnostic learning, one of the main open problems of the theory of multi-category pattern classification is the characterization of the way the confidence interval of a guaranteed risk should vary as a function of the fundamental parameters which are the sample size m and the number C of categories. This is especially the case when working under minimal learnability hypotheses. We consider margin classifiers based on classes of vector-valued functions with one component function per category. The classes of component functions are uniform Glivenko-Cantelli and the vector-valued functions take their values in a hypercube of \u211d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">C</sup> . For these classifiers, a well-known guaranteed risk based on a Rademacher complexity applies. Several studies have dealt with the derivation of an upper bound on this complexity. This article establishes a bound which is based on a new generalized Sauer-Shelah lemma. Under the additional assumption that the \u03b3-dimensions of the classes of component functions grow no faster than polynomially with \u03b3 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-1</sup> , its growth rate with C is a O (\u221aC In (C)). This behaviour holds true irrespective of the degree of the polynomial."}}
{"id": "UhbiWR8PdHT", "cdate": 1483228800000, "mdate": 1682613586806, "content": {"title": "Lp-norm Sauer-Shelah lemma for margin multi-category classifiers", "abstract": ""}}
{"id": "iB0x7LY2ELR", "cdate": 1388534400000, "mdate": 1682613586815, "content": {"title": "Model Selection for the \u21132-SVM by Following the Regularization Path", "abstract": "For a support vector machine, model selection consists in selecting the kernel function, the values of its parameters, and the amount of regularization. To set the value of the regularization parameter, one can minimize an appropriate objective function over the regularization path. A priori, this requires the availability of two elements: the objective function and an algorithm computing the regularization path at a reduced cost. The literature provides us with several upper bounds and estimates for the leave-one-out cross-validation error of the \u21132-SVM. However, no algorithm was available so far for fitting the entire regularization path of this machine. In this article, we introduce the first algorithm of this kind. It is involved in the specification of new methods to tune the corresponding penalization coefficient, whose objective function is a leave-one-out error bound or estimate. From a computational point of view, these methods appear especially appropriate when the Gram matrix is of low rank. A comparative study involving state-of-the-art alternatives provides us with an empirical confirmation of this advantage."}}
{"id": "ZIuu_xtacP1", "cdate": 1325376000000, "mdate": 1682613586951, "content": {"title": "Cascading Discriminant and Generative Models for Protein Secondary Structure Prediction", "abstract": "Most of the state-of-the-art methods for protein seconday structure prediction are complex combinations of discriminant models. They apply a local approach of the prediction which is known to induce a limit on the expected prediction accuracy. A priori, the use of generative models should make it possible to overcome this limitation. However, among the numerous hidden Markov models which have been dedicated to this task over more than two decades, none has come close to providing comparable performance. A major reason for this phenomenon is provided by the nature of the relevant information. Indeed, it is well known that irrespective of the model implemented, the prediction should benefit significantly from the availability of evolutionary information. Currently, this knowledge is embedded in position-specific scoring matrices which cannot be processed easily with hidden Markov models. With this observation at hand, the next significant advance should come from making the best of the two approaches, i.e., using a generative model on top of discriminant models. This article introduces the first hybrid architecture of this kind with state-of-the-art performance. The conjunction of the two levels of treatment makes it possible to optimize the recognition rate both at the residue level and at the segment level."}}
