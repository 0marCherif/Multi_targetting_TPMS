{"id": "rijPjgQedaS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Building Efficient Deep Neural Networks With Unitary Group Convolutions.", "abstract": "We propose unitary group convolutions (UGConvs), a building block for CNNs which compose a group convolution with unitary transforms in feature space to learn a richer set of representations than group convolution alone. UGConvs generalize two disparate ideas in CNN architecture, channel shuffling (i.e. ShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying insights that lead to a deeper understanding of each technique. We experimentally demonstrate that dense unitary transforms can outperform channel shuffling in DNN accuracy. On the other hand, different dense transforms exhibit comparable accuracy performance. Based on these observations we propose HadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar accuracy to circulant networks with lower computation complexity, and better accuracy than ShuffleNets with the same number of parameters and floating-point multiplies."}}
{"id": "rJN5jjb_-B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting", "abstract": "Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training ..."}}
