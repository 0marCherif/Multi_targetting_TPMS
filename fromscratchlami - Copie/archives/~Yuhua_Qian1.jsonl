{"id": "xIeRxBiUNT", "cdate": 1685577600000, "mdate": 1684136174563, "content": {"title": "Neighborhood Information-Based Method for Multivariate Association Mining", "abstract": "Most current data is multivariable, exploring and identifying valuable information in these datasets has far-reaching impacts. In particular, discovering meaningful hidden association patterns in multivariate plays an important role. Plenty of measures for multivariate association have been proposed, yet it is still an open research challenge for effectively capturing association patterns among three or more variables, especially the scenario without any prior knowledge about those relationships. To do so, we desire a distribution-free, association type-independent and non-parametrical measure. For practical applications, such a measure should <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">comparable</i> , <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">interpretable</i> , <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">scalable</i> , <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">intuitive</i> , <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">reliability</i> , and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">robust</i> . However, no exiting measures fulfill all of these desiderata. In this paper, taking advantage of the neighborhood information of a sample, we propose MNA, a maximal neighborhood multivariate association measure that satisfies all the above criteria. Extensive experiments on synthetic and real data show it outperforms state-of-the-art multivariate association measures."}}
{"id": "HsIQ0j6HcM", "cdate": 1685577600000, "mdate": 1684136169168, "content": {"title": "An efficient feature selection algorithm based on the description vector and hypergraph", "abstract": ""}}
{"id": "W6N2DYFCCH", "cdate": 1682899200000, "mdate": 1684136169119, "content": {"title": "A novel multi-label feature selection method with association rules and rough set", "abstract": ""}}
{"id": "9YIf8uRO4yb", "cdate": 1682899200000, "mdate": 1684136169075, "content": {"title": "How to describe the spatial near-far relations among concepts?", "abstract": ""}}
{"id": "WnOeE8O5kbM", "cdate": 1680307200000, "mdate": 1684136169109, "content": {"title": "A novel fuzzy hierarchical fusion attention convolution neural network for medical image super-resolution reconstruction", "abstract": ""}}
{"id": "TwDHvtEQDr", "cdate": 1677628800000, "mdate": 1684136174565, "content": {"title": "Multifuzzy $\\beta$-Covering Approximation Spaces and Their Information Measures", "abstract": "Fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering rough sets, as an effective extension of covering-based rough sets, have been concerned by many researchers. All fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering rough set models are constructed under a corresponding fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering approximation space. However, fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering is difficult to find directly from the real data. Fortunately, fuzzy information granulation provides a reasonable and effective way to obtain fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -coverings from the real data. Since fuzzy information granulation is capable of generating multiple fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -coverings, we introduce the notion of multifuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering approximation spaces. Fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering approximation spaces are a special case of multifuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering approximation spaces. Besides, we employ fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -neighborhood operators with reflexivity and symmetry to characterize the similarity between samples. In this article, we first present the definition of multifuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering approximation spaces and investigate some useful properties about fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering. Second, several information measures are explored in the context of multifuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering approximation spaces. On this basis, a novel heuristic fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering reduction method with the measure of monotone conditional entropy is proposed. Moreover, a general framework of attribute reduction based on fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering reduction is also designed. Finally, through the comparative and experimental analyses with other four state-of-the-art attribute reduction methods, the effectiveness and superiority of the proposed method are verified."}}
{"id": "xyja6EeekN", "cdate": 1672531200000, "mdate": 1684136169411, "content": {"title": "Generalization Performance of Pure Accuracy and its Application in Selective Ensemble Learning", "abstract": "The pure accuracy measure is used to eliminate random consistency from the accuracy measure. Biases to both majority and minority classes in the pure accuracy are lower than that in the accuracy measure. In this paper, we demonstrate that compared with the accuracy measure and F-measure, the pure accuracy measure is class distribution insensitive and discriminative for good classifiers. The advantages make the pure accuracy measure suitable for traditional classification. Further, we mainly focus on two points: exploring a tighter generalization bound on pure accuracy based learning paradigm and designing a learning algorithm based on the pure accuracy measure. Particularly, with the self-bounding property, we build an algorithm-independent generalization bound on the pure accuracy measure, which is tighter than the existing bound of an order <inline-formula><tex-math notation=\"LaTeX\">$O(1/\\sqrt{N})$</tex-math></inline-formula> (N is the number of instances). The proposed bound is free from making a smoothness or convex assumption on the hypothesis functions. In addition, we design a learning algorithm optimizing the pure accuracy measure and use it in the selective ensemble learning setting. The experiments on sixteen benchmark data sets and four image data sets demonstrate that the proposed method statistically performs better than the other eight representative benchmark algorithms."}}
{"id": "zM46bOEgoni", "cdate": 1640995200000, "mdate": 1684136181278, "content": {"title": "Noise-Tolerant Fuzzy-$\\beta$-Covering-Based Multigranulation Rough Sets and Feature Subset Selection", "abstract": "As a novel fuzzy covering, fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> covering has attracted considerable attention. However, the traditional fuzzy- <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> -covering-based rough set and most of its extended models cannot well fit the distribution of samples in real data, which limits their application in classification learning and decision making. First, the upper and lower approximations of these models have no inclusion relation, so they cannot characterize a given objective concept accurately. Moreover, most of these models are hard to resist the influence of noise data, resulting in poor robustness in feature learning. For these reasons, a robust rough set model is set forth by combining fuzzy rough sets, covering-based rough sets, and multigranulation rough sets. To this end, the optimistic and pessimistic lower and upper approximations of a target concept are reconstructed by means of the fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> neighborhood related to a family of fuzzy coverings, and a new multigranulation fuzzy rough set model is presented. Furthermore, a fuzzy dependence function is introduced to evaluate the classification ability of a family of fuzzy <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> coverings at different granularity levels. The dimensionality reduction of a given fuzzy covering decision table is carried out from the perspective of maintaining the discrimination power, and a forward algorithm for feature selection is developed by using the optimistic significance of candidate features as heuristic information. Three groups of numerical experiments on 16 different types of datasets demonstrate that the proposed model exhibits good robustness on datasets contaminated with noise and outperforms some state-of-the-art feature learning algorithms in terms of classification accuracy and the size of the selected feature subset."}}
{"id": "xtk-lUD86l", "cdate": 1640995200000, "mdate": 1684136180998, "content": {"title": "Enhanced Group Sparse Regularized Nonconvex Regression for Face Recognition", "abstract": "Regression analysis based methods have shown strong robustness and achieved great success in face recognition. In these methods, convex <inline-formula><tex-math notation=\"LaTeX\">$l_1$</tex-math></inline-formula> -norm and nuclear norm are usually utilized to approximate the <inline-formula><tex-math notation=\"LaTeX\">$l_0$</tex-math></inline-formula> -norm and rank function. However, such convex relaxations may introduce a bias and lead to a suboptimal solution. In this paper, we propose a novel Enhanced Group Sparse regularized Nonconvex Regression (EGSNR) method for robust face recognition. An upper bounded nonconvex function is introduced to replace <inline-formula><tex-math notation=\"LaTeX\">$l_1$</tex-math></inline-formula> -norm for sparsity, which alleviates the bias problem and adverse effects caused by outliers. To capture the characteristics of complex errors, we propose a mixed model by combining <inline-formula><tex-math notation=\"LaTeX\">$\\gamma$</tex-math></inline-formula> -norm and matrix <inline-formula><tex-math notation=\"LaTeX\">$\\gamma$</tex-math></inline-formula> -norm induced from the nonconvex function. Furthermore, an <inline-formula><tex-math notation=\"LaTeX\">$l_{2,\\gamma }$</tex-math></inline-formula> -norm based regularizer is designed to directly seek the interclass sparsity or group sparsity instead of traditional <inline-formula><tex-math notation=\"LaTeX\">$l_{2,1}$</tex-math></inline-formula> -norm. The locality of data, i.e., the distance between the query sample and multi-subspaces, is also taken into consideration. This enhanced group sparse regularizer enables EGSNR to learn more discriminative representation coefficients. Comprehensive experiments on several popular face datasets demonstrate that the proposed EGSNR outperforms the state-of-the-art regression based methods for robust face recognition."}}
{"id": "xHqNRFWI3F_", "cdate": 1640995200000, "mdate": 1684136183036, "content": {"title": "2D Human Pose Estimation with Explicit Anatomical Keypoints Structure Constraints", "abstract": "Recently, human pose estimation mainly focuses on how to design a more effective and better deep network structure as human features extractor, and most designed feature extraction networks only introduce the position of each anatomical keypoint to guide their training process. However, we found that some human anatomical keypoints kept their topology invariance, which can help to localize them more accurately when detecting the keypoints on the feature map. But to the best of our knowledge, there is no literature that has specifically studied it. Thus, in this paper, we present a novel 2D human pose estimation method with explicit anatomical keypoints structure constraints, which introduces the topology constraint term that consisting of the differences between the distance and direction of the keypoint-to-keypoint and their groundtruth in the loss object. More importantly, our proposed model can be plugged in the most existing bottom-up or top-down human pose estimation methods and improve their performance. The extensive experiments on the benchmark dataset: COCO keypoint dataset, show that our methods perform favorably against the most existing bottom-up and top-down human pose estimation methods, especially for Lite-HRNet, when our model is plugged into it, its AP scores separately raise by 2.9\\% and 3.3\\% on COCO val2017 and test-dev2017 datasets."}}
