{"id": "j7AccWAMsq", "cdate": 1675209600000, "mdate": 1683423394590, "content": {"title": "GRADES: Gradient Descent for Similarity Caching", "abstract": "A similarity cache can reply to a query for an object with similar objects stored locally. In some applications of similarity caches, queries and objects are naturally represented as points in a continuous space. This is for example the case of 360\u00b0 videos where user\u2019s head orientation\u2014expressed in spherical coordinates\u2014determines what part of the video needs to be retrieved, or of recommendation systems where a metric learning technique is used to embed the objects in a finite dimensional space with an opportune distance to capture content dissimilarity. Existing similarity caching policies are simple modifications of classic policies like LRU, LFU, and <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${q}$ </tex-math></inline-formula> LRU and ignore the continuous nature of the space where objects are embedded. In this paper, we propose GRADES, a new similarity caching policy that uses gradient descent to navigate the continuous space and find appropriate objects to store in the cache. We provide theoretical convergence guarantees and show GRADES increases the similarity of the objects served by the cache in both applications mentioned above."}}
{"id": "i_3NxuwKX4i", "cdate": 1672531200000, "mdate": 1680012371897, "content": {"title": "Federated Learning for Data Streams", "abstract": ""}}
{"id": "Qc-4vkYw8Kd", "cdate": 1672531200000, "mdate": 1680012371695, "content": {"title": "Federated Learning under Heterogeneous and Correlated Client Availability", "abstract": ""}}
{"id": "hZ9E5anzTym", "cdate": 1669852800000, "mdate": 1683879392287, "content": {"title": "A New Upper Bound on Cache Hit Probability for Non-Anticipative Caching Policies", "abstract": ""}}
{"id": "jJx00vsVVSF", "cdate": 1663939403268, "mdate": null, "content": {"title": "A Novel Model-Based Attribute Inference Attack in Federated Learning", "abstract": "In federated learning, clients such as mobile devices or data silos (e.g. hospitals and banks) collaboratively improve a shared model, while maintaining their data locally. Multiple recent works show that client\u2019s private information can still be disclosed to an adversary who just eavesdrops the messages exchanged between the targeted client and the server. In this paper, we propose a novel model-based attribute inference attack in federated learning which overcomes the limits of gradient-based ones. Furthermore, we provide an analytical lower-bound for the success of this attack. Empirical results using real world datasets confirm that our attribute inference attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in federated learning. Our attack results in higher reconstruction accuracy especially when the clients\u2019 datasets are heterogeneous (as is common in federated learning)."}}
{"id": "GgM5DiAb6A2", "cdate": 1654178847611, "mdate": null, "content": {"title": "FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings", "abstract": "Federated Learning (FL) is a novel approach enabling several clients holding sensitive data to collaboratively train machine learning models, without centralizing data. The cross-silo FL setting corresponds to the case of few ($2$--$50$) reliable clients, each holding medium to large datasets, and is typically found in applications such as healthcare, finance, or industry. While previous works have proposed representative datasets for cross-device FL, few realistic healthcare cross-silo FL datasets exist, thereby slowing algorithmic research in this critical application. In this work, we propose a novel cross-silo dataset suite focused on healthcare, FLamby (Federated Learning AMple Benchmark of Your cross-silo strategies), to bridge the gap between theory and practice of cross-silo FL.\nFLamby encompasses 7 healthcare datasets with natural splits, covering multiple tasks, modalities, and data volumes, each accompanied with baseline training code. As an illustration, we additionally benchmark standard FL algorithms on all datasets.\nOur flexible and modular suite allows researchers to easily download datasets, reproduce results and re-use the different components for their research. FLamby is available at~\\url{www.github.com/owkin/flamby}."}}
{"id": "qWUqoWC-YSU", "cdate": 1640995200000, "mdate": 1683879392677, "content": {"title": "Analyzing Count Min Sketch with Conservative Updates", "abstract": ""}}
{"id": "n0R_zoW-Xh", "cdate": 1640995200000, "mdate": 1683879392152, "content": {"title": "Guest Editorial: Introduction to the Special Section on Communication-Efficient Distributed Machine Learning", "abstract": "The papers in this special section focus on communication-efficient distributed machine learning. Machine learning, especially deep learning, has been successfully applied in a wealth of practical AI applications in the field of computer vision, natural language processing, healthcare, finance, robotics, etc. With the increasing size of machine learning models and training data sets, training deep learning models requires significant amount of computations and may take days to months on a single GPU or TPU. Therefore, it becomes a common practice to exploit distributed machine learning to accelerate the training process with multiple processors. Distributed machine learning typically requires the processors to exchange information repeatedly throughout the training process. With the fast-growing computing power of the AI processors, the data communications among processors gradually become the performance bottleneck and excessively limit the system scalability due to Amdahl\u2019s law. The design of communication-efficient distributed machine learning systems has attracted great attention from both academia and industry."}}
{"id": "f4wTWU5KK1D", "cdate": 1640995200000, "mdate": 1683879392273, "content": {"title": "Computing the Hit Rate of Similarity Caching", "abstract": "Similarity caching allows requests for an item <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$i$</tex> to be served by a similar item i\u2019. Applications include recommendation systems, multimedia retrieval, and machine learning. Recently, many similarity caching policies have been proposed, but still we do not know how to compute the hit rate even for simple policies, like SIM-LRU and RND-LRU that are straightforward modifications of classic caching algorithms. This paper proposes the first algorithm to compute the hit rate of similarity caching policies under the independent reference model for the request process. In particular, we show how to extend the popular time-to-live approximation in classic caching to similarity caching. The algorithm is evaluated on both synthetic and real world traces."}}
{"id": "eyIjyjthau", "cdate": 1640995200000, "mdate": 1683879392175, "content": {"title": "A Formal Analysis of the Count-Min Sketch with Conservative Updates", "abstract": "Count-Min Sketch with Conservative Updates (CMS-CU) is a popular algorithm to approximately count items\u2019 appearances in a data stream. Despite CMS-CU\u2019s widespread adoption, the theoretical analysis of its performance is still wanting because of its inherent difficulty. In this paper, we propose a novel approach to study CMS-CU and derive new upper bounds on the expected value and the CCDF of the estimation error under an i.i.d. request process. Our formulas can be successfully employed to derive improved estimates for the precision of heavy-hitter detection methods and improved configuration rules for CMS-CU. The bounds are evaluated both on synthetic and real traces."}}
