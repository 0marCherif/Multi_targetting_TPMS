{"id": "u7ugqk7VBP8", "cdate": 1663850159745, "mdate": null, "content": {"title": "Prefix Conditioning Unifies Language and Label Supervision", "abstract": "Image-classification datasets have been used to pretrain image recognition models. Recently, web-scale image-caption datasets have emerged as a source of powerful pretraining alternative. Image-caption datasets are more ``open-domain'', containing a wider variety of scene types and vocabulary words than traditional classification datasets, and models trained on these datasets have demonstrated strong performance on few- and zero-shot recognition tasks.\nWhen naively unifying image-classification and -caption dataset, we show that such dataset biases negatively affect pre-training by reducing the generalizability of learned representations and thus jeopardizing zero-shot performance since the unification can tailor the model for the classification dataset, making it vulnerable to the distribution shift from the dataset. In this work, we address the problem by disentangling the dataset bias using prefix tokens that inform a language encoder of the type of the input dataset (e.g., image-classification or caption) at training time. This approach allows the language encoder to share the knowledge from two datasets as well as switch the mode of feature extraction, i.e., image-classification dataset or image-caption dataset tailored mode, where we use image-caption mode in the zero-shot evaluation.\nOur method is generic and can be easily integrated into existing VL pre-training objectives such as CLIP or UniCL. In experiments, we show that this simple technique improves the performance in zero-shot image recognition accuracy and robustness to the image-level distribution shift."}}
{"id": "ZfcosR9vZ-j", "cdate": 1632875602883, "mdate": null, "content": {"title": "Pyramid Mini-Batching for Optimal Transport", "abstract": "Optimal transport theory provides a useful tool to measure the differences between two distributions.\nAligning distributions by minimizing optimal transport distances has been shown to be effective in a variety of machine learning settings, including generative modeling and domain adaptation. However, computing optimal transport distances over large numbers of data points is very time-consuming and intractable for measuring the distances between discrete distributions with large numbers of data points. In this work we propose a geometric sampling scheme which partitions the datasets into pyramid-based encodings. Our approach, Pyramid Mini-Batching, significantly improves the quality of optimal transport approximations and downstream alignments with minimal computational overhead. We perform experiments over the Discrete Optimal Transport benchmark to demonstrate the effectiveness of this strategy over multiple established optimal transport settings and see that our approach improves estimates of OT distances by nearly $30\\%$ for single pass estimation. Furthermore, we see that when attempting to minimize optimal transport distance our approach is ten times more effective than with random mini-batch sampling. To highlight the practical benefits of this approach, we use optimal transport distance in domain adaptation settings and show our approach produces state of the results on large-scale domain adaptation problems VisDA17 and DomainNet. Ablation studies indicate that our sampling approach could be combined with conventional distribution alignment approaches and over substantial improvements to their results."}}
{"id": "R7pcR8fOABD", "cdate": 1627288854669, "mdate": null, "content": {"title": "Self-supervised Visual Attribute Learning for Fashion Compatibility", "abstract": "Many self-supervised learning (SSL) methods have been successful in learning semantically meaningful visual representations by solving pretext tasks. However, prior work in SSL focuses on tasks like object recognition or detection, which aim to learn object shapes and assumes that the features should be invariant to concepts like colors and textures. Thus, these SSL methods perform poorly on downstream tasks where these concepts provide critical information. In this paper, we present an SSL framework that enables us to learn color and texture-aware features without requiring any labels during training.  Our approach consists of three self-supervised tasks designed to capture different concepts that are neglected in prior work that we can select from depending on the needs of our downstream tasks.  We evaluate our approach on fashion compatibility using Polyvore Outfits and In-Shop Clothing Retrieval using Deepfashion, improving upon prior SSL methods by 9.5-16\\%, and even outperforming some supervised approaches on Polyvore Outfits despite using no labels.  We also show that our approach can be used for transfer learning, demonstrating that we can train on one dataset while achieving high performance on a different dataset."}}
{"id": "77cNKCCjgw", "cdate": 1621629735929, "mdate": null, "content": {"title": "OpenMatch: Open-Set Semi-supervised Learning with Open-set Consistency Regularization", "abstract": " Semi-supervised learning (SSL) is an effective means to leverage unlabeled data to improve a model\u2019s performance. Typical SSL methods like FixMatch assume that labeled and unlabeled data share the same label space. However, in practice, unlabeled data can contain categories unseen in the labeled set, i.e., outliers, which can significantly harm the performance of SSL algorithms.\n  To address this problem, we propose a novel Open-set Semi-Supervised Learning (OSSL) approach called OpenMatch.\nLearning representations of inliers while rejecting outliers is essential for the success of OSSL. To this end, \nOpenMatch unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open-set soft-consistency regularization loss, which enhances the smoothness of the OVA-classifier with respect to input transformations and greatly improves outlier detection. \\ours achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data on CIFAR10. The code is available at \\url{https://github.com/VisionLearningGroup/OP_Match}.\n  "}}
{"id": "x4yx5aVeSP3", "cdate": 1620326683513, "mdate": null, "content": {"title": "Maximum classifier discrepancy for unsupervised domain adaptation", "abstract": "In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics.\nTo solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at \\url{this https URL}"}}
{"id": "HkxXyNnIPr", "cdate": 1569272795016, "mdate": null, "content": {"title": "ADVERSARIAL DROPOUT REGULARIZATION", "abstract": ""}}
{"id": "BJxdF7n8wB", "cdate": 1569272703758, "mdate": null, "content": {"title": "Asymmetric Tri-training for Unsupervised Domain Adaptation", "abstract": ""}}
{"id": "Hj-EF6feOpH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Strong-Weak Distribution Alignment for Adaptive Object Detection.", "abstract": "We propose an approach for unsupervised adaptation of object detectors from label-rich to label-poor domains which can significantly reduce annotation costs associated with detection. Recently, approaches that align distributions of source and target images using an adversarial loss have been proven effective for adapting object classifiers. However, for object detection, fully matching the entire distributions of source and target images to each other at the global image level may fail, as domains could have distinct scene layouts and different combinations of objects. On the other hand, strong matching of local features such as texture and color makes sense, as it does not change category level semantics. This motivates us to propose a novel method for detector adaptation based on strong local alignment and weak global alignment. Our key contribution is the weak alignment model, which focuses the adversarial alignment loss on images that are globally similar and puts less emphasis on aligning images that are globally dissimilar. Additionally, we design the strong domain alignment model to only look at local receptive fields of the feature map. We empirically verify the effectiveness of our method on four datasets comprising both large and small domain shifts. Our code is available at https://github.com/VisionLearningGroup/DA_Detection."}}
{"id": "HJIoJWZCZ", "cdate": 1518730170276, "mdate": null, "content": {"title": "Adversarial Dropout Regularization", "abstract": "We present a domain adaptation method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by ``fooling'' a special domain classifier network. However, a drawback of this approach is that the domain classifier simply labels the generated features as in-domain or not, without considering the boundaries between classes. This means that ambiguous target features can be generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), which encourages the generator to output more discriminative features for the target domain. Our key idea is to replace the traditional domain critic with a critic that detects non-discriminative features by using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvements over the state of the art."}}
{"id": "ryEJWe2HM", "cdate": 1517187292213, "mdate": null, "content": {"title": "Melody Generation for Pop Music via Word Representation of Musical Properties", "abstract": "Automatic melody generation for pop music has been a long-time aspiration for\nboth AI researchers and musicians. However, learning to generate euphonious\nmelody has turned out to be highly challenging due to a number of factors. Representation\nof multivariate property of notes has been one of the primary challenges.\nIt is also difficult to remain in the permissible spectrum of musical variety, outside\nof which would be perceived as a plain random play without auditory pleasantness.\nObserving the conventional structure of pop music poses further challenges.\nIn this paper, we propose to represent each note and its properties as a unique\n\u2018word,\u2019 thus lessening the prospect of misalignments between the properties, as\nwell as reducing the complexity of learning. We also enforce regularization policies\non the range of notes, thus encouraging the generated melody to stay close\nto what humans would find easy to follow. Furthermore, we generate melody\nconditioned on song part information, thus replicating the overall structure of a\nfull song. Experimental results demonstrate that our model can generate auditorily\npleasant songs that are more indistinguishable from human-written ones than\nprevious models."}}
