{"id": "HVe1C4jUSYJ", "cdate": 1681673610128, "mdate": 1681673610128, "content": {"title": "Insights into population behavior during the COVID-19 pandemic from cell phone mobility data and manifold learning", "abstract": "Understanding the complex interplay between human behavior, disease transmission and non-pharmaceutical interventions during the COVID-19 pandemic could provide valuable insights with which to focus future public health efforts. Cell phone mobility data offer a modern measurement instrument to investigate human mobility and behavior at an unprecedented scale. We investigate aggregated and anonymized mobility data, which measure how populations at the census-block-group geographic scale stayed at home in California, Georgia, Texas and Washington from the beginning of the pandemic. Using manifold learning techniques, we show that a low-dimensional embedding enables the identification of patterns of mobility behavior that align with stay-at-home orders, correlate with socioeconomic factors, cluster geographically, reveal subpopulations that probably migrated out of urban areas and, importantly, link to COVID-19 case counts. The analysis and approach provide local epidemiologists a framework for interpreting mobility data and behavior to inform policy makers\u2019 decision-making aimed at curbing the spread of COVID-19."}}
{"id": "FUMxFwyLQZ", "cdate": 1664184434832, "mdate": null, "content": {"title": "Transfer Learning with Deep Tabular Models", "abstract": "Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they are easily fine-tuned in new domains and learn reusable features. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we explore the benefits that representation learning provides for knowledge transfer in the tabular domain. We conduct experiments in a realistic medical diagnosis test bed with limited amounts of downstream data and find that transfer learning with deep tabular models provides a definitive advantage over gradient boosted decision tree methods. We further compare the supervised and self-supervised pretraining strategies and provide practical advice on transfer learning with tabular models. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications."}}
{"id": "b0RuGUYo8pA", "cdate": 1663850372846, "mdate": null, "content": {"title": "Transfer Learning with Deep Tabular Models", "abstract": "Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they are easily fine-tuned in new domains and learn reusable features. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we explore the benefits that representation learning provides for knowledge transfer in the tabular domain. We conduct experiments in a realistic medical diagnosis test bed with limited amounts of downstream data and find that transfer learning with deep tabular models provides a definitive advantage over gradient boosted decision tree methods. We further compare the supervised and self-supervised pretraining strategies and provide practical advice on transfer learning with tabular models. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications."}}
{"id": "Nf_XI3uVGaZ", "cdate": 1652737687134, "mdate": null, "content": {"title": "Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability", "abstract": "Conventional saliency maps highlight input features to which neural network predictions are highly sensitive. We take a different approach to saliency, in which we identify and analyze the network parameters, rather than inputs, which are responsible for erroneous decisions. We first verify that identified salient parameters are indeed responsible for misclassification by showing that turning these parameters off improves predictions on the associated samples more than turning off the same number of random or least salient parameters. We further validate the link between salient parameters and network misclassification errors by observing that fine-tuning a small number of the most salient parameters on a single sample results in error correction on other samples which were misclassified for similar reasons -- nearest neighbors in the saliency space. After validating our parameter-space saliency maps, we demonstrate that samples which cause similar parameters to malfunction are semantically similar. Further, we introduce an input-space saliency counterpart which reveals how image features cause specific network components to malfunction.\n"}}
{"id": "vaMtFLN-d7", "cdate": 1640995200000, "mdate": 1675323987234, "content": {"title": "Transfer Learning with Deep Tabular Models", "abstract": "Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they learn reusable features and are easily fine-tuned in new domains. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we demonstrate that upstream data gives tabular neural networks a decisive advantage over widely used GBDT models. We propose a realistic medical diagnosis benchmark for tabular transfer learning, and we present a how-to guide for using upstream data to boost performance with a variety of tabular neural network architectures. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications. Our code is available at https://github.com/LevinRoman/tabular-transfer-learning ."}}
{"id": "qEGBB9YB31", "cdate": 1632875723731, "mdate": null, "content": {"title": "Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability", "abstract": "Conventional saliency maps highlight input features to which neural network predictions are highly sensitive. We take a different approach to saliency, in which we identify and analyze the network parameters, rather than inputs, which are responsible for erroneous decisions.  We first verify that identified salient parameters are indeed responsible for misclassification by showing that turning these parameters off improves predictions on the associated samples, more than pruning the same number of random or least salient parameters.  We further validate the link between salient parameters and network misclassification errors by observing that fine-tuning a small number of the most salient parameters on a single sample results in error correction on other samples which were misclassified for similar reasons -- nearest neighbors in the saliency space. After validating our parameter-space saliency maps, we demonstrate that samples which cause similar parameters to malfunction are semantically similar.  Further, we introduce an input-space saliency counterpart which reveals how image features cause specific network components to malfunction."}}
{"id": "HMjxLedQRzC", "cdate": 1609459200000, "mdate": 1675323987237, "content": {"title": "Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability", "abstract": "Conventional saliency maps highlight input features to which neural network predictions are highly sensitive. We take a different approach to saliency, in which we identify and analyze the network parameters, rather than inputs, which are responsible for erroneous decisions. We find that samples which cause similar parameters to malfunction are semantically similar. We also show that pruning the most salient parameters for a wrongly classified sample often improves model behavior. Furthermore, fine-tuning a small number of the most salient parameters on a single sample results in error correction on other samples that are misclassified for similar reasons. Based on our parameter saliency method, we also introduce an input-space saliency technique that reveals how image features cause specific network components to malfunction. Further, we rigorously validate the meaningfulness of our saliency maps on both the dataset and case-study levels."}}
{"id": "h5shhcogiNC", "cdate": 1577836800000, "mdate": 1675323987236, "content": {"title": "Echo Chambers in Collaborative Filtering Based Recommendation Systems", "abstract": "Recommendation systems underpin the serving of nearly all online content in the modern age. From Youtube and Netflix recommendations, to Facebook feeds and Google searches, these systems are designed to filter content to the predicted preferences of users. Recently, these systems have faced growing criticism with respect to their impact on content diversity, social polarization, and the health of public discourse. In this work we simulate the recommendations given by collaborative filtering algorithms on users in the MovieLens data set. We find that prolonged exposure to system-generated recommendations substantially decreases content diversity, moving individual users into \"echo-chambers\" characterized by a narrow range of content. Furthermore, our work suggests that once these echo-chambers have been established, it is difficult for an individual user to break out by manipulating solely their own rating vector."}}
