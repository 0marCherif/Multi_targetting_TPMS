{"id": "yz_0w8xizq", "cdate": 1672531200000, "mdate": 1693347772340, "content": {"title": "Training Quantum Boltzmann Machines with Coresets", "abstract": "Recent work has proposed and explored using coreset techniques for quantum algorithms that operate on classical data sets to accelerate the applicability of these algorithms on near-term quantum devices. We apply these ideas to Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs state sampling are the main computational bottleneck during training. By using a coreset in place of the full data set, we try to minimize the number of steps needed and accelerate the overall training time. In a regime where computational time on quantum computers is a precious resource, we propose this might lead to substantial practical savings. We evaluate this approach on 6x6 binary images from an augmented bars and stripes data set using a QBM with 36 visible units and 8 hidden units. Using an Inception score inspired metric, we compare QBM training times with and without using coresets."}}
{"id": "7uf-Ws3FGNc", "cdate": 1640995200000, "mdate": 1681650890819, "content": {"title": "Critical Points in Quantum Generative Models", "abstract": ""}}
{"id": "6BtJ7sjlexs", "cdate": 1640995200000, "mdate": 1681650890826, "content": {"title": "Training Quantum Boltzmann Machines with Coresets", "abstract": ""}}
{"id": "2f1z55GVQN", "cdate": 1632875493348, "mdate": null, "content": {"title": "Critical Points in Quantum Generative Models", "abstract": "One of the most important properties of neural networks is the clustering of local minima of the loss function near the global minimum, enabling efficient training. Though generative models implemented on quantum computers are known to be more expressive than their traditional counterparts, it has empirically been observed that these models experience a transition in the quality of their local minima. Namely, below some critical number of parameters, all local minima are far from the global minimum in function value; above this critical parameter count, all local minima are good approximators of the global minimum. Furthermore, for a certain class of quantum generative models, this transition has empirically been observed to occur at parameter counts exponentially large in the problem size, meaning practical training of these models is out of reach. Here, we give the first proof of this transition in trainability, specializing to this latter class of quantum generative model. We use techniques inspired by those used to study the loss landscapes of classical neural networks. We also verify that our analytic results hold experimentally even at modest model sizes."}}
{"id": "ymVUzwsAkM", "cdate": 1609459200000, "mdate": 1681650890840, "content": {"title": "Enhancing Generative Models via Quantum Correlations", "abstract": ""}}
{"id": "om0Gi3Gl7_", "cdate": 1577836800000, "mdate": 1681650890853, "content": {"title": "Coreset Clustering on Small Quantum Computers", "abstract": ""}}
{"id": "eoWvG3TY68", "cdate": 1546300800000, "mdate": 1681650890919, "content": {"title": "Near-Term Quantum-Classical Associative Adversarial Networks", "abstract": ""}}
{"id": "CfVyDtcrMF", "cdate": 1483228800000, "mdate": 1681650890828, "content": {"title": "Variational Quantum Factoring", "abstract": ""}}
