{"id": "WCl2B5GZB42", "cdate": 1652697258696, "mdate": 1652697258696, "content": {"title": "CLOUDTRAN: CLOUD REMOVAL FROM MULTITEMPORAL SATELLITE IMAGES USING AXIAL TRANSFORMER NETWORKS", "abstract": "We present a method for cloud-removal from satellite images using axial transformer networks. The method considers a set of multitemporal images in a given region of interest together with the corresponding cloud masks, and delivers a cloud-free image for a specific day of the year. We propose the combination of an encoder-decoder model employing axial attention layers for the estimation of the low-resolution cloud-free image, together with a fully parallel upsampler that reconstructs the image at full resolution. The method is compared with various baselines and state-of-the-art methods on two Sentinel-2 datasets, showing significant improvements across multiple standard metrics used for image quality assessment."}}
{"id": "ZhROomigtmg", "cdate": 1651839014268, "mdate": 1651839014268, "content": {"title": "Discovery and recognition of motion primitives in human activities", "abstract": "We present a novel framework for the automatic discovery and recognition of motion primitives in videos of human activities. Given the 3D pose of a human in a video, human motion primitives are discovered by optimizing the \u2018motion flux\u2019, a quantity which captures the motion variation of a group of skeletal joints. A normalization of the primitives is proposed in order to make them invariant with respect to a subject anatomical variations and data sampling rate. The discovered primitives are unknown and unlabeled and are unsupervisedly collected into classes via a hierarchical non-parametric Bayes mixture model. Once classes are determined and labeled they are further analyzed for establishing models for recognizing discovered primitives. Each primitive model is defined by a set of learned parameters. Given new video data and given the estimated pose of the subject appearing on the video, the motion is segmented into primitives, which are recognized with a probability given according to the parameters of the learned models. Using our framework we build a publicly available dataset of human motion primitives, using sequences taken from well-known motion capture datasets. We expect that our framework, by providing an objective way for discovering and categorizing human motion, will be a useful tool in numerous research fields including video analysis, human inspired motion generation, learning by demonstration, intuitive human-robot interaction, and human behavior analysis."}}
{"id": "f2tNzvOciVR", "cdate": 1651838877977, "mdate": 1651838877977, "content": {"title": "Saliency prediction in the coherence theory of attention", "abstract": "In the coherence theory of attention, introduced by Rensink, O\u2019Regan, and Clark (2000), a coherence field is defined by a hierarchy of structures supporting the activities taking place across the different stages of visual attention. At the interface between low level and mid-level attention processing stages are the proto-objects; these are generated in parallel and collect features of the scene at specific location and time. These structures fade away if the region is no further attended by attention. We introduce a method to computationally model these structures. Our model is based experimentally on data collected in dynamic 3D environments via the Gaze Machine, a gaze measurement framework. This framework allows to record pupil motion at the required speed and projects the point of regard in the 3D space (Pirri et al., 2011, Pizzoli et al., 2011). To generate proto-objects the model is extended to vibrating circular membranes whose initial displacement is generated by the features that have been selected by classification. The energy of the vibrating membranes is used to predict saliency in visual search tasks."}}
{"id": "KNgrY5Pi_4C", "cdate": 1609459200000, "mdate": 1651838233924, "content": {"title": "Production Machine Learning Frameworks for Geospatial Big Data", "abstract": "We explore the use of production Machine Learning (ML) frameworks for automatically building ML models for cloud-based services that exploit geospatial big data and value-added products. We combine two widely used production ML frameworks to hierarchically decompose the tasks involved with the fetching and preprocessing of the data as well as with model training, evaluation, and selection. We assess the usability, reproducibility and performance of the frameworks both qualitatively and quantitatively. We examine the challenging case of a cloud-based seabed mapping service that process multispecrtal multibeam echosounder data captured in different marine surveys, involving a number of data processing and machine learning tasks."}}
{"id": "pns0E6Bm7US", "cdate": 1546300800000, "mdate": 1651838233923, "content": {"title": "Vision Based Modeling of Plants Phenotyping in Vertical Farming under Artificial Lighting", "abstract": "In this paper, we present a novel method for vision based plants phenotyping in indoor vertical farming under artificial lighting. The method combines 3D plants modeling and deep segmentation of the higher leaves, during a period of 25&ndash;30 days, related to their growth. The novelty of our approach is in providing 3D reconstruction, leaf segmentation, geometric surface modeling, and deep network estimation for weight prediction to effectively measure plant growth, under three relevant phenotype features: height, weight and leaf area. Together with the vision based measurements, to verify the soundness of our proposed method, we also harvested the plants at specific time periods to take manual measurements, collecting a great amount of data. In particular, we manually collected 2592 data points related to the plant phenotype and 1728 images of the plants. This allowed us to show with a good number of experiments that the vision based methods ensure a quite accurate prediction of the considered features, providing a way to predict plant behavior, under specific conditions, without any need to resort to human measurements."}}
{"id": "nuzKfc8jzN", "cdate": 1546300800000, "mdate": 1651838233924, "content": {"title": "Anticipation and next action forecasting in video: an end-to-end model with memory", "abstract": "Action anticipation and forecasting in videos do not require a hat-trick, as far as there are signs in the context to foresee how actions are going to be deployed. Capturing these signs is hard because the context includes the past. We propose an end-to-end network for action anticipation and forecasting with memory, to both anticipate the current action and foresee the next one. Experiments on action sequence datasets show excellent results indicating that training on histories with a dynamic memory can significantly improve forecasting performance."}}
{"id": "ekjrnNO8kPX", "cdate": 1546300800000, "mdate": 1651838233927, "content": {"title": "Deep execution monitor for robot assistive tasks", "abstract": "We consider a novel approach to high-level robot task execution for a robot assistive task. In this work we explore the problem of learning to predict the next subtask by introducing a deep model for both sequencing goals and for visually evaluating the state of a task. We show that deep learning for monitoring robot tasks execution very well supports the interconnection between task-level planning and robot operations. These solutions can also cope with the natural non-determinism of the execution monitor. We show that a deep execution monitor leverages robot performance. We measure the improvement taking into account some robot helping tasks performed at a warehouse."}}
{"id": "UbEPc3xCge", "cdate": 1546300800000, "mdate": 1651838233924, "content": {"title": "Visual search and recognition for robot task execution and monitoring", "abstract": "Visual search of relevant targets in the environment is a crucial robot skill. We propose a preliminary framework for the execution monitor of a robot task, taking care of the robot attitude to visually searching the environment for targets involved in the task. Visual search is also relevant to recover from a failure. The framework exploits deep reinforcement learning to acquire a \"common sense\" scene structure and it takes advantage of a deep convolutional network to detect objects and relevant relations holding between them. The framework builds on these methods to introduce a vision-based execution monitoring, which uses classical planning as a backbone for task execution. Experiments show that with the proposed vision-based execution monitor the robot can complete simple tasks and can recover from failures in autonomy."}}
{"id": "2a0BRMlDaBb", "cdate": 1546300800000, "mdate": 1651838233924, "content": {"title": "Anticipating Next Goal for Robot Plan Prediction", "abstract": "Goal reasoning is a main objective for robot task execution. Here we propose a deep model for learning to infer a next goal, while performing an activity. Because predicting the next goal state requires a robot language, not comparable to sentences, we introduce a specific metric for optimization, which is related to the representation the robot has of the scene. Experiments of the proposed idea and method have been done at a warehouse with a humanoid robot performing tasks assisting a maintenance technician working at a production line."}}
{"id": "-NRPDimaA8", "cdate": 1546300800000, "mdate": 1651838233925, "content": {"title": "Help by Predicting What to Do", "abstract": "Robots assisting humans with some specific tasks have been demonstrated on several occasions. A further challenging idea is to anticipate human needs by mining the future demand from the next action prediction. To trigger this anticipation mechanism a robot has to recognize what the human is doing now, foresee what the human will do next, and from their connection guesstimating what to do to help. We propose here a deep network combining the essential components of this challenging process leading to foreseeing the help that can be provided in human-robot collaboration."}}
