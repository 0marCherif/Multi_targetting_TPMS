{"id": "bjS_K1oJ68z", "cdate": 1672531200000, "mdate": 1681694520002, "content": {"title": "An Efficient Tester-Learner for Halfspaces", "abstract": "We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan (2023). In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution -- e.g., the Gaussian -- must pass the test. This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold. We consider the setting where the target distribution is Gaussian (or more generally any strongly log-concave distribution) in $d$ dimensions and the noise model is either Massart or adversarial (agnostic). For Massart noise, our tester-learner runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error $\\mathsf{opt} + \\epsilon$ for any strongly log-concave target distribution. For adversarial noise, our tester-learner obtains error $O(\\mathsf{opt}) + \\epsilon$ in polynomial time when the target distribution is Gaussian; for strongly log-concave distributions, we obtain $\\tilde{O}(\\mathsf{opt}) + \\epsilon$ in quasipolynomial time. Prior work on testable learning ignores the labels in the training set and checks that the empirical moments of the covariates are close to the moments of the base distribution. Here we develop new tests of independent interest that make critical use of the labels and combine them with the moment-matching approach of Gollakota et al. (2023). This enables us to simulate a variant of the algorithm of Diakonikolas et al. (2020) for learning noisy halfspaces using nonconvex SGD but in the testable learning setting."}}
{"id": "GzESlaXaN04", "cdate": 1652737431666, "mdate": null, "content": {"title": "Hardness of Noise-Free Learning for Two-Hidden-Layer Neural Networks", "abstract": "We give superpolynomial statistical query (SQ) lower bounds for learning two-hidden-layer ReLU networks with respect to Gaussian inputs in the standard (noise-free) model. No general SQ lower bounds were known for learning ReLU networks of any depth in this setting: previous SQ lower bounds held only for adversarial noise models (agnostic learning) (Kothari and Klivans 2014, Goel et al. 2020a, Diakonikolas et al. 2020a) or restricted models such as correlational SQ (Goel et al. 2020b, Diakonikolas et al. 2020b). Prior work hinted at the impossibility of our result: Vempala and Wilmes (2019) showed that general SQ lower bounds cannot apply to any real-valued family of functions that satisfies a simple non-degeneracy condition. To circumvent their result, we refine a lifting procedure due to Daniely and Vardi (2021) that reduces Boolean PAC learning problems to Gaussian ones. We show how to extend their technique to other learning models and, in many well-studied cases, obtain a more efficient reduction. As such, we also prove new cryptographic hardness results for PAC learning two-hidden-layer ReLU networks, as well as new lower bounds for learning constant-depth ReLU networks from membership queries."}}
{"id": "tAe4Ldm2VKN", "cdate": 1640995200000, "mdate": 1684025803774, "content": {"title": "Hardness of Noise-Free Learning for Two-Hidden-Layer Neural Networks", "abstract": "We give superpolynomial statistical query (SQ) lower bounds for learning two-hidden-layer ReLU networks with respect to Gaussian inputs in the standard (noise-free) model. No general SQ lower bounds were known for learning ReLU networks of any depth in this setting: previous SQ lower bounds held only for adversarial noise models (agnostic learning) (Kothari and Klivans 2014, Goel et al. 2020a, Diakonikolas et al. 2020a) or restricted models such as correlational SQ (Goel et al. 2020b, Diakonikolas et al. 2020b). Prior work hinted at the impossibility of our result: Vempala and Wilmes (2019) showed that general SQ lower bounds cannot apply to any real-valued family of functions that satisfies a simple non-degeneracy condition. To circumvent their result, we refine a lifting procedure due to Daniely and Vardi (2021) that reduces Boolean PAC learning problems to Gaussian ones. We show how to extend their technique to other learning models and, in many well-studied cases, obtain a more efficient reduction. As such, we also prove new cryptographic hardness results for PAC learning two-hidden-layer ReLU networks, as well as new lower bounds for learning constant-depth ReLU networks from membership queries."}}
{"id": "lP10GERci6f", "cdate": 1640995200000, "mdate": 1672682881857, "content": {"title": "A Moment-Matching Approach to Testable Learning and a New Characterization of Rademacher Complexity", "abstract": ""}}
{"id": "hWm7fFU9sW", "cdate": 1640995200000, "mdate": 1672682881854, "content": {"title": "On the Hardness of PAC-learning Stabilizer States with Noise", "abstract": ""}}
{"id": "zi8oalocbm", "cdate": 1577836800000, "mdate": 1672682881855, "content": {"title": "The Polynomial Method is Universal for Distribution-Free Correlational SQ Learning", "abstract": ""}}
{"id": "iWXb6YbrxO", "cdate": 1577836800000, "mdate": 1672682881856, "content": {"title": "Packing Tree Degree Sequences", "abstract": ""}}
{"id": "OKYouUSlQF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Statistical-Query Lower Bounds via Functional Gradients", "abstract": "We give the first statistical-query lower bounds for agnostically learning any non-polynomial activation with respect to Gaussian marginals (e.g., ReLU, sigmoid, sign). For the specific problem of ReLU regression (equivalently, agnostically learning a ReLU), we show that any statistical-query algorithm with tolerance $n^{-(1/\\epsilon)^b}$ must use at least $2^{n^c} \\epsilon$ queries for some constants $b, c &gt; 0$, where $n$ is the dimension and $\\epsilon$ is the accuracy parameter. Our results rule out {\\em general} (as opposed to correlational) SQ learning algorithms, which is unusual for real-valued learning problems. Our techniques involve a gradient boosting procedure for ``amplifying'' recent lower bounds due to Diakonikolas et al.\\ (COLT 2020) and Goel et al.\\ (ICML 2020) on the SQ dimension of functions computed by two-layer neural networks. The crucial new ingredient is the use of a nonstandard convex functional during the boosting procedure. This also yields a best-possible reduction between two commonly studied models of learning: agnostic learning and probabilistic concepts."}}
{"id": "25JAv8fYKdA", "cdate": 1577836800000, "mdate": null, "content": {"title": "Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent", "abstract": "We give the first superpolynomial lower bounds for learning one-layer neural networks with respect to the Gaussian distribution for a broad class of algorithms. In the regression setting, we prove ..."}}
