{"id": "K8TPaYT5n6", "cdate": 1683884407561, "mdate": 1683884407561, "content": {"title": "Generalization bounds for neural ordinary differential equations and deep residual networks", "abstract": "Neural ordinary differential equations (neural ODEs) are a popular family of continuous-depth deep learning models. In this work, we consider a large family of parameterized ODEs with continuous-in-time parameters, which include time-dependent neural ODEs. We derive a generalization bound for this class by a Lipschitz-based argument. By leveraging the analogy between neural ODEs and deep residual networks, our approach yields in particular a generalization bound for a class of deep residual networks. The bound involves the magnitude of the difference between successive weight matrices. We illustrate numerically how this quantity affects the generalization capability of neural networks."}}
{"id": "U0WaheJ3bKF", "cdate": 1672531200000, "mdate": 1683884229385, "content": {"title": "Leveraging the two timescale regime to demonstrate convergence of neural networks", "abstract": "We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime."}}
{"id": "38Hml4RisM", "cdate": 1640995200000, "mdate": 1683884229381, "content": {"title": "Scaling ResNets in the Large-depth Regime", "abstract": "Deep ResNets are recognized for achieving state-of-the-art results in complex machine learning tasks. However, the remarkable performance of these architectures relies on a training procedure that needs to be carefully crafted to avoid vanishing or exploding gradients, particularly as the depth $L$ increases. No consensus has been reached on how to mitigate this issue, although a widely discussed strategy consists in scaling the output of each layer by a factor $\\alpha_L$. We show in a probabilistic setting that with standard i.i.d. initializations, the only non-trivial dynamics is for $\\alpha_L = 1/\\sqrt{L}$ (other choices lead either to explosion or to identity mapping). This scaling factor corresponds in the continuous-time limit to a neural stochastic differential equation, contrarily to a widespread interpretation that deep ResNets are discretizations of neural ordinary differential equations. By contrast, in the latter regime, stability is obtained with specific correlated initializations and $\\alpha_L = 1/L$. Our analysis suggests a strong interplay between scaling and regularity of the weights as a function of the layer index. Finally, in a series of experiments, we exhibit a continuous range of regimes driven by these two parameters, which jointly impact performance before and after training."}}
{"id": "QT9ulkiN-LX", "cdate": 1621629901006, "mdate": null, "content": {"title": "Framing RNN as a kernel method: A neural ODE approach", "abstract": "Building on the interpretation of a recurrent neural network (RNN) as a continuous-time neural differential equation, we show, under appropriate conditions, that the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature. This connection allows us to frame a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a consequence, we obtain theoretical guarantees on generalization and stability for a large class of recurrent networks. Our results are illustrated on simulated datasets."}}
{"id": "_9xQSiBvKIj", "cdate": 1609459200000, "mdate": 1642412264040, "content": {"title": "Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs", "abstract": "Pierre Marion, Pawel Nowak, Francesco Piccinno. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "OqtvUkZ4hhs", "cdate": 1609459200000, "mdate": 1683884229382, "content": {"title": "Framing RNN as a kernel method: A neural ODE approach", "abstract": "Building on the interpretation of a recurrent neural network (RNN) as a continuous-time neural differential equation, we show, under appropriate conditions, that the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature. This connection allows us to frame a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a consequence, we obtain theoretical guarantees on generalization and stability for a large class of recurrent networks. Our results are illustrated on simulated datasets."}}
{"id": "Z5AuhJCO7J", "cdate": 1577836800000, "mdate": 1683884229394, "content": {"title": "An algorithm to compute the t-value of a digital net and of its projections", "abstract": ""}}
{"id": "MVmbGVeQGA", "cdate": 1577836800000, "mdate": 1683884229391, "content": {"title": "A Tool for Custom Construction of QMC and RQMC Point Sets", "abstract": "We present LatNet Builder, a software tool to find good parameters for lattice rules, polynomial lattice rules, and digital nets in base 2, for quasi-Monte Carlo (QMC) and randomized quasi-Monte Carlo (RQMC) sampling over the s-dimensional unit hypercube. The selection criteria are figures of merit that give different weights to different subsets of coordinates. They are upper bounds on the worst-case error (for QMC) or variance (for RQMC) for integrands rescaled to have a norm of at most one in certain Hilbert spaces of functions. We summarize what are the various Hilbert spaces, discrepancies, types of weights, figures of merit, types of constructions, and search methods supported by LatNet Builder. We briefly discuss its organization and we provide simple illustrations of what it can do."}}
