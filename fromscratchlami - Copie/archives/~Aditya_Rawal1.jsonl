{"id": "ZBcvnVtE2c", "cdate": 1653320188884, "mdate": 1653320188884, "content": {"title": "Memory Efficient Continual Learning with Transformers", "abstract": "See the attached paper for the full abstract"}}
{"id": "U07d1Y-x2E", "cdate": 1652737692567, "mdate": null, "content": {"title": "Memory Efficient Continual Learning with Transformers", "abstract": "In many real-world scenarios, data to train machine learning models becomes available over time. Unfortunately, these models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is difficult to prevent due to practical constraints. For instance, the amount of data that can be stored or the computational resources that can be used might be limited. Moreover, applications increasingly rely on large pre-trained neural networks, such as pre-trained Transformers, since compute or data might not be available in sufficiently large quantities to practitioners to train from scratch. In this paper, we devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters. Different than the existing approaches, our method is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks. On both image and text classification tasks, we empirically demonstrate that our method maintains a good predictive performance without retraining the model or increasing the number of model parameters over time. The resulting model is also significantly faster at inference time compared to Adapter-based state-of-the-art methods."}}
{"id": "jk1094_ZiN", "cdate": 1601308301751, "mdate": null, "content": {"title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search", "abstract": "Neural Architecture Search (NAS) explores a large space of architectural motifs --  \na compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands or more data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available.\nOur hope is that this work can inspire a new research direction in studying the performance of extracted components of models in a synthetic diagnostic setting optimized to provide informative evaluations."}}
{"id": "HJg_ECEKDr", "cdate": 1569439280239, "mdate": null, "content": {"title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning  to Generate Synthetic Training Data", "abstract": "This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is applicable to supervised, unsupervised, and reinforcement learning. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g.\\ a freshly initialized neural network) trains on before being tested on a target task. We then differentiate \\emph{through the entire learning process} via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Overall, GTNs represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions."}}
{"id": "Skg3104FDS", "cdate": 1569439203559, "mdate": null, "content": {"title": "First-Order Preconditioning via Hypergradient Descent", "abstract": "Standard gradient-descent methods are susceptible to a range of issues that can impede training, such as high correlations and different scaling in parameter space. These difficulties can be addressed by second-order approaches that apply a preconditioning matrix to the gradient to improve convergence.  Unfortunately, such algorithms typically struggle to scale to high-dimensional problems, in part because the calculation of specific preconditioners such as the inverse Hessian or Fisher information matrix is highly expensive. We introduce first-order preconditioning (FOP), a fast, scalable approach that generalizes previous work on hypergradient descent (Almeida et al., 1998; Maclaurin et al., 2015; Baydin et al., 2017) to learn a preconditioning matrix that only makes use of first-order information. Experiments show that FOP is able to improve the performance of standard deep learning optimizers on several visual classification tasks with minimal computational overhead. We also investigate the properties of the learned preconditioning matrices and perform a preliminary theoretical analysis of the algorithm."}}
{"id": "r1lrAiA5Ym", "cdate": 1538087884784, "mdate": null, "content": {"title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity", "abstract": "The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks."}}
{"id": "S1lVniC5Y7", "cdate": 1538087852416, "mdate": null, "content": {"title": "From Nodes to Networks: Evolving Recurrent Neural Networks", "abstract": "Gated recurrent networks such as those composed of Long Short-Term Memory\n(LSTM) nodes have recently been used to improve state of the art in many sequential\nprocessing tasks such as speech recognition and machine translation. However,\nthe basic structure of the LSTM node is essentially the same as when it was\nfirst conceived 25 years ago. Recently, evolutionary and reinforcement learning\nmechanisms have been employed to create new variations of this structure. This\npaper proposes a new method, evolution of a tree-based encoding of the gated\nmemory nodes, and shows that it makes it possible to explore new variations more\neffectively than other methods. The method discovers nodes with multiple recurrent\npaths and multiple memory cells, which lead to significant improvement in the\nstandard language modeling benchmark task. Remarkably, this node did not perform\nwell in another task, music modeling, but it was possible to evolve a different\nnode that did, demonstrating that the approach discovers customized structure for\neach task. The paper also shows how the search process can be speeded up by\ntraining an LSTM network to estimate performance of candidate structures, and\nby encouraging exploration of novel solutions. Thus, evolutionary design of complex\nneural network structures promises to improve performance of deep learning\narchitectures beyond human ability to do so."}}
