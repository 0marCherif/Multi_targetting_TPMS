{"id": "tntIAuQ50E", "cdate": 1667893315586, "mdate": null, "content": {"title": "On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline", "abstract": "We revisit a simple Learning-from-Scratch baseline for visuo-motor control that uses data augmentation and a shallow ConvNet. We find that this baseline has competitive performance with recent methods that leverage frozen visual representations trained on large-scale vision datasets."}}
{"id": "E-0zNz5J5BM", "cdate": 1653595782114, "mdate": null, "content": {"title": "Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning ", "abstract": "Learning generalizable policies that can adapt to unseen environments remains challenging in visual Reinforcement Learning (RL). Existing approaches try to acquire a robust representation via diversifying the appearances of in-domain observations for better generalization. Limited by the specific observations of the environment, these methods ignore the possibility of exploring diverse real-world image datasets. In this paper, we investigate how a visual RL agent would benefit from the off-the-shelf visual representations. Surprisingly, we find that the early layers in an ImageNet pre-trained ResNet model could provide rather generalizable representations for visual RL. Hence, we propose Pre-trained Image Encoder for Generalizable visual reinforcement learning (PIE-G), a simple yet effective framework that can generalize to the unseen visual scenarios in a zero-shot manner. Extensive experiments are conducted on DMControl Generalization Benchmark, DMControl Manipulation Tasks, and Drawer World to verify the effectiveness of PIE-G. Empirical evidence suggests PIE-G can significantly outperforms previous state-of-the-art methods in terms of generalization performance. In particular, PIE-G boasts a 55% generalization performance gain on average in the challenging video background setting."}}
{"id": "FQtku8rkp3", "cdate": 1652737480486, "mdate": null, "content": {"title": "Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning", "abstract": "Learning generalizable policies that can adapt to unseen environments remains challenging in visual Reinforcement Learning (RL). Existing approaches try to acquire a robust representation via diversifying the appearances of in-domain observations for better generalization. Limited by the specific observations of the environment, these methods ignore the possibility of exploring diverse real-world image datasets. In this paper, we investigate how a visual RL agent would benefit from the off-the-shelf visual representations. Surprisingly, we find that the early layers in an ImageNet pre-trained ResNet model could provide rather generalizable representations for visual RL. Hence, we propose Pre-trained Image Encoder for Generalizable visual reinforcement learning (PIE-G), a simple yet effective framework that can generalize to the unseen visual scenarios in a zero-shot manner. Extensive experiments are conducted on DMControl Generalization Benchmark, DMControl Manipulation Tasks, Drawer World, and CARLA to verify the effectiveness of PIE-G. Empirical evidence suggests PIE-G improves sample efficiency and significantly outperforms previous state-of-the-art methods in terms of generalization performance. In particular, PIE-G boasts a 55% generalization performance gain on average in the challenging video background setting. Project Page: https://sites.google.com/view/pie-g/home."}}
{"id": "hZBKztrzMo", "cdate": 1640995200000, "mdate": 1681651196435, "content": {"title": "Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning", "abstract": ""}}
{"id": "T7uEaQY7pTM", "cdate": 1640995200000, "mdate": 1667537187787, "content": {"title": "Extraneousness-Aware Imitation Learning", "abstract": "Visual imitation learning provides an effective framework to learn skills from demonstrations. However, the quality of the provided demonstrations usually significantly affects the ability of an agent to acquire desired skills. Therefore, the standard visual imitation learning assumes near-optimal demonstrations, which are expensive or sometimes prohibitive to collect. Previous works propose to learn from noisy demonstrations; however, the noise is usually assumed to follow a context-independent distribution such as a uniform or gaussian distribution. In this paper, we consider another crucial yet underexplored setting -- imitation learning with task-irrelevant yet locally consistent segments in the demonstrations (e.g., wiping sweat while cutting potatoes in a cooking tutorial). We argue that such noise is common in real world data and term them \"extraneous\" segments. To tackle this problem, we introduce Extraneousness-Aware Imitation Learning (EIL), a self-supervised approach that learns visuomotor policies from third-person demonstrations with extraneous subsequences. EIL learns action-conditioned observation embeddings in a self-supervised manner and retrieves task-relevant observations across visual demonstrations while excluding the extraneous ones. Experimental results show that EIL outperforms strong baselines and achieves comparable policies to those trained with perfect demonstration on both simulated and real-world robot control tasks. The project page can be found at https://sites.google.com/view/eil-website."}}
{"id": "RvRFphgiPJo", "cdate": 1640995200000, "mdate": 1667354091405, "content": {"title": "A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning", "abstract": "Visual reinforcement learning (RL), which makes decisions directly from high-dimensional visual inputs, has demonstrated significant potential in various domains. However, deploying visual RL techniques in the real world remains challenging due to their low sample efficiency and large generalization gaps. To tackle these obstacles, data augmentation (DA) has become a widely used technique in visual RL for acquiring sample-efficient and generalizable policies by diversifying the training data. This survey aims to provide a timely and essential review of DA techniques in visual RL in recognition of the thriving development in this field. In particular, we propose a unified framework for analyzing visual RL and understanding the role of DA in it. We then present a principled taxonomy of the existing augmentation techniques used in visual RL and conduct an in-depth discussion on how to better leverage augmented data in different scenarios. Moreover, we report a systematic empirical evaluation of DA-based techniques in visual RL and conclude by highlighting the directions for future research. As the first comprehensive survey of DA in visual RL, this work is expected to offer valuable guidance to this emerging field."}}
{"id": "J1gO9FOoxi", "cdate": 1640995200000, "mdate": 1681493972210, "content": {"title": "On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline", "abstract": ""}}
{"id": "AMN05M3_Xhq", "cdate": 1640995200000, "mdate": 1668642119840, "content": {"title": "USEEK: Unsupervised SE(3)-Equivariant 3D Keypoints for Generalizable Manipulation", "abstract": "Can a robot manipulate intra-category unseen objects in arbitrary poses with the help of a mere demonstration of grasping pose on a single object instance? In this paper, we try to address this intriguing challenge by using USEEK, an unsupervised SE(3)-equivariant keypoints method that enjoys alignment across instances in a category, to perform generalizable manipulation. USEEK follows a teacher-student structure to decouple the unsupervised keypoint discovery and SE(3)-equivariant keypoint detection. With USEEK in hand, the robot can infer the category-level task-relevant object frames in an efficient and explainable manner, enabling manipulation of any intra-category objects from and to any poses. Through extensive experiments, we demonstrate that the keypoints produced by USEEK possess rich semantics, thus successfully transferring the functional knowledge from the demonstration object to the novel ones. Compared with other object representations for manipulation, USEEK is more adaptive in the face of large intra-category shape variance, more robust with limited demonstrations, and more efficient at inference time."}}
{"id": "8tQ-cOhljJO", "cdate": 1640995200000, "mdate": 1681142017550, "content": {"title": "Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning", "abstract": ""}}
{"id": "-P0rgtJH0mk", "cdate": 1640995200000, "mdate": 1681651196438, "content": {"title": "Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning", "abstract": ""}}
