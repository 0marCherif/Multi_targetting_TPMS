{"id": "uCXNOeL0TG", "cdate": 1652737759224, "mdate": null, "content": {"title": "Fairness for Workers Who Pull the Arms: An Index Based Policy for Allocation of Restless Bandit Tasks", "abstract": "Motivated by applications such as machine repair, project monitoring, and anti-poaching patrol scheduling, we study intervention planning of stochastic processes under resource constraints. This planning problem has previously been modeled as restless multi-armed bandits (RMAB), where each arm is an intervention-dependent Markov Decision Process. However, the existing literature assumes all intervention resources belong to a single uniform pool, limiting their applicability to real-world settings where interventions are carried out by a set of workers, each with their own costs, budgets, and intervention effects. In this work, we consider a novel RMAB setting, called multi-worker restless bandits (MWRMAB) with heterogeneous workers. The goal is to plan an intervention schedule that maximizes the expected reward while satisfying budget constraints on each worker as well as fairness in terms of the load assigned to each worker. Our contributions are two-fold: (1)~we provide a multi-worker extension of the Whittle index to tackle heterogeneous costs and per-worker budget and (2)~ we develop an index-based scheduling policy to achieve fairness. Further, we evaluate our method on various cost structures and show that our method significantly outperforms other baselines in terms of fairness without sacrificing much in reward accumulated."}}
{"id": "Seb4JPLjcec", "cdate": 1646077526885, "mdate": null, "content": {"title": "Restless and Uncertain: Robust Policies for Restless Bandits via Deep Multi-Agent Reinforcement Learning", "abstract": "We introduce robustness in \\textit{restless multi-armed bandits} (RMABs), a popular model for constrained resource allocation among independent stochastic processes (arms). Nearly all RMAB techniques assume stochastic dynamics are precisely known. However, in many real-world settings, dynamics are estimated with significant \\textit{uncertainty}, e.g., via historical data, which can lead to bad outcomes if ignored. To address this, we develop an algorithm to compute minimax regret--robust policies for RMABs.\nOur approach uses a double oracle framework (oracles for \\textit{agent} and \\textit{nature}), which is often used for single-process robust planning but requires significant new techniques to accommodate the combinatorial nature of RMABs. Specifically, we design a deep reinforcement learning (RL) algorithm, DDLPO, which tackles the combinatorial challenge by learning an auxiliary ``$\\lambda$-network'' in tandem with policy networks per arm, greatly reducing sample complexity, with guarantees on convergence. DDLPO, of general interest, implements our reward-maximizing agent oracle. We then tackle the challenging regret-maximizing nature oracle, a non-stationary RL challenge, by formulating it as a multi-agent RL problem between a policy optimizer and adversarial nature. \nThis formulation is of general interest---we solve it for RMABs by creating a multi-agent extension of DDLPO with a shared critic. We show our approaches work well in three experimental domains."}}
{"id": "HJx2G1iNKv", "cdate": 1620613884947, "mdate": null, "content": {"title": "Collapsing Bandits and Their Applications to Public Health Interventions", "abstract": "We propose and study Collpasing Bandits, a new restless multi-armed bandit (RMAB) setting in which each arm follows a binary-state Markovian process with a special structure: when an arm is played, the state is fully observed, thus \"collapsing\" any uncertainty, but when an arm is passive, no observation is made, thus allowing uncertainty to evolve. The goal is to keep as many arms in the \"good\" state as possible by planning a limited budget of actions per round. Such Collapsing Bandits are natural models for many healthcare domains in which workers must simultaneously monitor patients and deliver interventions in a way that maximizes the health of their patient cohort. Our main contributions are as follows: (i) Building on the Whittle index technique for RMABs, we derive conditions under which the Collapsing Bandits problem is indexable. Our derivation hinges on novel conditions that characterize when the optimal policies may take the form of either \"forward\" or \"reverse\" threshold policies. (ii) We exploit the optimality of threshold policies to build fast algorithms for computing the Whittle index, including a closed-form. (iii) We evaluate our algorithm on several data distributions including data from a real-world healthcare task in which a worker must monitor and deliver interventions to maximize their patients' adherence to tuberculosis medication. Our algorithm achieves a 3-order-of-magnitude speedup compared to state-of-the-art RMAB techniques while achieving similar performance."}}
{"id": "JKLTJhZhNfD", "cdate": 1620546589386, "mdate": null, "content": {"title": "Beyond \"To Act or Not to Act\": Fast Lagrangian Approaches to General Multi-Action Restless Bandits", "abstract": "This paper presents new algorithms and theoretical results for solutions to Multi-action Multi-armed Restless Bandits, an important\nbut insufficiently studied generalization of traditional Multi-armed\nRestless Bandits (MARBs). Though MARBs are popular for modeling many problems, they are restricted to binary actions, i.e., \"to act\nor not to act\". This renders them unable to capture critical complexities faced by planners in real domains, such as a system manager balancing maintenance, repair, and job scheduling, or a health worker\ndeciding among treatments for a given patient. Limited previous\nwork on Multi-action MARBs has only been specialized to subproblems. Here we derive multiple algorithms for use on general\nMulti-action MARBs using Lagrangian relaxation techniques, leading to the following contributions: (i) We develop BLam, a bound optimization algorithm which leverages problem convexity to quickly\nand provably converge to the well-performing Lagrange policy; (ii)\nWe develop SampleLam, a fast sampling technique for estimating\nthe Lagrange policy, and derive a concentration bound to investigate its convergence properties; (iii) We derive best and worst case\ncomputational complexities for our algorithms as well as our main\ncompetitor; (iv) We provide experimental results comparing our\nalgorithms to baselines on simulated distributions, including one\nmotivated by a real-world community health intervention task. Our\napproach achieves significant, up to ten-fold speedups over more\ngeneral methods without sacrificing performance and is widely\napplicable across general Multi-action MARBs. Code is availabl"}}
{"id": "UblnTpMmaN-", "cdate": 1577836800000, "mdate": null, "content": {"title": "Collapsing Bandits and Their Application to Public Health Intervention", "abstract": "We propose and study Collapsing Bandits, a new restless multi-armed bandit (RMAB) setting in which each arm follows a binary-state Markovian process with a special structure: when an arm is played, the state is fully observed, thus\u201ccollapsing\u201d any uncertainty, but when an arm is passive, no observation is made, thus allowing uncertainty to evolve. The goal is to keep as many arms in the \u201cgood\u201d state as possible by planning a limited budget of actions per round. Such CollapsingBandits are natural models for many healthcare domains in which health workers must simultaneously monitor patients and deliver interventions in a way that maximizes the health of their patient cohort. Our main contributions are as follows: (i) Building on the Whittle index technique for RMABs, we derive conditions under which the Collapsing Bandits problem is indexable. Our derivation hinges on novel conditions that characterize when the optimal policies may take the form of either\u201cforward\u201d or \u201creverse\u201d threshold policies. (ii) We exploit the optimality of threshold policies to build fast algorithms for computing the Whittle index, including a closed-form. (iii) We evaluate our algorithm on several data distributions including data from a real-world healthcare task in which a worker must monitor and deliver interventions to maximize their patients\u2019 adherence to tuberculosis medication. Our algorithm achieves a 3-order-of-magnitude speedup compared to state-of-the-art RMAB techniques, while achieving similar performance. The code is available at:https://github.com/AdityaMate/collapsing_bandits"}}
{"id": "ipDMkJYafxR", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning to Prescribe Interventions for Tuberculosis Patients Using Digital Adherence Data", "abstract": "Digital Adherence Technologies (DATs) are an increasingly popular method for verifying patient adherence to many medications. We analyze data from one city served by 99DOTS, a phone-call-based DAT deployed for Tuberculosis (TB) treatment in India where nearly 3 million people are afflicted with the disease each year. The data contains nearly 17,000 patients and 2.1M dose records. We lay the groundwork for learning from this real-world data, including a method for avoiding the effects of unobserved interventions in training data used for machine learning. We then construct a deep learning model, demonstrate its interpretability, and show how it can be adapted and trained in three different clinical scenarios to better target and improve patient care. In the real-time risk prediction setting our model could be used to proactively intervene with 21% more patients and before 76% more missed doses than current heuristic baselines. For outcome prediction, our model performs 40% better than baseline methods, allowing cities to target more resources to clinics with a heavier burden of patients at risk of failure. Finally, we present a case study demonstrating how our model can be trained in an end-to-end decision focused learning setting to achieve 15% better solution quality in an example decision problem faced by health workers."}}
{"id": "QAfuvY20A9O", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning to Detect Heavy Drinking Episodes Using Smartphone Accelerometer Data", "abstract": ""}}
