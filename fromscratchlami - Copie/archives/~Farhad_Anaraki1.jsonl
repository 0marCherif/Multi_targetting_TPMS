{"id": "JtN6IICPsu", "cdate": 1648674199613, "mdate": 1648674199613, "content": {"title": "Kernel matrix approximation on class-imbalanced data with an application to scientific simulation", "abstract": "Generating low-rank approximations of kernel matrices that arise in nonlinear machine learning techniques holds the potential to significantly alleviate the memory and computational burdens. A compelling approach centers on finding a concise set of exemplars or landmarks to reduce the number of similarity measure evaluations from quadratic to linear concerning the data size. However, a key challenge is to regulate tradeoffs between the quality of landmarks and resource consumption. Despite the volume of research in this area, current understanding is limited regarding the performance of landmark selection techniques in the presence of class-imbalanced data sets that are becoming increasingly prevalent in many applications. Hence, this paper provides a comprehensive empirical investigation using several real-world imbalanced data sets, including scientific data, by evaluating the quality of approximate low-rank decompositions and examining their influence on the accuracy of downstream tasks. Furthermore, we present a new landmark selection technique called Distance-based Importance Sampling and Clustering (DISC), in which the relative importance scores are computed for improving accuracy-efficiency tradeoffs compared to existing works that range from probabilistic sampling to clustering methods. The proposed landmark selection method follows a coarse-to-fine strategy to capture the intrinsic structure of complex data sets, allowing us to substantially reduce the computational complexity and memory footprint with minimal loss in accuracy."}}
{"id": "v9ujDvCtw3b", "cdate": 1648674076941, "mdate": 1648674076941, "content": {"title": "Adaptive Data Compression for Classification Problems", "abstract": "Data subset selection is a crucial task in deploying machine learning algorithms under strict constraints regarding memory and computation resources. Despite extensive research in this area, a practical difficulty is the lack of rigorous strategies for identifying the optimal size of the reduced data to regulate trade-offs between accuracy and efficiency. Furthermore, existing methods are often built around specific machine learning models, and translating existing theoretical results into practice is challenging for practitioners. To address these problems, we propose two adaptive compression algorithms for classification problems by formulating data subset selection in the form of interactive teaching. The user interacts with the learning task at hand to adapt to the unique structure of the problem at hand, developing an iterative importance sampling scheme. We also propose to couple importance sampling and a diversity criterion to further control the evolution of the data summary over the rounds of interaction. We conduct extensive experiments on several data sets, including imbalanced and multiclass data, and various classification algorithms, such as ensemble learning and neural networks. Our results demonstrate the performance, efficiency, and ease of implementation of the underlying framework."}}
{"id": "BybWJCeu-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Randomized Clustered Nystrom for Large-Scale Kernel Machines", "abstract": "The Nystrom method has been popular for generating the low-rank approximation of kernel matrices that arise in many machine learning problems. The approximation quality of the Nystrom method depends crucially on the number of selected landmark points and the selection procedure. In this paper, we present a novel algorithm to compute the optimal Nystrom low-approximation when the number of landmark points exceed the target rank. Moreover, we introduce a randomized algorithm for generating landmark points that is scalable to large-scale data sets. The proposed method performs K-means clustering on low-dimensional random projections of a data set and, thus, leads to significant savings for high-dimensional data sets. Our theoretical results characterize the tradeoffs between the accuracy and efficiency of our proposed method. Extensive experiments demonstrate the competitive performance as well as the efficiency of our proposed method."}}
{"id": "HyZKV3WdWH", "cdate": 1388534400000, "mdate": null, "content": {"title": "Memory and Computation Efficient PCA via Very Sparse Random Projections", "abstract": "Algorithms that can efficiently recover principal components in very high-dimensional, streaming, and/or distributed data settings have become an important topic in the literature. In this paper, w..."}}
