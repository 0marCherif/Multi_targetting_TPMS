{"id": "X1ZFG__-jo", "cdate": 1664928785595, "mdate": null, "content": {"title": "Is Unsupervised Performance Estimation Impossible When Both Covariates and Labels shift?", "abstract": "Accurately estimating and explaining an ML model\u2019s performance on new datasets is increasingly critical in reliable ML model deployment. With no labels on the new datasets, performance estimation paradigms often assume either covariate shift or label shift, and thus lead to poor estimation accuracy when the assumptions are broken. Is unsupervised performance monitoring really impossible when both covariates and labels shift? In this paper, we give a negative answer. To do so, we introduce Sparse Joint Shift (SJS), a new distribution shift model considering the shift of labels and a few features. We characterize the mathematical conditions under which SJS is identifiable. This shows that unsupervised performance monitoring is indeed feasible when a few features and labels shift. In addition, we propose SEES, an algorithmic framework for performance estimation under SJS. Preliminary experiments show the superior estimation performance of SEES over existing paradigms. This opens the door to tackling the joint shift of both covariates and labels without observing new datasets\u2019 labels."}}
{"id": "4-oNRO0Fqy", "cdate": 1663850192275, "mdate": null, "content": {"title": "Model ChangeLists: Characterizing Changes in ML Prediction APIs", "abstract": "Updates to Machine Learning as a Service (MLaaS) APIs may affect downstream systems that depend on their predictions. However, performance changes introduced by these updates are poorly documented by providers and seldom studied in the literature. As a result, users are left wondering: do model updates introduce subtle performance changes that could adversely affect my system? Ideally, users would have access to a detailed ChangeList specifying the slices of data where model performance has improved and degraded since the update. But, producing a ChangeList is challenging because it requires (1) discovering slices in the absence of detailed annotations or metadata, (2) accurately attributing coherent concepts to the discovered slices, and (3) communicating them to the user in a digestable manner. We introduce Mocha, an interactive framework for building, verifying and releasing ChangeLists that addresses these challenges. Using it, we perform a large-scale analysis of three real-world MLaaS API updates. We produce a ChangeList for each, identifying over 100 coherent data slices on which the model\u2019s performance changed significantly. Notably, we find 63 instances where an update improves performance globally, but hurts performance on a coherent slice \u2013 a phenomenon not previously documented at scale in the literature. These findings underscore the importance of producing a detailed ChangeList when the model behind an API is updated."}}
{"id": "CZeIOfCjMf", "cdate": 1654496613981, "mdate": null, "content": {"title": "HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions", "abstract": "Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoptions in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performances. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and an integral part of analytics, it is critical to systematically study and compare different APIs with each other and to characterize how individual APIs change over time. However, this practically important topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition, and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the API\u2019s output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML  as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIs\u2019 performance changes substantially over time\u2014several APIs\u2019 accuracies dropped on specific benchmark datasets. Even when the API\u2019s aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs\u2019 performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS."}}
{"id": "dJqjKDrfs_F", "cdate": 1653256605567, "mdate": 1653256605567, "content": {"title": "FrugalML: How to use ML prediction APIs more accurately and cheaply", "abstract": "Prediction APIs offered for a fee are a fast-growing industry and an important part of machine learning as a service. While many such services are available, the heterogeneity in their price and performance makes it challenging for users to decide which API or combination of APIs to use for their own data and budget. We take a first step towards addressing this challenge by proposing FrugalML, a principled framework that jointly learns the strength and weakness of each API on different data, and performs an efficient optimization to automatically identify the best sequential strategy to adaptively use the available APIs within a budget constraint. Our theoretical analysis shows that natural sparsity in the formulation can be leveraged to make FrugalML efficient. We conduct systematic experiments using ML APIs from Google, Microsoft, Amazon, IBM, Baidu and other providers for tasks including facial emotion recognition, sentiment analysis and speech recognition. Across various tasks, FrugalML can achieve up to 90% cost reduction while matching the accuracy of the best single API, or up to 5% better accuracy while matching the best API\u2019s cost."}}
{"id": "BK0O0xLntFM", "cdate": 1652737357950, "mdate": null, "content": {"title": "Estimating and Explaining Model Performance When Both Covariates and Labels Shift", "abstract": "Deployed machine learning (ML) models often encounter new user data that differs from their training data. Therefore, estimating how well a given model might perform on the new data is an important step toward reliable ML applications. This is very challenging, however, as the data distribution can change in flexible ways, and we may not have any labels on the new data, which is often the case in monitoring settings. In this paper, we propose a new distribution shift model, Sparse Joint Shift (SJS), which considers the joint shift of both labels and a few features. This unifies and generalizes several existing shift models including label shift and sparse covariate shift, where only marginal feature or label distribution shifts are considered. We describe mathematical conditions under which SJS is identifiable. We further propose SEES, an algorithmic framework to characterize the distribution shift under SJS and to estimate a model\u2019s performance on new data without any labels. We conduct extensive experiments on several real-world datasets with various ML models. Across different datasets and distribution shifts, SEES achieves significant (up to an order of magnitude) shift estimation error improvements over existing approaches."}}
{"id": "gFDFKC4gHL4", "cdate": 1632875695229, "mdate": null, "content": {"title": "How Did the Model Change? Efficiently Assessing Machine Learning API Shifts ", "abstract": "ML prediction APIs from providers like Amazon and Google have made it simple to use ML in applications. A challenge for users is that such APIs continuously change over time as the providers update models, and changes can happen silently without users knowing. It is thus important to monitor when and how much the MLAPIs\u2019 performance shifts. To provide detailed change assessment, we model MLAPI shifts as confusion matrix differences, and propose a principled algorithmic framework, MASA, to provably assess these shifts efficiently given a sample budget constraint.MASAemploys an upper-confidence bound based approach to adaptively determine on which data point to query the ML API to estimate shifts. Empirically, we observe significant ML API shifts from 2020 to 2021 among 12 out of 36 applications using commercial APIs from Google, Microsoft, Amazon, and other providers. These real-world shifts include both improvements and reductions in accuracy. Extensive experiments show that MASA can estimate such API shifts more accurately than standard approaches given the same budget"}}
{"id": "AypVMhFfuc5", "cdate": 1632875694959, "mdate": null, "content": {"title": "FrugalMCT: Efficient Online ML API Selection for Multi-Label Classification Tasks", "abstract": "Multi-label classification tasks such as OCR and multi-object recognition are a major focus of the growing machine learning as a service industry. While many multi-label APIs are available, it is challenging for users to decide which API to use for their own data and budget, due to the heterogeneity in their prices and performance. Recent work has shown how to efficiently select and combine single label APIs to optimize performance and cost. However, its computation cost is exponential in the number of labels, and is not suitable for settings like OCR. In this work, we propose FrugalMCT, a principled framework that adaptively selects the APIs to use for different data in an online fashion while respecting the user\u2019s budget. It allows combining ML APIs\u2019 predictions for any single data point, and selects the best combination based on an accuracy estimator. We run systematic experiments using ML APIs from Google, Microsoft, Amazon, IBM, Tencent, and other providers for tasks including multi-label image classification, scene text recognition, and named entity recognition. Across these tasks, FrugalMCT can achieve over 90% cost reduction while matching the accuracy of the best single API, or up to 8% better accuracy while matching the best API\u2019s cost."}}
{"id": "Hk-FlMbAZ", "cdate": 1518730165763, "mdate": null, "content": {"title": "The Manifold Assumption and Defenses Against Adversarial Perturbations", "abstract": "In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and applies a \\emph{small perturbation} to  $\\bfx$ to produce another point $\\bfx'$ that $F$ classifies \\emph{incorrectly}.  In this paper, we propose taking into account \\emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \\|F(\\bfx)\\|_\\infty$ (i.e. how confident $F$ is about its prediction?). Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \\emph{confident regions of a good model should be well separated}. We give formalizations of this property and examine existing robust training objectives in view of them. Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence. However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions. We thus propose embedding objectives and algorithms, and perform an empirical study using this method. Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.\n"}}
{"id": "ry-o73WubS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training", "abstract": "In this paper we study leveraging <em>confidence information</em> induced by adversarial training to reinforce adversarial robustness of a given adversarially trained model. A natural measure of co..."}}
{"id": "HJ-Rli-_-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "DRACO: Byzantine-resilient Distributed Training via Redundant Gradients", "abstract": "Distributed model training is vulnerable to byzantine system failures and adversarial compute nodes, i.e., nodes that use malicious updates to corrupt the global model stored at a parameter server ..."}}
