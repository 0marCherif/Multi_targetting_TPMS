{"id": "I_JTFYOXlUJ", "cdate": 1672531200000, "mdate": 1682685638011, "content": {"title": "Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets", "abstract": "We focus on learning unknown dynamics from data using ODE-nets templated on implicit numerical initial value problem solvers. First, we perform Inverse Modified error analysis of the ODE-nets using unrolled implicit schemes for ease of interpretation. It is shown that training an ODE-net using an unrolled implicit scheme returns a close approximation of an Inverse Modified Differential Equation (IMDE). In addition, we establish a theoretical basis for hyper-parameter selection when training such ODE-nets, whereas current strategies usually treat numerical integration of ODE-nets as a black box. We thus formulate an adaptive algorithm which monitors the level of error and adapts the number of (unrolled) implicit solution iterations during the training process, so that the error of the unrolled approximation is less than the current learning loss. This helps accelerate training, while maintaining accuracy. Several numerical experiments are performed to demonstrate the advantages of the proposed algorithm compared to nonadaptive unrollings, and validate the theoretical analysis. We also note that this approach naturally allows for incorporating partially known physical terms in the equations, giving rise to what is termed ``gray box\" identification."}}
{"id": "xAfHSkTv5z", "cdate": 1640995200000, "mdate": 1682685637952, "content": {"title": "On convergence of neural network methods for solving elliptic interface problems", "abstract": "With the remarkable empirical success of neural networks across diverse scientific disciplines, rigorous error and convergence analysis are also being developed and enriched. However, there has been little theoretical work focusing on neural networks in solving interface problems. In this paper, we perform a convergence analysis of physics-informed neural networks (PINNs) for solving second-order elliptic interface problems. Specifically, we consider PINNs with domain decomposition technologies and introduce gradient-enhanced strategies on the interfaces to deal with boundary and interface jump conditions. It is shown that the neural network sequence obtained by minimizing a Lipschitz regularized loss function converges to the unique solution to the interface problem in $H^2$ as the number of samples increases. Numerical experiments are provided to demonstrate our theoretical analysis."}}
{"id": "cUUj6Xv0hkN", "cdate": 1640995200000, "mdate": 1682685637893, "content": {"title": "On Numerical Integration in Neural Ordinary Differential Equations", "abstract": "The combination of ordinary differential equations and neural networks, i.e., neural ordinary differential equations (Neural ODE), has been widely studied from various angles. However, deciphering ..."}}
{"id": "WnNymGz0VJn", "cdate": 1640995200000, "mdate": 1682685637897, "content": {"title": "Poisson Integrators based on splitting method for Poisson systems", "abstract": "We propose Poisson integrators for the numerical integration of separable Poisson systems. We analyze three situations in which the Poisson systems are separated in three ways and the Poisson integrators can be constructed by using the splitting method. Numerical results show that the Poisson integrators outperform the higher order non-Poisson integrators in phase orbit tracking, long-term energy conservation and efficiency."}}
{"id": "O33_8vw4Vi", "cdate": 1640995200000, "mdate": 1682685637895, "content": {"title": "Explicit K-symplectic methods for nonseparable non-canonical Hamiltonian systems", "abstract": "We propose efficient numerical methods for nonseparable non-canonical Hamiltonian systems which are explicit, K-symplectic in the extended phase space with long time energy conservation properties. They are based on extending the original phase space to several copies of the phase space and imposing a mechanical restraint on the copies of the phase space. Explicit K-symplectic methods are constructed for three non-canonical Hamiltonian systems. Numerical results show that they outperform the higher order Runge-Kutta methods in preserving the phase orbit and the energy of the system over long time."}}
{"id": "LQZTT8NDuG8", "cdate": 1640995200000, "mdate": 1682685637880, "content": {"title": "Error analysis based on inverse modified differential equations for discovery of dynamics using linear multistep methods and deep learning", "abstract": "Along with the practical success of the discovery of dynamics using deep learning, the theoretical analysis of this approach has attracted increasing attention. Prior works have established the grid error estimation with auxiliary conditions for the discovery of dynamics using linear multistep methods and deep learning. And we extend the existing error analysis in this work. We first introduce the concept of inverse modified differential equations (IMDE) for linear multistep methods and show that the learned model returns a close approximation of the IMDE. Based on the IMDE, we prove that the error between the discovered system and the target system is bounded by the sum of the LMM discretization error and the learning loss. Furthermore, the learning loss is quantified by combining the approximation and generalization theories of neural networks, and thereby we obtain the priori error estimates for the discovery of dynamics using linear multistep methods. Several numerical experiments are performed to verify the theoretical analysis."}}
{"id": "FFn-QqFbsw", "cdate": 1640995200000, "mdate": 1682685637888, "content": {"title": "Approximation capabilities of measure-preserving neural networks", "abstract": ""}}
{"id": "Eh66BaVy_D", "cdate": 1640995200000, "mdate": 1682685637932, "content": {"title": "VPNets: Volume-preserving neural networks for learning source-free dynamics", "abstract": ""}}
{"id": "ws4KfbYK97", "cdate": 1577836800000, "mdate": 1682685638037, "content": {"title": "SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems", "abstract": ""}}
{"id": "g7GQ9ij58wE", "cdate": 1577836800000, "mdate": 1682685638001, "content": {"title": "Deep Hamiltonian networks based on symplectic integrators", "abstract": "HNets is a class of neural networks on grounds of physical prior for learning Hamiltonian systems. This paper explains the influences of different integrators as hyper-parameters on the HNets through error analysis. If we define the network target as the map with zero empirical loss on arbitrary training data, then the non-symplectic integrators cannot guarantee the existence of the network targets of HNets. We introduce the inverse modified equations for HNets and prove that the HNets based on symplectic integrators possess network targets and the differences between the network targets and the original Hamiltonians depend on the accuracy orders of the integrators. Our numerical experiments show that the phase flows of the Hamiltonian systems obtained by symplectic HNets do not exactly preserve the original Hamiltonians, but preserve the network targets calculated; the loss of the network target for the training data and the test data is much less than the loss of the original Hamiltonian; the symplectic HNets have more powerful generalization ability and higher accuracy than the non-symplectic HNets in addressing predicting issues. Thus, the symplectic integrators are of critical importance for HNets."}}
