{"id": "vtv83s2Ps9", "cdate": 1664731448583, "mdate": null, "content": {"title": "Parameter Free Dual Averaging: Optimizing Lipschitz Functions in a Single Pass", "abstract": "Both gradient descent and dual averaging for convex Lipschitz functions\nhave convergence rates that are highly dependent on the choice of learning\nrate. Even when the Lipschitz constant is known, setting the learning rate to achieve the optimal convergence rate requires knowing a bound on the distance from the initial point to the solution set $D$. A number\nof approaches are known that relax this requirement, but they either\nrequire line searches, restarting (hyper-parameter grid search), or do not derive\nfrom the gradient descent or dual averaging frameworks (coin-betting).\nIn this work we describe a single pass method, with no back-tracking or line searches, \nderived from dual averaging,\nwhich does not require knowledge of $D$ yet asymptotically achieves\nthe optimal rate of convergence for the complexity class of Convex\nLipschitz functions.\n"}}
{"id": "2pYMlvmsNaK", "cdate": 1601308022856, "mdate": null, "content": {"title": "Dual Averaging is Surprisingly Effective for Deep Learning Optimization", "abstract": "First-order stochastic optimization methods are currently the most widely used class of methods for training deep neural networks. However, the choice of the optimizer has become an ad-hoc rule that can significantly affect the performance. For instance, SGD with momentum (SGD+M) is typically used in computer vision (CV) and Adam is used for training transformer models for Natural Language Processing (NLP). Using the wrong method can lead to significant performance degradation. Inspired by the dual averaging algorithm, we propose Modernized Dual Averaging (MDA), an optimizer that is able to perform as well as SGD+M in CV and as Adam in NLP. Our method is not adaptive and is significantly simpler than Adam. We show that MDA induces a decaying uncentered L 2 -regularization compared to vanilla SGD+M and hypothesize that this may explain why it works on NLP problems where SGD+M fails."}}
{"id": "oUeq1dAwFks", "cdate": 1577836800000, "mdate": null, "content": {"title": "End-to-End Variational Networks for Accelerated MRI Reconstruction.", "abstract": "The slow acquisition speed of magnetic resonance imaging (MRI) has led to the development of two complementary methods: acquiring multiple views of the anatomy simultaneously (parallel imaging) and acquiring fewer samples than necessary for traditional signal processing methods (compressed sensing). While the combination of these methods has the potential to allow much faster scan times, reconstruction from such undersampled multi-coil data has remained an open problem. In this paper, we present a new approach to this problem that extends previously proposed variational methods by learning fully end-to-end. Our method obtains new state-of-the-art results on the fastMRI dataset for both brain and knee MRIs."}}
{"id": "gbqGdVULe6", "cdate": 1577836800000, "mdate": null, "content": {"title": "MRI Banding Removal via Adversarial Training.", "abstract": "MRI images reconstructed from sub-sampled Cartesian data using deep learning techniques often show a characteristic banding (sometimes described as streaking), which is particularly strong in low signal-to-noise regions of the reconstructed image. In this work, we propose the use of an adversarial loss that penalizes banding structures without requiring any human annotation. Our technique greatly reduces the appearance of banding, without requiring any additional computation or post-processing at reconstruction time. We report the results of a blind comparison against a strong baseline by a group of expert evaluators (board-certified radiologists), where our approach is ranked superior at banding removal with no statistically significant loss of detail."}}
{"id": "ElwcH-3eFy", "cdate": 1577836800000, "mdate": null, "content": {"title": "Advancing machine learning for MR image reconstruction with an open competition: Overview of the 2019 fastMRI challenge.", "abstract": "Purpose: To advance research in the field of machine learning for MR image reconstruction with an open challenge. Methods: We provided participants with a dataset of raw k-space data from 1,594 consecutive clinical exams of the knee. The goal of the challenge was to reconstruct images from these data. In order to strike a balance between realistic data and a shallow learning curve for those not already familiar with MR image reconstruction, we ran multiple tracks for multi-coil and single-coil data. We performed a two-stage evaluation based on quantitative image metrics followed by evaluation by a panel of radiologists. The challenge ran from June to December of 2019. Results: We received a total of 33 challenge submissions. All participants chose to submit results from supervised machine learning approaches. Conclusion: The challenge led to new developments in machine learning for image reconstruction, provided insight into the current state of the art in the field, and highlighted remaining hurdles for clinical adoption."}}
{"id": "BJedt6VKPS", "cdate": 1569439103541, "mdate": null, "content": {"title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks", "abstract": "Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work."}}
{"id": "H1GFS4SgUS", "cdate": 1567802433346, "mdate": null, "content": {"title": "On the Curved Geometry of Accelerated Optimization", "abstract": "In this work we propose a differential geometric motivation for Nesterov's accelerated gradient method (AGM) for strongly-convex problems. By considering the optimization procedure as occurring on a Riemannian manifold with a natural structure, The AGM method can be seen as the proximal point method applied in this curved space. This viewpoint can also be extended to the continuous time case, where the accelerated gradient method arises from the natural block-implicit Euler discretization of an ODE on the manifold. We provide an analysis of the convergence rate of this ODE for quadratic objectives."}}
{"id": "SJeYHErxUB", "cdate": 1567802433127, "mdate": null, "content": {"title": "On the Ineffectiveness of Variance Reduced Optimization for Deep Learning", "abstract": "The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success. The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem. We show that naive application of the SVRG technique and related approaches fail, and explore why."}}
{"id": "RAJEh6eUJzO", "cdate": 1546300800000, "mdate": null, "content": {"title": "Offset Masking Improves Deep Learning based Accelerated MRI Reconstructions.", "abstract": "Deep learning approaches to accelerated MRI take a matrix of sampled Fourier-space lines as input and produce a spatial image as output. In this work we show that by careful choice of the offset used in the sampling procedure, the symmetries in k-space can be better exploited, producing higher quality reconstructions than given by standard equally-spaced samples or randomized samples motivated by compressed sensing."}}
{"id": "BIQkyz9VuLv", "cdate": 1546300800000, "mdate": null, "content": {"title": "GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil MRI Reconstruction.", "abstract": "Magnetic Resonance Image (MRI) acquisition is an inherently slow process which has spurred the development of two different acceleration methods: acquiring multiple correlated samples simultaneously (parallel imaging) and acquiring fewer samples than necessary for traditional signal processing methods (compressed sensing). Both methods provide complementary approaches to accelerating the speed of MRI acquisition. In this paper, we present a novel method to integrate traditional parallel imaging methods into deep neural networks that is able to generate high quality reconstructions even for high acceleration factors. The proposed method, called GrappaNet, performs progressive reconstruction by first mapping the reconstruction problem to a simpler one that can be solved by a traditional parallel imaging methods using a neural network, followed by an application of a parallel imaging method, and finally fine-tuning the output with another neural network. The entire network can be trained end-to-end. We present experimental results on the recently released fastMRI dataset and show that GrappaNet can generate higher quality reconstructions than competing methods for both $4\\times$ and $8\\times$ acceleration."}}
