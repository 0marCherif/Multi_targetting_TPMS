{"id": "cB6Uvk4UzX", "cdate": 1684349472838, "mdate": 1684349472838, "content": {"title": "Riemannian Stochastic Gradient Method for Nested Composition Optimization", "abstract": "This work considers optimization of the composition of functions in a nested form over Riemannian manifolds where each function contains an expectation. This type of problems is gaining popularity in applications such as policy evaluation in reinforcement learning or model customization in meta-learning. The standard Riemannian stochastic gradient methods for non-compositional optimization cannot be directly applied as the stochastic approximation of inner functions create bias in the gradients of the outer functions. For two-level composition optimization, we present a Riemannian Stochastic Composition Gradient Descent (R-SCGD) method that finds an approximate stationary point, with expected squared Riemannian gradient smaller than $\\epsilon$, in $\\cO(\\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer function and stochastic function and gradient oracles of the inner function. Furthermore, we generalize the R-SCGD algorithms for problems with multi-level nested compositional structures, with the same complexity of $\\cO(\\epsilon^{-2})$ for the first-order stochastic oracle. Finally, the performance of the R-SCGD method is numerically evaluated over a policy evaluation problem in reinforcement learning."}}
{"id": "pFJnDpULcL", "cdate": 1684349108234, "mdate": 1684349108234, "content": {"title": "On the Theoretical Guarantees for Parameter Estimation of Gaussian Random Field Models: A Sparse Precision Matrix Approach", "abstract": "Iterative methods for fitting a Gaussian Random Field (GRF) model via maximum likelihood (ML) estimation require solving a nonconvex optimization problem. The problem is aggravated for anisotropic GRFs where the number of covariance function parameters increases with the dimension. Even evaluation of the likelihood function requires $O(n^3)$ floating point operations, where $n$ denotes the number of data locations. In this paper, we propose a new two-stage procedure to estimate the parameters of second-order stationary GRFs. First, a convex likelihood problem regularized with a weighted $l_1$-norm, utilizing the available distance information between observation locations, is solved to fit a sparse precision (inverse covariance) matrix to the observed data. Second, the parameters of the covariance function are estimated by solving a least squares problem. Theoretical error bounds for the solutions of stage I and II problems are provided, and their tightness is investigated."}}
{"id": "9KAoNt8uAS", "cdate": 1684175676909, "mdate": 1684175676909, "content": {"title": "Fitting ARMA Time Series Models without Identification: A Proximal Approach", "abstract": "Fitting autoregressive moving average (ARMA) time series models requires model identification before parameter estimation. Model identification involves determining the order of the autoregressive and moving average components which is generally performed by inspection of the autocorrelation and partial autocorrelation functions or other offline methods. In this work, we regularize the parameter estimation optimization problem with a nonsmooth hierarchical sparsity-inducing penalty based on two path graphs that allows performing model identification and parameter estimation simultaneously. A proximal block coordinate descent algorithm is then proposed to solve the underlying optimization problem efficiently. The resulting model satisfies the required stationarity and invertibility conditions for ARMA models. Numerical studies supporting the performance of the proposed method and comparing it with other schemes are presented."}}
{"id": "kNdetpZ-XV", "cdate": 1684175490233, "mdate": null, "content": {"title": "Spatiotemporal wind forecasting by learning a hierarchically sparse inverse covariance matrix using wind directions", "abstract": "Given the advances in online data acquisition systems, statistical learning models are increasingly used to forecast wind speed. In electricity markets, wind farm production forecasts are needed for the day-ahead, intra-day, and real-time markets. In this work, we use a spatiotemporal model that leverages wind dynamics to forecast wind speed. Using a priori knowledge of the wind direction, we propose a maximum likelihood estimate of the inverse covariance matrix regularized with a hierarchical sparsity-inducing penalty. The resulting inverse covariance estimate not only exhibits the benefits of a sparse estimator, but also enables meaningful sparse structures by considering wind direction. A proximal method is used to solve the underlying optimization problem. The proposed methodology is used to forecast six-hour-ahead wind speeds in 20-minute time intervals for a case study in Texas. We compare our method with a number of other statistical methods. Prediction performance measures and the Diebold\u2013Mariano test show the potential of the proposed method, specifically when reasonably accurate estimates of the wind directions are available."}}
{"id": "T6V8dK-6U-", "cdate": 1683904178887, "mdate": null, "content": {"title": "Riemannian Stochastic Variance-Reduced Cubic Regularized Newton Method for Submanifold Optimization", "abstract": "We propose a stochastic variance-reduced cubic regularized Newton algorithm to optimize the finite-sum problem over a Riemannian submanifold of the Euclidean space. The proposed algorithm requires a full gradient and Hessian update at the beginning of each epoch while it performs stochastic variance-reduced updates in the iterations within each epoch. The iteration complexity of $\ud835\udc42(\\epsilon^{\u22123/2})$\nto obtain an $(\\epsilon,\\sqrt{\\epsilon})$-second-order stationary point, i.e., a point with the Riemannian gradient norm upper bounded by $-\\sqrt{\\epsilon}$ and minimum eigenvalue of Riemannian Hessian lower bounded by is established when the manifold is embedded in the Euclidean space. Furthermore, the paper proposes a computationally more appealing modification of the algorithm which only requires an inexact solution of the cubic regularized Newton subproblem with the same iteration complexity. The proposed algorithm is evaluated and compared with three other Riemannian second-order methods over two numerical studies on estimating the inverse scale matrix of the multivariate t-distribution on the manifold of symmetric positive definite matrices and estimating the parameter of a linear classifier on the sphere manifold."}}
{"id": "cVLtRJEiBC2", "cdate": 1683903475579, "mdate": 1683903475579, "content": {"title": "A First-Order Optimization Algorithm for Statistical Learning with Hierarchical Sparsity Structure", "abstract": "In many statistical learning problems, it is desired that the optimal solution conform to an a priori known sparsity structure represented by a directed acyclic graph. Inducing such structures by means of convex regularizers requires nonsmooth penalty functions that exploit group overlapping. Our study focuses on evaluating the proximal operator of the latent overlapping group lasso developed by Jacob et al. in 2009. We implemented an alternating direction method of multiplier with a sharing scheme to solve large-scale instances of the underlying optimization problem efficiently. In the absence of strong convexity, global linear convergence of the algorithm is established using the error bound theory. More specifically, the paper contributes to establishing primal and dual error bounds when the nonsmooth component in the objective function does not have a polyhedral epigraph. We also investigate the effect of the graph structure on the speed of convergence of the algorithm. Detailed numerical simulation studies over different graph structures supporting the proposed algorithm and two applications in learning are provided."}}
{"id": "l6fuEyhoc-", "cdate": 1683879384082, "mdate": null, "content": {"title": "Stochastic Composition Optimization of Functions Without Lipschitz Continuous Gradient", "abstract": "In this paper, we study stochastic optimization of two-level composition of functions without Lipschitz continuous gradient. The smoothness property is generalized by the notion of relative smoothness which provokes the Bregman gradient method. We propose three stochastic composition Bregman gradient algorithms for the three possible relatively smooth compositional scenarios and provide their sample complexities to achieve an \ud835\udf16-approximate stationary point. For the smooth of relatively smooth composition, the first algorithm requires \ue23b(\ud835\udf16\u22122) calls to the stochastic oracles of the inner function value and gradient as well as the outer function gradient. When both functions are relatively smooth, the second algorithm requires \ue23b(\ud835\udf16\u22123) calls to the inner function value stochastic oracle and \ue23b(\ud835\udf16\u22122) calls to the inner and outer functions gradients stochastic oracles. We further improve the second algorithm by variance reduction for the setting where just the inner function is smooth. The resulting algorithm requires \ue23b(\ud835\udf16\u22125/2) calls to the inner function value stochastic oracle, \ue23b(\ud835\udf16\u22123/2) calls to the inner function gradient, and \ue23b(\ud835\udf16\u22122) calls to the outer function gradient stochastic oracles. Finally, we numerically evaluate the performance of these three algorithms over two different examples."}}
