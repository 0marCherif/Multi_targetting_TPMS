{"id": "Jd_ttlYW1A", "cdate": 1621279628670, "mdate": null, "content": {"title": "Apollo: Transferable Architecture Exploration", "abstract": "The looming end of Moore\u2019s Law and ascending use of deep learning drives the design of custom accelerators that are optimized for specific neural architectures. Architecture exploration for such accelerators forms a challenging constrained optimization problem over a complex, high-dimensional, and structured input space with a costly to evaluate objective function. Existing approaches for accelerator design are sample-inefficient and do not transfer knowledge between related optimizations tasks with different design constraints, such as area and/or latency budget, or neural architecture configurations. In this work, we propose a transferable architecture exploration framework, dubbed APOLLO, that leverages recent advances in black-box function optimization for sample-efficient accelerator\ndesign. We use this framework to optimize accelerator configurations of a diverse set of neural architectures with alternative design constraints. We show that our framework finds high reward design configurations (up to 24.6% speedup) more sample-efficiently than a baseline black-box optimization approach. We further show that by transferring knowledge between target architectures with different design constraints, APOLLO is able to find optimal configurations faster and often with better objective value (up to 25% improvements). This encouraging outcome portrays a promising path forward to facilitate generating higher quality accelerators."}}
{"id": "pwwVuSICBgt", "cdate": 1601308378778, "mdate": null, "content": {"title": "Enabling Binary Neural Network Training on the Edge", "abstract": "The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloud-based infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higher-precision alternatives. In this paper, we demonstrate that they are also strongly robust to gradient quantization, thereby making the training of modern models on the edge a practical reality. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions and energy savings vs Courbariaux & Bengio's standard approach. Against the latter, we see coincident memory requirement and energy consumption drops of 2--6$\\times$, while reaching similar test accuracy, across a range of small-scale models trained to classify popular datasets. We also showcase ImageNet training of ResNetE-18, achieving a 3.12$\\times$ memory reduction over the aforementioned standard. Such savings will allow for unnecessary cloud offloading to be avoided, reducing latency and increasing energy efficiency while also safeguarding user privacy."}}
{"id": "xsx58rmaW2p", "cdate": 1601308110880, "mdate": null, "content": {"title": "Making Coherence Out of Nothing At All: Measuring Evolution of Gradient Alignment", "abstract": "We propose a new metric ($m$-coherence) to experimentally study the alignment of per-example gradients during training. Intuitively, given a sample of size $m$, $m$-coherence is the number of examples in the sample that benefit from a small step along the gradient of any one example on average. We show that compared to other commonly used metrics, $m$-coherence is more interpretable, cheaper to compute ($O(m)$ instead of $O(m^2)$) and mathematically cleaner. (We note that $m$-coherence is closely connected to gradient diversity, a quantity previously used in some theoretical bounds.) Using $m$-coherence, we study the evolution of alignment of per-example gradients in ResNet and EfficientNet models on ImageNet and several variants with label noise, particularly from the perspective of the recently proposed Coherent Gradients (CG) theory that provides a simple, unified explanation for memorization and generalization [Chatterjee, ICLR 20]. Although we have several interesting takeaways, our most surprising result concerns memorization. Naively, one might expect that when training with completely random labels, each example is fitted independently, and so $m$-coherence should be close to 1. However, this is not the case: $m$-coherence reaches moderately high values during training (though still much smaller than real labels), indicating that over-parameterized neural networks find common patterns even in scenarios where generalization is not possible. A detailed analysis of this phenomenon provides both a deeper confirmation of CG, but at the same point puts into sharp relief what is missing from the theory in order to provide a complete explanation of generalization in neural networks."}}
{"id": "ES9cpVTyLL", "cdate": 1601308110122, "mdate": null, "content": {"title": "Weak and Strong Gradient Directions: Explaining Memorization, Generalization, and Hardness of Examples at Scale", "abstract": "Coherent Gradients (CGH) [Chatterjee, ICLR 20] is a recently proposed hypothesis to explain why over-parameterized neural networks trained with gradient descent generalize well even though they have sufficient capacity to memorize the training set. The key insight of CGH is that, since the overall gradient for a single step of SGD is the sum of the per-example gradients, it is strongest in directions that reduce the loss on multiple examples if such directions exist. In this paper, we validate CGH on ResNet, Inception, and VGG models on ImageNet. Since the techniques presented in the original paper do not scale beyond toy models and datasets, we propose new methods. By posing the problem of suppressing weak gradient directions as a problem of robust mean estimation, we develop a coordinate-based median of means approach. We present two versions of this algorithm, M3, which partitions a mini-batch into 3 groups and computes the median, and a more efficient version RM3, which reuses gradients from previous two time steps to compute the median. Since they suppress weak gradient directions without requiring per-example gradients, they can be used to train models at scale. Experimentally, we find that they indeed greatly reduce overfitting (and memorization) and thus provide the first convincing evidence that CGH holds at scale. We also propose a new test of CGH that does not depend on adding noise to training labels or on suppressing weak gradient directions. Using the intuition behind CGH, we posit that the examples learned early in the training process (i.e., \"easy\" examples) are precisely those that have more in common with other training examples. Therefore, as per CGH, the easy examples should generalize better amongst themselves than the hard examples amongst themselves. We validate this hypothesis with detailed experiments, and believe that it provides further orthogonal evidence for CGH."}}
{"id": "lX1vaki-r-S", "cdate": 1600139433050, "mdate": null, "content": {"title": "Circuit-Based Intrinsic Methods to Detect Overfitting", "abstract": "The focus of this paper is on intrinsic methods to\ndetect overfitting. By intrinsic methods, we mean\nmethods that rely only on the model and the training data, as opposed to traditional methods (we\ncall them extrinsic methods) that rely on performance on a test set or on bounds from model complexity. We propose a family of intrinsic methods\ncalled Counterfactual Simulation (CFS) which\nanalyze the flow of training examples through\nthe model by identifying and perturbing rare patterns. By applying CFS to logic circuits we get a\nmethod that has no hyper-parameters and works\nuniformly across different types of models such\nas neural networks, random forests and lookup\ntables. Experimentally, CFS can separate models\nwith different levels of overfit using only their\nlogic circuit representations without any access\nto the high level structure. By comparing lookup\ntables, neural networks, and random forests using CFS, we get insight into why neural networks\ngeneralize. In particular, we find that stochastic\ngradient descent in neural nets does not lead to\n\u201cbrute force\u201d memorization, but finds common patterns (whether we train with actual or randomized\nlabels), and neural networks are not unlike forests\nin this regard. Finally, we identify a limitation\nwith our proposal that makes it unsuitable in an\nadversarial setting, but points the way to future\nwork on robust intrinsic methods."}}
{"id": "iUj4VljkKQO", "cdate": 1600139346460, "mdate": null, "content": {"title": "Making Coherence Out of Nothing At All: Measuring the Evolution of Gradient Alignment", "abstract": "We propose a new metric (m-coherence) to experimentally study the alignment of per-example gradients during training. Intuitively, given a sample of size m, m-coherence is the number of examples in the sample that benefit from a small step along the gradient of any one example on average. We show that compared to other commonly used metrics, m-coherence is more interpretable, cheaper to compute (O(m) instead of O(m2)) and mathematically cleaner. (We note that m-coherence is closely connected to gradient diversity, a quantity previously used in some theoretical bounds.) Using m-coherence, we study the evolution of alignment of per-example gradients in ResNet and Inception models on ImageNet and several variants with label noise, particularly from the perspective of the recently proposed Coherent Gradients (CG) theory that provides a simple, unified explanation for memorization and generalization [Chatterjee, ICLR 20]. Although we have several interesting takeaways, our most surprising result concerns memorization. Naively, one might expect that when training with completely random labels, each example is fitted independently, and so m-coherence should be close to 1. However, this is not the case: m-coherence reaches much higher values during training (100s), indicating that over-parameterized neural networks find common patterns even in scenarios where generalization is not possible. A detailed analysis of this phenomenon provides both a deeper confirmation of CG, but at the same point puts into sharp relief what is missing from the theory in order to provide a complete explanation of generalization in neural networks."}}
{"id": "5ngZ0ZV1xWu", "cdate": 1600139199606, "mdate": null, "content": {"title": "Weak and Strong Gradient Directions: Explaining Memorization, Generalization, and Hardness of Examples at Scale", "abstract": "Coherent Gradients (CGH) is a recently proposed hypothesis to explain why over-parameterized neural networks trained with gradient descent generalize well even though they have sufficient capacity to memorize the training set. The key insight of CGH is that, since the overall gradient for a single step of SGD is the sum of the per-example gradients, it is strongest in directions that reduce the loss on multiple examples if such directions exist. In this paper, we validate CGH on ResNet, Inception, and VGG models on ImageNet. Since the techniques presented in the original paper do not scale beyond toy models and datasets, we propose new methods. By posing the problem of suppressing weak gradient directions as a problem of robust mean estimation, we develop a coordinate-based median of means approach. We present two versions of this algorithm, M3, which partitions a mini-batch into 3 groups and computes the median, and a more efficient version RM3, which reuses gradients from previous two time steps to compute the median. Since they suppress weak gradient directions without requiring per-example gradients, they can be used to train models at scale. Experimentally, we find that they indeed greatly reduce overfitting (and memorization) and thus provide the first convincing evidence that CGH holds at scale. We also propose a new test of CGH that does not depend on adding noise to training labels or on suppressing weak gradient directions. Using the intuition behind CGH, we posit that the examples learned early in the training process (i.e., \"easy\" examples) are precisely those that have more in common with other training examples. Therefore, as per CGH, the easy examples should generalize better amongst themselves than the hard examples amongst themselves. We validate this hypothesis with detailed experiments, and believe that it provides further orthogonal evidence for CGH."}}
{"id": "ryeFY0EFwS", "cdate": 1569439361417, "mdate": null, "content": {"title": "Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization", "abstract": "An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting."}}
{"id": "S1XFzYyPM", "cdate": 1518469755289, "mdate": null, "content": {"title": "Learning and Memorization", "abstract": "In the machine learning research community, it is generally believed that\nthere is a tension between memorization and generalization. In this work we\nexamine to what extent this tension exists, by exploring if it is\npossible to generalize through memorization alone. Although direct memorization\nwith a lookup table obviously does not generalize, we find that introducing\ndepth in the form of a network of support-limited lookup tables leads to\ngeneralization that is significantly above chance and closer to those\nobtained by standard learning algorithms on several tasks derived from MNIST \nand CIFAR-10. Furthermore, we demonstrate through a series of\nempirical results that our approach allows for a smooth tradeoff between\nmemorization and generalization and exhibits some of the most salient\ncharacteristics of neural networks: depth improves performance; random data\ncan be memorized and yet there is generalization on real data; and \nmemorizing random data is harder in a certain sense than memorizing real\ndata. The extreme simplicity of the algorithm and potential connections\nwith stability provide important insights into the impact of depth on\nlearning algorithms, and point to several interesting directions for future\nresearch."}}
{"id": "Sk-y0qb_br", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning and Memorization", "abstract": "In the machine learning research community, it is generally believed that there is a tension between memorization and generalization. In this work we examine to what extent this tension exists by e..."}}
