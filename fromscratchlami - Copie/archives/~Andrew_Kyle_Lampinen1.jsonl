{"id": "D2zq0luHq6", "cdate": 1672143652066, "mdate": 1672143652066, "content": {"title": "Can language models handle recursively nested grammatical structures? A case study on comparing models and humans", "abstract": "How should we compare the capabilities of language models and humans? Here, I consider a case study: processing of recursively nested grammatical structures. Prior work has suggested that language models cannot handle these structures as reliably as humans can. However, the humans were provided with instructions and training before being evaluated, while the language models were evaluated zero-shot. I therefore attempt to more closely match the evaluation paradigms by providing language models with few-shot prompts. A simple prompt, which contains substantially less content than the human training, allows large language models to consistently outperform the human results. The same prompt even allows extrapolation to more deeply nested conditions than have been tested in humans. Further, a reanalysis of the prior human experiments suggests that the humans may not perform above chance at the difficult structures initially. These results suggest that large language models can in fact process recursively nested grammatical structures comparably to humans. This case study highlights how discrepancies in the quantity of experiment-specific context can confound comparisons of language models and humans. I use this case study to reflect on the broader challenge of comparing human and model capabilities, and to suggest that there is an important difference between evaluating cognitive models of a specific phenomenon and evaluating broadly-trained models."}}
{"id": "qCi50QTfX3", "cdate": 1672143584777, "mdate": 1672143584777, "content": {"title": "Language models show human-like content effects on reasoning", "abstract": "Abstract reasoning is a key ability for an intelligent system. Large language models achieve abovechance performance on abstract reasoning tasks, but exhibit many imperfections. However, human\nabstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of\nthe reasoning problem. For example, humans reason much more reliably about logical rules that are\ngrounded in everyday situations than arbitrary rules about abstract attributes. The training experiences\nof language models similarly endow them with prior expectations that reflect human knowledge and\nbeliefs. We therefore hypothesized that language models would show human-like content effects on\nabstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason,\n1968). We find that state of the art large language models (with 7 or 70 billion parameters; Hoffmann\net al., 2022) reflect many of the same patterns observed in humans across these tasks \u2014 like humans,\nmodels reason more effectively about believable situations than unrealistic or abstract ones. Our findings have implications for understanding both these cognitive effects, and the factors that contribute\nto language model performance."}}
{"id": "et9T2bwf_v", "cdate": 1672143474007, "mdate": 1672143474007, "content": {"title": "Can language models learn from explanations in context?", "abstract": "Language Models (LMs) can perform new\ntasks by adapting to a few in-context examples.\nFor humans, explanations that connect examples to task principles can improve learning.\nWe therefore investigate whether explanations\nof few-shot examples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how different\ntypes of explanations, instructions, and controls affect zero- and few-shot performance.\nWe analyze these results using statistical multilevel modeling techniques that account for the\nnested dependencies among conditions, tasks,\nprompts, and models. We find that explanations can improve performance\u2014even without tuning. Furthermore, explanations handtuned for performance on a small validation set\noffer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform\ncarefully matched controls, suggesting that the\nbenefits are due to the link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit.\nIn summary, explanations can support the incontext lea"}}
{"id": "-NOQJw5z_KY", "cdate": 1652737746819, "mdate": null, "content": {"title": "Semantic Exploration from Language Abstractions and Pretrained Representations", "abstract": "Effective exploration is a challenge in reinforcement learning (RL). Novelty-based exploration methods can suffer in high-dimensional state spaces, such as continuous partially-observable 3D environments. We address this challenge by defining novelty using semantically meaningful state abstractions, which can be found in learned representations shaped by natural language. In particular, we evaluate vision-language representations, pretrained on natural image captioning datasets. We show that these pretrained representations drive meaningful, task-relevant exploration and improve performance on 3D simulated environments. We also characterize why and how language provides useful abstractions for exploration by considering the impacts of using representations from a pretrained model, a language oracle, and several ablations. We demonstrate the benefits of our approach with on- and off-policy RL algorithms and in two very different task domains---one that stresses the identification and manipulation of everyday objects, and one that requires navigational exploration in an expansive world. Our results suggest that using language-shaped representations could improve exploration for various algorithms and agents in challenging environments."}}
{"id": "lHj-q9BSRjF", "cdate": 1652737415529, "mdate": null, "content": {"title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers", "abstract": "Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having a large number of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. Our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and indicate how future work might encourage both in-context and in-weights learning in domains beyond language. "}}
{"id": "xTw1eWU6rna", "cdate": 1634067446196, "mdate": null, "content": {"title": "Task-driven Discovery of Perceptual Schemas for Generalization in Reinforcement Learning", "abstract": "Deep reinforcement learning (Deep RL) has recently seen significant progress in developing algorithms for generalization. However, most algorithms target a single type of generalization setting. In this work, we study generalization across three disparate task structures: (a) tasks composed of spatial and temporal compositions of regularly occurring object motions; (b) tasks composed of active perception of and navigation towards regularly occurring 3D objects; and (c) tasks composed of navigating through sequences of regularly occurring object-configurations. These diverse task structures all share an underlying idea of compositionality: task completion always involves combining reoccurring segments of task-oriented perception and behavior. We hypothesize that an agent can generalize within a task structure if it can discover representations that capture these reoccurring task-segments. For our tasks, this corresponds to representations for recognizing individual object motions, for navigation towards 3D objects, and for navigating through object-configurations. Taking inspiration from cognitive science, we term representations for reoccurring segments of an agent's experience, \"perceptual schemas\". We propose Composable Perceptual Schemas (CPS), which learns a composable state representation where perceptual schemas are distributed across multiple, relatively small recurrent \"subschema\" modules. Our main technical novelty is an expressive attention function that enables subschemas to dynamically attend to features shared across all positions in the agent's observation. Our experiments indicate our feature-attention mechanism enables CPS to generalize better than recurrent architectures that attend to observations with spatial attention."}}
{"id": "BduNVoPyXBK", "cdate": 1632875696131, "mdate": null, "content": {"title": "Task-driven Discovery of Perceptual Schemas for Generalization in Reinforcement Learning", "abstract": "Deep reinforcement learning (Deep RL) has recently seen significant progress in developing algorithms for generalization. However, most algorithms target a single type of generalization setting. In this work, we study generalization across three disparate task structures: (a) tasks composed of spatial and temporal compositions of regularly occurring object motions; (b) tasks composed of active perception of and navigation towards regularly occurring 3D objects; and (c) tasks composed of navigating through sequences of regularly occurring object-configurations. These diverse task structures all share an underlying idea of compositionality: task completion always involves combining reoccurring segments of task-oriented perception and behavior. We hypothesize that an agent can generalize within a task structure if it can discover representations that capture these reoccurring task-segments. For our tasks, this corresponds to representations for recognizing individual object motions, for navigation towards 3D objects, and for navigating through object-configurations. Taking inspiration from cognitive science, we term representations for reoccurring segments of an agent's experience, \"perceptual schemas\". We propose Composable Perceptual Schemas (CPS), which learns a composable state representation where perceptual schemas are distributed across multiple, relatively small recurrent \"subschema\" modules. Our main technical novelty is an expressive attention function that enables subschemas to dynamically attend to features shared across all positions in the agent's observation. Our experiments indicate our feature-attention mechanism enables CPS to generalize better than recurrent architectures that attend to observations with spatial attention."}}
{"id": "XeqjsCVLk1m", "cdate": 1632875469328, "mdate": null, "content": {"title": "Tell me why!\u2014Explanations support learning relational and causal structure", "abstract": "Explanations play a considerable role in human learning, especially in areas that remain major challenges for AI\u2014forming abstractions, and learning about the relational  and  causal  structure  of  the  world. Here, we explore whether machine learning models might likewise benefit from explanations.  We outline a family of relational tasks that involve selecting an object that is the odd one out in a set (i.e., unique along one of many possible feature dimensions). Odd-one-out tasks require agents to reason over multi-dimensional relationships among a set of objects. We show that agents do not learn these tasks well from reward alone, but achieve >90% performance when they are also trained to generate language explaining object properties or why a choice is correct or incorrect. In further experiments, we show how predicting explanations enables agents to generalize appropriately from ambiguous, causally-confounded training, and even to meta-learn to perform experimental interventions to identify causal structure. We show that explanations help overcome the tendency of agents to fixate on simple features, and explore which aspects of explanations make them most beneficial. Our results suggest that learning from explanations is a powerful principle that could offer a promising path towards training more robust and general machine learning systems."}}
{"id": "wfiVgITyCC_", "cdate": 1621629765578, "mdate": null, "content": {"title": "Towards mental time travel: a hierarchical memory for reinforcement learning agents", "abstract": "Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), that helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore \"mentally time-travel\"--remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new words. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improve agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments."}}
{"id": "3E0HuwrGRd4", "cdate": 1601926992283, "mdate": null, "content": {"title": "What shapes feature representations? Exploring datasets, architectures, and training", "abstract": "In naturalistic learning problems, a model\u2019s input contains a wide range of features,\nsome useful for the task at hand, and others not. Of the useful features, which\nones does the model use? Of the task-irrelevant features, which ones does the\nmodel represent? Answers to these questions are important for understanding the\nbasis of models\u2019 decisions, for example to ensure they are equitable and unbiased,\nas well as for building new models that learn versatile, adaptable representations\nuseful beyond their original training task. We study these questions using synthetic\ndatasets in which the task-relevance of different input features can be controlled\ndirectly. We find that when two features redundantly predict the label, the model\npreferentially represents one, and its preference reflects what was most linearly\ndecodable from the untrained model. Over training, task-relevant features are\nenhanced, and task-irrelevant features are partially suppressed. Interestingly, in\nsome cases, an easier, weakly predictive feature can suppress a more strongly\npredictive, but harder one. Additionally, models trained to recognize both easy\nand hard features learn representations most similar to models that use only the\neasy feature. Further, easy features lead to more consistent representations across\nmodel runs than do hard features. Finally, models have more in common with an\nuntrained model than with models trained on a different task. Our results highlight\nthe complex processes that determine which features a model represents."}}
