{"id": "v33mNDKGeC", "cdate": 1678354615377, "mdate": 1678354615377, "content": {"title": "A Strong Baseline for Batch Imitation Learning", "abstract": "Imitation of expert behaviour is a highly desirable and safe approach to the problem of sequential decision making. We provide an easy-to-implement, novel algorithm for imitation learning under a strict data paradigm, in which the agent must learn solely from data collected a priori. This paradigm allows our algorithm to be used for environments in which safety or cost are of critical concern. Our algorithm requires no additional hyper-parameter tuning beyond any standard batch reinforcement learning (RL) algorithm, making it an ideal baseline for such data-strict regimes. Furthermore, we provide formal sample complexity guarantees for the algorithm in finite Markov Decision Problems. In doing so, we formally demonstrate an unproven claim from Kearns & Singh (1998). On the empirical side, our contribution is twofold. First, we develop a practical, robust and principled evaluation protocol for offline RL methods, making use of only the dataset provided for model selection. This stands in contrast to the vast majority of previous works in offline RL, which tune hyperparameters on the evaluation environment, limiting the practical applicability when deployed in new, cost-critical environments. As such, we establish precedent for the development and fair evaluation of offline RL algorithms. Second, we evaluate our own algorithm on challenging continuous control benchmarks, demonstrating its practical applicability and competitiveness with state-of-the-art performance, despite being a simpler algorithm."}}
{"id": "HYI5cUUjclq", "cdate": 1646077522475, "mdate": null, "content": {"title": "Efficient Inference for Dynamic Topic Modeling with Large Vocabularies", "abstract": "Dynamic topic modeling is a well established tool for capturing the temporal dynamics of the topics of a corpus. In this work, we develop a scalable dynamic topic model by utilizing the correlation among the words in the vocabulary. By correlating previously independent temporal processes for words, our new model allows us to reliably estimate the topic representations containing less frequent words. We develop an amortised variational inference method with self-normalised importance sampling approximation to the word distribution that dramatically reduces the computational complexity and the number of variational parameters in order to handle large vocabularies. With extensive experiments on text datasets, we show that our method significantly outperforms the previous works by modeling word correlations, and it is able to handle real world data with a large vocabulary which could not be processed by previous continuous dynamic topic models. "}}
{"id": "c04LDyEm-N4", "cdate": 1633015334107, "mdate": null, "content": {"title": "Efficient Automated Online Experimentation with Multi-Fidelity", "abstract": "Prominent online experimentation approaches in industry, such as A/B testing, are often not scalable with respect to the number of candidate models. To address this shortcoming, recent work has introduced an automated online experimentation (AOE) scheme that uses a probabilistic model of user behavior to predict online performance of candidate models. While effective, these predictions of online performance may be biased due to various unforeseen circumstances, such as user modelling bias, a shift in data distribution or an incomplete set of features. In this work, we leverage advances from multi-fidelity optimization in order to combine AOE with Bayesian optimization (BO). This mitigates the effect of biased predictions, while still retaining scalability and performance. Furthermore, our approach also allows us to optimally adjust the number of users in a test cell, which is typically kept constant for online experimentation schemes, leading to a more effective allocation of resources. Our synthetic experiments show that our method yields improved performance, when compared to AOE, BO and other baseline approaches."}}
{"id": "xFpkJUMS9te", "cdate": 1633015333608, "mdate": null, "content": {"title": "Contrastive Embedding of Structured Space for Bayesian Optimization", "abstract": "Bayesian optimisation (BO) has been used to search in structured spaces described by a context-free grammar, such as chemical molecules. Previous work has used a probabilistic generative model, such as a variational autoencoder, to learn a mapping for the structured representations into a compact continuous embedding within which BO can take advantage of local proximity and identify good search areas. However, the resultant embedding does not fully capture the structural proximity relations of the input space, which leads to inefficient search. In this paper, we propose to use contrastive learning to learn an alternative embedding. We outline how a subtree replacement strategy can generate structurally similar pairs of samples from the input space for use in contrastive learning. We demonstrate that the resulting embedding captures more of the structural proximity relationships of the input space and improves BO performance when applied to a synthetic arithmetic expression fitting task and a real-world molecule optimisation task."}}
{"id": "HiL-mXQxdTr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Variational Information Distillation for Knowledge Transfer.", "abstract": "Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding hand-crafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer."}}
{"id": "Hkg1AtaVhm", "cdate": 1540835782688, "mdate": null, "content": {"title": "MXFusion: A Modular Deep Probabilistic Programming Library", "abstract": "Modularity is a key feature of deep learning libraries but has not been fully exploited for probabilistic programming. \nWe propose to improve modularity of probabilistic programming language by offering not only plain probabilistic distributions but also sophisticated probabilistic model such as Bayesian non-parametric models as fundamental building blocks. We demonstrate this idea by presenting a modular probabilistic programming language MXFusion, which includes a new type of re-usable building blocks, called probabilistic modules. A probabilistic module consists of a set of random variables with associated probabilistic distributions and dedicated inference methods. Under the framework of variational inference, the pre-specified inference methods of individual probabilistic modules can be transparently used for inference of the whole probabilistic model."}}
{"id": "SyevOObYcm", "cdate": 1539016815513, "mdate": null, "content": {"title": "MXFusion: A Modular Deep Probabilistic Programming Library", "abstract": "Modularity is a key feature of deep learning libraries but has not been fully exploited for probabilistic programming. \nWe propose to improve modularity of probabilistic programming language by offering not only plain probabilistic distributions but also sophisticated probabilistic model such as Bayesian non-parametric models as fundamental building blocks. We demonstrate this idea by presenting a modular probabilistic programming language MXFusion, which includes a new type of re-usable building blocks, called probabilistic modules. A probabilistic module consists of a set of random variables with associated probabilistic distributions and dedicated inference methods. Under the framework of variational inference, the pre-specified inference methods of individual probabilistic modules can be transparently used for inference of the whole probabilistic model."}}
{"id": "B1xnPsA5KX", "cdate": 1538087779842, "mdate": null, "content": {"title": "Modular Deep Probabilistic Programming", "abstract": "Modularity is a key feature of deep learning libraries but has not been fully exploited for probabilistic programming. We propose to improve modularity of probabilistic programming language by offering not only plain probabilistic distributions but also sophisticated probabilistic model such as Bayesian non-parametric models as fundamental building blocks. We demonstrate this idea by presenting a modular probabilistic programming language MXFusion, which includes a new type of re-usable building blocks, called probabilistic modules. A probabilistic module consists of a set of random variables with associated probabilistic distributions and dedicated inference methods. Under the framework of variational inference, the pre-specified inference methods of individual probabilistic modules can be transparently used for inference of the whole probabilistic model. We demonstrate the power and convenience of probabilistic modules in MXFusion with various examples of Gaussian process models, which are evaluated with experiments on real data."}}
{"id": "SJWeShZ_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Structured Variationally Auto-encoded Optimization", "abstract": "We tackle the problem of optimizing a black-box objective function defined over a highly-structured input space. This problem is ubiquitous in science and engineering. In machine learning, inferrin..."}}
{"id": "ByGGxCeAZ", "cdate": 1509117961665, "mdate": null, "content": {"title": "Auto-Differentiating Linear Algebra", "abstract": "Development systems for deep learning, such as Theano, Torch,\nTensorFlow, or MXNet, are easy-to-use tools for creating complex\nneural network models. Since gradient computations are automatically\nbaked in, and execution is mapped to high performance hardware, these\nmodels can be trained end-to-end on large amounts of data. However, it\nis currently not easy to implement many basic machine learning\nprimitives in these systems (such as Gaussian processes, least squares\nestimation, principal components analysis, Kalman smoothing), mainly\nbecause they lack efficient support of linear algebra primitives as\ndifferentiable operators. We detail how a number of matrix\ndecompositions (Cholesky, LQ, symmetric eigen) can be  implemented as\ndifferentiable operators. We have implemented these primitives in\nMXNet, running on CPU and GPU in single and double precision. We\nsketch use cases of these new operators, learning Gaussian process and\nBayesian linear regression models. Our implementation is based on\nBLAS/LAPACK APIs, for which highly tuned implementations are available\non all major CPUs and GPUs. "}}
