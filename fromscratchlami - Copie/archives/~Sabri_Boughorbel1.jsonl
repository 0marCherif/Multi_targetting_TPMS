{"id": "PIdZJ6Y2QMQ", "cdate": 1672531200000, "mdate": 1694329642950, "content": {"title": "LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking", "abstract": "The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language. The framework also features zero- and few-shot learning settings. A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand. The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We plan to open-source the framework for the community (https://github.com/qcri/LLMeBench/). A video demonstrating the framework is available online (https://youtu.be/FkQn4UjYA0s)."}}
{"id": "OygwVPm3Mk", "cdate": 1672531200000, "mdate": 1696141977250, "content": {"title": "Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department", "abstract": "Language modeling have shown impressive progress in generating compelling text with good accuracy and high semantic coherence. An interesting research direction is to augment these powerful models for specific applications using contextual information. In this work, we explore multi-modal language modeling for healthcare applications. We are interested in outcome prediction and patient triage in hospital emergency department based on text information in chief complaints and vital signs recorded at triage. We adapt Perceiver - a modality-agnostic transformer-based model that has shown promising results in several applications. Since vital-sign modality is represented in tabular format, we modified Perceiver position encoding to ensure permutation invariance. We evaluated the multi-modal language model for the task of diagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the experimental analysis, we show that mutli-modality improves the prediction performance compared with models trained solely on text or vital signs. We identified disease categories for which multi-modality leads to performance improvement and show that for these categories, vital signs have added predictive power. By analyzing the cross-attention layer, we show how multi-modality contributes to model predictions. This work gives interesting insights on the development of multi-modal language models for healthcare applications."}}
{"id": "NQ9xRG60KS", "cdate": 1672531200000, "mdate": 1687862725871, "content": {"title": "Benchmarking Arabic AI with Large Language Models", "abstract": "With large Foundation Models (FMs), language technologies (AI in general) are entering a new paradigm: eliminating the need for developing large-scale task-specific datasets and supporting a variety of tasks through set-ups ranging from zero-shot to few-shot learning. However, understanding FMs capabilities requires a systematic benchmarking effort by comparing FMs performance with the state-of-the-art (SOTA) task-specific models. With that goal, past work focused on the English language and included a few efforts with multiple languages. Our study contributes to ongoing research by evaluating FMs performance for standard Arabic NLP and Speech processing, including a range of tasks from sequence tagging to content classification across diverse domains. We start with zero-shot learning using GPT-3.5-turbo, Whisper, and USM, addressing 33 unique tasks using 59 publicly available datasets resulting in 96 test setups. For a few tasks, FMs performs on par or exceeds the performance of the SOTA models but for the majority it under-performs. Given the importance of prompt for the FMs performance, we discuss our prompt strategies in detail and elaborate on our findings. Our future work on Arabic AI will explore few-shot prompting, expand the range of tasks, and investigate additional open-source models."}}
{"id": "5tNgSBO0gM", "cdate": 1672531200000, "mdate": 1700033406235, "content": {"title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic", "abstract": "While significant progress has been made in benchmarking Large Language Models (LLMs) across various tasks, there is a lack of comprehensive evaluation of their abilities in responding to multi-turn instructions in less-commonly tested languages like Arabic. Our paper offers a detailed examination of the proficiency of open LLMs in such scenarios in Arabic. Utilizing a customized Arabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a uniform evaluator for both English and Arabic queries to assess and compare the performance of the LLMs on various open-ended tasks. Our findings reveal variations in model responses on different task categories, e.g., logic vs. literacy, when instructed in English or Arabic. We find that fine-tuned base models using multilingual and multi-turn datasets could be competitive to models trained from scratch on multilingual data. Finally, we hypothesize that an ensemble of small, open LLMs could perform competitively to proprietary LLMs on the benchmark."}}
{"id": "o3JfQ0xB66k", "cdate": 1640995200000, "mdate": 1681654525227, "content": {"title": "Analysis of risk factors progression of preterm delivery using electronic health records", "abstract": ""}}
{"id": "VenYgO3Z9Wq", "cdate": 1640995200000, "mdate": 1681654525250, "content": {"title": "PredictPTB: an interpretable preterm birth prediction model using attention-based recurrent neural networks", "abstract": ""}}
{"id": "9djG6sWPVj2", "cdate": 1617705307175, "mdate": null, "content": {"title": "Fairness in TabNet Model by Disentangled Representation for the Prediction of Hospital No-Show", "abstract": "Patient no-shows is a major burden for health centers leading to loss of revenue, increased waiting time and deteriorated health outcome. Developing machine learning (ML) models for the prediction of no-shows could help addressing this important issue. It is crucial to consider fair ML models for no-show prediction in order to ensure equality of opportunity in accessing healthcare services. In this wo rk, we are interested in developing deep learning models for no-show prediction based on tabular data while ensuring fairness properties. Our baseline model, TabNet, uses on attentive feature transforme rs and has shown promising results for tabular data. We propose Fair-TabNet based on representation learning that disentangles predictive from sensitive components. The model is trained to jointly min imize loss functions on no-shows and sensitive variables while ensuring that the sensitive and prediction representations are orthogonal. In the experimental analysis, we used a hospital dataset of 210, 000 appointments collected in 2019. Our preliminary results show that the proposed Fair-TabNet improves the predictive, fairness performance and convergence speed over TabNet for the task of appointment no-show prediction. The comparison with the state-of-the art models for tabular data shows promising results and could be further improved by a better tuning of hyper-parameters."}}
{"id": "rIdHUikgnjt", "cdate": 1609459200000, "mdate": 1681654525188, "content": {"title": "Fairness in TabNet Model by Disentangled Representation for the Prediction of Hospital No-Show", "abstract": ""}}
{"id": "ePMbRBnNCn", "cdate": 1609459200000, "mdate": 1681654525248, "content": {"title": "DASSI: differential architecture search for splice identification from DNA sequences", "abstract": ""}}
{"id": "0QrunLuE5tn", "cdate": 1609459200000, "mdate": 1681654525289, "content": {"title": "Learning a Shared Model for Motorized Prosthetic Joints to Predict Ankle-Joint Motion", "abstract": ""}}
