{"id": "ORp91sAbzI", "cdate": 1663850266725, "mdate": null, "content": {"title": "Leveraging Unlabeled Data to Track Memorization", "abstract": "Deep neural networks may easily memorize noisy labels present in real-world data, which degrades their ability to generalize. It is therefore important to track and evaluate the robustness of models against noisy label memorization. We propose a metric, called $\\textit{susceptibility}$, to gauge such memorization for neural networks. Susceptibility is simple and easy to compute during training. Moreover, it does not require access to ground-truth labels and it only uses unlabeled data. We empirically show the effectiveness of our metric in tracking memorization on various architectures and datasets and provide theoretical insights into the design of the susceptibility metric. Finally, we show through extensive experiments on datasets with synthetic and real-world label noise that one can utilize susceptibility and the overall training accuracy to distinguish models that maintain a low memorization on the training set and generalize well to unseen clean data. "}}
{"id": "McjGUq1H-mm", "cdate": 1652737459562, "mdate": null, "content": {"title": "Stochastic Second-Order Methods Improve Best-Known Sample Complexity of SGD for Gradient-Dominated Functions", "abstract": "We study the performance of Stochastic Cubic Regularized Newton (SCRN) on a class of functions satisfying gradient dominance property with $1\\le\\alpha\\le2$ which holds in a wide range of applications in machine learning and signal processing. This condition ensures that any first-order stationary point is a global optimum. We prove that the total sample complexity of SCRN in achieving $\\epsilon$-global optimum is $\\mathcal{O}(\\epsilon^{-7/(2\\alpha)+1})$ for $1\\le\\alpha< 3/2$ and $\\mathcal{\\tilde{O}}(\\epsilon^{-2/(\\alpha)})$ for $3/2\\le\\alpha\\le 2$. SCRN improves the best-known sample complexity of stochastic gradient descent. Even under a weak version of gradient dominance property, which is applicable to policy-based reinforcement learning (RL), SCRN  achieves the same improvement over stochastic policy gradient methods. Additionally, we show that the average sample complexity of SCRN can be reduced to ${\\mathcal{O}}(\\epsilon^{-2})$ for $\\alpha=1$ using a variance reduction method with time-varying batch sizes. Experimental results in various RL settings showcase the remarkable performance of SCRN compared to first-order methods."}}
{"id": "WGWzwdjm8mS", "cdate": 1601308347750, "mdate": null, "content": {"title": "Early Stopping by Gradient Disparity", "abstract": "Validation-based early-stopping methods are one of the most popular techniques used to avoid over-training deep neural networks. They require to set aside a reliable unbiased validation set, which can be expensive in applications offering limited amounts of data. In this paper, we propose to use \\emph{gradient disparity}, which we define as the $\\ell_2$ norm distance between the gradient vectors of two batches drawn from the training set. It comes from a probabilistic upper bound on the difference between the classification errors over a given batch, when the network is trained on this batch and when the network is trained on another batch of points sampled from the same dataset. We empirically show that gradient disparity is a very promising early-stopping criterion when data is limited, because it uses all the training samples during training. Furthermore, we show in a wide range of experimental settings that gradient disparity is not only strongly related to the usual generalization error between the training and test sets, but that it is also much more informative about the level of label noise.  "}}
{"id": "Hygq3JrtwS", "cdate": 1569439665559, "mdate": null, "content": {"title": "On the Reflection of Sensitivity in the Generalization Error", "abstract": "Even though recent works have brought some insight into the performance improvement of techniques used in state-of-the-art deep-learning models, more work is needed to understand the generalization properties of over-parameterized deep neural networks. We shed light on this matter by linking the loss function to the output\u2019s sensitivity to its input. We find a rather strong empirical relation between the output sensitivity and the variance in the bias-variance decomposition of the loss function, which hints on using sensitivity as a metric for comparing generalization performance of networks, without requiring labeled data. We find that sensitivity is decreased by applying popular methods which improve the generalization performance of the model, such as (1) using a deep network rather than a wide one, (2) adding convolutional layers to baseline classifiers instead of adding fully connected layers, (3) using batch normalization, dropout and max-pooling, and (4) applying parameter initialization techniques."}}
{"id": "rk-fXhZu-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning Hawkes Processes Under Synchronization Noise", "abstract": "Multivariate Hawkes processes (MHP) are widely used in a variety of fields to model the occurrence of discrete events. Prior work on learning MHPs has only focused on inference in the presence of p..."}}
{"id": "SyWvSD-O-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Coordinate Descent with Bandit Sampling", "abstract": "Coordinate descent methods minimize a cost function by updating a single decision variable (corresponding to one coordinate) at a time. Ideally, we would update the decision variable that yields the largest marginal decrease in the cost function. However, finding this coordinate would require checking all of them, which is not computationally practical. Therefore, we propose a new adaptive method for coordinate descent. First, we define a lower bound on the decrease of the cost function when a coordinate is updated and, instead of calculating this lower bound for all coordinates, we use a multi-armed bandit algorithm to learn which coordinates result in the largest marginal decrease and simultaneously perform coordinate descent. We show that our approach improves the convergence of the coordinate methods both theoretically and experimentally."}}
{"id": "Skbvnjb_ZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Dictionary Learning Based on Sparse Distribution Tomography", "abstract": "We propose a new statistical dictionary learning algorithm for sparse signals that is based on an $\\alpha$-stable innovation model. The parameters of the underlying model\u2014that is, the atoms of the ..."}}
{"id": "B1Z53eWOZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Back To The Source: An Online Approach for Sensor Placement and Source Localization", "abstract": "Source localization, the act of finding the originator of a disease or rumor in a network, has become an important problem in sociology and epidemiology. The localization is done using the infection state and time of infection of a few designated sensor nodes; however, maintaining sensors can be very costly in practice. We propose the first online approach to source localization: We deploy a priori only a small number of sensors (which reveal if they are reached by an infection) and then iteratively choose the best location to place a new sensor in order to localize the source. This approach allows for source localization with a very small number of sensors; moreover, the source can be found while the epidemic is still ongoing. Our method applies to a general network topology and performs well even with random transmission delays."}}
