{"id": "0Kv4Z0SRam", "cdate": 1681852042989, "mdate": null, "content": {"title": "Near-Optimal Statistical Query Lower Bounds for Agnostically Learning Intersections of Halfspaces with Gaussian Marginals", "abstract": "We consider the well-studied problem of learning intersections of halfspaces under the Gaussian distribution in the challenging \\emph{agnostic learning} model. Recent work of Diakonikolas et al. (2021) shows that any Statistical Query (SQ) algorithm for agnostically learning the class of intersections of k halfspaces over \u211dn to constant excess error either must make queries of tolerance at most $n^{\u2212\\tilde\\Omega(\\sqrt{\\log k})}$ or must make $2^{n^{\\Omega(1)}}$ queries. We strengthen this result by improving the tolerance requirement to $n^{\u2212\\tilde\\Omega(\\log k)}$. This lower bound is essentially best possible since an SQ algorithm of Klivans et al. (2008) agnostically learns this class to any constant excess error using $n^{O(\\log k)}$ queries of tolerance $n^{\u2212O(\\log k)}$. We prove two variants of our lower bound, each of which combines ingredients from Diakonikolas et al. (2021) with (an extension of) a different earlier approach for agnostic SQ lower bounds for the Boolean setting due to Dachman-Soled et al. (2014). Our approach also yields lower bounds for agnostically SQ learning the class of \"convex subspace juntas\" (studied by Vempala, 2010) and the class of sets with bounded Gaussian surface area; all of these lower bounds are nearly optimal since they essentially match known upper bounds from Klivans et al. (2008).\n"}}
{"id": "Jv5ZMpQU28E", "cdate": 1672531200000, "mdate": 1681651838194, "content": {"title": "Algorithms and Complexity for Computing Nash Equilibria in Adversarial Team Games", "abstract": ""}}
{"id": "4BPFwvKOvo5", "cdate": 1663850393962, "mdate": null, "content": {"title": "Towards convergence to Nash equilibria in two-team zero-sum games", "abstract": "Contemporary applications of machine learning raise important and overlooked theoretical questions regarding optimization in two-team games. Formally, two-team zero-sum games are defined as multi-player games where players are split into two competing sets of agents, each experiencing a utility identical to that of their teammates and opposite to that of the opposing team. We focus on the solution concept of Nash equilibria and prove $\\textrm{CLS}$-hardness of computing them in this class of games. To further examine the capabilities of online learning algorithms in games with full-information feedback, we propose a benchmark of a simple ---yet nontrivial--- family of such games. These games do not enjoy the properties used to prove convergence for relevant algorithms. In particular, we use a dynamical systems perspective to demonstrate that gradient descent-ascent, its optimistic variant, optimistic multiplicative weights update, and extra gradient fail to converge (even locally) to a Nash equilibrium. On a brighter note, we propose a first-order method that leverages control theory techniques and under some conditions enjoys last-iterate local convergence to a Nash equilibrium. We also believe our proposed method is of independent interest for general min-max optimization."}}
{"id": "mjzm6btqgV", "cdate": 1663850190275, "mdate": null, "content": {"title": "Efficiently Computing Nash Equilibria in Adversarial Team Markov Games", "abstract": "    Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, in light of computational intractability barriers in general-sum games, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications.\n    \n    In this work, we depart from those prior results by investigating infinite-horizon \\emph{adversarial team Markov games}, a natural and well-motivated class of games in which a team of identically-interested players---in the absence of any explicit coordination or communication---is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary $\\epsilon$-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as $1/\\epsilon$.\n    \n    The proposed algorithm is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers."}}
{"id": "QedyATtQ1H", "cdate": 1652737467829, "mdate": null, "content": {"title": "On the convergence of policy gradient methods to Nash equilibria in general stochastic games", "abstract": "Learning in stochastic games is a notoriously difficult problem because, in addition to each other's strategic decisions, the players must also contend with the fact that the game itself evolves over time, possibly in a very complicated manner. Because of this, the convergence properties of popular learning algorithms \u2014 like policy gradient and its variants \u2014 are poorly understood, except in specific classes of games (such as potential or two-player, zero-sum games). In view of this, we examine the long-run behavior of policy gradient methods with respect to Nash equilibrium policies that are second-order stationary (SOS) in a sense similar to the type of sufficiency conditions used in optimization. Our first result is that SOS policies are locally attracting with high probability, and we show that policy gradient trajectories with gradient estimates provided by the REINFORCE algorithm achieve an $\\mathcal{O}(1/\\sqrt{n})$ distance-squared convergence rate if the method's step-size is chosen appropriately. Subsequently, specializing to the class of deterministic Nash policies, we show that this rate can be improved dramatically and, in fact, policy gradient methods converge within a finite number of iterations in that case."}}
{"id": "zJNqte0b-xn", "cdate": 1652737459110, "mdate": null, "content": {"title": "First-Order Algorithms for Min-Max Optimization in Geodesic Metric Spaces", "abstract": "From optimal transport to robust dimensionality reduction, many machine learning applications\ncan be cast into the min-max optimization problems over Riemannian manifolds. Though many\nmin-max algorithms have been analyzed in the Euclidean setting, it has been elusive how these\nresults translate to the Riemannian case. Zhang et al. (2022) have recently identified that geodesic convex\nconcave Riemannian problems admit always Sion\u2019s saddle point solutions. Immediately, an important\nquestion that arises is if a performance gap between the Riemannian and the optimal Euclidean space\nconvex concave algorithms is necessary. Our work is the first to answer the question in the negative:\nWe prove that the Riemannian corrected extragradient (RCEG) method achieves last-iterate at a\nlinear convergence rate at the geodesically strongly convex concave case, matching the euclidean one.\nOur results also extend to the stochastic or non-smooth case where RCEG & Riemanian gradient\nascent descent (RGDA) achieve respectively near-optimal convergence rates up to factors depending\non curvature of the manifold. Finally, we empirically demonstrate the effectiveness of RCEG in\nsolving robust PCA."}}
{"id": "eb9xysj0UP", "cdate": 1640995200000, "mdate": 1683810569008, "content": {"title": "Near-Optimal Statistical Query Lower Bounds for Agnostically Learning Intersections of Halfspaces with Gaussian Marginals", "abstract": "We consider the well-studied problem of learning intersections of halfspaces under the Gaussian distribution in the challenging \\emph{agnostic learning} model. Recent work of Diakonikolas et al. (2021) shows that any Statistical Query (SQ) algorithm for agnostically learning the class of intersections of $k$ halfspaces over $\\mathbb{R}^n$ to constant excess error either must make queries of tolerance at most $n^{-\\tilde{\\Omega}(\\sqrt{\\log k})}$ or must make $2^{n^{\\Omega(1)}}$ queries. We strengthen this result by improving the tolerance requirement to $n^{-\\tilde{\\Omega}(\\log k)}$. This lower bound is essentially best possible since an SQ algorithm of Klivans et al. (2008) agnostically learns this class to any constant excess error using $n^{O(\\log k)}$ queries of tolerance $n^{-O(\\log k)}$. We prove two variants of our lower bound, each of which combines ingredients from Diakonikolas et al. (2021) with (an extension of) a different earlier approach for agnostic SQ lower bounds for the Boolean setting due to Dachman-Soled et al. (2014). Our approach also yields lower bounds for agnostically SQ learning the class of \"convex subspace juntas\" (studied by Vempala, 2010) and the class of sets with bounded Gaussian surface area; all of these lower bounds are nearly optimal since they essentially match known upper bounds from Klivans et al. (2008)."}}
{"id": "Pb2nlkb6m4", "cdate": 1640995200000, "mdate": 1683810568995, "content": {"title": "The Computational Complexity of Multi-player Concave Games and Kakutani Fixed Points", "abstract": "Kakutani's Fixed Point theorem is a fundamental theorem in topology with numerous applications in game theory and economics. Computational formulations of Kakutani exist only in special cases and are too restrictive to be useful in reductions. In this paper, we provide a general computational formulation of Kakutani's Fixed Point Theorem and we prove that it is PPAD-complete. As an application of our theorem we are able to characterize the computational complexity of the following fundamental problems: (1) Concave Games. Introduced by the celebrated works of Debreu and Rosen in the 1950s and 60s, concave $n$-person games have found many important applications in Economics and Game Theory. We characterize the computational complexity of finding an equilibrium in such games. We show that a general formulation of this problem belongs to PPAD, and that finding an equilibrium is PPAD-hard even for a rather restricted games of this kind: strongly-concave utilities that can be expressed as multivariate polynomials of a constant degree with axis aligned box constraints. (2) Walrasian Equilibrium. Using Kakutani's fixed point Arrow and Debreu we resolve an open problem related to Walras's theorem on the existence of price equilibria in general economies. There are many results about the PPAD-hardness of Walrasian equilibria, but the inclusion in PPAD is only known for piecewise linear utilities. We show that the problem with general convex utilities is in PPAD. Along the way we provide a Lipschitz continuous version of Berge's maximum theorem that may be of independent interest."}}
{"id": "NKjVg4cyio", "cdate": 1640995200000, "mdate": 1683810568736, "content": {"title": "First-Order Algorithms for Min-Max Optimization in Geodesic Metric Spaces", "abstract": "From optimal transport to robust dimensionality reduction, a plethora of machine learning applications can be cast into the min-max optimization problems over Riemannian manifolds. Though many min-max algorithms have been analyzed in the Euclidean setting, it has proved elusive to translate these results to the Riemannian case. Zhang et al. [2022] have recently shown that geodesic convex concave Riemannian problems always admit saddle-point solutions. Inspired by this result, we study whether a performance gap between Riemannian and optimal Euclidean space convex-concave algorithms is necessary. We answer this question in the negative-we prove that the Riemannian corrected extragradient (RCEG) method achieves last-iterate convergence at a linear rate in the geodesically strongly-convex-concave case, matching the Euclidean result. Our results also extend to the stochastic or non-smooth case where RCEG and Riemanian gradient ascent descent (RGDA) achieve near-optimal convergence rates up to factors depending on curvature of the manifold."}}
{"id": "M_qrFw2CLNU", "cdate": 1640995200000, "mdate": 1683810568723, "content": {"title": "Efficiently Computing Nash Equilibria in Adversarial Team Markov Games", "abstract": "Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications. In this work, we depart from those prior results by investigating infinite-horizon \\emph{adversarial team Markov games}, a natural and well-motivated class of games in which a team of identically-interested players -- in the absence of any explicit coordination or communication -- is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary $\\epsilon$-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as $1/\\epsilon$. The proposed algorithm is particularly natural and practical, and it is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers. Along the way, we significantly extend an important characterization of optimal policies in adversarial (normal-form) team games due to Von Stengel and Koller (GEB `97)."}}
