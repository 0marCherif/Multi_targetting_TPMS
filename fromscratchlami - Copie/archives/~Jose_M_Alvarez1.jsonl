{"id": "KXzg8lmMtRE", "cdate": 1699722470810, "mdate": 1699722470810, "content": {"title": "Towards Viewpoint Robustness in Bird's Eye View Segmentation", "abstract": "Autonomous vehicles (AV) require that neural networks used for perception be robust to different viewpoints if they are to be deployed across many types of vehicles without the repeated cost of data collection and labeling for each. AV companies typically focus on collecting data from diverse scenarios and locations, but not camera rig configurations, due to cost. As a result, only a small number of rig variations exist across most fleets. In this paper, we study how AV perception models are affected by changes in camera viewpoint and propose a way to scale them across vehicle types without repeated data collection and labeling. Using bird\u2019s eye view (BEV) segmentation as a motivating task, we find through extensive experiments that existing perception models are surprisingly sensitive to changes in camera viewpoint. When trained with data from one camera rig, small changes to pitch, yaw, depth, or height of the camera at inference time lead to large drops in performance. We introduce a technique for novel view synthesis and use it to transform collected data to the viewpoint of target rigs, allowing us to train BEV segmentation models for diverse target rigs without any additional data collection or labeling cost. To analyze the impact of viewpoint changes, we leverage synthetic data to mitigate other gaps (content, ISP, etc). Our approach is then trained on real data and evaluated on synthetic data, enabling evaluation on diverse target rigs. We release all data for use in future work. Our method is able to recover an average of 14.7% of the IoU that is otherwise lost when deploying to new rigs."}}
{"id": "OUkP7gpSCu4", "cdate": 1678307812846, "mdate": 1678307812846, "content": {"title": "Emotion recognition in context", "abstract": "Understanding what a person is experiencing from her frame of reference is essential in our everyday life. For this reason, one can think that machines with this type of ability would interact better with people. However, there are no current systems capable of understanding in detail people\u2019s emotional states. Previous research on computer vision to recognize emotions has mainly focused on analyzing the facial expression, usually classifying it into the 6 basic emotions [11]. However, the context plays an important role in emotion perception, and when the context is incorporated, we can infer more emotional states. In this paper we present the \u201cEmotions in Context Database\u201d (EMOTIC), a dataset of images containing people in context in non-controlled environments. In these images, people are annotated with\n26 emotional categories and also with the continuous dimensions valence, arousal, and dominance [21]. With the EMOTIC dataset, we trained a Convolutional Neural Network model that jointly analyses the person and the whole scene to recognize rich information about emotional states. With this, we show the importance of considering the context for recognizing people\u2019s emotions in images, and provide a benchmark in the task of emotion recognition in visual context."}}
{"id": "AdoLc5e-MER", "cdate": 1667535170362, "mdate": 1667535170362, "content": {"title": "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo", "abstract": "We propose a cost volume-based neural network for depth inference from multi-view images. We demonstrate that building a cost volume pyramid in a coarse-to-fine manner instead of constructing a cost volume at a fixed resolution leads to a compact, lightweight network and allows us inferring high resolution depth maps to achieve better reconstruction results. To this end, we first build a cost volume based on uniform sampling of fronto-parallel planes across the entire depth range at the coarsest resolution of an image. Then, given current depth estimate, we construct new cost volumes iteratively on the pixel wise depth residual to perform depth map refinement. While sharing similar in-sight with Point-MVSNet as predicting and refining depth iteratively, we show that working on cost volume pyramid can lead to a more compact, yet efficient network structure compared with the Point-MVSNet on 3D points. We further provide detailed analyses of the relation between (residual)depth sampling and image resolution, which serves as a principle for building compact cost volume pyramid. Experimental results on benchmark datasets show that our model can perform 6x faster and has similar performance as state-of-the-art methods."}}
{"id": "sRLRS2tnj6", "cdate": 1667351487095, "mdate": 1667351487095, "content": {"title": "Less is more: Towards compact cnns", "abstract": "To attain a favorable performance on large-scale datasets, convolutional neural networks (CNNs) are usually designed to have very high capacity involving millions of parameters. In this work, we aim at optimizing the number of neurons in a network, thus the number of parameters. We show that, by incorporating sparse constraints into the objective function, it is possible to decimate the number of neurons during the training stage. As a result, the number of parameters and the memory footprint of the neural network are also reduced, which is also desirable at the test time. We evaluated our method on several well-known CNN structures including AlexNet, and VGG over different datasets including ImageNet. Extensive experimental results demonstrate that our method leads to compact networks. Taking first fully connected layer as an example, our compact CNN contains only 30% of the original neurons without any degradation of the top-1 classification accuracy."}}
{"id": "R1qL8VgJGdY", "cdate": 1665616560336, "mdate": 1665616560336, "content": {"title": "Self-supervised Learning of Depth Inference for Multi-view Stereo", "abstract": "Recent supervised multi-view depth estimation networks\nhave achieved promising results. Similar to all supervised approaches, these networks require ground-truth data\nduring training. However, collecting a large amount of\nmulti-view depth data is very challenging. Here, we propose a self-supervised learning framework for multi-view\nstereo that exploit pseudo labels from the input data. We\nstart by learning to estimate depth maps as initial pseudo\nlabels under an unsupervised learning framework relying on image reconstruction loss as supervision. We\nthen refine the initial pseudo labels using a carefully designed pipeline leveraging depth information inferred from\na higher resolution image and neighboring views. We use\nthese high-quality pseudo labels as the supervision signal to train the network and improve, iteratively, its performance by self-training. Extensive experiments on the\nDTU dataset show that our proposed self-supervised learning framework outperforms existing unsupervised multiview stereo networks by a large margin and performs\non par compared to the supervised counterpart. Code\nis available at https://github.com/JiayuYANG/\nSelf-supervised-CVP-MVSNet."}}
{"id": "jJgJM9I-P7", "cdate": 1665616280649, "mdate": null, "content": {"title": "Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo", "abstract": "Recent cost volume pyramid based deep neural networks have unlocked the potential of efficiently leveraging\nhigh-resolution images for depth inference from multi-view\nstereo. In general, those approaches assume that the depth\nof each pixel follows a unimodal distribution. Boundary\npixels usually follow a multi-modal distribution as they represent different depths; Therefore, the assumption results in\nan erroneous depth prediction at the coarser level of the\ncost volume pyramid and can not be corrected in the refinement levels leading to wrong depth predictions. In contrast,\nwe propose constructing the cost volume by non-parametric\ndepth distribution modeling to handle pixels with unimodal\nand multi-modal distributions. Our approach outputs multiple depth hypotheses at the coarser level to avoid errors\nin the early stage. As we perform local search around\nthese multiple hypotheses in subsequent levels, our approach does not maintain the rigid depth spatial ordering\nand, therefore, we introduce a sparse cost aggregation network to derive information within each volume. We evaluate\nour approach extensively on two benchmark datasets: DTU\nand Tanks & Temples. Our experimental results show that\nour model outperforms existing methods by a large margin\nand achieves superior performance on boundary regions."}}
{"id": "mKNAOg7CLX", "cdate": 1663850394872, "mdate": null, "content": {"title": "Towards Dynamic Sparsification by Iterative Prune-Grow LookAheads", "abstract": "Model sparsification is a process of removing redundant connections in a neural network, making it more compact and faster. Most pruning methods start with a dense pretrained model, which is computationally intensive to train. Other pruning approaches perform compression at initialization which saves training time, however, at the cost of final accuracy as an unreliable architecture can be selected given weak feature representation. In this work, we re-formulate network sparsification as an exploitation-exploration process during initial training to enable dynamic learning of network sparsification. The exploitation phase assumes architecture stability and trains it to maximize accuracy. Whereas the exploration phase challenges the current architecture with a novel $\\textit{LookAhead}$ step that reactivates pruned parameters, quickly updates them together with existing ones, and reconfigures the sparse architecture with a pruning-growing paradigm. We demonstrate that $\\textit{LookAhead}$ methodology can effectively and efficiently oversee both architecture and performance during training, enabling early pruning with a capability of future recovery to correct previous poor pruning selections. Extensive results on ImageNet and CIFAR datasets show consistent improvements over the prior art by large margins, for varying networks towards both structured and unstructured sparsity. For example, our method surpasses recent work by $+1.3\\%$ top-1 accuracy at the same compression ratio for ResNet50-ImageNet unstructured sparsity. Moreover, our structured sparsity results also improve upon the previous best hardware-aware pruning method by $+0.8\\%$ top-1 accuracy for MobileNet-ImageNet sparsification, offering $+134$ in hardware FPS(im/s), while halving the training cost."}}
{"id": "eWNFhI7EmJ-", "cdate": 1652921461609, "mdate": 1652921461609, "content": {"title": "Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers", "abstract": "Panoptic segmentation involves a combination of joint semantic segmentation and instance segmentation, where image contents are divided into two types: things and stuff. We present Panoptic SegFormer, a general framework for panoptic segmentation with transformers. It contains three innovative components: an efficient deeply-supervised mask decoder, a query decoupling strategy, and an improved post-processing method. We also use Deformable DETR to efficiently process multi-scale features, which is a fast and efficient version of DETR. Specifically, we supervise the attention modules in the mask decoder in a layer-wise manner. This deep supervision strategy lets the attention modules quickly focus on meaningful semantic regions. It improves performance and reduces the number of required training epochs by half compared to Deformable DETR. Our query decoupling strategy decouples the responsibilities of the query set and avoids mutual interference between things and stuff. In addition, our post-processing strategy improves performance without additional costs by jointly considering classification and segmentation qualities to resolve conflicting mask overlaps. Our approach increases the accuracy 6.2\\% PQ over the baseline DETR model. Panoptic SegFormer achieves state-of-the-art results on COCO test-dev with 56.2\\% PQ. It also shows stronger zero-shot robustness over existing methods. The code is released at https://github.com/zhiqi-li/Panoptic-SegFormer."}}
{"id": "oBUwjCxIy9Z", "cdate": 1652921052055, "mdate": 1652921052055, "content": {"title": "Understanding The Robustness in Vision Transformers", "abstract": "Recent studies show that Vision Transformers(ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of the emerging visual grouping in Vision Transformers, which indicates that self-attention may promote robustness through improved mid-level representations. We further propose a family of fully attentional networks (FANs) that strengthen this capability by incorporating an attentional channel processing design. We validate the design comprehensively on various hierarchical backbones. Our model achieves a state of-the-art 87.1% accuracy and 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also demonstrate state-of-the-art accuracy and robustness in two downstream tasks: semantic segmentation and object detection. Code will be available at https://github.com/NVlabs/FAN"}}
{"id": "cUOR-_VsavA", "cdate": 1652737852946, "mdate": null, "content": {"title": "Structural Pruning via Latency-Saliency Knapsack", "abstract": "Structural pruning can simplify network architecture and improve inference speed. We propose Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget on targeting device. For filter importance ranking, HALP leverages latency lookup table to track latency reduction potential and global saliency score to gauge accuracy drop. Both metrics can be evaluated very efficiently during pruning, allowing us to reformulate global structural pruning under a reward maximization problem given target constraint. This makes the problem solvable via our augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off. We examine HALP on both classification and detection tasks, over varying networks, on ImageNet and VOC datasets, on different platforms. In particular, for ResNet-50/-101 pruning on ImageNet, HALP improves network throughput by $1.60\\times$/$1.90\\times$ with $+0.3\\%$/$-0.2\\%$ top-1 accuracy changes, respectively. For SSD pruning on VOC, HALP improves throughput by $1.94\\times$ with only a $0.56$ mAP drop. HALP consistently outperforms prior art, sometimes by large margins. Project page at \\url{https://halp-neurips.github.io/}."}}
