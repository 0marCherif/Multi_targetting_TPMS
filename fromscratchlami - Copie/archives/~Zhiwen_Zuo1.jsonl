{"id": "czQ5UtlMC7x", "cdate": 1672531200000, "mdate": 1700893497060, "content": {"title": "CRFAST: Clip-Based Reference-Guided Facial Image Semantic Transfer", "abstract": "This paper presents a new task for CLIP-based reference-guided facial image semantic transfer: the source facial image is translated to the output image with the high-level semantic attributes from the reference image while maintaining identity preservation. To this end, we employ the powerful generative capability of StyleGAN generator and the rich semantic knowledge of CLIP encoder to accomplish such a task. Additionally, a novel contrastive loss is designed to comprehensively explore the rich semantic information of CLIP for facial semantic concepts. This loss guides the semantic transfer toward desired directions from different perspectives in the pre-defined CLIP space. Besides, a simple yet effective semantic-preserved modulation module is proposed to explicitly map CLIP embeddings of reference image to the latent space. Experiments demonstrate that our approach achieves realistic facial image semantic transfer driven by reference images with various facial semantics."}}
{"id": "RmWukP5JRy", "cdate": 1672531200000, "mdate": 1693887857801, "content": {"title": "TeSTNeRF: Text-Driven 3D Style Transfer via Cross-Modal Learning", "abstract": "Text-driven 3D style transfer aims at stylizing a scene according to the text and generating arbitrary novel views with consistency. Simply combining image/video style transfer methods and novel view synthesis methods results in flickering when changing viewpoints, while existing 3D style transfer methods learn styles from images instead of texts. To address this problem, we for the first time design an efficient text-driven model for 3D style transfer, named TeSTNeRF, to stylize the scene using texts via cross-modal learning: we leverage an advanced text encoder to embed the texts in order to control 3D style transfer and align the input text and output stylized images in latent space. Furthermore, to obtain better visual results, we introduce style supervision, learning feature statistics from style images and utilizing 2D stylization results to rectify abrupt color spill. Extensive experiments demonstrate that TeSTNeRF significantly outperforms existing methods and provides a new way to guide 3D style transfer."}}
{"id": "LlLPpGHNYZ", "cdate": 1672531200000, "mdate": 1693887857800, "content": {"title": "MicroAST: Towards Super-fast Ultra-Resolution Arbitrary Style Transfer", "abstract": "Arbitrary style transfer (AST) transfers arbitrary artistic styles onto content images. Despite the recent rapid progress, existing AST methods are either incapable or too slow to run at ultra-resolutions (e.g., 4K) with limited resources, which heavily hinders their further applications. In this paper, we tackle this dilemma by learning a straightforward and lightweight model, dubbed MicroAST. The key insight is to completely abandon the use of cumbersome pre-trained Deep Convolutional Neural Networks (e.g., VGG) at inference. Instead, we design two micro encoders (content and style encoders) and one micro decoder for style transfer. The content encoder aims at extracting the main structure of the content image. The style encoder, coupled with a modulator, encodes the style image into learnable dual-modulation signals that modulate both intermediate features and convolutional filters of the decoder, thus injecting more sophisticated and flexible style signals to guide the stylizations. In addition, to boost the ability of the style encoder to extract more distinct and representative style signals, we also introduce a new style signal contrastive loss in our model. Compared to the state of the art, our MicroAST not only produces visually superior results but also is 5-73 times smaller and 6-18 times faster, for the first time enabling super-fast (about 0.5 seconds) AST at 4K ultra-resolutions."}}
{"id": "53Vu7cRijh", "cdate": 1672531200000, "mdate": 1693887857802, "content": {"title": "Generative Image Inpainting with Segmentation Confusion Adversarial Training and Contrastive Learning", "abstract": "This paper presents a new adversarial training framework for image inpainting with segmentation confusion adversarial training (SCAT) and contrastive learning. SCAT plays an adversarial game between an inpainting generator and a segmentation network, which provides pixel-level local training signals and can adapt to images with free-form holes. By combining SCAT with standard global adversarial training, the new adversarial training framework exhibits the following three advantages simultaneously: (1) the global consistency of the repaired image, (2) the local fine texture details of the repaired image, and (3) the flexibility of handling images with free-form holes. Moreover, we propose the textural and semantic contrastive learning losses to stabilize and improve our inpainting model's training by exploiting the feature representation space of the discriminator, in which the inpainting images are pulled closer to the ground truth images but pushed farther from the corrupted images. The proposed contrastive losses better guide the repaired images to move from the corrupted image data points to the real image data points in the feature representation space, resulting in more realistic completed images. We conduct extensive experiments on two benchmark datasets, demonstrating our model's effectiveness and superiority both qualitatively and quantitatively."}}
{"id": "rmrjE5zr7Ks", "cdate": 1640995200000, "mdate": 1668134056874, "content": {"title": "Style Fader Generative Adversarial Networks for Style Degree Controllable Artistic Style Transfer", "abstract": ""}}
{"id": "SC64HZQOWZ", "cdate": 1640995200000, "mdate": 1668134056624, "content": {"title": "DivSwapper: Towards Diversified Patch-based Arbitrary Style Transfer", "abstract": ""}}
{"id": "RP8B0P_Mkq", "cdate": 1640995200000, "mdate": 1668134056610, "content": {"title": "Dual distribution matching GAN", "abstract": ""}}
{"id": "BmYHdT0g67e", "cdate": 1640995200000, "mdate": 1668134056589, "content": {"title": "AesUST: Towards Aesthetic-Enhanced Universal Style Transfer", "abstract": ""}}
{"id": "5hwi-Eot7n", "cdate": 1640995200000, "mdate": 1668134056639, "content": {"title": "Texture Reformer: Towards Fast and Universal Interactive Texture Transfer", "abstract": ""}}
{"id": "hm0i-cunzGW", "cdate": 1621629894545, "mdate": null, "content": {"title": "Artistic Style Transfer with Internal-external Learning and Contrastive Learning", "abstract": "Although existing artistic style transfer methods have achieved significant improvement with deep neural networks, they still suffer from artifacts such as disharmonious colors and repetitive patterns. Motivated by this, we propose an internal-external style transfer method with two contrastive losses. Specifically, we utilize internal statistics of a single style image to determine the colors and texture patterns of the stylized image, and in the meantime, we leverage the external information of the large-scale style dataset to learn the human-aware style information, which makes the color distributions and texture patterns in the stylized image more reasonable and harmonious. In addition, we argue that existing style transfer methods only consider the content-to-stylization and style-to-stylization relations, neglecting the stylization-to-stylization relations. To address this issue, we introduce two contrastive losses, which pull the multiple stylization embeddings closer to each other when they share the same content or style, but push far away otherwise. We conduct extensive experiments, showing that our proposed method can not only produce visually more harmonious and satisfying artistic images, but also promote the stability and consistency of rendered video clips."}}
