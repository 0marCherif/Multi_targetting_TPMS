{"id": "8VU4ThTd9i", "cdate": 1621605112298, "mdate": null, "content": {"title": "Stein\u2019s method for multivariate Brownian approximations of sums under dependence", "abstract": "We use Stein\u2019s method to obtain a bound on the distance between scaled p-dimensional random walks and a -dimensional (correlated) Brownian motion. We consider dependence schemes including those in which the summands in scaled sums are weakly dependent and their  components are strongly correlated. As an example application, we prove a functional limit theorem for exceedances in an m-scans process, together with a bound on the rate of convergence. We also find a bound on the rate of convergence of scaled U-statistics to Brownian motion, representing an example of a sum of strongly dependent terms."}}
{"id": "eBgmvYMOmWJ", "cdate": 1621605037310, "mdate": null, "content": {"title": "Functional approximations with Stein's method of exchangeable pairs", "abstract": "We combine the method of exchangeable pairs with Stein's method for functional approximation. As a result, we give a general linearity condition under which an abstract Gaussian approximation theorem for stochastic processes holds. We apply this approach to estimate the distance of a sum of random variables, chosen from an array according to a random permutation, from a Gaussian mixture process. This result lets us prove a functional combinatorial central limit theorem. We also consider a graph-valued process and bound the speed of convergence of the distribution of its rescaled edge counts to a continuous Gaussian process."}}
{"id": "0dDHMJirtY-", "cdate": 1621604998989, "mdate": null, "content": {"title": "Stein's method of exchangeable pairs in multivariate functional approximations", "abstract": "In this paper we develop a framework for multivariate functional approximation by a suitable Gaussian process via an exchangeable pairs coupling that satisfies a suitable approximate linear regression property, thereby building on work by Barbour (1990) and Kasprzak (2020). We demonstrate the applicability of our results by applying it to joint subgraph counts in an Erd\u0151s-Renyi random graph model on the one hand and to vectors of weighted, degenerate U-processes on the other hand. As a concrete instance of the latter class of examples, we provide a bound for the functional approximation of a vector of success runs of different lengths by a suitable Gaussian process which, even in the situation of just a single run, would be outside the scope of the existing theory."}}
{"id": "ks7VvXvRTd", "cdate": 1621604842707, "mdate": null, "content": {"title": "Functional Convergence of U-processes with Size-Dependent Kernels", "abstract": "We consider sequences of U-processes based on symmetric kernels of a fixed order, that possibly depend on the sample size. Our main contribution is the derivation of a set of analytic sufficient conditions, under which the aforementioned U-processes weakly converge to a linear combination of time-changed independent Brownian motions. In view of the underlying symmetric structure, the involved time-changes and weights remarkably depend only on the order of the U-statistic, and have consequently a universal nature. Checking these sufficient conditions requires calculations that have roughly the same complexity as those involved in the computation of fourth moments and cumulants. As such, when applied to the degenerate case, our findings are infinite-dimensional extensions of the central limit theorems (CLTs) proved in de Jong (1990) and D\u00f6bler and Peccati (2017). As important tools in our analysis, we exploit the multidimensional central limit theorems established in D\u00f6bler and Peccati (2019) together with upper bounds on absolute moments of degenerate U-statistics by Ibragimov and Sharakhmetov (2002), and also prove some novel multiplication formulae for degenerate symmetric U-statistics -- allowing for different sample sizes -- that are of independent interest. We provide applications to random geometric graphs and to a class of U-statistics of order two, whose Gaussian fluctuations have been recently studied by Robins et al. (2016), in connection with quadratic estimators in non-parametric models. In particular, our application to random graphs yields a class of new functional central limit theorems for subgraph counting statistics, extending previous findings in the literature. Finally, some connections with invariance principles in changepoint analysis are established."}}
{"id": "YJVfgDd0W3p", "cdate": 1621604682160, "mdate": null, "content": {"title": "Validated Variational Inference via Practical Posterior Error Bounds", "abstract": "Variational inference has become an increasingly attractive fast alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, a major obstacle to the widespread use of variational methods is the lack of post-hoc accuracy measures that are both theoretically justified and computationally efficient. In this paper, we provide rigorous bounds on the error of posterior mean and uncertainty estimates that arise from full-distribution approximations, as in variational inference. Our bounds are widely applicable, as they require only that the approximating and exact posteriors have polynomial moments. Our bounds are also computationally efficient for variational inference because they require only standard values from variational objectives, straightforward analytic calculations, and simple Monte Carlo estimates. We show that our analysis naturally leads to a new and improved workflow for validated variational inference. Finally, we demonstrate the utility of our proposed workflow and error bounds on a robust regression problem and on a real-data example with a widely used multilevel hierarchical model."}}
{"id": "Aew_XxfSOE", "cdate": 1621604606383, "mdate": null, "content": {"title": "Scalable Gaussian Process Inference with Finite-data Mean and Variance Guarantees", "abstract": "Gaussian processes (GPs) offer a flexible class of priors for nonparametric Bayesian regression, but popular GP posterior inference methods are typically prohibitively slow or lack desirable finite-data guarantees on quality. We develop an approach to scalable approximate GP regression with finite-data guarantees on the accuracy of pointwise posterior mean and variance estimates. Our main contribution is a novel objective for approximate inference in the nonparametric setting: the preconditioned Fisher (pF) divergence. We show that unlike the Kullback--Leibler divergence (used in variational inference), the pF divergence bounds the 2-Wasserstein distance, which in turn provides tight bounds the pointwise difference of the mean and variance functions. We demonstrate that, for sparse GP likelihood approximations, we can minimize the pF divergence efficiently. Our experiments show that optimizing the pF divergence has the same computational requirements as variational sparse GPs while providing comparable empirical performance--in addition to our novel finite-data quality guarantees."}}
