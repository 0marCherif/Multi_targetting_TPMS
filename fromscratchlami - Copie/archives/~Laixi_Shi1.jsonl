{"id": "0tUr_jGD5O", "cdate": 1692145301332, "mdate": 1692145301332, "content": {"title": "The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model", "abstract": "This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a  policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. \nDespite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL.\n\nAssuming access to a generative model that draws samples based on the nominal MDP, we characterize the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or $\\chi^2$ divergence. The algorithm studied here is a model-based method called {\\em distributionally robust value iteration}, which is shown to be near-optimal for the full range of uncertainty levels. Somewhat surprisingly, our results uncover that RMDPs are not necessarily easier or harder to learn than standard MDPs. The statistical consequence incurred by the robustness requirement depends heavily on the size and shape of the uncertainty set: in the case w.r.t.~the TV distance, the minimax sample complexity of RMDPs is always smaller than that of standard MDPs; in the case w.r.t.~the $\\chi^2$ divergence, the sample complexity of RMDPs can often far exceed the standard MDP counterpart.  "}}
{"id": "yE1_GpmDOPL", "cdate": 1676827098430, "mdate": null, "content": {"title": "A Trajectory is Worth Three Sentences: Multimodal Transformer for Offline Reinforcement Learning", "abstract": "Transformers hold tremendous promise in solving offline reinforcement learning (RL) by formulating it as a sequence modeling problem inspired by language modeling (LM). Prior works using transformers model a sample (trajectory) of RL as one sequence analogous to a sequence of words (one sentence) in LM, despite the fact that each trajectory includes tokens from three diverse modalities: state, action, and reward, while a sentence contains words only. Rather than taking a modality-agnostic approach which uniformly models the tokens from different modalities as one sequence, we propose a multimodal sequence modeling approach in which a trajectory (one ``sentence'') of three modalities (state, action, reward) is disentangled into three unimodal ones (three ``sentences''). We investigate the correlation of different modalities during sequential decision-making and use the insights to design a multimodal transformer, named Decision Transducer (DTd). DTd outperforms prior art in offline RL on the conducted D4RL benchmarks and enjoys better sample efficiency and algorithm flexibility. Our code is made publicly \\href{https://github.com/berniewang8177/Official-codebase-for-Decision-Transducer/}{here}"}}
{"id": "5DKHY-Ag62E", "cdate": 1663849971907, "mdate": null, "content": {"title": "Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity", "abstract": "This paper concerns the central issues of model robustness and sample efficiency in offline reinforcement learning (RL), which aims to learn to perform decision making from history data without active exploration. Due to uncertainties and variabilities of the environment, it is critical to learn a robust policy---with as few samples as possible---that performs well even when the deployed environment deviates from the nominal one used to collect the history dataset. We consider a distributionally robust formulation of offline RL, focusing on tabular robust Markov decision processes with an uncertainty set specified by the Kullback-Leibler divergence in both finite-horizon and infinite-horizon settings. To combat with sample scarcity, a model-based algorithm that combines distributionally robust value iteration with the principle of pessimism in the face of uncertainty is proposed, by penalizing the robust value estimates with a carefully designed data-driven penalty term. Under a mild and tailored assumption of the history dataset that measures distribution shift without requiring full coverage of the state-action space, we establish the finite-sample complexity of the proposed algorithm, and further show it is almost unimprovable in light of a nearly-matching information-theoretic lower bound up to a polynomial factor of the (effective) horizon length. To the best our knowledge, this provides the first provably near-optimal robust offline RL algorithm that learns under model uncertainty and partial coverage. "}}
{"id": "_cFdPHRLuJ", "cdate": 1652737621848, "mdate": null, "content": {"title": "Curriculum Reinforcement Learning using Optimal Transport via Gradual Domain Adaptation", "abstract": "Curriculum Reinforcement Learning (CRL) aims to create a sequence of tasks, starting from easy ones and gradually learning towards difficult tasks. In this work, we focus on the idea of framing CRL as interpolations between a source (auxiliary) and a target task distribution. Although existing studies have shown the great potential of this idea, it remains unclear how to formally quantify and generate the movement between task distributions. Inspired by the insights from gradual domain adaptation in semi-supervised learning, we create a natural curriculum by breaking down the potentially large task distributional shift in CRL into smaller shifts. We propose GRADIENT which formulates CRL as an optimal transport problem with a tailored distance metric between tasks. Specifically, we generate a sequence of task distributions as a geodesic interpolation between the source and target distributions, which are actually the Wasserstein barycenter. Different from many existing methods, our algorithm considers a task-dependent contextual distance metric and is capable of handling nonparametric distributions in both continuous and discrete context settings. In addition, we theoretically show that GRADIENT enables smooth transfer between subsequent stages in the curriculum under certain conditions. We conduct extensive experiments in locomotion and manipulation tasks and show that our proposed GRADIENT achieves higher performance than baselines in terms of learning efficiency and asymptotic performance."}}
{"id": "yUWnW4LzUmu", "cdate": 1640995200000, "mdate": 1664426077624, "content": {"title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity", "abstract": "Offline or batch reinforcement learning seeks to learn a near-optimal policy using history data without active exploration of the environment. To counter the insufficient coverage and sample scarci..."}}
{"id": "IVoNSkEkrZ8", "cdate": 1640995200000, "mdate": 1664426077581, "content": {"title": "Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity", "abstract": "This paper concerns the central issues of model robustness and sample efficiency in offline reinforcement learning (RL), which aims to learn to perform decision making from history data without active exploration. Due to uncertainties and variabilities of the environment, it is critical to learn a robust policy -- with as few samples as possible -- that performs well even when the deployed environment deviates from the nominal one used to collect the history dataset. We consider a distributionally robust formulation of offline RL, focusing on a tabular non-stationary finite-horizon robust Markov decision process with an uncertainty set specified by the Kullback-Leibler divergence. To combat with sample scarcity, a model-based algorithm that combines distributionally robust value iteration with the principle of pessimism in the face of uncertainty is proposed, by penalizing the robust value estimates with a carefully designed data-driven penalty term. Under a mild and tailored assumption of the history dataset that measures distribution shift without requiring full coverage of the state-action space, we establish the finite-sample complexity of the proposed algorithm, and further show it is almost unimprovable in light of a nearly-matching information-theoretic lower bound up to a polynomial factor of the horizon length. To the best our knowledge, this provides the first provably near-optimal robust offline RL algorithm that learns under model uncertainty and partial coverage."}}
{"id": "5w6VZ6HYHn3", "cdate": 1640995200000, "mdate": 1664426077541, "content": {"title": "Settling the Sample Complexity of Model-Based Offline Reinforcement Learning", "abstract": "This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications. We demonstrate that the model-based (or \"plug-in\") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\\frac{1}{1-\\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\\star}_{\\text{clipped}}$. We prove that model-based offline RL yields $\\varepsilon$-accuracy with a sample complexity of \\[ \\begin{cases} \\frac{H^{4}SC_{\\text{clipped}}^{\\star}}{\\varepsilon^{2}} & (\\text{finite-horizon MDPs}) \\frac{SC_{\\text{clipped}}^{\\star}}{(1-\\gamma)^{3}\\varepsilon^{2}} & (\\text{infinite-horizon MDPs}) \\end{cases} \\] up to log factor, which is minimax optimal for the entire $\\varepsilon$-range. Our algorithms are \"pessimistic\" variants of value iteration with Bernstein-style penalties, and do not require sophisticated variance reduction."}}
{"id": "p2i7X5rW_wo", "cdate": 1631047004597, "mdate": null, "content": {"title": "Fusion-Based Digital Image Correlation Framework for Strain Measurement", "abstract": "We address the problem of enabling two-dimensional digital image correlation (DIC) for strain measurement on large three-dimensional objects with curved surfaces. It is challenging to acquire full-field qualified images of the surface required by DIC due to geometric distortion and the narrow visual field of the surface that a single image can cover. To overcome this issue, we propose an end-to-end DIC framework incorporating the image fusion principle to achieve full-field strain measurement over the curved surface. With a sequence of blurry images as inputs, we first recover sharp images using blind deconvolution, then project recovered sharp images to the curved surface using camera poses estimated by our proposed perspective-n-point (PnP) method called  \\textbf{RRWLM}. Images on the curved surface are stitched and then unfolded for strain analysis using DIC. Numerical experiments are conducted to validate our framework using \\textbf{RRWLM} with comparisons to existing methods. "}}
{"id": "PK36dId_35h", "cdate": 1631046672408, "mdate": null, "content": {"title": "Robust Camera Pose Estimation For Image Stitching", "abstract": "Camera pose estimation plays a crucial role in stitching overlapped images captured by a camera to achieve a broad view of interest. In this paper, we propose a robust camera pose estimation approach to stitching images of a large 3D surface with known geometry. In particular, given a collection of images, we first construct a relative pose matrix estimation of all image pairs from the collection, where each entry of the matrix is calculated by solving a perspective-n-point(PnP) problem over the corresponding pair of images. To continue, we jointly estimate all camera poses by solving an optimization problem that exploits the underlying rank-2 relative pose matrices and the joint sparsity of camera pose errors.  Finally, images are projected onto the 3D surface of interest based on estimated camera poses for the subsequent stitching process.  Numerical experiments demonstrate that our proposed method outperforms existing methods in terms of both camera pose estimation and image stitching quality."}}
{"id": "YA0wIYi-yM3", "cdate": 1621630052593, "mdate": null, "content": {"title": "Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free Reinforcement Learning", "abstract": "Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing  exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved towards characterizing the minimax-optimal regret, which scales on the order of $\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g., $S^6A^4 \\,\\mathrm{poly}(H)$ for existing model-free methods).\n\nTo overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improves --- by at least a factor of $S^5A^3$ --- upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called {\\em reference-advantage decomposition}), the proposed algorithm employs an {\\em early-settled}  reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate exploration-exploitation trade-offs."}}
