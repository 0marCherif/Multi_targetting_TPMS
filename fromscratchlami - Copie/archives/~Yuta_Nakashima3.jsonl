{"id": "OgK9XuZhO6", "cdate": 1668387739223, "mdate": 1668387739223, "content": {"title": "Transferring Domain-Agnostic Knowledge in Video Question Answering", "abstract": "Video question answering (VideoQA) is designed to answer a given question based\non a relevant video clip. The current available large-scale datasets have made it possible\nto formulate VideoQA as the joint understanding of visual and language information.\nHowever, this training procedure is costly and still less competent with human performance. In this paper, we investigate a transfer learning method by the introduction of\ndomain-agnostic knowledge and domain-specific knowledge. First, we develop a novel\ntransfer learning framework, which finetunes the pre-trained model by applying domainagnostic knowledge as the medium. Second, we construct a new VideoQA dataset with\n21,412 human-generated question-answer samples for comparable transfer of knowledge. Our experiments show that: (i) domain-agnostic knowledge is transferable and (ii)\nour proposed transfer learning framework can boost VideoQA performance effectively."}}
{"id": "fbs4D_0f40", "cdate": 1668387602984, "mdate": 1668387602984, "content": {"title": "Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions", "abstract": "To understand movies, humans constantly reason over the dialogues and actions shown in specific scenes and relate them to the overall storyline already seen. Inspired by this behaviour, we design ROLL,\na model for knowledge-based video story question answering that leverages three crucial aspects of movie understanding: dialog comprehension,\nscene reasoning, and storyline recalling. In ROLL, each of these tasks is in\ncharge of extracting rich and diverse information by 1) processing scene\ndialogues, 2) generating unsupervised video scene descriptions, and 3)\nobtaining external knowledge in a weakly supervised fashion. To answer\na given question correctly, the information generated by each inspiredcognitive task is encoded via Transformers and fused through a modality\nweighting mechanism, which balances the information from the different sources. Exhaustive evaluation demonstrates the effectiveness of our\napproach, which yields a new state-of-the-art on two challenging video\nquestion answering datasets: KnowIT VQA and TVQA+."}}
{"id": "kXlZCD9Z7B", "cdate": 1668387558845, "mdate": 1668387558845, "content": {"title": "BERT Representations for Video Question Answering", "abstract": "Visual question answering (VQA) aims at answering\nquestions about the visual content of an image or a video.\nCurrently, most work on VQA is focused on image-based\nquestion answering, and less attention has been paid into\nanswering questions about videos. However, VQA in video\npresents some unique challenges that are worth studying:\nit not only requires to model a sequence of visual features\nover time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a\nsequential modelling technique based on Transformers, to\nencode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments,\nwe exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two\nwell-known video VQA datasets: TVQA and Pororo."}}
{"id": "uWnl1lEXbo5", "cdate": 1668387386996, "mdate": 1668387386996, "content": {"title": "Gender and Racial Bias in Visual Question Answering Datasets", "abstract": "Vision-and-language tasks have increasingly drawn more attention\nas a means to evaluate human-like reasoning in machine learning\nmodels. A popular task in the field is visual question answering\n(VQA), which aims to answer questions about images. However,\nVQA models have been shown to exploit language bias by learning\nthe statistical correlations between questions and answers without looking into the image content: e.g., questions about the color\nof a banana are answered with yellow, even if the banana in the\nimage is green. If societal bias (e.g., sexism, racism, ableism, etc.)\nis present in the training data, this problem may be causing VQA\nmodels to learn harmful stereotypes. For this reason, we investigate\ngender and racial bias in five VQA datasets. In our analysis, we\nfind that the distribution of answers is highly different between\nquestions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that\nspecific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets.\nOur findings suggest that there are dangers associated to using\nVQA datasets without considering and dealing with the potentially\nharmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset\ncollection process."}}
{"id": "9OZZPJbF0Qv", "cdate": 1648712992319, "mdate": 1648712992319, "content": {"title": "Quantifying Societal Bias Amplification in Image Captioning", "abstract": "We study societal bias amplification in image captioning. Image captioning models have been shown to perpetuate gender and racial biases, however, metrics to measure, quantify, and evaluate the societal bias in captions are not yet standardized. We provide a comprehensive study on the strengths and limitations of each metric, and propose LIC, a metric to study captioning bias amplification. We argue that, for image captioning, it is not enough to focus on the correct prediction of the protected attribute, and the whole context should be taken into account. We conduct extensive evaluation on traditional and state-of-the-art image captioning models, and surprisingly find that, by only focusing on the protected attribute prediction, bias mitigation models are unexpectedly amplifying bias.\n"}}
{"id": "U_kE8PYg9Bs", "cdate": 1648712426966, "mdate": 1648712426966, "content": {"title": "Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation", "abstract": "Have you ever looked at a painting and wondered what is the story behind it? This work presents a framework to bring art closer to people by generating comprehensive descriptions of fine-art paintings. Generating informative descriptions for artworks, however, is extremely challenging, as it requires to 1) describe multiple aspects of the image such as its style, content, or composition, and 2) provide background and contextual knowledge about the artist, their influences, or the historical period. To address these challenges, we introduce a multi-topic and knowledgeable art description framework, which modules the generated sentences according to three artistic topics and, additionally, enhances each description with external knowledge. The framework is validated through an exhaustive analysis, both quantitative and qualitative, as well as a comparative human evaluation, demonstrating outstanding results in terms of both topic diversity and information veracity."}}
{"id": "nBTMD4Mae0", "cdate": 1633052271024, "mdate": 1633052271024, "content": {"title": "Depthwise Spatio-Temporal STFT Convolutional Neural Networks for Human Action Recognition", "abstract": "Conventional 3D convolutional neural networks (CNNs) are computationally expensive, memory intensive, prone to overfitting, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose spatio-temporal short term Fourier transform (STFT) blocks, a new class of convolutional blocks that can serve as an alternative to the 3D convolutional layer and its variants in 3D CNNs. An STFT block consists of non-trainable convolution layers that capture spatially and/or temporally local Fourier information using a STFT kernel at multiple low frequency points, followed by a set of trainable linear weights for learning channel correlations. The STFT blocks significantly reduce the space-time complexity in 3D CNNs. In general, they use 3.5 to 4.5 times less parameters and 1.5 to 1.8 times less computational costs when compared to the state-of-the-art methods. Furthermore, their feature learning capabilities are significantly better than the conventional 3D convolutional layer and its variants. Our extensive evaluation on seven action recognition datasets, including Something-something v1 and v2, Jester, Diving-48, Kinetics-400, UCF 101, and HMDB 51, demonstrate that STFT blocks based 3D CNNs achieve on par or even better performance compared to the state-of-the-art methods."}}
{"id": "l3Gxq04YdIh", "cdate": 1633052125528, "mdate": 1633052125528, "content": {"title": "Iternet: Retinal image segmentation utilizing structural redundancy in vessel networks", "abstract": "Retinal vessel segmentation is of great interest for diagnosis of retinal vascular diseases. To further improve the performance of vessel segmentation, we propose IterNet, a new model based on UNet, with the ability to find obscured details of the vessel from the segmented vessel image itself, rather than the raw input image. IterNet consists of multiple iterations of a mini-UNet, which can be 4X deeper than the common UNet. IterNet also adopts the weight-sharing and skip-connection features to facilitate training; therefore, even with such a large architecture, IterNet can still learn from merely 10 20 labeled images, without pre-training or any prior knowledge. IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are the best scores in the literature. The source code is available."}}
{"id": "FJ1KTnl-XhF", "cdate": 1633051926636, "mdate": 1633051926636, "content": {"title": "Learners\u2019 Efficiency Prediction Using Facial Behavior Analysis", "abstract": "In the e-learning context, how much the learner is concentrated and engaged, or the learners\u2019 efficiency, is essential for providing adaptive and flexible materials, timely suggestions, etc., which can lead to efficient learning. In this work, we explore to predict learners\u2019 efficiency with a realistic configuration, in which we use a webcam or a laptop PC\u2019s built-in camera. Specifically, we first provide a feasible definition of the learners\u2019 efficiency, and based on this definition, we predict one\u2019s efficiency from facial behavior. We predict the learners\u2019 efficiency using various convolutional neural networks. Results are discussed using different evaluation metrics."}}
{"id": "x7hZ-pjKBL", "cdate": 1609459200000, "mdate": 1631061259757, "content": {"title": "PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation", "abstract": "We propose a new 2D pose refinement network that learns to predict the human bias in the estimated 2D pose. There are biases in 2D pose estimations that are due to differences between annotations of 2D joint locations based on annotators' perception and those defined by motion capture (MoCap) systems. These biases are crafted into publicly available 2D pose datasets and cannot be removed with existing error reduction approaches. Our proposed pose refinement network allows us to efficiently remove the human bias in the estimated 2D poses and achieve highly accurate multi-view 3D human pose estimation."}}
