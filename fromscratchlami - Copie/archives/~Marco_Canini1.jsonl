{"id": "x4UAxfT2kr", "cdate": 1680076538025, "mdate": null, "content": {"title": "Efficient Sparse Collective Communication and its application to Accelerate Distributed Deep Learning", "abstract": "Efficient collective communication is crucial to parallel-computing applications such as distributed training of large-scale recommendation systems and natural language processing models. Existing collective communication libraries focus on optimizing operations for dense inputs, resulting in transmissions of many zeros when inputs are sparse. This counters current trends that see increasing data sparsity in large models.We propose OmniReduce, an efficient streaming aggregation system that exploits sparsity to maximize effective bandwidth use by sending only non-zero data blocks. We demonstrate that this idea is beneficial and accelerates distributed training by up to 8.2\u00d7. Even at 100 Gbps, OmniReduce delivers 1.4\u20132.9\u00d7 better performance for network-bottlenecked DNNs."}}
{"id": "vc3Tq9X2svD", "cdate": 1672531200000, "mdate": 1680076879355, "content": {"title": "In-Network Aggregation with Transport Transparency for Distributed Training", "abstract": ""}}
{"id": "56zQJJKJyUf", "cdate": 1672531200000, "mdate": 1680076879347, "content": {"title": "FilFL: Accelerating Federated Learning via Client Filtering", "abstract": ""}}
{"id": "H1Orn4M6xt", "cdate": 1649188529073, "mdate": 1649188529073, "content": {"title": "Rethinking gradient sparsification as total error minimization", "abstract": "Gradient compression is a widely-established remedy to tackle the communication bottleneck in distributed training of large deep neural networks (DNNs). Under the error-feedback framework, Top-k sparsification, sometimes with  as little as 0.1% of the gradient size, enables training to the same model quality as the uncompressed case for a similar iteration count. From the optimization perspective, we find that Top-k is the communication-optimal sparsifier given a per-iteration  element budget. We argue that to further the benefits of gradient sparsification, especially for DNNs, a different perspective is necessary\u2014one that moves from per-iteration optimality to consider optimality for the entire training. We identify that the total error\u2014the sum of the compression errors for all iterations\u2014encapsulates sparsification throughout training. Then, we propose a communication complexity model that minimizes the total error under a communication budget for the entire training. We find that the hard-threshold sparsifier, a variant of the Top-k sparsifier with  determined by a constant hard-threshold, is the optimal sparsifier for this model. Motivated by this, we provide convex and non-convex convergence analyses for the hard-threshold sparsifier with error-feedback. We show that hard-threshold has the same asymptotic convergence and linear speedup property as SGD in both the case, and unlike with Top-k sparsifier, has no impact due to data-heterogeneity. Our diverse experiments on various DNNs and a logistic regression model demonstrate that the hard-threshold sparsifier is more communication-efficient than Top-k."}}
{"id": "yoXyYuDvxZn", "cdate": 1640995200000, "mdate": 1680076879396, "content": {"title": "Unlocking the Power of Inline Floating-Point Operations on Programmable Switches", "abstract": ""}}
{"id": "x49FH_IXXD", "cdate": 1640995200000, "mdate": 1680076879395, "content": {"title": "Renaissance: A self-stabilizing distributed SDN control plane using in-band communications", "abstract": ""}}
{"id": "sEz8IS1fda", "cdate": 1640995200000, "mdate": 1680076879365, "content": {"title": "Direct nonlinear acceleration", "abstract": ""}}
{"id": "edVfZpIO9f", "cdate": 1640995200000, "mdate": 1680076879506, "content": {"title": "RDMA is Turing complete, we just did not know it yet!", "abstract": ""}}
{"id": "DsW2qyrUZ0", "cdate": 1640995200000, "mdate": 1680076879505, "content": {"title": "Empirical analysis of federated learning in heterogeneous environments", "abstract": ""}}
{"id": "1xM6TPiy7g", "cdate": 1640995200000, "mdate": 1680076879393, "content": {"title": "SAGE: Software-based Attestation for GPU Execution", "abstract": ""}}
