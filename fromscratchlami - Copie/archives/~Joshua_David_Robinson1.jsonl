{"id": "DFnxtsXZKcF", "cdate": 1675970197991, "mdate": null, "content": {"title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning", "abstract": "Recent work has shown the utility of developing machine learning models that respect the symmetries of eigenvectors. \nThese works promote sign invariance, since for any eigenvector $v$ the negation $-v$ is also an eigenvector.\nIn this work, we demonstrate that sign equivariance is useful for applications such as building orthogonally equivariant models and link prediction. To obtain these benefits, we develop novel sign equivariant neural network architectures. These models are based on our analytic characterization of the sign equivariant polynomials  and thus inherit provable expressiveness properties."}}
{"id": "P4Xr7J9rxp", "cdate": 1674998119530, "mdate": 1674998119530, "content": {"title": "A simple, efficient and scalable contrastive masked autoencoder for learning visual representations", "abstract": "We introduce CAN, a simple, efficient and scalable method for self-supervised\nlearning of visual representations. Our framework is a minimal and conceptually\nclean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the\nnoise prediction approach used in diffusion models. The learning mechanisms are\ncomplementary to one another: contrastive learning shapes the embedding space\nacross a batch of image samples; masked autoencoders focus on reconstruction of\nthe low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image.\nThe combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with 50% of patches in both views\nbeing masked at random, yielding a considerable efficiency improvement over\nprior contrastive learning methods. Extensive empirical studies demonstrate that\nCAN achieves strong downstream performance under both linear and finetuning\nevaluations on transfer learning and robustness tasks. CAN outperforms MAE and\nSimCLR when pre-training on ImageNet, but is especially useful for pre-training\non larger uncurated datasets such as JFT-300M: for linear probe on ImageNet,\nCAN achieves 75.4% compared to 73.4% for SimCLR and 64.1% for MAE. The\nfinetuned performance on ImageNet of our ViT-L model is 86.1%, compared to\n85.5% for SimCLR, and 85.4% for MAE. The overall FLOPs load of SimCLR is\n70% higher than CAN for ViT-L models1"}}
{"id": "Q-UHqMorzil", "cdate": 1663850019071, "mdate": null, "content": {"title": "Sign and Basis Invariant Networks for Spectral Graph Representation Learning", "abstract": "We introduce SignNet and BasisNet---new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if v is an eigenvector then so is -v; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that under certain conditions our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. When used with Laplacian eigenvectors, our networks are provably more expressive than existing spectral methods on graphs; for instance, they subsume all spectral graph convolutions, certain spectral graph invariants, and previously proposed graph positional encodings as special cases. Experiments show that our networks significantly outperform existing baselines on molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes. Our code is available at https://github.com/cptq/SignNet-BasisNet."}}
{"id": "qmV_tOHp7B9", "cdate": 1663849838805, "mdate": null, "content": {"title": "CAN: A simple, efficient and scalable contrastive masked autoencoder framework for learning visual representations", "abstract": "We introduce CAN, a simple, efficient and scalable method for self-supervised learning of visual representations. Our framework is a minimal and conceptually clean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the noise prediction approach used in diffusion models. The learning mechanisms are \\emph{complementary} to one another: contrastive learning shapes the embedding space across a batch of image samples; masked autoencoders focus on reconstruction of the low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image. The combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with $50\\%$ of patches in \\emph{both views} being masked at random, yielding a considerable efficiency improvement over prior contrastive learning methods. Extensive empirical studies on linear evaluation, finetuning, transfer learning, and robustness demonstrate that our approach achieves strong downstream performance. For instance, when pre-training ViT-B encoders on the curated ImageNet dataset, CAN achieves $74.8\\%$ top-1 linear probing accuracy, an absolute improvement of $6.8\\%$ over MAE and $1.3\\%$ over SimCLR with the same architecture and data augmentations. CAN is especially useful for pre-training on larger uncurated datasets such as JFT-300M: the finetuned performance on ImageNet of our ViT-L model is $85.9\\%$, compared to $85.0\\%$ for SimCLR, and $85.4\\%$ for MAE. For linear probe on ImageNet, CAN achieves $75.4\\%$ compared to $71.8\\%$ for SimCLR and $64.1\\%$ for MAE. The overall FLOPs load is $41\\%$ \\emph{lower} than SimCLR\\footnote{Our code will be released at \\url{www.xxx.yyy}.}. "}}
{"id": "39XK7VJ0sKG", "cdate": 1652737814561, "mdate": null, "content": {"title": "Neural Set Function Extensions: Learning with Discrete Functions in High Dimensions", "abstract": "Integrating functions on discrete domains into neural networks is key to developing their capability to reason about discrete objects. But, discrete domains are (1) not naturally amenable to gradient-based optimization, and (2) incompatible with deep learning architectures that rely on representations in high-dimensional vector spaces. In this work, we address both difficulties for set functions, which capture many important discrete problems. First, we develop a framework for extending set functions onto low-dimensional continuous domains, where many extensions are naturally defined. Our framework subsumes many well-known extensions as special cases. Second, to avoid undesirable low-dimensional neural network bottlenecks, we convert low-dimensional extensions into representations in high-dimensional spaces, taking inspiration from the success of semidefinite programs for combinatorial optimization. Empirically, we observe benefits of our extensions for unsupervised neural combinatorial optimization, in particular with high-dimensional representations."}}
{"id": "zSeoDvsDCe", "cdate": 1652737572803, "mdate": null, "content": {"title": "Sign and Basis Invariant Networks for Spectral Graph Representation Learning", "abstract": "We introduce SignNet and BasisNet---new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if v is an eigenvector then so is -v; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. Moreover, when used with Laplacian eigenvectors, our architectures are provably expressive for graph representation learning: they can approximate any spectral graph convolution, can compute spectral invariants that go beyond message passing neural networks, and can provably simulate previously proposed graph positional encodings. Experiments show the strength of our networks for molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes."}}
{"id": "BlM64by6gc", "cdate": 1646223668661, "mdate": null, "content": {"title": "Sign and Basis Invariant Networks for Spectral Graph Representation Learning", "abstract": "Many machine learning tasks involve processing eigenvectors derived from data. Especially valuable are Laplacian eigenvectors, which capture useful structural information about graphs and other geometric objects. However, ambiguities arise when computing eigenvectors: for each eigenvector v, the sign flipped -v is also an eigenvector. More generally, higher dimensional eigenspaces contain infinitely many choices of eigenvector bases. In this work we introduce SignNet and BasisNet --- new neural architectures that are invariant to all requisite symmetries and hence process collections of eigenspaces in a principled manner. Our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the proper invariances. They are also theoretically strong for graph representation learning --- they can provably approximate any spectral graph convolution,  spectral invariants that go beyond message passing neural networks, and other graph positional encodings. Experiments show the strength of our networks for learning spectral graph filters and learning graph positional encodings."}}
{"id": "vNDHZZa-Q92", "cdate": 1632875673217, "mdate": null, "content": {"title": "Neural Extensions: Training Neural Networks with Set Functions", "abstract": "Integrating discrete computational steps into deep learning architectures is an important consideration when learning to reason over discrete items. However, many tasks that involve discrete choices are defined via (combinatorial) set functions, and thereby pose challenges for end-to-end training. In this work, we explore a general framework to construct continuous extensions of such discrete functions that enables training via gradient methods. Our framework includes well-known extensions such as the Lovasz extension of submodular set functions and facilitates the design of novel continuous extensions based on problem-specific considerations, including constraints. We demonstrate the versatility of our framework on tasks ranging from combinatorial optimization to image classification. "}}
{"id": "ud-WYSo9JSL", "cdate": 1621630053691, "mdate": null, "content": {"title": "Can contrastive learning avoid shortcut solutions?", "abstract": "The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via \u201cshortcuts\", i.e., by inadvertently suppressing important predictive features.  We find that feature extraction is influenced by  the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. "}}
{"id": "rpvgwhbHrg5", "cdate": 1609459200000, "mdate": 1645724078511, "content": {"title": "Can contrastive learning avoid shortcut solutions?", "abstract": "The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via \"shortcuts\", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: \\url{https://github.com/joshr17/IFM}."}}
