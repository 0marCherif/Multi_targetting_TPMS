{"id": "kYhAGNlCmtG", "cdate": 1599113704358, "mdate": null, "content": {"title": "Local Guarantees in Graph Cuts and Clustering", "abstract": "Correlation Clustering is an elegant model that captures fundamental graph cut problems such as Min s\u2212t Cut, Multiway Cut, and Multicut, extensively studied in combinatorial optimization. Here, we are given a graph with edges labeled + or \u2212 and the goal is to produce a clustering that agrees with the labels as much as possible: + edges within clusters and \u2212 edges across clusters. The classical approach towards Correlation Clustering (and other graph cut problems) is to optimize a global objective. We depart from this and study local objectives: minimizing the maximum number of disagreements for edges incident on a single node, and the analogous max min agreements objective. This naturally gives rise to a family of basic min-max graph cut problems. A prototypical representative is Min Max s\u2212t Cut: find an s\u2212t cut minimizing the largest number of cut edges incident on any node. We present the following results: (1) an O(n\u203e\u221a)-approximation for the problem of minimizing the maximum total weight of disagreement edges incident on any node (thus providing the first known approximation for the above family of min-max graph cut problems), (2) a remarkably simple 7-approximation for minimizing local disagreements in complete graphs (improving upon the previous best known approximation of 48), and (3) a 1/(2+\u03b5)-approximation for maximizing the minimum total weight of agreement edges incident on any node, hence improving upon the 1/(4+\u03b5)-approximation that follows from the study of approximate pure Nash equilibria in cut and party affiliation games."}}
{"id": "JDXNZ8ohqq", "cdate": 1599113615376, "mdate": null, "content": {"title": "Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process", "abstract": "We consider networks, trained via stochastic gradient descent to minimize \u21132 loss, with the training labels perturbed by independent noise at each iteration. We characterize the behavior of the training dynamics near any parameter vector that achieves zero training error, in terms of an implicit regularization term corresponding to the sum over the data points, of the squared \u21132 norm of the gradient of the model with respect to the parameter vector, evaluated at each data point. This holds for networks of any connectivity, width, depth, and choice of activation function. We interpret this implicit regularization term for three simple settings: matrix sensing, two layer ReLU networks trained on one-dimensional data, and two layer networks with sigmoid activations trained on a single datapoint. For these settings, we show why this new and general implicit regularization effect drives the networks towards \"simple\" models."}}
{"id": "G1r8G_HCQpW", "cdate": 1599113492146, "mdate": null, "content": {"title": "Active Local Learning", "abstract": "In this work we consider active local learning: given a query point x, and active access to an unlabeled training set S, output the prediction h(x) of a near-optimal h\u2208H using significantly fewer labels than would be needed to actually learn h fully. In particular, the number of label queries should be independent of the complexity of H, and the function h should be well-defined, independent of x. This immediately also implies an algorithm for distance estimation: estimating the value opt(H) from many fewer labels than needed to actually learn a near-optimal h\u2208H, by running local learning on a few random query points and computing the average error.\nFor the hypothesis class consisting of functions supported on the interval [0,1] with Lipschitz constant bounded by L, we present an algorithm that makes O((1/\u03f56)log(1/\u03f5)) label queries from an unlabeled pool of O((L/\u03f54)log(1/\u03f5)) samples. It estimates the distance to the best hypothesis in the class to an additive error of \u03f5 for an arbitrary underlying distribution. We further generalize our algorithm to more than one dimensions. We emphasize that the number of labels used is independent of the complexity of the hypothesis class which depends on L. Furthermore, we give an algorithm to locally estimate the values of a near-optimal function at a few query points of interest with number of labels independent of L.\nWe also consider the related problem of approximating the minimum error that can be achieved by the Nadaraya-Watson estimator under a linear diagonal transformation with eigenvalues coming from a small range. For a d-dimensional pointset of size N, our algorithm achieves an additive approximation of \u03f5, makes O\u0303 (d/\u03f52) queries and runs in O\u0303 (d2/\u03f5d+4+dN/\u03f52) time."}}
{"id": "aHr_qpgu8JV", "cdate": 1577836800000, "mdate": null, "content": {"title": "Estimating decision tree learnability with polylogarithmic sample complexity", "abstract": "We show that top-down decision tree learning heuristics are amenable to highly efficient learnability estimation: for monotone target functions, the error of the decision tree hypothesis constructed by these heuristics can be estimated with polylogarithmically many labeled examples, exponentially smaller than the number necessary to run these heuristics, and indeed, exponentially smaller than information-theoretic minimum required to learn a good decision tree. This adds to a small but growing list of fundamental learning algorithms that have been shown to be amenable to learnability estimation. En route to this result, we design and analyze sample-efficient minibatch versions of top-down decision tree learning heuristics and show that they achieve the same provable guarantees as the full-batch versions. We further give \"active local\" versions of these heuristics: given a test point $x^\\star$, we show how the label $T(x^\\star)$ of the decision tree hypothesis $T$ can be computed with polylogarithmically many labeled examples, exponentially smaller than the number necessary to learn $T$."}}
{"id": "VRucmftI1id", "cdate": 1577836800000, "mdate": null, "content": {"title": "Universal guarantees for decision tree induction via a higher-order splitting criterion", "abstract": "We propose a simple extension of top-down decision tree learning heuristics such as ID3, C4.5, and CART. Our algorithm achieves provable guarantees for all target functions $f: \\{-1,1\\}^n \\to \\{-1,1\\}$ with respect to the uniform distribution, circumventing impossibility results showing that existing heuristics fare poorly even for simple target functions. The crux of our extension is a new splitting criterion that takes into account the correlations between $f$ and small subsets of its attributes. The splitting criteria of existing heuristics (e.g. Gini impurity and information gain), in contrast, are based solely on the correlations between $f$ and its individual attributes. Our algorithm satisfies the following guarantee: for all target functions $f : \\{-1,1\\}^n \\to \\{-1,1\\}$, sizes $s\\in \\mathbb{N}$, and error parameters $\\epsilon$, it constructs a decision tree of size $s^{\\tilde{O}((\\log s)^2/\\epsilon^2)}$ that achieves error $\\le O(\\mathsf{opt}_s) + \\epsilon$, where $\\mathsf{opt}_s$ denotes the error of the optimal size $s$ decision tree. A key technical notion that drives our analysis is the noise stability of $f$, a well-studied smoothness measure."}}
{"id": "943AL-5X1Z1", "cdate": 1577836800000, "mdate": null, "content": {"title": "Universal guarantees for decision tree induction via a higher-order splitting criterion", "abstract": "We propose a simple extension of {\\sl top-down decision tree learning heuristics} such as ID3, C4.5, and CART. Our algorithm achieves provable guarantees for all target functions $f: \\{-1,1\\}^n \\to \\{-1,1\\}$ with respect to the uniform distribution, circumventing impossibility results showing that existing heuristics fare poorly even for simple target functions. The crux of our extension is a new splitting criterion that takes into account the correlations between $f$ and {\\sl small subsets} of its attributes. The splitting criteria of existing heuristics (e.g. Gini impurity and information gain), in contrast, are based solely on the correlations between $f$ and its {\\sl individual} attributes. Our algorithm satisfies the following guarantee: for all target functions $f : \\{-1,1\\}^n \\to \\{-1,1\\}$, sizes $s\\in \\N$, and error parameters $\\eps$, it constructs a decision tree of size $s^{\\tilde{O}((\\log s)^2/\\eps^2)}$ that achieves error $\\le O(\\opt_s) + \\eps$, where $\\opt_s$ denotes the error of the optimal size-$s$ decision tree for $f$. A key technical notion that drives our analysis is the {\\sl noise stability} of $f$, a well-studied smoothness measure of $f$."}}
{"id": "0ZBj2hbDoc0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Estimating decision tree learnability with polylogarithmic sample complexity", "abstract": "We show that top-down decision tree learning heuristics (such as ID3, C4.5, and CART) are amenable to highly efficient {\\sl learnability estimation}: for monotone target functions, the error of the decision tree hypothesis constructed by these heuristics can be estimated with {\\sl polylogarithmically} many labeled examples, exponentially smaller than the number necessary to run these heuristics, and indeed, exponentially smaller than information-theoretic minimum required to learn a good decision tree. This adds to a small but growing list of fundamental learning algorithms that have been shown to be amenable to learnability estimation. En route to this result, we design and analyze sample-efficient {\\sl minibatch} versions of top-down decision tree learning heuristics and show that they achieve the same provable guarantees as the full-batch versions. We further give ``active local'' versions of these heuristics: given a test point $x^\\star$, we show how the label $T(x^\\star)$ of the decision tree hypothesis $T$ can be computed with polylogarithmically many labeled examples, exponentially smaller than the number necessary to learn~$T$."}}
{"id": "SJ4GKO-dbS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression", "abstract": "In this paper, we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix $\\mat{A} \\in \\R^{n \\times d}$ where every row $a \\in \\R^d$ has $\\|a\\|_2^2 \\leq L$ and numerical sparsity $\\leq s$, i.e. $\\|a\\|_1^2 / \\|a\\|_2^2 \\leq s$, we provide faster algorithms for these problems for many parameter settings. For top eigenvector computation, when $\\gap &gt; 0$ is the relative gap between the top two eigenvectors of $\\mat{A}^\\top \\mat{A}$ and $r$ is the stable rank of $\\mat{A}$ we obtain a running time of $\\otilde(nd + r(s + \\sqrt{r s}) / \\gap^2)$ improving upon the previous best unaccelerated running time of $O(nd + r d / \\gap^2)$. As $r \\leq d$ and $s \\leq d$ our algorithm everywhere improves or matches the previous bounds for all parameter settings. For regression, when $\\mu &gt; 0$ is the smallest eigenvalue of $\\mat{A}^\\top \\mat{A}$ we obtain a running time of $\\otilde(nd + (nL / \\mu) \\sqrt{s nL / \\mu})$ improving upon the previous best unaccelerated running time of $\\otilde(nd + n L d / \\mu)$. This result expands when regression can be solved in nearly linear time from when $L/\\mu = \\otilde(1)$ to when $L / \\mu = \\otilde(d^{2/3} / (sn)^{1/3})$. Furthermore, we obtain similar improvements even when row norms and numerical sparsities are non-uniform and we show how to achieve even faster running times by accelerating using approximate proximal point \\cite{frostig2015regularizing} / catalyst \\cite{lin2015universal}. Our running times depend only on the size of the input and natural numerical measures of the matrix, i.e. eigenvalues and $\\ell_p$ norms, making progress on a key open problem regarding optimal running times for efficient large-scale learning."}}
