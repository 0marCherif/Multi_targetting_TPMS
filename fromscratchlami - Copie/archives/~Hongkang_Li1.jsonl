{"id": "aqh34_8zeee", "cdate": 1672531200000, "mdate": 1681490805056, "content": {"title": "A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity", "abstract": ""}}
{"id": "nsPUH0mxwD", "cdate": 1672158819711, "mdate": 1672158819711, "content": {"title": "Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling", "abstract": "Graph convolutional networks (GCNs) have recently achieved great empirical success in learning graph-structured data. To address its scalability issue due to the recursive embedding of neighboring features, graph topology sampling has been proposed to reduce the memory and computational cost of training GCNs, and it has achieved comparable test performance to those without topology sampling in many empirical studies. To the best of our knowledge, this paper provides the first theoretical justification of graph topology sampling in training (up to) three-layer GCNs for semi-supervised node classification. We formally characterize some sufficient conditions on graph topology sampling such that GCN training leads to a diminishing generalization error. Moreover, our method tackles the nonconvex interaction of weights across layers, which is under-explored in the existing theoretical analyses of GCNs. This paper characterizes the impact of graph structures and topology sampling on the generalization performance and sample complexity explicitly, and the theoretical findings are also justified through numerical experiments."}}
{"id": "jClGv3Qjhb", "cdate": 1663850194267, "mdate": null, "content": {"title": "A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity", "abstract": "Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, the theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a three-layer ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover,  this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs. "}}
{"id": "ig4E0Y11pX", "cdate": 1663850142579, "mdate": null, "content": {"title": "Theoretical  Characterization of Neural Network Generalization with Group Imbalance", "abstract": "Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high \\textit{average} accuracy could be accompanied by low accuracy in a \\textit{minority} group. Despite various algorithmic efforts to improve the minority group accuracy, a theoretical study of the generalization performance of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in the medium regime and all mean are close to zero,  the learning performance is most desirable in the sense of a small sample complexity, a fast training rate, and a high average and group-level testing accuracy. Moreover, we show that increasing the fraction of the minority group in the training data does not necessarily improve the generalization performance of the minority group.  Our theoretical results are validated on both synthetic and empirical datasets such as CelebA and CIFAR-10 in image classification."}}
{"id": "BmFp-r1ViID", "cdate": 1640995200000, "mdate": 1681490806233, "content": {"title": "Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling", "abstract": ""}}
{"id": "-ZP793s3nV", "cdate": 1640995200000, "mdate": 1681650122297, "content": {"title": "Learning and generalization of one-hidden-layer neural networks, going beyond standard Gaussian data", "abstract": ""}}
{"id": "mLeIhe67Li6", "cdate": 1601308215738, "mdate": null, "content": {"title": "Learning One-hidden-layer Neural Networks on Gaussian Mixture Models with Guaranteed Generalizability", "abstract": "We analyze the learning problem of fully connected neural networks with the sigmoid activation function for binary classification in the teacher-student setup, where the outputs are assumed to be generated by a ground-truth teacher neural network with unknown parameters, and the learning objective is to estimate the teacher network model by minimizing a non-convex cross-entropy risk function of the training data over a student neural network. This paper analyzes a  general and practical scenario that the input features follow a Gaussian mixture model of a finite number of Gaussian distributions of various mean and variance. We propose a gradient descent algorithm with a tensor initialization approach and show that our algorithm converges linearly to a critical point that has a  diminishing distance to the ground-truth model with guaranteed generalizability. We characterize the required number of samples for successful convergence, referred to as the sample complexity,  as a function of the parameters of the Gaussian mixture model. We prove analytically that when any mean or variance in the mixture model is large, or when all variances are close to zero, the sample complexity increases, and the convergence slows down, indicating a more challenging learning problem. Although focusing on one-hidden-layer neural networks, to the best of our knowledge, this paper provides the first explicit characterization of the impact of the parameters of the input distributions on the sample complexity and learning rate."}}
