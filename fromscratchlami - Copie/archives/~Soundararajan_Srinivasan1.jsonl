{"id": "u-X6jggg7vq", "cdate": 1640995200000, "mdate": 1686947978492, "content": {"title": "On Optimizing Interventions in Shared Autonomy", "abstract": "Shared autonomy refers to approaches for enabling an autonomous agent to collaborate with a human with the aim of improving human performance. However, besides improving performance, it may often also be beneficial that the agent concurrently accounts for preserving the user\u2019s experience or satisfaction of collaboration. In order to address this additional goal, we examine approaches for improving the user experience by constraining the number of interventions by the autonomous agent. We propose two model-free reinforcement learning methods that can account for both hard and soft constraints on the number of interventions. We show that not only does our method outperform the existing baseline, but also eliminates the need to manually tune a black-box hyperparameter for controlling the level of assistance. We also provide an in-depth analysis of intervention scenarios in order to further illuminate system understanding."}}
{"id": "iS0Vla1fg2", "cdate": 1640995200000, "mdate": 1686947978494, "content": {"title": "SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content", "abstract": "We present SLATE, a sequence labeling approach for extracting tasks from free-form content such as digitally handwritten (or \"inked\") notes on a virtual whiteboard. Our approach allows us to create a single, low-latency model to simultaneously perform sentence segmentation and classification of these sentences into task/non-task sentences. SLATE greatly outperforms a baseline two-model (sentence segmentation followed by classification model) approach, achieving a task F1 score of 84.4%, a sentence segmentation (boundary similarity) score of 88.4% and three times lower latency compared to the baseline. Furthermore, we provide insights into tackling challenges of performing NLP on the inking domain. We release both our code and dataset for this novel task."}}
{"id": "Hnb4ccJDOiH", "cdate": 1640995200000, "mdate": 1686947978490, "content": {"title": "Topic Segmentation in the Wild: Towards Segmentation of Semi-structured & Unstructured Chats", "abstract": "Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured texts. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin."}}
{"id": "5jgPofLCPQ", "cdate": 1640995200000, "mdate": 1686947978499, "content": {"title": "SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content", "abstract": "Apurva Gandhi, Ryan Serrao, Biyi Fang, Gilbert Antonius, Jenna Hong, Tra My Nguyen, Sheng Yi, Ehi Nosakhare, Irene Shaffer, Soundararajan Srinivasan. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track. 2022."}}
{"id": "2DbtA-vNBh", "cdate": 1640995200000, "mdate": 1686947978498, "content": {"title": "Towards Optimal Resource Allocation for Big Data Analytics", "abstract": ""}}
{"id": "Z3jHPBHcpr3", "cdate": 1609459200000, "mdate": 1686947978493, "content": {"title": "On Optimizing Interventions in Shared Autonomy", "abstract": "Shared autonomy refers to approaches for enabling an autonomous agent to collaborate with a human with the aim of improving human performance. However, besides improving performance, it may often also be beneficial that the agent concurrently accounts for preserving the user's experience or satisfaction of collaboration. In order to address this additional goal, we examine approaches for improving the user experience by constraining the number of interventions by the autonomous agent. We propose two model-free reinforcement learning methods that can account for both hard and soft constraints on the number of interventions. We show that not only does our method outperform the existing baseline, but also eliminates the need to manually tune a black-box hyperparameter for controlling the level of assistance. We also provide an in-depth analysis of intervention scenarios in order to further illuminate system understanding."}}
{"id": "WGEJJdmpDaT", "cdate": 1609459200000, "mdate": 1686947978500, "content": {"title": "Optimal Resource Allocation for Serverless Queries", "abstract": "Optimizing resource allocation for analytical workloads is vital for reducing costs of cloud-data services. At the same time, it is incredibly hard for users to allocate resources per query in serverless processing systems, and they frequently misallocate by orders of magnitude. Unfortunately, prior work focused on predicting peak allocation while ignoring aggressive trade-offs between resource allocation and run-time. Additionally, these methods fail to predict allocation for queries that have not been observed in the past. In this paper, we tackle both these problems. We introduce a system for optimal resource allocation that can predict performance with aggressive trade-offs, for both new and past observed queries. We introduce the notion of a performance characteristic curve (PCC) as a parameterized representation that can compactly capture the relationship between resources and performance. To tackle training data sparsity, we introduce a novel data augmentation technique to efficiently synthesize the entire PCC using a single run of the query. Lastly, we demonstrate the advantages of a constrained loss function coupled with GNNs, over traditional ML methods, for capturing the domain specific behavior through an extensive experimental evaluation over SCOPE big data workloads at Microsoft."}}
{"id": "qNm8n5mbpR", "cdate": 1577836800000, "mdate": 1686947978491, "content": {"title": "Seagull: An Infrastructure for Load Prediction and Optimized Resource Allocation", "abstract": ""}}
{"id": "fpHVEZt0w_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Seagull: An Infrastructure for Load Prediction and Optimized Resource Allocation", "abstract": "Microsoft Azure is dedicated to guarantee high quality of service to its customers, in particular, during periods of high customer activity, while controlling cost. We employ a Data Science (DS) driven solution to predict user load and leverage these predictions to optimize resource allocation. To this end, we built the Seagull infrastructure that processes per-server telemetry, validates the data, trains and deploys ML models. The models are used to predict customer load per server (24h into the future), and optimize service operations. Seagull continually re-evaluates accuracy of predictions, fallback to previously known good models and triggers alerts as appropriate. We deployed this infrastructure in production for PostgreSQL and MySQL servers across all Azure regions, and applied it to the problem of scheduling server backups during low-load time. This minimizes interference with user-induced load and improves customer experience."}}
{"id": "kgiEVw1mH3l", "cdate": 1420070400000, "mdate": 1686947978493, "content": {"title": "A scalable solution for group feature selection", "abstract": "In many applications, we may want to build a classifier with high confidence, while reducing the number of features. We consider the case where features are assigned to predefined groups and cannot be removed individually. An additional and important constraint is that the datasets may be very large and may not fit in memory. We use logistic regression with group penalty, which results in sparse solutions at the group level. In our implementation, we apply L-BFGS to approximate the quadratic loss function of logistic regression and use Block Co-ordinate Descent to solve for each group. Our contributions can be summarized as follows: (1) we discuss different scalable approaches, depending on characteristics of the dataset, such as, large number of data points or large number of features or large number of groups; (2) for datasets with large number of data points and few groups of features, we identify the bottlenecks for scalability; (3) we present Spark solutions in Python and discuss the advantages of our solution over alternate solutions; (4) we present the experiments and results on synthetic data and real data from manufacturing applications."}}
