{"id": "qVXaYcOPaH", "cdate": 1672531200000, "mdate": 1682318410299, "content": {"title": "A General Algorithm for Solving Rank-one Matrix Sensing", "abstract": "Matrix sensing has many real-world applications in science and engineering, such as system control, distance embedding, and computer vision. The goal of matrix sensing is to recover a matrix $A_\\star \\in \\mathbb{R}^{n \\times n}$, based on a sequence of measurements $(u_i,b_i) \\in \\mathbb{R}^{n} \\times \\mathbb{R}$ such that $u_i^\\top A_\\star u_i = b_i$. Previous work [ZJD15] focused on the scenario where matrix $A_{\\star}$ has a small rank, e.g. rank-$k$. Their analysis heavily relies on the RIP assumption, making it unclear how to generalize to high-rank matrices. In this paper, we relax that rank-$k$ assumption and solve a much more general matrix sensing problem. Given an accuracy parameter $\\delta \\in (0,1)$, we can compute $A \\in \\mathbb{R}^{n \\times n}$ in $\\widetilde{O}(m^{3/2} n^2 \\delta^{-1} )$, such that $ |u_i^\\top A u_i - b_i| \\leq \\delta$ for all $i \\in [m]$. We design an efficient algorithm with provable convergence guarantees using stochastic gradient descent for this problem."}}
{"id": "unHZBUna-r", "cdate": 1640995200000, "mdate": 1683235215071, "content": {"title": "Differentially Oblivious Relational Database Operators", "abstract": ""}}
{"id": "iLKVtC4DuW", "cdate": 1640995200000, "mdate": 1683235214408, "content": {"title": "Adaptive and Dynamic Multi-Resolution Hashing for Pairwise Summations", "abstract": "In this paper, we propose Adam-Hash: an adaptive and dynamic multi-resolution hashing data-structure for fast pairwise summation estimation. Given a data-set X \u2282 \u211d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> , a binary function f : \u211d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> \u00d7 \u211d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> \u2192 \u211d, and a point y \u2208 \u211d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> , the Pairwise Summation Estimate $PS{E_X}(y): = \\frac{1}{{\\left| X \\right|}}\\sum\\nolimits_{x \\in X} {f(x,y)} $. For any given data-set X, we need to design a data-structure such that given any query point y \u2208 \u211d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> , the data-structure approximately estimates PSE <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">X</inf> (y) in time that is sub-linear in |X|. Prior works on this problem have focused exclusively on the case where the data-set is static, and the queries are independent. In this paper, we design a hashing-based PSE data-structure which works for the more practical dynamic setting in which insertions, deletions, and replacements of points are allowed. Moreover, our proposed Adam-Hash is also robust to adaptive PSE queries, where an adversary can choose query q <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">j</inf> \u2208 \u211d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> depending on the output from previous queries q <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</inf> , q <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</inf> , \u2026, q <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">j\u20131</inf> ."}}
{"id": "GdS1iso5bE", "cdate": 1640995200000, "mdate": 1683235214489, "content": {"title": "A Sublinear Adversarial Training Algorithm", "abstract": "Adversarial training is a widely used strategy for making neural networks resistant to adversarial perturbations. For a neural network of width $m$, $n$ input training data in $d$ dimension, it takes $\\Omega(mnd)$ time cost per training iteration for the forward and backward computation. In this paper we analyze the convergence guarantee of adversarial training procedure on a two-layer neural network with shifted ReLU activation, and shows that only $o(m)$ neurons will be activated for each input data per iteration. Furthermore, we develop an algorithm for adversarial training with time cost $o(m n d)$ per iteration by applying half-space reporting data structure."}}
{"id": "0Lwgy9V8bLX", "cdate": 1640995200000, "mdate": 1683235214321, "content": {"title": "Adore: Differentially Oblivious Relational Database Operators", "abstract": "There has been a recent effort in applying differential privacy on memory access patterns to enhance data privacy. This is called differential obliviousness. Differential obliviousness is a promising direction because it provides a principled trade-off between performance and desired level of privacy. To date, it is still an open question whether differential obliviousness can speed up database processing with respect to full obliviousness. In this paper, we present the design and implementation of three new major database operators: selection with projection, grouping with aggregation, and foreign key join. We prove that they satisfy the notion of differential obliviousness. Our differentially oblivious operators have reduced cache complexity, runtime complexity, and output size compared to their state-of-the-art fully oblivious counterparts. We also demonstrate that our implementation of these differentially oblivious operators can outperform their state-of-the-art fully oblivious counterparts by up to $7.4\\times$."}}
{"id": "-IqszFQVH35", "cdate": 1640995200000, "mdate": 1683235214770, "content": {"title": "Adaptive and Dynamic Multi-Resolution Hashing for Pairwise Summations", "abstract": "In this paper, we propose Adam-Hash: an adaptive and dynamic multi-resolution hashing data-structure for fast pairwise summation estimation. Given a data-set $X \\subset \\mathbb{R}^d$, a binary function $f:\\mathbb{R}^d\\times \\mathbb{R}^d\\to \\mathbb{R}$, and a point $y \\in \\mathbb{R}^d$, the Pairwise Summation Estimate $\\mathrm{PSE}_X(y) := \\frac{1}{|X|} \\sum_{x \\in X} f(x,y)$. For any given data-set $X$, we need to design a data-structure such that given any query point $y \\in \\mathbb{R}^d$, the data-structure approximately estimates $\\mathrm{PSE}_X(y)$ in time that is sub-linear in $|X|$. Prior works on this problem have focused exclusively on the case where the data-set is static, and the queries are independent. In this paper, we design a hashing-based PSE data-structure which works for the more practical \\textit{dynamic} setting in which insertions, deletions, and replacements of points are allowed. Moreover, our proposed Adam-Hash is also robust to adaptive PSE queries, where an adversary can choose query $q_j \\in \\mathbb{R}^d$ depending on the output from previous queries $q_1, q_2, \\dots, q_{j-1}$."}}
{"id": "g_z2NDwdYFP", "cdate": 1609459200000, "mdate": 1683265051416, "content": {"title": "ZEN: Efficient Zero-Knowledge Proofs for Neural Networks", "abstract": ""}}
{"id": "HDzGXiILFW", "cdate": 1546300800000, "mdate": 1683265051377, "content": {"title": "Training Deep Nets with Progressive Batch Normalization on Multi-GPUs", "abstract": "Batch normalization (BN) enables us to train various deep neural networks faster. However, the training accuracy will be significantly influenced with the decrease of input mini-batch size. To increase the model accuracy, a global mean and variance among all the input batch can be used, nevertheless communication across all devices is required in each BN layer, which reduces the training speed greatly. To address this problem, we propose progressive batch normalization, which can achieve a good balance between model accuracy and efficiency in multiple-GPU training. Experimental results show that our algorithm can obtain significant performance improvement over traditional BN without data synchronization across GPUs, achieving up to 18.4% improvement on training DeepLab for semantic segmentation task across 8 GPUs."}}
