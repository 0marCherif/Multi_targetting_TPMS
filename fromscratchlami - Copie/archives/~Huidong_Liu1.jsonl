{"id": "zcBlm0Boms", "cdate": 1668976054397, "mdate": 1668976054397, "content": {"title": "Self Pre-training with Masked Autoencoders for Medical Image Analysis", "abstract": "Masked Autoencoder (MAE) has recently been shown to be\neffective in pre-training Vision Transformers (ViT) for natural image\nanalysis. By performing the pretext task of reconstructing the original\nimage from only partial observations, the encoder, which is a ViT, is encouraged to aggregate contextual information to infer content in masked\nimage regions. We believe that this context aggregation ability is also\nessential to the medical image domain where each anatomical structure\nis functionally and mechanically connected to other structures and regions. However, there is no ImageNet-scale medical image dataset for pretraining. Thus, in this paper, we investigate a self pre-training paradigm\nwith MAE for medical images, i.e., models are pre-trained on the same\ntarget dataset. To validate the MAE self pre-training, we consider three\ndiverse medical image tasks including chest X-ray disease classification,\nCT abdomen multi-organ segmentation and MRI brain tumor segmentation. It turns out MAE self pre-training benefits all the tasks markedly.\nSpecifically, the mAUC on lung disease classification is increased by 9.4%.\nThe average DSC on brain tumor segmentation is improved from 77.4%\nto 78.9%. Most interestingly, on the small-scale multi-organ segmentation dataset (N=30), the average DSC improves from 78.8% to 83.5%\nand the HD95 is reduced by 60%, indicating its effectiveness in limited\ndata scenarios. The segmentation and classification results reveal the\npromising potential of MAE self pre-training for medical image analysis."}}
{"id": "0mB8e2MCva", "cdate": 1668975937441, "mdate": 1668975937441, "content": {"title": "Chest radiograph disentanglement for covid-19 outcome prediction", "abstract": "Chest radiographs (CXRs) are often the primary front-line diagnostic imaging modality. Pulmonary diseases manifest as characteristic changes in lung tissue texture rather than anatomical structure. Hence, we expect that studying changes in only lung tissue texture without the influence of possible structure variations would be advantageous for downstream prognostic and predictive modeling tasks. In this paper, we propose a generative framework, Lung Swapping Autoencoder (LSAE), that learns a factorized representation of a CXR, to \\textit{disentangle} the tissue texture representation from the anatomic structure representation.\nUpon learning the disentaglement, we leverage LSAE in two applications. 1) After adapting the texture encoder in LSAE to thoracic disease classification task on the large-scale ChestX-ray14 database (N=112,120), we achieve a competitive result (mAUC: 79.0$\\%$) with unsupervised pre-training.\nMoreover, when compared with Inception v3 on our multi-institutional\nCOVID-19 dataset, COVOC (N=340), for a COVID-19 outcome prediction task (estimating need for ventilation), the texture encoder achieves 13$\\%$ less error with a 77$\\%$ smaller model size, further demonstrating the efficacy of texture representation for lung diseases. 2) We leverage the LSAE for data augmentation, by generating hybrid lung images with textures and labels from the COVOC training data and lung structures from ChestX-ray14. This further improves ventilation outcome prediction on COVOC.\n"}}
{"id": "-D5TVtzt3fP", "cdate": 1663849878660, "mdate": null, "content": {"title": "Sparse Tokens for Dense Prediction - The Medical Image Segmentation Case", "abstract": "Can we use sparse tokens for dense prediction, e.g., segmentation? Although token sparsification has been applied to Vision Transformers (ViT) for acceleration on classification tasks, it is still unknown how to perform segmentation from sparse tokens. To this end, we reformulate segmentation as a sparse encoding -> token completion -> dense decoding (SCD) pipeline.\nWe first show empirically that naively applying existing approaches from classification token pruning and masked image modeling (MIM) \nleads to failure and training inefficiency. This is caused by inappropriate sampling algorithms and the low quality of the restored dense features. In this paper, we propose Soft-topK Token Pruning (STP) and Multi-layer Token Assembly (MTA) to address the above problems.\nParticularly, in the sparse encoding stage, STP predicts token-wise importance scores with a lightweight sub-network and samples topK-scored tokens. The intractable gradients of topK are approximated through a continuous perturbed score distribution.\nIn the token completion stage, MTA restores a full token sequence by assembling both sparse output tokens and pruned intermediate tokens from multiple layers. Compared to MIM which fills the pruned positions with mask tokens, MTA produces more informative representations allowing more accurate segmentation. The last dense decoding stage is compatible with decoders of existing segmentation frameworks, e.g., UNETR. Experiments show SCD pipelines equipped with our STP and MTA are much faster than baselines without token sparsification in both training (up to 120% higher throughput) and inference (up to 60.6% higher throughput) while maintaining segmentation quality."}}
{"id": "g8A7N5B5t0", "cdate": 1640995200000, "mdate": 1668715144611, "content": {"title": "Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs", "abstract": "Well-labeled datasets of chest radiographs (CXRs) are difficult to acquire due to the high cost of annotation. Thus, it is desirable to learn a robust and transferable representation in an unsupervised manner to benefit tasks that lack labeled data. Unlike natural images, medical images have their own domain prior; e.g., we observe that many pulmonary diseases, such as the COVID-19, manifest as changes in the lung tissue texture rather than the anatomical structure. Therefore, we hypothesize that studying only the texture without the influence of structure variations would be advantageous for downstream prognostic and predictive modeling tasks. In this paper, we propose a generative framework, the Lung Swapping Autoencoder (LSAE), that learns factorized representations of a CXR to disentangle the texture factor from the structure factor. Specifically, by adversarial training, the LSAE is optimized to generate a hybrid image that preserves the lung shape in one image but inherits the lung texture of another. To demonstrate the effectiveness of the disentangled texture representation, we evaluate the texture encoder $Enc^t$ in LSAE on ChestX-ray14 (N=112,120), and our own multi-institutional COVID-19 outcome prediction dataset, COVOC (N=340 (Subset-1) + 53 (Subset-2)). On both datasets, we reach or surpass the state-of-the-art by finetuning $Enc^t$ in LSAE that is 77% smaller than a baseline Inception v3. Additionally, in semi-and-self supervised settings with a similar model budget, $Enc^t$ in LSAE is also competitive with the state-of-the-art MoCo. By \"re-mixing\" the texture and shape factors, we generate meaningful hybrid images that can augment the training set. This data augmentation method can further improve COVOC prediction performance. The improvement is consistent even when we directly evaluate the Subset-1 trained model on Subset-2 without any fine-tuning."}}
{"id": "JtmU9c5m8o", "cdate": 1640995200000, "mdate": 1668715144204, "content": {"title": "Deep learning for survival analysis in breast cancer with whole slide image data", "abstract": "Whole slide tissue images contain detailed data on the sub-cellular structure of cancer. Quantitative analyses of this data can lead to novel biomarkers for better cancer diagnosis and prognosis and can improve our understanding of cancer mechanisms. Such analyses are challenging to execute because of the sizes and complexity of whole slide image data and relatively limited volume of training data for machine learning methods."}}
{"id": "D6NUJ6ozbRs", "cdate": 1640995200000, "mdate": 1668715144667, "content": {"title": "HaloAE: An HaloNet based Local Transformer Auto-Encoder for Anomaly Detection and Localization", "abstract": "Unsupervised anomaly detection and localization is a crucial task as it is impossible to collect and label all possible anomalies. Many studies have emphasized the importance of integrating local and global information to achieve accurate segmentation of anomalies. To this end, there has been a growing interest in Transformer, which allows modeling long-range content interactions. However, global interactions through self attention are generally too expensive for most image scales. In this study, we introduce HaloAE, the first auto-encoder based on a local 2D version of Transformer with HaloNet. With HaloAE, we have created a hybrid model that combines convolution and local 2D block-wise self-attention layers and jointly performs anomaly detection and segmentation through a single model. We achieved competitive results on the MVTec dataset, suggesting that vision models incorporating Transformer could benefit from a local computation of the self-attention operation, and pave the way for other applications."}}
{"id": "Crnq8wqlT_b", "cdate": 1640995200000, "mdate": 1668715144699, "content": {"title": "Self Pre-training with Masked Autoencoders for Medical Image Analysis", "abstract": "Masked Autoencoder (MAE) has recently been shown to be effective in pre-training Vision Transformers (ViT) for natural image analysis. By reconstructing full images from partially masked inputs, a ViT encoder aggregates contextual information to infer masked image regions. We believe that this context aggregation ability is particularly essential to the medical image domain where each anatomical structure is functionally and mechanically connected to other structures and regions. Because there is no ImageNet-scale medical image dataset for pre-training, we investigate a self pre-training paradigm with MAE for medical image analysis tasks. Our method pre-trains a ViT on the training set of the target data instead of another dataset. Thus, self pre-training can benefit more scenarios where pre-training data is hard to acquire. Our experimental results show that MAE self pre-training markedly improves diverse medical image tasks including chest X-ray disease classification, abdominal CT multi-organ segmentation, and MRI brain tumor segmentation. Code is available at https://github.com/cvlab-stonybrook/SelfMedMAE"}}
{"id": "OOaY4GZIJ7", "cdate": 1632875700189, "mdate": null, "content": {"title": "Efficient Semi-Discrete Optimal Transport Using the Maximum Relative Error between Distributions", "abstract": "Semi-Discrete Optimal Transport (SDOT) transforms a continuous distribution to a discrete distribution. However, existing SDOT algorithms for high dimensional distributions have two limitations. 1) It is difficult to evaluate the quality of the transport maps produced by SDOT algorithms, because computing a high-dimensional Wasserstein distance for SDOT is intractable and 2) The transport map cannot guarantee that all target points have the corresponding source points that are mapped to them. To address these limitations, we introduce the Maximum Relative Error (\\texttt{MRE}) between the target distribution and the transported distribution computed by an SDOT map. If the \\texttt{MRE} is smaller than 1, then every target point is guaranteed to have an area in the source distribution that is mapped to it. We propose a statistical method to compute the lower and upper bounds of the \\texttt{MRE} given a confidence threshold and a precision. The gap between the lower bound and the upper bound approaches 0 as the number of samples goes to infinity. We present an efficient Epoch Gradient Descent algorithm for SDOT (SDOT-EGD) that computes the learning rate, number of iterations, and number of epochs in order to guarantee an arbitrarily small \\texttt{MRE} in expectation. Experiments on both low and high-dimensional data show that SDOT-EGD is much faster and converges much better than state-of-the-art SDOT algorithms. We also show our method's potential to improve GAN training by avoiding the oscillation caused by randomly changing the association between noise and the real images."}}
{"id": "qa7ljA-Ep2", "cdate": 1609459200000, "mdate": 1668715144484, "content": {"title": "Chest Radiograph Disentanglement for COVID-19 Outcome Prediction", "abstract": "Chest radiographs (CXRs) are often the primary front-line diagnostic imaging modality. Pulmonary diseases manifest as characteristic changes in lung tissue texture rather than anatomical structure. Hence, we expect that studying changes in only lung tissue texture without the influence of possible structure variations would be advantageous for downstream prognostic and predictive modeling tasks. In this paper, we propose a generative framework, Lung Swapping Autoencoder (LSAE), that learns a factorized representation of a CXR to disentangle the tissue texture representation from the anatomic structure representation. Upon learning the disentanglement, we leverage LSAE in two applications. 1) After adapting the texture encoder in LSAE to a thoracic disease classification task on the large-scale ChestX-ray14 database (N\u00a0=\u00a0112,120), we achieve a competitive result (mAUC: 79.0 $$\\%$$ ) with unsupervised pre-training. Moreover, when compared with Inception v3 on our multi-institutional COVID-19 dataset, COVOC (N\u00a0=\u00a0340), for a COVID-19 outcome prediction task (estimating need for ventilation), the texture encoder achieves 13 $$\\%$$ less error with a 77 $$\\%$$ smaller model size, further demonstrating the efficacy of texture representation for lung diseases. 2) We leverage the LSAE for data augmentation by generating hybrid lung images with textures and labels from the COVOC training data and lung structures from ChestX-ray14. This further improves ventilation outcome prediction on COVOC. The code is available here: https://github.com/cvlab-stonybrook/LSAE ."}}
{"id": "bV4iou8XHog", "cdate": 1609459200000, "mdate": 1668715144666, "content": {"title": "CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification", "abstract": "Modern Web systems such as social media and e-commerce contain rich contents expressed in images and text. Leveraging information from multi-modalities can improve the performance of machine learning tasks such as classification and recommendation. In this paper, we propose the Cross-Modality Attention Contrastive Language-Image Pre-training (CMA-CLIP), a new framework which unifies two types of cross-modality attentions, sequence-wise attention and modality-wise attention, to effectively fuse information from image and text pairs. The sequence-wise attention enables the framework to capture the fine-grained relationship between image patches and text tokens, while the modality-wise attention weighs each modality by its relevance to the downstream tasks. In addition, by adding task specific modality-wise attentions and multilayer perceptrons, our proposed framework is capable of performing multi-task classification with multi-modalities. We conduct experiments on a Major Retail Website Product Attribute (MRWPA) dataset and two public datasets, Food101 and Fashion-Gen. The results show that CMA-CLIP outperforms the pre-trained and fine-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA dataset for multi-task classification. It also surpasses the state-of-the-art method on Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on Food101 Dataset. Through detailed ablation studies, we further demonstrate the effectiveness of both cross-modality attention modules and our method's robustness against noise in image and text inputs, which is a common challenge in practice."}}
