{"id": "ksRqtF0fS2", "cdate": 1694056343287, "mdate": 1694056343287, "content": {"title": "Skill-Based Few-Shot Selection for In-Context Learning", "abstract": "In-Context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based representations for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across four cross-domain semantic parsing tasks and four backbone models show that Skill-KNN significantly outperforms existing methods."}}
{"id": "ZW5eBi_eyV", "cdate": 1694056149043, "mdate": 1694056149043, "content": {"title": "How Do In-Context Examples Affect Compositional Generalization?", "abstract": "Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm."}}
{"id": "12-I0-XoAt6", "cdate": 1684002603187, "mdate": null, "content": {"title": "Making Language Models Better Reasoners with Step-Aware Verifier", "abstract": "Large language models such as GPT-3\nand PaLM have shown remarkable perfor-\nmance in few-shot learning. However, they\nstill struggle with reasoning tasks such as\nthe arithmetic benchmark GSM8K. Recent\nadvances deliberately guide the language\nmodel to generate a chain of reasoning steps\nbefore producing the final answer, success-\nfully boosting the GSM8K benchmark from\n17.9% to 58.1% in terms of problem solv-\ning rate. In this paper, we propose a new\napproach, DIVERSE (Diverse Verifier on\nReasoning Step), to further advance their rea-\nsoning capability. DIVERSE first explores\ndifferent prompts to enhance the diversity in\nreasoning paths. Second, DIVERSE intro-\nduces a verifier to distinguish good answers\nfrom bad answers for a better weighted vot-\ning. Finally, DIVERSE verifies the correct-\nness of each single step rather than all the\nsteps in a whole. We conduct extensive ex-\nperiments using the latest language model\ncode-davinci-002 and demonstrate that DI-\nVERSE can achieve new state-of-the-art per-\nformance on six out of eight reasoning bench-\nmarks (e.g., GSM8K 74.4% \u2192 83.2%), out-\nperforming the PaLM model with 540B pa-\nrameters"}}
{"id": "QB1dMPEXau5", "cdate": 1663850110849, "mdate": null, "content": {"title": "Does Deep Learning Learn to Abstract? A Systematic Probing Framework", "abstract": "Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a \"memorize-then-abstract\" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales."}}
{"id": "ktrw68Cmu9c", "cdate": 1663849915462, "mdate": null, "content": {"title": "CodeT:  Code Generation with Generated Tests", "abstract": "The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results."}}
{"id": "O50443AsCP", "cdate": 1632875515311, "mdate": null, "content": {"title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor", "abstract": "Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes the improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com/microsoft/Table-Pretraining."}}
