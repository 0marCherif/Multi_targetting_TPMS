{"id": "iyc9TjZYAcN", "cdate": 1684346154201, "mdate": 1684346154201, "content": {"title": "Split Conformal Prediction for Dependent Data", "abstract": "Split conformal prediction is a popular tool to obtain predictive intervals from general statistical algorithms, with few assumptions beyond data exchangeability. We show that coverage guarantees from split CP can be extended to dependent processes, such as the class of stationary \u03b2-mixing processes, by adding a small coverage penalty. In particular, we show that the empirical coverage bounds for some \u03b2-mixing processes match the order of the bounds under exchangeability. The framework introduced also extends to non-stationary processes and to other CP methods, and experiments corroborate our split CP\u2019s coverage guarantees under dependent data."}}
{"id": "PwAchHxtSA", "cdate": 1682899200000, "mdate": 1684169418099, "content": {"title": "Sample average approximation with heavier tails I: non-asymptotic bounds with weak assumptions and stochastic constraints", "abstract": "We derive new and improved non-asymptotic deviation inequalities for the sample average approximation (SAA) of an optimization problem. Our results give strong error probability bounds that are \u201csub-Gaussian\u201d even when the randomness of the problem is fairly heavy tailed. Additionally, we obtain good (often optimal) dependence on the sample size and geometrical parameters of the problem. Finally, we allow for random constraints on the SAA and unbounded feasible sets, which also do not seem to have been considered before in the non-asymptotic literature. Our proofs combine different ideas of potential independent interest: an adaptation of Talagrand\u2019s \u201cgeneric chaining\u201d bound for sub-Gaussian processes; \u201clocalization\u201d ideas from the Statistical Learning literature; and the use of standard conditions in Optimization (metric regularity, Slater-type conditions) to control fluctuations of the feasible set."}}
{"id": "ApmrIW65n4", "cdate": 1682899200000, "mdate": 1684169417895, "content": {"title": "Sample average approximation with heavier tails II: localization in stochastic convex optimization and persistence results for the Lasso", "abstract": "Localization\u201d has proven to be a valuable tool in the Statistical Learning literature as it allows sharp risk bounds in terms of the problem geometry. Localized bounds seem to be much less exploited in the stochastic optimization literature. In addition, there is an obvious interest in both communities in obtaining risk bounds that require weak moment assumptions or \u201cheavier-tails\u201d. In this work we use a localization toolbox to derive risk bounds in two specific applications. The first is in portfolio risk minimization with conditional value-at-risk constraints. We consider a setting where, among all assets with high returns, there is a portion of dimension g, unknown to the investor, that has significant less risk than the other remaining portion. Our rates for the SAA problem show that \u201crisk inflation\u201d, caused by a multiplicative factor, affects the statistical rate only via a term proportional to g. As the \u201cnormalized risk\u201d increases, the contribution in the rate from the extrinsic dimension diminishes while the dependence on g is kept fixed. Localization is a key tool to show this property. As a second application of our localization toolbox, we obtain sharp oracle inequalities for least-squares estimators with a Lasso-type constraint under weak moment assumptions. One main consequence of these inequalities is to obtain persistence, as posed by Greenshtein and Ritov, with covariates having heavier tails. This gives improvements in prior work of Bartlett, Mendelson and Neeman."}}
{"id": "f3lan7edupj", "cdate": 1640995200000, "mdate": 1684169418098, "content": {"title": "Subtractive random forests", "abstract": "Motivated by online recommendation systems, we study a family of random forests. The vertices of the forest are labeled by integers. Each non-positive integer $i\\le 0$ is the root of a tree. Vertices labeled by positive integers $n \\ge 1$ are attached sequentially such that the parent of vertex $n$ is $n-Z_n$, where the $Z_n$ are i.i.d.\\ random variables taking values in $\\mathbb N$. We study several characteristics of the resulting random forest. In particular, we establish bounds for the expected tree sizes, the number of trees in the forest, the number of leaves, the maximum degree, and the height of the forest. We show that for all distributions of the $Z_n$, the forest contains at most one infinite tree, almost surely. If ${\\mathbb E} Z_n < \\infty$, then there is a unique infinite tree and the total size of the remaining trees is finite, with finite expected value if ${\\mathbb E}Z_n^2 < \\infty$. If ${\\mathbb E} Z_n = \\infty$ then almost surely all trees are finite."}}
{"id": "JATSTBQ9T-F", "cdate": 1640995200000, "mdate": 1684169418100, "content": {"title": "ExactBoost: Directly Boosting the Margin in Combinatorial and Non-decomposable Metrics", "abstract": "Many classification algorithms require the use of surrogate losses when the intended loss function is combinatorial or non-decomposable. This paper introduces a fast and exact stagewise optimization algorithm, dubbed ExactBoost, that boosts stumps to the actual loss function. By developing a novel extension of margin theory to the non-decomposable setting, it is possible to provably bound the generalization error of ExactBoost for many important metrics with different levels of non-decomposability. Through extensive examples, it is shown that such theoretical guarantees translate to competitive empirical performance. In particular, when used as an ensembler, ExactBoost is able to significantly outperform other surrogate-based and exact algorithms available."}}
{"id": "TEik9Yu_Rf", "cdate": 1623624208305, "mdate": 1623624208305, "content": {"title": "On The Convergence to Equilibrium of Kac's Random Walk on Matrices", "abstract": "We consider Kac's random walk on n-dimensional rotation matrices, where each step is a random rotation in the plane generated by two randomly picked coordinates. We show that this process converges to the Haar measure on SO(n) in the L\u00b2 transportation cost (Wasserstein) metric in O(n\u00b2 lnn) steps. We also prove that our bound is at most a O (In n) factor away from optimal. Previous bounds, due to Diaconis/Saloff-Coste and Pak/Sidenko, had extra powers of n and held only for L\u00b9 transportation cost. Our proof method includes a general result of independent interest, akin to the path coupling method of Bubley and Dyer. Suppose that P is a Markov chain on a Polish length space (M, d) and that for all x, y \u2208 M with d(x, y) \u226a 1 there is a coupling (X, Y) of one step of P from x and y (resp.) that contracts distances by a (\u03be + o(1)) factor on average. Then the map \u03bc \u21a6 \u03bc P is \u03be-contracting in the transportation cost metric."}}
{"id": "LVUXzZCj1se", "cdate": 1623624103033, "mdate": 1623624103033, "content": {"title": "Estimating graph parameters via random walks", "abstract": "An algorithm observes the trajectories of random walks over an unknown graph G, starting from the same vertex x, as well as the degrees along the trajectories. For all finite connected graphs, one can estimate the number of edges m up to a bounded factor in O(t3/4relm/d\u2212\u2212\u2212\u2212\u221a) steps, where trel is the relaxation time of the lazy random walk on G and d is the minimum degree in G. Alternatively, m can be estimated in O(tunif+t5/6reln\u2212\u2212\u221a), where n is the number of vertices and tunif is the uniform mixing time on G. The number of vertices n can then be estimated up to a bounded factor in an additional O(tunifmn) steps. Our algorithms are based on counting the number of intersections of random walk paths X,Y, i.e. the number of pairs (t,s) such that Xt=Ys. This improves on previous estimates which only consider collisions (i.e.~times t with Xt=Yt). We also show that the complexity of our algorithms is optimal, even when restricting to graphs with a prescribed relaxation time. Finally, we show that, given either m or the mixing time of G, we can compute the \"other parameter\" with a self-stopping algorithm."}}
{"id": "JcbHP7nox8s", "cdate": 1623623776236, "mdate": 1623623776236, "content": {"title": "Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality", "abstract": "A number of statistical estimation problems can be addressed by semidefinite programs (SDP). While SDPs are solvable in polynomial time using interior point methods, in practice generic SDP solvers do not scale well to high-dimensional problems. In order to cope with this problem, Burer and Monteiro proposed a non-convex rank-constrained formulation, which has good performance in practice but is still poorly understood theoretically. In this paper we study the rank-constrained version of SDPs arising in MaxCut and in Z2 and SO(d) synchronization problems. We establish a Grothendieck-type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum. We use this structural information to prove that SDPs can be solved within a known accuracy, by applying the Riemannian trust-region method to this non-convex problem, while constraining the rank to be of order one. For the MaxCut problem, our inequality implies that any local maximizer of the rank-constrained SDP provides a (1\u22121/(k\u22121))\u00d70.878 approximation of the MaxCut, when the rank is fixed to k. We then apply our results to data matrices generated according to the Gaussian Z2 synchronization problem, and the two-groups stochastic block model with large bounded degree. We prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods."}}
{"id": "SW3w_jIvGu", "cdate": 1546300800000, "mdate": 1627654751522, "content": {"title": "Random walks on graphs: new bounds on hitting, meeting, coalescing and returning", "abstract": "We prove new results on lazy random walks on finite graphs. To start, we obtain new estimates on return probabilities Pt(x, x) and the maximum expected hitting time thit, both in terms of the relaxation time. We also prove a discretetime version of the first-named author's \u201cMeeting time lemma\u201d that bounds the probability of a random walk hitting a deterministic trajectory in terms of hitting times of static vertices. The meeting time result is then used to bound the expected full coalescence time of multiple random walks over a graph. This last theorem is a discretetime version of a result by the first-named author, which had been previously conjectured by Aldous and Fill. Our bounds improve on recent results by Lyons and Oveis-Gharan; Kanade et al; and (in certain regimes) Cooper et al."}}
{"id": "FibLaZGAqzH", "cdate": 1546300800000, "mdate": 1627654751707, "content": {"title": "Variance-Based Extragradient Methods with Line Search for Stochastic Variational Inequalities", "abstract": "In this paper, we propose dynamic sampled stochastic approximated (DS-SA) extragradient methods for stochastic variational inequalities (SVIs) that are robust with respect to an unknown Lipschitz constant $L$. We propose, to the best of our knowledge, the first provably convergent robust SA method with variance reduction, either for SVIs or stochastic optimization, assuming just an unbiased stochastic oracle within a large sample regime. This widens the applicability and improves, up to constants, the desired efficient acceleration of previous variance reduction methods, all of which still assume knowledge of $L$ (and, hence, are not robust against its estimate). Precisely, compared to the iteration and oracle complexities of $\\mathcal{O}(\\epsilon^{-2})$ of previous robust methods with a small stepsize policy, our robust method uses a DS-SA line search scheme obtaining the faster iteration complexity of $\\mathcal{O}(\\epsilon^{-1})$ with oracle complexity of $(\\ln L)\\mathcal{O}(d\\epsilon^{-2})$ (up to log factors on $\\epsilon^{-1}$) for a $d$-dimensional space. This matches, up to constants, the sample complexity of the sample average approximation estimator which does not assume additional problem information (such as $L$). Differently from previous robust methods for ill-conditioned problems, we allow an unbounded feasible set and an oracle with multiplicative noise (MN) whose variance is not necessarily uniformly bounded. These properties are appreciated in our complexity estimates which depend only on $L$ and local variances or fourth moments at solutions. The robustness and variance reduction properties of our DS-SA line search scheme come at the expense of nonmartingale-like dependencies (NMDs) due to the needed inner statistical estimation of a lower bound for $L$. In order to handle an NMD and an MN, our proofs rely on a novel iterative localization argument based on empirical process theory."}}
