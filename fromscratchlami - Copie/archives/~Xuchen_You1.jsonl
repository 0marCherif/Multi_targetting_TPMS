{"id": "RFvA2v0h09W", "cdate": 1672531200000, "mdate": 1681662724355, "content": {"title": "Analyzing Convergence in Quantum Neural Networks: Deviations from Neural Tangent Kernels", "abstract": "A quantum neural network (QNN) is a parameterized mapping efficiently implementable on near-term Noisy Intermediate-Scale Quantum (NISQ) computers. It can be used for supervised learning when combined with classical gradient-based optimizers. Despite the existing empirical and theoretical investigations, the convergence of QNN training is not fully understood. Inspired by the success of the neural tangent kernels (NTKs) in probing into the dynamics of classical neural networks, a recent line of works proposes to study over-parameterized QNNs by examining a quantum version of tangent kernels. In this work, we study the dynamics of QNNs and show that contrary to popular belief it is qualitatively different from that of any kernel regression: due to the unitarity of quantum operations, there is a non-negligible deviation from the tangent kernel regression derived at the random initialization. As a result of the deviation, we prove the at-most sublinear convergence for QNNs with Pauli measurements, which is beyond the explanatory power of any kernel regression dynamics. We then present the actual dynamics of QNNs in the limit of over-parameterization. The new dynamics capture the change of convergence rate during training and implies that the range of measurements is crucial to the fast QNN convergence."}}
{"id": "xxfE6oFdya", "cdate": 1640995200000, "mdate": 1671549044232, "content": {"title": "A Convergence Theory for Over-parameterized Variational Quantum Eigensolvers", "abstract": "The Variational Quantum Eigensolver (VQE) is a promising candidate for quantum applications on near-term Noisy Intermediate-Scale Quantum (NISQ) computers. Despite a lot of empirical studies and recent progress in theoretical understanding of VQE's optimization landscape, the convergence for optimizing VQE is far less understood. We provide the first rigorous analysis of the convergence of VQEs in the over-parameterization regime. By connecting the training dynamics with the Riemannian Gradient Flow on the unit-sphere, we establish a threshold on the sufficient number of parameters for efficient convergence, which depends polynomially on the system dimension and the spectral ratio, a property of the problem Hamiltonian, and could be resilient to gradient noise to some extent. We further illustrate that this overparameterization threshold could be vastly reduced for specific VQE instances by establishing an ansatz-dependent threshold paralleling our main result. We showcase that our ansatz-dependent threshold could serve as a proxy of the trainability of different VQE ansatzes without performing empirical experiments, which hence leads to a principled way of evaluating ansatz design. Finally, we conclude with a comprehensive empirical study that supports our theoretical findings."}}
{"id": "PMxTNTKtoeG", "cdate": 1620773576845, "mdate": null, "content": {"title": "On Second-Order Group Influence Functions for Black-Box Predictions", "abstract": "With the rapid adoption of machine learning systems in sensitive applications, there is an increasing need to make black-box models explainable. Often we want to identify an influential group of training samples in a particular test prediction for a given machine learning model. Existing influence functions tackle this problem by using first-order approximations of the effect of removing a sample from the training set on model parameters. To compute the influence of a group of training samples (rather than an individual point) in model predictions, the change in optimal model parameters after removing that group from the training set can be large. Thus, in such cases, the first-order approximation can be loose. In this paper, we address this issue and propose second-order influence functions for identifying influential groups in test-time predictions. For linear models, across different sizes and types of groups, we show that using the proposed second-order influence function improves the correlation between the computed influence values and the ground truth ones. We also show that second-order influence functions could be used with optimization techniques to improve the selection of the most influential group for a test-sample."}}
{"id": "ztx3_5DjNp1", "cdate": 1609459200000, "mdate": 1681662724362, "content": {"title": "ICCAD Special Session Paper: Quantum Variational Methods for Quantum Applications", "abstract": "Quantum Variational Methods are promising near-term applications of quantum machines, not only because of their potential advantages in solving certain computational tasks and understanding quantum physics but also because of their feasibility on near-term quantum machines. However, many challenges remain in order to unleash the full potential of quantum variational methods, especially in the design of efficient training methods for each domain-specific quantum variational ansatzes. This paper proposes a theory-guided principle in order to tackle the training issue of quantum variational methods and highlights some successful examples."}}
{"id": "pCGoGjbCLg1", "cdate": 1609459200000, "mdate": 1671549044790, "content": {"title": "Exponentially Many Local Minima in Quantum Neural Networks", "abstract": "Quantum Neural Networks (QNNs), or the so-called variational quantum circuits, are important quantum applications both because of their similar promises as classical neural networks and because of ..."}}
{"id": "ktb2eQAUz1", "cdate": 1609459200000, "mdate": 1681662724360, "content": {"title": "Exponentially Many Local Minima in Quantum Neural Networks", "abstract": "Quantum Neural Networks (QNNs), or the so-called variational quantum circuits, are important quantum applications both because of their similar promises as classical neural networks and because of the feasibility of their implementation on near-term intermediate-size noisy quantum machines (NISQ). However, the training task of QNNs is challenging and much less understood. We conduct a quantitative investigation on the landscape of loss functions of QNNs and identify a class of simple yet extremely hard QNN instances for training. Specifically, we show for typical under-parameterized QNNs, there exists a dataset that induces a loss function with the number of spurious local minima depending exponentially on the number of parameters. Moreover, we show the optimality of our construction by providing an almost matching upper bound on such dependence. While local minima in classical neural networks are due to non-linear activations, in quantum neural networks local minima appear as a result of the quantum interference phenomenon. Finally, we empirically confirm that our constructions can indeed be hard instances in practice with typical gradient-based optimizers, which demonstrates the practical value of our findings."}}
{"id": "X8I5M9Ybl9O", "cdate": 1609459200000, "mdate": 1681662724352, "content": {"title": "Quantum Exploration Algorithms for Multi-Armed Bandits", "abstract": "Identifying the best arm of a multi-armed bandit is a central problem in bandit optimization. We study a quantum computational version of this problem with coherent oracle access to states encoding the reward probabilities of each arm as quantum amplitudes. Specifically, we provide an algorithm to find the best arm with fixed confidence based on variable-time amplitude amplification and estimation. This algorithm gives a quadratic speedup compared to the best possible classical result in terms of query complexity. We also prove a matching quantum lower bound (up to poly-logarithmic factors)."}}
{"id": "SDnFj2nRJgR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Quantum exploration algorithms for multi-armed bandits", "abstract": "Identifying the best arm of a multi-armed bandit is a central problem in bandit optimization. We study a quantum computational version of this problem with coherent oracle access to states encoding the reward probabilities of each arm as quantum amplitudes. Specifically, we show that we can find the best arm with fixed confidence using $\\tilde{O}\\bigl(\\sqrt{\\sum_{i=2}^n\\Delta^{\\smash{-2}}_i}\\bigr)$ quantum queries, where $\\Delta_{i}$ represents the difference between the mean reward of the best arm and the $i^\\text{th}$-best arm. This algorithm, based on variable-time amplitude amplification and estimation, gives a quadratic speedup compared to the best possible classical result. We also prove a matching quantum lower bound (up to poly-logarithmic factors)."}}
{"id": "KKfi2ezBghv", "cdate": 1577836800000, "mdate": 1681662724354, "content": {"title": "On Second-Order Group Influence Functions for Black-Box Predictions", "abstract": "With the rapid adoption of machine learning systems in sensitive applications, there is an increasing need to make black-box models explainable. Often we want to identify an influential group of tr..."}}
{"id": "HyeNo333jE", "cdate": 1557085340269, "mdate": null, "content": {"title": "A Spectral Method for Off-Policy Evaluation in Contextual Bandits under Distribution Shift", "abstract": "Contextual bandits capture the partial-feedback nature in an interactive system.\n\nAlgorithms for contextual bandits have wide applications in automated decision making such as recommender system and automated stock trading.\nEvaluating the cumulative reward of a target policy given the historical trajectories of a logging policy (i.e. off-policy evaluation) in contextual bandit setting is a task of importance, as it provides an estimate of the performance of a new policy without experimenting with it.\n\nOne (common and well-studied) solution is the Inverse Propensity Score (IPS) estimator. The idea of such methods is to estimate the expectation through importance sampling (i.e. re-weighting the data with a ratio associated with the logging and evaluation policy).\nExisting work assumes the stationarity of the distribution over context space, which is not always true in a real-world scenario.\nMore practical modeling considers the shift of context/reward distributions between the logged data and the contexts observed in order to evaluate a target policy in the future.\n\nSuch a problem is difficult in general due to the high-Dimensionality of the context space, as observed in our experiments.\nIn this paper, we propose an intent shift model which proposes to introduce an intent variable to capture the distributional shift on context and reward. Under the intent shift model, we propose a consistent spectral estimator for the reweighting factor and its finite-sample analysis and provide an MSE bound on the performance of our final estimator. Experiments show that our estimator outperforms the existing ones. "}}
