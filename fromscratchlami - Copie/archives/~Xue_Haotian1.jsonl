{"id": "15lSKp0wBnm", "cdate": 1663850464060, "mdate": null, "content": {"title": "3D-IntPhys: Learning 3D Visual Intuitive Physics for Fluids, Rigid Bodies, and Granular Materials", "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models purely from unlabeled images. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, in which we impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We evaluate the models on three challenging scenarios involving fluid, granular materials, and rigid objects, where standard detection and tracking methods are not applicable. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that, once trained, our model can achieve strong generalization in complex scenarios under extrapolate settings."}}
{"id": "WevBjPK4V3j", "cdate": 1663849951573, "mdate": null, "content": {"title": "Evaluation of Attribution Explanations without Ground Truth", "abstract": "This paper proposes a metric to evaluate the objectiveness of explanation methods of neural networks, i.e., the accuracy of the estimated importance/attribution/saliency values of input variables. This is crucial for the development of explainable AI, but it also presents significant challenges. The core challenge is that people usually cannot obtain the ground-truth value of the attribution of each input variable. Thus, we design a metric to evaluate the objectiveness of the attribution map without ground truth. Our metric is used to evaluate eight benchmark methods of attribution explanations, which provides new insights into attribution methods. We will release the code when the paper is accepted."}}
{"id": "MmC5WTB-z7", "cdate": 1632875442998, "mdate": null, "content": {"title": "A HYPOTHESIS FOR THE COGNITIVE DIFFICULTY OF IMAGES", "abstract": "This paper proposes a hypothesis to analyze the underlying reason for the cognitive difficulty of an image from two perspectives, i.e. a cognitive image usually makes a DNN strongly activated by cognitive concepts; discarding massive non-cognitive concepts may also help the DNN focus on cognitive concepts. Based on this hypothesis, we use multi-variate interactions to represent cognitive concepts and non-cognitive concepts contained in an image, and further design a set of image revision operations to decrease the cognitive difficulty of the image. In experiments, we found that the revised image was usually more cognitive than the original one. Besides, we also discovered that strengthening cognitive concepts and weakening non-cognitive concepts could improve the aesthetic level of an image."}}
{"id": "SkxwY3NtPr", "cdate": 1569438846763, "mdate": null, "content": {"title": "Towards a Unified Evaluation of Explanation Methods without Ground Truth", "abstract": "This paper proposes a set of criteria to evaluate the objectiveness of explanation\nmethods of neural networks, which is crucial for the development of explainable\nAI, but it also presents significant challenges. The core challenge is that people\nusually cannot obtain ground-truth explanations of the neural network. To this\nend, we design four metrics to evaluate the explanation result without ground-truth\nexplanations. Our metrics can be broadly applied to nine benchmark methods of\ninterpreting neural networks, which provides new insights of explanation methods."}}
