{"id": "NllxqEzaCU", "cdate": 1700776152559, "mdate": 1700776152559, "content": {"title": "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?", "abstract": "Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entity recognition can be used to improve performance on InfoSeek by retrieving relevant documents, showing a significant space for improvement."}}
{"id": "zA3YwchxMvP", "cdate": 1672531200000, "mdate": 1679982932564, "content": {"title": "Connecting Vision and Language with Video Localized Narratives", "abstract": ""}}
{"id": "YEGt_e_7kv", "cdate": 1672531200000, "mdate": 1679982932351, "content": {"title": "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?", "abstract": ""}}
{"id": "S0PRWzdI2Fv", "cdate": 1668597834787, "mdate": 1668597834787, "content": {"title": "Pali: A jointly-scaled multilingual language-image model", "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks.PaLI(PathwaysLanguage andImage model) extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design."}}
{"id": "nIb1rKYD80", "cdate": 1667349271373, "mdate": 1667349271373, "content": {"title": "All You May Need for VQA are Image Captions", "abstract": "Visual Question Answering (VQA) has benefited from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-theart zero-shot accuracy by double digits and achieve a level of robustness that lacks in the same model trained on human-annotated VQA data."}}
{"id": "mWVoBz4W0u", "cdate": 1663850174653, "mdate": null, "content": {"title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI, a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design."}}
{"id": "zL2tiWJKx_", "cdate": 1640995200000, "mdate": 1679982932842, "content": {"title": "All You May Need for VQA are Image Captions", "abstract": ""}}
{"id": "xj5KpNbSMx", "cdate": 1640995200000, "mdate": 1679982933092, "content": {"title": "MetaCLUE: Towards Comprehensive Visual Metaphors Research", "abstract": ""}}
{"id": "r2A3rFlFt9", "cdate": 1640995200000, "mdate": 1679982933127, "content": {"title": "2.5D visual relationship detection", "abstract": ""}}
{"id": "RraIcQMRDCd", "cdate": 1640995200000, "mdate": 1667414835064, "content": {"title": "PreSTU: Pre-Training for Scene-Text Understanding", "abstract": "The ability to read and reason about texts in an image is often lacking in vision-and-language (V&L) models. How can we learn V&L models that exhibit strong scene-text understanding (STU)? In this paper, we propose PreSTU, a simple pre-training recipe specifically designed for scene-text understanding. PreSTU combines a simple OCR-aware pre-training objective with a large-scale image-text dataset with off-the-shelf OCR signals. We empirically demonstrate the superiority of this pre-training objective on TextVQA, TextCaps, ST-VQA, and VizWiz-VQA. We also study which factors affect STU performance, where we highlight the importance of image resolution and dataset scale during pre-training."}}
