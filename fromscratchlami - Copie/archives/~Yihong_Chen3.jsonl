{"id": "Sq9Orta9l5i", "cdate": 1662812621142, "mdate": null, "content": {"title": "ReFactorGNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective", "abstract": "Factorisation-based Models~(FMs), such as DistMult, have enjoyed enduring success for Knowledge Graph Completion~(KGC) tasks, often outperforming Graph Neural Networks~(GNNs). However, unlike GNNs, FMs struggle to incorporate node features and to generalise to unseen nodes in inductive settings. Our work bridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new architecture draws upon \\textit{both} modelling paradigms, which previously were largely thought of as disjoint. Concretely, using a message-passing formalism, we show how FMs can be cast as GNNs by reformulating the gradient descent procedure as message-passing operations, which forms the basis of our ReFactorGNNs. Across a multitude of well-established KGC benchmarks, our ReFactorGNNs achieve comparable transductive performance to FMs, and state-of-the-art inductive performance while using an order of magnitude fewer parameters."}}
{"id": "81LQV4k7a7X", "cdate": 1652737739009, "mdate": null, "content": {"title": "ReFactor GNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective", "abstract": "Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node features and generalise to unseen nodes in inductive settings. Our work bridges the gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture draws upon $\\textit{both}$ modelling paradigms, which previously were largely thought of as disjoint. Concretely, using a message-passing formalism, we show how FMs can be cast as GNNs by reformulating the gradient descent procedure as message-passing operations, which forms the basis of our ReFactor GNNs. Across a multitude of well-established KGC benchmarks, our ReFactor GNNs achieve comparable transductive performance to FMs, and state-of-the-art inductive performance while using an order of magnitude fewer parameters."}}
{"id": "Qa3uS3H7-Le", "cdate": 1624392515817, "mdate": null, "content": {"title": "Relation Prediction as an Auxiliary Training Objective for Improving Multi-Relational Graph Representations", "abstract": "Learning good representations of multi-relational graphs is essential to downstream applications like knowledge base completion (KBC). In this paper, we propose a new self-supervised objective for mult-relational graph representation learning, via simply incorporating relation prediction into the commonly used 1vsAll objective. We analyse how this new objective impacts multi-relational learning in KBC. Experiments on a variety of datasets and models show that relation prediction can significantly improve entity ranking, the most widely used evaluation task for KBC, yielding a  $6.1\\%$ increase in MRR and $9.9\\%$ increase in Hits@1 on FB15k-237 as well as a $3.1 \\%$ increase in MRR and $3.4\\%$ in Hits@1 on Aristo-v4. Moreover, we observe that the proposed objective is especially effective on highly multi-relational datasets i.e. datasets with a large number of predicates, and generates better representations when larger embedding sizes are used."}}
{"id": "vQzcqQWIS0q", "cdate": 1601308188895, "mdate": null, "content": {"title": "Learnable Embedding sizes for Recommender Systems", "abstract": "The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy. PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models. Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99% parameters. As for the computation cost, PEP only brings an additional 20-30% time cost compare with base models. "}}
