{"id": "R0G0S6S-f4z", "cdate": 1702539709599, "mdate": 1702539709599, "content": {"title": "SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding", "abstract": "Large language models (LLMs) have shown impressive ability for open-domain NLP tasks. However, LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction and entity typing. To this end, we present SeqGPT, a bilingual (i.e., English and Chinese) open-source autoregressive model specially enhanced for open-domain natural language understanding. We express all NLU tasks with two atomic tasks, which define fixed instructions to restrict the input and output format but still ``open'' for arbitrarily varied label sets. The model is first instruction-tuned with extremely fine-grained labeled data synthesized by ChatGPT and then further fine-tuned by 233 different atomic tasks from 152 datasets across various domains. The experimental results show that SeqGPT has decent classification and extraction ability, and is capable of performing language understanding tasks on unseen domains. We also conduct empirical studies on the scaling of data and model size as well as on the transfer across tasks. Our model is accessible at this https URL."}}
{"id": "Eax7uFS014Z", "cdate": 1702532304087, "mdate": 1702532304087, "content": {"title": "Unveiling the Siren\u2019s Song: Towards Reliable Fact-Conflicting Hallucination Detection", "abstract": "Despite their impressive generative capabilities, LLMs are hindered by fact-conflicting hallucinations in real-world applications. The accurate identification of hallucinations in texts generated by LLMs, especially in complex inferential scenarios, is a relatively unexplored area. To address this gap, we present FACTCHD, a dedicated benchmark designed for the detection of fact-conflicting hallucinations from LLMs. FACTCHD features a diverse dataset that spans various factuality patterns, including vanilla, multi-hop, comparison, and set operation. A distinctive element of FACTCHD is its integration of fact-based evidence chains, significantly enhancing the depth of evaluating the detectors' explanations. Experiments on different LLMs expose the shortcomings of current approaches in detecting factual errors accurately. Furthermore, we introduce TRUTH-TRIANGULATOR which synthesizes reflective considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming to yield more credible detection through the amalgamation of predictive results and evidence."}}
{"id": "cVAPebwn4b4", "cdate": 1683884012803, "mdate": 1683884012803, "content": {"title": "DeepKE: A deep learning based knowledge extraction toolkit for knowledge base population", "abstract": "We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system for real-time extraction of various tasks, and a demo video."}}
{"id": "j6bCgFfRGo", "cdate": 1672531200000, "mdate": 1674708807615, "content": {"title": "A Concept Knowledge Graph for User Next Intent Prediction at Alipay", "abstract": "This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability."}}
{"id": "_8ZuxGYmGe_", "cdate": 1663849914982, "mdate": null, "content": {"title": "CAMVR: Context-Adaptive Multi-View Representation Learning for Dense Retrieval", "abstract": "The recently proposed MVR (Multi-View Representation) model achieves remarkable performance in open-domain dense retrieval. In MVR, the document can match with multi-view queries by encoding the document into multiple representations. However, these representations tend to collapse into the same one when the percentage of documents answering multiple queries in training data is low. In this paper, we propose a CAMVR (Context-Adaptive Multi-View Representation) learning framework, which explicitly avoids the collapse problem by aligning each viewer token with different document snippets. In CAMVR, each viewer token is placed before each snippet to capture the local and global information with the consideration that answers of different view queries may scatter in one document. In addition, the view of the snippet containing the answer is used to explicitly supervise the learning process, from which the interpretability of view representation is provided. The extensive experiments show that CAMVR outperforms the existing models and achieves state-of-the-art results."}}
{"id": "NRHajbzg8y0P", "cdate": 1663849861037, "mdate": null, "content": {"title": "Multimodal Analogical Reasoning over Knowledge Graphs", "abstract": "Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. We hope our work can deliver benefits and inspire future research. Code and datasets are available in https://github.com/zjunlp/MKG_Analogy."}}
{"id": "Q8GnGqT-GTJ", "cdate": 1652737328542, "mdate": null, "content": {"title": "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning", "abstract": "Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstrate that RetroPrompt can obtain better performance in both few-shot and zero-shot settings. Besides, we further illustrate that our proposed RetroPrompt can yield better generalization abilities with new datasets. Detailed analysis of memorization indeed reveals RetroPrompt can reduce the reliance of language models on memorization; thus, improving generalization for downstream tasks. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt."}}
{"id": "GX1zAZrTpfO", "cdate": 1652256467053, "mdate": 1652256467053, "content": {"title": "Location Based Link Prediction for Knowledge Graph", "abstract": "Link prediction is the basis of complement and analysis of knowledge graph. To leverage the rich location characteristics in the location-related entities and in their relationships, this paper presents a location-based knowledge graph link prediction method. This method first classifies relations by analyzing the semantic features of entities and relationships, and then proposes a method for mining features and rules based on location-based entities and relationships. Secondly, by mining the entity location features and rules, we construct the constrains on the prediction results of vectorization methods for entities and relations, and get the final results. Based on the experiments of WikiData, FB and WN datasets, we proved that the method has good effect on location-based relationship and entity link prediction."}}
{"id": "RUjm4h12Umd", "cdate": 1652255819479, "mdate": 1652255819479, "content": {"title": "Knowledge Collaborative Fine-tuning for Low-resource Knowledge Graph Completion", "abstract": "Knowledge graph completion can make the knowledge graph more complete. Unfortunately, most of existing methods on knowledge graph completion assume that the entities or relations in the knowledge graph have sufficient triple instances. However, there are a great deal of long-tail triples in general domains. Furthermore, it is challenging to obtain a large amount of high-quality annotation data in vertical domains. To address these issues, we propose a knowledge collaborative fine-tuning approach for low-resource knowledge graph completion. We leverage the structured knowledge to construct the initial prompt template and learn the optimal templates, labels and model parameters through a collaborative fine-tuning algorithm. Our method leverages the explicit structured knowledge in the knowledge graph and the implicit triple knowledge from the language model, which can be applied to the tasks of link prediction and relation extraction. Experimental results show that our approach can obtain state-of-the-art performance on three knowledge graph reasoning datasets and five relation extraction datasets."}}
{"id": "BrSzNDTrWfq", "cdate": 1647562076275, "mdate": null, "content": {"title": "From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer", "abstract": "Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset OpenBG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC."}}
