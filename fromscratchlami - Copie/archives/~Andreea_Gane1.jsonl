{"id": "jlTOaOPOuwk", "cdate": 1609459200000, "mdate": 1637118748226, "content": {"title": "Rethinking Attention with Performers", "abstract": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers."}}
{"id": "gB2r5TdLog", "cdate": 1609459200000, "mdate": 1681269301832, "content": {"title": "Improving Protein Function Annotation via Unsupervised Pre-training: Robustness, Efficiency, and Insights", "abstract": ""}}
{"id": "Ua6zuk0WRH", "cdate": 1601308078003, "mdate": null, "content": {"title": "Rethinking Attention with Performers", "abstract": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low  estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers. "}}
{"id": "w-0pKwz5NaR", "cdate": 1577836800000, "mdate": 1681269301494, "content": {"title": "Population-Based Black-Box Optimization for Biological Sequence Design", "abstract": ""}}
{"id": "M6dCfDIp0Dc", "cdate": 1546300800000, "mdate": 1681269301345, "content": {"title": "Direct Optimization through arg max for Discrete Variational Auto-Encoder", "abstract": ""}}
{"id": "HJ5AUm-CZ", "cdate": 1518730156070, "mdate": null, "content": {"title": "The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples", "abstract": "Hierarchical Bayesian methods have the potential to unify many related tasks (e.g. k-shot classification, conditional, and unconditional generation) by framing each as inference within a single generative model. We show that existing approaches for learning such models can fail on expressive generative networks such as PixelCNNs, by describing the global distribution with little reliance on latent variables. To address this, we develop a modification of the Variational Autoencoder in which encoded observations are decoded to new elements from the same class; the result, which we call a Variational Homoencoder (VHE), may be understood as training a hierarchical latent variable model which better utilises latent variables in these cases. Using this framework enables us to train a hierarchical PixelCNN for the Omniglot dataset, outperforming all existing models on test set likelihood. With a single model we achieve both strong one-shot generation and near human-level classification, competitive with state-of-the-art discriminative classifiers. The VHE objective extends naturally to richer dataset structures such as factorial or hierarchical categories, as we illustrate by training models to separate character content from simple variations in drawing style, and to generalise the style of an alphabet to new characters."}}
{"id": "80eBDPHTfZ", "cdate": 1514764800000, "mdate": 1681269301795, "content": {"title": "The Variational Homoencoder: Learning to learn high capacity generative models from few examples", "abstract": ""}}
{"id": "qlzdZDdhsZ", "cdate": 1451606400000, "mdate": 1681269301844, "content": {"title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems", "abstract": ""}}
{"id": "nOu50D-8qn", "cdate": 1388534400000, "mdate": 1681269301415, "content": {"title": "Learning with Maximum A-Posteriori Perturbation Models", "abstract": ""}}
