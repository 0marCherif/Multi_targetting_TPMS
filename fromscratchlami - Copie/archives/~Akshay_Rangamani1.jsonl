{"id": "p3_z68kKrus", "cdate": 1601308204713, "mdate": null, "content": {"title": "For interpolating kernel machines, minimizing the norm of the ERM solution minimizes stability", "abstract": "We study the average CV Leave One Out stability of kernel ridge-less regression and derive corresponding risk bounds. We show that the interpolating solution with minimum norm minimizes a bound on CV Leave One Out stability, which in turn is controlled by the condition number of the empirical kernel matrix. The latter can be characterized in the asymptotic regime where both the dimension and cardinality of the data go to infinity. Under the assumption of random kernel matrices, the corresponding test error should be expected to follow a double descent curve."}}
{"id": "qX_dZMOrTyB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Deep Learning-Based Target Tracking and Classification for Low Quality Videos Using Coded Aperture Cameras", "abstract": "Compressive sensing has seen many applications in recent years. One type of compressive sensing device is the Pixel-wise Code Exposure (PCE) camera, which has low power consumption and individual control of pixel exposure time. In order to use PCE cameras for practical applications, a time consuming and lossy process is needed to reconstruct the original frames. In this paper, we present a deep learning approach that directly performs target tracking and classification in the compressive measurement domain without any frame reconstruction. In particular, we propose to apply You Only Look Once (YOLO) to detect and track targets in the frames and we propose to apply Residual Network (ResNet) for classification. Extensive simulations using low quality optical and mid-wave infrared (MWIR) videos in the SENSIAC database demonstrated the efficacy of our proposed approach."}}
{"id": "Wm1Da0-xFF", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Scale Invariant Flatness Measure for Deep Network Minima", "abstract": "It has been empirically observed that the flatness of minima obtained from training deep networks seems to correlate with better generalization. However, for deep networks with positively homogeneous activations, most measures of sharpness/flatness are not invariant to rescaling of the network parameters, corresponding to the same function. This means that the measure of flatness/sharpness can be made as small or as large as possible through rescaling, rendering the quantitative measures meaningless. In this paper we show that for deep networks with positively homogenous activations, these rescalings constitute equivalence relations, and that these equivalence relations induce a quotient manifold structure in the parameter space. Using this manifold structure and an appropriate metric, we propose a Hessian-based measure for flatness that is invariant to rescaling. We use this new measure to confirm the proposition that Large-Batch SGD minima are indeed sharper than Small-Batch SGD minima."}}
{"id": "NOtHpZwJRV0", "cdate": 1546300800000, "mdate": null, "content": {"title": "Target tracking and classification using compressive sensing camera for SWIR videos", "abstract": "The pixel-wise code exposure (PCE) camera is a compressive sensing camera that has several advantages, such as low power consumption and high compression ratio. Moreover, one notable advantage is the capability to control individual pixel exposure time. Conventional approaches of using PCE cameras involve a time-consuming and lossy process to reconstruct the original frames and then use those frames for target tracking and classification. Otherwise, conventional approaches will fail if compressive measurements are used. In this paper, we present a deep learning approach that directly performs target tracking and classification in the compressive measurement domain without any frame reconstruction. Our approach has two parts: tracking and classification. The tracking has been done via detection using You Only Look Once (YOLO), and the classification is achieved using residual network (ResNet). Extensive simulations using short-wave infrared (SWIR) videos demonstrated the efficacy of our proposed approach."}}
{"id": "dLF87qSd6zj", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Greedy Pursuit Algorithm for Separating Signals from Nonlinear Compressive Observations", "abstract": "In this paper we study the unmixing problem which aims to separate a set of structured signals from their superposition. In this paper, we consider the scenario in which the mixture is observed via nonlinear compressive measurements. We present a fast, robust, greedy algorithm called Unmixing Matching Pursuit (UnmixMP) to solve this problem. We prove rigorously that the algorithm can recover the constituents from their noisy nonlinear compressive measurements with arbitrarily small error. We compare our algorithm to the Demixing with Hard Thresholding (DHT) algorithm [1], in a number of experiments on synthetic and real data."}}
{"id": "cD-wsCwp_2c", "cdate": 1514764800000, "mdate": null, "content": {"title": "Automated software vulnerability detection with machine learning", "abstract": "Thousands of security vulnerabilities are discovered in production software each year, either reported publicly to the Common Vulnerabilities and Exposures database or discovered internally in proprietary code. Vulnerabilities often manifest themselves in subtle ways that are not obvious to code reviewers or the developers themselves. With the wealth of open source code available for analysis, there is an opportunity to learn the patterns of bugs that can lead to security vulnerabilities directly from data. In this paper, we present a data-driven approach to vulnerability detection using machine learning, specifically applied to C and C++ programs. We first compile a large dataset of hundreds of thousands of open-source functions labeled with the outputs of a static analyzer. We then compare methods applied directly to source code with methods applied to artifacts extracted from the build process, finding that source-based models perform better. We also compare the application of deep neural network models with more traditional models such as random forests and find the best performance comes from combining features learned by deep models with tree-based models. Ultimately, our highest performing model achieves an area under the precision-recall curve of 0.49 and an area under the ROC curve of 0.87."}}
{"id": "LdJXDTYloOL", "cdate": 1514764800000, "mdate": null, "content": {"title": "ChieF: A Change Pattern based Interpretable Failure Analyzer", "abstract": "Discovering the underlying dynamics leading up to an industrial asset failure is an important problem to be solved for successful development of Predictive Maintenance techniques. Existing work has largely focused on building complex ML/AI models for developing Predictive Maintenance solution patterns, but has largely avoided developing methods to explain the underlying failure dynamics. In this paper, we use an old but significantly improved change-pattern based technique to analyze IoT sensor data and failure information to generate useful and interpretable failure-centric insight. We discuss a solution pattern that we call ChieF, which when applied on multi-variate time series datasets, discover the leading failure indicators, generate associative patterns among multiple features, and output temporal dynamics of changes. Experimental analysis of ChieF on four datasets uncovers insights that may be valuable for predictive maintenance."}}
{"id": "5zDcM72Ve1_", "cdate": 1514764800000, "mdate": null, "content": {"title": "Sparse Coding and Autoencoders", "abstract": "In this work we study the landscape of squared loss of an Autoencoder when the data generative model is that of \u201cSparse Coding\u201d/\u201cDictionary Learning\u201d. The neural net considered is an <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{n}$</tex> mapping and has a single ReLU activation layer of size <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$h &gt; n$</tex> . The net has access to vectors <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$y\\in \\mathbb{R}^{n}$</tex> obtained as <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$y=A^{\\ast}x^{\\ast}$</tex> where <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$x^{\\ast}\\in \\mathbb{R}^{h}$</tex> are sparse high dimensional vectors and <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$A^{\\ast}\\in \\mathbb{R}^{n\\times h}$</tex> is an overcomplete incoherent matrix. Under very mild distributional assumptions on <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$x^{\\ast}$</tex> , we prove that the norm of the expected gradient of the squared loss function is asymptotically (in sparse code dimension) negligible for all points in a small neighborhood of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$A^{\\ast}$</tex> . This is supported with experimental evidence using synthetic data. We conduct experiments to suggest that <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$A^{\\ast}$</tex> sits at the bottom of a well in the landscape and we also give experiments showing that gradient descent on this loss function gets columnwise very close to the original dictionary even with far enough initialization. Along the way we prove that a layer of ReLU gates can be set up to automatically recover the support of the sparse codes. Since this property holds independent of the loss function we believe that it could be of independent interest. A full version of this paper is accessible at: https://arxiv.org/abs/1708.03735"}}
{"id": "0r9TSgdKVi", "cdate": 1514764800000, "mdate": null, "content": {"title": "Reconstruction-Free Deep Convolutional Neural Networks for Partially Observed Images", "abstract": "Conventional image discrimination tasks are performed on fully observed images. In challenging real imaging scenarios, where sensing systems are energy demanding or need to operate with limited bandwidth and exposure-time budgets, or defective pixels, where the data collected often suffers from missing information, and this makes the task extremely hard. In this paper, we leverage Convolutional Neural Networks (CNNs) to extract information from partially observed images. While pre-trained CNNs fail significantly even with such a small percentage of the input missing, our proposed framework demonstrates the ability to overcome it after training on fully-observed and partially-observed images at a few observation ratios. We demonstrate that our method is indeed reconstruction-free, retraining-free and generalizable to previously untrained-on observation ratios and it remains effective in two different visual tasks - image classification and object detection. Our framework performs well even for test images with only 10% of pixels available and outperforms the reconstruct-then-classify pipeline in these challenging scenarios for small observation fractions."}}
{"id": "rji1WE-BCpy", "cdate": 1483228800000, "mdate": null, "content": {"title": "Critical Points Of An Autoencoder Can Provably Recover Sparsely Used Overcomplete Dictionaries", "abstract": "In \"Dictionary Learning\" one tries to recover incoherent matrices $A^* \\in \\mathbb{R}^{n \\times h}$ (typically overcomplete and whose columns are assumed to be normalized) and sparse vectors $x^* \\in \\mathbb{R}^h$ with a small support of size $h^p$ for some $0 <p < 1$ while having access to observations $y \\in \\mathbb{R}^n$ where $y = A^*x^*$. In this work we undertake a rigorous analysis of whether gradient descent on the squared loss of an autoencoder can solve the dictionary learning problem. The \"Autoencoder\" architecture we consider is a $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ mapping with a single ReLU activation layer of size $h$. Under very mild distributional assumptions on $x^*$, we prove that the norm of the expected gradient of the standard squared loss function is asymptotically (in sparse code dimension) negligible for all points in a small neighborhood of $A^*$. This is supported with experimental evidence using synthetic data. We also conduct experiments to suggest that $A^*$ is a local minimum. Along the way we prove that a layer of ReLU gates can be set up to automatically recover the support of the sparse codes. This property holds independent of the loss function. We believe that it could be of independent interest."}}
