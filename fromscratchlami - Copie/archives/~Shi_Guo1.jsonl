{"id": "p8sLbLH-HP", "cdate": 1640995200000, "mdate": 1668053260667, "content": {"title": "A Differentiable Two-stage Alignment Scheme for Burst Image Reconstruction with Large Shift", "abstract": ""}}
{"id": "fLZBqA-BA1", "cdate": 1640995200000, "mdate": 1668053260645, "content": {"title": "Efficient Long-Range Attention Network for Image Super-Resolution", "abstract": "Recently, transformer-based methods have demonstrated impressive results in various vision tasks, including image super-resolution (SR), by exploiting the self-attention (SA) for feature extraction. However, the computation of SA in most existing transformer based models is very expensive, while some employed operations may be redundant for the SR task. This limits the range of SA computation and consequently limits the SR performance. In this work, we propose an efficient long-range attention network (ELAN) for image SR. Specifically, we first employ shift convolution (shift-conv) to effectively extract the image local structural information while maintaining the same level of complexity as 1 $$\\,\\times \\,$$ 1 convolution, then propose a group-wise multi-scale self-attention (GMSA) module, which calculates SA on non-overlapped groups of features using different window sizes to exploit the long-range image dependency. A highly efficient long-range attention block (ELAB) is then built by simply cascading two shift-conv with a GMSA module, which is further accelerated by using a shared attention mechanism. Without bells and whistles, our ELAN follows a fairly simple design by sequentially cascading the ELABs. Extensive experiments demonstrate that ELAN obtains even better results against the transformer-based SR models but with significantly less complexity. The source codes of ELAN can be found at https://github.com/xindongzhang/ELAN."}}
{"id": "oFhxoOYzg4", "cdate": 1609459200000, "mdate": 1668053260693, "content": {"title": "HDR Video Reconstruction: A Coarse-to-fine Network and A Real-world Benchmark Dataset", "abstract": ""}}
{"id": "JDFJ3urvGE", "cdate": 1609459200000, "mdate": 1668053260692, "content": {"title": "Joint Denoising and Demosaicking With Green Channel Prior for Real-World Burst Images", "abstract": ""}}
{"id": "HsLE0aMgd6H", "cdate": 1546300800000, "mdate": null, "content": {"title": "Toward Convolutional Blind Denoising of Real Photographs.", "abstract": "While deep convolutional neural networks (CNNs) have achieved impressive success in image denoising with additive white Gaussian noise (AWGN), their performance remains limited on real-world noisy photographs. The main reason is that their learned models are easy to overfit on the simplified AWGN model which deviates severely from the complicated real-world noise model. In order to improve the generalization ability of deep CNN denoisers, we suggest training a convolutional blind denoising network (CBDNet) with more realistic noise model and real-world noisy-clean image pairs. On the one hand, both signal-dependent noise and in-camera signal processing pipeline is considered to synthesize realistic noisy images. On the other hand, real-world noisy photographs and their nearly noise-free counterparts are also included to train our CBDNet. To further provide an interactive strategy to rectify denoising result conveniently, a noise estimation subnetwork with asymmetric learning to suppress under-estimation of noise level is embedded into CBDNet. Extensive experimental results on three datasets of real-world noisy pho- tographs clearly demonstrate the superior performance of CBDNet over state-of-the-arts in terms of quantitative met- rics and visual quality. The code has been made available at https://github.com/GuoShi28/CBDNet."}}
{"id": "S1VJ3TWdZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Learning Dynamic Guidance for Depth Image Enhancement", "abstract": "The depth images acquired by consumer depth sensors (e.g., Kinect and ToF) usually are of low resolution and insufficient quality. One natural solution is to incorporate with high resolution RGB camera for exploiting their statistical correlation. However, most existing methods are intuitive and limited in characterizing the complex and dynamic dependency between intensity and depth images. To address these limitations, we propose a weighted analysis representation model for guided depth image enhancement, which advances the conventional methods in two aspects: (i) task driven learning and (ii) dynamic guidance. First, we generalize the analysis representation model by including a guided weight function for dependency modeling. And the task-driven learning formulation is introduced to obtain the optimized guidance tailored to specific enhancement task. Second, the depth image is gradually enhanced along with the iterations, and thus the guidance should also be dynamically adjusted to account for the updating of depth image. To this end, stage-wise parameters are learned for dynamic guidance. Experiments on guided depth image upsampling and noisy depth image restoration validate the effectiveness of our method."}}
