{"id": "zalV0SL7ajs", "cdate": 1677628800000, "mdate": 1684035188556, "content": {"title": "Introduction to the Special Issue on Domain-Specific System-on-Chip Architectures and Run-Time Management Techniques", "abstract": ""}}
{"id": "_qCNgEWheBd", "cdate": 1677628800000, "mdate": 1684035190222, "content": {"title": "Domain-Specific Architectures: Research Problems and Promising Approaches", "abstract": "Process technology-driven performance and energy efficiency improvements have slowed down as we approach physical design limits. General-purpose manycore architectures attempt to circumvent this challenge, but they have a significant performance and energy-efficient gap compared to special-purpose solutions. Domain-specific architectures (DSAs), an instance of heterogeneous architectures, efficiently combine general-purpose cores and specialized hardware accelerators to boost energy efficiency and provide programming flexibility. Indeed, the hardware, software, and systems aspects in DSAs are highly tailored to maximize the energy efficiency of applications in a target domain. As DSAs and their conceptualization advance rapidly, there is a strong need to understand the research problems that need immediate attention. This article discusses the primary research directions in the design and runtime management of DSAs. Then, it surveys some promising approaches and highlights the outstanding research needs."}}
{"id": "u0MHV19E2n", "cdate": 1673287854235, "mdate": null, "content": {"title": "Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation", "abstract": "Transformers have shown great success in medical image segmentation. However, transformers may exhibit a limited generalization ability due to the underlying single-scale self-attention (SA) mechanism. In this paper, we address this issue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT) backbone network, which improves the generalizability of the model by computing SA at multiple scales. We also incorporate an attention-based decoder, namely Cascaded Attention Decoding (CASCADE), for further refinement of the multi-stage features generated by MERIT. Finally, we introduce an effective multi-stage feature mixing loss aggregation (MUTATION) method for better model training via implicit ensembling. Our experiments on two widely used medical image segmentation benchmarks (i.e., Synapse Multi-organ and ACDC) demonstrate the superior performance of MERIT over state-of-the-art methods. Our MERIT architecture and MUTATION loss aggregation can be used with other downstream medical image and semantic segmentation tasks."}}
{"id": "vlTagZYSht", "cdate": 1672531200000, "mdate": 1684035188626, "content": {"title": "An IIoT-Based Approach to the Integrated Management of Machinery in the Construction Industry", "abstract": "In recent years, considerable advances in connecting industrial equipment to the Internet allowed a higher level of operational efficiency, productivity, and automation. Nevertheless, the construction industry still presents challenging requirements, like long-distance wireless communication, interconnection of out-of- coverage areas, constant mobility of machinery, and support for legacy and proprietary systems. All these requirements pose different challenges when compared with traditional smart factory Industry 4.0 solutions. This paper discusses some of the current key questions regarding fleet management in the construction industry, identifies requirements for heavy-duty machinery, and proposes an Industrial Internet of Things (IIoT)-based solution for the integrated monitoring of such machinery. Also, it proposes and presents an open, innovative solution for the integrated management of non-standardized civil construction vehicle technologies. The developed prototype uses the J1939 protous and protocol to collect machinery status and Long-Range Wide Area Network (LoRaWAN) to communicate the monitored data. The solution was assessed at the construction site of the new port of Sines in Portugal. A detailed description of the proposed system is provided, along with information on trials and evaluation results regarding its performance. Additionally, the paper provides insights into open issues and challenges in this field and how our solution can contribute to overcoming them."}}
{"id": "uXkb6jUtWC", "cdate": 1672531200000, "mdate": 1684035190197, "content": {"title": "Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation", "abstract": "Transformers have shown great success in medical image segmentation. However, transformers may exhibit a limited generalization ability due to the underlying single-scale self-attention (SA) mechanism. In this paper, we address this issue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT) backbone network, which improves the generalizability of the model by computing SA at multiple scales. We also incorporate an attention-based decoder, namely Cascaded Attention Decoding (CASCADE), for further refinement of multi-stage features generated by MERIT. Finally, we introduce an effective multi-stage feature mixing loss aggregation (MUTATION) method for better model training via implicit ensembling. Our experiments on two widely used medical image segmentation benchmarks (i.e., Synapse Multi-organ, ACDC) demonstrate the superior performance of MERIT over state-of-the-art methods. Our MERIT architecture and MUTATION loss aggregation can be used with downstream medical image and semantic segmentation tasks."}}
{"id": "kt3mWqxY_K", "cdate": 1672531200000, "mdate": 1681795808967, "content": {"title": "Efficient On-device Training via Gradient Filtering", "abstract": "Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\\times$ speedup and 77.1% memory savings on ImageNet classification with only 0.1% accuracy loss. Finally, our method is easy to implement and deploy; over 20$\\times$ speedup and 90% energy savings have been observed compared to highly optimized baselines in MKLDNN and CUDNN on NVIDIA Jetson Nano. Consequently, our approach opens up a new direction of research with a huge potential for on-device training."}}
{"id": "h5NCUfkqkYj", "cdate": 1672531200000, "mdate": 1684035188764, "content": {"title": "MOHAWK: Mobility and Heterogeneity-Aware Dynamic Community Selection for Hierarchical Federated Learning", "abstract": "The recent developments in Federated Learning (FL) focus on optimizing the learning process for data, hardware, and model heterogeneity. However, most approaches assume all devices are stationary, charging, and always connected to the Wi-Fi when training on local data. We argue that when real devices move around, the FL process is negatively impacted and the device energy spent for communication is increased. To mitigate such effects, we propose a dynamic community selection algorithm which improves the communication energy efficiency and two new aggregation strategies that boost the learning performance in Hierarchical FL (HFL). For real mobility traces, we show that compared to state-of-the-art HFL solutions, our approach is scalable, achieves better accuracy on multiple datasets, converges up to 3.88 \u00d7 faster, and is significantly more energy efficient for both IID and non-IID scenarios.1"}}
{"id": "cFFQdFKmWeA", "cdate": 1672531200000, "mdate": 1684035189732, "content": {"title": "Demo Abstract: A Hardware Prototype Targeting Federated Learning with User Mobility and Device Heterogeneity", "abstract": "This paper presents a new hardware prototype to explore how centralized and hierarchical federated learning systems are impacted by real-world devices distribution, availability, and heterogeneity. Our results show considerable learning performance degradation and wasted energy during training when users mobility is accounted for. Hence, we provide a prototype that can be used as a design exploration tool to better design, calibrate and evaluate FL systems for real-world deployment."}}
{"id": "VAxNAdUTCpg", "cdate": 1672531200000, "mdate": 1684035188802, "content": {"title": "A Blockchain-Based Privacy-Preserving Model for Consent and Transparency in Human-Centered Internet of Things", "abstract": "The inclusion of human-related aspects in the Internet of Things paradigm leads to the development of models and solutions that address several challenges of our society. The adoption of these novel approaches is expanding rapidly on the road to what is now termed Society 5.0. However, leaving aside all the potential benefits that come from the interaction with these novel systems, an increasing number of people are concerned with the amount of data these systems can collect and share with data requesters. Several legal frameworks call for the adoption of practices regarding data protection and pushing for data control by the data owners. Unfortunately, most human-centric IoT-based systems lack mechanisms for managing resources and data in the user domain. Moreover, these tasks are typically delegated to a central entity; this necessarily implies a relationship of trust and can lead to problems related to transparency. To cope with these issues in this paper we present a privacy-preserving model that leverages the intrinsic features of the blockchain technology for consent management and transparency in Human-Centered Internet of Things environments. To show the feasibility of our approach, the proposed model is implemented, deployed in a test environment, and assessed using realistic scenarios."}}
{"id": "OFHy54RC7b", "cdate": 1672531200000, "mdate": 1681748253706, "content": {"title": "ZiCo: Zero-shot NAS via Inverse Coefficient of Variation on Gradients", "abstract": "Neural Architecture Search (NAS) is widely used to automatically obtain the neural network with the best performance among a large number of candidate architectures. To reduce the search time, zero-shot NAS aims at designing training-free proxies that can predict the test performance of a given architecture. However, as shown recently, none of the zero-shot proxies proposed to date can actually work consistently better than a naive proxy, namely, the number of network parameters (#Params). To improve this state of affairs, as the main theoretical contribution, we first reveal how some specific gradient properties across different samples impact the convergence rate and generalization capacity of neural networks. Based on this theoretical analysis, we propose a new zero-shot proxy, ZiCo, the first proxy that works consistently better than #Params. We demonstrate that ZiCo works better than State-Of-The-Art (SOTA) proxies on several popular NAS-Benchmarks (NASBench101, NATSBench-SSS/TSS, TransNASBench-101) for multiple applications (e.g., image classification/reconstruction and pixel-level prediction). Finally, we demonstrate that the optimal architectures found via ZiCo are as competitive as the ones found by one-shot and multi-shot NAS methods, but with much less search time. For example, ZiCo-based NAS can find optimal architectures with 78.1%, 79.4%, and 80.4% test accuracy under inference budgets of 450M, 600M, and 1000M FLOPs, respectively, on ImageNet within 0.4 GPU days. Our code is available at https://github.com/SLDGroup/ZiCo."}}
