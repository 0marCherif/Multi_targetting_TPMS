{"id": "Z5J22uu3Lgj", "cdate": 1704067200000, "mdate": 1708055152077, "content": {"title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations", "abstract": "Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the subspace is found, we can directly project the original embeddings into the null space to boost language agnosticism without finetuning. We systematically evaluate our method on various tasks including the challenging language-agnostic QA retrieval task. Empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs."}}
{"id": "QpIWQs8gATa", "cdate": 1704067200000, "mdate": 1708055152020, "content": {"title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes", "abstract": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation."}}
{"id": "Gkki04RcvWF", "cdate": 1704067200000, "mdate": 1708055152124, "content": {"title": "Augment before You Try: Knowledge-Enhanced Table Question Answering via Table Expansion", "abstract": "Table question answering is a popular task that assesses a model's ability to understand and interact with structured data. However, the given table often does not contain sufficient information for answering the question, necessitating the integration of external knowledge. Existing methods either convert both the table and external knowledge into text, which neglects the structured nature of the table; or they embed queries for external sources in the interaction with the table, which complicates the process. In this paper, we propose a simple yet effective method to integrate external information in a given table. Our method first constructs an augmenting table containing the missing information and then generates a SQL query over the two tables to answer the question. Experiments show that our method outperforms strong baselines on three table QA benchmarks. Our code is publicly available at https://github.com/UCSB-NLP-Chang/Augment_tableQA."}}
{"id": "n50fIO-OT5Q", "cdate": 1698796800000, "mdate": 1702460657859, "content": {"title": "Clustering of conversational bandits with posterior sampling for user preference learning and elicitation", "abstract": ""}}
{"id": "9no8H5jM9S", "cdate": 1690872454280, "mdate": 1690872454280, "content": {"title": "Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets", "abstract": "Federated learning involves collaborative training with private data from multiple platforms, while not violating data privacy. We study the problem of federated domain adaptation for Named Entity Recognition (NER), where we seek to transfer knowledge across different platforms with data of multiple domains. In addition, we consider a practical and challenging scenario, where NER datasets of different platforms of federated learning are annotated with heterogeneous tag sets, ie, different sets of entity types. The goal is to train a global model with federated learning, such that it can predict with a complete tag set, ie, with all the occurring entity types for data across all platforms. To cope with the heterogeneous tag sets in a multi-domain setting, we propose a distillation approach along with a mechanism of instance weighting to facilitate knowledge transfer across platforms. Besides, we release two re-annotated clinic NER datasets, for testing the proposed method in the clinic domain. Our method shows superior empirical performance for NER with federated learning."}}
{"id": "U-irVmiKy3", "cdate": 1684350014204, "mdate": 1684350014204, "content": {"title": "Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling", "abstract": "Supervised training of existing deep learning models for sequence labeling relies on large scale labeled datasets. Such datasets are generally created with crowd-source labeling. However, crowd-source labeling for tasks of sequence labeling can be expensive and time-consuming. Further, crowd-source labeling by external annotators may not be appropriate for data that contains user private information. Considering the above limitations of crowd-source labeling, we study interactive sequence labeling that allows training directly with the user feedback, which alleviates the annotation cost and maintains the user privacy. We identify two bias, namely, context bias and feedback bias, by formulating interactive sequence labeling via a Structural Causal Model (SCM). To alleviate the context and feedback bias based on the SCM, we identify the frequent context tokens as confounders in the backdoor adjustment and further propose an entropy-based modulation that is inspired by information theory. entities more sample-efficiently. With extensive experiments, we validate that our approach can effectively alleviate the biases and our models can be efficiently learnt with the user feedback."}}
{"id": "LytT95COWf", "cdate": 1684123636501, "mdate": 1684123636501, "content": {"title": "Understanding and improving recurrent networks for human activity recognition by continuous attention", "abstract": "Deep neural networks, including recurrent networks, have been successfully applied to human activity recognition. Unfortunately, the final representation learned by recurrent networks might encode some noise (irrelevant signal components, unimportant sensor modalities, etc.). Besides, it is difficult to interpret the recurrent networks to gain insight into the models' behavior. To address these issues, we propose two attention models for human activity recognition: temporal attention and sensor attention. These two mechanisms adaptively focus on important signals and sensor modalities. To further improve the understandability and mean Fl score, we add continuity constraints, considering that continuous sensor signals are more robust than discrete ones. We evaluate the approaches on three datasets and obtain state-of-the-art results. Furthermore, qualitative analysis shows that the attention learned by the models agree well with human intuition.\n"}}
{"id": "6venG64mwf", "cdate": 1683906851820, "mdate": 1683906851820, "content": {"title": "Few-Shot Class-Incremental Learning for Named Entity Recognition", "abstract": "Previous  work  of  class-incremental  learning for  Named  Entity  Recognition  (NER)  relies on the assumption that there exists abundance of labeled data for the training of new classes.In  this  work,  we  study  a  more  challenging but  practical  problem,i.e.,  few-shot  class-incremental learning for NER, where an NER model is trained with only few labeled samples of the new classes, without forgetting knowledge of the old ones.   To alleviate the problem of catastrophic forgetting in few-shot class-incremental  learning,  we  generate  synthetic data of the old classes using the trained NER model, augmenting the training of new classes.We further develop a framework that distills from the NER model from previous steps with both synthetic data, and real data from the cur-rent training set.   Experimental results show that our approach achieves significant improvements over existing baselines."}}
{"id": "sgFibh-1Ir2", "cdate": 1672531200000, "mdate": 1708055152041, "content": {"title": "Interpretable Unsupervised Log Anomaly Detection", "abstract": "Modern software systems\u2019 increasing complexity and scale makes it challenging to accurately detect system issues and outages, which have been tackled as an anomaly detection task. Conventionally, such anomalous events barely happen, and annotating them is time-consuming and impractical in big data streams. Even with automated anomaly detection, resolving issues promptly is a remaining challenge that can only be done by providing specific contexts such as root causes, target/affected services, and more. To address these fundamentally important problems, we present Grid Transformer (GT), a framework designed to detect and explain $\\log$ anomalies in an unsupervised setting. We first train an Auto-Encoder model to generate pseudo labels. Then, we train the proposed grid transformer that not only predicts anomalies but also generates why a particular instance is an anomaly. Through extensive experiments, we demonstrate the effectiveness of our approach where it is shown to outperform the other $\\log$ anomaly detection models by 20% while also able to generate time-wise and message-wise explanations of the anomalies."}}
{"id": "sC6viayWwYf", "cdate": 1672531200000, "mdate": 1695961183695, "content": {"title": "Understanding Demonstration-based Learning from a Causal Perspective", "abstract": ""}}
