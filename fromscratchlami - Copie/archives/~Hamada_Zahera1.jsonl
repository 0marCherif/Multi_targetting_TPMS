{"id": "zXXJ0Ij-MR", "cdate": 1675324470740, "mdate": 1675324470740, "content": {"title": "MULTPAX: Keyphrase Extraction Using Language Models and Knowledge Graphs", "abstract": "Keyphrase extraction aims to identify a small set of phrases that best describe the content of text. The automatic generation of keyphrases has become essential for many natural language applications such as text categorization, indexing, and summarization. In this paper, we propose MULTPAX, a multitask framework for extracting present and absent keyphrases using pre-trained language models and knowledge graphs. In particular, our framework contains three components: first, MULTPAX identifies present keyphrases from an input document. Then, MULTPAX links with external knowledge graphs to get more relevant phrases. Finally, MULTPAX ranks the extracted phrases based on their semantic relatedness to the input document and return top-k phrases as a final output. We conducted several experiments on four benchmark datasets to evaluate the performance of MULTPAX against different state-of-the-art baselines. The evaluation results demonstrate that our approach significantly outperforms the state-of-the-art baselines, with a significance t-test \ud835\udc5d<0.041\n. Our source code and datasets are public available at https://github.com/dice-group/MultPAX."}}
{"id": "c13CDPR-SCT", "cdate": 1672531200000, "mdate": 1695992423830, "content": {"title": "Using Pre-Trained Language Models for Abstractive DBPEDIA Summarization: A Comparative Study", "abstract": "Purpose: This study addresses the limitations of current short abstracts of DBPEDIA entities, which often lack a comprehensive overview due to their creating method (i.e., selecting the first two-three sentences from the full DBPEDIA abstracts). Methodology: We leverage pre-trained language models to generate abstractive summaries of DBPEDIA abstracts in six languages (English, French, German, Italian, Spanish, and Dutch). We performed several experiments to assess the quality of generated summaries by language models. In particular, we evaluated the generated summaries using human judgments and automated metrics (Self-ROUGE and BERTScore). Additionally, we studied the correlation between human judgments and automated metrics in evaluating the generated summaries under different aspects: informativeness, coherence, conciseness, and fluency. Findings: Pre-trained language models generate summaries more concise and informative than existing short abstracts. Specifically, BART-based models effectively overcome the limitations of DBPEDIA short abstracts, especially for longer ones. Moreover, we show that BERTScore and ROUGE-1 are reliable metrics for assessing the informativeness and coherence of the generated summaries with respect to the full DBPEDIA abstracts. We also find a negative correlation between conciseness and human ratings. Furthermore, fluency evaluation remains challenging without human judgment. Value: This study has significant implications for various applications in machine learning and natural language processing that rely on DBPEDIA resources. By providing succinct and comprehensive summaries, our approach enhances the quality of DBPEDIA abstracts and contributes to the semantic web community."}}
{"id": "r1juslJ0QV_", "cdate": 1642076414703, "mdate": 1642076414703, "content": {"title": "Fine-tuned BERT Model for Multi-Label Tweets Classification", "abstract": "In this paper, we describe our approach to classify disaster-related tweets into multilabel information types (i.e, labels). We aim to filter first relevant tweets during disasters. Then, we assign tweets relevant information types. Information types can be SearchAndRescue, MovePeople and Volunteer. We employ a fine-tuned BERT model with 10 BERT layers. Further, we submitted our approach to the TREC-IS 2019 challenge, the evaluation results showed that our approach outperforms the F1-score of median score in identifying actionable information."}}
{"id": "x4YsXoQZj_", "cdate": 1642076367605, "mdate": 1642076367605, "content": {"title": "ProBERT: Product Data Classification with Fine-tuning BERT Model", "abstract": "Abstract. In this paper, we describe our submission to the semantic web challenge on mining the product data in websites (MWPD2020). The dataset provided 19K instances of product data collected from various websites. The task is to predict the category, defined as hierarchical taxonomy as provided in the training set, of the product titles in the test set. In our approach, we present a simple BERT-based model (dubbed ProBERT) for classifying product data into one or more categories. We trained our system on products titles and descriptions to learn semantic representation. The participated systems are evaluated using weighted-average precision, recall and F1-score. "}}
{"id": "b366wU4e3m4", "cdate": 1642076304499, "mdate": 1642076304499, "content": {"title": "ASSET: A Semi-supervised Approach for Entity Typing in Knowledge Graphs", "abstract": "Entity typing in knowledge graphs (KGs) aims to infer missing types of entities and might be considered one of the most significant tasks of knowledge graph construction since type information is highly relevant for querying, quality assurance, and KG applications. While supervised learning approaches for entity typing have been proposed, they require large amounts of (manually) labeled data, which can be expensive to obtain. In this paper, we propose a novel approach for KG entity typing that leverages semi-supervised learning from massive unlabeled data. Our approach follows a teacher-student paradigm that allows combining a small amount of labeled data with a large amount of unlabeled data to boost performance. We conduct several experiments on two benchmarking datasets (FB15k-ET and YAGO43k-ET). Our results demonstrate the effectiveness of our approach in improving entity typing in KGs. Given type information for only 1% of entities, our approach ASSET predicts missing types with a F1-score of 0.47 and 0.64 on the datasets FB15k-ET and YAGO43k-ET, respectively, outperforming supervised baselines. "}}
{"id": "qtSTjrx4T1Z", "cdate": 1640995200000, "mdate": 1695933199903, "content": {"title": "Tab2Onto: Unsupervised Semantification with Knowledge Graph Embeddings", "abstract": "A large amount of data is generated every day by different systems and applications. In many cases, this data comes in a tabular format that lacks semantic representation and poses new challenges in data modelling. For semantic applications, it then becomes necessary to lift the data to a richer representation, such as a knowledge graph that adheres to a semantic ontology. We propose Tab2Onto, an unsupervised approach for learning ontologies from tabular data using knowledge graph embeddings, clustering, and a human in the loop. We conduct a set of experiments to investigate our approach on a benchmarking dataset from a medical domain and learn the ontology of diseases. Our code and datasets are provided at https://tab2onto.dice-research.org/ ."}}
{"id": "e2qNZXxFcE", "cdate": 1640995200000, "mdate": 1695992423835, "content": {"title": "MultPAX: Keyphrase Extraction Using Language Models and Knowledge Graphs", "abstract": "Keyphrase extraction aims to identify a small set of phrases that best describe the content of text. The automatic generation of keyphrases has become essential for many natural language applications such as text categorization, indexing, and summarization. In this paper, we propose MultPAX, a multitask framework for extracting present and absent keyphrases using pre-trained language models and knowledge graphs. In particular, our framework contains three components: first, MultPAX identifies present keyphrases from an input document. Then, MultPAX links with external knowledge graphs to get more relevant phrases. Finally, MultPAX ranks the extracted phrases based on their semantic relatedness to the input document and return top-k phrases as a final output. We conducted several experiments on four benchmark datasets to evaluate the performance of MultPAX against different state-of-the-art baselines. The evaluation results demonstrate that our approach significantly outperforms the state-of-the-art baselines, with a significance t-test $$p < 0.041$$ . Our source code and datasets are public available at https://github.com/dice-group/MultPAX ."}}
{"id": "dRuHjv4sdLv", "cdate": 1609459200000, "mdate": 1695992423843, "content": {"title": "I-AID: Identifying Actionable Information From Disaster-Related Tweets", "abstract": "Social media plays a significant role in disaster management by providing valuable data about affected people, donations, and help requests. Recent studies highlight the need to filter information on social media into fine-grained content labels. However, identifying useful information from massive amounts of social media posts during a crisis is a challenging task. In this paper, we propose I-AID, a multimodel approach to automatically categorize tweets into multi-label information types and filter critical information from the enormous volume of social media data. I-AID incorporates three main components: i) a BERT-based encoder to capture the semantics of a tweet and represent as a low-dimensional vector, ii) a graph attention network (GAT) to apprehend correlations between tweets\u2019 words/entities and the corresponding information types, and iii) a  <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Relation Network</i>  as a learnable distance metric to compute the similarity between tweets and their corresponding information types in a supervised way. We conducted several experiments on two real publicly-available datasets. Our results indicate that I-AID outperforms state-of-the-art approaches in terms of weighted average F1 score by +6% and +4% on the TREC-IS dataset and COVID-19 Tweets, respectively."}}
{"id": "KJs0Iw4OPoW", "cdate": 1609459200000, "mdate": 1695933200056, "content": {"title": "ASSET: A Semi-supervised Approach for Entity Typing in Knowledge Graphs", "abstract": "Entity typing in knowledge graphs (KGs) aims to infer missing types of entities and might be considered one of the most significant tasks of knowledge graph construction since type information is highly relevant for querying, quality assurance, and KG applications. While supervised learning approaches for entity typing have been proposed, they require large amounts of (manually) labeled data, which can be expensive to obtain. In this paper, we propose a novel approach for KG entity typing that leverages semi-supervised learning from massive unlabeled data. Our approach follows a teacher-student paradigm that allows combining a small amount of labeled data with a large amount of unlabeled data to boost performance. We conduct several experiments on two benchmarking datasets (FB15k-ET and YAGO43k-ET). Our results demonstrate the effectiveness of our approach in improving entity typing in KGs. Given type information for only 1% of entities, our approach ASSET predicts missing types with a F1-score of 0.47 and 0.64 on the datasets FB15k-ET and YAGO43k-ET, respectively, outperforming supervised baselines."}}
{"id": "4KJ119hxpW", "cdate": 1609459200000, "mdate": 1695992423843, "content": {"title": "ONTOCONNECT: Domain-Agnostic Ontology Alignment using Graph Embedding with Negative Sampling", "abstract": "The ontology alignment task aims at linking two or more different ontologies from the same domain or different domains. Over the years, many techniques have been proposed for ontology instance alignment, schema alignment, and link discovery. Most of the available approaches require human intervention or work within a specific domain and follow a rule-based and logic-based approach. In this paper, we present an ontology alignment approach using graph embedding with negative sampling that is independent of the domain and does not require any human intervention."}}
