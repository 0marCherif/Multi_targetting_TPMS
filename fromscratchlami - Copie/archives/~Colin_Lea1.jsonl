{"id": "xrrsr0RTLZi", "cdate": 1672531200000, "mdate": 1692798202058, "content": {"title": "Latent Phrase Matching for Dysarthric Speech", "abstract": "Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies have emphasized interest in personalized speech models from people with atypical speech patterns. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases."}}
{"id": "7OiLqzTjZh", "cdate": 1672531200000, "mdate": 1681649819666, "content": {"title": "From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition", "abstract": ""}}
{"id": "sm02bid1jt", "cdate": 1640995200000, "mdate": 1681649819674, "content": {"title": "Nonverbal Sound Detection for Disordered Speech", "abstract": ""}}
{"id": "fKnJQSJNd6o", "cdate": 1640995200000, "mdate": 1681649819665, "content": {"title": "Reflow: Automatically Improving Touch Interactions in Mobile Applications through Pixel-based Refinements", "abstract": ""}}
{"id": "iQ6v7p2RSht", "cdate": 1609459200000, "mdate": 1632918929265, "content": {"title": "SEP-28k: A Dataset for Stuttering Event Detection from Podcasts with People Who Stutter", "abstract": "The ability to automatically detect stuttering events in speech could help speech pathologists track an individual\u2019s fluency over time or help improve speech recognition systems for people with atypical speech patterns. Despite increasing interest in this area, existing public datasets are too small to build generalizable dysfluency detection systems and lack sufficient annotations. In this work, we introduce Stuttering Events in Podcasts (SEP-28k), a dataset containing over 28k clips labeled with five event types including blocks, prolongations, sound repetitions, word repetitions, and interjections. Audio comes from public podcasts largely consisting of people who stutter interviewing other people who stutter. We benchmark a set of acoustic models on SEP-28k and the public FluencyBank dataset and highlight how simply increasing the amount of training data improves relative detection performance by 28% and 24% F1 on each. Annotations from over 32k clips across both datasets will be publicly released."}}
{"id": "Tp8en9jqwKz", "cdate": 1609459200000, "mdate": 1681649819680, "content": {"title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech", "abstract": ""}}
{"id": "DaUkBgBVOfs", "cdate": 1609459200000, "mdate": 1632918929033, "content": {"title": "Audio- and Gaze-driven Facial Animation of Codec Avatars", "abstract": "Codec Avatars are a recent class of learned, photorealistic face models that accurately represent the geometry and texture of a person in 3D (i.e., for virtual reality), and are almost indistinguishable from video [28]. In this paper we describe the first approach to animate these parametric models in real-time which could be deployed on commodity virtual reality hardware using audio and/or eye tracking. Our goal is to display expressive conversations between individuals that exhibit important social signals such as laughter and excitement solely from la-tent cues in our lossy input signals. To this end we collected over 5 hours of high frame rate 3D face scans across three participants including traditional neutral speech as well as expressive and conversational speech. We investigate a multimodal fusion approach that dynamically identifies which sensor encoding should animate which parts of the face at any time. See the supplemental video which demonstrates our ability to generate full face motion far beyond the typically neutral lip articulations seen in competing work: https://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/"}}
{"id": "7bLGrRQCR7", "cdate": 1609459200000, "mdate": 1632918929403, "content": {"title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech", "abstract": "Dysfluencies and variations in speech pronunciation can severely degrade speech recognition performance, and for many individuals with moderate-to-severe speech disorders, voice operated systems do not work. Current speech recognition systems are trained primarily with data from fluent speakers and as a consequence do not generalize well to speech with dysfluencies such as sound or word repetitions, sound prolongations, or audible blocks. The focus of this work is on quantitative analysis of a consumer speech recognition system on individuals who stutter and production-oriented approaches for improving performance for common voice assistant tasks (i.e., \"what is the weather?\"). At baseline, this system introduces a significant number of insertion and substitution errors resulting in intended speech Word Error Rates (isWER) that are 13.64\\% worse (absolute) for individuals with fluency disorders. We show that by simply tuning the decoding parameters in an existing hybrid speech recognition system one can improve isWER by 24\\% (relative) for individuals with fluency disorders. Tuning these parameters translates to 3.6\\% better domain recognition and 1.7\\% better intent recognition relative to the default setup for the 18 study participants across all stuttering severities."}}
{"id": "6zwrhjj3T33", "cdate": 1609459200000, "mdate": 1632918929177, "content": {"title": "SEP-28k: A Dataset for Stuttering Event Detection From Podcasts With People Who Stutter", "abstract": "The ability to automatically detect stuttering events in speech could help speech pathologists track an individual's fluency over time or help improve speech recognition systems for people with atypical speech patterns. Despite increasing interest in this area, existing public datasets are too small to build generalizable dysfluency detection systems and lack sufficient annotations. In this work, we introduce Stuttering Events in Podcasts (SEP-28k), a dataset containing over 28k clips labeled with five event types including blocks, prolongations, sound repetitions, word repetitions, and interjections. Audio comes from public podcasts largely consisting of people who stutter interviewing other people who stutter. We benchmark a set of acoustic models on SEP-28k and the public FluencyBank dataset and highlight how simply increasing the amount of training data improves relative detection performance by 28\\% and 24\\% F1 on each. Annotations from over 32k clips across both datasets will be publicly released."}}
{"id": "iTFRA-QdTz", "cdate": 1582750028646, "mdate": null, "content": {"title": "Recognizing Surgical Activities with Recurrent Neural Networks", "abstract": "We apply recurrent neural networks to the task of recognizing surgical activities from robot kinematics. Prior work in this area focuses on recognizing short, low-level activities, or gestures, and has been based on variants of hidden Markov models and conditional random fields. In contrast, we work on recognizing both gestures and longer, higher-level activites, or maneuvers, and we model the mapping from kinematics to gestures/maneuvers with recurrent neural networks. To our knowledge, we are the first to apply recurrent neural networks to this task. Using a single model and a single set of hyperparameters, we match state-of-the-art performance for gesture recognition and advance state-of-the-art performance for maneuver recognition, in terms of both accuracy and edit distance."}}
