{"id": "zD9k0jAqmx", "cdate": 1684156706461, "mdate": 1684156706461, "content": {"title": "One Network to Approximate Them All: Amortized Variational Inference of Ising Ground States", "abstract": "For a wide range of combinatorial optimization problems, finding the optimal solutions is equivalent to finding the ground states of corresponding Ising Hamilto- nians. Recent work shows that these ground states are found more efficiently by variational approaches using autoregressive models than by traditional methods. In contrast to previous works, where for every problem instance a new model has to be trained, we aim at a single model that approximates the ground states for a whole family of Hamiltonians. We demonstrate that autoregregressive neural networks can be trained to achieve this goal and are able to generalize across a class of problems. We iteratively approximate the ground state based on a representation of the Hamiltonian that is provided by a graph neural network. Our experiments show that solving a large number of related problem instances by a single model can be considerably more efficient than solving them individually."}}
{"id": "0_zI1KBpU1", "cdate": 1683880419324, "mdate": 1683880419324, "content": {"title": "One Network to Approximate Them All: Amortized Variational Inference of Ising Ground States", "abstract": "For a wide range of combinatorial optimization problems, finding the optimal solutions is equivalent to finding the ground states of corresponding Ising Hamiltonians. Recent work shows that these ground states are found more efficiently by variational approaches using autoregressive models than by traditional methods. In contrast to previous works, where for every problem instance a new model has to be trained, we aim at a single model that approximates the ground states for a whole family of Hamiltonians. We demonstrate that autoregregressive neural networks can be trained to achieve this goal and are able to generalize across a class of problems. We iteratively approximate the ground state based on a representation of the Hamiltonian that is provided by a graph neural network. Our experiments show that solving a large number of related problem instances by a single model can be considerably more efficient than solving them individually.\n"}}
{"id": "Ea6hRpGjZDp", "cdate": 1683481278121, "mdate": null, "content": {"title": "Unconstrained generation of synthetic antibody\u2013antigen structures to guide machine learning methodology for antibody specificity prediction", "abstract": "Machine learning (ML) is a key technology for accurate prediction of antibody\u2013antigen binding. Two orthogonal problems hinder the application of ML to antibody-specificity prediction and the benchmarking thereof: the lack of a unified ML formalization of immunological antibody-specificity prediction problems and the unavailability of large-scale synthetic datasets to benchmark real-world relevant ML methods and dataset design. Here we developed the Absolut! software suite that enables parameter-based unconstrained generation of synthetic lattice-based three-dimensional antibody\u2013antigen-binding structures with ground-truth access to conformational paratope, epitope and affinity. We formalized common immunological antibody-specificity prediction problems as ML tasks and confirmed that for both sequence- and structure-based tasks, accuracy-based rankings of ML methods trained on experimental data hold for ML methods trained on Absolut!-generated data. The Absolut! framework has the potential to enable real-world relevant development and benchmarking of ML strategies for biotherapeutics design."}}
{"id": "Us6BtPZGei3", "cdate": 1676591080968, "mdate": null, "content": {"title": "Learning to Modulate pre-trained Models in RL", "abstract": "Reinforcement Learning (RL) has experienced great success in complex games and simulations. However, RL agents are often highly specialized for a particular task, and it is difficult to adapt a trained agent to a new task. In supervised learning, an established paradigm is multi-task pre-training followed by fine-tuning. A similar trend is emerging in RL, where agents are pre-trained on data collections that comprise a multitude of tasks. Despite these developments, it remains an open challenge how to adapt such pre-trained agents to novel tasks while retaining performance on the pre-training tasks. In this regard, we pre-train an agent on a set of tasks from the Meta-World benchmark suite and adapt it to tasks from Continual-World. We conduct a comprehensive comparison of fine-tuning methods originating from supervised learning in our setup. Our findings show that fine-tuning is feasible, but for existing methods, performance on previously learned tasks often deteriorates. Therefore, we propose a novel approach that avoids forgetting by modulating the information flow of the pre-trained model. Our method outperforms existing fine-tuning approaches, and achieves state-of-the-art performance on the Continual-World benchmark. To facilitate future research in this direction, we collect datasets for all Meta-World tasks and make them publicly available."}}
{"id": "9CvMkA8oi8O", "cdate": 1665251237669, "mdate": null, "content": {"title": "InfODist: Online distillation with Informative rewards improves generalization in Curriculum Learning", "abstract": "Curriculum learning (CL) is an essential part of human learning, just as reinforcement learning (RL) is. However, CL agents that are trained using RL with neural networks produce limited generalization to later tasks in the curriculum. We show that online distillation using learned informative rewards tackles this problem. Here, we consider a reward to be informative if it is positive when the agent makes progress towards the goal and negative otherwise. Thus, an informative reward allows an agent to learn immediately to avoid states which are irrelevant to the task. And, the value and policy networks do not utilize their limited capacity to fit targets for these irrelevant states. Consequently, this improves generalization to later tasks. Our contributions: First, we propose InfODist, an online distillation method that makes use of informative rewards to significantly improve generalization in CL. Second, we show that training with informative rewards ameliorates the capacity loss phenomenon that was previously attributed to non-stationarities during the training process. Third, we show that learning from task-irrelevant states explains the capacity loss and subsequent impaired generalization. In conclusion, our work is a crucial step toward scaling curriculum learning to complex real world tasks.\n"}}
{"id": "rEZ1Hxd51R", "cdate": 1665251231826, "mdate": null, "content": {"title": "Foundation Models for History Compression in Reinforcement Learning", "abstract": "Agents interacting under partial observability require access to past observations via a memory mechanism in order to approximate the true state of the environment.\nRecent work suggests that leveraging language as abstraction provides benefits for creating a representation of past events.\nHistory Compression via Language Models (HELM) leverages a pretrained Language Model (LM) for representing the past. \nIt relies on a randomized attention mechanism to translate environment observations to token embeddings.\nIn this work, we show that the representations resulting from this attention mechanism can collapse under certain conditions. \nThis causes blindness of the agent to subtle changes in the environment that may be crucial for solving a certain task. \nWe propose a solution to this problem consisting of two parts. \nFirst, we improve upon HELM by substituting the attention mechanism with a feature-wise centering-and-scaling operation. \nSecond, we take a step toward semantic history compression by leveraging foundation models, such as CLIP, to encode observations, which further improves performance. \nBy combining foundation models, our agent is able to solve the challenging MiniGrid-Memory environment.\nSurprisingly, however, our experiments suggest that this is not due to the semantic enrichment of the representation presented to the LM, but rather due to the discriminative power provided by CLIP. \nWe make our code publicly available at https://github.com/ml-jku/helm."}}
{"id": "KmxQ3v7nMmd", "cdate": 1664943347077, "mdate": null, "content": {"title": "Foundation Models for History Compression in Reinforcement Learning", "abstract": "Agents interacting under partial observability require access to past observations via a memory mechanism in order to approximate the true state of the environment.\nRecent work suggests that leveraging language as abstraction provides benefits for creating a representation of past events.\nHistory Compression via Language Models (HELM) leverages a pretrained Language Model (LM) for representing the past. \nIt relies on a randomized attention mechanism to translate environment observations to token embeddings.\nIn this work, we show that the representations resulting from this attention mechanism can collapse under certain conditions. \nThis causes blindness of the agent to subtle changes in the environment that may be crucial for solving a certain task. \nWe propose a solution to this problem consisting of two parts. \nFirst, we improve upon HELM by substituting the attention mechanism with a feature-wise centering-and-scaling operation. \nSecond, we take a step toward semantic history compression by leveraging foundation models, such as CLIP, to encode observations, which further improves performance. \nBy combining foundation models, our agent is able to solve the challenging MiniGrid-Memory environment.\nSurprisingly, however, our experiments suggest that this is not due to the semantic enrichment of the representation presented to the LM, but rather due to the discriminative power provided by CLIP. \nWe make our code publicly available at https://github.com/ml-jku/helm."}}
{"id": "97C6klf5shp", "cdate": 1664358386713, "mdate": null, "content": {"title": "Toward Semantic History Compression for Reinforcement Learning", "abstract": "Agents interacting under partial observability require access to past observations via a memory mechanism in order to approximate the true state of the environment.\nRecent work suggests that leveraging language as abstraction provides benefits for creating a representation of past events.\nHistory Compression via Language Models (HELM) leverages a pretrained Language Model (LM) for representing the past. \nIt relies on a randomized attention mechanism to translate environment observations to token embeddings.\nIn this work, we show that the representations resulting from this attention mechanism can collapse under certain conditions. \nThis causes blindness of the agent to subtle changes in the environment that may be crucial for solving a certain task. \nWe propose a solution to this problem consisting of two parts. \nFirst, we improve upon HELM by substituting the attention mechanism with a feature-wise centering-and-scaling operation. \nSecond, we take a step toward semantic history compression by leveraging foundation models, such as CLIP, to encode observations, which further improves performance. \nBy combining foundation models, our agent is able to solve the challenging MiniGrid-Memory environment.\nSurprisingly, however, our experiments suggest that this is not due to the semantic enrichment of the representation presented to the LM, but rather due to the discriminative power provided by CLIP. \nWe make our code publicly available at https://github.com/ml-jku/helm."}}
{"id": "dIX34JWnIAL", "cdate": 1664248837741, "mdate": null, "content": {"title": "Robust task-specific adaption of models for drug-target interaction prediction", "abstract": "HyperNetworks have been established as an effective technique to achieve fast adaptation of parameters for neural networks. Recently, HyperNetworks conditioned on descriptors of tasks have improved multi-task generalization in various domains, such as personalized federated learning and neural architecture search. Especially powerful results were achieved in few- and zero-shot settings, attributed to the increased information sharing by the HyperNetwork. With the rise of new diseases fast discovery of drugs is needed which requires proteo-chemometric models that are able to generalize drug-target interaction predictions in low-data scenarios. State-of-the-art methods apply a few fully-connected layers to concatenated learned embeddings of the protein target and drug compound. In this work, we develop a task-conditioned HyperNetwork approach for the problem of predicting drug-target interactions in drug discovery. We show that when model parameters are predicted for the fully-connected layers processing the drug compound embedding, based on the protein target embedding, predictive performance can be improved over previous methods. Two additional components of our architecture, a) switching to L1 loss, and b) integrating a context module for proteins, further boost performance and robustness. On an established benchmark for proteo-chemometrics models, our architecture outperforms previous methods in all settings, including few- and zero-shot settings. In an ablation study, we analyze the importance of each of the components of our HyperNetwork approach."}}
{"id": "XrMWUuEevr", "cdate": 1663850106543, "mdate": null, "content": {"title": "Context-enriched molecule representations improve few-shot drug discovery", "abstract": "A central task in computational drug discovery is to construct models from known active molecules to find further promising molecules for subsequent screening. However, typically only very few active molecules are known. Therefore, few-shot learning methods have the potential to improve the effectiveness of this critical phase of the drug discovery process. We introduce a new method for few-shot drug discovery. Its main idea is to enrich a molecule representation by knowledge about known context or reference molecules. Our novel concept for molecule representation enrichment is to associate molecules from both the support set and the query set with a large set of reference (context) molecules through a modern Hopfield network. Intuitively, this enrichment step is analogous to a human expert who would associate a given molecule with familiar molecules whose properties are known. The enrichment step reinforces and amplifies the covariance structure of the data, while simultaneously removing spurious correlations arising from the decoration of molecules. Our approach is compared with other few-shot methods for drug discovery on the FS-Mol benchmark dataset. On FS-Mol, our approach outperforms all compared methods and therefore sets a new state-of-the art for few-shot learning in drug discovery. An ablation study shows that the enrichment step of our method is the key to improve the predictive quality. In a domain shift experiment, we further demonstrate the robustness of our method."}}
